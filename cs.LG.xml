<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11095</link><description>&lt;p&gt;
&#28608;&#21457;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#20197;&#23454;&#29616;&#38646;-shot&#20219;&#21153;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization. (arXiv:2305.11095v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;Whisper&#27169;&#22411;&#65292;&#25104;&#21151;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#30340;&#25552;&#31034;&#27604;&#40664;&#35748;&#25552;&#31034;&#24615;&#33021;&#25552;&#21319;&#20102;10%&#21040;45&#65285;&#65292;&#23637;&#29616;&#20102;Whisper&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#22810;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;Web&#35268;&#27169;&#35821;&#38899;&#27169;&#22411;Whisper&#30340;&#26032;&#20852;&#21151;&#33021;&#65292;&#22312;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#35843;&#25972;&#27169;&#22411;&#21518;&#65292;&#36866;&#24212;&#20102;&#26410;&#35265;&#36807;&#30340;AVSR&#65292;CS-ASR&#21644;ST&#19977;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#35201;&#20040;&#21033;&#29992;&#21478;&#19968;&#20010;&#22823;&#35268;&#27169;&#27169;&#22411;&#65292;&#35201;&#20040;&#31616;&#21333;&#22320;&#25805;&#20316;&#40664;&#35748;&#25552;&#31034;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#20351;&#36825;&#19977;&#20010;&#38646;-shot&#20219;&#21153;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;10%&#21040;45&#65285;&#65292;&#29978;&#33267;&#22312;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;SotA&#30417;&#30563;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;Whisper&#30340;&#35768;&#22810;&#26377;&#36259;&#23646;&#24615;&#65292;&#21253;&#25324;&#20854;&#25552;&#31034;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#21475;&#38899;&#30340;&#20559;&#22909;&#20197;&#21450;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22810;&#35821;&#35328;&#29702;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/jasonppy/PromptingWhisper&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#22495;&#36866;&#24212;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;UniDA&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30446;&#26631;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11092</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Universal Domain Adaptation from Foundation Models. (arXiv:2305.11092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#22495;&#36866;&#24212;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;UniDA&#26041;&#27861;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#34920;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#30446;&#26631;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;CLIP&#25110;DINOv2&#65289;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#35270;&#35273;&#20219;&#21153;&#20013;&#21331;&#36234;&#30340;&#23398;&#20064;&#21644;&#36716;&#31227;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#24182;&#36866;&#24212;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#22522;&#30784;&#27169;&#22411;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#36890;&#29992;&#22495;&#36866;&#24212;&#65288;UniDA&#65289;&#65292;&#21363;&#20351;&#29992;&#28304;&#22495;&#26631;&#35760;&#25968;&#25454;&#21644;&#30446;&#26631;&#22495;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#36866;&#24212;&#30446;&#26631;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#23545;&#29616;&#26377;&#29366;&#24577;&#19979;&#30340;UniDA&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#23613;&#31649;&#22522;&#30784;&#27169;&#22411;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#20165;&#22312;&#28304;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#30340;UniDA&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#36229;&#36234;&#22522;&#20934;&#12290;&#36825;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;UniDA&#38656;&#35201;&#26032;&#30340;&#30740;&#31350;&#21162;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#30446;&#26631;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning and transferring capabilities on a wide range of visual tasks, by training on a large corpus of data and adapting to specific downstream tasks. It is, however, interesting that foundation models have not been fully explored for universal domain adaptation (UniDA), which is to learn models using labeled data in a source domain and unlabeled data in a target one, such that the learned models can successfully adapt to the target data. In this paper, we make comprehensive empirical studies of state-of-the-art UniDA methods using foundation models. We first demonstrate that, while foundation models greatly improve the performance of the baseline methods that train the models on the source data alone, existing UniDA methods generally fail to improve over the baseline. This suggests that new research efforts are very necessary for UniDA using foundation models. To this end, we propose a very simple method of target 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#20844;&#24335;&#26469;&#23454;&#29616;&#20219;&#24847;&#31163;&#25955;&#29366;&#24577;Markov&#36807;&#31243;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Blackout Diffusion&#8221;&#30340;&#24212;&#29992;&#65292;&#23427;&#21487;&#20197;&#20174;&#31354;&#22270;&#20687;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.11089</link><description>&lt;p&gt;
&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;&#65306;Blackout Diffusion
&lt;/p&gt;
&lt;p&gt;
Blackout Diffusion: Generative Diffusion Models in Discrete-State Spaces. (arXiv:2305.11089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#20844;&#24335;&#26469;&#23454;&#29616;&#20219;&#24847;&#31163;&#25955;&#29366;&#24577;Markov&#36807;&#31243;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Blackout Diffusion&#8221;&#30340;&#24212;&#29992;&#65292;&#23427;&#21487;&#20197;&#20174;&#31354;&#22270;&#20687;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#26031;&#25193;&#25955;&#36807;&#31243;&#26469;&#35757;&#32451;&#21453;&#21521;&#36716;&#25442;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#20110;&#20174;&#39640;&#26031;&#22122;&#22768;&#20013;&#29983;&#25104;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25968;&#25454;&#24120;&#24120;&#22788;&#20110;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20013;&#65292;&#21253;&#25324;&#35768;&#22810;&#31185;&#23398;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20351;&#29992;&#31934;&#30830;&#65288;&#32780;&#19981;&#26159;&#21464;&#20998;&#65289;&#20998;&#26512;&#65292;&#38024;&#23545;&#20219;&#24847;&#31163;&#25955;&#29366;&#24577;Markov&#36807;&#31243;&#24320;&#21457;&#20102;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#29702;&#35770;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#19982;&#29616;&#26377;&#30340;&#36830;&#32493;&#29366;&#24577;&#39640;&#26031;&#25193;&#25955;&#20197;&#21450;&#20854;&#20182;&#31163;&#25955;&#25193;&#25955;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#30830;&#23450;&#20102;&#36830;&#32493;&#26102;&#38388;&#24773;&#20917;&#19979;&#30340;&#21453;&#21521;&#38543;&#26426;&#36807;&#31243;&#21644;&#24471;&#20998;&#20989;&#25968;&#65292;&#20197;&#21450;&#31163;&#25955;&#26102;&#38388;&#24773;&#20917;&#19979;&#30340;&#21453;&#21521;&#26144;&#23556;&#12290;&#20316;&#20026;&#27492;&#26694;&#26550;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;Blackout Diffusion&#8221;&#65292;&#23427;&#23398;&#20250;&#20102;&#20174;&#31354;&#22270;&#20687;&#20013;&#29983;&#25104;&#26679;&#26412;&#32780;&#19981;&#26159;&#20174;&#22122;&#22768;&#20013;&#29983;&#25104;&#12290;&#22312;CIFAR-10&#12289;Binarized MNIST&#21644;CelebA&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#23427;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical generative diffusion models rely on a Gaussian diffusion process for training the backward transformations, which can then be used to generate samples from Gaussian noise. However, real world data often takes place in discrete-state spaces, including many scientific applications. Here, we develop a theoretical formulation for arbitrary discrete-state Markov processes in the forward diffusion process using exact (as opposed to variational) analysis. We relate the theory to the existing continuous-state Gaussian diffusion as well as other approaches to discrete diffusion, and identify the corresponding reverse-time stochastic process and score function in the continuous-time setting, and the reverse-time mapping in the discrete-time setting. As an example of this framework, we introduce ``Blackout Diffusion'', which learns to produce samples from an empty image instead of from noise. Numerical experiments on the CIFAR-10, Binarized MNIST, and CelebA datasets confirm the feasibili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;&#65288;DDCF&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#24847;&#22270;&#21644;&#20559;&#22909;&#22240;&#32032;&#36827;&#34892;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#24314;&#31435;&#29420;&#31435;&#30340;&#31232;&#30095;&#20559;&#22909;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.11084</link><description>&lt;p&gt;
&#20559;&#22909;&#36824;&#26159;&#24847;&#22270;&#65311;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Preference or Intent? Double Disentangled Collaborative Filtering. (arXiv:2305.11084v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;&#65288;DDCF&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23545;&#24847;&#22270;&#21644;&#20559;&#22909;&#22240;&#32032;&#36827;&#34892;&#20998;&#31163;&#65292;&#24182;&#36890;&#36807;&#35299;&#32544;&#34920;&#31034;&#24314;&#31435;&#29420;&#31435;&#30340;&#31232;&#30095;&#20559;&#22909;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36873;&#25321;&#29289;&#21697;&#26102;&#36890;&#24120;&#26377;&#19981;&#21516;&#30340;&#24847;&#22270;&#65292;&#32780;&#22312;&#30456;&#21516;&#24847;&#22270;&#19979;&#20182;&#20204;&#30340;&#20559;&#22909;&#20063;&#21487;&#33021;&#19981;&#21516;&#12290;&#20256;&#32479;&#30340;&#21327;&#21516;&#36807;&#28388;&#26041;&#27861;&#36890;&#24120;&#23558;&#24847;&#22270;&#21644;&#20559;&#22909;&#22240;&#32032;&#32416;&#32544;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#65292;&#36825;&#26174;&#33879;&#38480;&#21046;&#20102;&#25512;&#33616;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#37325;&#35299;&#32544;&#21327;&#21516;&#36807;&#28388;&#65288;DDCF&#65289;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#26041;&#27861;&#12290;&#19968;&#32423;&#35299;&#32544;&#26159;&#20026;&#20102;&#23558;&#24847;&#22270;&#21644;&#20559;&#22909;&#30340;&#24433;&#21709;&#22240;&#32032;&#20998;&#24320;&#65292;&#32780;&#31532;&#20108;&#32423;&#35299;&#32544;&#26159;&#20026;&#20102;&#26500;&#24314;&#29420;&#31435;&#30340;&#31232;&#30095;&#20559;&#22909;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DDCF&#26041;&#27861;&#22312;&#25512;&#33616;&#31934;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
People usually have different intents for choosing items, while their preferences under the same intent may also different. In traditional collaborative filtering approaches, both intent and preference factors are usually entangled in the modeling process, which significantly limits the robustness and interpretability of recommendation performances. For example, the low-rating items are always treated as negative feedback while they actually could provide positive information about user intent. To this end, in this paper, we propose a two-fold representation learning approach, namely Double Disentangled Collaborative Filtering (DDCF), for personalized recommendations. The first-level disentanglement is for separating the influence factors of intent and preference, while the second-level disentanglement is performed to build independent sparse preference representations under individual intent with limited computational complexity. Specifically, we employ two variational autoencoder net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21475;&#32617;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#22312;&#21475;&#32617;&#20154;&#33080;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#30340;&#21487;&#33021;&#36129;&#29486;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.11077</link><description>&lt;p&gt;
&#21475;&#32617;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Face Detection Algorithms for Masked Face Detection. (arXiv:2305.11077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21475;&#32617;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#19968;&#32452;&#20195;&#34920;&#24615;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#22312;&#21475;&#32617;&#20154;&#33080;&#26816;&#27979;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24615;&#33021;&#30340;&#21487;&#33021;&#36129;&#29486;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#38656;&#35201;&#24212;&#23545;&#35768;&#22810;&#25361;&#25112;&#65292;&#20363;&#22914;&#23039;&#24577;&#65292;&#20809;&#29031;&#21644;&#27604;&#20363;&#30340;&#21464;&#21270;&#12290;&#26368;&#36817;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#30340;&#19968;&#20010;&#23376;&#31867;&#21035;&#26159;&#36974;&#25377;&#33080;&#37096;&#26816;&#27979;&#65292;&#25110;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25506;&#27979;&#25140;&#21475;&#32617;&#30340;&#33080;&#37096;&#12290;&#22312;&#26032;&#20896;&#30123;&#24773;&#29190;&#21457;&#21518;&#30340;&#19977;&#24180;&#37324;&#65292;&#29616;&#26377;&#20154;&#33080;&#26816;&#27979;&#31639;&#27861;&#22312;&#21475;&#32617;&#20154;&#33080;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#28982;&#32570;&#20047;&#23454;&#35777;&#35777;&#25454;&#12290;&#26412;&#25991;&#39318;&#20808;&#31616;&#35201;&#22238;&#39038;&#20102;&#26368;&#20808;&#36827;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#21644;&#19987;&#20026;&#21475;&#32617;&#20154;&#33080;&#38382;&#39064;&#32780;&#35774;&#35745;&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#21475;&#32617;&#20154;&#33080;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#24182;&#27604;&#36739;&#20102;&#19968;&#32452;&#33391;&#22909;&#20195;&#34920;&#24615;&#30340;&#20154;&#33080;&#26816;&#27979;&#22120;&#22312;&#21475;&#32617;&#20154;&#33080;&#26816;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#20851;&#20110;&#20854;&#24615;&#33021;&#21487;&#33021;&#30340;&#36129;&#29486;&#22240;&#32032;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contemporary face detection algorithms have to deal with many challenges such as variations in pose, illumination, and scale. A subclass of the face detection problem that has recently gained increasing attention is occluded face detection, or more specifically, the detection of masked faces. Three years on since the advent of the COVID-19 pandemic, there is still a complete lack of evidence regarding how well existing face detection algorithms perform on masked faces. This article first offers a brief review of state-of-the-art face detectors and detectors made for the masked face problem, along with a review of the existing masked face datasets. We evaluate and compare the performances of a well-representative set of face detectors at masked face detection and conclude with a discussion on the possible contributing factors to their performance.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21487;&#26356;&#22909;&#30340;&#29702;&#35299;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;Pubmed&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11070</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Enriching language models with graph-based context information to better understand textual data. (arXiv:2305.11070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11070
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#21487;&#26356;&#22909;&#30340;&#29702;&#35299;&#25991;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;BERT&#27169;&#22411;&#22312;Pubmed&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#36935;&#21040;&#30340;&#25991;&#26412;&#20855;&#26377;&#30456;&#20114;&#32852;&#31995;&#30340;&#24773;&#20917;&#30456;&#24403;&#22810;&#12290;&#20363;&#22914;&#65292;Wikipedia&#25991;&#31456;&#36890;&#36807;&#36229;&#38142;&#25509;&#24341;&#29992;&#20854;&#20182;&#25991;&#31456;&#65292;&#31185;&#23398;&#35770;&#25991;&#36890;&#36807;&#24341;&#29992;&#25110;&#65288;&#20849;&#21516;&#65289;&#20316;&#32773;&#19982;&#20854;&#20182;&#35770;&#25991;&#30456;&#20851;&#32852;&#65292;&#32780;&#25512;&#25991;&#21017;&#36890;&#36807;&#20851;&#27880;&#24444;&#27492;&#25110;&#36716;&#21457;&#20869;&#23481;&#26469;&#20851;&#32852;&#12290;&#22240;&#27492;&#65292;&#31867;&#20284;&#20110;&#22270;&#24418;&#30340;&#32467;&#26500;&#21487;&#20197;&#34920;&#31034;&#29616;&#26377;&#30340;&#32852;&#31995;&#65292;&#24182;&#34987;&#35270;&#20026;&#25429;&#25417;&#25991;&#26412;&#30340;&#8220;&#19978;&#19979;&#25991;&#8221;&#12290;&#22240;&#27492;&#65292;&#25552;&#21462;&#21644;&#25972;&#21512;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#33258;&#21160;&#29702;&#35299;&#25991;&#26412;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#23558;&#22522;&#20110;&#22270;&#24418;&#30340;&#19978;&#19979;&#25991;&#21270;&#32435;&#20837;BERT&#27169;&#22411;&#20250;&#22686;&#24378;&#20854;&#22312;&#20998;&#31867;&#20219;&#21153;&#31034;&#20363;&#19978;&#30340;&#34920;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;Pubmed&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35823;&#24046;&#20174;8.51&#65285;&#38477;&#33267;7.96&#65285;&#65292;&#21516;&#26102;&#20165;&#22686;&#21152;&#20102;1.6&#65285;&#30340;&#21442;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the "context" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.  Our source code: https://github.com/tryptofanik/gc-bert
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11067</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#20869;&#23481;&#20016;&#23500;&#12289;&#25925;&#20107;&#36830;&#36143;&#30340;&#28459;&#30011;
&lt;/p&gt;
&lt;p&gt;
Generating coherent comic with rich story using ChatGPT and Stable Diffusion. (arXiv:2305.11067v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#21644;Stable Diffusion&#29983;&#25104;&#36830;&#36143;&#28459;&#30011;&#25925;&#20107;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#35780;&#20272;AI&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#36827;&#34892;fine-tuning&#65292;&#21462;&#24471;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#20445;&#25345;&#38899;&#20048;&#23478;&#38899;&#20048;&#39118;&#26684;&#30340;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#26410;&#23436;&#25104;&#30340;&#38899;&#20048;&#20316;&#21697;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#29983;&#25104;&#26377;&#36259;&#30340;&#28459;&#30011;&#25925;&#20107;&#65292;&#24182;&#20445;&#25345;&#33402;&#26415;&#23478;&#30340;&#33402;&#26415;&#39118;&#26684;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#29983;&#25104;&#24773;&#33410;&#21644;&#23545;&#35805;&#65292;&#28982;&#21518;&#20351;&#29992;stable diffusion&#29983;&#25104;&#28459;&#30011;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;AI&#29983;&#25104;&#25925;&#20107;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;LoRA&#12289;ControlNet&#31561;&#26041;&#27861;&#23545;stable diffusion&#36827;&#34892;fine-tuning&#65292;&#36798;&#21040;&#20102;&#22312;&#35282;&#33394;&#24544;&#23454;&#24230;&#21644;&#33402;&#26415;&#39118;&#26684;&#19978;&#30340;SOTA&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.
&lt;/p&gt;</description></item><item><title>PETAL&#31639;&#27861;&#21033;&#29992;&#24050;&#30693;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#23558;&#27491;&#28436;&#27169;&#22411;&#30340;&#32447;&#24615;&#21270;&#23884;&#20837;&#21040;&#27169;&#22411;&#26412;&#36523;&#20013;&#12290;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#27491;&#28436;&#27169;&#22411;&#26469;&#35299;&#20915;&#20855;&#26377;&#24050;&#30693;&#25110;&#37096;&#20998;&#24050;&#30693;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#21453;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11056</link><description>&lt;p&gt;
&#36879;&#36807;&#24179;&#22343;&#32447;&#24615;&#21270;&#27714;&#35299;&#21453;&#38382;&#39064;&#65306;PETAL&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
PETAL: Physics Emulation Through Averaged Linearizations for Solving Inverse Problems. (arXiv:2305.11056v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11056
&lt;/p&gt;
&lt;p&gt;
PETAL&#31639;&#27861;&#21033;&#29992;&#24050;&#30693;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#23558;&#27491;&#28436;&#27169;&#22411;&#30340;&#32447;&#24615;&#21270;&#23884;&#20837;&#21040;&#27169;&#22411;&#26412;&#36523;&#20013;&#12290;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#27491;&#28436;&#27169;&#22411;&#26469;&#35299;&#20915;&#20855;&#26377;&#24050;&#30693;&#25110;&#37096;&#20998;&#24050;&#30693;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#38382;&#39064;&#25551;&#36848;&#30340;&#26159;&#36890;&#36807;&#35266;&#27979;&#25968;&#25454;&#36824;&#21407;&#20449;&#21495;&#30340;&#20219;&#21153;&#12290;&#36890;&#24120;&#26469;&#35828;&#65292;&#35266;&#27979;&#25968;&#25454;&#36890;&#36807;&#24212;&#29992;&#20110;&#26410;&#30693;&#20449;&#21495;&#30340;&#26576;&#20123;&#38750;&#32447;&#24615;&#27491;&#28436;&#27169;&#22411;&#32780;&#20135;&#29983;&#12290;&#21453;&#36716;&#38750;&#32447;&#24615;&#27491;&#28436;&#27169;&#22411;&#36890;&#24120;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#28041;&#21450;&#22312;&#19968;&#31995;&#21015;&#20272;&#35745;&#20540;&#19978;&#35745;&#31639;&#21644;&#21453;&#36716;&#32447;&#24615;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#23398;&#20064;&#21152;&#26435;&#24179;&#22343;&#27169;&#22411;&#65292;&#23558;&#27491;&#28436;&#27169;&#22411;&#30340;&#32447;&#24615;&#21270;&#23884;&#20837;&#21040;&#27169;&#22411;&#26412;&#36523;&#20013;&#65292;&#26174;&#24335;&#22320;&#34701;&#21512;&#24050;&#30693;&#30340;&#29289;&#29702;&#30693;&#35782;&#65292;&#25552;&#20986;&#20102;PETAL(Physics Emulation Through Averaged Linearizations)&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26367;&#20195;&#27491;&#28436;&#27169;&#22411;(emulator)&#26469;&#35299;&#20915;&#20855;&#26377;&#24050;&#30693;&#25110;&#37096;&#20998;&#24050;&#30693;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#21453;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems describe the task of recovering an underlying signal of interest given observables. Typically, the observables are related via some non-linear forward model applied to the underlying unknown signal. Inverting the non-linear forward model can be computationally expensive, as it often involves computing and inverting a linearization at a series of estimates. Rather than inverting the physics-based model, we instead train a surrogate forward model (emulator) and leverage modern auto-grad libraries to solve for the input within a classical optimization framework. Current methods to train emulators are done in a black box supervised machine learning fashion and fail to take advantage of any existing knowledge of the forward model. In this article, we propose a simple learned weighted average model that embeds linearizations of the forward model around various reference points into the model itself, explicitly incorporating known physics. Grounding the learned model with phy
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#23567;&#22122;&#22768;&#20998;&#26512;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;L2&#27491;&#21017;&#21270;&#33539;&#25968;&#30340;&#28508;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#25968;&#38454;RKHS&#27491;&#21017;&#21270;&#22120;&#31867;&#26469;&#35299;&#20915;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#22987;&#32456;&#20135;&#29983;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11055</link><description>&lt;p&gt;
Tikhonov&#21644;RKHS&#27491;&#21017;&#21270;&#30340;&#23567;&#22122;&#22768;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Small noise analysis for Tikhonov and RKHS regularizations. (arXiv:2305.11055v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11055
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#23567;&#22122;&#22768;&#20998;&#26512;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;L2&#27491;&#21017;&#21270;&#33539;&#25968;&#30340;&#28508;&#22312;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#25968;&#38454;RKHS&#27491;&#21017;&#21270;&#22120;&#31867;&#26469;&#35299;&#20915;&#19981;&#31283;&#23450;&#24615;&#65292;&#36825;&#20123;&#27491;&#21017;&#21270;&#22120;&#22987;&#32456;&#20135;&#29983;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#21453;&#38382;&#39064;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21508;&#31181;&#27491;&#21017;&#21270;&#33539;&#25968;&#30340;&#22522;&#26412;&#27604;&#36739;&#20998;&#26512;&#20173;&#28982;&#26410;&#35299;&#20915;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#23567;&#22122;&#22768;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;Tikhonov&#21644;RKHS&#27491;&#21017;&#21270;&#33539;&#25968;&#22312;&#39640;&#26031;&#22122;&#22768;&#30340;&#19981;&#36866;&#23450;&#32447;&#24615;&#21453;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;&#35813;&#26694;&#26550;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#20272;&#35745;&#22120;&#22312;&#23567;&#22122;&#22768;&#26497;&#38480;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#25581;&#31034;&#20102;&#20256;&#32479;L2&#27491;&#21017;&#21270;&#30340;&#28508;&#22312;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#30340;&#33258;&#36866;&#24212;&#20998;&#25968;&#38454;RKHS&#27491;&#21017;&#21270;&#22120;&#31867;&#26469;&#35299;&#20915;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#36890;&#36807;&#35843;&#25972;&#20998;&#25968;&#20809;&#28369;&#24230;&#21442;&#25968;&#65292;&#35813;&#31867;&#35206;&#30422;&#20102;L2 Tikhonov&#21644;RKHS&#27491;&#21017;&#21270;&#22120;&#12290;&#19968;&#20010;&#20196;&#20154;&#24778;&#22855;&#30340;&#35266;&#28857;&#26159;&#65292;&#36890;&#36807;&#36825;&#20123;&#20998;&#25968;&#38454;RKHS&#36827;&#34892;&#36807;&#24230;&#24179;&#28369;&#22987;&#32456;&#20135;&#29983;&#26368;&#20339;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#20294;&#26368;&#20339;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#34928;&#20943;&#24471;&#22826;&#24555;&#32780;&#26080;&#27861;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization plays a pivotal role in ill-posed machine learning and inverse problems. However, the fundamental comparative analysis of various regularization norms remains open. We establish a small noise analysis framework to assess the effects of norms in Tikhonov and RKHS regularizations, in the context of ill-posed linear inverse problems with Gaussian noise. This framework studies the convergence rates of regularized estimators in the small noise limit and reveals the potential instability of the conventional L2-regularizer. We solve such instability by proposing an innovative class of adaptive fractional RKHS regularizers, which covers the L2 Tikhonov and RKHS regularizations by adjusting the fractional smoothness parameter. A surprising insight is that over-smoothing via these fractional RKHSs consistently yields optimal convergence rates, but the optimal hyper-parameter may decay too fast to be selected in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; NODE-ImgNet&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992; PDE &#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#28982;&#22320;&#36991;&#20813;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20266;&#20687;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.11049</link><description>&lt;p&gt;
NODE-ImgNet: &#19968;&#31181;&#20351;&#29992; PDE &#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NODE-ImgNet: a PDE-informed effective and robust model for image denoising. (arXiv:2305.11049v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500; NODE-ImgNet&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22359;&#65292;&#23427;&#26159;&#19968;&#31181;&#20351;&#29992; PDE &#30340;&#26377;&#25928;&#21644;&#31283;&#20581;&#30340;&#22270;&#20687;&#21435;&#22122;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#28982;&#22320;&#36991;&#20813;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20266;&#20687;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#20256;&#32479;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#26694;&#26550; NODE-ImgNet&#65292;&#23558;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;NODEs&#65289;&#19982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22359;&#30456;&#32467;&#21512;&#12290; NODE-ImgNet &#26159;&#26412;&#36136;&#19978;&#30340; PDE &#27169;&#22411;&#65292;&#21160;&#24577;&#31995;&#32479;&#26159;&#38544;&#24335;&#23398;&#20064;&#30340;&#65292;&#32780;&#19981;&#26159;&#26174;&#24335;&#25351;&#23450; PDE&#65292;&#33258;&#28982;&#22320;&#36991;&#20813;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#24341;&#20837;&#20266;&#20687;&#30340;&#20856;&#22411;&#38382;&#39064;&#12290;&#36890;&#36807;&#35843;&#29992;&#36825;&#31181; NODE &#32467;&#26500;&#65292;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#27531;&#24046;&#32593;&#32476;&#65288;ResNet&#65289;&#30340;&#36830;&#32493;&#21464;&#20307;&#65292;&#32487;&#25215;&#20854;&#22312;&#22270;&#20687;&#21435;&#22122;&#20013;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#65292;&#21253;&#25324;&#21463;&#39640;&#26031;&#22122;&#22768;&#24178;&#25200;&#30340;&#28784;&#24230;&#21644;&#24425;&#33394;&#22270;&#20687;&#21435;&#22122;&#65292;&#20197;&#21450;&#30495;&#23454;&#22122;&#22768;&#22270;&#20687;&#23398;&#20064;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#19968;&#33268;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20174;&#23567;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the traditional partial differential equation (PDE) approach for image denoising, we propose a novel neural network architecture, referred as NODE-ImgNet, that combines neural ordinary differential equations (NODEs) with convolutional neural network (CNN) blocks. NODE-ImgNet is intrinsically a PDE model, where the dynamic system is learned implicitly without the explicit specification of the PDE. This naturally circumvents the typical issues associated with introducing artifacts during the learning process. By invoking such a NODE structure, which can also be viewed as a continuous variant of a residual network (ResNet) and inherits its advantage in image denoising, our model achieves enhanced accuracy and parameter efficiency. In particular, our model exhibits consistent effectiveness in different scenarios, including denoising gray and color images perturbed by Gaussian noise, as well as real-noisy images, and demonstrates superiority in learning from small image datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;DC&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#23376;&#27169;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#24615;&#36136;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20840;&#38754;&#65292;&#21516;&#26102;&#22312;&#35821;&#38899;&#29305;&#24449;&#36873;&#25321;&#21644;&#25991;&#26723;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11046</link><description>&lt;p&gt;
DC&#35268;&#21010;&#31639;&#27861;&#22312;&#23376;&#27169;&#26368;&#23567;&#21270;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Difference of Submodular Minimization via DC Programming. (arXiv:2305.11046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;DC&#35268;&#21010;&#31639;&#27861;&#26469;&#35299;&#20915;&#23376;&#27169;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#25910;&#25947;&#24615;&#36136;&#27604;&#29616;&#26377;&#31639;&#27861;&#26356;&#20840;&#38754;&#65292;&#21516;&#26102;&#22312;&#35821;&#38899;&#29305;&#24449;&#36873;&#25321;&#21644;&#25991;&#26723;&#25688;&#35201;&#31561;&#24212;&#29992;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#26368;&#23567;&#21270;&#20004;&#20010;&#23376;&#27169;&#65288;DS&#65289;&#20989;&#25968;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#33258;&#28982;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20154;&#30693;&#36947;DS&#38382;&#39064;&#21487;&#20197;&#31561;&#20215;&#22320;&#36716;&#21270;&#20026;&#20004;&#20010;&#20984;&#65288;DC&#65289;&#20989;&#25968;&#30340;&#24046;&#24322;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#31639;&#27861;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#12290;&#23545;&#20110;DC&#38382;&#39064;&#65292;&#19968;&#20010;&#32463;&#20856;&#30340;&#31639;&#27861;&#21483;&#20570;DC&#31639;&#27861;&#65288;DCA&#65289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DCA&#21450;&#20854;&#23436;&#25972;&#24418;&#24335;&#65288;CDCA&#65289;&#30340;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23545;&#24212;&#20110;DS&#26368;&#23567;&#21270;&#30340;DC&#31243;&#24207;&#20013;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;DCA&#30340;&#29616;&#26377;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;DS&#38382;&#39064;&#30340;&#25910;&#25947;&#24615;&#36136;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;DCA&#32467;&#26524;&#19982;&#29616;&#26377;&#30340;DS&#31639;&#27861;&#28385;&#36275;&#30456;&#21516;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#26356;&#23436;&#25972;&#30340;&#25910;&#25947;&#24615;&#36136;&#25551;&#36848;&#12290;&#23545;&#20110;CDCA&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26356;&#24378;&#30340;&#23616;&#37096;&#26368;&#23567;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#25968;&#23383;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20004;&#20010;&#24212;&#29992;&#8212;&#8212;&#35821;&#38899;&#35821;&#26009;&#24211;&#36873;&#25321;&#29305;&#24449;&#20248;&#21270;&#21644;&#25991;&#26723;&#25688;&#35201;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimizing the difference of two submodular (DS) functions is a problem that naturally occurs in various machine learning problems. Although it is well known that a DS problem can be equivalently formulated as the minimization of the difference of two convex (DC) functions, existing algorithms do not fully exploit this connection. A classical algorithm for DC problems is called the DC algorithm (DCA). We introduce variants of DCA and its complete form (CDCA) that we apply to the DC program corresponding to DS minimization. We extend existing convergence properties of DCA, and connect them to convergence properties on the DS problem. Our results on DCA match the theoretical guarantees satisfied by existing DS algorithms, while providing a more complete characterization of convergence properties. In the case of CDCA, we obtain a stronger local minimality guarantee. Our numerical results show that our proposed algorithms outperform existing baselines on two applications: speech corpus sel
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#30456;&#20851;&#24341;&#29702;&#21644;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#19968;&#20123;&#20854;&#20182;&#25216;&#26415;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#19978;&#38480;&#65292;&#24182;&#19988;&#33021;&#22815;&#24674;&#22797;&#35768;&#22810;&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#65292;&#22914;&#22522;&#20110;&#20114;&#20449;&#24687;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#12289;&#38543;&#26426;chaining&#21644;PAC-Bayes&#19981;&#31561;&#24335;&#30340;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.11042</link><description>&lt;p&gt;
&#19968;&#31181;&#20449;&#24687;&#35770;&#36890;&#29992;&#27867;&#21270;&#30028;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A unified framework for information-theoretic generalization bounds. (arXiv:2305.11042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#21435;&#30456;&#20851;&#24341;&#29702;&#21644;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#19968;&#20123;&#20854;&#20182;&#25216;&#26415;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#24471;&#21040;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#19978;&#38480;&#65292;&#24182;&#19988;&#33021;&#22815;&#24674;&#22797;&#35768;&#22810;&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#65292;&#22914;&#22522;&#20110;&#20114;&#20449;&#24687;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#12289;&#38543;&#26426;chaining&#21644;PAC-Bayes&#19981;&#31561;&#24335;&#30340;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#23548;&#20986;&#23398;&#20064;&#31639;&#27861;&#30340;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#12290;&#20027;&#35201;&#30340;&#25216;&#26415;&#24037;&#20855;&#26159;&#22522;&#20110;&#25913;&#21464;&#27979;&#24230;&#21644;&#26494;&#24347;Young&#19981;&#31561;&#24335;&#22312;$L_{\psi_p}$Orlicz&#31354;&#38388;&#20013;&#30340;&#27010;&#29575;&#21435;&#30456;&#20851;&#24615;&#24341;&#29702;&#12290;&#37319;&#29992;&#21435;&#30456;&#20851;&#24615;&#24341;&#29702;&#19982;&#20854;&#20182;&#25216;&#26415;&#65292;&#22914;&#23545;&#31216;&#21270;&#12289;&#32806;&#21512;&#21644;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#20013;&#30340;chaining&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26032;&#30340;&#27867;&#21270;&#35823;&#24046;&#19978;&#38480;&#65292;&#21253;&#25324;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#65292;&#21516;&#26102;&#65292;&#25105;&#20204;&#20063;&#24674;&#22797;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;&#27867;&#21270;&#30028;&#65292;&#21253;&#25324;&#22522;&#20110;&#20114;&#20449;&#24687;&#12289;&#26465;&#20214;&#20114;&#20449;&#24687;&#12289;&#38543;&#26426;chaining&#21644;PAC-Bayes&#19981;&#31561;&#24335;&#30340;&#30028;&#12290;&#27492;&#22806;&#65292;Fernique-Talagrand&#19978;&#30028;&#20063;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#21576;&#29616;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a general methodology for deriving information-theoretic generalization bounds for learning algorithms. The main technical tool is a probabilistic decorrelation lemma based on a change of measure and a relaxation of Young's inequality in $L_{\psi_p}$ Orlicz spaces. Using the decorrelation lemma in combination with other techniques, such as symmetrization, couplings, and chaining in the space of probability measures, we obtain new upper bounds on the generalization error, both in expectation and in high probability, and recover as special cases many of the existing generalization bounds, including the ones based on mutual information, conditional mutual information, stochastic chaining, and PAC-Bayes inequalities. In addition, the Fernique-Talagrand upper bound on the expected supremum of a subgaussian process emerges as a special case.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24471;&#20986;&#20102;&#21435;&#22122;&#22343;&#26041;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#33258;&#32534;&#30721;&#22120;&#30456;&#36739;&#20110;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.11041</link><description>&lt;p&gt;
&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#39640;&#32500;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Asymptotics of Denoising Autoencoders. (arXiv:2305.11041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#24471;&#20986;&#20102;&#21435;&#22122;&#22343;&#26041;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#33258;&#32534;&#30721;&#22120;&#30456;&#36739;&#20110;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24212;&#29992;&#24102;&#26377;&#32465;&#23450;&#26435;&#37325;&#21644;&#36339;&#36291;&#36830;&#25509;&#30340;&#20108;&#23618;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#26469;&#21435;&#22122;&#39640;&#26031;&#28151;&#21512;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#26497;&#38480;&#19979;&#32771;&#34385;&#65292;&#20854;&#20013;&#35757;&#32451;&#26679;&#26412;&#25968;&#21644;&#36755;&#20837;&#32500;&#25968;&#20849;&#21516;&#36235;&#21521;&#20110;&#26080;&#31351;&#22823;&#65292;&#32780;&#38544;&#34255;&#21333;&#20803;&#25968;&#20445;&#25345;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21435;&#22122;&#22343;&#26041;&#27979;&#35797;&#35823;&#24046;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#22522;&#20110;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#34920;&#24449;&#20102;&#25152;&#32771;&#34385;&#30340;&#26550;&#26500;&#22312;&#33258;&#32534;&#30721;&#22120;&#65288;&#27809;&#26377;&#20851;&#32852;&#21040;&#20027;&#25104;&#20998;&#20998;&#26512;&#65289;&#30340;&#36339;&#36291;&#36830;&#25509;&#30456;&#20851;&#24615;&#20043;&#19978;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#20934;&#30830;&#22320;&#25429;&#25417;&#20102;&#19968;&#31995;&#21015;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the problem of denoising data from a Gaussian mixture using a two-layer non-linear autoencoder with tied weights and a skip connection. We consider the high-dimensional limit where the number of training samples and the input dimension jointly tend to infinity while the number of hidden units remains bounded. We provide closed-form expressions for the denoising mean-squared test error. Building on this result, we quantitatively characterize the advantage of the considered architecture over the autoencoder without the skip connection that relates closely to principal component analysis. We further show that our results accurately capture the learning curves on a range of real data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26032;&#22411;&#30340;&#37327;&#23376;&#24863;&#30693;&#22120;QVP-G&#12290;&#23545;&#27604;&#32463;&#20856;&#24863;&#30693;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;QVP-G&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;QVP&#26356;&#21152;&#20934;&#30830;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.11040</link><description>&lt;p&gt;
&#20351;&#29992;Grover&#31639;&#27861;&#27169;&#25311;&#21464;&#20998;&#37327;&#23376;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Simulation of a Variational Quantum Perceptron using Grover's Algorithm. (arXiv:2305.11040v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32467;&#21512;&#20102;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#26032;&#22411;&#30340;&#37327;&#23376;&#24863;&#30693;&#22120;QVP-G&#12290;&#23545;&#27604;&#32463;&#20856;&#24863;&#30693;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;QVP-G&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#27604;QVP&#26356;&#21152;&#20934;&#30830;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#24863;&#30693;&#22120;&#12289;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#34987;&#35748;&#20026;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#32452;&#20214;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21644;Grover&#31639;&#27861;&#30340;&#26032;&#22411;&#37327;&#23376;&#24863;&#30693;&#22120;&#12290;&#36890;&#36807;&#35745;&#31639;&#23427;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#20998;&#26512;&#23427;&#20204;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#23558;&#36825;&#20004;&#20010;&#37327;&#23376;&#27169;&#22411;&#19982;&#32463;&#20856;&#24863;&#30693;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;QVP&#21644;QVP-G&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20004;&#20010;&#37327;&#23376;&#27169;&#22411;&#27604;CP&#26356;&#26377;&#25928;&#29575;&#65292;&#25105;&#20204;&#30340;&#26032;&#22411;&#24314;&#35758;&#27169;&#22411;QVP-G&#32988;&#36807;QVP&#65292;&#35777;&#26126;&#20102;Grover&#31639;&#27861;&#21487;&#20197;&#24212;&#29992;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#38500;&#20102;&#38750;&#32467;&#26500;&#21270;&#25628;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quantum perceptron, the variational circuit, and the Grover algorithm have been proposed as promising components for quantum machine learning. This paper presents a new quantum perceptron that combines the quantum variational circuit and the Grover algorithm. However, this does not guarantee that this quantum variational perceptron with Grover's algorithm (QVPG) will have any advantage over its quantum variational (QVP) and classical counterparts. Here, we examine the performance of QVP and QVP-G by computing their loss function and analyzing their accuracy on the classification task, then comparing these two quantum models to the classical perceptron (CP). The results show that our two quantum models are more efficient than CP, and our novel suggested model QVP-G outperforms the QVP, demonstrating that the Grover can be applied to the classification task and even makes the model more accurate, besides the unstructured search problems.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#25968;&#25454;&#21253;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#25968;&#25454;&#21253;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11039</link><description>&lt;p&gt;
Deep PackGen&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#25968;&#25454;&#21253;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation. (arXiv:2305.11039v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11039
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#25968;&#25454;&#21253;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#24615;&#32593;&#32476;&#25968;&#25454;&#21253;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21457;&#23637;&#20197;&#21450;&#26356;&#24555;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#21487;&#29992;&#24615;&#65292;&#36890;&#36807;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#65292;&#22686;&#24378;&#20102;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#20013;&#24515;&#65288;&#38450;&#24481;&#32773;&#65289;&#30340;&#23433;&#20840;&#23039;&#24577;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;AI / ML&#27169;&#22411;&#30340;&#25903;&#25345;&#20063;&#22686;&#21152;&#20102;&#36867;&#36991;&#23433;&#20840;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#38450;&#24481;&#32773;&#38656;&#35201;&#31215;&#26497;&#20934;&#22791;&#38024;&#23545;&#21033;&#29992;NIDS&#26816;&#27979;&#26426;&#21046;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#22522;&#20110;&#27969;&#21644;&#22522;&#20110;&#25968;&#25454;&#21253;&#30340;&#29305;&#24449;&#36827;&#34892;&#25200;&#21160;&#21487;&#20197;&#27450;&#39575;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23545;&#27969;&#22522;&#30784;&#29305;&#24449;&#36827;&#34892;&#25200;&#21160;&#38590;&#20197;&#36870;&#21521;&#24037;&#31243;&#65292;&#32780;&#20351;&#29992;&#23545;&#25968;&#25454;&#21253;&#29305;&#24449;&#36827;&#34892;&#25200;&#21160;&#30340;&#26679;&#26412;&#19981;&#21487;&#37325;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26694;&#26550;&#8220;Deep PackGen&#8221;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#23545;&#25239;&#24615;&#25968;&#25454;&#21253;&#65292;&#24182;&#26088;&#22312;&#36229;&#36234;&#29616;&#26377;&#23545;&#25239;&#29983;&#25104;&#24615;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25968;&#25454;&#21253;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in artificial intelligence (AI) and machine learning (ML) algorithms, coupled with the availability of faster computing infrastructure, have enhanced the security posture of cybersecurity operations centers (defenders) through the development of ML-aided network intrusion detection systems (NIDS). Concurrently, the abilities of adversaries to evade security have also increased with the support of AI/ML models. Therefore, defenders need to proactively prepare for evasion attacks that exploit the detection mechanisms of NIDS. Recent studies have found that the perturbation of flow-based and packet-based features can deceive ML models, but these approaches have limitations. Perturbations made to the flow-based features are difficult to reverse-engineer, while samples generated with perturbations to the packet-based features are not playable.  Our methodological framework, Deep PackGen, employs deep reinforcement learning to generate adversarial packets and aims to over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.11033</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65306;&#26368;&#36817;&#25991;&#29486;&#20013;&#25216;&#26415;&#21644;&#24120;&#35265;&#36235;&#21183;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature. (arXiv:2305.11033v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#25991;&#29486;&#20013;&#30340;&#21487;&#35270;&#21270;&#38382;&#31572;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#23545;&#35813;&#39046;&#22495;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#21270;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#39044;&#27979;&#20013;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#38656;&#35201;&#31639;&#27861;&#22238;&#31572;&#26377;&#20851;&#29305;&#23450;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#20998;&#26512;&#20102;25&#39033;&#26368;&#26032;&#30740;&#31350;&#21644;6&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#19979;&#36733;&#38142;&#25509;&#12290;&#20316;&#32773;&#28145;&#20837;&#35843;&#30740;&#20102;&#35813;&#39046;&#22495;&#30340;&#22810;&#39033;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20102;&#20998;&#26512;&#27604;&#36739;&#65292;&#21253;&#25324;&#32467;&#26524;&#12289;&#26368;&#26032;&#25216;&#26415;&#12289;&#24120;&#35265;&#38169;&#35823;&#20197;&#21450;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#25913;&#36827;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) is an emerging area of interest for researches, being a recent problem in natural language processing and image prediction. In this area, an algorithm needs to answer questions about certain images. As of the writing of this survey, 25 recent studies were analyzed. Besides, 6 datasets were analyzed and provided their link to download. In this work, several recent pieces of research in this area were investigated and a deeper analysis and comparison among them were provided, including results, the state-of-the-art, common errors, and possible points of improvement for future researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20048;&#35266;&#31574;&#30053;&#35780;&#20272;&#23376;&#31243;&#24207;&#20197;&#40723;&#21169;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#32447;&#24615;MDP&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20855;&#26377;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.11032</link><description>&lt;p&gt;
&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL. (arXiv:2305.11032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550;&#65292;&#37319;&#29992;&#20048;&#35266;&#31574;&#30053;&#35780;&#20272;&#23376;&#31243;&#24207;&#20197;&#40723;&#21169;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#32447;&#24615;MDP&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#20855;&#26377;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#23545;&#20110;&#36817;&#26399;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25104;&#21151;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#31574;&#30053;&#20248;&#21270;&#30340;&#29616;&#26377;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#30456;&#24403;&#26377;&#38480; - &#23427;&#20204;&#35201;&#20040;&#23616;&#38480;&#20110;&#34920;&#26684;MDP&#65292;&#35201;&#20040;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#23384;&#22312;&#39640;&#24230;&#20122;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#20248;&#21270;&#26694;&#26550; - &#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#12290;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#32463;&#20856;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;[Kakade&#65292;2001]&#19982;&#20048;&#35266;&#31574;&#30053;&#35780;&#20272;&#23376;&#31243;&#24207;&#31616;&#21333;&#32452;&#21512;&#20197;&#40723;&#21169;&#25506;&#32034;&#12290;&#23545;&#20110;$d$-&#32500;&#32447;&#24615;MDP&#65292;&#20048;&#35266;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20855;&#26377;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#22312;$\tilde{O}(d^2/\varepsilon^3)$ &#27425;&#37319;&#26679;&#20869;&#23398;&#20064; $\varepsilon$ -&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20248;&#32500;&#24230;&#20381;&#36182;&#20851;&#31995;$\tilde {\Theta}(d^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#35745;&#31639;&#39640;&#25928;&#31639;&#27861;&#12290;&#23427;&#20063;&#36229;&#36234;&#20102;&#30446;&#21069;&#39046;&#20808;&#30340;&#19968;&#20123;&#29366;&#24577;of-the-art&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited -- they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework -- Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves over state-of-the-a
&lt;/p&gt;</description></item><item><title>&#22823;&#35268;&#27169;&#24182;&#34892;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#36890;&#36807;&#25277;&#21462;$K^n$&#20010;&#21487;&#33021;&#30340;&#26679;&#26412;&#32452;&#21512;&#65292;&#36991;&#20813;&#20102;&#21407;&#26041;&#27861;&#20013;&#22823;&#37327;&#28508;&#22312;&#21464;&#37327;&#25968;&#30446;&#23548;&#33268;&#26377;&#25928;&#24615;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11022</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24182;&#34892;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;
&lt;/p&gt;
&lt;p&gt;
Massively Parallel Reweighted Wake-Sleep. (arXiv:2305.11022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11022
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#24182;&#34892;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#36890;&#36807;&#25277;&#21462;$K^n$&#20010;&#21487;&#33021;&#30340;&#26679;&#26412;&#32452;&#21512;&#65292;&#36991;&#20813;&#20102;&#21407;&#26041;&#27861;&#20013;&#22823;&#37327;&#28508;&#22312;&#21464;&#37327;&#25968;&#30446;&#23548;&#33268;&#26377;&#25928;&#24615;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#65288;RWS&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#24120;&#36890;&#29992;&#30340;&#27169;&#22411;&#25191;&#34892;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20174;&#28508;&#22312;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#20013;&#25277;&#21462;$K$&#20010;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#37325;&#35201;&#24615;&#21152;&#26435;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#20272;&#35745;&#12290;RWS&#28982;&#21518;&#26356;&#26032;&#20854;&#36817;&#20284;&#21518;&#39564;&#27010;&#29575;&#65292;&#21521;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#20272;&#35745;&#31227;&#21160;&#12290;&#28982;&#32780;&#65292;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#26377;&#25928;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#65292;&#25152;&#38656;&#26679;&#26412;&#25968;&#19982;&#28508;&#22312;&#21464;&#37327;&#30340;&#25968;&#37327;&#21576;&#25351;&#25968;&#20851;&#31995;&#12290;&#22312;&#25152;&#26377;&#20294;&#26368;&#23567;&#30340;&#27169;&#22411;&#20013;&#23454;&#29616;&#22914;&#27492;&#22823;&#25968;&#37327;&#30340;&#37325;&#35201;&#24615;&#26679;&#26412;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290; &#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22823;&#35268;&#27169;&#24182;&#34892;&#30340;RWS&#65292;&#36890;&#36807;&#25277;&#21462;&#25152;&#26377;$n$&#20010;&#28508;&#22312;&#21464;&#37327;&#30340;$K$&#20010;&#26679;&#26412;&#65292;&#24182;&#21333;&#29420;&#32771;&#34385;&#25152;&#26377;$K^n$&#20010;&#21487;&#33021;&#30340;&#26679;&#26412;&#32452;&#21512;&#65292;&#36991;&#20813;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#34429;&#28982;&#32771;&#34385;$K^n$&#20010;&#32452;&#21512;&#20284;&#20046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20294;&#25152;&#38656;&#30340;&#35745;&#31639;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#35745;&#31639;&#32467;&#26500;&#31616;&#21270;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reweighted wake-sleep (RWS) is a machine learning method for performing Bayesian inference in a very general class of models. RWS draws $K$ samples from an underlying approximate posterior, then uses importance weighting to provide a better estimate of the true posterior. RWS then updates its approximate posterior towards the importance-weighted estimate of the true posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that the number of samples required for effective importance weighting is exponential in the number of latent variables. Attaining such a large number of importance samples is intractable in all but the smallest models. Here, we develop massively parallel RWS, which circumvents this issue by drawing $K$ samples of all $n$ latent variables, and individually reasoning about all $K^n$ possible combinations of samples. While reasoning about $K^n$ combinations might seem intractable, the required computations can be performed in polynomial time by exploiti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24230;&#37327;&#24352;&#37327;&#22330;$g_ab$&#23558;&#27431;&#20960;&#37324;&#24471;&#31574;&#30053;&#21442;&#25968;&#31354;&#38388;&#25512;&#24191;&#21040;&#19968;&#33324;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24230;&#37327;&#24352;&#37327;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11017</link><description>&lt;p&gt;
&#28145;&#24230;&#24230;&#37327;&#24352;&#37327;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep Metric Tensor Regularized Policy Gradient. (arXiv:2305.11017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#24230;&#37327;&#24352;&#37327;&#22330;$g_ab$&#23558;&#27431;&#20960;&#37324;&#24471;&#31574;&#30053;&#21442;&#25968;&#31354;&#38388;&#25512;&#24191;&#21040;&#19968;&#33324;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24230;&#37327;&#24352;&#37327;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#20013;&#37325;&#35201;&#30340;&#19968;&#31867;&#12290;&#36807;&#21435;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#30528;&#30524;&#20110;&#20351;&#29992;&#19968;&#38454;&#31574;&#30053;&#26799;&#24230;&#20449;&#24687;&#26469;&#35757;&#32451;&#31574;&#30053;&#32593;&#32476;&#12290;&#19982;&#36825;&#20123;&#24037;&#20316;&#19981;&#21516;&#65292;&#26412;&#25991;&#30340;&#30740;&#31350;&#22522;&#20110;&#36825;&#26679;&#30340;&#20449;&#24565;&#65306;&#21512;&#29702;&#21033;&#29992;&#21644;&#25511;&#21046;&#19982;&#31574;&#30053;&#26799;&#24230;&#30456;&#20851;&#30340;&#28023;&#26862;&#20449;&#24687;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#19968;&#20010;&#20851;&#38190;&#28023;&#26862;&#20449;&#24687;&#26159;&#28023;&#26862;&#36319;&#36394;&#20540;&#65292;&#23427;&#32473;&#20986;&#20102;&#27431;&#20960;&#37324;&#24471;&#31574;&#30053;&#21442;&#25968;&#31354;&#38388;&#20013;&#31574;&#30053;&#26799;&#24230;&#21521;&#37327;&#22330;&#30340;&#21457;&#25955;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#24341;&#20837;&#24230;&#37327;&#24352;&#37327;&#22330;$g_ab$&#26469;&#23558;&#27431;&#20960;&#37324;&#24471;&#31574;&#30053;&#21442;&#25968;&#31354;&#38388;&#25512;&#24191;&#21040;&#19968;&#33324;&#30340;&#40654;&#26364;&#27969;&#24418;&#19978;&#12290;&#36825;&#36890;&#36807;&#26032;&#24320;&#21457;&#30340;&#25968;&#23398;&#24037;&#20855;&#12289;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#21644;&#24230;&#37327;&#24352;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26469;&#23454;&#29616;&#12290;&#25317;&#26377;&#36825;&#20123;&#25216;&#26415;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24230;&#37327;&#24352;&#37327;&#27491;&#21017;&#21270;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy gradient algorithms are an important family of deep reinforcement learning techniques. Many past research endeavors focused on using the first-order policy gradient information to train policy networks. Different from these works, we conduct research in this paper driven by the believe that properly utilizing and controlling Hessian information associated with the policy gradient can noticeably improve the performance of policy gradient algorithms. One key Hessian information that attracted our attention is the Hessian trace, which gives the divergence of the policy gradient vector field in the Euclidean policy parametric space. We set the goal to generalize this Euclidean policy parametric space into a general Riemmanian manifold by introducing a metric tensor field $g_ab$ in the parametric space. This is achieved through newly developed mathematical tools, deep learning algorithms, and metric tensor deep neural networks (DNNs). Armed with these technical developments, we propo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25293;&#21334;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#26368;&#20248;&#25293;&#21334;&#35774;&#35745;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#25509;&#19981;&#21516;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.11005</link><description>&lt;p&gt;
&#25293;&#21334;&#35774;&#35745;&#20013;&#30340;&#27169;&#24335;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mode Connectivity in Auction Design. (arXiv:2305.11005v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11005
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25293;&#21334;&#35774;&#35745;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#26368;&#20248;&#25293;&#21334;&#35774;&#35745;&#12290;&#22312;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#25509;&#19981;&#21516;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#25293;&#21334;&#35774;&#35745;&#26159;&#31639;&#27861;&#21338;&#24328;&#35770;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#38750;&#24120;&#31616;&#21333;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#38382;&#39064;&#20063;&#24456;&#38590;&#12290;&#26368;&#36817;&#19981;&#21516;&#30340;&#32463;&#27982;&#23398;&#21487;&#24494;&#20998;&#29702;&#35770;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#24050;&#30693;&#30340;&#26368;&#20248;&#25293;&#21334;&#26426;&#21046;&#65292;&#21457;&#29616;&#26377;&#36259;&#30340;&#26032;&#26426;&#21046;&#12290;&#20026;&#20102;&#29702;&#35770;&#19978;&#35777;&#26126;&#23427;&#20204;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;RochetNet&#65292;&#24182;&#30740;&#31350;&#25152;&#35859;&#30340;&#20223;&#23556;&#26497;&#22823;&#21270;&#25293;&#21334;&#30340;&#24191;&#20041;&#29256;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20204;&#28385;&#36275;&#27169;&#24335;&#36830;&#36890;&#24615;&#65292;&#21363;&#23616;&#37096;&#26368;&#20248;&#35299;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#25509;&#65292;&#36335;&#24452;&#19978;&#30340;&#27599;&#20010;&#35299;&#37117;&#20960;&#20046;&#21644;&#20004;&#20010;&#23616;&#37096;&#26368;&#20248;&#35299;&#20043;&#19968;&#19968;&#26679;&#22909;&#12290;&#27169;&#24335;&#36830;&#36890;&#24615;&#26368;&#36817;&#34987;&#35777;&#26126;&#26159;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#36259;&#30340;&#32463;&#39564;&#21644;&#29702;&#35770;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#23545;&#21487;&#24494;&#20998;&#32463;&#27982;&#23398;&#39046;&#22495;&#20013;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#35774;&#35745;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal auction design is a fundamental problem in algorithmic game theory. This problem is notoriously difficult already in very simple settings. Recent work in differentiable economics showed that neural networks can efficiently learn known optimal auction mechanisms and discover interesting new ones. In an attempt to theoretically justify their empirical success, we focus on one of the first such networks, RochetNet, and a generalized version for affine maximizer auctions. We prove that they satisfy mode connectivity, i.e., locally optimal solutions are connected by a simple, piecewise linear path such that every solution on the path is almost as good as one of the two local optima. Mode connectivity has been recently investigated as an intriguing empirical and theoretically justifiable property of neural networks used for prediction problems. Our results give the first such analysis in the context of differentiable economics, where neural networks are used directly for solving non-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#21046;&#25513;&#30721;&#26426;&#21046;&#26469;&#23454;&#29616;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#29305;&#23450;&#30693;&#35782;&#20849;&#20139;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#32456;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.10997</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#21046;&#25513;&#30721;&#20998;&#20139;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks. (arXiv:2305.10997v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10997
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#21046;&#25513;&#30721;&#26426;&#21046;&#26469;&#23454;&#29616;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#29305;&#23450;&#30693;&#35782;&#20849;&#20139;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#26368;&#32456;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#20195;&#29702;&#26088;&#22312;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#36880;&#28176;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;&#12290;&#36825;&#38656;&#35201;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#21033;&#29992;&#20197;&#21069;&#30340;&#30693;&#35782;&#24182;&#36991;&#20813;&#36951;&#24536;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#21442;&#25968;&#38548;&#31163;&#26041;&#27861;&#8212;&#8212;&#35843;&#21046;&#25513;&#30721;&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#34429;&#28982;&#32456;&#36523;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#22312;&#21333;&#20010;&#20195;&#29702;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#65292;&#20294;&#20173;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22810;&#20010;&#20195;&#29702;&#22914;&#20309;&#24444;&#27492;&#20998;&#20139;&#32456;&#36523;&#23398;&#20064;&#30693;&#35782;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35843;&#21046;&#25513;&#30721;&#25152;&#20351;&#29992;&#30340;&#21442;&#25968;&#38548;&#31163;&#26426;&#21046;&#29305;&#21035;&#36866;&#21512;&#22312;&#20998;&#24067;&#24335;&#21644;&#21435;&#20013;&#24515;&#21270;&#30340;&#32456;&#36523;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20195;&#29702;&#20043;&#38388;&#20132;&#25442;&#30693;&#35782;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#29305;&#23450;&#20219;&#21153;&#30693;&#35782;&#38548;&#31163;&#21040;&#29305;&#23450;&#30340;&#25513;&#30721;&#20013;&#65292;&#35753;&#20195;&#29702;&#21487;&#20197;&#25353;&#38656;&#36716;&#31227;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#23454;&#29616;&#24378;&#22823;&#32780;&#26377;&#25928;&#30340;&#20998;&#24067;&#24335;&#32456;&#36523;&#23398;&#20064;&#12290;&#25105;&#20204;&#20551;&#35774;&#21160;&#24577;&#24180;&#40836;&#20840;&#20998;&#24067;&#24335;&#24322;&#27493;&#24773;&#20917;&#19979;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning agents aim to learn multiple tasks sequentially over a lifetime. This involves the ability to exploit previous knowledge when learning new tasks and to avoid forgetting. Modulating masks, a specific type of parameter isolation approach, have recently shown promise in both supervised and reinforcement learning. While lifelong learning algorithms have been investigated mainly within a single-agent approach, a question remains on how multiple agents can share lifelong learning knowledge with each other. We show that the parameter isolation mechanism used by modulating masks is particularly suitable for exchanging knowledge among agents in a distributed and decentralized system of lifelong learners. The key idea is that the isolation of specific task knowledge to specific masks allows agents to transfer only specific knowledge on-demand, resulting in robust and effective distributed lifelong learning. We assume fully distributed and asynchronous scenarios with dynamic age
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#20197;&#21450;&#24433;&#21709;&#20998;&#37197;&#30340;&#22240;&#32032;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#36866;&#21512;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.10994</link><description>&lt;p&gt;
&#29702;&#35299;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#38544;&#31169;&#39044;&#31639;
&lt;/p&gt;
&lt;p&gt;
Understanding how Differentially Private Generative Models Spend their Privacy Budget. (arXiv:2305.10994v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#20197;&#21450;&#24433;&#21709;&#20998;&#37197;&#30340;&#22240;&#32032;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#22411;&#36866;&#21512;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20135;&#29983;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#20943;&#23569;&#38544;&#31169;&#39118;&#38505;&#12290;&#20294;&#26159;&#22312;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#26435;&#34913;&#23427;&#20204;&#20043;&#38388;&#30340;&#38544;&#31169;-&#25928;&#29992;&#20851;&#31995;&#12290;&#26412;&#25991;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#65292;&#20998;&#26512;&#20102;DP&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#24182;&#25506;&#35752;&#20102;&#24433;&#21709;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#25105;&#20204;&#23545;&#22270;&#24418;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#36866;&#29992;&#20110;&#19981;&#21516;&#35774;&#32622;&#21644;&#20219;&#21153;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models trained with Differential Privacy (DP) are increasingly used to produce synthetic data while reducing privacy risks. Navigating their specific privacy-utility tradeoffs makes it challenging to determine which models would work best for specific settings/tasks. In this paper, we fill this gap in the context of tabular data by analyzing how DP generative models distribute privacy budgets across rows and columns, arguably the main source of utility degradation. We examine the main factors contributing to how privacy budgets are spent, including underlying modeling techniques, DP mechanisms, and data dimensionality.  Our extensive evaluation of both graphical and deep generative models sheds light on the distinctive features that render them suitable for different settings and tasks. We show that graphical models distribute the privacy budget horizontally and thus cannot handle relatively wide datasets while the performance on the task they were optimized for monotonicall
&lt;/p&gt;</description></item><item><title>SPENSER&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#25506;&#32034;&#30340;&#33258;&#21160;&#35774;&#35745;&#21644;&#21442;&#25968;&#21270;&#30340;SNNs &#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.10987</link><description>&lt;p&gt;
SPENSER&#65306;&#38754;&#21521;&#21367;&#31215;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
SPENSER: Towards a NeuroEvolutionary Approach for Convolutional Spiking Neural Networks. (arXiv:2305.10987v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10987
&lt;/p&gt;
&lt;p&gt;
SPENSER&#26159;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36827;&#21270;&#31639;&#27861;&#25506;&#32034;&#30340;&#33258;&#21160;&#35774;&#35745;&#21644;&#21442;&#25968;&#21270;&#30340;SNNs &#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#33021;&#28304;&#25928;&#29575;&#21644;&#29983;&#29289;&#21487;&#34892;&#24615;&#32780;&#21463;&#21040;&#20102;&#36817;&#26469;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;SNNs &#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#22240;&#20026;&#30446;&#21069;&#36824;&#27809;&#26377;&#20851;&#20110;SNNs &#30340;&#26368;&#20339;&#23398;&#20064;&#31639;&#27861;&#30340;&#20849;&#35782;&#12290;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;SNNs &#26159;&#22522;&#20110;ANNs &#36716;&#21270;&#25110;&#20351;&#29992;&#22522;&#20110;&#33033;&#20914;&#30340;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#24320;&#21457;&#21644;&#27979;&#35797;&#19981;&#21516;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#36827;&#34892;&#25163;&#24037;&#26550;&#26500;&#21644;&#21442;&#25968;&#35843;&#25972;&#12290;&#31070;&#32463;&#36827;&#21270;&#65288;NE&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;ANNs &#21644;&#35843;&#25972;&#21442;&#25968;&#30340;&#26377;&#25928;&#26041;&#24335;&#65292;&#20294;&#20854;&#22312;SNNs &#20013;&#30340;&#24212;&#29992;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;DENSER &#26159;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#21644;&#32467;&#26500;&#35821;&#27861;&#36827;&#21270;&#65288;SGE&#65289;&#21407;&#21017;&#30340;&#33258;&#21160;&#35774;&#35745;ANNs &#21644;&#21442;&#25968;&#21270;&#30340;NE&#26694;&#26550;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SPENSER&#65292;&#19968;&#31181;&#22522;&#20110;DENSER &#30340;SNNs &#29983;&#25104;NE&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have attracted recent interest due to their energy efficiency and biological plausibility. However, the performance of SNNs still lags behind traditional Artificial Neural Networks (ANNs), as there is no consensus on the best learning algorithm for SNNs. Best-performing SNNs are based on ANN to SNN conversion or learning with spike-based backpropagation through surrogate gradients. The focus of recent research has been on developing and testing different learning strategies, with hand-tailored architectures and parameter tuning. Neuroevolution (NE), has proven successful as a way to automatically design ANNs and tune parameters, but its applications to SNNs are still at an early stage. DENSER is a NE framework for the automatic design and parametrization of ANNs, based on the principles of Genetic Algorithms (GA) and Structured Grammatical Evolution (SGE). In this paper, we propose SPENSER, a NE framework for SNN generation based on DENSER, for image clas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#38382;&#39064;&#65292;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#26159;&#23548;&#33268;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#25552;&#20986;&#23398;&#20064;&#20026;&#31232;&#30095;&#32593;&#32476;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#24182;&#20998;&#24320;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10964</link><description>&lt;p&gt;
&#23398;&#20064;&#20026;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#20989;&#25968;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
Learning Activation Functions for Sparse Neural Networks. (arXiv:2305.10964v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#38382;&#39064;&#65292;&#21457;&#29616;&#28608;&#27963;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#26159;&#23548;&#33268;&#38382;&#39064;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#25552;&#20986;&#23398;&#20064;&#20026;&#31232;&#30095;&#32593;&#32476;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#24182;&#20998;&#24320;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#25512;&#26029;&#26102;&#21487;&#20197;&#33410;&#30465;&#22823;&#37327;&#33021;&#37327;&#21644;&#20869;&#23384;&#65292;&#21516;&#26102;&#21487;&#20197;&#34920;&#29616;&#20986;&#31867;&#20284;&#20110;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290; &#28982;&#32780;&#65292;&#22312;&#39640;&#20462;&#21098;&#27604;&#29575;&#19979;SNN&#30340;&#20934;&#30830;&#24230;&#38477;&#20302;&#21487;&#33021;&#22312;&#20851;&#38190;&#37096;&#32626;&#26465;&#20214;&#19979;&#25104;&#20026;&#38382;&#39064;&#12290; &#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#22797;&#26434;&#30340;&#20462;&#21098;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#20851;&#27880;&#34987;&#24573;&#30053;&#30340;&#22240;&#32032;&#65306;&#36229;&#21442;&#25968;&#21644;&#28608;&#27963;&#20989;&#25968;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20934;&#30830;&#24230;&#19979;&#38477;&#21487;&#20197;&#39069;&#22806;&#24402;&#22240;&#20110;&#65288;i&#65289;&#26222;&#36941;&#20351;&#29992;ReLU&#20316;&#20026;&#28608;&#27963;&#20989;&#25968;&#30340;&#40664;&#35748;&#36873;&#25321;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20351;&#29992;&#19982;&#23494;&#38598;&#32593;&#32476;&#30456;&#21516;&#30340;&#36229;&#21442;&#25968;&#26469;&#24494;&#35843;SNN&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23398;&#20064;&#20026;&#31232;&#30095;&#32593;&#32476;&#35843;&#25972;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#23558;&#20854;&#19982;&#31232;&#30095;&#32593;&#32476;&#30340;&#20998;&#24320;&#36229;&#21442;&#25968;&#20248;&#21270;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290; &#36890;&#36807;&#23545;&#22312;MNIST&#19978;&#35757;&#32451;&#30340;&#27969;&#34892;DNN&#27169;&#22411;&#65288;LeNet-5&#65292;VGG-16&#65292;ResNet-18&#21644;EfficientNet-B0&#65289;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Sparse Neural Networks (SNNs) can potentially demonstrate similar performance to their dense counterparts while saving significant energy and memory at inference. However, the accuracy drop incurred by SNNs, especially at high pruning ratios, can be an issue in critical deployment conditions. While recent works mitigate this issue through sophisticated pruning techniques, we shift our focus to an overlooked factor: hyperparameters and activation functions. Our analyses have shown that the accuracy drop can additionally be attributed to (i) Using ReLU as the default choice for activation functions unanimously, and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts. Thus, we focus on learning a novel way to tune activation functions for sparse networks and combining these with a separate hyperparameter optimization (HPO) regime for sparse networks. By conducting experiments on popular DNN models (LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CI
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#21644;&#29289;&#29702;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#20919;&#21364;&#28082;&#20919;&#21364;&#30340;&#30005;&#27744;&#21253;&#28201;&#24230;&#30340;&#26041;&#27861;&#65292;&#20854;&#23454;&#29616;&#20102;&#26368;&#20248;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.10952</link><description>&lt;p&gt;
&#20351;&#29992;&#29289;&#29702;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#65306;&#25511;&#21046;&#20919;&#21364;&#28082;&#20919;&#21364;&#30340;&#30005;&#27744;&#21253;&#30340;1D PDE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic Methods using Physics-Informed Neural Networks: Control of a 1D PDE Model for Fluid-Cooled Battery Packs. (arXiv:2305.10952v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10952
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#31639;&#27861;&#21644;&#29289;&#29702;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#20919;&#21364;&#28082;&#20919;&#21364;&#30340;&#30005;&#27744;&#21253;&#28201;&#24230;&#30340;&#26041;&#27861;&#65292;&#20854;&#23454;&#29616;&#20102;&#26368;&#20248;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#65292;&#29992;&#20110;&#25511;&#21046;&#20351;&#29992;&#20919;&#21364;&#28082;&#30340;&#30005;&#27744;&#32452;&#30340;&#28201;&#24230;&#12290;&#27169;&#22411;&#26159;&#30001;&#19968;&#20010;&#24102;&#26377;&#25511;&#21046;&#23545;&#27969;&#39033;&#30340;&#32806;&#21512;&#30340;1D&#20559;&#24494;&#20998;&#26041;&#31243;(PDE)&#24314;&#27169;&#30340;&#12290;&#21704;&#23494;&#39039;-&#38597;&#21508;&#27604;-&#36125;&#23572;&#26364;&#65288;HJB&#65289;&#26041;&#31243;&#26159;&#19968;&#20010;PDE&#65292;&#29992;&#20110;&#35780;&#20272;&#20540;&#20989;&#25968;&#30340;&#26368;&#20248;&#24615;&#24182;&#30830;&#23450;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#20215;&#20540;&#32593;&#32476;&#35270;&#20026;&#29289;&#29702;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;(PINN)&#20197;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;&#30340;HJB&#26041;&#31243;&#65292;&#32780;&#19981;&#26159;&#31163;&#25955;&#26102;&#38388;&#30340;Bellman&#26368;&#20248;&#26041;&#31243;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;&#20026;&#29615;&#22659;&#25552;&#20379;&#20248;&#21270;&#25511;&#21046;&#30340;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#20010;&#28151;&#21512;&#31574;&#30053;&#26041;&#27861;&#65292;&#20351;&#29992;HJB&#26041;&#31243;&#26356;&#26032;&#20215;&#20540;&#32593;&#32476;&#24182;&#20687;PPO&#19968;&#26679;&#26356;&#26032;&#31574;&#30053;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#25511;&#21046;&#36825;&#20010;PDE&#31995;&#32479;&#26041;&#38754;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an actor-critic algorithm for controlling the temperature of a battery pack using a cooling fluid. This is modeled by a coupled 1D partial differential equation (PDE) with a controlled advection term that determines the speed of the cooling fluid. The Hamilton-Jacobi-Bellman (HJB) equation is a PDE that evaluates the optimality of the value function and determines an optimal controller. We propose an algorithm that treats the value network as a Physics-Informed Neural Network (PINN) to solve for the continuous-time HJB equation rather than a discrete-time Bellman optimality equation, and we derive an optimal controller for the environment that we exploit to achieve optimal control. Our experiments show that a hybrid-policy method that updates the value network using the HJB equation and updates the policy network identically to PPO achieves the best results in the control of this PDE system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#36171;&#33021;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#26041;&#26696;&#12289;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#12289;&#35745;&#31639;&#36164;&#28304;&#21644;RIS&#21453;&#23556;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#33021;&#25928;&#25512;&#29702;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.10931</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#36171;&#33021;&#30340;&#36793;&#32536;&#25512;&#29702;&#30340;&#26446;&#38597;&#26222;&#35834;&#22827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lyapunov-Driven Deep Reinforcement Learning for Edge Inference Empowered by Reconfigurable Intelligent Surfaces. (arXiv:2305.10931v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21160;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;&#36171;&#33021;&#30340;6G&#32593;&#32476;&#20013;&#65292;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#26041;&#26696;&#12289;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#12289;&#35745;&#31639;&#36164;&#28304;&#21644;RIS&#21453;&#23556;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#33021;&#25928;&#25512;&#29702;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#22791;&#21487;&#37325;&#26500;&#26234;&#33021;&#34920;&#38754;(RIS)&#30340;6G&#32593;&#32476;&#20013;&#26080;&#32447;&#36793;&#32536;&#33021;&#32791;&#20302;&#12289;&#26102;&#24310;&#20302;&#12289;&#20934;&#30830;&#24230;&#39640;&#30340;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22330;&#26223;&#65292;&#20854;&#20013;&#19968;&#32452;&#35774;&#22791;&#19981;&#26029;&#20135;&#29983;&#25910;&#38598;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21160;&#24577;&#25490;&#38431;&#31995;&#32479;&#22788;&#29702;&#12290;&#22522;&#20110;&#26446;&#38597;&#26222;&#35834;&#22827;&#38543;&#26426;&#20248;&#21270;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#30340;&#23130;&#23035;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#23398;&#20064;&#31639;&#27861;&#65292;&#32852;&#21512;&#20248;&#21270;&#25968;&#25454;&#21387;&#32553;&#26041;&#26696;&#12289;&#26080;&#32447;&#36164;&#28304;&#20998;&#37197;&#65288;&#21363;&#21151;&#29575;&#12289;&#20256;&#36755;&#39044;&#32534;&#30721;&#65289;&#12289;&#35745;&#31639;&#36164;&#28304;&#65288;&#21363;CPU&#21608;&#26399;&#65289;&#21644;RIS&#21453;&#23556;&#21442;&#25968;&#65288;&#21363;&#30456;&#31227;&#65289;&#65292;&#20197;&#23454;&#29616;&#28385;&#36275;&#31471;&#21040;&#31471;(E2E)&#26102;&#24310;&#21644;&#25512;&#29702;&#20934;&#30830;&#24615;&#30340;&#33021;&#25928;&#25512;&#29702;&#20998;&#31867;&#12290;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#21551;&#29992;&#20102;&#21160;&#24577;&#25511;&#21046;&#31995;&#32479;&#21644;&#26080;&#32447;&#20256;&#25773;&#29615;&#22659;&#65292;&#23454;&#29616;&#20102;&#20302;&#22797;&#26434;&#24230;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel algorithm for energy-efficient, low-latency, accurate inference at the wireless edge, in the context of 6G networks endowed with reconfigurable intelligent surfaces (RISs). We consider a scenario where new data are continuously generated/collected by a set of devices and are handled through a dynamic queueing system. Building on the marriage between Lyapunov stochastic optimization and deep reinforcement learning (DRL), we devise a dynamic learning algorithm that jointly optimizes the data compression scheme, the allocation of radio resources (i.e., power, transmission precoding), the computation resources (i.e., CPU cycles), and the RIS reflectivity parameters (i.e., phase shifts), with the aim of performing energy-efficient edge classification with end-to-end (E2E) delay and inference accuracy constraints. The proposed strategy enables dynamic control of the system and of the wireless propagation environment, performing a low-complexity optimization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21382;&#21490;&#25991;&#26412;&#20013;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#21363;&#20351;&#25968;&#25454;&#27880;&#37322;&#24456;&#23569;&#20063;&#21487;&#20197;&#33719;&#24471;&#20986;&#20154;&#24847;&#26009;&#30340;&#22909;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#26102;&#26399;&#22900;&#24441;&#35805;&#35821;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#27169;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#36801;&#31227;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.10928</link><description>&lt;p&gt;
&#21382;&#21490;&#25253;&#32440;&#24191;&#21578;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multilingual Event Extraction from Historical Newspaper Adverts. (arXiv:2305.10928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21382;&#21490;&#25991;&#26412;&#20013;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#21363;&#20351;&#25968;&#25454;&#27880;&#37322;&#24456;&#23569;&#20063;&#21487;&#20197;&#33719;&#24471;&#20986;&#20154;&#24847;&#26009;&#30340;&#22909;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#26102;&#26399;&#22900;&#24441;&#35805;&#35821;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#27169;&#24335;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#36801;&#31227;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#24110;&#21161;&#21382;&#21490;&#23398;&#23478;&#20998;&#26512;&#27604;&#25163;&#24037;&#21487;&#34892;&#24471;&#22810;&#30340;&#25991;&#26412;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#26377;&#30528;&#23454;&#38469;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#33719;&#21462;&#22823;&#22411;&#27880;&#37322;&#21382;&#21490;&#25991;&#26412;&#25968;&#25454;&#38598;&#38750;&#24120;&#22256;&#38590;&#65292;&#22240;&#20026;&#21482;&#26377;&#19987;&#19994;&#39046;&#22495;&#30340;&#19987;&#23478;&#25165;&#33021;&#21487;&#38752;&#22320;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26159;&#22312;&#29616;&#20195;&#35821;&#35328;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#24212;&#29992;&#20110;&#21382;&#21490;&#35821;&#26009;&#24211;&#26102;&#25928;&#26524;&#26174;&#33879;&#38477;&#20302;&#12290;&#36825;&#23545;&#36739;&#23569;&#30740;&#31350;&#30340;&#20219;&#21153;&#20197;&#21450;&#38750;&#33521;&#35821;&#35821;&#35328;&#26469;&#35828;&#23588;&#20854;&#26840;&#25163;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#32858;&#28966;&#20110;&#21382;&#21490;&#25991;&#26412;&#39046;&#22495;&#20013;&#26410;&#34987;&#20805;&#20998;&#24320;&#21457;&#30340;&#20107;&#20214;&#25552;&#21462;&#20219;&#21153;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#33521;&#35821;&#12289;&#27861;&#35821;&#21644;&#33655;&#20848;&#35821;&#65292;&#30001;&#26089;&#26399;&#27542;&#27665;&#26102;&#26399;&#30340;&#25253;&#32440;&#24191;&#21578;&#26500;&#25104;&#65292;&#25253;&#36947;&#20102;&#20174;&#22900;&#24441;&#20013;&#33258;&#30001;&#30340;&#34987;&#22900;&#24441;&#20154;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#21363;&#20351;&#25968;&#25454;&#27880;&#37322;&#24456;&#23569;&#65292;&#36890;&#36807;&#20174;&#29616;&#20195;&#25968;&#25454;&#38598;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#21644;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#20986;&#20154;&#24847;&#26009;&#30340;&#22909;&#32467;&#26524;&#65307;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#21147;&#65292;&#23637;&#31034;&#20102;&#35813;&#26102;&#26399;&#22900;&#24441;&#35805;&#35821;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#27169;&#24335;&#65307;&#21516;&#26102;&#65292;&#35821;&#35328;&#36801;&#31227;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#22810;&#35821;&#35328;&#20107;&#20214;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP methods can aid historians in analyzing textual materials in greater volumes than manually feasible. Developing such methods poses substantial challenges though. First, acquiring large, annotated historical datasets is difficult, as only domain experts can reliably label them. Second, most available off-the-shelf NLP models are trained on modern language texts, rendering them significantly less effective when applied to historical corpora. This is particularly problematic for less well studied tasks, and for languages other than English. This paper addresses these challenges while focusing on the under-explored task of event extraction from a novel domain of historical texts. We introduce a new multilingual dataset in English, French, and Dutch composed of newspaper ads from the early modern colonial period reporting on enslaved people who liberated themselves from enslavement. We find that: 1) even with scarce annotated data, it is possible to achieve surprisingly good results by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10924</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30340;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Structural Pruning for Diffusion Models. (arXiv:2305.10924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diff-Pruning&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;Taylor&#23637;&#24320;&#36807;&#31243;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#65292;&#20174;&#32780;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#24615;&#33021;&#31283;&#23450;&#65292;&#24182;&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPM&#65289;&#30340;&#36716;&#22411;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#36890;&#24120;&#28041;&#21450;&#21040;&#26174;&#33879;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26399;&#38388;&#37117;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-Pruning&#65292;&#19968;&#31181;&#19987;&#20026;&#20174;&#39044;&#20808;&#23384;&#22312;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#36731;&#37327;&#32423;&#25193;&#25955;&#27169;&#22411;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#21387;&#32553;&#26041;&#27861;&#65292;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;Diff-Pruning&#30340;&#26412;&#36136;&#26159;&#36890;&#36807;&#21098;&#26525;&#26102;&#38388;&#27493;&#38271;&#30340;Taylor&#23637;&#24320;&#65292;&#22312;&#36807;&#28388;&#25481;&#26080;&#36129;&#29486;&#25193;&#25955;&#27493;&#39588;&#21644;&#25972;&#21512;&#26377;&#20449;&#24687;&#30340;&#26799;&#24230;&#26469;&#35782;&#21035;&#37325;&#35201;&#26435;&#37325;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#31361;&#20986;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#20248;&#28857;&#65306;1&#65289;&#25928;&#29575;&#65306;&#23427;&#21487;&#20197;&#20197;&#21407;&#22987;&#35757;&#32451;&#25237;&#20837;&#30340;&#20165;10&#65285;&#21040;20&#65285;&#30340;&#20195;&#20215;&#23454;&#29616;&#32422;50&#65285;&#30340;FLOPs&#20943;&#23569;; 2&#65289;&#19968;&#33268;&#24615;: &#21098;&#26525;&#21518;&#30340;&#25193;&#25955;&#27169;&#22411;&#20135;&#29983;&#30340;&#25928;&#26524;&#19982;&#21407;&#22987;&#27169;&#22411;&#30456;&#24403;&#65292;&#19981;&#20250;&#24433;&#21709;&#29983;&#25104;&#24314;&#27169;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across four diverse datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;QPP&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10923</link><description>&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65306;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction: From Ad-hoc to Conversational Search. (arXiv:2305.10923v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#20174;Ad-hoc&#21040;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;QPP&#26041;&#27861;&#22312;&#20132;&#20114;&#24335;&#25628;&#32034;&#20013;&#26159;&#21542;&#20855;&#26377;&#25512;&#24191;&#24212;&#29992;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;(QPP)&#26159;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;QPP&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#30456;&#20851;&#21028;&#26029;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;QPP&#22312;Ad-hoc&#25628;&#32034;&#20013;&#38750;&#24120;&#26377;&#25928;&#21644;&#26377;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#35805;&#24335;&#25628;&#32034;(CS)&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637; &#12290;&#26377;&#25928;&#30340;QPP&#33021;&#22815;&#24110;&#21161;CS&#31995;&#32479;&#22312;&#19979;&#19968;&#36718;&#20915;&#23450;&#36866;&#24403;&#30340;&#34892;&#21160;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;CS&#30340;QPP&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#29616;&#21644;&#30740;&#31350;&#29616;&#26377;&#30340;QPP&#26041;&#27861;&#22312;CS&#19978;&#30340;&#26377;&#25928;&#24615;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#34429;&#28982;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#36890;&#36947;&#26816;&#32034;&#20219;&#21153;&#30456;&#21516;&#65292;&#20294;CS&#20013;&#30340;&#29992;&#25143;&#26597;&#35810;&#21462;&#20915;&#20110;&#23545;&#35805;&#21382;&#21490;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;QPP&#25361;&#25112;&#12290;&#25105;&#20204;&#23588;&#20854;&#26159;&#25506;&#35752;&#20174;Ad-hoc&#25628;&#32034;&#20013;QPP&#26041;&#27861;&#30340;&#30740;&#31350;&#32467;&#26524;&#22312;&#19977;&#20010;CS&#35774;&#32622;&#20013;&#30340;&#25512;&#24191;&#31243;&#24230;:(i) &#35780;&#20272;&#22522;&#20110;&#26597;&#35810;&#37325;&#20889;&#30340;&#26816;&#32034;&#26041;&#27861;&#30340;&#19981;&#21516;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Query performance prediction (QPP) is a core task in information retrieval. The QPP task is to predict the retrieval quality of a search system for a query without relevance judgments. Research has shown the effectiveness and usefulness of QPP for ad-hoc search. Recent years have witnessed considerable progress in conversational search (CS). Effective QPP could help a CS system to decide an appropriate action to be taken at the next turn. Despite its potential, QPP for CS has been little studied. We address this research gap by reproducing and studying the effectiveness of existing QPP methods in the context of CS. While the task of passage retrieval remains the same in the two settings, a user query in CS depends on the conversational history, introducing novel QPP challenges. In particular, we seek to explore to what extent findings from QPP methods for ad-hoc search generalize to three CS settings: (i) estimating the retrieval quality of different query rewriting-based retrieval met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10913</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-Supervised Visual-Textual Grounding with Semantic Prior Refinement. (arXiv:2305.10913v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#30340;&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#30446;&#26631;&#26159;&#23454;&#29616;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#65292;&#36890;&#36807;&#32852;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35270;&#35273;-&#25991;&#26412;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#20165;&#21033;&#29992;&#22270;&#20687;-&#21477;&#23376;&#23545;&#23398;&#20064;&#23454;&#20307;&#34920;&#31034;&#20013;&#30340;&#21306;&#22495;-&#30701;&#35821;&#23545;&#24212;&#20851;&#31995;&#12290;&#19982;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#65292;&#20854;&#38590;&#24230;&#26356;&#22823;&#65292;&#22240;&#20026;&#26080;&#27861;&#33719;&#24471;&#36793;&#30028;&#26694;&#21644;&#25991;&#26412;&#30701;&#35821;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#20808;&#39564;&#32454;&#21270;&#27169;&#22411;&#65288;SPRM&#65289;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#26159;&#36890;&#36807;&#32452;&#21512;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#30340;&#36755;&#20986;&#24471;&#21040;&#30340;&#12290;&#31532;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22359;&#26088;&#22312;&#36820;&#22238;&#25991;&#26412;&#30701;&#35821;&#21644;&#36793;&#30028;&#26694;&#20043;&#38388;&#30340;&#31895;&#30053;&#23545;&#40784;&#12290;&#31532;&#20108;&#20010;&#35757;&#32451;&#36807;&#30340;&#27169;&#22359;&#30001;&#20004;&#20010;&#23376;&#32452;&#20214;&#32452;&#25104;&#65292;&#29992;&#20110;&#32454;&#21270;&#31895;&#30053;&#30340;&#23545;&#40784;&#20197;&#25552;&#39640;&#26368;&#32456;&#30701;&#35821;-&#36793;&#30028;&#26694;&#23545;&#40784;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#27169;&#22411;&#30340;&#35757;&#32451;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#22270;&#20687;&#21644;&#21477;&#23376;&#20043;&#38388;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#20351;&#21516;&#19968;&#21477;&#23376;&#21644;&#19968;&#20010;&#26032;&#30340;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#30456;&#20284;&#24230;&#26368;&#23567;&#21270;&#65292;&#20197;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#35757;&#32451;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using only image-sentence pairs, weakly-supervised visual-textual grounding aims to learn region-phrase correspondences of the respective entity mentions. Compared to the supervised approach, learning is more difficult since bounding boxes and textual phrases correspondences are unavailable. In light of this, we propose the Semantic Prior Refinement Model (SPRM), whose predictions are obtained by combining the output of two main modules. The first untrained module aims to return a rough alignment between textual phrases and bounding boxes. The second trained module is composed of two sub-components that refine the rough alignment to improve the accuracy of the final phrase-bounding box alignments. The model is trained to maximize the multimodal similarity between an image and a sentence, while minimizing the multimodal similarity of the same sentence and a new unrelated image, carefully selected to help the most during training. Our approach shows state-of-the-art results on two popula
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10906</link><description>&lt;p&gt;
RobustFair: &#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#25932;&#23545;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RobustFair: Adversarial Evaluation through Fairness Confusion Directed Gradient Search. (arXiv:2305.10906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#30340;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#30456;&#32467;&#21512;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#25552;&#39640;DNN&#30340;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#30340;&#21487;&#20449;&#24230;&#32463;&#24120;&#21463;&#21040;&#36731;&#24494;&#25932;&#23545;&#25200;&#21160;&#30340;&#25361;&#25112;&#65292;&#36825;&#19981;&#20165;&#20250;&#30772;&#22351;&#39044;&#27979;&#20934;&#30830;&#24615;&#65288;&#40065;&#26834;&#24615;&#65289;&#32780;&#19988;&#21487;&#33021;&#20026;&#31867;&#20284;&#30340;&#36755;&#20837;&#23548;&#33268;&#26377;&#20559;&#39044;&#27979;&#65288;&#20010;&#20307;&#20844;&#24179;&#24615;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#20934;&#30830;&#20844;&#27491;&#24230;&#26469;&#24378;&#21046;&#23454;&#26045;&#20934;&#30830;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#20043;&#38388;&#30340;&#35856;&#21644;&#24179;&#34913;&#12290;&#23427;&#24341;&#20837;&#20102;&#20844;&#24179;&#28151;&#28102;&#30697;&#38453;&#30340;&#27010;&#24565;&#26469;&#23558;&#39044;&#27979;&#20998;&#31867;&#20026;&#30495;&#27491;&#20844;&#24179;&#12289;&#30495;&#27491;&#26377;&#20559;&#12289;&#20551;&#27491;&#20844;&#24179;&#21644;&#20551;&#26377;&#20559;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35856;&#27874;&#35780;&#20272;&#26041;&#27861;RobustFair&#65292;&#20351;&#29992;&#36890;&#36807;&#20844;&#24179;&#28151;&#28102;&#23450;&#21521;&#26799;&#24230;&#25628;&#32034;&#21046;&#20316;&#30340;&#25932;&#23545;&#25200;&#21160;&#65292;&#23545;DNN&#30340;&#20934;&#30830;&#20844;&#27491;&#24615;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;Taylor&#23637;&#24320;&#26469;&#36817;&#20284;&#25932;&#23545;&#23454;&#20363;&#30340;&#22522;&#26412;&#30495;&#23454;&#24615;&#65292;RobustFair&#21487;&#20197;&#29305;&#21035;&#35782;&#21035;&#19982;&#34394;&#20551;&#20844;&#24179;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#36825;&#36890;&#24120;&#22312;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#38590;&#20197;&#25417;&#25720;&#65292;&#22312;&#20010;&#20307;&#20844;&#24179;&#35780;&#20272;&#20013;&#32570;&#22833;&#12290;RobustFair&#21487;&#20197;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The trustworthiness of DNNs is often challenged by their vulnerability to minor adversarial perturbations, which may not only undermine prediction accuracy (robustness) but also cause biased predictions for similar inputs (individual fairness). Accurate fairness has been recently proposed to enforce a harmonic balance between accuracy and individual fairness. It induces the notion of fairness confusion matrix to categorize predictions as true fair, true biased, false fair, and false biased. This paper proposes a harmonic evaluation approach, RobustFair, for the accurate fairness of DNNs, using adversarial perturbations crafted through fairness confusion directed gradient search. By using Taylor expansions to approximate the ground truths of adversarial instances, RobustFair can particularly identify the robustness defects entangled for spurious fairness, which are often elusive in robustness evaluation, and missing in individual fairness evaluation. RobustFair can boost robustness and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26680;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;KMM&#65292;&#20854;&#29992;&#20110;&#36229;&#36234;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#30697;&#26041;&#27861;&#27169;&#22411;&#65292;&#35299;&#38500;&#20102;&#20851;&#20110;&#20351;&#29992; $\varphi$-&#25955;&#24230;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.10898</link><description>&lt;p&gt;
&#36229;&#36234;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#65306;&#26680;&#30697;&#27861;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation Beyond Data Reweighting: Kernel Method of Moments. (arXiv:2305.10898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26680;&#30697;&#27861;&#20272;&#35745;&#22120;&#65292;&#31216;&#20026;KMM&#65292;&#20854;&#29992;&#20110;&#36229;&#36234;&#25968;&#25454;&#37325;&#26032;&#21152;&#26435;&#30340;&#30697;&#26041;&#27861;&#27169;&#22411;&#65292;&#35299;&#38500;&#20102;&#20851;&#20110;&#20351;&#29992; $\varphi$-&#25955;&#24230;&#30456;&#20851;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#19982;&#32479;&#35745;&#23398;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#37117;&#20250;&#20986;&#29616;&#30697;&#32422;&#26463;&#21644;&#26465;&#20214;&#23545;&#24212;&#65292;&#20854;&#20013;&#65292;&#24191;&#20041;&#30697;&#27861;&#65288;GMM&#65289;&#20316;&#20026;&#19968;&#20010;&#20272;&#35745;&#27169;&#22411;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24448;&#24448;&#30001;&#20110;&#20351;&#29992; $\varphi$-&#25955;&#24230;&#30340;&#30456;&#20851;&#38480;&#21046;&#23558;&#20505;&#36873;&#20998;&#24067;&#38480;&#21046;&#20026;&#25968;&#25454;&#26679;&#26412;&#30340;&#37325;&#26032;&#21152;&#26435;&#12290;&#32780;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30697;&#20272;&#35745;&#26041;&#27861;&#8212;&#8212;&#22522;&#20110;&#26368;&#22823;&#22343;&#20540;&#20559;&#24046;&#30340;&#32463;&#39564;&#20284;&#28982;&#20272;&#35745;&#22120;&#65292;&#21363;&#26680;&#30697;&#27861;(KMM)&#65292;&#20854;&#23454;&#29616;&#36229;&#36234;&#20102;&#23545;&#25968;&#25454;&#30340;&#37325;&#26032;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moment restrictions and their conditional counterparts emerge in many areas of machine learning and statistics ranging from causal inference to reinforcement learning. Estimators for these tasks, generally called methods of moments, include the prominent generalized method of moments (GMM) which has recently gained attention in causal inference. GMM is a special case of the broader family of empirical likelihood estimators which are based on approximating a population distribution by means of minimizing a $\varphi$-divergence to an empirical distribution. However, the use of $\varphi$-divergences effectively limits the candidate distributions to reweightings of the data samples. We lift this long-standing limitation and provide a method of moments that goes beyond data reweighting. This is achieved by defining an empirical likelihood estimator based on maximum mean discrepancy which we term the kernel method of moments (KMM). We provide a variant of our estimator for conditional moment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#23567;&#39118;&#38505;&#37325;&#26032;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#22312;&#22343;&#26041;&#35823;&#24046;&#20998;&#35299;&#26694;&#26550;&#20869;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#37325;&#26032;&#26657;&#20934;&#27010;&#29575;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#24179;&#34913;&#26657;&#20934;&#21644;&#38160;&#24230;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#26742;&#25968;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22823;&#32422;$O(n^{-2/3})$&#30340;&#39118;&#38505;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.10886</link><description>&lt;p&gt;
&#20998;&#31867;&#22120;&#26368;&#23567;&#39118;&#38505;&#37325;&#26032;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Minimum-Risk Recalibration of Classifiers. (arXiv:2305.10886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#23567;&#39118;&#38505;&#37325;&#26032;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#22312;&#22343;&#26041;&#35823;&#24046;&#20998;&#35299;&#26694;&#26550;&#20869;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#37325;&#26032;&#26657;&#20934;&#27010;&#29575;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#24179;&#34913;&#26657;&#20934;&#21644;&#38160;&#24230;&#30830;&#23450;&#20102;&#26368;&#20248;&#30340;&#26742;&#25968;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22823;&#32422;$O(n^{-2/3})$&#30340;&#39118;&#38505;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#26657;&#20934;&#27010;&#29575;&#20998;&#31867;&#22120;&#23545;&#20110;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#20010;&#32508;&#21512;&#30340;&#29702;&#35770;&#26469;&#25972;&#21512;&#26657;&#20934;&#21644;&#38160;&#24230;&#65288;&#36825;&#23545;&#20110;&#20445;&#25345;&#39044;&#27979;&#21147;&#33267;&#20851;&#37325;&#35201;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20998;&#35299;&#26694;&#26550;&#20869;&#20171;&#32461;&#20102;&#26368;&#23567;&#39118;&#38505;&#37325;&#26032;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#37325;&#26032;&#26657;&#20934;&#27010;&#29575;&#20998;&#31867;&#22120;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22343;&#21248;&#36136;&#37327;&#20998;&#26742;&#65288;UMB&#65289;&#37325;&#26032;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#39118;&#38505;&#19978;&#30028;&#65292;&#20854;&#39034;&#24207;&#20026;$\tilde{O}(B/n+1/B^2)$&#65292;&#20854;&#20013;$B$&#26159;&#26742;&#30340;&#25968;&#37327;&#65292;$n$&#26159;&#26679;&#26412;&#22823;&#23567;&#12290;&#36890;&#36807;&#24179;&#34913;&#26657;&#20934;&#21644;&#38160;&#24230;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;UMB&#30340;&#26368;&#20248;&#26742;&#25968;&#19982;$n^{1/3}$&#25104;&#27604;&#20363;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#22823;&#32422;$O(n^{-2/3})$&#30340;&#39118;&#38505;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#23545;&#20102;&#26631;&#31614;&#31232;&#23569;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recalibrating probabilistic classifiers is vital for enhancing the reliability and accuracy of predictive models. Despite the development of numerous recalibration algorithms, there is still a lack of a comprehensive theory that integrates calibration and sharpness (which is essential for maintaining predictive power). In this paper, we introduce the concept of minimum-risk recalibration within the framework of mean-squared-error (MSE) decomposition, offering a principled approach for evaluating and recalibrating probabilistic classifiers. Using this framework, we analyze the uniform-mass binning (UMB) recalibration method and establish a finite-sample risk upper bound of order $\tilde{O}(B/n + 1/B^2)$ where $B$ is the number of bins and $n$ is the sample size. By balancing calibration and sharpness, we further determine that the optimal number of bins for UMB scales with $n^{1/3}$, resulting in a risk bound of approximately $O(n^{-2/3})$. Additionally, we tackle the challenge of label
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StawGAN&#30340;&#32467;&#26500;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#32763;&#35793;&#25104;&#30333;&#22825;&#24425;&#33394;&#22270;&#20687;&#65292;&#24182;&#22312;&#30446;&#26631;&#22495;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#24418;&#29366;&#21644;&#39640;&#28165;&#26224;&#24230;&#23545;&#35937;&#30340;&#32763;&#35793;&#65292;&#36798;&#21040;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10882</link><description>&lt;p&gt;
StawGAN: &#38754;&#21521;&#32418;&#22806;&#22270;&#20687;&#32763;&#35793;&#30340;&#32467;&#26500;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
StawGAN: Structural-Aware Generative Adversarial Networks for Infrared Image Translation. (arXiv:2305.10882v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;StawGAN&#30340;&#32467;&#26500;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#21487;&#20197;&#23558;&#28909;&#32418;&#22806;&#22270;&#20687;&#32763;&#35793;&#25104;&#30333;&#22825;&#24425;&#33394;&#22270;&#20687;&#65292;&#24182;&#22312;&#30446;&#26631;&#22495;&#20013;&#25552;&#20379;&#26356;&#22909;&#30340;&#24418;&#29366;&#21644;&#39640;&#28165;&#26224;&#24230;&#23545;&#35937;&#30340;&#32763;&#35793;&#65292;&#36798;&#21040;&#20102;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#23558;&#22812;&#38388;&#28909;&#32418;&#22806;&#22270;&#20687;&#65288;NTIT2DC&#65289;&#32763;&#35793;&#25104;&#30333;&#22825;&#24425;&#33394;&#22270;&#20687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#25552;&#39640;&#30446;&#26631;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#23545;&#20854;&#19978;&#33394;&#12290;&#25152;&#25552;&#20986;&#30340;&#32467;&#26500;&#24863;&#30693;&#65288;StawGAN&#65289;&#22312;&#30446;&#26631;&#22495;&#20013;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24418;&#29366;&#21644;&#39640;&#28165;&#26224;&#24230;&#23545;&#35937;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#22312;DroneVeichle &#25968;&#25454;&#38598;&#30340;&#33322;&#25293;&#22270;&#20687;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26356;&#20934;&#30830;&#30340;&#32763;&#35793;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/LuigiSigillo/StawGAN&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of translating night-time thermal infrared images, which are the most adopted image modalities to analyze night-time scenes, to daytime color images (NTIT2DC), which provide better perceptions of objects. We introduce a novel model that focuses on enhancing the quality of the target generation without merely colorizing it. The proposed structural aware (StawGAN) enables the translation of better-shaped and high-definition objects in the target domain. We test our model on aerial images of the DroneVeichle dataset containing RGB-IR paired images. The proposed approach produces a more accurate translation with respect to other state-of-the-art image translation models. The source code is available at https://github.com/LuigiSigillo/StawGAN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36317;&#31163;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#30340;&#29305;&#24449;&#65292;&#26469;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.10869</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#20445;&#25252;&#30340;&#20998;&#24067;&#24335;&#22270;&#23398;&#20064;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Free Lunch for Privacy Preserving Distributed Graph Learning. (arXiv:2305.10869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20445;&#25252;&#38544;&#31169;&#30340;&#20998;&#24067;&#24335;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36317;&#31163;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#30340;&#29305;&#24449;&#65292;&#26469;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#12289;&#26426;&#22120;&#20154;&#12289;&#36890;&#20449;&#12289;&#21307;&#23398;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#22270;&#24418;&#23398;&#20064;&#27491;&#22312;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#36825;&#20123;&#23646;&#20110;&#19981;&#21516;&#23454;&#20307;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#21253;&#21547;&#20851;&#38190;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20849;&#20139;&#30340;&#38544;&#31169;&#38382;&#39064;&#20351;&#24471;&#24212;&#29992;&#22270;&#24418;&#23398;&#20064;&#21464;&#24471;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36890;&#36807;&#25552;&#21462;&#29992;&#25143;&#20391;&#30340;&#29305;&#24449;&#26469;&#39044;&#22788;&#29702;&#25968;&#25454;&#65292;&#24182;&#20165;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#36827;&#34892;&#19979;&#19968;&#27493;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#31169;&#20154;&#23646;&#24615;&#36827;&#34892;&#25512;&#26029;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#24067;&#24335;&#22270;&#24418;&#23398;&#20064;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#26426;&#22120;&#23398;&#20064;&#12290;&#20026;&#20102;&#22312;&#26381;&#21153;&#22120;&#31471;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#21644;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#21644;&#36317;&#31163;&#65292;&#32780;&#19981;&#38656;&#35201;&#23454;&#38469;&#30340;&#29305;&#24449;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#30340;&#32467;&#26500;&#29305;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#19988;&#39640;&#24230;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning on graphs is becoming prevalent in a wide range of applications including social networks, robotics, communication, medicine, etc. These datasets belonging to entities often contain critical private information. The utilization of data for graph learning applications is hampered by the growing privacy concerns from users on data sharing. Existing privacy-preserving methods pre-process the data to extract user-side features, and only these features are used for subsequent learning. Unfortunately, these methods are vulnerable to adversarial attacks to infer private attributes. We present a novel privacy-respecting framework for distributed graph learning and graph-based machine learning. In order to perform graph learning and other downstream tasks on the server side, this framework aims to learn features as well as distances without requiring actual features while preserving the original structural properties of the raw data. The proposed framework is quite generic and highly a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10865</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Semantically Aligned Task Decomposition in Multi-Agent Reinforcement Learning. (arXiv:2305.10865v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26032;&#26041;&#27861;SAMA&#65292;&#36890;&#36807;&#25552;&#21069;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#20219;&#21153;&#20998;&#35299;&#26469;&#35299;&#20915;ASG&#26041;&#27861;&#23384;&#22312;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#21644;&#29983;&#25104;&#38750;&#23454;&#38469;&#20219;&#21153;&#22870;&#21169;&#30340;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#22411;MARL&#20013;&#30340;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#30528;&#37325;&#20110;&#36866;&#24403;&#30340;&#20449;&#29992;&#20998;&#37197;&#12290;&#33258;&#21160;&#23376;&#30446;&#26631;&#29983;&#25104;&#65288;ASG&#65289;&#26159;&#26368;&#36817;&#20986;&#29616;&#30340;&#19968;&#31181;&#21487;&#34892;&#30340;MARL&#26041;&#27861;&#65292;&#20854;&#28789;&#24863;&#26469;&#33258;&#20110;&#22312;&#20869;&#22312;&#39537;&#21160;&#30340;&#22686;&#24378;&#23398;&#20064;&#20013;&#21033;&#29992;&#23376;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20174;&#31232;&#30095;&#22870;&#21169;&#20013;&#36827;&#34892;&#22797;&#26434;&#20219;&#21153;&#35268;&#21010;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26080;&#30097;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;"&#35299;&#32806;"&#20915;&#31574;&#26041;&#27861;&#65292;&#21363;&#22312;MARL&#20013;&#30340;&#35821;&#20041;&#23545;&#40784;&#20219;&#21153;&#20998;&#35299;&#65288;SAMA&#65289;&#65292;&#21463;&#21040;&#35299;&#32806;&#34920;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
The difficulty of appropriately assigning credit is particularly heightened in cooperative MARL with sparse reward, due to the concurrent time and structural scales involved. Automatic subgoal generation (ASG) has recently emerged as a viable MARL approach inspired by utilizing subgoals in intrinsically motivated reinforcement learning. However, end-to-end learning of complex task planning from sparse rewards without prior knowledge, undoubtedly requires massive training samples. Moreover, the diversity-promoting nature of existing ASG methods can lead to the "over-representation" of subgoals, generating numerous spurious subgoals of limited relevance to the actual task reward and thus decreasing the sample efficiency of the algorithm. To address this problem and inspired by the disentangled representation learning, we propose a novel "disentangled" decision-making method, Semantically Aligned task decomposition in MARL (SAMA), that prompts pretrained language models with chain-of-thou
&lt;/p&gt;</description></item><item><title>Quiver &#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#22522;&#20110; GPU &#30340; GNN &#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24037;&#20316;&#36127;&#36733;&#25351;&#26631;&#26469;&#39044;&#27979; GNN &#35831;&#27714;&#30340;&#19981;&#35268;&#21017;&#35745;&#31639;&#65292;&#24182;&#31649;&#29702; GPU &#29992;&#20110;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#65292;&#27604;&#29616;&#26377;&#31995;&#32479;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798; 15x&#12290;</title><link>http://arxiv.org/abs/2305.10863</link><description>&lt;p&gt;
Quiver: &#22522;&#20110;&#24037;&#20316;&#36127;&#36733;&#24863;&#30693;&#30340;&#20302;&#24310;&#36831;&#12289;&#39640;&#21534;&#21520;&#37327;&#30340; GNN &#26381;&#21153;&#25903;&#25345; GPU
&lt;/p&gt;
&lt;p&gt;
Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving with Workload Awareness. (arXiv:2305.10863v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10863
&lt;/p&gt;
&lt;p&gt;
Quiver &#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#22522;&#20110; GPU &#30340; GNN &#26381;&#21153;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#24037;&#20316;&#36127;&#36733;&#25351;&#26631;&#26469;&#39044;&#27979; GNN &#35831;&#27714;&#30340;&#19981;&#35268;&#21017;&#35745;&#31639;&#65292;&#24182;&#31649;&#29702; GPU &#29992;&#20110;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#65292;&#27604;&#29616;&#26377;&#31995;&#32479;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798; 15x&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#30340;&#25512;&#29702;&#26381;&#21153;&#31995;&#32479;&#24517;&#39035;&#22312;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20294;&#30001;&#20110;&#37319;&#26679;&#30340;&#22270;&#33410;&#28857;&#21644;&#32858;&#21512;&#30340; GNN &#29305;&#24449;&#23384;&#22312;&#20559;&#24046;&#65292;&#31995;&#32479;&#38754;&#20020;&#19981;&#35268;&#21017;&#35745;&#31639;&#30340;&#25361;&#25112;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#21033;&#29992; GPU &#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#20165;&#20351;&#29992; GPU &#23545;&#23569;&#37327;&#22270;&#33410;&#28857;&#36827;&#34892;&#37319;&#26679;&#30340;&#24615;&#33021;&#20302;&#20110;&#22522;&#20110; CPU &#30340;&#37319;&#26679;&#65307;&#32780;&#23545;&#35768;&#22810;&#29305;&#24449;&#36827;&#34892;&#32858;&#21512;&#20250;&#20135;&#29983; GPU &#21644; CPU &#20043;&#38388;&#30340;&#39640;&#25968;&#25454;&#31227;&#21160;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340; GNN &#26381;&#21153;&#31995;&#32479;&#20351;&#29992; CPU &#36827;&#34892;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65292;&#38480;&#21046;&#20102;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102; Quiver&#65292;&#19968;&#31181;&#20998;&#24067;&#24335;&#22522;&#20110; GPU &#30340; GNN &#26381;&#21153;&#31995;&#32479;&#65292;&#20855;&#26377;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;Quiver &#30340;&#20851;&#38190;&#24605;&#36335;&#26159;&#21033;&#29992;&#24037;&#20316;&#36127;&#36733;&#25351;&#26631;&#26469;&#39044;&#27979; GNN &#35831;&#27714;&#30340;&#19981;&#35268;&#21017;&#35745;&#31639;&#65292;&#24182;&#31649;&#29702; GPU &#29992;&#20110;&#22270;&#37319;&#26679;&#21644;&#29305;&#24449;&#32858;&#21512;&#65306;(1) &#23545;&#20110;&#22270;&#37319;&#26679;&#65292;Quiver &#35745;&#31639;&#27010;&#29575;&#37319;&#26679;&#30340;&#22270;&#22823;&#23567;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#33410;&#28857;&#37319;&#26679;&#24182;&#34892;&#24230;&#30340;&#25351;&#26631;&#65307;(2) &#23545;&#20110;&#29305;&#24449;&#32858;&#21512;&#65292;Quiver &#37319;&#29992;&#21015;&#27714;&#21644;&#30340;&#26041;&#24335;&#28040;&#38500;&#25968;&#25454;&#31227;&#21160;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Quiver &#22312;&#23454;&#29616; 94&#65285; &#30340; GPU &#21033;&#29992;&#29575;&#30340;&#21516;&#26102;&#65292;&#27604;&#29616;&#26377;&#30340; GNN &#26381;&#21153;&#31995;&#32479;&#24615;&#33021;&#25552;&#39640;&#20102;&#22810;&#36798; 15 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systems for serving inference requests on graph neural networks (GNN) must combine low latency with high throughout, but they face irregular computation due to skew in the number of sampled graph nodes and aggregated GNN features. This makes it challenging to exploit GPUs effectively: using GPUs to sample only a few graph nodes yields lower performance than CPU-based sampling; and aggregating many features exhibits high data movement costs between GPUs and CPUs. Therefore, current GNN serving systems use CPUs for graph sampling and feature aggregation, limiting throughput.  We describe Quiver, a distributed GPU-based GNN serving system with low-latency and high-throughput. Quiver's key idea is to exploit workload metrics for predicting the irregular computation of GNN requests, and governing the use of GPUs for graph sampling and feature aggregation: (1) for graph sampling, Quiver calculates the probabilistic sampled graph size, a metric that predicts the degree of parallelism in graph
&lt;/p&gt;</description></item><item><title>Q-SHED&#26159;&#19968;&#31181;&#22522;&#20110;&#28023;&#26862;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#37327;&#21270;&#30340;&#36793;&#32536;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36890;&#20449;&#39640;&#25928;&#12289;&#31283;&#20581;&#25910;&#25947;&#36895;&#29575;&#31561;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.10852</link><description>&lt;p&gt;
Q-SHED: &#22522;&#20110;&#28023;&#26862;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#37327;&#21270;&#30340;&#36793;&#32536;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Q-SHED: Distributed Optimization at the Edge via Hessian Eigenvectors Quantization. (arXiv:2305.10852v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10852
&lt;/p&gt;
&lt;p&gt;
Q-SHED&#26159;&#19968;&#31181;&#22522;&#20110;&#28023;&#26862;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#37327;&#21270;&#30340;&#36793;&#32536;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36890;&#20449;&#39640;&#25928;&#12289;&#31283;&#20581;&#25910;&#25947;&#36895;&#29575;&#31561;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#32593;&#32476;&#38656;&#35201;&#36890;&#20449;&#39640;&#25928;&#12289;&#31283;&#20581;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#12290; &#26032;&#29275;&#39039;&#31867;&#22411;&#65288;NT&#65289;&#26041;&#27861;&#20316;&#20026;&#20855;&#26377;&#36275;&#22815;&#35745;&#31639;&#33021;&#21147;&#30340;&#36793;&#32536;&#35774;&#22791;&#20013;&#20855;&#26377;&#40065;&#26834;&#25910;&#25947;&#36895;&#29575;&#30340; DO &#38382;&#39064;&#30340;&#20419;&#25104;&#32773;&#65292;&#26368;&#36817;&#34987;&#25552;&#20513;&#12290; &#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Q-SHED&#65292;&#19968;&#31181;&#21407;&#22987;&#30340;&#29992;&#20110; DO &#30340; NT &#31639;&#27861;&#65292;&#20854;&#29305;&#24449;&#26159;&#22522;&#20110;&#22686;&#37327;&#28023;&#26862;&#30697;&#38453;&#29305;&#24449;&#21521;&#37327;&#37327;&#21270;&#30340;&#26032;&#22411;&#27604;&#29305;&#20998;&#37197;&#26041;&#26696;&#12290; &#36825;&#31181;&#25552;&#35758;&#30340;&#25216;&#26415;&#19982;&#26368;&#36817;&#30340; SHED &#31639;&#27861;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#23427;&#32487;&#25215;&#20102; SHED &#31639;&#27861;&#30340;&#21560;&#24341;&#20154;&#29305;&#24449;&#65292;&#20363;&#22914;&#25152;&#38656;&#30340;&#27969;&#37327;&#35745;&#31639;&#25968;&#37327;&#24456;&#23569;&#65292;&#21516;&#26102;&#22312;&#20301;&#20998;&#36776;&#29575;&#19978;&#20855;&#26377;&#24102;&#23485;&#21487;&#21464;&#24615;&#12290;&#25105;&#20204;&#22312;&#19982;&#31454;&#20105;&#23545;&#25163;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#35777;&#23454;&#20102;&#31639;&#27861;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge networks call for communication efficient (low overhead) and robust distributed optimization (DO) algorithms. These are, in fact, desirable qualities for DO frameworks, such as federated edge learning techniques, in the presence of data and system heterogeneity, and in scenarios where internode communication is the main bottleneck. Although computationally demanding, Newton-type (NT) methods have been recently advocated as enablers of robust convergence rates in challenging DO problems where edge devices have sufficient computational power. Along these lines, in this work we propose Q-SHED, an original NT algorithm for DO featuring a novel bit-allocation scheme based on incremental Hessian eigenvectors quantization. The proposed technique is integrated with the recent SHED algorithm, from which it inherits appealing features like the small number of required Hessian computations, while being bandwidth-versatile at a bit-resolution level. Our empirical evaluation against competing 
&lt;/p&gt;</description></item><item><title>GETMusic&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#38899;&#20048;&#34920;&#31034;GETScore&#21644;&#25193;&#25955;&#27169;&#22411;GETDiff&#12290;GETScore&#20351;&#29992;&#26631;&#35760;&#34920;&#31034;&#38899;&#31526;&#65292;&#23558;&#23427;&#20204;&#26377;&#24207;&#22320;&#32452;&#32455;&#36215;&#26469;&#65292;&#32780;GETDiff&#20351;&#29992;&#36974;&#30422;&#23545;&#30446;&#26631;&#36712;&#36947;&#36827;&#34892;&#30772;&#22351;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#36712;&#36947;&#12290;</title><link>http://arxiv.org/abs/2305.10841</link><description>&lt;p&gt;
GETMusic&#65306;&#20351;&#29992;&#32479;&#19968;&#30340;&#34920;&#31034;&#21644;&#25193;&#25955;&#26694;&#26550;&#29983;&#25104;&#20219;&#24847;&#38899;&#20048;&#26354;&#30446;
&lt;/p&gt;
&lt;p&gt;
GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework. (arXiv:2305.10841v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10841
&lt;/p&gt;
&lt;p&gt;
GETMusic&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#38899;&#20048;&#34920;&#31034;GETScore&#21644;&#25193;&#25955;&#27169;&#22411;GETDiff&#12290;GETScore&#20351;&#29992;&#26631;&#35760;&#34920;&#31034;&#38899;&#31526;&#65292;&#23558;&#23427;&#20204;&#26377;&#24207;&#22320;&#32452;&#32455;&#36215;&#26469;&#65292;&#32780;GETDiff&#20351;&#29992;&#36974;&#30422;&#23545;&#30446;&#26631;&#36712;&#36947;&#36827;&#34892;&#30772;&#22351;&#65292;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#36712;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#38899;&#20048;&#29983;&#25104;&#26088;&#22312;&#21019;&#24314;&#38899;&#31526;&#65292;&#20026;&#29992;&#25143;&#21019;&#20316;&#38899;&#20048;&#25552;&#20379;&#24110;&#21161;&#65292;&#20363;&#22914;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#30446;&#26631;&#20048;&#22120;&#36712;&#36947;&#25110;&#22522;&#20110;&#29992;&#25143;&#25552;&#20379;&#30340;&#28304;&#36712;&#36947;&#36827;&#34892;&#21019;&#20316;&#12290;&#32771;&#34385;&#21040;&#28304;&#36712;&#36947;&#21644;&#30446;&#26631;&#36712;&#36947;&#20043;&#38388;&#30340;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#24615;&#32452;&#21512;&#65292;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#20219;&#24847;&#36712;&#36947;&#30340;&#32479;&#19968;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#20316;&#21697;&#30001;&#20110;&#38899;&#20048;&#34920;&#31034;&#21644;&#27169;&#22411;&#26550;&#26500;&#30340;&#22266;&#26377;&#38480;&#21046;&#32780;&#26410;&#33021;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GETMusic&#65288;`GET'&#20195;&#34920;GEnerate music Tracks&#65289;&#30340;&#32479;&#19968;&#34920;&#31034;&#21644;&#25193;&#25955;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#38899;&#20048;&#34920;&#31034;GETScore&#21644;&#19968;&#20010;&#21517;&#20026;GETDiff&#30340;&#25193;&#25955;&#22411;&#27169;&#22411;&#12290;GETScore&#23558;&#38899;&#31526;&#34920;&#31034;&#20026;&#26631;&#35760;&#65292;&#24182;&#22312;&#20108;&#32500;&#32467;&#26500;&#20013;&#20117;&#28982;&#26377;&#24207;&#22320;&#32452;&#32455;&#23427;&#20204;&#65292;&#36712;&#36947;&#22402;&#30452;&#22534;&#21472;&#24182;&#27700;&#24179;&#22320;&#38543;&#26102;&#38388;&#25512;&#36827;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#36712;&#36947;&#38543;&#26426;&#34987;&#36873;&#20026;&#30446;&#26631;&#25110;&#28304;&#12290;&#22312;&#21069;&#21521;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#36974;&#30422;&#23545;&#30446;&#26631;&#36712;&#36947;&#36827;&#34892;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrumental tracks from scratch, or based on user-provided source tracks. Considering the diverse and flexible combination between source and target tracks, a unified model capable of generating any arbitrary tracks is of crucial necessity. Previous works fail to address this need due to inherent constraints in music representations and model architectures. To address this need, we propose a unified representation and diffusion framework named GETMusic (`GET' stands for GEnerate music Tracks), which includes a novel music representation named GETScore, and a diffusion model named GETDiff. GETScore represents notes as tokens and organizes them in a 2D structure, with tracks stacked vertically and progressing horizontally over time. During training, tracks are randomly selected as either the target or source. In the forward process, target tracks are corrupted by masking
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#34920;&#24449;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#25968;&#25454;&#28857;&#30340;&#31934;&#30830;&#24230;&#24182;&#24110;&#21161;&#33258;&#21160;&#20390;&#27979;&#24322;&#24120;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.10840</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Deep Neural Networks through Statistical Inference on Latent Space. (arXiv:2305.10840v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#31354;&#38388;&#34920;&#24449;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#25968;&#25454;&#28857;&#30340;&#31934;&#30830;&#24230;&#24182;&#24110;&#21161;&#33258;&#21160;&#20390;&#27979;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#31354;&#38388;&#34920;&#24449;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#22788;&#29702;&#25968;&#25454;&#28857;&#30340;&#31934;&#30830;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#35299;&#20915;&#24403;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#8220;&#36807;&#20110;&#33258;&#20449;&#8221;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#32593;&#32476;&#33021;&#22815;&#27491;&#30830;&#20998;&#31867;&#30340;&#37096;&#20998;&#35757;&#32451;&#38598;&#25152;&#29983;&#25104;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#24182;&#20197;&#27492;&#24314;&#31435;&#20102;&#33021;&#22815;&#25429;&#25417;&#39044;&#27979;&#32467;&#26524;&#21487;&#33021;&#24615;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#24120;&#29992;&#26041;&#27861;&#19968;&#33324;&#23384;&#22312;&#8220;&#36807;&#20110;&#33258;&#20449;&#8221;&#30340;&#38382;&#39064;&#65292;&#29978;&#33267;&#23545;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#28857;&#20381;&#28982;&#22914;&#27492;&#12290;&#19982;&#20043;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21457;&#29616;&#31934;&#24230;&#27424;&#20339;&#30340;&#25968;&#25454;&#28857;&#65292;&#22240;&#27492;&#23545;&#20110;&#24322;&#24120;&#20540;&#30340;&#33258;&#21160;&#20390;&#27979;&#26159;&#26377;&#24110;&#21161;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-quantification methods are applied to estimate the confidence of deep-neural-networks classifiers over their predictions. However, most widely used methods are known to be overconfident. We address this problem by developing an algorithm that exploits the latent-space representation of data points fed into the network, to assess the accuracy of their prediction. Using the latent-space representation generated by the fraction of training set that the network classifies correctly, we build a statistical model that is able to capture the likelihood of a given prediction. We show on a synthetic dataset that commonly used methods are mostly overconfident. Overconfidence occurs also for predictions made on data points that are outside the distribution that generated the training data. In contrast, our method can detect such out-of-distribution data points as inaccurately predicted, thus aiding in the automatic detection of outliers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Ahead-of-Time &#65288;AoT&#65289;P-Tuning&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;Transformer&#23618;&#20043;&#21069;&#28155;&#21152;&#36755;&#20837;&#30456;&#20851;&#30340;&#20559;&#32622;&#65292;&#23454;&#29616;&#20102;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#33410;&#32422;&#12290;&#35813;&#26041;&#27861;&#22312;GLUE&#21644;SuperGLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;BitFit&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#32780;&#25512;&#29702;&#24320;&#38144;&#21364;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.10835</link><description>&lt;p&gt;
Ahead-of-Time P-Tuning&#65306;&#19968;&#31181;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#33410;&#32422;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ahead-of-Time P-Tuning. (arXiv:2305.10835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Ahead-of-Time &#65288;AoT&#65289;P-Tuning&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#27599;&#20010;Transformer&#23618;&#20043;&#21069;&#28155;&#21152;&#36755;&#20837;&#30456;&#20851;&#30340;&#20559;&#32622;&#65292;&#23454;&#29616;&#20102;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#33410;&#32422;&#12290;&#35813;&#26041;&#27861;&#22312;GLUE&#21644;SuperGLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;BitFit&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#32780;&#25512;&#29702;&#24320;&#38144;&#21364;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Ahead-of-Time &#65288;AoT&#65289;P-Tuning&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27599;&#20010;Transformer&#23618;&#20043;&#21069;&#28155;&#21152;&#36755;&#20837;&#30456;&#20851;&#30340;&#20559;&#32622;&#65292;&#20197;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;RoBERTa&#21644;DeBERTa&#27169;&#22411;&#22312;GLUE&#21644;SuperGLUE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;AoT P-Tuning&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;BitFit&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25928;&#29575;&#26356;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;AoT P-Tuning&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#35777;&#26126;&#23427;&#19982;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#24341;&#20837;&#30340;&#24320;&#38144;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#39592;&#24178;LM&#36827;&#34892;&#22810;&#20219;&#21153;&#25512;&#29702;&#65292;&#20174;&#32780;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Ahead-of-Time (AoT) P-Tuning, a novel parameter-efficient fine-tuning method for pre-trained Language Models (LMs) that adds input-dependent bias before each Transformer layer. We evaluate AoT P-Tuning on GLUE and SuperGLUE benchmarking datasets using RoBERTa and DeBERTa models, showing that it outperforms BitFit and is comparable or better than other baseline methods for efficient fine-tuning. Additionally, we assess the inference overhead of AoT P-Tuning and demonstrate that it introduces negligible overhead compared to established baseline methods. Our method enables multi-task inference with a single backbone LM, making it a practical solution for real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FastFit&#65292;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;STFT&#26367;&#25442;U-Net&#32534;&#30721;&#22120;&#30340;&#22768;&#38899;&#29983;&#25104;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21644;&#29983;&#25104;&#26102;&#38388;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#24182;&#22312;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#20004;&#20493;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10823</link><description>&lt;p&gt;
FastFit: &#29992;&#22810;&#20010;STFT&#26367;&#25442;U-Net&#32534;&#30721;&#22120;&#23454;&#29616;&#23454;&#26102;&#36845;&#20195;&#31070;&#32463;&#22768;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs. (arXiv:2305.10823v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FastFit&#65292;&#19968;&#31181;&#20351;&#29992;&#22810;&#20010;STFT&#26367;&#25442;U-Net&#32534;&#30721;&#22120;&#30340;&#22768;&#38899;&#29983;&#25104;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21644;&#29983;&#25104;&#26102;&#38388;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#24182;&#22312;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36817;&#20004;&#20493;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22768;&#30721;&#22120;&#32467;&#26500;FastFit&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#30701;&#26102;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;STFT&#65289;&#26367;&#25442;U-Net&#32534;&#30721;&#22120;&#65292;&#22312;&#19981;&#25439;&#22833;&#38899;&#39057;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#32534;&#30721;&#22120;&#22359;&#37117;&#26367;&#25442;&#20026;&#19968;&#20010;STFT&#65292;&#20854;&#21442;&#25968;&#31561;&#20110;&#27599;&#20010;&#35299;&#30721;&#22120;&#22359;&#30340;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#24418;&#25104;&#36339;&#36291;&#36830;&#25509;&#12290;FastFit&#20960;&#20046;&#23558;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#21644;&#29983;&#25104;&#26102;&#38388;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#20445;&#30495;&#24230;&#12290;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#20445;&#25345;&#39640;&#38899;&#39057;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#22522;&#20934;&#36845;&#20195;&#22768;&#30721;&#22120;&#36817;&#20004;&#20493;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;FastFit&#22312;&#25991;&#26412;&#36716;&#35821;&#38899;&#35780;&#20272;&#22330;&#26223;&#20013;&#65292;&#21253;&#25324;&#22810;&#35828;&#35805;&#20154;&#21644;&#38646;&#26679;&#26412;&#25991;&#26412;&#36716;&#35821;&#38899;&#20013;&#65292;&#20135;&#29983;&#31867;&#20284;&#20110;&#20854;&#20182;&#22522;&#32447;&#30340;&#38899;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents FastFit, a novel neural vocoder architecture that replaces the U-Net encoder with multiple short-time Fourier transforms (STFTs) to achieve faster generation rates without sacrificing sample quality. We replaced each encoder block with an STFT, with parameters equal to the temporal resolution of each decoder block, leading to the skip connection. FastFit reduces the number of parameters and the generation time of the model by almost half while maintaining high fidelity. Through objective and subjective evaluations, we demonstrated that the proposed model achieves nearly twice the generation speed of baseline iteration-based vocoders while maintaining high sound quality. We further showed that FastFit produces sound qualities similar to those of other baselines in text-to-speech evaluation scenarios, including multi-speaker and zero-shot text-to-speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;X-Ray Microbeam&#25968;&#25454;&#38598;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;&#22810;&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65292;&#36827;&#32780;&#25913;&#36827;&#20102;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10775</link><description>&lt;p&gt;
&#37319;&#29992;X&#23556;&#32447;&#24494;&#26463;&#25968;&#25454;&#20960;&#20309;&#21464;&#25442;&#22686;&#24378;&#35821;&#38899;&#21457;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset. (arXiv:2305.10775v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;X-Ray Microbeam&#25968;&#25454;&#38598;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;&#22810;&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65292;&#36827;&#32780;&#25913;&#36827;&#20102;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20998;&#26512;&#35821;&#38899;&#21457;&#38899;&#23545;&#20110;&#35821;&#38899;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22768;&#38376;&#30340;X-Y&#22352;&#26631;&#20005;&#37325;&#20381;&#36182;&#20110;&#21457;&#35328;&#32773;&#30340;&#35299;&#21078;&#32467;&#26500;&#21644;&#39063;&#31890;&#20301;&#32622;&#30340;&#21487;&#21464;&#24615;&#65292;&#29616;&#26377;&#30340;X&#23556;&#32447;&#24494;&#26463;&#25968;&#25454;&#38598;&#65288;XRMB&#65289;&#20013;&#30340;&#35299;&#21078;&#26631;&#24535;&#29289;&#26144;&#23556;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#21457;&#38899;&#36947;&#30340;&#25972;&#20010;&#35299;&#21078;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#65292;&#25913;&#36827;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21464;&#25442;&#23558;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;6&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65306;&#21767;&#32541;&#24352;&#24230;&#65288;LA&#65289;&#12289;&#21767;&#37096;&#31361;&#20986;&#65288;LP&#65289;&#12289;&#33292;&#20307;&#25910;&#32553;&#20301;&#32622;&#65288;TTCL&#65289;&#12289;&#24230;&#25968;&#65288;TBCD&#65289;&#12289;&#33292;&#23574;&#25910;&#32553;&#20301;&#32622;&#65288;TTCL&#65289;&#21644;&#24230;&#25968;&#65288;TTCD&#65289;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#36129;&#29486;&#26159;&#23558;&#33133;&#26495;&#36861;&#36394;&#24310;&#20280;&#21040;&#25512;&#27979;&#30340;&#21693;&#21897;&#21069;&#32447;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#33292;&#20307;&#25910;&#32553;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and the variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue Body Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL) and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction.
&lt;/p&gt;</description></item><item><title>Seq-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#26426;&#21046;&#65292;&#36991;&#20813;&#20102;&#30001;&#21333;&#20010;&#21521;&#37327;&#33410;&#28857;&#34920;&#31034;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;</title><link>http://arxiv.org/abs/2305.10771</link><description>&lt;p&gt;
Seq-HGNN: &#23398;&#20064;&#24322;&#26500;&#22270;&#19978;&#30340;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Seq-HGNN: Learning Sequential Node Representation on Heterogeneous Graph. (arXiv:2305.10771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10771
&lt;/p&gt;
&lt;p&gt;
Seq-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#26426;&#21046;&#65292;&#36991;&#20813;&#20102;&#30001;&#21333;&#20010;&#21521;&#37327;&#33410;&#28857;&#34920;&#31034;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#20013;&#65292;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#24471;&#21040;&#20102;&#36805;&#36895;&#21457;&#23637;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;HGNN&#35774;&#35745;&#20102;&#22810;&#31181;&#37327;&#36523;&#23450;&#21046;&#30340;&#22270;&#21367;&#31215;&#26469;&#25429;&#33719;&#24322;&#26500;&#22270;&#20013;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;HGNN&#36890;&#24120;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#22810;&#23618;&#22270;&#21367;&#31215;&#35745;&#31639;&#20013;&#30340;&#21333;&#20010;&#21521;&#37327;&#65292;&#36825;&#20351;&#24471;&#39640;&#23618;&#22270;&#21367;&#31215;&#23618;&#26080;&#27861;&#21306;&#20998;&#26469;&#33258;&#19981;&#21516;&#20851;&#31995;&#21644;&#19981;&#21516;&#39034;&#24207;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20449;&#24687;&#20256;&#36882;&#20013;&#30340;&#20449;&#24687;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#21363;Seq-HGNN&#65292;&#23427;&#20855;&#26377;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#12290;&#20026;&#20102;&#36991;&#20813;&#30001;&#21333;&#20010;&#21521;&#37327;&#33410;&#28857;&#34920;&#31034;&#24341;&#36215;&#30340;&#20449;&#24687;&#20002;&#22833;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#24207;&#21015;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#33410;&#28857;&#20449;&#24687;&#20256;&#36882;&#26399;&#38388;&#23558;&#27599;&#20010;&#33410;&#28857;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#20803;&#36335;&#24452;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the rapid development of heterogeneous graph neural networks (HGNNs) in information retrieval (IR) applications. Many existing HGNNs design a variety of tailor-made graph convolutions to capture structural and semantic information in heterogeneous graphs. However, existing HGNNs usually represent each node as a single vector in the multi-layer graph convolution calculation, which makes the high-level graph convolution layer fail to distinguish information from different relations and different orders, resulting in the information loss in the message passing. %insufficient mining of information. To this end, we propose a novel heterogeneous graph neural network with sequential node representation, namely Seq-HGNN. To avoid the information loss caused by the single vector node representation, we first design a sequential node representation learning mechanism to represent each node as a sequence of meta-path representations during the node message passing. The
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10769</link><description>&lt;p&gt;
&#36861;&#36214;&#33976;&#39311;&#65306;&#21152;&#36895;&#37319;&#26679;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36890;&#24120;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#26102;&#26679;&#26412;&#21512;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;&#20256;&#32479;&#30340;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21152;&#36895;&#37319;&#26679;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#31163;&#25955;&#26102;&#38388;&#27493;&#39588;&#22330;&#26223;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#35838;&#31243;&#25165;&#33021;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36861;&#36214;&#33976;&#39311;&#65288;CUD&#65289;&#65292;&#23427;&#40723;&#21169;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#8220;&#36861;&#36214;&#8221;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CUD&#35843;&#25972;&#20102;&#21407;&#22987;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#20351;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21644;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#23545;&#40784;&#65292;&#21033;&#29992;&#22522;&#20110;&#40857;&#26684;-&#24211;&#22612;&#30340;&#22810;&#27493;&#23545;&#40784;&#33976;&#39311;&#36827;&#34892;&#31934;&#30830;&#30340;ODE&#20272;&#35745;&#65292;&#21516;&#26102;&#38450;&#27490;&#24322;&#27493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#32500;&#31649;&#36947;&#24067;&#23616;&#33258;&#21160;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23436;&#25104;&#20219;&#21153;&#65292;&#24182;&#30830;&#20445;&#39640;&#36136;&#37327;&#24067;&#23616;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10760</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31649;&#36947;&#24067;&#23616;&#33258;&#21160;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Design Method of Building Pipeline Layout Based on Deep Reinforcement Learning. (arXiv:2305.10760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#19977;&#32500;&#31649;&#36947;&#24067;&#23616;&#33258;&#21160;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23436;&#25104;&#20219;&#21153;&#65292;&#24182;&#30830;&#20445;&#39640;&#36136;&#37327;&#24067;&#23616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31649;&#36947;&#24067;&#23616;&#35774;&#35745;&#26159;&#24314;&#31569;&#34892;&#19994;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;&#30446;&#21069;&#65292;&#31649;&#36947;&#24067;&#23616;&#26159;&#30001;&#24037;&#31243;&#24072;&#25163;&#21160;&#35774;&#35745;&#30340;&#65292;&#36825;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#33258;&#21160;&#21270;&#21644;&#31616;&#21270;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#20943;&#36731;&#24037;&#31243;&#24072;&#30340;&#36127;&#25285;&#24182;&#33410;&#30465;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#29983;&#25104;&#19977;&#32500;&#31649;&#36947;&#24067;&#23616;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#31354;&#38388;&#30340;&#20960;&#20309;&#29305;&#24449;&#25277;&#35937;&#20986;&#26469;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#29615;&#22659;&#65292;&#24182;&#26681;&#25454;&#19977;&#20010;&#32422;&#26463;&#65288;&#31649;&#36947;&#38271;&#24230;&#12289;&#24367;&#22836;&#21644;&#23433;&#35013;&#36317;&#31163;&#65289;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#25910;&#38598;&#25968;&#25454;&#24182;&#35757;&#32451;DRL&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;DRL&#27169;&#22411;&#26469;&#33258;&#21160;&#35774;&#35745;&#21333;&#20010;&#31649;&#36947;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRL&#27169;&#22411;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#23436;&#25104;&#31354;&#38388;&#31649;&#36947;&#24067;&#23616;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#39640;&#36136;&#37327;&#30340;&#24067;&#23616;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The layout design of pipelines is a critical task in the construction industry. Currently, pipeline layout is designed manually by engineers, which is time-consuming and laborious. Automating and streamlining this process can reduce the burden on engineers and save time. In this paper, we propose a method for generating three-dimensional layout of pipelines based on deep reinforcement learning (DRL). Firstly, we abstract the geometric features of space to establish a training environment and define reward functions based on three constraints: pipeline length, elbow, and installation distance. Next, we collect data through interactions between the agent and the environment and train the DRL model. Finally, we use the well-trained DRL model to automatically design a single pipeline. Our results demonstrate that DRL models can complete the pipeline layout task in space in a much shorter time than traditional algorithms while ensuring high-quality layout outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;GNN-to-MLP&#33976;&#39311;&#26694;&#26550;&#65292;&#23558;GNNs&#20013;&#30340;&#20302;/&#39640;&#39057;&#30693;&#35782;&#27880;&#20837;MLP&#12290;&#36890;&#36807;&#23558;GNNs&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20998;&#35299;&#20026;&#20302;/&#39640;&#39057;&#25104;&#20998;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#25512;&#23548;&#23427;&#20204;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#35299;&#20915;&#20102;&#29616;&#26377;GNN-to-MLP&#33976;&#39311;&#20013;&#30340;&#20449;&#24687;&#28153;&#27809;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10758</link><description>&lt;p&gt;
&#20174;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#25552;&#21462;&#20302;/&#39640;&#39057;&#30693;&#35782;&#27880;&#20837;MLP&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;GNN-to-MLP&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework. (arXiv:2305.10758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;GNN-to-MLP&#33976;&#39311;&#26694;&#26550;&#65292;&#23558;GNNs&#20013;&#30340;&#20302;/&#39640;&#39057;&#30693;&#35782;&#27880;&#20837;MLP&#12290;&#36890;&#36807;&#23558;GNNs&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20998;&#35299;&#20026;&#20302;/&#39640;&#39057;&#25104;&#20998;&#65292;&#22312;&#31354;&#38388;&#22495;&#20013;&#25512;&#23548;&#23427;&#20204;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36824;&#35299;&#20915;&#20102;&#29616;&#26377;GNN-to-MLP&#33976;&#39311;&#20013;&#30340;&#20449;&#24687;&#28153;&#27809;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22788;&#29702;&#19982;&#22270;&#30456;&#20851;&#30340;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#21487;&#23454;&#29616;&#30340;&#25512;&#26029;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;MLPs&#20173;&#28982;&#26159;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#30340;&#20027;&#21147;&#20891;&#12290;&#20026;&#20102;&#32553;&#23567;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#31934;&#24515;&#35774;&#35745;&#30340;&#25945;&#24072;GNN&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#23398;&#29983;MLP&#20013;&#65292;&#36825;&#34987;&#31216;&#20026;GNN-to-MLP&#33976;&#39311;&#12290;&#20294;&#26159;&#65292;&#33976;&#39311;&#30340;&#36807;&#31243;&#36890;&#24120;&#20250;&#23548;&#33268;&#20449;&#24687;&#25439;&#22833;&#65292;&#8220;&#21738;&#20123;GNN&#30340;&#30693;&#35782;&#27169;&#24335;&#26356;&#21487;&#33021;&#20250;&#34987;&#20445;&#30041;&#24182;&#33976;&#39311;&#21040;MLP&#20013;&#65311;&#8221;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#39057;&#35889;&#22495;&#20013;&#23558;GNNs&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#20998;&#35299;&#20026;&#20302;/&#39640;&#39057;&#25104;&#20998;&#65292;&#28982;&#21518;&#25512;&#23548;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#29616;&#26377;GNN-to-MLP&#33976;&#39311;&#23384;&#22312;&#28508;&#22312;&#20449;&#24687;&#28153;&#27809;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;GNNs&#30340;&#39640;&#39057;&#30693;&#35782;&#21487;&#33021;&#34987;&#20302;&#39057;&#30693;&#35782;&#25152;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the great success of Graph Neural Networks (GNNs) in handling graph-related tasks. However, MLPs remain the primary workhorse for practical industrial applications due to their desirable inference efficiency and scalability. To reduce their gaps, one can directly distill knowledge from a well-designed teacher GNN to a student MLP, which is termed as GNN-to-MLP distillation. However, the process of distillation usually entails a loss of information, and ``which knowledge patterns of GNNs are more likely to be left and distilled into MLPs?" becomes an important question. In this paper, we first factorize the knowledge learned by GNNs into low- and high-frequency components in the spectral domain and then derive their correspondence in the spatial domain. Furthermore, we identified a potential information drowning problem for existing GNN-to-MLP distillation, i.e., the high-frequency knowledge of the pre-trained GNNs may be overwhelmed by the low-frequency know
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10748</link><description>&lt;p&gt;
&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#29702;&#35299;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Physics Inspired Approaches Towards Understanding Gaussian Processes. (arXiv:2305.10748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#20998;&#26512;&#20102;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#25552;&#20986;&#20102;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#957;&#20351;&#24471;&#24615;&#33021;&#26356;&#20339;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;GP&#38598;&#25104;&#25928;&#26524;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#25439;&#22833;&#39046;&#22495;&#30340;&#29289;&#29702;&#23646;&#24615;&#30340;&#25237;&#31080;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20869;&#26680;&#21487;&#20197;&#23558;&#20808;&#39564;&#26377;&#20851;&#28508;&#22312;&#20989;&#25968;&#30340;&#20449;&#24565;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;(GP)&#20013;&#20197;&#24418;&#25104;&#24402;&#32435;&#20559;&#32622;&#65292;&#20294;&#38500;&#20102;&#20869;&#26680;&#36873;&#25321;&#22806;&#65292;GP&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#24456;&#38590;&#29702;&#35299;&#12290;&#26412;&#25991;&#21033;&#29992;&#29289;&#29702;&#23398;&#26041;&#27861;&#23545;GP&#27169;&#22411;&#30340;&#25439;&#22833;&#26223;&#35266;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#28436;&#31034;&#20102;Matern&#20869;&#26680;&#30340;&#957;&#36830;&#32493;&#24615;&#65292;&#24182;&#27010;&#36848;&#20102;&#26799;&#24230;&#22330;&#20851;&#38190;&#28857;&#30340;&#28798;&#21464;&#29702;&#35770;&#26041;&#38754;&#12290;&#36890;&#36807;&#23558;&#957;&#30452;&#25509;&#21253;&#21547;&#22312;Matern&#20869;&#26680;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#25991;&#29486;&#20013;&#957;&#30340;&#20856;&#22411;&#20540;&#22686;&#21152;&#20102;&#35745;&#31639;&#36895;&#24230;&#65292;&#20294;&#20854;&#22312;&#24615;&#33021;&#26041;&#38754;&#36828;&#38750;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#20107;&#20808;&#35780;&#20272;GP&#38598;&#21512;&#25928;&#26524;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#20110;&#25439;&#22833;&#26223;&#35266;&#29289;&#29702;&#23646;&#24615;&#30340;&#21508;&#31181;&#25237;&#31080;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#22312;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#23545;GP&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#20026;&#36229;&#21442;&#25968;&#20248;&#21270;&#21644;&#27169;&#22411;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior beliefs about the latent function to shape inductive biases can be incorporated into a Gaussian Process (GP) via the kernel. However, beyond kernel choices, the decision-making process of GP models remains poorly understood. In this work, we contribute an analysis of the loss landscape for GP models using methods from physics. We demonstrate $\nu$-continuity for Matern kernels and outline aspects of catastrophe theory at critical points in the loss landscape. By directly including $\nu$ in the hyperparameter optimisation for Matern kernels, we find that typical values of $\nu$ are far from optimal in terms of performance, yet prevail in the literature due to the increased computational speed. We also provide an a priori method for evaluating the effect of GP ensembles and discuss various voting approaches based on physical properties of the loss landscape. The utility of these approaches is demonstrated for various synthetic and real datasets. Our findings provide an enhanced und
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .</title><link>http://arxiv.org/abs/2305.10744</link><description>&lt;p&gt;
&#38754;&#21521;&#21095;&#38598;&#24335;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Resource Allocation in Episodic Markov Decision Processes. (arXiv:2305.10744v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#38271;&#26399;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21095;&#38598;&#24335;MDP&#27169;&#22411;&#65292;&#25552;&#20986;&#22312;&#38754;&#20020;&#36716;&#25442;&#21644;&#22870;&#21169;&#29305;&#24615;&#19981;&#30830;&#23450;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#26696;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#25552;&#20379;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#20854;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ .
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#38271;&#26399;&#30340;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#23427;&#38656;&#35201;&#22312;&#22810;&#20010;&#26102;&#38388;&#27573;&#20869;&#36827;&#34892;&#22810;&#38454;&#27573;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21095;&#38598;&#24335;&#26377;&#38480;&#26102;&#38388;&#27573;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#65292;&#20854;&#20013;&#36716;&#25442;&#21644;&#22870;&#21169;&#20197;&#21450;&#27599;&#19968;&#27425;&#30340;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#37117;&#26159;&#38750;&#23450;&#24577;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31561;&#25928;&#30340;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#37325;&#26500;&#26041;&#27861;&#65292;&#22522;&#20110;&#21344;&#26377;&#24230;&#37327;&#65292;&#20026;&#27492;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36164;&#28304;&#20998;&#37197;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#22788;&#29702;&#20102;&#22312;&#20272;&#31639;&#30495;&#23454;&#21487;&#34892;&#38598;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#35823;&#24046;&#65292;&#36825;&#26159;&#30456;&#23545;&#29420;&#31435;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23545;&#20110;&#38543;&#26426;&#22870;&#21169;&#21644;&#36164;&#28304;&#28040;&#32791;&#20989;&#25968;&#65292;&#22312;&#32447;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#30340;&#26399;&#26395;&#36951;&#25022;&#21463;&#21040;&#30028;&#38480;&#32422;&#26463;&#65292;&#20854;&#30028;&#38480;&#21463;&#21040; $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ &#30340;&#32422;&#26463;&#65292;&#20854;&#20013; $\rho\in(0,1)$ &#26159;&#39044;&#31639;&#21442;&#25968;&#65292;$H$ &#26159;&#22320;&#24179;&#32447;&#38271;&#24230;&#65292;$S$ &#21644; $A$ &#26159;. . .
&lt;/p&gt;
&lt;p&gt;
This paper studies a long-term resource allocation problem over multiple periods where each period requires a multi-stage decision-making process. We formulate the problem as an online resource allocation problem in an episodic finite-horizon Markov decision process with unknown non-stationary transitions and stochastic non-stationary reward and resource consumption functions for each episode. We provide an equivalent online linear programming reformulation based on occupancy measures, for which we develop an online mirror descent algorithm. Our online dual mirror descent algorithm for resource allocation deals with uncertainties and errors in estimating the true feasible set, which is of independent interest. We prove that under stochastic reward and resource consumption functions, the expected regret of the online mirror descent algorithm is bounded by $O(\rho^{-1}{H^{3/2}}S\sqrt{AT})$ where $\rho\in(0,1)$ is the budget parameter, $H$ is the length of the horizon, $S$ and $A$ are the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BEBE&#30340;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1654&#23567;&#26102;&#30340;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#25968;&#25454;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#20316;&#32773;&#20351;&#29992;&#20102;&#21313;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10740</link><description>&lt;p&gt;
&#19968;&#31181;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#30340;&#22522;&#20934;&#65292;&#20351;&#29992;&#21160;&#29289;&#25658;&#24102;&#30340;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
A benchmark for computational analysis of animal behavior, using animal-borne tags. (arXiv:2305.10740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10740
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BEBE&#30340;&#21160;&#29289;&#34892;&#20026;&#35745;&#31639;&#20998;&#26512;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1654&#23567;&#26102;&#30340;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#25968;&#25454;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#65292;&#20316;&#32773;&#20351;&#29992;&#20102;&#21313;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#25658;&#24102;&#30340;&#20256;&#24863;&#22120;&#65288;&#8220;&#29983;&#29289;&#35760;&#24405;&#22120;&#8221;&#65289;&#21487;&#20197;&#35760;&#24405;&#19968;&#31995;&#21015;&#21160;&#21147;&#23398;&#21644;&#29615;&#22659;&#25968;&#25454;&#65292;&#25581;&#31034;&#21160;&#29289;&#29983;&#24577;&#29983;&#29702;&#23398;&#24182;&#25913;&#21892;&#20445;&#25252;&#24037;&#20316;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23545;&#20110;&#35299;&#37322;&#29983;&#29289;&#35760;&#24405;&#22120;&#35760;&#24405;&#30340;&#22823;&#37327;&#25968;&#25454;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#27809;&#26377;&#26631;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bio-logger Ethogram Benchmark&#65288;BEBE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#34892;&#20026;&#27880;&#37322;&#65292;&#26631;&#20934;&#21270;&#24314;&#27169;&#20219;&#21153;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;BEBE&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#12289;&#26368;&#20855;&#20998;&#31867;&#22810;&#26679;&#24615;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#36825;&#31181;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#20061;&#20010;&#20998;&#31867;&#21333;&#20803;&#20013;149&#20010;&#20010;&#20307;&#25910;&#38598;&#30340;1654&#23567;&#26102;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;BEBE&#19978;&#35780;&#20272;&#20102;&#21313;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#24037;&#20316;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#35780;&#20272;&#20195;&#30721;&#24050;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/earthspecies/BEBE&#65292;&#20197;&#20415;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animal-borne sensors ('bio-loggers') can record a suite of kinematic and environmental data, which can elucidate animal ecophysiology and improve conservation efforts. Machine learning techniques are useful for interpreting the large amounts of data recorded by bio-loggers, but there exists no standard for comparing the different machine learning techniques in this domain. To address this, we present the Bio-logger Ethogram Benchmark (BEBE), a collection of datasets with behavioral annotations, standardized modeling tasks, and evaluation metrics. BEBE is to date the largest, most taxonomically diverse, publicly available benchmark of this type, and includes 1654 hours of data collected from 149 individuals across nine taxa. We evaluate the performance of ten different machine learning methods on BEBE, and identify key challenges to be addressed in future work. Datasets, models, and evaluation code are made publicly available at https://github.com/earthspecies/BEBE, to enable community 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10738</link><description>&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36890;&#29992;&#26694;&#26550;TGC &#29992;&#20110; deep temporal graph clustering, &#35299;&#20915;&#20102;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#22788;&#29702;&#30340;&#38590;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#21160;&#24577;&#20449;&#24687;&#30340;&#32858;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102; TGC &#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#22270;&#32858;&#31867;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#22312;&#26080;&#30417;&#30563;&#22330;&#26223;&#19979;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36866;&#29992;&#20110;&#26102;&#38388;&#22270;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861; - &#21487;&#20197;&#25429;&#33719;&#20851;&#38190;&#30340;&#21160;&#24577;&#20132;&#20114;&#20449;&#24687;&#65292;&#24182;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#35768;&#22810;&#38754;&#21521;&#32858;&#31867;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#26102;&#38388;&#22270;&#21482;&#33021;&#20316;&#20026;&#38745;&#24577;&#22270;&#26469;&#22788;&#29702;&#12290;&#36825;&#19981;&#20165;&#23548;&#33268;&#20102;&#21160;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#20063;&#24341;&#21457;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#28040;&#32791;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TGC&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26102;&#38388;&#22270;&#28145;&#24230;&#32858;&#31867;&#65292;&#23427;&#35843;&#25972;&#20102;&#28145;&#24230;&#32858;&#31867;&#25216;&#26415;&#65288;&#32858;&#31867;&#20998;&#37197;&#20998;&#24067;&#21644;&#37051;&#25509;&#30697;&#38453;&#37325;&#26500;&#65289;&#65292;&#20197;&#36866;&#24212;&#26102;&#38388;&#22270;&#22522;&#20110;&#20132;&#20114;&#24207;&#21015;&#30340;&#25209;&#22788;&#29702;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#20960;&#20010;&#26041;&#38754;&#35752;&#35770;&#20102;&#26102;&#38388;&#22270;&#32858;&#31867;&#19982;&#29616;&#26377;&#38745;&#24577;&#22270;&#32858;&#31867;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;TGC&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
&lt;/p&gt;</description></item><item><title>FedMR&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#37325;&#32452;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#28151;&#27927;&#21644;&#37325;&#32452;&#27599;&#20010;&#23618;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#32531;&#35299;&#23376;&#20248;&#25110;&#26377;&#20559;&#23616;&#37096;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.10730</link><description>&lt;p&gt;
FedMR&#65306;&#22522;&#20110;&#27169;&#22411;&#37325;&#32452;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMR: Federated Learning via Model Recombination. (arXiv:2305.10730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10730
&lt;/p&gt;
&lt;p&gt;
FedMR&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#37325;&#32452;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#28151;&#27927;&#21644;&#37325;&#32452;&#27599;&#20010;&#23618;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#32531;&#35299;&#23376;&#20248;&#25110;&#26377;&#20559;&#23616;&#37096;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#24471;&#23458;&#25143;&#31471;&#22312;&#19981;&#27844;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#35757;&#32451;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20110;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMR&#65288;&#32852;&#37030;&#27169;&#22411;&#37325;&#32452;&#65289;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#26412;&#22320;&#27169;&#22411;&#30340;&#27599;&#20010;&#23618;&#36827;&#34892;&#28151;&#27927;&#21644;&#37325;&#32452;&#65292;&#22312;&#23458;&#25143;&#31471;&#36827;&#34892;&#23616;&#37096;&#35757;&#32451;&#26102;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#22810;&#26679;&#30340;&#21021;&#22987;&#27169;&#22411;&#30340;&#26032;&#27169;&#22411;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#23376;&#20248;&#25110;&#26377;&#20559;&#23616;&#37096;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Federated Learning (FL) enables global model training across clients without compromising their raw data, existing Federated Averaging (FedAvg)-based methods suffer from the problem of low inference performance, especially for unevenly distributed data among clients. This is mainly because i) FedAvg initializes client models with the same global models, which makes the local training hard to escape from the local search for optimal solutions; and ii) by averaging model parameters in a coarse manner, FedAvg eclipses the individual characteristics of local models. To address such issues that strongly limit the inference capability of FL, we propose a novel and effective FL paradigm named FedMR (Federated Model Recombination). Unlike conventional FedAvg-based methods, the cloud server of FedMR shuffles each layer of collected local models and recombines them to achieve new models for local training on clients. Due to the diversified initialization models for clients coupled with 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; Vision Transformer &#27169;&#22411;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807; 2:4 &#32467;&#26500;&#21270;&#31232;&#30095;&#21098;&#26525;&#21644;&#22522;&#20110;&#31232;&#30095;&#33976;&#39311;&#24863;&#30693;&#30340;&#37327;&#21270;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21152;&#36895;.</title><link>http://arxiv.org/abs/2305.10727</link><description>&lt;p&gt;
&#21033;&#29992;GPU&#21451;&#22909;&#30340;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#20248;&#21270;Vision Transformer
&lt;/p&gt;
&lt;p&gt;
Boost Vision Transformer with GPU-Friendly Sparsity and Quantization. (arXiv:2305.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545; Vision Transformer &#27169;&#22411;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#36890;&#36807; 2:4 &#32467;&#26500;&#21270;&#31232;&#30095;&#21098;&#26525;&#21644;&#22522;&#20110;&#31232;&#30095;&#33976;&#39311;&#24863;&#30693;&#30340;&#37327;&#21270;&#35757;&#32451;&#23454;&#29616;&#20102;GPU&#21152;&#36895;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#27169;&#22411;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20063;&#22791;&#21463;&#20851;&#27880;&#12290;&#30001;&#20110;&#20854;&#22534;&#21472;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#22359;&#65292;&#23558; Vision Transformer &#37096;&#32626;&#21040; GPU &#19978;&#21152;&#36895;&#36816;&#34892;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#30446;&#21069;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21387;&#32553;&#26041;&#26696;&#65292;&#26368;&#22823;&#38480;&#24230;&#21033;&#29992;&#20102;&#22522;&#20110; 2:4 &#32454;&#31890;&#24230;&#32467;&#26500;&#31232;&#30095;&#24615;&#21644;&#37327;&#21270;&#30340; GPU &#21451;&#22909;&#24615;&#12290;&#36890;&#36807; 2:4 &#32467;&#26500;&#21270;&#31232;&#30095;&#21098;&#26525;&#23558;&#19968;&#20010;&#21407;&#22987;&#30340;&#22823;&#27169;&#22411;&#21098;&#26525;&#25104;&#31232;&#30095;&#27169;&#22411;&#65292;&#21033;&#29992; FP16 &#25968;&#25454;&#31867;&#22411;&#23545;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#31232;&#30095;&#33976;&#39311;&#24863;&#30693;&#30340;&#37327;&#21270;&#35757;&#32451;&#23558;&#28014;&#28857;&#31232;&#30095;&#27169;&#22411;&#36827;&#19968;&#27493;&#37327;&#21270;&#20026;&#22266;&#23450;&#28857;&#27169;&#22411;&#65292;&#21033;&#29992;&#25972;&#25968;&#24352;&#37327;&#35745;&#31639;&#23545;&#20854;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#39069;&#22806;&#30340; 2:4 &#31232;&#30095;&#35745;&#31639;&#21152;&#36895;&#12290;&#22312;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#36807;&#31243;&#20013;&#20351;&#29992;&#28151;&#21512;&#31574;&#30053;&#30693;&#35782;&#33976;&#39311;&#12290;&#25152;&#25552;&#20986;&#30340;&#21387;&#32553;&#26041;&#26696;&#28789;&#27963;&#65292;&#25903;&#25345;&#30417;&#30563;&#24335;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer extends its success from the language to the vision domain. Because of the stacked self-attention and cross-attention blocks, the acceleration deployment of vision transformer on GPU hardware is challenging and also rarely studied. This paper thoroughly designs a compression scheme to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization. Specially, an original large model with dense weight parameters is first pruned into a sparse one by 2:4 structured pruning, which considers the GPU's acceleration of 2:4 structured sparse pattern with FP16 data type, then the floating-point sparse model is further quantized into a fixed-point one by sparse-distillation-aware quantization aware training, which considers GPU can provide an extra speedup of 2:4 sparse calculation with integer tensors. A mixed-strategy knowledge distillation is used during the pruning and quantization process. The proposed compression scheme is flexible to support superv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#32447;&#24615;&#26144;&#23556;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;RevIN&#21644;CI&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#32447;&#24615;&#26144;&#23556;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21608;&#26399;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.10721</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#32447;&#24615;&#26144;&#23556;&#30340;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping. (arXiv:2305.10721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#32447;&#24615;&#26144;&#23556;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#20986;&#20102;RevIN&#21644;CI&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#32447;&#24615;&#26144;&#23556;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21608;&#26399;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#26377;&#21508;&#31181;&#19987;&#38376;&#35774;&#35745;&#26469;&#25429;&#25417;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;&#22797;&#26434;&#30340;&#26550;&#26500;&#30456;&#27604;&#65292;&#21333;&#20010;&#32447;&#24615;&#23618;&#21487;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#24443;&#24213;&#30740;&#31350;&#20102;&#26368;&#36817;&#26041;&#27861;&#30340;&#20869;&#22312;&#26377;&#25928;&#24615;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#32467;&#35770;&#65306;1&#65289;&#32447;&#24615;&#26144;&#23556;&#23545;&#20110;&#20808;&#21069;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#65307;2&#65289;RevIN&#65288;&#21487;&#36870;&#35268;&#33539;&#21270;&#65289;&#21644;CI&#65288;&#36890;&#36947;&#29420;&#31435;&#65289;&#22312;&#25552;&#39640;&#24635;&#20307;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65307;3&#65289;&#24403;&#22686;&#21152;&#36755;&#20837;&#35270;&#37326;&#26102;&#65292;&#32447;&#24615;&#26144;&#23556;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#26102;&#38388;&#24207;&#21015;&#30340;&#21608;&#26399;&#29305;&#24449;&#65292;&#24182;&#20855;&#26377;&#23545;&#19981;&#21516;&#36890;&#36947;&#19981;&#21516;&#21608;&#26399;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#23454;&#39564;&#35299;&#37322;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#20195;&#30721;&#21487;&#22312;\url{https://git}&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting has gained significant attention in recent years. While there are various specialized designs for capturing temporal dependency, previous studies have demonstrated that a single linear layer can achieve competitive forecasting performance compared to other complex architectures. In this paper, we thoroughly investigate the intrinsic effectiveness of recent approaches and make three key observations: 1) linear mapping is critical to prior long-term time series forecasting efforts; 2) RevIN (reversible normalization) and CI (Channel Independent) play a vital role in improving overall forecasting performance; and 3) linear mapping can effectively capture periodic features in time series and has robustness for different periods across channels when increasing the input horizon. We provide theoretical and experimental explanations to support our findings and also discuss the limitations and future works. Our framework's code is available at \url{https://git
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#65288;DS-TS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#31361;&#28982;&#24615;&#21464;&#21270;&#21644;&#24179;&#28369;&#24615;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.10718</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#31283;&#24577;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discounted Thompson Sampling for Non-Stationary Bandit Problems. (arXiv:2305.10718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#65288;DS-TS&#65289;&#65292;&#21487;&#20197;&#35299;&#20915;&#31361;&#28982;&#24615;&#21464;&#21270;&#21644;&#24179;&#28369;&#24615;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38750;&#31283;&#24577;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#21463;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;NS-MAB&#36890;&#24120;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#24314;&#27169;&#65306;&#31361;&#28982;&#24615;&#21464;&#21270;&#21644;&#24179;&#28369;&#24615;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;&#25240;&#25187;&#27748;&#26222;&#26862;&#37319;&#26679;&#31639;&#27861;&#65288;DS-TS&#65289;&#20197;&#35299;&#20915;&#36825;&#20004;&#20010;&#38750;&#31283;&#24577;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23558;&#25240;&#25187;&#22240;&#23376;&#32435;&#20837;&#27748;&#26222;&#26862;&#37319;&#26679;&#26469;&#34987;&#21160;&#36866;&#24212;&#21464;&#21270;&#12290;DS-TS&#26041;&#27861;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20294;&#32570;&#20047;&#23545;&#36951;&#25022;&#19978;&#38480;&#30340;&#20998;&#26512;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#39640;&#26031;&#20808;&#39564;&#30340;DS-TS&#21487;&#20197;&#22312;&#31361;&#28982;&#24615;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36817;&#20046;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#38480;&#65288;$\tilde{O} (\sqrt {TB_T})$&#65289;&#65292;&#22312;&#24179;&#28369;&#24615;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616; $\tilde{O}(T^{\beta})$ &#30340;&#36817;&#20046;&#26368;&#20248;&#36951;&#25022;&#19978;&#38480;&#65292;&#20854;&#20013; $T$ &#26159;&#26102;&#38388;&#27493;&#25968;&#65292;$B_T$ &#26159;&#26029;&#28857;&#25968;&#65292;$\beta$ &#19982;&#25910;&#30410;&#20998;&#24067;&#30340;&#24179;&#28369;&#24615;&#26377;&#20851;&#65292;$\tilde{O}$ &#26159;&#23545;&#25968;&#36951;&#25022;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-stationary multi-armed bandit (NS-MAB) problems have recently received significant attention. NS-MAB are typically modelled in two scenarios: abruptly changing, where reward distributions remain constant for a certain period and change at unknown time steps, and smoothly changing, where reward distributions evolve smoothly based on unknown dynamics. In this paper, we propose Discounted Thompson Sampling (DS-TS) with Gaussian priors to address both non-stationary settings. Our algorithm passively adapts to changes by incorporating a discounted factor into Thompson Sampling. DS-TS method has been experimentally validated, but analysis of the regret upper bound is currently lacking. Under mild assumptions, we show that DS-TS with Gaussian priors can achieve nearly optimal regret bound on the order of $\tilde{O}(\sqrt{TB_T})$ for abruptly changing and $\tilde{O}(T^{\beta})$ for smoothly changing, where $T$ is the number of time steps, $B_T$ is the number of breakpoints, $\beta$ is asso
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10716</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Time-Series Pre-Trained Models. (arXiv:2305.10716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#26159;&#20027;&#35201;&#31867;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#21487;&#20197;&#20811;&#26381;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#21151;&#22320;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25366;&#25496;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#25104;&#26412;&#30340;&#21407;&#22240;&#65292;&#26500;&#24314;&#22823;&#35268;&#27169;&#12289;&#33391;&#22909;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#36880;&#28176;&#24341;&#36215;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;TS-PTMs&#65289;&#65292;&#26088;&#22312;&#25351;&#23548;&#20102;&#35299;&#12289;&#24212;&#29992;&#21644;&#30740;&#31350;TS-PTMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20808;&#31616;&#35201;&#20171;&#32461;&#20102;TSM&#20013;&#20351;&#29992;&#30340;&#20856;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#39044;&#35757;&#32451;&#25216;&#26415;&#27010;&#36848;&#20102;TS-PTMs&#12290;&#25105;&#20204;&#25506;&#35752;&#30340;&#20027;&#35201;&#31867;&#21035;&#21253;&#25324;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;TS-PTMs&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantage
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10713</link><description>&lt;p&gt;
&#24179;&#22374;&#24230;&#24863;&#30693;&#30340;Prompt&#36873;&#25321;&#33021;&#25552;&#39640;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#25552;&#31034;&#24050;&#25104;&#20026;&#35775;&#38382;&#23427;&#20204;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#36825;&#28608;&#21457;&#20102;&#33258;&#21160;&#36873;&#25321;&#26377;&#25928;&#35821;&#35328;&#25552;&#31034;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;Prompt&#24179;&#22374;&#24230;&#65292;&#19968;&#31181;&#37327;&#21270;&#35821;&#35328;&#25552;&#31034;&#39044;&#26399;&#25928;&#29992;&#30340;&#26032;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#21463;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#24179;&#22374;&#24230;&#27491;&#21017;&#21270;&#21551;&#21457;&#65292;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#21442;&#25968;&#25200;&#21160;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35813;&#24230;&#37327;&#30340;&#29702;&#35770;&#22522;&#30784;&#21450;&#20854;&#19982;&#20854;&#20182;Prompt&#36873;&#25321;&#24230;&#37327;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#26041;&#27861;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;Prompt&#24179;&#22374;&#24230;&#19982;&#29616;&#26377;&#24230;&#37327;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;6&#20010;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#20248;&#20110;&#20197;&#21069;&#30340;Prompt&#36873;&#25321;&#24230;&#37327;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;5&#65285;&#65292;Pearson&#30456;&#20851;&#24615;&#25552;&#39640;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#22238;&#24402;&#21644;X-PINNs&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#26410;&#30693;&#37096;&#20998;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10706</link><description>&lt;p&gt;
&#22522;&#20110;&#31526;&#21495;&#22238;&#24402;&#21644;eXtended&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#30340;&#28784;&#30418;&#23398;&#20064;&#21160;&#21147;&#23398;&#26041;&#31243;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework Based on Symbolic Regression Coupled with eXtended Physics-Informed Neural Networks for Gray-Box Learning of Equations of Motion from Data. (arXiv:2305.10706v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10706
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#22238;&#24402;&#21644;X-PINNs&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#26410;&#30693;&#37096;&#20998;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#21644;&#31639;&#27861;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#26041;&#31243;&#30340;&#26410;&#30693;&#37096;&#20998;&#12290;&#35813;&#26694;&#26550;&#22522;&#20110;eXtended&#29289;&#29702;&#21551;&#21457;&#31070;&#32463;&#32593;&#32476;&#65288;X-PINNs&#65289;&#12289;&#26102;&#31354;&#39046;&#22495;&#20998;&#35299;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#22312;&#39046;&#22495;&#30028;&#38754;&#19978;&#26045;&#21152;&#36890;&#37327;&#36830;&#32493;&#24615;&#26469;&#22686;&#24378;&#21407;&#22987;X-PINN&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#33879;&#21517;&#30340;Allen-Cahn&#26041;&#31243;&#26469;&#28436;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#37319;&#29992;Frobenius&#30697;&#38453;&#33539;&#25968;&#26469;&#35780;&#20272;X-PINN&#39044;&#27979;&#30340;&#31934;&#24230;&#65292;&#32467;&#26524;&#34920;&#29616;&#20986;&#26497;&#20339;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#37319;&#29992;&#31526;&#21495;&#22238;&#24402;&#20174;&#25968;&#25454;&#20013;&#30830;&#23450;&#26041;&#31243;&#30340;&#26410;&#30693;&#37096;&#20998;&#30340;&#23553;&#38381;&#24418;&#24335;&#65292;&#24182;&#19988;&#32467;&#26524;&#35777;&#23454;&#20102;&#22522;&#20110;X-PINNs&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#27979;&#35797;&#35813;&#26694;&#26550;&#22312;&#31867;&#20284;&#23454;&#38469;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#65292;&#23558;&#38543;&#26426;&#22122;&#22768;&#28155;&#21152;&#21040;&#25968;&#25454;&#38598;&#20013;&#20197;&#27169;&#25311;&#23384;&#22312;&#28909;&#22122;&#22768;&#25110;&#20202;&#22120;&#35823;&#24046;&#30340;&#24773;&#20917;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#23545;&#22823;&#37327;&#22122;&#22768;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a framework and an algorithm to uncover the unknown parts of nonlinear equations directly from data. The framework is based on eXtended Physics-Informed Neural Networks (X-PINNs), domain decomposition in space-time, but we augment the original X-PINN method by imposing flux continuity across the domain interfaces. The well-known Allen-Cahn equation is used to demonstrate the approach. The Frobenius matrix norm is used to evaluate the accuracy of the X-PINN predictions and the results show excellent performance. In addition, symbolic regression is employed to determine the closed form of the unknown part of the equation from the data, and the results confirm the accuracy of the X-PINNs based approach. To test the framework in a situation resembling real-world data, random noise is added to the datasets to mimic scenarios such as the presence of thermal noise or instrument errors. The results show that the framework is stable against significant amount of noise. As the final p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.10703</link><description>&lt;p&gt;
ReGen: &#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGen: Zero-Shot Text Classification via Training Data Generation with Progressive Dense Retrieval. (arXiv:2305.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28176;&#36827;&#24335;&#23494;&#38598;&#26816;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#65292;&#38646;&#26679;&#26412;&#23398;&#20064;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#35768;&#22810;&#20851;&#27880;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#25968;&#21313;&#20159;&#32423;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#20174;&#36890;&#29992;&#39046;&#22495;&#30340;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20013;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#23545;&#27604;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#31867;&#21035;&#25551;&#36848;&#24615;&#35805;&#35821;&#23398;&#20064;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#20197;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#65292;&#21363;&#23637;&#31034;&#22686;&#24378;&#30340;&#35805;&#35821;&#29983;&#25104;&#21644;&#33258;&#19968;&#33268;&#24615;&#24341;&#23548;&#36807;&#28388;&#65292;&#20197;&#25552;&#39640;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#35206;&#30422;&#29575;&#65292;&#21516;&#26102;&#21024;&#38500;&#22122;&#22768;&#26679;&#26412;&#12290;&#23545;&#20061;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;REGEN&#30456;&#36739;&#20110;&#26368;&#24378;&#30340;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;4.3%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#20351;&#29992;&#22823;&#22411;NLG&#27169;&#22411;&#30340;&#22522;&#32447;&#30456;&#27604;&#33410;&#30465;&#20102;&#32422;70&#65285;&#30340;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;REGEN&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of large language models (LLMs), zero-shot learning has attracted much attention for various NLP tasks. Different from prior works that generate training data with billion-scale natural language generation (NLG) models, we propose a retrieval-enhanced framework to create training data from a general-domain unlabeled corpus. To realize this, we first conduct contrastive pretraining to learn an unsupervised dense retriever for extracting the most relevant documents using class-descriptive verbalizers. We then further propose two simple strategies, namely Verbalizer Augmentation with Demonstrations and Self-consistency Guided Filtering to improve the topic coverage of the dataset while removing noisy examples. Experiments on nine datasets demonstrate that REGEN achieves 4.3% gain over the strongest baselines and saves around 70% of the time compared to baselines using large NLG models. Besides, REGEN can be naturally integrated with recently proposed large language mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#25968;&#25454;&#65292;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#29983;&#25104;SDE&#27169;&#22411;&#12290;&#31216;&#20043;&#20026;Dirchlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;&#12290;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#20005;&#26684;&#38480;&#21046;&#30340;&#26679;&#26412;&#65292;&#19988;&#36866;&#29992;&#20110;&#29983;&#25104;&#29983;&#29289;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.10699</link><description>&lt;p&gt;
&#29983;&#29289;&#24207;&#21015;&#29983;&#25104;&#30340;Dirichlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dirichlet Diffusion Score Model for Biological Sequence Generation. (arXiv:2305.10699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#31163;&#25955;&#25968;&#25454;&#65292;&#20351;&#29992;&#27010;&#29575;&#21333;&#32431;&#24418;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#36827;&#34892;&#24314;&#27169;&#30340;&#29983;&#25104;SDE&#27169;&#22411;&#12290;&#31216;&#20043;&#20026;Dirchlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;&#12290;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#20005;&#26684;&#38480;&#21046;&#30340;&#26679;&#26412;&#65292;&#19988;&#36866;&#29992;&#20110;&#29983;&#25104;&#29983;&#29289;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#29983;&#29289;&#24207;&#21015;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#38656;&#35201;&#28385;&#36275;&#22797;&#26434;&#30340;&#38480;&#21046;&#65292;&#22240;&#27492;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#35299;&#20915;&#26159;&#24456;&#33258;&#28982;&#30340;&#38382;&#39064;&#12290;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#27169;&#22411;&#26159;&#19968;&#31181;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#20855;&#26377;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#26368;&#21021;&#25552;&#20986;&#30340;SDE&#19981;&#26159;&#33258;&#28982;&#22320;&#29992;&#20110;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#12290;&#20026;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#31163;&#25955;&#25968;&#25454;&#65288;&#20363;&#22914;&#29983;&#29289;&#24207;&#21015;&#65289;&#30340;&#29983;&#25104;SDE&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#31354;&#38388;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#38543;&#26426;&#20998;&#24067;&#30340;&#24179;&#31283;&#20998;&#24067;&#26159;Dirichlet&#20998;&#24067;&#12290;&#36825;&#20351;&#24471;&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#36827;&#34892;&#25193;&#25955;&#23545;&#20110;&#24314;&#27169;&#31163;&#25955;&#25968;&#25454;&#26159;&#33258;&#28982;&#30340;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;Dirchlet&#25193;&#25955;&#20998;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#29420;&#29983;&#25104;&#20219;&#21153;&#35777;&#26126;&#20102;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#29983;&#25104;&#28385;&#36275;&#20005;&#26684;&#38480;&#21046;&#30340;&#26679;&#26412;&#12290;&#36825;&#31181;&#29983;&#25104;&#27169;&#22411;&#20063;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#29983;&#29289;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing biological sequences is an important challenge that requires satisfying complex constraints and thus is a natural problem to address with deep generative modeling. Diffusion generative models have achieved considerable success in many applications. Score-based generative stochastic differential equations (SDE) model is a continuous-time diffusion model framework that enjoys many benefits, but the originally proposed SDEs are not naturally designed for modeling discrete data. To develop generative SDE models for discrete data such as biological sequences, here we introduce a diffusion process defined in the probability simplex space with stationary distribution being the Dirichlet distribution. This makes diffusion in continuous space natural for modeling discrete data. We refer to this approach as Dirchlet diffusion score model. We demonstrate that this technique can generate samples that satisfy hard constraints using a Sudoku generation task. This generative model can also 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24322;&#26500;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#37030;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#35752;&#35770;&#20102;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#30340;&#32447;&#24615;&#21152;&#36895;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#31561;&#26435;&#37325;&#24179;&#22343;&#26412;&#22320;Q&#20272;&#35745;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2305.10697</link><description>&lt;p&gt;
&#24322;&#26500;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31119;&#38899;&#65306;&#32447;&#24615;&#21152;&#36895;&#21644;&#26356;&#22810;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Blessing of Heterogeneity in Federated Q-learning: Linear Speedup and Beyond. (arXiv:2305.10697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24322;&#26500;&#32676;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#32852;&#37030;Q&#23398;&#20064;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#65292;&#35752;&#35770;&#20102;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#30340;&#32447;&#24615;&#21152;&#36895;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#31561;&#26435;&#37325;&#24179;&#22343;&#26412;&#22320;Q&#20272;&#35745;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#25968;&#25454;&#30001;&#22810;&#20010;&#20195;&#29702;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#25910;&#38598;&#26102;&#65292;&#32852;&#37030;RL&#31639;&#27861;&#20801;&#35768;&#21327;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;&#26412;&#25991;&#32771;&#34385;&#32852;&#37030;Q&#23398;&#20064;&#65292;&#20854;&#30446;&#30340;&#26159;&#36890;&#36807;&#23450;&#26399;&#32858;&#21512;&#20165;&#22312;&#26412;&#22320;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#26412;&#22320;Q&#20272;&#35745;&#26469;&#23398;&#20064;&#26368;&#20248;Q&#20989;&#25968;&#12290;&#38024;&#23545;&#26080;&#38480;&#26102;&#38388;&#33976;&#39311;&#26631;&#35760;&#20915;&#31574;&#36807;&#31243;&#65292;&#25105;&#20204;&#20026;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#30340;&#32852;&#37030;Q&#23398;&#20064;&#25552;&#20379;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#30028;&#38480;&#23637;&#31034;&#20102;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#21152;&#36895;&#20197;&#21450;&#20854;&#20182;&#26174;&#33879;&#38382;&#39064;&#21442;&#25968;&#30340;&#26356;&#23574;&#38160;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;Q&#23398;&#20064;&#26041;&#27861;&#37319;&#29992;&#31561;&#26435;&#37325;&#24179;&#22343;&#26412;&#22320;Q&#20272;&#35745;&#65292;&#36825;&#22312;&#24322;&#27493;&#35774;&#32622;&#20013;&#21487;&#33021;&#20250;&#39640;&#24230;&#27425;&#20248;&#65292;&#22240;&#20026;&#30001;&#20110;&#19981;&#21516;&#30340;&#26412;&#22320;&#34892;&#20026;&#31574;&#30053;&#65292;&#26412;&#22320;&#36712;&#36857;&#21487;&#33021;&#39640;&#24230;&#24322;&#26500;&#12290;&#29616;&#26377;&#30340;&#26679;&#26412;&#26368;&#20248;&#21270;&#31574;&#30053;&#22312;&#24322;&#27493;&#35774;&#32622;&#20013;&#23384;&#22312;&#24040;&#22823;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
When the data used for reinforcement learning (RL) are collected by multiple agents in a distributed manner, federated versions of RL algorithms allow collaborative learning without the need of sharing local data. In this paper, we consider federated Q-learning, which aims to learn an optimal Q-function by periodically aggregating local Q-estimates trained on local data alone. Focusing on infinite-horizon tabular Markov decision processes, we provide sample complexity guarantees for both the synchronous and asynchronous variants of federated Q-learning. In both cases, our bounds exhibit a linear speedup with respect to the number of agents and sharper dependencies on other salient problem parameters. Moreover, existing approaches to federated Q-learning adopt an equally-weighted averaging of local Q-estimates, which can be highly sub-optimal in the asynchronous setting since the local trajectories can be highly heterogeneous due to different local behavior policies. Existing sample com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UnbiasedGBM&#26041;&#27861;&#20197;&#35299;&#20915;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#20013;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#20559;&#22686;&#30410;&#36827;&#34892;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10696</link><description>&lt;p&gt;
&#26080;&#20559;&#32622;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#26080;&#20559;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Unbiased Gradient Boosting Decision Tree with Unbiased Feature Importance. (arXiv:2305.10696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UnbiasedGBM&#26041;&#27861;&#20197;&#35299;&#20915;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#20013;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#20559;&#22686;&#30410;&#36827;&#34892;&#23545;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#20915;&#31574;&#26641;&#30340;&#26500;&#24314;&#36807;&#31243;&#26159;GBDT&#20013;&#26368;&#20851;&#38190;&#30340;&#32452;&#25104;&#37096;&#20998;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#38271;&#26399;&#20197;&#26469;&#65292;&#20915;&#31574;&#26641;&#31639;&#27861;&#19968;&#30452;&#22240;&#20854;&#23545;&#20855;&#26377;&#22823;&#37327;&#28508;&#22312;&#20998;&#35010;&#30340;&#29305;&#24449;&#30340;&#20559;&#35265;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#36825;&#31181;&#20559;&#35265;&#22312;GBDT&#20013;&#24341;&#20837;&#20102;&#20005;&#37325;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;GBDT&#30340;&#20559;&#24046;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#20559;&#24046;&#28304;&#20110;1&#65289;&#27599;&#20010;&#20998;&#35010;&#22686;&#30410;&#20272;&#35745;&#20013;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#21644;2&#65289;&#30001;&#20110;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#35780;&#20272;&#20998;&#35010;&#25913;&#36827;&#24182;&#30830;&#23450;&#26368;&#20339;&#20998;&#35010;&#32780;&#23548;&#33268;&#30340;&#20998;&#35010;&#21457;&#29616;&#31639;&#27861;&#20013;&#30340;&#20559;&#24046;&#12290;&#22522;&#20110;&#36825;&#31181;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#20559;&#22686;&#30410;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#20351;&#29992;&#34955;&#22806;&#26679;&#26412;&#36827;&#34892;&#27979;&#37327;&#30340;&#22686;&#30410;&#37325;&#35201;&#24615;&#30340;&#26080;&#20559;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#26080;&#20559;&#23646;&#24615;&#32435;&#20837;&#21040;&#20998;&#35010;&#21457;&#29616;&#31639;&#27861;&#20013;&#65292;&#24182;&#24320;&#21457;&#20102;UnbiasedGBM&#26469;&#35299;&#20915;GBDT&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient Boosting Decision Tree (GBDT) has achieved remarkable success in a wide variety of applications. The split finding algorithm, which determines the tree construction process, is one of the most crucial components of GBDT. However, the split finding algorithm has long been criticized for its bias towards features with a large number of potential splits. This bias introduces severe interpretability and overfitting issues in GBDT. To this end, we provide a fine-grained analysis of bias in GBDT and demonstrate that the bias originates from 1) the systematic bias in the gain estimation of each split and 2) the bias in the split finding algorithm resulting from the use of the same data to evaluate the split improvement and determine the best split. Based on the analysis, we propose unbiased gain, a new unbiased measurement of gain importance using out-of-bag samples. Moreover, we incorporate the unbiased property into the split finding algorithm and develop UnbiasedGBM to solve the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38376;&#25511;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#20154;&#24037;&#35774;&#35745;&#30340;&#22240;&#23376;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;2048&#32500;&#24230;&#31354;&#38388;&#20013;&#29983;&#25104;&#26356;&#26377;&#24847;&#20041;&#30340;&#22240;&#23376;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#23545;2,000&#21482;&#20013;&#22269;&#24066;&#22330;&#32929;&#31080;&#23454;&#39564;&#20013;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10693</link><description>&lt;p&gt;
&#38376;&#25511;&#28145;&#24230;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#22240;&#23376;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Gated Deeper Models are Effective Factor Learners. (arXiv:2305.10693v1 [q-fin.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38376;&#25511;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#20154;&#24037;&#35774;&#35745;&#30340;&#22240;&#23376;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#22312;2048&#32500;&#24230;&#31354;&#38388;&#20013;&#29983;&#25104;&#26356;&#26377;&#24847;&#20041;&#30340;&#22240;&#23376;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#23545;2,000&#21482;&#20013;&#22269;&#24066;&#22330;&#32929;&#31080;&#23454;&#39564;&#20013;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#36164;&#20135;&#65288;&#20363;&#22914;&#29305;&#26031;&#25289;&#32929;&#31080;&#65289;&#30340;&#36229;&#39069;&#22238;&#25253;&#23545;&#20110;&#25152;&#26377;&#25237;&#36164;&#32773;&#37117;&#26377;&#30410;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#20154;&#31867;&#34892;&#20026;&#24433;&#21709;&#30340;&#24066;&#22330;&#21160;&#24577;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#20351;&#36825;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#30740;&#31350;&#32773;&#25163;&#24037;&#21046;&#23450;&#20102;&#19968;&#20123;&#22240;&#23376;&#20316;&#20026;&#20449;&#21495;&#26469;&#25351;&#23548;&#20182;&#20204;&#30340;&#25237;&#36164;&#36807;&#31243;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#30475;&#24453;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19982;&#36825;&#20123;&#20154;&#24037;&#35774;&#35745;&#30340;&#22240;&#23376;&#30456;&#32467;&#21512;&#65292;&#39044;&#27979;&#36229;&#39069;&#22238;&#25253;&#30340;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20116;&#23618;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;2048&#32500;&#24230;&#31354;&#38388;&#20013;&#29983;&#25104;&#26356;&#26377;&#24847;&#20041;&#30340;&#22240;&#23376;&#12290;&#29616;&#20195;&#32593;&#32476;&#35774;&#35745;&#25216;&#26415;&#34987;&#29992;&#26469;&#22686;&#24378;&#40065;&#26834;&#24615;&#35757;&#32451;&#24182;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38376;&#25511;&#32593;&#32476;&#65292;&#21160;&#24577;&#36807;&#28388;&#22122;&#22768;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20013;&#22269;&#24066;&#22330;&#30340;&#36817;&#19977;&#24180;&#35760;&#24405;&#20013;&#30340;2,000&#21482;&#32929;&#31080;&#36827;&#34892;&#20102;&#27169;&#22411;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Precisely forecasting the excess returns of an asset (e.g., Tesla stock) is beneficial to all investors. However, the unpredictability of market dynamics, influenced by human behaviors, makes this a challenging task. In prior research, researcher have manually crafted among of factors as signals to guide their investing process. In contrast, this paper view this problem in a different perspective that we align deep learning model to combine those human designed factors to predict the trend of excess returns. To this end, we present a 5-layer deep neural network that generates more meaningful factors in a 2048-dimensional space. Modern network design techniques are utilized to enhance robustness training and reduce overfitting. Additionally, we propose a gated network that dynamically filters out noise-learned features, resulting in improved performance. We evaluate our model over 2,000 stocks from the China market with their recent three years records. The experimental results show tha
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#21435;&#22122;&#25193;&#25955;&#26159;&#19968;&#31181;&#38543;&#26426;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25968;&#27493;&#39588;&#20869;&#20174; Ising &#27169;&#22411;&#30340; Gibbs &#27979;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10690</link><description>&lt;p&gt;
&#37319;&#26679;&#65292;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Sampling, Diffusions, and Stochastic Localization. (arXiv:2305.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10690
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#21435;&#22122;&#25193;&#25955;&#26159;&#19968;&#31181;&#38543;&#26426;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25968;&#27493;&#39588;&#20869;&#20174; Ising &#27169;&#22411;&#30340; Gibbs &#27979;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#26159;&#20174;&#39640;&#32500;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#25104;&#21151;&#25216;&#26415;&#65292;&#21487;&#20197;&#26126;&#30830;&#32473;&#20986;&#25110;&#20174;&#26679;&#26412;&#38598;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#31471;&#28857;&#26159;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#28418;&#31227;&#36890;&#24120;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#38543;&#26426;&#23450;&#20301;&#26159;&#22312;&#39640;&#32500;&#20013;&#35777;&#26126;&#39532;&#23572;&#31185;&#22827;&#38142;&#21644;&#20854;&#20182;&#20989;&#25968;&#19981;&#31561;&#24335;&#28151;&#21512;&#30340;&#25104;&#21151;&#25216;&#26415;&#12290;[EAMS2022]&#20013;&#24341;&#20837;&#20102;&#38543;&#26426;&#23450;&#20301;&#30340;&#31639;&#27861;&#29256;&#26412;&#65292;&#20197;&#33719;&#24471;&#20174;&#26576;&#20123;&#32479;&#35745;&#21147;&#23398;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#26377;&#19977;&#20010;&#30446;&#26631;&#65306;&#65288;i&#65289;&#23558;[EAMS2022]&#30340;&#26500;&#36896;&#25512;&#24191;&#21040;&#20854;&#20182;&#38543;&#26426;&#23450;&#20301;&#36807;&#31243;&#65307;&#65288;ii&#65289;&#28548;&#28165;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#21435;&#22122;&#25193;&#25955;&#26159;&#38543;&#26426;&#23450;&#20301;&#65292;&#20294;&#20854;&#20182;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#35270;&#35282;&#33258;&#28982;&#25552;&#20986;&#30340;&#31034;&#20363;&#65307;&#65288;iii&#65289;&#25551;&#36848;&#20174;&#36825;&#31181;&#32852;&#31995;&#20013;&#24471;&#20986;&#30340;&#19968;&#20123;&#35265;&#35299;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23545;&#25968;&#27493;&#39588;&#20869;&#20174; Ising &#27169;&#22411;&#30340; Gibbs &#27979;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusions are a successful technique to sample from high-dimensional distributions can be either explicitly given or learnt from a collection of samples. They implement a diffusion process whose endpoint is a sample from the target distribution and whose drift is typically represented as a neural network. Stochastic localization is a successful technique to prove mixing of Markov Chains and other functional inequalities in high dimension. An algorithmic version of stochastic localization was introduced in [EAMS2022], to obtain an algorithm that samples from certain statistical mechanics models.  This notes have three objectives: (i) Generalize the construction [EAMS2022] to other stochastic localization processes; (ii) Clarify the connection between diffusions and stochastic localization. In particular we show that standard denoising diffusions are stochastic localizations but other examples that are naturally suggested by the proposed viewpoint; (iii) Describe some insights that foll
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#30446;&#26631;&#25915;&#20987;&#26041;&#24335;&#65292;&#21363;&#36890;&#36807;&#22870;&#21169;&#27602;&#21270;&#25915;&#20987;&#26469;&#24178;&#25200;&#35757;&#32451;&#36807;&#31243;&#65292;&#25915;&#20987;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#19988;&#25915;&#20987;&#39044;&#31639;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;DRL&#29615;&#22659;&#21644;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.10681</link><description>&lt;p&gt;
&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#30446;&#26631;&#22870;&#21169;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Black-Box Targeted Reward Poisoning Attack Against Online Deep Reinforcement Learning. (arXiv:2305.10681v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#30446;&#26631;&#25915;&#20987;&#26041;&#24335;&#65292;&#21363;&#36890;&#36807;&#22870;&#21169;&#27602;&#21270;&#25915;&#20987;&#26469;&#24178;&#25200;&#35757;&#32451;&#36807;&#31243;&#65292;&#25915;&#20987;&#20855;&#26377;&#36890;&#29992;&#24615;&#65292;&#19988;&#25915;&#20987;&#39044;&#31639;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25915;&#20987;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;DRL&#29615;&#22659;&#21644;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#40657;&#30418;&#30446;&#26631;&#25915;&#20987;&#65292;&#36890;&#36807;&#22870;&#21169;&#27602;&#21270;&#25915;&#20987;&#26469;&#24178;&#25200;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#36866;&#29992;&#20110;&#26410;&#30693;&#31639;&#27861;&#23398;&#20064;&#30340;&#36890;&#29992;&#29615;&#22659;&#65292;&#24182;&#19988;&#38656;&#35201;&#26377;&#38480;&#30340;&#25915;&#20987;&#39044;&#31639;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#21033;&#29992;&#36890;&#29992;&#26694;&#26550;&#24182;&#25214;&#21040;&#20445;&#35777;&#22312;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#33324;&#20551;&#35774;&#19979;&#25915;&#20987;&#39640;&#25928;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#26368;&#20248;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#39044;&#31639;&#19979;&#39640;&#25928;&#22320;&#23558;&#23398;&#20064;&#20195;&#29702;&#24341;&#23548;&#21040;&#21508;&#31181;&#30446;&#26631;&#31574;&#30053;&#19979;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#27969;&#34892;&#30340;DRL&#29615;&#22659;&#21644;&#26368;&#20808;&#36827;&#30340;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first black-box targeted attack against online deep reinforcement learning through reward poisoning during training time. Our attack is applicable to general environments with unknown dynamics learned by unknown algorithms and requires limited attack budgets and computational resources. We leverage a general framework and find conditions to ensure efficient attack under a general assumption of the learning algorithms. We show that our attack is optimal in our framework under the conditions. We experimentally verify that with limited budgets, our attack efficiently leads the learning agent to various target policies under a diverse set of popular DRL environments and state-of-the-art learners.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;&#26694;&#26550;&#65292;&#21517;&#20026;STEP&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21160;&#24577;&#22270;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10673</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#21160;&#24577;&#22270;&#30340;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs. (arXiv:2305.10673v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;&#26694;&#26550;&#65292;&#21517;&#20026;STEP&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#21160;&#24577;&#22270;&#30340;&#35757;&#32451;&#21644;&#37096;&#32626;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#19981;&#38656;&#35201;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22270;&#30340;&#26222;&#21450;&#22312;&#35757;&#32451;&#21644;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#38754;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#26102;&#38388;&#21644;&#23384;&#20648;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#21407;&#22987;&#22270;&#21098;&#26525;&#25104;&#19968;&#20010;&#23567;&#32780;&#39640;&#24230;&#20449;&#24687;&#21270;&#30340;&#22270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#24471;&#23545;&#20462;&#21098;&#21518;&#30340;&#22270;&#21644;&#22823;&#22270;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#32463;&#39564;&#26377;&#25928;&#65292;&#20294;&#24403;&#21069;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#38745;&#24577;&#25110;&#38750;&#26102;&#38388;&#22270;&#65292;&#36825;&#20123;&#22270;&#23545;&#21160;&#24577;&#22330;&#26223;&#30340;&#30452;&#25509;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38656;&#35201;&#26631;&#31614;&#20316;&#20026;&#22522;&#26412;&#20107;&#23454;&#26469;&#23398;&#20064;&#20449;&#24687;&#32467;&#26500;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26631;&#31614;&#38590;&#20197;&#33719;&#24471;&#30340;&#26032;&#38382;&#39064;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26080;&#30417;&#30563;&#22270;&#21098;&#26525;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;STEP&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#30417;&#30563;&#30340;&#26102;&#38388;&#21098;&#26525;&#26694;&#26550;&#65292;&#23427;&#23398;&#20064;&#20174;&#36755;&#20837;&#30340;&#21160;&#24577;&#22270;&#20013;&#21435;&#38500;&#28508;&#22312;&#20887;&#20313;&#30340;&#36793;&#32536;&#12290;&#20174;&#25216;&#26415;&#21644;&#24037;&#19994;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20811;&#26381;&#20102;&#29616;&#26377;&#21160;&#24577;&#22270;&#21098;&#26525;&#25216;&#26415;&#30340;&#38480;&#21046;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20197;&#21098;&#26525;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10668</link><description>&lt;p&gt;
MetaGAD&#65306;&#23398;&#20064;&#20803;&#36716;&#31227;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection. (arXiv:2305.10668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MetaGAD&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#20174;&#26080;&#26631;&#35760;&#33410;&#28857;&#21040;&#26377;&#26631;&#35760;&#33410;&#28857;&#20043;&#38388;&#30340;&#20803;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#21508;&#20010;&#39046;&#22495;&#20449;&#24687;&#23433;&#20840;&#38382;&#39064;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#22914;&#37329;&#34701;&#27450;&#35784;&#12289;&#31038;&#20250;&#22403;&#22334;&#37038;&#20214;&#12289;&#32593;&#32476;&#20837;&#20405;&#31561;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#25191;&#34892;&#30340;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#24448;&#24448;&#22826;&#26114;&#36149;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26377;&#20851;&#24322;&#24120;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#21487;&#33021;&#20250;&#23558;&#34987;&#35782;&#21035;&#30340;&#24322;&#24120;&#35270;&#20026;&#25968;&#25454;&#22122;&#22768;&#25110;&#19981;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#36890;&#24120;&#21487;&#33719;&#21462;&#26377;&#38480;&#30340;&#26631;&#35760;&#24322;&#24120;&#65292;&#36825;&#20123;&#26631;&#35760;&#24322;&#24120;&#20855;&#26377;&#25512;&#36827;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25506;&#32034;&#23569;&#37327;&#26631;&#35760;&#24322;&#24120;&#21644;&#22823;&#37327;&#26080;&#26631;&#35760;&#33410;&#28857;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#24037;&#20316;&#30456;&#24403;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#22270;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;MetaGAD&#65292;&#23398;&#20064;&#20803;&#36716;&#31227;&#30693;&#35782;&#26469;&#36827;&#34892;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#23454;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.10664</link><description>&lt;p&gt;
&#26435;&#37325;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26080;&#38480;&#23485;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance. (arXiv:2305.10664v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#36827;&#34892;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;Neal&#65288;1996&#65289;&#30340;&#32463;&#20856;&#32780;&#26377;&#24433;&#21709;&#21147;&#30340;&#20316;&#21697;&#24050;&#30693;&#65292;&#20855;&#26377;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26080;&#38480;&#23485;&#24230;&#26631;&#24230;&#26497;&#38480;&#26159;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#65292;&#24403;&#32593;&#32476;&#26435;&#37325;&#20855;&#26377;&#26377;&#30028;&#20808;&#39564;&#26041;&#24046;&#26102;&#12290;Neal&#30340;&#32467;&#26524;&#24050;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#20010;&#38544;&#34255;&#23618;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#32476;&#65292;&#20063;&#20855;&#26377;&#39640;&#26031;&#36807;&#31243;&#26631;&#24230;&#26497;&#38480;&#12290;&#39640;&#26031;&#36807;&#31243;&#30340;&#26131;&#22788;&#29702;&#23646;&#24615;&#20801;&#35768;&#30452;&#25509;&#30340;&#21518;&#39564;&#25512;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#30456;&#27604;&#26377;&#38480;&#23485;&#24230;&#30340;&#32593;&#32476;&#65292;&#26497;&#22823;&#22320;&#31616;&#21270;&#20102;&#26497;&#38480;&#36807;&#31243;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#32463;&#20856;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#22833;&#25928;&#65292;&#25454;&#36866;&#24403;&#26465;&#20214;&#19979;&#30340;&#31283;&#23450;$\alpha$&#36807;&#31243;&#30340;&#26631;&#24230;&#26497;&#38480;&#30340;&#25991;&#29486;&#36739;&#22810;&#30340;&#26159;&#21069;&#21521;&#27169;&#25311;&#65292;&#32780;&#22312;&#36825;&#20123;&#26435;&#37325;&#19979;&#30340;&#21518;&#39564;&#25512;&#26029;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#20855;&#26377;&#26080;&#30028;&#26041;&#24046;&#26435;&#37325;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21518;&#39564;&#25512;&#26029;&#30340;&#26032;&#29702;&#35770;&#27934;&#23519;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#39564;&#25910;&#32553;&#36895;&#29575;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#21518;&#39564;&#20998;&#24067;&#38598;&#20013;&#22312;&#20855;&#26377;&#38750;&#26631;&#20934;&#36229;&#21442;&#25968;&#20381;&#36182;&#24615;&#30340;&#31232;&#30095;&#20419;&#36827;&#21644;&#22343;&#20540;&#25910;&#32553;&#20808;&#39564;&#21608;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, \emph{when the network weights have bounded prior variance}. Neal's result has been extended to networks with multiple hidden layers and to convolutional neural networks, also with Gaussian process scaling limits. The tractable properties of Gaussian processes then allow straightforward posterior inference and uncertainty quantification, considerably simplifying the study of the limit process compared to a network of finite width. Neural network weights with unbounded variance, however, pose unique challenges. In this case, the classical central limit theorem breaks down and it is well known that the scaling limit is an $\alpha$-stable process under suitable conditions. However, current literature is primarily limited to forward simulations under these processes and the problem of posterior inference under s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#30340;&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.10659</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#38556;&#30861;&#31243;&#24230;&#36827;&#34892;&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Use of Speech Impairment Severity for Dysarthric Speech Recognition. (arXiv:2305.10659v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#21644;&#35828;&#35805;&#20154;&#36523;&#20221;&#30340;&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#21507;&#30151;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#22240;&#32032;&#19982;&#35828;&#35805;&#20154;&#36523;&#20221;&#31561;&#22240;&#32032;&#25152;&#23548;&#33268;&#30340;&#35828;&#35805;&#20154;&#23618;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#20351;&#29992;&#35828;&#35805;&#20154;&#36523;&#20221;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#25216;&#26415;&#65292;&#21363;&#65306;a&#65289;&#22810;&#20219;&#21153;&#35757;&#32451;&#65292;&#21253;&#25324;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#39044;&#27979;&#35823;&#24046;&#65307;b&#65289;&#20197;&#35828;&#35805;&#20154;-&#38556;&#30861;&#31243;&#24230;&#20026;&#37325;&#28857;&#30340;&#36741;&#21161;&#29305;&#24449;&#35843;&#25972;&#65307;c&#65289;&#20165;&#38024;&#23545;&#35828;&#35805;&#20154;&#36523;&#20221;&#21644;&#38556;&#30861;&#31243;&#24230;&#36827;&#34892;&#30340;&#32467;&#26500;&#21270;LHUC&#21464;&#25442;&#12290;&#22312;UASpeech&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#39069;&#22806;&#30340;&#35821;&#38899;&#38556;&#30861;&#31243;&#24230;&#32435;&#20837;&#26368;&#20808;&#36827;&#30340;&#28151;&#21512;DNN&#12289;E2E Conformer&#21644;&#39044;&#35757;&#32451;&#30340;Wav2vec 2.0 ASR&#31995;&#32479;&#20013;&#65292;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#23383;&#38169;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#65292;&#26368;&#39640;&#21487;&#36798;4.78%&#65288;&#30456;&#23545;&#20110;14.03%&#30340;WER&#38477;&#20302;&#65289;&#12290;&#20351;&#29992;&#26368;&#20339;&#31995;&#32479;&#65292;&#22312;UASpeech&#19978;&#21487;&#20197;&#33719;&#24471;&#24050;&#21457;&#24067;&#30340;&#26368;&#20302;WER&#20026;17.82%&#65288;&#23545;&#20110;&#38750;&#24120;&#20302;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#20026;51.25%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity associated factors such as gender, and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech.
&lt;/p&gt;</description></item><item><title>DeepEdit&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#21160;&#21644;&#20132;&#20114;&#24335;&#20998;&#21106;&#65292;&#20801;&#35768;&#38750;&#24120;&#24555;&#36895;&#30340;&#25968;&#25454;&#38598;&#20998;&#21106;&#65292;&#19988;&#36890;&#36807;&#29992;&#25143;&#20132;&#20114;&#65288;&#22914;&#28857;&#20987;&#65289;&#21487;&#20197;&#36827;&#19968;&#27493;&#20248;&#21270;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10655</link><description>&lt;p&gt;
DeepEdit&#65306;&#22522;&#20110;&#28145;&#24230;&#21487;&#32534;&#36753;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#20132;&#20114;&#24335;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
DeepEdit: Deep Editable Learning for Interactive Segmentation of 3D Medical Images. (arXiv:2305.10655v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10655
&lt;/p&gt;
&lt;p&gt;
DeepEdit&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21307;&#23398;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#21160;&#21644;&#20132;&#20114;&#24335;&#20998;&#21106;&#65292;&#20801;&#35768;&#38750;&#24120;&#24555;&#36895;&#30340;&#25968;&#25454;&#38598;&#20998;&#21106;&#65292;&#19988;&#36890;&#36807;&#29992;&#25143;&#20132;&#20114;&#65288;&#22914;&#28857;&#20987;&#65289;&#21487;&#20197;&#36827;&#19968;&#27493;&#20248;&#21270;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#33258;&#21160;&#20998;&#21106;&#26159;&#35786;&#26029;&#21644;&#20171;&#20837;&#20219;&#21153;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#22270;&#20687;&#65292;&#36825;&#23545;&#19987;&#23478;&#26631;&#27880;&#32773;&#26469;&#35828;&#21487;&#33021;&#26159;&#28902;&#29712;&#19988;&#32791;&#26102;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepEdit&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#27880;&#37322;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#33258;&#21160;&#21644;&#21322;&#33258;&#21160;&#20998;&#21106;&#65292;&#24182;&#36890;&#36807;&#28857;&#20987;-based&#20462;&#27491;&#36827;&#34892;&#20114;&#21160;&#20998;&#21106;&#12290;DeepEdit&#23558;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65306;&#38750;&#20132;&#20114;&#24335;&#65288;&#21363;&#20351;&#29992;nnU-Net&#12289;UNET&#25110;UNETR&#36827;&#34892;&#33258;&#21160;&#20998;&#21106;&#65289;&#21644;&#20132;&#20114;&#24335;&#20998;&#21106;&#26041;&#27861;&#65288;&#21363;DeepGrow&#65289;&#65292;&#21512;&#24182;&#20026;&#19968;&#20010;&#21333;&#19968;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#23427;&#20801;&#35768;&#36731;&#26494;&#38598;&#25104;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25490;&#21517;&#31574;&#30053;&#65288;&#21363;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#35745;&#31639;&#65289;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#22521;&#35757;&#21644;&#29992;&#25143;&#20132;&#20114;&#20223;&#30495;&#26469;&#35757;&#32451;DeepEdit&#12290;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#21307;&#29983;&#21487;&#20197;&#20351;&#29992;&#31639;&#27861;&#23558;&#20307;&#31215;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#21322;&#33258;&#21160;&#20998;&#21106;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#20132;&#20114;&#65288;&#22914;&#28857;&#20987;&#65289;&#26469;&#20248;&#21270;&#20998;&#21106;&#32467;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepEdit&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#26174;&#30528;&#20943;&#23569;&#20102;&#19987;&#23478;&#26631;&#27880;&#32773;&#30340;&#27880;&#37322;&#26102;&#38388;&#21644;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic segmentation of medical images is a key step for diagnostic and interventional tasks. However, achieving this requires large amounts of annotated volumes, which can be tedious and time-consuming task for expert annotators. In this paper, we introduce DeepEdit, a deep learning-based method for volumetric medical image annotation, that allows automatic and semi-automatic segmentation, and click-based refinement. DeepEdit combines the power of two methods: a non-interactive (i.e. automatic segmentation using nnU-Net, UNET or UNETR) and an interactive segmentation method (i.e. DeepGrow), into a single deep learning model. It allows easy integration of uncertainty-based ranking strategies (i.e. aleatoric and epistemic uncertainty computation) and active learning. We propose and implement a method for training DeepEdit by using standard training combined with user interaction simulation. Once trained, DeepEdit allows clinicians to quickly segment their datasets by using the algorit
&lt;/p&gt;</description></item><item><title>STREAMLINE&#26159;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20999;&#29255;&#35782;&#21035;&#12289;&#20999;&#29255;&#24863;&#30693;&#39044;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#19977;&#27493;&#27969;&#31243;&#65292;&#32531;&#35299;&#20102;&#24037;&#20316;&#26631;&#35760;&#25968;&#25454;&#20013;&#22330;&#26223;&#39537;&#21160;&#30340;&#20999;&#29255;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10643</link><description>&lt;p&gt;
STREAMLINE: &#38754;&#21521;&#30495;&#23454;&#22810;&#20998;&#24067;&#22330;&#26223;&#30340;&#27969;&#24335;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STREAMLINE: Streaming Active Learning for Realistic Multi-Distributional Settings. (arXiv:2305.10643v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10643
&lt;/p&gt;
&lt;p&gt;
STREAMLINE&#26159;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20999;&#29255;&#35782;&#21035;&#12289;&#20999;&#29255;&#24863;&#30693;&#39044;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#19977;&#27493;&#27969;&#31243;&#65292;&#32531;&#35299;&#20102;&#24037;&#20316;&#26631;&#35760;&#25968;&#25454;&#20013;&#22330;&#26223;&#39537;&#21160;&#30340;&#20999;&#29255;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#12289;&#21355;&#26143;&#25104;&#20687;&#31561;&#22810;&#20010;&#23454;&#38469;&#29992;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#37327;&#24050;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#26080;&#20559;&#24046;&#27169;&#22411;&#21462;&#20915;&#20110;&#24314;&#31435;&#20195;&#34920;&#32473;&#23450;&#20219;&#21153;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#35768;&#22810;&#29615;&#22659;&#20013;&#65292;&#25968;&#25454;&#26469;&#33258;&#39640;&#27969;&#37327;&#27969;&#65292;&#27599;&#31181;&#24773;&#20917;&#20986;&#29616;&#22312;&#38543;&#26426;&#20132;&#38169;&#30340;&#24773;&#20917;&#19979;&#65292;&#39057;&#29575;&#21508;&#24322;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#23454;&#38469;&#30340;&#27969;&#24335;&#22330;&#26223;&#65292;&#20854;&#20013;&#25968;&#25454;&#23454;&#20363;&#20197;&#21450;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#27969;&#20013;&#21462;&#26679;&#12290;&#21033;&#29992;&#23376;&#27169;&#20449;&#24687;&#37327;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27969;&#24335;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;STREAMLINE&#65292;&#36890;&#36807;&#20999;&#29255;&#35782;&#21035;&#12289;&#20999;&#29255;&#24863;&#30693;&#39044;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#19977;&#27493;&#27969;&#31243;&#65292;&#32531;&#35299;&#20102;&#24037;&#20316;&#26631;&#35760;&#25968;&#25454;&#20013;&#22330;&#26223;&#39537;&#21160;&#30340;&#20999;&#29255;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;STREAMLINE&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#27969;&#24335;&#22330;&#26223;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have consistently shown great performance in several real-world use cases like autonomous vehicles, satellite imaging, etc., effectively leveraging large corpora of labeled training data. However, learning unbiased models depends on building a dataset that is representative of a diverse range of realistic scenarios for a given task. This is challenging in many settings where data comes from high-volume streams, with each scenario occurring in random interleaved episodes at varying frequencies. We study realistic streaming settings where data instances arrive in and are sampled from an episodic multi-distributional data stream. Using submodular information measures, we propose STREAMLINE, a novel streaming active learning framework that mitigates scenario-driven slice imbalance in the working labeled data via a three-step procedure of slice identification, slice-aware budgeting, and data selection. We extensively evaluate STREAMLINE on real-world streaming scenarios
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10638</link><description>&lt;p&gt;
&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#36827;&#34892;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Incremental Causal Graph Learning for Online Unsupervised Root Cause Analysis. (arXiv:2305.10638v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#26080;&#30417;&#30563;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;&#35813;&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;&#27169;&#22411;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#35302;&#21457;&#28857;&#26816;&#27979;&#65292;&#22686;&#37327;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#30340;&#20219;&#21153;&#26159;&#20998;&#26512;&#31995;&#32479;&#30417;&#25511;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#31995;&#32479;&#25925;&#38556;/&#22833;&#25928;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#26377;&#25928;&#30340;RCA&#21487;&#20197;&#22823;&#22823;&#21152;&#36895;&#31995;&#32479;&#25925;&#38556;&#24674;&#22797;&#65292;&#24182;&#20943;&#36731;&#31995;&#32479;&#25439;&#22833;&#25110;&#36130;&#21153;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#24320;&#21457;&#31163;&#32447;RCA&#31639;&#27861;&#19978;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#25163;&#21160;&#21551;&#21160;RCA&#36807;&#31243;&#65292;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#25968;&#25454;&#26469;&#35757;&#32451;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#26032;&#30340;&#31995;&#32479;&#25925;&#38556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CORAL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;RCA&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#35302;&#21457;RCA&#36807;&#31243;&#24182;&#22686;&#37327;&#26356;&#26032;RCA&#27169;&#22411;&#12290;CORAL&#21253;&#25324;&#35302;&#21457;&#28857;&#26816;&#27979;&#12289;&#22686;&#37327;&#35299;&#32544;&#22240;&#26524;&#22270;&#23398;&#20064;&#21644;&#22522;&#20110;&#32593;&#32476;&#20256;&#25773;&#30340;&#26681;&#26412;&#21407;&#22240;&#23450;&#20301;&#12290;&#35302;&#21457;&#28857;&#26816;&#27979;&#32452;&#20214;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#31995;&#32479;&#29366;&#24577;&#36716;&#25442;&#24182;&#36827;&#34892;&#20934;&#23454;&#26102;&#26816;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;m&#30340;&#22312;&#32447;&#35302;&#21457;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of root cause analysis (RCA) is to identify the root causes of system faults/failures by analyzing system monitoring data. Efficient RCA can greatly accelerate system failure recovery and mitigate system damages or financial losses. However, previous research has mostly focused on developing offline RCA algorithms, which often require manually initiating the RCA process, a significant amount of time and data to train a robust model, and then being retrained from scratch for a new system fault.  In this paper, we propose CORAL, a novel online RCA framework that can automatically trigger the RCA process and incrementally update the RCA model. CORAL consists of Trigger Point Detection, Incremental Disentangled Causal Graph Learning, and Network Propagation-based Root Cause Localization. The Trigger Point Detection component aims to detect system state transitions automatically and in near-real-time. To achieve this, we develop an online trigger point detection approach based on m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#27861;(AUMP-SVGD)&#26469;&#24212;&#23545; Stein Variational Gradient Descent (SVGD)&#26041;&#27861;&#30340;&#26041;&#24046;&#23849;&#28291;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;SVGD&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10636</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#27861;
&lt;/p&gt;
&lt;p&gt;
Augmented Message Passing Stein Variational Gradient Descent. (arXiv:2305.10636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#28040;&#24687;&#20256;&#36882;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#27861;(AUMP-SVGD)&#26469;&#24212;&#23545; Stein Variational Gradient Descent (SVGD)&#26041;&#27861;&#30340;&#26041;&#24046;&#23849;&#28291;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;SVGD&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD)&#26159;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#22522;&#20110;&#31890;&#23376;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#25910;&#25947;&#24615;&#36973;&#21463;&#26041;&#24046;&#23849;&#28291;&#30340;&#24433;&#21709;&#65292;&#36825;&#20250;&#38477;&#20302;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25910;&#25947;&#36807;&#31243;&#20013;&#26377;&#38480;&#31890;&#23376;&#30340;&#31561;&#21521;&#24615;&#23646;&#24615;&#65292;&#34920;&#26126;&#26377;&#38480;&#31890;&#23376;&#30340;SVGD&#26080;&#27861;&#22312;&#25972;&#20010;&#26679;&#26412;&#31354;&#38388;&#20013;&#20256;&#25773;&#12290;&#30456;&#21453;&#65292;&#25152;&#26377;&#31890;&#23376;&#20542;&#21521;&#20110;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#32858;&#38598;&#22312;&#31890;&#23376;&#20013;&#24515;&#21608;&#22260;&#65292;&#24182;&#19988;&#25105;&#20204;&#25552;&#20379;&#20102;&#27492;&#32858;&#31867;&#30340;&#20998;&#26512;&#30028;&#38480;&#12290;&#20026;&#36827;&#19968;&#27493;&#25913;&#21892;SVGD&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#27861;(AUMP-SVGD)&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#20004;&#38454;&#27573;&#20248;&#21270;&#36807;&#31243;&#65292;&#19981;&#38656;&#35201;&#30446;&#26631;&#20998;&#24067;&#30340;&#31232;&#30095;&#24615;&#65292;&#19981;&#20687;MP-SVGD&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#26041;&#24046;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stein Variational Gradient Descent (SVGD) is a popular particle-based method for Bayesian inference. However, its convergence suffers from the variance collapse, which reduces the accuracy and diversity of the estimation. In this paper, we study the isotropy property of finite particles during the convergence process and show that SVGD of finite particles cannot spread across the entire sample space. Instead, all particles tend to cluster around the particle center within a certain range and we provide an analytical bound for this cluster. To further improve the effectiveness of SVGD for high-dimensional problems, we propose the Augmented Message Passing SVGD (AUMP-SVGD) method, which is a two-stage optimization procedure that does not require sparsity of the target distribution, unlike the MP-SVGD method. Our algorithm achieves satisfactory accuracy and overcomes the variance collapse problem in various benchmark problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#35268;&#27169;&#32479;&#35745;&#35774;&#32622;&#20013;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;&#21450;&#20854;&#38543;&#26426;&#29256;&#26412;&#30340;&#24615;&#33021;&#21644;&#38750;&#20809;&#28369;&#29256;&#26412;&#30340;&#20462;&#25913;&#39640;&#26031;&#29275;&#39039;&#25110;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#30340;&#23545;&#27604;&#34920;&#29616;&#65292;&#24182;&#21246;&#21202;&#20986;&#22312;&#32479;&#35745;&#22122;&#22768;&#19979;&#20462;&#25913;&#39640;&#26031;&#29275;&#39039;&#27861;&#30340;&#20108;&#27425;&#25910;&#25947;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.10634</link><description>&lt;p&gt;
&#22312;&#22122;&#22768;&#19979;&#20462;&#25913;&#30340;&#39640;&#26031;&#29275;&#39039;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modified Gauss-Newton Algorithms under Noise. (arXiv:2305.10634v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#35268;&#27169;&#32479;&#35745;&#35774;&#32622;&#20013;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;&#21450;&#20854;&#38543;&#26426;&#29256;&#26412;&#30340;&#24615;&#33021;&#21644;&#38750;&#20809;&#28369;&#29256;&#26412;&#30340;&#20462;&#25913;&#39640;&#26031;&#29275;&#39039;&#25110;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#30340;&#23545;&#27604;&#34920;&#29616;&#65292;&#24182;&#21246;&#21202;&#20986;&#22312;&#32479;&#35745;&#22122;&#22768;&#19979;&#20462;&#25913;&#39640;&#26031;&#29275;&#39039;&#27861;&#30340;&#20108;&#27425;&#25910;&#25947;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;&#21450;&#20854;&#38543;&#26426;&#29256;&#26412;&#24050;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#12290;&#23427;&#20204;&#30340;&#38750;&#20809;&#28369;&#29256;&#26412;&#65292;&#20462;&#25913;&#30340;&#39640;&#26031;&#29275;&#39039;&#25110;&#36817;&#31471;&#32447;&#24615;&#31639;&#27861;&#65292;&#22312;&#22823;&#35268;&#27169;&#32479;&#35745;&#35774;&#32622;&#20013;&#30456;&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;&#21487;&#20197;&#23548;&#33268;&#25130;&#28982;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20004;&#31867;&#31639;&#27861;&#22312;&#29702;&#35770;&#19978;&#23545;&#19968;&#20010;&#29702;&#24819;&#21270;&#30340;&#32479;&#35745;&#31034;&#20363;&#21644;&#23454;&#39564;&#20013;&#23398;&#20064;&#38382;&#39064;&#65288;&#21253;&#25324;&#32467;&#26500;&#21270;&#39044;&#27979;&#65289;&#30340;&#25130;&#28982;&#19981;&#21516;&#30340;&#34920;&#29616;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#21246;&#21202;&#20986;&#22312;&#32479;&#35745;&#22122;&#22768;&#19979;&#27963;&#36291;&#30340;&#20462;&#25913;&#39640;&#26031;&#29275;&#39039;&#27861;&#30340;&#20108;&#27425;&#25910;&#25947;&#21306;&#22495;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#38543;&#26426;&#65288;&#27425;&#65289;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;&#38750;&#20809;&#28369;&#22797;&#21512;&#30446;&#26631;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gauss-Newton methods and their stochastic version have been widely used in machine learning and signal processing. Their nonsmooth counterparts, modified Gauss-Newton or prox-linear algorithms, can lead to contrasting outcomes when compared to gradient descent in large-scale statistical settings. We explore the contrasting performance of these two classes of algorithms in theory on a stylized statistical example, and experimentally on learning problems including structured prediction. In theory, we delineate the regime where the quadratic convergence of the modified Gauss-Newton method is active under statistical noise. In the experiments, we underline the versatility of stochastic (sub)-gradient descent to minimize nonsmooth composite objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#28369;&#21270;&#30340;&#25439;&#22833;&#26469;&#20248;&#21270;&#22312;&#32447;SGD&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#20351;&#29992;$n \gtrsim d^{k^\star/2}$&#20010;&#26679;&#26412;&#23398;&#20064;&#21333;&#25351;&#25968;&#27169;&#22411;$w^\star$&#65292;&#24182;&#19982;&#24352;&#37327;PCA&#21644;&#23567;&#25209;&#37327;SGD&#30340;&#27491;&#21017;&#21270;&#25928;&#24212;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.10633</link><description>&lt;p&gt;
&#24179;&#28369;&#21270;&#39118;&#26223;&#21487;&#25552;&#21319;SGD&#20449;&#21495;&#65306;&#23398;&#20064;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Smoothing the Landscape Boosts the Signal for SGD: Optimal Sample Complexity for Learning Single Index Models. (arXiv:2305.10633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#24179;&#28369;&#21270;&#30340;&#25439;&#22833;&#26469;&#20248;&#21270;&#22312;&#32447;SGD&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#20351;&#29992;$n \gtrsim d^{k^\star/2}$&#20010;&#26679;&#26412;&#23398;&#20064;&#21333;&#25351;&#25968;&#27169;&#22411;$w^\star$&#65292;&#24182;&#19982;&#24352;&#37327;PCA&#21644;&#23567;&#25209;&#37327;SGD&#30340;&#27491;&#21017;&#21270;&#25928;&#24212;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$d$&#32500;&#24230;&#19978;&#20351;&#29992;&#21508;&#21521;&#21516;&#24615;&#39640;&#26031;&#20998;&#24067;&#26469;&#23398;&#20064;&#21333;&#25351;&#25968;&#27169;&#22411;$\sigma(w^\star \cdot x)$&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;$w^\star$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#26159;&#30001;&#38142;&#25509;&#20989;&#25968;$\sigma$&#30340;&#20449;&#24687;&#25351;&#25968;$k^\star$&#25152;&#20915;&#23450;&#30340;&#65292;&#23427;&#34987;&#23450;&#20041;&#20026;$\sigma$&#30340;&#31532;&#19968;&#20010;&#38750;&#38646;Hermite&#31995;&#25968;&#30340;&#25351;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#22522;&#20110;&#24179;&#28369;&#25439;&#22833;&#30340;&#22312;&#32447;SGD&#20351;&#29992;$n \gtrsim d^{k^\star/2}$&#20010;&#26679;&#26412;&#21487;&#20197;&#23398;&#20064;$w^\star$&#65292;&#24357;&#34917;&#20102;&#19978;&#19979;&#30028;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20316;&#32773;&#36824;&#23558;&#20854;&#19982;&#24352;&#37327;PCA&#30340;&#32479;&#35745;&#20998;&#26512;&#21644;&#23567;&#25209;&#37327;SGD&#22312;&#32463;&#39564;&#25439;&#22833;&#19978;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#25928;&#24212;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on the task of learning a single index model $\sigma(w^\star \cdot x)$ with respect to the isotropic Gaussian distribution in $d$ dimensions. Prior work has shown that the sample complexity of learning $w^\star$ is governed by the information exponent $k^\star$ of the link function $\sigma$, which is defined as the index of the first nonzero Hermite coefficient of $\sigma$. Ben Arous et al. (2021) showed that $n \gtrsim d^{k^\star-1}$ samples suffice for learning $w^\star$ and that this is tight for online SGD. However, the CSQ lower bound for gradient based methods only shows that $n \gtrsim d^{k^\star/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns $w^\star$ with $n \gtrsim d^{k^\star/2}$ samples. We also draw connections to statistical analyses of tensor PCA and to the implicit regularization effects of minibatch SGD on empirical losses.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;</title><link>http://arxiv.org/abs/2305.10631</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Subabdominal MRI Image Segmentation Algorithm Based on Multi-Scale Feature Pyramid Network and Dual Attention Mechanism. (arXiv:2305.10631v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10631
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#20998;&#21106;&#31639;&#27861;&#65292;&#20351;&#29992;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32534;&#30721;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#65292;&#35774;&#35745;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#20197;&#20445;&#25345;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;U-Net&#22312;&#20998;&#21106;&#30452;&#32928;&#30284;&#27835;&#30103;&#26399;&#38388;&#30340;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#26102;&#65292;&#30001;&#20110;&#22810;&#27425;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#23548;&#33268;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#24046;&#36317;&#21644;&#38169;&#20301;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21644;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;MRI&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22312;&#20110;&#35774;&#35745;&#20102;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#22312;&#32534;&#30721;&#20013;&#20351;&#29992;&#20102;&#31354;&#27934;&#21367;&#31215;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#20197;&#36991;&#20813;&#35821;&#20041;&#24046;&#36317;&#12290;2&#65289;&#35774;&#35745;&#20102;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#20445;&#25345;U-Net&#30340;&#31354;&#38388;&#20449;&#24687;&#24182;&#20943;&#23569;&#38169;&#20301;&#12290;&#23545;&#23376;&#33145;&#37096;MRI&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20043;&#65292;&#22810;&#23610;&#24230;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#21487;&#20197;&#20943;&#23569;&#35821;&#20041;&#24046;&#36317;&#65292;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#20351;&#32534;&#30721;&#21644;&#35299;&#30721;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aimed to solve the semantic gap and misalignment issue between encoding and decoding because of multiple convolutional and pooling operations in U-Net when segmenting subabdominal MRI images during rectal cancer treatment. A MRI Image Segmentation is proposed based on a multi-scale feature pyramid network and dual attention mechanism. Our innovation is the design of two modules: 1) a dilated convolution and multi-scale feature pyramid network are used in the encoding to avoid the semantic gap. 2) a dual attention mechanism is designed to maintain spatial information of U-Net and reduce misalignment. Experiments on a subabdominal MRI image dataset show the proposed method achieves better performance than others methods. In conclusion, a multi-scale feature pyramid network can reduce the semantic gap, and the dual attention mechanism can make an alignment of features between encoding and decoding.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#31283;&#23450;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#19981;&#31283;&#23450;&#24615;&#24182;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#19981;&#26159;&#38543;&#26426;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#24335;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#25968;&#25454;&#26080;&#20851;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19981;&#31283;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#26356;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10625</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23616;&#37096;&#19981;&#31283;&#23450;&#24615;&#27979;&#37327;&#21644;&#20943;&#23569;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Measuring and Mitigating Local Instability in Deep Neural Networks. (arXiv:2305.10625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10625
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#30340;&#36755;&#20986;&#19981;&#31283;&#23450;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#19981;&#31283;&#23450;&#24615;&#24182;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#19981;&#26159;&#38543;&#26426;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#24335;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#20316;&#32773;&#30740;&#31350;&#20102;&#25968;&#25454;&#26080;&#20851;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19981;&#31283;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#26356;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#25104;&#20026;&#25968;&#30334;&#19975;&#29992;&#25143;&#20381;&#36182;&#30340;&#23454;&#38469;&#22330;&#26223;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#26500;&#24314;&#32773;&#24448;&#24448;&#24456;&#38590;&#30830;&#20445;&#21487;&#38752;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#20687;&#38543;&#26426;&#21021;&#22987;&#21270;&#36825;&#26679;&#30340;&#26080;&#20851;&#32454;&#33410;&#21487;&#33021;&#24847;&#22806;&#22320;&#25913;&#21464;&#35757;&#32451;&#31995;&#32479;&#30340;&#36755;&#20986;&#65292;&#21487;&#33021;&#24102;&#26469;&#28798;&#38590;&#24615;&#30340;&#21518;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#23545;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#22312;&#21516;&#19968;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#39044;&#27979;&#20173;&#28982;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#30456;&#24403;&#19968;&#37096;&#20998;&#26597;&#35810;&#30340;&#39044;&#27979;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#21407;&#21017;&#30340;&#25351;&#26631;&#65292;&#22914;&#36328;&#35757;&#32451;&#36816;&#34892;&#25110;&#21333;&#27425;&#35757;&#32451;&#20869;&#30340;&#27599;&#20010;&#26679;&#26412;&#30340;&#8220;&#26631;&#31614;&#29109;&#8221;&#65292;&#26469;&#37327;&#21270;&#36825;&#31181;&#29616;&#35937;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#24182;&#19981;&#26159;&#38543;&#26426;&#20986;&#29616;&#30340;&#65292;&#32780;&#26159;&#20197;&#25968;&#25454;&#30456;&#20851;&#30340;&#26041;&#24335;&#32858;&#38598;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#26080;&#20851;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#31181;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#34920;&#26126;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19981;&#31283;&#23450;&#24615;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#26356;&#24191;&#27867;&#20351;&#29992;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are becoming integral components of real world services relied upon by millions of users. Unfortunately, architects of these systems can find it difficult to ensure reliable performance as irrelevant details like random initialization can unexpectedly change the outputs of a trained system with potentially disastrous consequences. We formulate the model stability problem by studying how the predictions of a model change, even when it is retrained on the same data, as a consequence of stochasticity in the training process. For Natural Language Understanding (NLU) tasks, we find instability in predictions for a significant fraction of queries. We formulate principled metrics, like per-sample ``label entropy'' across training runs or within a single training run, to quantify this phenomenon. Intriguingly, we find that unstable predictions do not appear at random, but rather appear to be clustered in data-specific ways. We study data-agnostic regularization meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#20869;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36830;&#36890;&#24615;&#36136;&#12290;&#23384;&#22312;&#19968;&#20010;&#22823;&#30340;&#27979;&#22320;&#20984;&#25104;&#20998;&#65292;&#23545;&#21508;&#31181;&#20248;&#21270;&#21160;&#21147;&#23398;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20854;&#20013;&#21448;&#26377;&#19968;&#20010;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#27979;&#22320;&#36830;&#25509;&#30340;&#38750;&#20856;&#22411;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26143;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>http://arxiv.org/abs/2305.10623</link><description>&lt;p&gt;
&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#30340;&#35299;&#31354;&#38388;&#30340;&#26143;&#24418;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
The star-shaped space of solutions of the spherical negative perceptron. (arXiv:2305.10623v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#27169;&#22411;&#30340;&#35299;&#31354;&#38388;&#23637;&#24320;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#20869;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36830;&#36890;&#24615;&#36136;&#12290;&#23384;&#22312;&#19968;&#20010;&#22823;&#30340;&#27979;&#22320;&#20984;&#25104;&#20998;&#65292;&#23545;&#21508;&#31181;&#20248;&#21270;&#21160;&#21147;&#23398;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20854;&#20013;&#21448;&#26377;&#19968;&#20010;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#27979;&#22320;&#36830;&#25509;&#30340;&#38750;&#20856;&#22411;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26143;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31070;&#32463;&#32593;&#32476;&#26223;&#35266;&#30340;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#20302;&#33021;&#37197;&#32622;&#36890;&#24120;&#20986;&#29616;&#22312;&#22797;&#26434;&#30340;&#36830;&#36890;&#32467;&#26500;&#20013;&#65292;&#22312;&#37027;&#37324;&#21487;&#20197;&#26500;&#24314;&#36828;&#36317;&#31163;&#35299;&#20043;&#38388;&#30340;&#38646;&#33021;&#36335;&#24452;&#12290;&#26412;&#25991;&#30740;&#31350;&#29699;&#24418;&#36127;&#24863;&#30693;&#22120;&#65292;&#19968;&#20010;&#20316;&#20026;&#36830;&#32493;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#30340;&#20856;&#22411;&#38750;&#20984;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#20174;&#24179;&#34913;&#28857;&#37319;&#26679;&#30340;&#39030;&#28857;&#37197;&#32622;&#30340;&#21333;&#32431;&#24418;&#20013;&#33021;&#37327;&#38556;&#30861;&#30340;&#36890;&#29992;&#20998;&#26512;&#26041;&#27861;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#21306;&#22495;&#20869;&#65292;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#26174;&#31034;&#20986;&#31616;&#21333;&#30340;&#36830;&#36890;&#24615;&#36136;&#12290;&#23384;&#22312;&#19968;&#20010;&#22823;&#30340;&#27979;&#22320;&#20984;&#25104;&#20998;&#65292;&#23545;&#21508;&#31181;&#20248;&#21270;&#21160;&#21147;&#23398;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#20010;&#21306;&#22495;&#20869;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22823;&#22810;&#25968;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#27979;&#22320;&#36830;&#25509;&#30340;&#38750;&#20856;&#22411;&#40065;&#26834;&#35299;&#20915;&#26041;&#26696;&#30340;&#23376;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26143;&#24418;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#20998;&#26512;&#24615;&#22320;&#34920;&#24449;&#20102;&#35299;&#31354;&#38388;&#30340;&#36830;&#25509;&#32467;&#26500;&#32452;&#32455;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies on the landscape of neural networks have shown that low-energy configurations are often found in complex connected structures, where zero-energy paths between pairs of distant solutions can be constructed. Here we consider the spherical negative perceptron, a prototypical non-convex neural network model framed as a continuous constraint satisfaction problem. We introduce a general analytical method for computing energy barriers in the simplex with vertex configurations sampled from the equilibrium. We find that in the over-parameterized regime the solution manifold displays simple connectivity properties. There exists a large geodesically convex component that is attractive for a wide range of optimization dynamics. Inside this region we identify a subset of atypically robust solutions that are geodesically connected with most other solutions, giving rise to a star-shaped geometry. We analytically characterize the organization of the connected space of solutions and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10616</link><description>&lt;p&gt;
CNN &#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Evaluation Metrics for CNNs Compression. (arXiv:2305.10616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#35780;&#20272;&#24230;&#37327;&#32508;&#36848;&#65292;&#20174;&#32780;&#20026;&#26631;&#20934;&#21270;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#36129;&#29486;&#21147;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#24320;&#21457;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#31038;&#21306;&#20284;&#20046;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#21387;&#32553;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35782;&#21035;&#19981;&#21516;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#21512;&#36866;&#30340;&#21387;&#32553;&#25216;&#26415;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#35780;&#20272;&#24230;&#37327;&#30340;&#32508;&#36848;&#26469;&#20026;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#26631;&#20934;&#21270;&#36129;&#29486;&#12290;&#36825;&#20123;&#24230;&#37327;&#24050;&#34987;&#23454;&#29616;&#21040;NetZIP&#65292;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#22522;&#20934;&#20043;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#19968;&#20123;&#34987;&#23457;&#26597;&#30340;&#24230;&#37327;&#65292;&#20998;&#21035;&#32858;&#28966;&#20110;&#30446;&#26631;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lot of research effort devoted by researcher into developing different techniques for neural networks compression, yet the community seems to lack standardised ways of evaluating and comparing between different compression techniques, which is key to identifying the most suitable compression technique for different applications. In this paper we contribute towards standardisation of neural network compression by providing a review of evaluation metrics. These metrics have been implemented into NetZIP, a standardised neural network compression bench. We showcase some of the metrics reviewed using three case studies focusing on object classification, object detection, and edge devices.
&lt;/p&gt;</description></item><item><title>ACRoBat&#26159;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;&#26694;&#26550;&#65292;&#22312;&#32534;&#35793;&#26102;&#36827;&#34892;&#28151;&#21512;&#38745;&#24577;+&#21160;&#24577;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471;&#24352;&#37327;&#20195;&#30721;&#29983;&#25104;&#65292;&#21487;&#23558;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798;8.5&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.10611</link><description>&lt;p&gt;
ACRoBat&#65306;&#22312;&#32534;&#35793;&#26102;&#20248;&#21270;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
ACRoBat: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time. (arXiv:2305.10611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10611
&lt;/p&gt;
&lt;p&gt;
ACRoBat&#26159;&#19968;&#31181;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;&#26694;&#26550;&#65292;&#22312;&#32534;&#35793;&#26102;&#36827;&#34892;&#28151;&#21512;&#38745;&#24577;+&#21160;&#24577;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471;&#24352;&#37327;&#20195;&#30721;&#29983;&#25104;&#65292;&#21487;&#23558;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798;8.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#25511;&#21046;&#27969;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#25216;&#26415;&#65292;&#24120;&#29992;&#20110;&#35774;&#35745;&#34920;&#36798;&#21147;&#24378;&#21644;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#65292;&#20363;&#22914;&#25991;&#26412;&#35299;&#26512;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25552;&#21069;&#36864;&#20986;&#28145;&#24230;&#27169;&#22411;&#31561;&#12290;&#28982;&#32780;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25511;&#21046;&#27969;&#20998;&#21449;&#20351;&#24471;&#25209;&#22788;&#29702;&#38590;&#20197;&#25163;&#21160;&#25191;&#34892;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; ACRoBat &#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#28151;&#21512;&#38745;&#24577;+&#21160;&#24577;&#32534;&#35793;&#22120;&#20248;&#21270;&#21644;&#31471;&#21040;&#31471;&#24352;&#37327;&#20195;&#30721;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#28145;&#24230;&#23398;&#20064;&#35745;&#31639;&#30340;&#39640;&#25928;&#33258;&#21160;&#25209;&#22788;&#29702;&#12290;&#22312; NVIDIA GeForce RTX 3070 GPU &#19978;&#65292;ACRoBat &#30340;&#24615;&#33021;&#27604;&#29616;&#26377;&#30340;&#33258;&#21160;&#25209;&#22788;&#29702;&#26694;&#26550; DyNet &#25552;&#39640;&#20102;&#22810;&#36798; 8.5 &#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic control flow is an important technique often used to design expressive and efficient deep learning computations for applications such as text parsing, machine translation, exiting early out of deep models and so on. However, the resulting control flow divergence makes batching, an important performance optimization, difficult to perform manually. In this paper, we present ACRoBat, a framework that enables efficient automatic batching for dynamic deep learning computations by performing hybrid static+dynamic compiler optimizations and end-to-end tensor code generation. ACRoBat performs up to 8.5X better than DyNet, a state-of-the-art framework for automatic batching, on an Nvidia GeForce RTX 3070 GPU.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2305.10601</link><description>&lt;p&gt;
Tree of Thoughts: &#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#20197;&#21450;&#33258;&#25105;&#35780;&#20272;&#21644;&#20840;&#23616;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#30340;&#36890;&#29992;&#38382;&#39064;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20173;&#28982;&#21463;&#38480;&#20110;&#22522;&#20110;&#26631;&#35760;&#12289;&#20174;&#24038;&#21040;&#21491;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#38656;&#35201;&#25506;&#32034;&#12289;&#25112;&#30053;&#21069;&#30651;&#25110;&#21021;&#22987;&#20915;&#31574;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#30340;&#20219;&#21153;&#20013;&#65292;&#20182;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;&#8212;&#8212;&#24605;&#32500;&#20043;&#26641;&#65288;ToT&#65289;&#65292;&#23427;&#23558;&#36890;&#24120;&#29992;&#20110;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#38142;&#26041;&#27861;&#27867;&#21270;&#65292;&#24182;&#20351;&#29992;&#19968;&#33268;&#30340;&#25991;&#26412;&#21333;&#20301;&#65288;&#24605;&#32500;&#65289;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#24605;&#32500;&#20316;&#20026;&#35299;&#20915;&#38382;&#39064;&#30340;&#20013;&#38388;&#27493;&#39588;&#12290;&#24605;&#32500;&#20043;&#26641;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#32771;&#34385;&#22810;&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#36335;&#24452;&#21644;&#33258;&#25105;&#35780;&#20272;&#26469;&#36827;&#34892;&#28145;&#24605;&#29087;&#34385;&#30340;&#20915;&#31574;&#65292;&#24182;&#20915;&#23450;&#19979;&#19968;&#27493;&#30340;&#34892;&#21160;&#65292;&#21516;&#26102;&#22312;&#24517;&#35201;&#26102;&#21521;&#21069;&#25110;&#21521;&#21518;&#36319;&#36394;&#20197;&#36827;&#34892;&#20840;&#23616;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ToT&#26174;&#33879;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#20915;&#38382;&#39064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24352;&#37327;&#31215;&#22312;&#36229;&#32500;&#35745;&#31639;&#20013;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#23558;&#20854;&#30830;&#23450;&#20026;&#20013;&#24515;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#23427;&#26159;&#26368;&#36890;&#29992;&#12289;&#26368;&#20855;&#34920;&#29616;&#21147;&#21644;&#26368;&#21387;&#32553;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#35823;&#24046;&#35299;&#32465;&#21644;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10572</link><description>&lt;p&gt;
&#24352;&#37327;&#31215;&#19982;&#36229;&#32500;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Tensor Products and Hyperdimensional Computing. (arXiv:2305.10572v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24352;&#37327;&#31215;&#22312;&#36229;&#32500;&#35745;&#31639;&#20013;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#23558;&#20854;&#30830;&#23450;&#20026;&#20013;&#24515;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#23427;&#26159;&#26368;&#36890;&#29992;&#12289;&#26368;&#20855;&#34920;&#29616;&#21147;&#21644;&#26368;&#21387;&#32553;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#20855;&#26377;&#26080;&#35823;&#24046;&#35299;&#32465;&#21644;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20043;&#21069;&#23545;&#22270;&#23884;&#20837;&#30340;&#20998;&#26512;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#19968;&#20123;&#32467;&#26524;&#25512;&#24191;&#21644;&#25299;&#23637;&#21040;&#21521;&#37327;&#31526;&#21495;&#32467;&#26500; (VSA) &#21644;&#36229;&#32500;&#35745;&#31639; (HDC) &#30340;&#19968;&#33324;&#35774;&#32622;&#20013;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25506;&#32034;&#36229;&#21472;&#21152;&#12289;&#27491;&#20132;&#21644;&#24352;&#37327;&#31215;&#20043;&#38388;&#30340;&#25968;&#23398;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#24352;&#37327;&#31215;&#34920;&#31034;&#30830;&#23450;&#20026;&#20013;&#24515;&#34920;&#31034;&#65292;&#24182;&#20855;&#26377;&#19968;&#22871;&#29420;&#29305;&#30340;&#23646;&#24615;&#12290;&#36825;&#21253;&#25324;&#23427;&#26159;&#26368;&#36890;&#29992;&#21644;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#34920;&#31034;&#65292;&#20063;&#26159;&#26368;&#21387;&#32553;&#30340;&#34920;&#31034;&#65292;&#20855;&#26377;&#26080;&#35823;&#24046;&#35299;&#32465;&#21644;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following up on a previous analysis of graph embeddings, we generalize and expand some results to the general setting of vector symbolic architectures (VSA) and hyperdimensional computing (HDC). Importantly, we explore the mathematical relationship between superposition, orthogonality, and tensor product. We establish the tensor product representation as the central representation, with a suite of unique properties. These include it being the most general and expressive representation, as well as being the most compressed representation that has errorrless unbinding and detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#39044;&#26399;&#29983;&#29702;&#23398;&#19968;&#33268;&#30340;&#20687;&#32032;&#32423;&#21442;&#25968;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.10569</link><description>&lt;p&gt;
&#21160;&#24577;PET&#20013;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#29983;&#29702;&#33647;&#20195;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning for Physiologically-Based Pharmacokinetic Modeling in Dynamic PET. (arXiv:2305.10569v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#32780;&#20934;&#30830;&#30340;&#33647;&#20195;&#21160;&#21147;&#23398;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#19982;&#39044;&#26399;&#29983;&#29702;&#23398;&#19968;&#33268;&#30340;&#20687;&#32032;&#32423;&#21442;&#25968;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#27491;&#30005;&#23376;&#21457;&#23556;&#26029;&#23618;&#25104;&#20687;&#65288;dPET&#65289;&#21487;&#20197;&#25552;&#20379;&#33647;&#29289;&#31034;&#36394;&#21058;&#30340;&#26102;&#38388;&#20998;&#36776;&#22270;&#20687;&#65292;&#20174;&#32780;&#23450;&#37327;&#27979;&#37327;&#29983;&#29702;&#36807;&#31243;&#12290;&#22522;&#20110;&#20307;&#32032;&#30340;&#29983;&#29702;&#33647;&#20195;&#21160;&#21147;&#23398;&#65288;PBPK&#65289;&#27169;&#22411;&#21487;&#20197;&#23545;&#26102;&#38388;-&#27963;&#24615;&#26354;&#32447;&#65288;TAC&#65289;&#36827;&#34892;&#20307;&#32032;&#32423;&#21035;&#30340;&#24314;&#27169;&#65292;&#20026;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#25552;&#20379;&#30456;&#20851;&#35786;&#26029;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;TAC&#25311;&#21512;&#31574;&#30053;&#36895;&#24230;&#24930;&#65292;&#24182;&#24573;&#30053;&#30456;&#37051;&#20307;&#32032;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26102;&#31354;UNet&#26469;&#20272;&#35745;&#32473;&#23450;F-18&#33073;&#27687;&#33889;&#33796;&#31958;&#65288;FDG&#65289;dPET&#30340;TAC&#30340;&#21160;&#21147;&#23398;&#21442;&#25968;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#25439;&#22833;&#20844;&#24335;&#26469;&#24378;&#21046;&#27979;&#37327;TAC&#19982;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#21442;&#25968;&#29983;&#25104;&#30340;TAC&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#22120;&#23448;&#32423;&#21035;&#19978;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#36895;&#24230;&#26174;&#33879;&#26356;&#24555;&#65292;&#21516;&#26102;&#29983;&#25104;&#30340;&#20687;&#32032;&#32423;&#21442;&#25968;&#22270;&#19982;&#39044;&#26399;&#30340;&#29983;&#29702;&#23398;&#19968;&#33268;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33258;&#30417;&#30563;&#32593;&#32476;&#24212;&#29992;&#20110;PBPk&#27169;&#22411;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic positron emission tomography imaging (dPET) provides temporally resolved images of a tracer enabling a quantitative measure of physiological processes. Voxel-wise physiologically-based pharmacokinetic (PBPK) modeling of the time activity curves (TAC) can provide relevant diagnostic information for clinical workflow. Conventional fitting strategies for TACs are slow and ignore the spatial relation between neighboring voxels. We train a spatio-temporal UNet to estimate the kinetic parameters given TAC from F-18-fluorodeoxyglucose (FDG) dPET. This work introduces a self-supervised loss formulation to enforce the similarity between the measured TAC and those generated with the learned kinetic parameters. Our method provides quantitatively comparable results at organ-level to the significantly slower conventional approaches, while generating pixel-wise parametric images which are consistent with expected physiology. To the best of our knowledge, this is the first self-supervised net
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2305.10564</link><description>&lt;p&gt;
&#23545;&#25918;&#24323;&#20998;&#31867;&#22120;&#36827;&#34892;&#21453;&#20107;&#23454;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Counterfactually Comparing Abstaining Classifiers. (arXiv:2305.10564v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#25351;&#30340;&#26159;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#39044;&#27979;&#26102;&#30340;&#39044;&#27979;&#24615;&#33021;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#24323;&#20998;&#31867;&#22120;&#21487;&#20197;&#36873;&#25321;&#22312;&#19981;&#30830;&#23450;&#26102;&#25918;&#24323;&#23545;&#36755;&#20837;&#30340;&#39044;&#27979;&#12290;&#36825;&#20123;&#20998;&#31867;&#22120;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#38382;&#39064;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20445;&#30041;&#19981;&#30830;&#23450;&#30340;&#39044;&#27979;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#40657;&#30418;&#25918;&#24323;&#20998;&#31867;&#22120;&#26102;&#65292;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#26469;&#32771;&#34385;&#20998;&#31867;&#22120;&#22312;&#23427;&#30340;&#25918;&#24323;&#39044;&#27979;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#25918;&#23556;&#31185;&#21307;&#29983;&#19981;&#30830;&#23450;&#20854;&#35786;&#26029;&#25110;&#24403;&#39550;&#39542;&#21592;&#22312;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#20013;&#19981;&#27880;&#24847;&#26102;&#65292;&#36825;&#20123;&#32570;&#22833;&#30340;&#39044;&#27979;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#35270;&#35282;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#25918;&#24323;&#20998;&#31867;&#22120;&#65292;&#23558;&#25918;&#24323;&#39044;&#27979;&#35270;&#20026;&#32570;&#22833;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#22260;&#32469;&#30528;&#23450;&#20041;&#19968;&#20010;&#25918;&#24323;&#20998;&#31867;&#22120;&#30340;&#21453;&#20107;&#23454;&#24471;&#20998;&#65292;&#21363;&#20998;&#31867;&#22120;&#27809;&#26377;&#25918;&#24323;&#30340;&#24773;&#20917;&#19979;&#30340;&#39044;&#27979;&#24615;&#33021;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#25351;&#23450;&#20102;&#26465;&#20214;... (&#27492;&#22788;&#30465;&#30053;)
&lt;/p&gt;
&lt;p&gt;
Abstaining classifiers have the option to abstain from making predictions on inputs that they are unsure about. These classifiers are becoming increasingly popular in high-stake decision-making problems, as they can withhold uncertain predictions to improve their reliability and safety. When evaluating black-box abstaining classifier(s), however, we lack a principled approach that accounts for what the classifier would have predicted on its abstentions. These missing predictions are crucial when, e.g., a radiologist is unsure of their diagnosis or when a driver is inattentive in a self-driving car. In this paper, we introduce a novel approach and perspective to the problem of evaluating and comparing abstaining classifiers by treating abstentions as missing data. Our evaluation approach is centered around defining the counterfactual score of an abstaining classifier, defined as the expected performance of the classifier had it not been allowed to abstain. We specify the conditions unde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Temporal Fusion Transformer&#23545;&#19981;&#21516;&#26102;&#38388;&#33539;&#22260;&#21644;&#32593;&#32476;&#32423;&#21035;&#30340;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#36827;&#34892;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;TFT&#22312;&#21464;&#30005;&#31449;&#32423;&#21035;&#21644;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#28304;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.10559</link><description>&lt;p&gt;
&#21033;&#29992;Temporal Fusion Transformer&#30340;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#65306;&#32593;&#26684;&#23618;&#27425;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Short-Term Electricity Load Forecasting Using the Temporal Fusion Transformer: Effect of Grid Hierarchies and Data Sources. (arXiv:2305.10559v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Temporal Fusion Transformer&#23545;&#19981;&#21516;&#26102;&#38388;&#33539;&#22260;&#21644;&#32593;&#32476;&#32423;&#21035;&#30340;&#30701;&#26399;&#30005;&#21147;&#36127;&#33655;&#36827;&#34892;&#39044;&#27979;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;TFT&#22312;&#21464;&#30005;&#31449;&#32423;&#21035;&#21644;&#20351;&#29992;&#39069;&#22806;&#25968;&#25454;&#28304;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#28304;&#36716;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#37197;&#30005;&#32593;&#25552;&#20986;&#20102;&#29305;&#27530;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#31934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#32593;&#26684;&#31649;&#29702;&#12290;&#26032;&#39062;&#30340;&#24314;&#27169;&#26041;&#27861;&#65292;&#20363;&#22914;Transformer&#26550;&#26500;&#65292;&#23588;&#20854;&#26159;Temporal Fusion Transformer&#65288;TFT&#65289;&#65292;&#24050;&#25104;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#23558;TFT&#24212;&#29992;&#20110;&#30005;&#21147;&#36127;&#33655;&#39044;&#27979;&#38382;&#39064;&#65292;&#20027;&#35201;&#32771;&#34385;&#21333;&#20010;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#21327;&#21464;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;TFT&#26550;&#26500;&#22312;&#19981;&#21516;&#26102;&#38388;&#33539;&#22260;&#65288;&#25552;&#21069;&#19968;&#22825;&#21644;&#25552;&#21069;&#19968;&#21608;&#65289;&#21644;&#32593;&#32476;&#32423;&#21035;&#65288;&#32593;&#26684;&#21644;&#21464;&#30005;&#31449;&#32423;&#21035;&#65289;&#30340;&#23567;&#26102;&#30701;&#26399;&#36127;&#33655;&#39044;&#27979;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;TFT&#26550;&#26500;&#22312;&#25972;&#20010;&#32593;&#26684;&#30340;&#25552;&#21069;&#19968;&#22825;&#39044;&#27979;&#20013;&#24182;&#19981;&#27604;&#26368;&#20808;&#36827;&#30340;LSTM&#27169;&#22411;&#25552;&#20379;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#37319;&#29992;&#20998;&#23618;&#26041;&#27861;&#20197;&#21450;&#21253;&#25324;&#39069;&#22806;&#30340;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#22825;&#27668;&#21464;&#37327;&#65289;&#26102;&#65292;TFT&#30340;&#32467;&#26524;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#22312;&#20855;&#20307;&#39044;&#27979;&#38382;&#39064;&#19978;&#31934;&#36873;&#25968;&#25454;&#28304;&#21644;&#24314;&#27169;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments related to the energy transition pose particular challenges for distribution grids. Hence, precise load forecasts become more and more important for effective grid management. Novel modeling approaches such as the Transformer architecture, in particular the Temporal Fusion Transformer (TFT), have emerged as promising methods for time series forecasting. To date, just a handful of studies apply TFTs to electricity load forecasting problems, mostly considering only single datasets and a few covariates. Therefore, we examine the potential of the TFT architecture for hourly short-term load forecasting across different time horizons (day-ahead and week-ahead) and network levels (grid and substation level). We find that the TFT architecture does not offer higher predictive performance than a state-of-the-art LSTM model for day-ahead forecasting on the entire grid. However, the results display significant improvements for the TFT when applied at the substation level with a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#26681;&#25454;&#34917;&#19969;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10552</link><description>&lt;p&gt;
&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Multiple Instance Learning with Distance-Aware Self-Attention. (arXiv:2305.10552v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30340;&#28145;&#24230;&#22810;&#31034;&#20363;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#26681;&#25454;&#34917;&#19969;&#20043;&#38388;&#30340;&#31354;&#38388;&#20851;&#31995;&#21160;&#24577;&#35843;&#25972;&#26435;&#37325;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#35201;&#27714;&#23545;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#36827;&#34892;&#26631;&#35760;&#65292;&#20294;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#20165;&#23545;&#23454;&#20363;&#30340;&#38598;&#21512;&#65288;&#21253;&#65289;&#21487;&#29992;&#12290;&#36825;&#31181;&#38382;&#39064;&#35774;&#32622;&#34987;&#31216;&#20026;&#22810;&#37325;&#31034;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#65292;&#22312;&#21307;&#30103;&#39046;&#22495;&#23588;&#20854;&#30456;&#20851;&#65292;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#34987;&#20998;&#25104;&#36739;&#23567;&#30340;&#34917;&#19969;&#65292;&#20294;&#26631;&#31614;&#36866;&#29992;&#20110;&#25972;&#20010;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;MIL&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#33258;&#25105;&#20851;&#27880;&#26469;&#25429;&#25417;&#34917;&#19969;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#21253;&#20013;&#25152;&#26377;&#20854;&#20182;&#34917;&#19969;&#23545;&#27599;&#20010;&#34917;&#19969;&#36827;&#34892;&#19981;&#21516;&#30340;&#21152;&#26435;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#27809;&#26377;&#32771;&#34385;&#36739;&#22823;&#22270;&#20687;&#20013;&#34917;&#19969;&#20043;&#38388;&#30340;&#30456;&#23545;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;MIL&#27169;&#22411;&#65292;&#20855;&#26377;&#36317;&#31163;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#65288;DAS-MIL&#65289;&#65292;&#23427;&#22312;&#24314;&#27169;&#34917;&#19969;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#26102;&#26126;&#30830;&#32771;&#34385;&#30456;&#23545;&#31354;&#38388;&#20449;&#24687;&#12290;&#19982;&#29616;&#26377;&#30456;&#20851;&#27169;&#22411;&#19981;&#21516;&#65292;DAS-MIL&#20351;&#29992;&#36317;&#31163;&#24863;&#30693;&#27880;&#24847;&#26426;&#21046;&#26681;&#25454;&#34917;&#19969;&#20043;&#38388;&#30340;&#36317;&#31163;&#21160;&#24577;&#35843;&#25972;&#34917;&#19969;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional supervised learning tasks require a label for every instance in the training set, but in many real-world applications, labels are only available for collections (bags) of instances. This problem setting, known as multiple instance learning (MIL), is particularly relevant in the medical domain, where high-resolution images are split into smaller patches, but labels apply to the image as a whole. Recent MIL models are able to capture correspondences between patches by employing self-attention, allowing them to weigh each patch differently based on all other patches in the bag. However, these approaches still do not consider the relative spatial relationships between patches within the larger image, which is especially important in computational pathology. To this end, we introduce a novel MIL model with distance-aware self-attention (DAS-MIL), which explicitly takes into account relative spatial information when modelling the interactions between patches. Unlike existing rela
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#31232;&#30095;&#28145;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22312;&#23485;&#24230;&#36275;&#22815;&#22823;&#26102;&#65292;&#36739;&#31232;&#30095;&#30340;&#32593;&#32476;&#22312;&#27973;&#23618;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10550</link><description>&lt;p&gt;
&#26080;&#38480;&#23485;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#31232;&#30095;&#28145;&#24230;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Sparsity-depth Tradeoff in Infinitely Wide Deep Neural Networks. (arXiv:2305.10550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#31232;&#30095;&#28145;&#24230;&#21644;&#27867;&#21270;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22312;&#23485;&#24230;&#36275;&#22815;&#22823;&#26102;&#65292;&#36739;&#31232;&#30095;&#30340;&#32593;&#32476;&#22312;&#27973;&#23618;&#26102;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#31070;&#32463;&#27963;&#21160;&#22914;&#20309;&#24433;&#21709;&#20855;&#26377;&#28145;&#24230;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23485;&#24230;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;(NNGP)&#26680;&#65292;&#20854;&#20855;&#26377;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;(ReLU)&#28608;&#27963;&#21644;&#39044;&#23450;&#25968;&#37327;&#30340;&#27963;&#36291;&#31070;&#32463;&#20803;&#12290;&#20351;&#29992;NNGP&#26680;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#65292;&#36739;&#31232;&#30095;&#30340;&#32593;&#32476;&#22312;&#27973;&#23618;&#26102;&#20248;&#20110;&#38750;&#31232;&#30095;&#30340;&#32593;&#32476;&#12290;&#36890;&#36807;&#25193;&#23637;&#29616;&#26377;&#30340;&#26680;&#23725;&#22238;&#24402;&#30340;&#19968;&#33324;&#21270;&#35823;&#24046;&#29702;&#35770;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate how sparse neural activity affects the generalization performance of a deep Bayesian neural network at the large width limit. To this end, we derive a neural network Gaussian Process (NNGP) kernel with rectified linear unit (ReLU) activation and a predetermined fraction of active neurons. Using the NNGP kernel, we observe that the sparser networks outperform the non-sparse networks at shallow depths on a variety of datasets. We validate this observation by extending the existing theory on the generalization error of kernel-ridge regression.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#36870;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#65292;&#33258;&#21160;&#21457;&#29616;&#22870;&#21169;&#20989;&#25968;&#24182;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#23547;&#25214;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.10548</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21457;&#29616;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning. (arXiv:2305.10548v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31163;&#32447;&#36870;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#65292;&#33258;&#21160;&#21457;&#29616;&#22870;&#21169;&#20989;&#25968;&#24182;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#31574;&#30053;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#20013;&#23547;&#25214;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#65288;&#20363;&#22914;&#40060;&#32676;&#21644;&#32454;&#33740;&#32676;&#33853;&#65289;&#20013;&#30340;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20010;&#20307;&#30446;&#26631;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#36870;&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#20854;&#22312;&#28041;&#21450;&#36830;&#32493;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#21644;&#22810;&#20010;&#20132;&#20114;&#20195;&#29702;&#30340;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31163;&#32447;&#36870;&#21521;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IMARL&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;ReF-ER&#25216;&#26415;&#21644;&#23548;&#21521;&#25104;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#28436;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#22870;&#21169;&#20989;&#25968;&#24182;&#23398;&#20064;&#20195;&#29702;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#25429;&#25417;&#21040;&#20102;&#25552;&#20379;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#65292;&#24182;&#22312;&#21253;&#25324;OpenAI gym&#20013;&#30340;&#21333;&#20195;&#29702;&#27169;&#22411;&#21644;&#28041;&#21450;&#22810;&#20195;&#29702;&#30340;&#27169;&#22411;&#20013;&#30340;&#38382;&#39064;&#22495;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The pr
&lt;/p&gt;</description></item><item><title>GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10544</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#30340;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476;&#36827;&#34892;&#27010;&#29575;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tractable Probabilistic Graph Representation Learning with Graph-Induced Sum-Product Networks. (arXiv:2305.10544v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10544
&lt;/p&gt;
&lt;p&gt;
GSPNs&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#65292;&#24182;&#36890;&#36807;&#26435;&#37325;&#20849;&#20139;&#21644;&#26641;&#29366;&#35745;&#31639;&#22270;&#30340;&#20248;&#21183;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#21644;&#31215;&#32593;&#32476; (GSPN)&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26694;&#26550;&#65292;&#29992;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#21487;&#20197;&#21487;&#35745;&#31639;&#22320;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#12290;&#21463;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#30001;&#39030;&#28857;&#24341;&#36215;&#30340;&#35745;&#31639;&#26641;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#21644;&#31215;&#32593;&#32476;&#65288;SPN&#65289;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#29238;SPN&#30340;&#21442;&#25968;&#26159;&#20854;&#23376;&#32423;&#30340;&#21518;&#39564;&#28151;&#21512;&#27010;&#29575;&#30340;&#21487;&#23398;&#20064;&#21464;&#25442;&#12290;&#30001;&#20110;&#26435;&#37325;&#20849;&#20139;&#21644;GSPN&#30340;&#26641;&#29366;&#35745;&#31639;&#22270;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#32431;&#27010;&#29575;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#28145;&#24230;&#22270;&#32593;&#32476;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#32570;&#20047;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#21644;&#22270;&#20998;&#31867;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30456;&#23545;&#20110;&#27969;&#34892;&#30340;&#31070;&#32463;&#27169;&#22411;&#30340;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22238;&#31572;&#27010;&#29575;&#26597;&#35810;&#30340;&#33021;&#21147;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph-Induced Sum-Product Networks (GSPNs), a new probabilistic framework for graph representation learning that can tractably answer probabilistic queries. Inspired by the computational trees induced by vertices in the context of message-passing neural networks, we build hierarchies of sum-product networks (SPNs) where the parameters of a parent SPN are learnable transformations of the a-posterior mixing probabilities of its children's sum units. Due to weight sharing and the tree-shaped computation graphs of GSPNs, we obtain the efficiency and efficacy of deep graph networks with the additional advantages of a purely probabilistic model. We show the model's competitiveness on scarce supervision scenarios, handling missing data, and graph classification in comparison to popular neural models. We complement the experiments with qualitative analyses on hyper-parameters and the model's ability to answer probabilistic queries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#30028;&#23450;&#20102;&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#38388;&#38553;&#65292;&#32467;&#26524;&#19982;&#35299;&#30721;&#22120;&#30340;&#22797;&#26434;&#31243;&#24230;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.10540</link><description>&lt;p&gt;
&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#27867;&#21270;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Neural Belief Propagation Decoders. (arXiv:2305.10540v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#30028;&#23450;&#20102;&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#38388;&#38553;&#65292;&#32467;&#26524;&#19982;&#35299;&#30721;&#22120;&#30340;&#22797;&#26434;&#31243;&#24230;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#37319;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#19979;&#19968;&#20195;&#36890;&#20449;&#31995;&#32479;&#30340;&#35299;&#30721;&#22120;&#12290;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#26159;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#65288;NBP&#65289;&#65292;&#23427;&#23558;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#36845;&#20195;&#23637;&#24320;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21442;&#25968;&#20197;&#25968;&#25454;&#39537;&#21160;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;NBP&#35299;&#30721;&#22120;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#35299;&#30721;&#31639;&#27861;&#26377;&#25152;&#25913;&#36827;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;NBP&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35299;&#30721;&#22120;&#30340;&#27867;&#21270;&#38388;&#38553;&#26159;&#32463;&#39564;&#21644;&#26399;&#26395;&#35823;&#30721;&#29575;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#30028;&#23450;&#20102;&#36825;&#31181;&#24046;&#36317;&#65292;&#24182;&#34920;&#26126;&#23427;&#19982;&#35299;&#30721;&#22120;&#30340;&#22797;&#26434;&#31243;&#24230;&#65288;&#21363;&#20195;&#30721;&#21442;&#25968;&#65288;&#22359;&#38271;&#24230;&#12289;&#28040;&#24687;&#38271;&#24230;&#12289;&#21464;&#37327;/&#26816;&#26597;&#33410;&#28857;&#24230;&#25968;&#65289;&#12289;&#35299;&#30721;&#36845;&#20195;&#27425;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#65289;&#26377;&#20851;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#35268;&#21644;&#19981;&#35268;&#21017;&#22855;&#20598;&#26657;&#39564;&#30697;&#38453;&#30340;&#32467;&#26524;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#32452;&#20851;&#20110;&#31070;&#32463;&#32622;&#20449;&#20256;&#25773;&#35793;&#30721;&#22120;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning based approaches are being increasingly used for designing decoders for next generation communication systems. One widely used framework is neural belief propagation (NBP), which unfolds the belief propagation (BP) iterations into a deep neural network and the parameters are trained in a data-driven manner. NBP decoders have been shown to improve upon classical decoding algorithms. In this paper, we investigate the generalization capabilities of NBP decoders. Specifically, the generalization gap of a decoder is the difference between empirical and expected bit-error-rate(s). We present new theoretical results which bound this gap and show the dependence on the decoder complexity, in terms of code parameters (blocklength, message length, variable/check node degrees), decoding iterations, and the training dataset size. Results are presented for both regular and irregular parity-check matrices. To the best of our knowledge, this is the first set of theoretical results on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#26469;&#39564;&#35777;&#12289;&#20445;&#25252;&#21644;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#32463;&#39564;&#36830;&#36143;&#24615;&#21644;&#25919;&#31574;&#25913;&#36827;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10528</link><description>&lt;p&gt;
&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#32570;&#38519;&#34892;&#20026;&#30340;&#21487;&#25193;&#23637;&#21644;&#23433;&#20840;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems. (arXiv:2305.10528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#23398;&#20064;&#23545;&#35805;&#31995;&#32479;&#20013;&#21033;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#26469;&#39564;&#35777;&#12289;&#20445;&#25252;&#21644;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22823;&#35268;&#27169;&#21830;&#19994;&#29615;&#22659;&#20013;&#30340;&#32463;&#39564;&#36830;&#36143;&#24615;&#21644;&#25919;&#31574;&#25913;&#36827;&#20043;&#38388;&#30340;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#20026;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#39537;&#21160;&#21147;&#65292;&#25913;&#21892;&#20102;&#30446;&#26631;&#20026;&#23548;&#21521;&#30340;&#20195;&#29702;&#20154;&#19982;&#20154;&#20043;&#38388;&#26356;&#33258;&#28982;&#30340;&#20114;&#21160;&#65292;&#25552;&#39640;&#20102;&#29992;&#25143;&#30340;&#28385;&#24847;&#24230;&#65292;&#20294;&#22312;&#22823;&#22411;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#24179;&#34913;&#25919;&#31574;&#25913;&#36827;&#21644;&#32463;&#39564;&#36830;&#36143;&#24615;&#32463;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21382;&#21490;&#22238;&#24402;&#20107;&#20214;&#25253;&#21578;&#20013;&#30340;&#39640;&#31934;&#24230;&#26679;&#26412;&#23545;&#25919;&#31574;&#36827;&#34892;&#39564;&#35777;&#12289;&#23433;&#20840;&#20445;&#25252;&#21644;&#25913;&#36827;&#65292;&#20197;&#20415;&#22312;&#22312;&#32447;&#37096;&#32626;&#21069;&#36827;&#34892;&#20462;&#27491;&#12290;&#20316;&#32773;&#23545;&#30495;&#23454;&#30340;&#23545;&#35805;&#31995;&#32479;&#21644;&#23454;&#38469;&#30340;&#22238;&#24402;&#20107;&#20214;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20182;&#20204;&#30340;&#29983;&#20135;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-Policy reinforcement learning has been a driving force for the state-of-the-art conversational AIs leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10519</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Statistical Knowledge Assessment for Generative Language Models. (arXiv:2305.10519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#30693;&#35782;&#36755;&#20986;&#12290;&#36890;&#36807;&#23545;14&#31181;GLMs&#36827;&#34892;&#32508;&#21512;&#27604;&#36739;&#65292;&#21457;&#29616;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#32780;&#23545;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLMs&#65289;&#23637;&#31034;&#20102;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#39640;&#25928;&#22238;&#31572;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#32473;&#23450;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;GLM&#26159;&#21542;&#22987;&#32456;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#31572;&#26696;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#28508;&#21464;&#37327;&#21644;KaRR&#24230;&#37327;&#25351;&#23548;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#26694;&#26550;&#65292;&#35813;&#24230;&#37327;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#24418;&#24335;&#19978;&#30340;&#36830;&#32493;&#27010;&#29575;&#37327;&#21270;&#20854;&#30693;&#35782;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;14&#31181;GLM&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#21253;&#25324;LLaMA&#12289;Alpaca&#12289;OPT&#21644;&#20854;&#20182;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32479;&#35745;&#30693;&#35782;&#35780;&#20272;&#28085;&#30422;&#20102;600&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#24378;&#30456;&#20851;&#24615;&#65288;0.43 Kendall's $\tau$&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#25903;&#26550;&#32467;&#26500;&#30340;GLM&#30340;&#30693;&#35782;&#36981;&#24490;&#32553;&#25918;&#23450;&#24459;&#65292;&#24182;&#19988;&#22312;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24494;&#35843;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#25345;&#32493;&#29983;&#25104;&#20107;&#23454;&#27491;&#30830;&#30340;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models (GLMs) have demonstrated capabilities to store factual knowledge and answer queries efficiently. Given varying prompts, does a GLM consistently generate factually correct answers? In this paper, we introduce a statistical knowledge assessment framework guided by latent variables and the KaRR metric, which quantifies a model's knowledge by computing its continuous probability across diverse text forms. We conduct a comprehensive comparison of knowledge across 14 GLMs using our framework, including LLaMA, Alpaca, OPT, and others. Our statistical knowledge assessment encompasses 600 relation types and exhibits a strong correlation (0.43 Kendall's $\tau$) with human evaluation. Our findings reveal that the knowledge in GLMs with the same backbone architecture adheres to the scaling law, and that tuning on instruction-following data may compromise the model's ability to generate factually correct text consistently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#21608;&#26399;&#24615;&#27880;&#20837;&#25915;&#20987;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#21487;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65307;&#24403;&#25915;&#20987;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(log(n)p/(1-p)^2) &#12290;&#21363;&#20351;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#20272;&#35745;&#22120;&#20173;&#21487;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.10506</link><description>&lt;p&gt;
&#26356;&#22810;&#33039;&#25968;&#25454;&#19979;&#30340;&#31995;&#32479;&#35782;&#21035;&#31934;&#30830;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#12290;&#22312;&#21608;&#26399;&#24615;&#27880;&#20837;&#25915;&#20987;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#21487;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65307;&#24403;&#25915;&#20987;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(log(n)p/(1-p)^2) &#12290;&#21363;&#20351;&#26377;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#20272;&#35745;&#22120;&#20173;&#21487;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25932;&#23545;&#29615;&#22659;&#19979;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;&#31995;&#32479;&#35782;&#21035;&#38382;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;Lasso&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#29305;&#24615;&#65292;&#28041;&#21450;&#21040;&#23545;&#20110;&#25915;&#20987;&#26102;&#21051;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#20004;&#31181;&#19981;&#21516;&#22330;&#26223;&#12290;&#30001;&#20110;&#25910;&#38598;&#30340;&#26679;&#26412;&#30456;&#20851;&#65292;&#29616;&#26377;&#30340;Lasso&#32467;&#26524;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#31995;&#32479;&#31283;&#23450;&#19988;&#25915;&#20987;&#20197;&#21608;&#26399;&#24615;&#27880;&#20837;&#26102;&#65292;&#31995;&#32479;&#21160;&#24577;&#30340;&#31934;&#30830;&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;O(n)&#65292;&#20854;&#20013;n&#26159;&#29366;&#24577;&#30340;&#32500;&#24230;&#12290;&#24403;&#25915;&#20987;&#22312;&#27599;&#20010;&#26102;&#38388;&#23454;&#20363;&#20013;&#20197;&#27010;&#29575;p&#36827;&#34892;&#26102;&#65292;&#31934;&#30830;&#24674;&#22797;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23558;&#25353;O(log (n)p / (1-p)^2)&#36827;&#34892;&#32553;&#25918;&#12290;&#35813;&#32467;&#26524;&#22312;&#28176;&#36817;&#29366;&#24577;&#19979;&#24847;&#21619;&#30528;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#20110;&#30495;&#23454;&#31995;&#32479;&#21160;&#24577;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#21363;&#20351;&#36229;&#36807;&#19968;&#21322;&#30340;&#25968;&#25454;&#21463;&#25439;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#20173;&#28982;&#33021;&#22815;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the system identification problem for linear discrete-time systems under adversaries and analyze two lasso-type estimators. We study both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We show that when the system is stable and the attacks are injected periodically, the sample complexity for the exact recovery of the system dynamics is O(n), where n is the dimension of the states. When the adversarial attacks occur at each time instance with probability p, the required sample complexity for the exact recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, even when more than half of the data is compromised, our estimators still learn th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#27169;&#22411;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20316;&#20026;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.10504</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Free Robust Average-Reward Reinforcement Learning. (arXiv:2305.10504v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#27169;&#22411;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#20960;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20316;&#20026;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36890;&#36807;&#22312;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#26469;&#35299;&#20915;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#26080;&#27169;&#22411;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#29702;&#35770;&#19978;&#25551;&#36848;&#20102;&#40065;&#26834;&#24179;&#22343;&#22870;&#21169;Bellman&#26041;&#31243;&#30340;&#35299;&#32467;&#26500;&#65292;&#36825;&#23545;&#25105;&#20204;&#21518;&#38754;&#30340;&#25910;&#25947;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#40065;&#26834;&#30456;&#23545;&#20215;&#20540;&#36845;&#20195;(TD)&#21644;&#40065;&#26834;RVI Q-learning&#65292;&#24182;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#20204;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#31034;&#20363;&#65292;&#21253;&#25324;&#27745;&#26579;&#27169;&#22411;&#12289;&#24635;&#21464;&#24046;&#12289;&#21345;&#26041;&#25955;&#24230;&#12289;KL&#25955;&#24230;&#21644;Wasserstein&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10502</link><description>&lt;p&gt;
EENED&#65306;&#22522;&#20110;&#21367;&#31215;&#21464;&#21387;&#22120;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
EENED: End-to-End Neural Epilepsy Detection based on Convolutional Transformer. (arXiv:2305.10502v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#65292;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#24182;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;EEG&#20449;&#21495;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#65288;Transformer&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#20381;&#36182;&#24615;&#65292;&#32780;CNN&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#22914;&#38191;&#40831;&#27874;&#20043;&#31867;&#30340;&#23616;&#37096;&#29305;&#24449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EENED&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#20803;&#30315;&#30187;&#26816;&#27979;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;CNN&#21644;Transformer&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#22312;Transformer&#32534;&#30721;&#22120;&#20013;&#24341;&#20837;&#21367;&#31215;&#27169;&#22359;&#65292;EENED&#21487;&#20197;&#23398;&#20064;&#24739;&#32773;EEG&#20449;&#21495;&#29305;&#24449;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#27880;&#24847;&#21040;&#19982;&#30315;&#30187;&#23494;&#20999;&#30456;&#20851;&#30340;&#23616;&#37096;EEG&#24322;&#24120;&#31361;&#21464;&#65292;&#22914;&#23574;&#38160;&#27874;&#30340;&#20986;&#29616;&#21644;&#32531;&#24930;&#27874;&#30340;&#25955;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;Transformer&#21644;CNN&#25429;&#25417;EEG&#20449;&#21495;&#19981;&#21516;&#23610;&#24230;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#26377;&#26395;&#25552;&#39640;&#30315;&#30187;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#23558;&#24456;&#24555;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Transformer and Convolution neural network (CNN) based models have shown promising results in EEG signal processing. Transformer models can capture the global dependencies in EEG signals through a self-attention mechanism, while CNN models can capture local features such as sawtooth waves. In this work, we propose an end-to-end neural epilepsy detection model, EENED, that combines CNN and Transformer. Specifically, by introducing the convolution module into the Transformer encoder, EENED can learn the time-dependent relationship of the patient's EEG signal features and notice local EEG abnormal mutations closely related to epilepsy, such as the appearance of spikes and the sprinkling of sharp and slow waves. Our proposed framework combines the ability of Transformer and CNN to capture different scale features of EEG signals and holds promise for improving the accuracy and reliability of epilepsy detection. Our source code will be released soon on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;Dir-GNN&#65292;&#24182;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10498</link><description>&lt;p&gt;
&#36793;&#26041;&#21521;&#24615;&#25552;&#39640;&#20102;&#24322;&#36136;&#22270;&#19978;&#30340;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Edge Directionality Improves Learning on Heterophilic Graphs. (arXiv:2305.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;Dir-GNN&#65292;&#24182;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#24314;&#27169;&#20851;&#31995;&#25968;&#25454;&#30340;&#20107;&#23454;&#26631;&#20934;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#26159;&#26377;&#21521;&#30340;&#65292;&#20294;&#20170;&#22825;&#22823;&#22810;&#25968;GNN&#27169;&#22411;&#37117;&#36890;&#36807;&#20351;&#22270;&#25104;&#20026;&#26080;&#21521;&#22270;&#26469;&#23436;&#20840;&#24573;&#30053;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#26679;&#20570;&#30340;&#21407;&#22240;&#26159;&#21382;&#21490;&#24615;&#30340;&#65306;1&#65289;&#35768;&#22810;&#26089;&#26399;&#30340;&#35889;GNN&#21464;&#20307;&#26126;&#30830;&#35201;&#27714;&#22270;&#26159;&#26080;&#21521;&#30340;&#65292;2&#65289;&#20851;&#20110;&#21516;&#31867;&#22270;&#30340;&#31532;&#19968;&#25209;&#22522;&#20934;&#27979;&#35797;&#24182;&#26410;&#21457;&#29616;&#20351;&#29992;&#26041;&#21521;&#24615;&#26377;&#26126;&#26174;&#30340;&#22686;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24322;&#31867;&#35774;&#32622;&#20013;&#65292;&#23558;&#22270;&#24418;&#35270;&#20026;&#26377;&#21521;&#22270;&#21487;&#20197;&#22686;&#21152;&#22270;&#30340;&#20869;&#22312;&#21516;&#36136;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#20174;&#27491;&#30830;&#20351;&#29992;&#26041;&#21521;&#24615;&#20449;&#24687;&#20013;&#21487;&#33021;&#24471;&#21040;&#30340;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Directed Graph Neural Network&#65288;Dir-GNN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#38754;&#21521;&#26377;&#21521;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#12290;Dir-GNN&#21487;&#20197;&#29992;&#20110;&#25193;&#23637;&#20219;&#20309;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#20197;&#36890;&#36807;&#23545;&#27599;&#20010;&#33410;&#28857;&#25191;&#34892;&#21333;&#29420;&#30340;&#36827;&#20986;&#28040;&#24687;&#32858;&#21512;&#26469;&#32771;&#34385;&#36793;&#26041;&#21521;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#35780;&#20272;&#20102;Dir-GNN&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26041;&#21521;&#24615;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;Dir-GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outg
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#29992;&#20110;&#24494;&#35843;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#21512;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;UCF-101&#21644;MSR-VTT&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10474</link><description>&lt;p&gt;
&#20445;&#30041;&#20320;&#33258;&#24049;&#30340;&#30456;&#20851;&#24615;&#65306;&#29992;&#20110;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#29992;&#20110;&#24494;&#35843;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#21512;&#25104;&#12290;&#32463;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;UCF-101&#21644;MSR-VTT&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#21512;&#25104;&#36830;&#32493;&#30340;&#21160;&#30011;&#24103;&#65292;&#26082;&#20855;&#26377;&#20809;&#30495;&#23454;&#24863;&#65292;&#21448;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#24615;&#20173;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#22312;&#21487;&#20197;&#20351;&#29992;&#25104;&#20159;&#32423;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21516;&#26102;&#65292;&#25910;&#38598;&#30456;&#20284;&#35268;&#27169;&#30340;&#35270;&#39057;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#22270;&#20687;&#23545;&#24212;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#35757;&#32451;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#30340;&#35745;&#31639;&#20195;&#20215;&#26356;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#39057;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#35270;&#39057;&#21512;&#25104;&#20219;&#21153;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35270;&#39057;&#25193;&#25955;&#20013;&#22825;&#30495;&#22320;&#23558;&#22270;&#20687;&#22122;&#22768;&#20808;&#39564;&#25193;&#23637;&#20026;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#35270;&#39057;&#22122;&#22768;&#20808;&#39564;&#65292;&#20854;&#22312;&#35270;&#39057;&#25193;&#25955;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#26356;&#22909;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411; Preserve Your Own Correlation (PYoCo) &#22312; UCF-101 &#21644; MSR-VTT &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#23545;&#35270;&#39057;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33647;&#29289;&#20998;&#23376;&#21103;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10473</link><description>&lt;p&gt;
&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33647;&#29289;&#20998;&#23376;&#30340;&#21103;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Predicting Side Effect of Drug Molecules using Recurrent Neural Networks. (arXiv:2305.10473v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10473
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#33647;&#29289;&#20998;&#23376;&#21103;&#20316;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#21644;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#23450;&#21644;&#39564;&#35777;&#20998;&#23376;&#23646;&#24615;&#65292;&#22914;&#21103;&#20316;&#29992;&#65292;&#26159;&#20998;&#23376;&#21512;&#25104;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#21644;&#32791;&#26102;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#36827;&#34892;&#21103;&#20316;&#29992;&#39044;&#27979;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#22797;&#26434;&#27169;&#22411;&#30340;&#38382;&#39064;&#24182;&#22823;&#22823;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identification and verification of molecular properties such as side effects is one of the most important and time-consuming steps in the process of molecule synthesis. For example, failure to identify side effects before submission to regulatory groups can cost millions of dollars and months of additional research to the companies. Failure to identify side effects during the regulatory review can also cost lives. The complexity and expense of this task have made it a candidate for a machine learning-based solution. Prior approaches rely on complex model designs and excessive parameter counts for side effect predictions. We believe reliance on complex models only shifts the difficulty away from chemists rather than alleviating the issue. Implementing large models is also expensive without prior access to high-performance computers. We propose a heuristic approach that allows for the utilization of simple neural networks, specifically the recurrent neural network, with a 98+% reduction 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20061;&#20010;&#25216;&#24039;&#26469;&#24110;&#21161;&#29983;&#24577;&#23398;&#23478;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#25216;&#24039;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24120;&#35265;&#38169;&#35823;&#12289;&#38519;&#38449;&#25110;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10472</link><description>&lt;p&gt;
&#29983;&#24577;&#23398;&#23478;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20061;&#20010;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Nine tips for ecologists using machine learning. (arXiv:2305.10472v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20061;&#20010;&#25216;&#24039;&#26469;&#24110;&#21161;&#29983;&#24577;&#23398;&#23478;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36825;&#20123;&#25216;&#24039;&#38024;&#23545;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#24120;&#35265;&#38169;&#35823;&#12289;&#38519;&#38449;&#25110;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#31934;&#24230;&#21644;&#28789;&#27963;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#20026;&#20102;&#29983;&#24577;&#23398;&#23478;&#21512;&#36866;&#19988;&#39640;&#25928;&#30340;&#24037;&#20855;&#12290;&#20294;&#26159;&#65292;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#27809;&#26377;&#22312;&#36825;&#20010;&#39046;&#22495;&#26377;&#32463;&#39564;&#30340;&#29983;&#24577;&#23398;&#23478;&#26469;&#35828;&#21487;&#33021;&#20250;&#26377;&#20123;&#38590;&#20197;&#25509;&#21463;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#25216;&#24039;&#26469;&#24110;&#21161;&#29983;&#24577;&#23398;&#23478;&#23454;&#26045;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#22240;&#20026;&#35768;&#22810;&#29983;&#24577;&#23398;&#30740;&#31350;&#26088;&#22312;&#23558;&#25968;&#25454;&#24402;&#20837;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65292;&#20363;&#22914;&#29983;&#24577;&#29366;&#24577;&#25110;&#29983;&#29289;&#23454;&#20307;&#12290;&#36825;&#20061;&#20010;&#25216;&#24039;&#20013;&#65292;&#27599;&#19968;&#20010;&#37117;&#25552;&#20986;&#20102;&#22312;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#30340;&#24120;&#35265;&#38169;&#35823;&#12289;&#38519;&#38449;&#25110;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#29983;&#24577;&#23398;&#30740;&#31350;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their high predictive performance and flexibility, machine learning models are an appropriate and efficient tool for ecologists. However, implementing a machine learning model is not yet a trivial task and may seem intimidating to ecologists with no previous experience in this area. Here we provide a series of tips to help ecologists in implementing machine learning models. We focus on classification problems as many ecological studies aim to assign data into predefined classes such as ecological states or biological entities. Each of the nine tips identifies a common error, trap or challenge in developing machine learning models and provides recommendations to facilitate their use in ecological studies.
&lt;/p&gt;</description></item><item><title>Bike2Vec&#26159;&#19968;&#31181;&#36890;&#36807;&#21382;&#21490;&#27604;&#36187;&#32467;&#26524;&#23398;&#20064;&#36710;&#25163;&#21644;&#27604;&#36187;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#36710;&#25163;&#21644;&#27604;&#36187;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.10471</link><description>&lt;p&gt;
Bike2Vec: &#36335;&#19978;&#33258;&#34892;&#36710;&#36710;&#25163;&#21644;&#27604;&#36187;&#30340;&#21521;&#37327;&#23884;&#20837;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Bike2Vec: Vector Embedding Representations of Road Cycling Riders and Races. (arXiv:2305.10471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10471
&lt;/p&gt;
&lt;p&gt;
Bike2Vec&#26159;&#19968;&#31181;&#36890;&#36807;&#21382;&#21490;&#27604;&#36187;&#32467;&#26524;&#23398;&#20064;&#36710;&#25163;&#21644;&#27604;&#36187;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#29992;&#20110;&#36710;&#25163;&#21644;&#27604;&#36187;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#23884;&#20837;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20197;&#33719;&#24471;&#38750;&#25968;&#23383;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#28982;&#21518;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#32844;&#19994;&#20844;&#36335;&#33258;&#34892;&#36710;&#39046;&#22495;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21521;&#37327;&#23884;&#20837;&#24212;&#29992;&#65292;&#36890;&#36807;&#23637;&#31034;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#32467;&#26524;&#23398;&#20064;&#36710;&#25163;&#21644;&#27604;&#36187;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#39564;&#35777;&#32467;&#26524;&#23884;&#20837;&#25429;&#33719;&#20102;&#36710;&#25163;&#21644;&#27604;&#36187;&#30340;&#26377;&#36259;&#29305;&#24449;&#12290;&#36825;&#20123;&#23884;&#20837;&#21487;&#29992;&#20110;&#35832;&#22914;&#26089;&#26399;&#20154;&#25165;&#35782;&#21035;&#21644;&#27604;&#36187;&#32467;&#26524;&#39044;&#27979;&#31561;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vector embeddings have been successfully applied in several domains to obtain effective representations of non-numeric data which can then be used in various downstream tasks. We present a novel application of vector embeddings in professional road cycling by demonstrating a method to learn representations for riders and races based on historical results. We use unsupervised learning techniques to validate that the resultant embeddings capture interesting features of riders and races. These embeddings could be used for downstream prediction tasks such as early talent identification and race outcome prediction.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10468</link><description>&lt;p&gt;
&#36830;&#25509;&#38544;&#34255;&#31070;&#32463;&#20803;&#65288;CHNNet&#65289;&#65306;&#19968;&#31181;&#24555;&#36895;&#25910;&#25947;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence. (arXiv:2305.10468v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24515;&#30446;&#30340;&#26159;&#27169;&#20223;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#25353;&#23618;&#27425;&#32467;&#26500;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#65292;&#22240;&#20026;&#21516;&#19968;&#23618;&#20013;&#30340;&#31070;&#32463;&#20803;&#20043;&#38388;&#27809;&#26377;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20114;&#30456;&#36830;&#25509;&#30340;&#65292;&#20351;&#24471;&#31070;&#32463;&#20803;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;&#32593;&#32476;&#20013;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20316;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23618;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core purpose of developing artificial neural networks was to mimic the functionalities of biological neural networks. However, unlike biological neural networks, traditional artificial neural networks are often structured hierarchically, which can impede the flow of information between neurons as the neurons in the same layer have no connections between them. Hence, we propose a more robust model of artificial neural networks where the hidden neurons, residing in the same hidden layer, are interconnected, enabling the neurons to learn complex patterns and speeding up the convergence rate. With the experimental study of our proposed model as fully connected layers in shallow and deep networks, we demonstrate that the model results in a significant increase in convergence rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#25968;&#37051;&#25509;&#30697;&#38453;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#32593;&#32476;&#30340;&#22810;&#20010;&#23646;&#24615;&#21644;&#25552;&#39640;&#32467;&#26524;&#65292;&#19988;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#22521;&#35757;&#25110;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.10467</link><description>&lt;p&gt;
&#20351;&#29992;&#36136;&#25968;&#37051;&#25509;&#30697;&#38453;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Analysing Biomedical Knowledge Graphs using Prime Adjacency Matrices. (arXiv:2305.10467v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36136;&#25968;&#37051;&#25509;&#30697;&#38453;&#20998;&#26512;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#32593;&#32476;&#30340;&#22810;&#20010;&#23646;&#24615;&#21644;&#25552;&#39640;&#32467;&#26524;&#65292;&#19988;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#22521;&#35757;&#25110;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#22823;&#22810;&#25968;&#29616;&#35937;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#34987;&#34920;&#36798;&#20026;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#26694;&#26550;&#8212;&#8212;Prime Adjacency Matrix&#65288;PAM&#65289;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;KG&#65292;&#23427;&#20801;&#35768;&#38750;&#24120;&#39640;&#25928;&#30340;&#32593;&#32476;&#20998;&#26512;&#12290;PAM&#21033;&#29992;&#36136;&#25968;&#26469;&#34920;&#31034;&#25972;&#20010;KG&#65292;&#24182;&#24555;&#36895;&#35745;&#31639;&#32593;&#32476;&#30340;&#22810;&#20010;&#23646;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#30740;&#31350;&#24182;&#25552;&#20379;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65306;&#19968;&#20010;&#26159;&#38024;&#23545;COVID-19&#30340;&#33647;&#29289;&#37325;&#22797;&#21033;&#29992;&#65292;&#21478;&#19968;&#20010;&#26159;&#37325;&#35201;&#30340;&#20803;&#36335;&#24452;&#25552;&#21462;&#30340;&#24212;&#29992;&#24615;&#26469;&#35828;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#27809;&#26377;&#20219;&#20309;&#30340;&#22521;&#35757;&#65292;&#21487;&#20197;&#22312;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#27604;&#21407;&#22987;&#30340;&#27969;&#31243;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most phenomena related to biomedical tasks are inherently complex, and in many cases, are expressed as signals on biomedical Knowledge Graphs (KGs). In this work, we introduce the use of a new representation framework, the Prime Adjacency Matrix (PAM) for biomedical KGs, which allows for very efficient network analysis. PAM utilizes prime numbers to enable representing the whole KG with a single adjacency matrix and the fast computation of multiple properties of the network. We illustrate the applicability of the framework in the biomedical domain by working on different biomedical knowledge graphs and by providing two case studies: one on drug-repurposing for COVID-19 and one on important metapath extraction. We show that we achieve better results than the original proposed workflows, using very simple methods that require no training, in considerably less time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#32473;&#23450;&#23569;&#37327;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25490;&#38500;&#24050;&#30693;&#30340;&#24322;&#24120;&#28857;&#65292;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10464</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#23569;&#37327;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Reconstruction Error-based Anomaly Detection with Few Outlying Examples. (arXiv:2305.10464v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#32473;&#23450;&#23569;&#37327;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25490;&#38500;&#24050;&#30693;&#30340;&#24322;&#24120;&#28857;&#65292;&#29992;&#20110;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#28145;&#24230;&#23398;&#20064;&#26816;&#27979;&#24322;&#24120;&#30340;&#26041;&#27861;&#65292;&#20854;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26469;&#37325;&#26500;&#20195;&#34920;&#27491;&#24120;&#25968;&#25454;&#30340;&#26679;&#26412;&#38598;&#65292;&#28982;&#21518;&#25351;&#20986;&#37027;&#20123;&#37325;&#26500;&#35823;&#24046;&#36275;&#22815;&#22823;&#30340;&#25968;&#25454;&#20026;&#24322;&#24120;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26500;&#24120;&#24120;&#33021;&#22815;&#24456;&#22909;&#22320;&#37325;&#26500;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;&#29305;&#21035;&#24403;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#24322;&#24120;&#24773;&#20917;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#26356;&#20026;&#26126;&#26174;&#12290;&#24403;&#36825;&#20123;&#24322;&#24120;&#24773;&#20917;&#26377;&#26631;&#31614;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#21322;&#30417;&#30563;&#65292;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#24573;&#30053;&#24322;&#24120;&#24773;&#20917;&#24182;&#22312;&#27491;&#24120;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#35753;&#22522;&#20110;&#37325;&#26500;&#35823;&#24046;&#30340;&#32467;&#26500;&#33021;&#22815;&#35753;&#27169;&#22411;&#23558;&#24050;&#30693;&#30340;&#24322;&#24120;&#28857;&#25490;&#38500;&#22312;&#27491;&#24120;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21033;&#29992;&#20102;&#23569;&#37327;&#24322;&#24120;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstruction error-based neural architectures constitute a classical deep learning approach to anomaly detection which has shown great performances. It consists in training an Autoencoder to reconstruct a set of examples deemed to represent the normality and then to point out as anomalies those data that show a sufficiently large reconstruction error. Unfortunately, these architectures often become able to well reconstruct also the anomalies in the data. This phenomenon is more evident when there are anomalies in the training set. In particular when these anomalies are labeled, a setting called semi-supervised, the best way to train Autoencoders is to ignore anomalies and minimize the reconstruction error on normal data. The goal of this work is to investigate approaches to allow reconstruction error-based architectures to instruct the model to put known anomalies outside of the domain description of the normal data. Specifically, our strategy exploits a limited number of anomalous e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#22330;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#30340;&#21021;&#22987;&#22330;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10460</link><description>&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#22330;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#20197;&#25552;&#39640;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Topology Optimization using Neural Networks with Conditioning Field Initialization for Improved Efficiency. (arXiv:2305.10460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#22330;&#21021;&#22987;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#30340;&#21021;&#22987;&#22330;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#26465;&#20214;&#22330;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#26088;&#22312;&#65288;1&#65289;&#25913;&#36827;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#65288;2&#65289;&#36890;&#36807;&#22312;&#26410;&#20248;&#21270;&#22495;&#19978;&#20351;&#29992;&#20808;&#21069;&#30340;&#21021;&#22987;&#22330;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#24314;&#31435;&#19968;&#20010;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65292;&#38024;&#23545;&#21333;&#20010;&#25299;&#25169;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20010;&#26696;&#35757;&#32451;&#65292;&#20197;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#12290;&#23427;&#20197;&#22495;&#22352;&#26631;&#20316;&#20026;&#36755;&#20837;&#65292;&#34920;&#24449;&#27599;&#20010;&#22352;&#26631;&#22788;&#30340;&#23494;&#24230;&#65292;&#20854;&#20013;&#25299;&#25169;&#30001;&#36830;&#32493;&#30340;&#23494;&#24230;&#22330;&#34920;&#31034;&#12290;&#20301;&#31227;&#36890;&#36807;&#26377;&#38480;&#20803;&#27714;&#35299;&#22120;&#27714;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;&#21021;&#22987;&#35774;&#35745;&#22495;&#19978;&#35745;&#31639;&#20986;&#30340;&#24212;&#21464;&#33021;&#22330;&#20316;&#20026;&#38468;&#21152;&#30340;&#26465;&#20214;&#22330;&#36755;&#20837;&#65292;&#22312;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#20013;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#12290;&#19982;&#21333;&#29420;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#24212;&#21464;&#33021;&#22330;&#30340;&#21152;&#20837;&#25552;&#39640;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose conditioning field initialization for neural network based topology optimization. In this work, we focus on (1) improving upon existing neural network based topology optimization, (2) demonstrating that by using a prior initial field on the unoptimized domain, the efficiency of neural network based topology optimization can be further improved. Our approach consists of a topology neural network that is trained on a case by case basis to represent the geometry for a single topology optimization problem. It takes in domain coordinates as input to represent the density at each coordinate where the topology is represented by a continuous density field. The displacement is solved through a finite element solver. We employ the strain energy field calculated on the initial design domain as an additional conditioning field input to the neural network throughout the optimization. The addition of the strain energy field input improves the convergence speed compared to standalone neura
&lt;/p&gt;</description></item><item><title>AnalogNAS&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#38024;&#23545;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#25512;&#29702;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#22312;&#36793;&#32536;&#29615;&#22659;&#19979;&#30340;&#21306;&#22495;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10459</link><description>&lt;p&gt;
AnalogNAS: &#19968;&#31181;&#29992;&#20110;&#37319;&#29992;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#23454;&#29616;&#20934;&#30830;&#25512;&#29702;&#30340;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnalogNAS: A Neural Network Design Framework for Accurate Inference with Analog In-Memory Computing. (arXiv:2305.10459v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10459
&lt;/p&gt;
&lt;p&gt;
AnalogNAS&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#38024;&#23545;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#25512;&#29702;&#21152;&#36895;&#22120;&#36827;&#34892;&#20248;&#21270;&#65292;&#25552;&#39640;&#22312;&#36793;&#32536;&#29615;&#22659;&#19979;&#30340;&#21306;&#22495;&#21644;&#21151;&#32791;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#31163;&#19981;&#24320;&#39640;&#25928;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#21644;&#26032;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;&#30446;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20027;&#35201;&#38024;&#23545;&#24120;&#35268;&#24179;&#21488;&#30340;&#36890;&#29992;&#37096;&#32626;&#12290;&#32780;&#22312;&#36793;&#32536;&#25512;&#29702;&#20013;&#65292;&#38656;&#35201;&#20302;&#24310;&#36831;&#12289;&#32039;&#20945;&#19988;&#21151;&#32791;&#25928;&#29575;&#39640;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#25104;&#26412;&#26041;&#38754;&#24517;&#39035;&#21010;&#31639;&#12290;&#22522;&#20110; typica von Neumann &#32467;&#26500;&#30340;&#25968;&#23383;&#22788;&#29702;&#22120;&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#20013;&#36816;&#29992;&#36739;&#20026;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#36827;&#20986;&#20869;&#23384;&#36827;&#34892;&#22788;&#29702;&#12290;&#30456;&#21453;&#65292;&#37319;&#29992;&#27169;&#25311;/&#28151;&#21512;&#20449;&#21495;&#20869;&#23384;&#35745;&#31639;&#30828;&#20214;&#21152;&#36895;&#22120;&#21487;&#20197;&#36731;&#26131;&#36229;&#36234; von Neumann &#32467;&#26500;&#20013;&#30340;&#20869;&#23384;&#22681;&#65292;&#21152;&#36895;&#25512;&#29702;&#24037;&#20316;&#36127;&#36733;&#12290;&#35813;&#30828;&#20214;&#21152;&#36895;&#22120;&#21487;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#21306;&#22495;&#21644;&#21151;&#32791;&#25928;&#29575;&#65292;&#36825;&#22312;&#36793;&#32536;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AnalogNAS &#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#38024;&#23545;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#25512;&#29702;&#21152;&#36895;&#22120;&#30340;&#37096;&#32626;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#30828;&#20214;&#20223;&#30495;&#20197;&#35780;&#20272;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Deep Learning (DL) is driven by efficient Deep Neural Network (DNN) design and new hardware accelerators. Current DNN design is primarily tailored for general-purpose use and deployment on commercially viable platforms. Inference at the edge requires low latency, compact and power-efficient models, and must be cost-effective. Digital processors based on typical von Neumann architectures are not conducive to edge AI given the large amounts of required data movement in and out of memory. Conversely, analog/mixed signal in-memory computing hardware accelerators can easily transcend the memory wall of von Neuman architectures when accelerating inference workloads. They offer increased area and power efficiency, which are paramount in edge resource-constrained environments. In this paper, we propose AnalogNAS, a framework for automated DNN design targeting deployment on analog In-Memory Computing (IMC) inference accelerators. We conduct extensive hardware simulations to d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#32467;&#26500;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22686;&#24378;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20877;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10457</link><description>&lt;p&gt;
&#38543;&#26426;&#21367;&#31215;&#26680;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Time Series Clustering With Random Convolutional Kernels. (arXiv:2305.10457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38543;&#26426;&#21367;&#31215;&#32467;&#26500;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22686;&#24378;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20877;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#21487;&#25551;&#36848;&#24191;&#27867;&#30340;&#33258;&#28982;&#21644;&#31038;&#20250;&#29616;&#35937;&#65292;&#22914;&#27668;&#20505;&#12289;&#22320;&#38663;&#12289;&#32929;&#31080;&#20215;&#26684;&#25110;&#32593;&#31449;&#35775;&#38382;&#36235;&#21183;&#12290;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26377;&#21161;&#20110;&#25214;&#21040;&#24322;&#24120;&#20540;&#65292;&#36825;&#20123;&#24322;&#24120;&#20540;&#21487;&#33021;&#20195;&#34920;&#28201;&#24230;&#24322;&#24120;&#12289;&#28779;&#23665;&#29190;&#21457;&#12289;&#24066;&#22330;&#24178;&#25200;&#25110;&#27450;&#35784;&#24615;&#32593;&#31449;&#27969;&#37327;&#12290;&#22522;&#20110;&#33258;&#21160;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#37319;&#29992;&#38543;&#26426;&#26680;&#25216;&#26415;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#19968;&#20010;&#38543;&#26426;&#21367;&#31215;&#32467;&#26500;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#22686;&#24378;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#32858;&#31867;&#31639;&#27861;&#23545;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#22522;&#20934;&#19978;&#25913;&#21892;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series can describe a wide range of natural and social phenomena. A few samples are climate and seismic measures trends, stock prices, or website visits. Time-series clustering helps to find outliers that, related to these instances, could represent temperature anomalies, imminent volcanic eruptions, market disturbances, or fraudulent web traffic. Founded on the success of automatic feature extraction techniques, specifically employing random kernels, we develop a new method for time series clustering consisting of two steps. First, a random convolutional structure transforms the data into an enhanced feature representation. Afterwards, a clustering algorithm classifies the transformed data. The method improves state-of-the-art results on time series clustering benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#25361;&#25112;&#26041;&#26696;&#30340;&#38480;&#21046;&#19979;&#35780;&#20272;&#19981;&#21516;&#31454;&#20105;&#32773;(&#31639;&#27861;)&#34920;&#29616;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20351;&#29992;&#21807;&#19968;&#30340;&#25968;&#25454;&#38598;&#12289;&#35268;&#23450;&#26368;&#23567;&#25552;&#20132;&#27425;&#25968;&#21644;&#19968;&#32452;&#24615;&#33021;&#25351;&#26631;&#31561;&#25514;&#26045;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#20123;&#24046;&#24322;&#33021;&#21542;&#23545;&#26368;&#32456;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10452</link><description>&lt;p&gt;
&#23545;&#27604;&#20998;&#31867;&#22120;&#22312;&#25361;&#25112;&#26041;&#26696;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Comparison of classifiers in challenge scheme. (arXiv:2305.10452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#25361;&#25112;&#26041;&#26696;&#30340;&#38480;&#21046;&#19979;&#35780;&#20272;&#19981;&#21516;&#31454;&#20105;&#32773;(&#31639;&#27861;)&#34920;&#29616;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20351;&#29992;&#21807;&#19968;&#30340;&#25968;&#25454;&#38598;&#12289;&#35268;&#23450;&#26368;&#23567;&#25552;&#20132;&#27425;&#25968;&#21644;&#19968;&#32452;&#24615;&#33021;&#25351;&#26631;&#31561;&#25514;&#26045;&#12290;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#20123;&#24046;&#24322;&#33021;&#21542;&#23545;&#26368;&#32456;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#25361;&#25112;&#26041;&#26696;&#20316;&#20026;&#19968;&#31181;&#20247;&#21253;&#26426;&#21046;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35828;&#65292;&#25361;&#25112;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#25361;&#25112;&#26041;&#26696;&#30340;&#38480;&#21046;&#19979;&#35780;&#20272;&#19981;&#21516;&#31454;&#20105;&#32773;&#65288;&#31639;&#27861;&#65289;&#34920;&#29616;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20351;&#29992;&#21807;&#19968;&#30340;&#25968;&#25454;&#38598;&#65288;&#22823;&#23567;&#22266;&#23450;&#65289;&#12289;&#35268;&#23450;&#26368;&#23567;&#25552;&#20132;&#27425;&#25968;&#21644;&#19968;&#32452;&#24615;&#33021;&#25351;&#26631;&#31561;&#25514;&#26045;&#12290;&#20998;&#31867;&#22120;&#25353;&#24615;&#33021;&#25351;&#26631;&#25490;&#24207;&#65292;&#20294;&#24120;&#24120;&#21457;&#29616;&#31454;&#20105;&#23545;&#25163;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#24494;&#20046;&#20854;&#24494;&#65292;&#29978;&#33267;&#21482;&#26377;&#21315;&#20998;&#20043;&#19968;&#24038;&#21491;&#12290;&#22240;&#27492;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#36825;&#20123;&#24046;&#24322;&#33021;&#21542;&#23545;&#26368;&#32456;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent decades, challenges have become very popular in scientific research as these are crowdsourcing schemes. In particular, challenges are essential for developing machine learning algorithms. For the challenges settings, it is vital to establish the scientific question, the dataset (with adequate quality, quantity, diversity, and complexity), performance metrics, as well as a way to authenticate the participants' results (Gold Standard). This paper addresses the problem of evaluating the performance of different competitors (algorithms) under the restrictions imposed by the challenge scheme, such as the comparison of multiple competitors with a unique dataset (with fixed size), a minimal number of submissions and, a set of metrics chosen to assess performance. The algorithms are sorted according to the performance metric. Still, it is common to observe performance differences among competitors as small as hundredths or even thousandths, so the question is whether the differences 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#65288;GDS&#65289;&#65292;&#26412;&#25991;&#25506;&#31350;&#20102;&#38543;&#26426;&#25506;&#32034;&#12289;&#21322;&#33258;&#21160;&#25506;&#32034;&#21644;&#33258;&#21160;&#25506;&#32034;&#19977;&#31181;&#19981;&#21516;&#27169;&#24335;&#30340;GDS&#25506;&#32034;&#26041;&#27861;&#22312;&#33337;&#33334;&#35774;&#35745;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10451</link><description>&lt;p&gt;
&#26426;&#26500;&#22914;&#20309;&#24433;&#21709;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#21516;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65311;&#20197;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#33337;&#33334;&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does agency impact human-AI collaborative design space exploration? A case study on ship design with deep generative models. (arXiv:2305.10451v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10451
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#65288;GDS&#65289;&#65292;&#26412;&#25991;&#25506;&#31350;&#20102;&#38543;&#26426;&#25506;&#32034;&#12289;&#21322;&#33258;&#21160;&#25506;&#32034;&#21644;&#33258;&#21160;&#25506;&#32034;&#19977;&#31181;&#19981;&#21516;&#27169;&#24335;&#30340;GDS&#25506;&#32034;&#26041;&#27861;&#22312;&#33337;&#33334;&#35774;&#35745;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#21442;&#25968;&#21270;&#26041;&#27861;&#36890;&#36807;&#26681;&#25454;&#22522;&#30784;&#35774;&#35745;&#29983;&#25104;&#21464;&#21270;&#26469;&#38480;&#21046;&#23545;&#22810;&#26679;&#21270;&#35774;&#35745;&#30340;&#25506;&#32034;&#12290;&#30456;&#21453;&#65292;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#35774;&#35745;&#21019;&#24314;&#32039;&#20945;&#19988;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#35774;&#35745;&#31354;&#38388;&#65288;GDS&#65289;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25506;&#32034;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;GDS&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#33337;&#20307;&#35774;&#35745;&#20013;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26500;&#24314;&#20102;&#19968;&#20010;GDS&#65292;&#35813;&#32593;&#32476;&#20351;&#29992;&#21508;&#31181;&#31867;&#22411;&#33337;&#33334;&#30340;52,591&#31181;&#35774;&#35745;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19977;&#31181;&#25506;&#32034;&#27169;&#24335;&#65306;&#38543;&#26426;&#25506;&#32034;&#65288;REM&#65289;&#12289;&#21322;&#33258;&#21160;&#25506;&#32034;&#65288;SAEM&#65289;&#21644;&#33258;&#21160;&#25506;&#32034;&#65288;AEM&#65289;&#65292;&#20854;&#20013;&#29992;&#25143;&#30340;&#21442;&#19982;&#31243;&#24230;&#21508;&#19981;&#30456;&#21516;&#65292;&#20197;&#25506;&#32034;GDS&#24182;&#33719;&#21462;&#26032;&#39062;&#19988;&#20248;&#21270;&#30340;&#35774;&#35745;&#12290;&#22312;REM&#20013;&#65292;&#29992;&#25143;&#22522;&#20110;&#30452;&#35273;&#25163;&#21160;&#25506;&#32034;GDS&#12290;&#22312;SAEM&#20013;&#65292;&#29992;&#25143;&#21644;&#20248;&#21270;&#22120;&#22343;&#39537;&#21160;&#25506;&#32034;&#12290;&#20248;&#21270;&#22120;&#19987;&#27880;&#20110;&#25506;&#32034;&#22810;&#26679;&#21270;&#30340;&#20248;&#21270;&#35774;&#35745;&#65292;&#32780;&#29992;&#25143;&#23558;&#25506;&#32034;&#37325;&#28857;&#25918;&#22312;&#20854;&#33258;&#24049;&#30340;&#35774;&#35745;&#20559;&#22909;&#19978;&#12290;AEM&#20351;&#29992;&#20248;&#21270;&#22120;&#25506;&#32034;GDS&#20197;&#29983;&#25104;&#20248;&#21270;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical parametric approaches restrict the exploration of diverse designs by generating variations based on a baseline design. In contrast, generative models provide a solution by leveraging existing designs to create compact yet diverse generative design spaces (GDSs). However, the effectiveness of current exploration methods in complex GDSs, especially in ship hull design, remains unclear. To that end, we first construct a GDS using a generative adversarial network, trained on 52,591 designs of various ship types. Next, we constructed three modes of exploration, random (REM), semi-automated (SAEM) and automated (AEM), with varying levels of user involvement to explore GDS for novel and optimised designs. In REM, users manually explore the GDS based on intuition. In SAEM, both the users and optimiser drive the exploration. The optimiser focuses on exploring a diverse set of optimised designs, while the user directs the exploration towards their design preference. AEM uses an optimiser
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#65292;&#22312;MIT-BIH&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;93.3%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10450</link><description>&lt;p&gt;
&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29702;&#35299;&#27491;&#24120;&#21644;&#24322;&#24120;&#24515;&#33039;
&lt;/p&gt;
&lt;p&gt;
Understanding of Normal and Abnormal Hearts by Phase Space Analysis and Convolutional Neural Networks. (arXiv:2305.10450v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10450
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#65292;&#22312;MIT-BIH&#25968;&#25454;&#24211;&#19978;&#21462;&#24471;&#20102;93.3%&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30142;&#30149;&#26159;&#29616;&#20195;&#24037;&#19994;&#21270;&#31038;&#20250;&#20013;&#33268;&#27515;&#22240;&#32032;&#20043;&#19968;&#65292;&#23548;&#33268;&#20844;&#20849;&#21355;&#29983;&#31995;&#32479;&#30340;&#39640;&#26114;&#24320;&#25903;&#12290;&#22240;&#39640;&#26114;&#25104;&#26412;&#65292;&#24320;&#21457;&#20998;&#26512;&#26041;&#27861;&#20197;&#25913;&#21892;&#24515;&#33039;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#23558;&#24515;&#33039;&#30005;&#27963;&#21160;&#24314;&#27169;&#65292;&#24182;&#30740;&#31350;&#36215;&#28304;&#20110;&#30830;&#23450;&#24615;&#21160;&#24577;&#30340;&#24515;&#33039;&#39057;&#35889;&#30340;&#21464;&#21270;&#12290;&#23558;&#26102;&#38388;&#24207;&#21015;&#24515;&#30005;&#22270;(ECG)&#22270;&#20687;&#25552;&#21462;&#30456;&#31354;&#38388;&#36712;&#36857;&#65292;&#24182;&#22522;&#20110; MIT-BIH &#25968;&#25454;&#24211;&#20013;&#35760;&#24405;&#30340; 44 &#20010; MLII &#22270;&#20687;&#24212;&#29992;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#23558;&#30456;&#31354;&#38388;&#35760;&#24405;&#30340;&#26368;&#39640; Q-R &#36317;&#31163;&#20043;&#38388;&#30011;&#19968;&#26465;&#30452;&#32447;&#12290;&#23545;&#30456;&#31354;&#38388;&#22270;&#20687;&#35757;&#32451;&#20108;&#36827;&#21046; CNN &#20998;&#31867;&#27169;&#22411;&#20197;&#20998;&#31867;&#27491;&#24120;&#21644;&#24322;&#24120;&#24515;&#33039;&#65292;&#35813;&#26041;&#27861;&#36798;&#21040;&#24179;&#22343;&#20934;&#30830;&#29575; 93.3%&#65292;&#35777;&#26126;&#20102;&#30456;&#31354;&#38388;&#20998;&#26512;&#21644; CNN &#22343;&#22312;&#24515;&#33039;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiac diseases are one of the leading mortality factors in modern, industrialized societies, which cause high expenses in public health systems. Due to high costs, developing analytical methods to improve cardiac diagnostics is essential. The heart's electric activity was first modeled using a set of nonlinear differential equations. Following this, variations of cardiac spectra originating from deterministic dynamics are investigated. Analyzing a normal human heart's power spectra offers His-Purkinje network, which possesses a fractal-like structure. Phase space trajectories are extracted from the time series electrocardiogram (ECG) graph with third-order derivate Taylor Series. Here in this study, phase space analysis and Convolutional Neural Networks (CNNs) method are applied to 44 records via the MIT-BIH database recorded with MLII. In order to increase accuracy, a straight line is drawn between the highest Q-R distance in the phase space images of the records. Binary CNN classif
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10449</link><description>&lt;p&gt;
&#21512;&#20316;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#12290; &#65288;arXiv:2305.10449v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Cooperation Is All You Need. (arXiv:2305.10449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10449
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#36234;&#8220;&#26641;&#31361;&#27665;&#20027;&#8221;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Cooperator&#30340;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;Transformers&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;ChatGPT&#65289;&#22312;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#21151;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290; Transformers&#22522;&#20110;&#38271;&#26399;&#20197;&#26469;&#30340;&#8220;&#31215;&#20998;-&#21457;&#23556;&#8221;&#8220;&#28857;&#8221;&#31070;&#32463;&#20803;&#30340;&#27010;&#24565;&#65292;&#32780;Cooperator&#21017;&#21463;&#21040;&#26368;&#36817;&#31070;&#32463;&#29983;&#29289;&#23398;&#31361;&#30772;&#30340;&#21551;&#31034;&#65292;&#36825;&#20123;&#31361;&#30772;&#34920;&#26126;&#65292;&#31934;&#31070;&#29983;&#27963;&#30340;&#32454;&#32990;&#22522;&#30784;&#21462;&#20915;&#20110;&#26032;&#30382;&#23618;&#20013;&#20855;&#26377;&#20004;&#20010;&#21151;&#33021;&#19978;&#19981;&#21516;&#28857;&#30340;&#19978;&#30382;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29992;&#20110;RL&#26102;&#65292;&#22522;&#20110;Cooperator&#30340;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#27604;&#22522;&#20110;Transformer&#30340;&#31639;&#27861;&#24555;&#24471;&#22810;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10442</link><description>&lt;p&gt;
CBAGAN-RRT: &#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning. (arXiv:2305.10442v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;RRT&#31639;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#21021;&#22987;&#36335;&#24452;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#19988;&#25910;&#25947;&#36895;&#24230;&#36807;&#24930;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#65292;&#20197;&#35774;&#35745;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;GAN&#27169;&#22411;&#29983;&#25104;&#30340;&#36335;&#24452;&#27010;&#29575;&#20998;&#24067;&#29992;&#20110;&#24341;&#23548;RRT&#31639;&#27861;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30001; \cite {zhang2021generative} &#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#65288;&#22914;IOU&#20998;&#25968;&#65292;Dice&#20998;&#25968;&#65289;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#65288;&#22914;&#36335;&#24452;&#38271;&#24230;&#21644;&#25104;&#21151;&#29575;&#65289;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10434</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#20250;&#22312;&#20154;&#20204;&#30340;&#33041;&#28023;&#20013;&#21576;&#29616;&#22270;&#20687;&#65292;&#32780;&#38750;&#35270;&#35273;&#25991;&#26412;&#21017;&#26080;&#27861;&#36798;&#21040;&#27492;&#25928;&#26524;&#12290;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#23558;&#26377;&#21161;&#20110;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3620&#20010;&#33521;&#35821;&#21477;&#23376;&#21450;&#20854;&#22810;&#20010;&#20154;&#31867;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#35270;&#35273;&#24615;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#25991;&#26412;&#21644;&#35270;&#35273;&#36164;&#20135;&#30340;&#25991;&#26723;&#26469;&#21019;&#24314;&#36828;&#31243;&#30417;&#30563;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDAC&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#37319;&#29992;Vision Transformer&#65288;ViT&#65289;&#26694;&#26550;&#25552;&#21462;&#21487;&#36866;&#24212;&#29305;&#24449;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;FDAC&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10432</link><description>&lt;p&gt;
&#27169;&#22411;&#23545;&#27604;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Model-Contrastive Federated Domain Adaptation. (arXiv:2305.10432v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDAC&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;&#37319;&#29992;Vision Transformer&#65288;ViT&#65289;&#26694;&#26550;&#25552;&#21462;&#21487;&#36866;&#24212;&#29305;&#24449;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;FDAC&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;FDA&#65289;&#26088;&#22312;&#23558;&#30693;&#35782;&#20174;&#28304;&#23458;&#25143;&#31471;&#65288;&#39046;&#22495;&#65289;&#21512;&#20316;&#22320;&#36716;&#31227;&#21040;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#30446;&#26631;&#23458;&#25143;&#31471;&#65292;&#32780;&#19981;&#38656;&#35201;&#20256;&#36882;&#20219;&#20309;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#28304;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#23548;&#33268;&#30693;&#35782;&#20256;&#36882;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;FDA&#21462;&#24471;&#20102;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20294;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#21033;&#29992;&#24322;&#26500;&#39046;&#22495;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#26080;&#27861;&#23454;&#29616;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FDAC&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21644;Vision Transformer&#65288;ViT&#65289;&#35299;&#20915;&#22522;&#20110;&#32852;&#37030;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#35757;&#32451;&#20986;&#20248;&#31168;&#30340;&#27169;&#22411;&#65292;&#32780;ViT&#26550;&#26500;&#22312;&#25552;&#21462;&#36866;&#24212;&#29305;&#24449;&#26041;&#38754;&#27604;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#26356;&#22909;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FDAC&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22312;&#32852;&#37030;&#35774;&#32622;&#20013;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#26469;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;FDAC&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;FDA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated domain adaptation (FDA) aims to collaboratively transfer knowledge from source clients (domains) to the related but different target client, without communicating the local data of any client. Moreover, the source clients have different data distributions, leading to extremely challenging in knowledge transfer. Despite the recent progress in FDA, we empirically find that existing methods can not leverage models of heterogeneous domains and thus they fail to achieve excellent performance. In this paper, we propose a model-based method named FDAC, aiming to address {\bf F}ederated {\bf D}omain {\bf A}daptation based on {\bf C}ontrastive learning and Vision Transformer (ViT). In particular, contrastive learning can leverage the unlabeled data to train excellent models and the ViT architecture performs better than convolutional neural networks (CNNs) in extracting adaptable features. To the best of our knowledge, FDAC is the first attempt to learn transferable representations by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.10089</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20013;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#27169;&#20223;&#19987;&#23478;&#30340;&#22870;&#21169;&#20540;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35789;&#20856;&#24207;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35753;&#23398;&#20064;&#32773;&#30340;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.09860</link><description>&lt;p&gt;
Epsilon Sampling Rocks: &#30740;&#31350;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#24050;&#32463;&#26174;&#31034;&#20986;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26102;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;MBR&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20363;&#22914;&#31062;&#20808;&#37319;&#26679;&#65292;&#26680;&#37319;&#26679;&#21644;top-k&#37319;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#23427;&#20204;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;epsilon&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25152;&#26377;&#23567;&#20110;epsilon&#30340;&#26631;&#35760;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#33719;&#24471;&#20844;&#24179;&#30340;&#27010;&#29575;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;epsilon&#37319;&#26679;&#30340;MBR&#35299;&#30721;&#26174;&#33879;&#20248;&#20110;&#19981;&#20165;&#26159;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#32780;&#19988;&#36824;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;MBR&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.09557</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#23398;&#20064;&#65306;&#31934;&#36873;&#21253;&#19982;&#38543;&#26426;&#21253;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Aggregated Data: Curated Bags versus Random Bags. (arXiv:2305.09557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#65292;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#37096;&#32626;&#30340;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#36825;&#20123;&#31995;&#32479;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#38382;&#39064;&#65292;&#19968;&#31181;&#26041;&#27861;&#26159;&#20197;&#32858;&#21512;&#30340;&#24418;&#24335;&#25910;&#38598;&#21644;&#21457;&#24067;&#25968;&#25454;&#26631;&#31614;&#65292;&#20174;&#32780;&#21487;&#20197;&#23558;&#21333;&#20010;&#29992;&#25143;&#30340;&#20449;&#24687;&#19982;&#20854;&#20182;&#29992;&#25143;&#30340;&#20449;&#24687;&#32452;&#21512;&#36215;&#26469;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#21512;&#25968;&#25454;&#26631;&#31614;&#32780;&#38750;&#21333;&#20010;&#26631;&#31614;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#33258;&#28982;&#30340;&#32858;&#21512;&#26041;&#27861;&#65306;&#22522;&#20110;&#20849;&#21516;&#29305;&#24449;&#23558;&#25968;&#25454;&#28857;&#20998;&#32452;&#30340;&#31934;&#36873;&#21253;&#21644;&#23558;&#25968;&#25454;&#28857;&#38543;&#26426;&#20998;&#32452;&#30340;&#38543;&#26426;&#21253;&#12290;&#23545;&#20110;&#31934;&#36873;&#21253;&#35774;&#32622;&#21644;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#32780;&#19981;&#20250;&#23548;&#33268;&#25968;&#25454;&#32858;&#21512;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20197;&#19979;&#35266;&#23519;&#65306;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#20043;&#21644;&#21487;&#20197;&#34920;&#31034;&#20026;&#27599;&#20010;&#21253;&#30340;&#26799;&#24230;&#30340;&#21152;&#26435;&#21644;&#65292;&#20854;&#20013;&#26435;&#37325;&#26159;&#21253;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss functio
&lt;/p&gt;</description></item><item><title>&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#22826;&#38451;&#32768;&#26001;&#26631;&#31614;&#12290;&#23427;&#21487;&#29992;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#28436;&#21270;&#20197;&#21450;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#20110;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.09492</link><description>&lt;p&gt;
&#31354;&#38388;&#22825;&#27668;&#30740;&#31350;&#20013;&#30340;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22330;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Solar Active Region Magnetogram Image Dataset for Studies of Space Weather. (arXiv:2305.09492v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#22826;&#38451;&#32768;&#26001;&#26631;&#31614;&#12290;&#23427;&#21487;&#29992;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#28436;&#21270;&#20197;&#21450;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#20110;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#32654;&#22269;&#22269;&#23478;&#33322;&#31354;&#33322;&#22825;&#23616;&#65288;NASA&#65289;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#30340;&#30913;&#22270;&#65288;&#34913;&#37327;&#30913;&#22330;&#24378;&#24230;&#30340;&#22270;&#20687;&#65289;&#30340;&#20840;&#38754;&#25910;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;&#19977;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;SDO&#22320;&#38663;&#23398;&#21644;&#30913;&#23398;&#20202;&#65288;HMI&#65289;&#22826;&#38451;&#27963;&#36291;&#21306;&#65288;&#22823;&#30913;&#36890;&#21306;&#22495;&#65292;&#36890;&#24120;&#26159;&#29190;&#21457;&#20107;&#20214;&#30340;&#28304;&#22836;&#65289;&#30340;&#30913;&#22270;&#20197;&#21450;&#30456;&#24212;&#32768;&#26001;&#27963;&#21160;&#30340;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#20197;&#21450;&#19982;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#30340;&#22270;&#20687;&#20998;&#26512;&#25110;&#22826;&#38451;&#29289;&#29702;&#23398;&#30740;&#31350;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#23545;&#37027;&#20123;&#30740;&#31350;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20154;&#21592;&#20135;&#29983;&#20852;&#36259;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#32463;&#20856;&#21644;&#28145;&#24230;&#65289;&#12289;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#20197;&#21450;&#22238;&#24402;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26368;&#23567;&#22788;&#29702;&#19988;&#29992;&#25143;&#21487;&#37197;&#32622;&#30340;&#19968;&#33268;&#22823;&#23567;&#22826;&#38451;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this dataset we provide a comprehensive collection of magnetograms (images quantifying the strength of the magnetic field) from the National Aeronautics and Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The dataset incorporates data from three sources and provides SDO Helioseismic and Magnetic Imager (HMI) magnetograms of solar active regions (regions of large magnetic flux, generally the source of eruptive events) as well as labels of corresponding flaring activity. This dataset will be useful for image analysis or solar physics research related to magnetic structure, its evolution over time, and its relation to solar flares. The dataset will be of interest to those researchers investigating automated solar flare prediction methods, including supervised and unsupervised machine learning (classical and deep), binary and multi-class classification, and regression. This dataset is a minimally processed, user configurable dataset of consistently sized images of sola
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#31574;&#30053;&#25511;&#21046;&#65292;&#21487;&#20197;&#20445;&#38556;&#32852;&#21512;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#30830;&#20445;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#21516;&#26102;&#21487;&#20197;&#23457;&#35745;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.09134</link><description>&lt;p&gt;
&#20445;&#38556;&#32852;&#21512;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#23433;&#20840;&#30340;&#26234;&#33021;&#31574;&#30053;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Smart Policy Control for Securing Federated Learning Management System. (arXiv:2305.09134v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#31574;&#30053;&#25511;&#21046;&#65292;&#21487;&#20197;&#20445;&#38556;&#32852;&#21512;&#23398;&#20064;&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#30830;&#20445;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#21516;&#26102;&#21487;&#20197;&#23457;&#35745;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#26234;&#24935;&#22478;&#24066;&#12289;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#21644;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#23548;&#33268;&#20102;&#22823;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#24120;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#22810;&#20010;&#21442;&#19982;&#32773;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27599;&#20010;FL&#21442;&#19982;&#32773;&#23454;&#26045;&#20102;&#21508;&#31181;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#24403;&#21069;&#30340;FL&#26550;&#26500;&#19981;&#20801;&#35768;&#23545;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#23457;&#35745;&#12290;&#21478;&#22806;&#65292;&#24403;&#21069;&#26550;&#26500;&#20013;&#27809;&#26377;&#20840;&#23616;&#27169;&#22411;&#21487;&#39564;&#35777;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#31574;&#30053;&#25511;&#21046;&#26469;&#20445;&#38556;FL&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;FL&#21442;&#19982;&#32773;&#20391;&#24320;&#21457;&#21644;&#37096;&#32626;&#22522;&#20110;&#26234;&#33021;&#21512;&#32422;&#30340;&#26412;&#22320;&#35757;&#32451;&#31574;&#30053;&#25511;&#21046;&#12290;&#36825;&#20010;&#31574;&#30053;&#25511;&#21046;&#34987;&#29992;&#26469;&#39564;&#35777;&#35757;&#32451;&#36807;&#31243;&#65292;&#30830;&#20445;&#27599;&#20010;FL&#21442;&#19982;&#32773;&#37117;&#36981;&#23432;&#25968;&#25454;&#20445;&#25252;&#25919;&#31574;&#65292;&#20174;&#32780;&#23454;&#29616;FL&#31649;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of Internet of Things (IoT) devices in smart cities, intelligent healthcare systems, and various real-world applications have resulted in the generation of vast amounts of data, often analyzed using different Machine Learning (ML) models. Federated learning (FL) has been acknowledged as a privacy-preserving machine learning technology, where multiple parties cooperatively train ML models without exchanging raw data. However, the current FL architecture does not allow for an audit of the training process due to the various data-protection policies implemented by each FL participant. Furthermore, there is no global model verifiability available in the current architecture. This paper proposes a smart contract-based policy control for securing the Federated Learning (FL) management system. First, we develop and deploy a smart contract-based local training policy control on the FL participants' side. This policy control is used to verify the training process, ensuri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Tractography&#20013;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;RL&#31639;&#27861;&#36873;&#25321;&#12289;&#20195;&#29702;&#20154;&#36755;&#20837;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#25773;&#31181;&#31574;&#30053;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.09041</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;Tractography&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
What Matters in Reinforcement Learning for Tractography. (arXiv:2305.09041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;Tractography&#20013;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#25552;&#20986;&#20102;&#20851;&#20110;RL&#31639;&#27861;&#36873;&#25321;&#12289;&#20195;&#29702;&#20154;&#36755;&#20837;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#25773;&#31181;&#31574;&#30053;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#23398;&#20064;Tractography&#36807;&#31243;&#24182;&#35757;&#32451;&#20195;&#29702;&#20154;&#22312;&#27809;&#26377;&#25163;&#21160;&#31579;&#36873;&#30340;&#21442;&#32771;&#27969;&#32447;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#30333;&#36136;&#32467;&#26500;&#12290;&#34429;&#28982;&#25253;&#21578;&#30340;&#34920;&#29616;&#39047;&#20855;&#31454;&#20105;&#21147;&#65292;&#20294;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22797;&#26434;&#65292;&#24182;&#19988;&#23545;&#20110;&#20854;&#22810;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#21644;&#24433;&#21709;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65292;&#20363;&#22914;RL&#31639;&#27861;&#30340;&#36873;&#25321;&#65292;&#25773;&#31181;&#31574;&#30053;&#65292;&#36755;&#20837;&#20449;&#21495;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#24182;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#24433;&#21709;&#12290;&#26412;&#27425;&#30740;&#31350;&#20849;&#35757;&#32451;&#20102;&#32422;7400&#20010;&#27169;&#22411;&#65292;&#20849;&#35745;&#36817;41000&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25351;&#23548;&#28909;&#34935;&#20110;&#25506;&#32034;&#28145;&#24230;RL&#22312;Tractography&#20013;&#21487;&#33021;&#24615;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26368;&#32456;&#25552;&#20986;&#20102;&#20851;&#20110;RL&#31639;&#27861;&#30340;&#36873;&#25321;&#12289;&#20195;&#29702;&#20154;&#36755;&#20837;&#12289;&#22870;&#21169;&#20989;&#25968;&#21644;&#25773;&#31181;&#31574;&#30053;&#30340;&#19968;&#31995;&#21015;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning (RL) has been proposed to learn the tractography procedure and train agents to reconstruct the structure of the white matter without manually curated reference streamlines. While the performances reported were competitive, the proposed framework is complex, and little is still known about the role and impact of its multiple parts. In this work, we thoroughly explore the different components of the proposed framework, such as the choice of the RL algorithm, seeding strategy, the input signal and reward function, and shed light on their impact. Approximately 7,400 models were trained for this work, totalling nearly 41,000 hours of GPU time. Our goal is to guide researchers eager to explore the possibilities of deep RL for tractography by exposing what works and what does not work with the category of approach. As such, we ultimately propose a series of recommendations concerning the choice of RL algorithm, the input to the agents, the reward function
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2305.08099</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#35299;&#32806;&#35821;&#38899;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Neural Factor Analysis for Disentangling Utterance-level Speech Representations. (arXiv:2305.08099v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#31070;&#32463;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#24050;&#32463;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#20302;&#26631;&#27880;&#36164;&#28304;&#24773;&#20917;&#19979;&#35777;&#26126;&#38750;&#24120;&#26377;&#29992;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#25216;&#26415;&#22312;&#35828;&#35805;&#20154;&#12289;&#24773;&#24863;&#21644;&#35821;&#35328;&#35782;&#21035;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#38382;&#39064;&#36827;&#34892;&#20102;&#25506;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#20998;&#26512;&#27169;&#22411;&#65292;&#20351;&#29992;HuBERT&#20013;&#30340;&#32858;&#31867;&#26041;&#27861;&#26469;&#21457;&#29616;&#38544;&#34255;&#30340;&#22768;&#23398;&#21333;&#20803;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#20803;&#23545;&#40784;SSL&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#20135;&#29983;&#35299;&#32806;&#21518;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#20026;&#19987;&#38376;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;Utterance&#27700;&#24179;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SSNFA&#27169;&#22411;&#22312;&#35828;&#35805;&#20154;&#35782;&#21035;&#12289;&#35821;&#35328;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#22343;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;SSL&#27169;&#22411;&#65292;&#24182;&#19988;&#27809;&#26377;&#20219;&#20309;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#25110;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) speech models such as wav2vec and HuBERT have demonstrated state-of-the-art performance on automatic speech recognition (ASR) and proved to be extremely useful in low label-resource settings. However, the success of SSL models has yet to transfer to utterance-level tasks such as speaker, emotion, and language recognition, which still require supervised fine-tuning of the SSL models to obtain good performance. We argue that the problem is caused by the lack of disentangled representations and an utterance-level learning objective for these tasks. Inspired by how HuBERT uses clustering to discover hidden acoustic units, we formulate a factor analysis (FA) model that uses the discovered hidden acoustic units to align the SSL features. The underlying utterance-level representations are disentangled from the content of speech using probabilistic inference on the aligned features. Furthermore, the variational lower bound derived from the FA model provides an ut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2305.08018</link><description>&lt;p&gt;
DRew&#65306;&#24102;&#24310;&#36831;&#30340;&#21160;&#24577;&#37325;&#36830;&#28040;&#24687;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
DRew: Dynamically Rewired Message Passing with Delay. (arXiv:2305.08018v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#32467;&#26500;&#30340;&#26694;&#26550;&#65292;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#21160;&#24577;&#37325;&#36830;&#26469;&#30830;&#20445;&#36880;&#28176;&#23494;&#38598;&#21270;&#30340;&#22270;&#24418;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#20801;&#35768;&#36328;&#23618;&#33410;&#28857;&#20043;&#38388;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#23384;&#22312;&#36807;&#24230;&#21387;&#32553;&#29616;&#35937;&#65292;&#23548;&#33268;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#21482;&#22312;&#33410;&#28857;&#30340;&#30456;&#37051;&#23621;&#20043;&#38388;&#36827;&#34892;&#23616;&#37096;&#28040;&#24687;&#20256;&#36882;&#12290;&#35797;&#22270;&#20351;&#22270;&#24418;&#8220;&#26356;&#36830;&#36890;&#8221;&#24182;&#19988;&#26356;&#36866;&#21512;&#38271;&#31243;&#20219;&#21153;&#30340;&#37325;&#36830;&#26041;&#27861;&#36890;&#24120;&#20250;&#22833;&#21435;&#22522;&#20110;&#22270;&#24418;&#36317;&#31163;&#25552;&#20379;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#20351;&#36828;&#31243;&#33410;&#28857;&#22312;&#27599;&#19968;&#23618;&#20013;&#31435;&#21363;&#36890;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;MPNN&#26550;&#26500;&#65292;&#20197;&#25191;&#34892;&#22522;&#20110;&#23618;&#30340;&#37325;&#36830;&#65292;&#20197;&#30830;&#20445;&#36880;&#28176;&#21152;&#23494;&#22270;&#24418;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24310;&#36831;&#26426;&#21046;&#65292;&#23427;&#20801;&#35768;&#26681;&#25454;&#23618;&#21644;&#23427;&#20204;&#30340;&#30456;&#20114;&#36317;&#31163;&#22312;&#33410;&#28857;&#20043;&#38388;&#36827;&#34892;&#36339;&#36291;&#36830;&#25509;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#38271;&#31243;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#20854;&#20248;&#20110;&#22270;&#24418;&#21464;&#25442;&#22120;&#21644;&#22810;&#36339;MPNN&#12290;
&lt;/p&gt;
&lt;p&gt;
Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs `more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07854</link><description>&lt;p&gt;
&#22522;&#20110;&#21305;&#37197;&#29305;&#24449;&#25552;&#21462;&#30340;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction. (arXiv:2305.07854v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07854
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#31639;&#27861;&#26469;&#21306;&#20998;&#23398;&#20064;&#26469;&#33258;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#30340;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#38656;&#35201;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#24320;&#21457;&#20934;&#30830;&#21487;&#38752;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20005;&#26684;&#30340;&#25968;&#25454;&#38544;&#31169;&#27861;&#24459;&#21644;&#20016;&#23500;&#30340;&#36793;&#32536;&#24037;&#19994;&#25968;&#25454;&#38656;&#35201;&#20998;&#25955;&#24335;&#25968;&#25454;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#20998;&#25955;&#24335;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#24037;&#19994;&#20581;&#24247;&#39044;&#27979;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#30001;&#20110;&#19981;&#21516;&#30340;&#36864;&#21270;&#26426;&#21046;&#21644;&#19981;&#24179;&#31561;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#25152;&#23548;&#33268;&#30340;&#25968;&#25454;&#24322;&#26500;&#24615;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#24320;&#21457;&#31934;&#24230;&#39640;&#30340;&#35757;&#32451;&#27169;&#22411;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#32479;&#35745;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;FL&#22312;&#20581;&#24247;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29305;&#24449;&#30456;&#20284;&#24615;&#21305;&#37197;&#30340;&#21442;&#25968;&#32858;&#21512;&#31639;&#27861;&#30340; FL &#20581;&#24247;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from h
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.07424</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#30340;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;unsup-SimCSE&#65292;&#22312;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27599;&#20010;&#23884;&#20837;&#20165;&#26469;&#33258;&#20110;&#19968;&#20010;&#21477;&#23376;&#23454;&#20363;&#65292;&#25105;&#20204;&#31216;&#36825;&#20123;&#23884;&#20837;&#20026;&#23454;&#20363;&#32423;&#23884;&#20837;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#23884;&#20837;&#34987;&#35270;&#20026;&#26159;&#19968;&#31867;&#29420;&#29305;&#30340;&#31867;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IS-CSE&#65288;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#65289;&#26469;&#24179;&#28369;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#20174;&#21160;&#24577;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#26816;&#32034;&#23884;&#20837;&#20197;&#33719;&#24471;&#27491;&#23884;&#20837;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23545;&#32452;&#20013;&#30340;&#23884;&#20837;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#29983;&#25104;&#24179;&#28369;&#23454;&#20363;&#23884;&#20837;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#22343;78.30&#65285;&#65292;79.47&#65285;&#65292;77.73&#65285;&#21644;79.42&#65285;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.07031</link><description>&lt;p&gt;
&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Hawkes Process based on Controlled Differential Equations. (arXiv:2305.07031v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#65292;&#21487;&#31934;&#30830;&#35745;&#31639;&#23545;&#25968;&#20284;&#28982;&#65292;&#24182;&#33021;&#22815;&#27491;&#30830;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#65292;&#36866;&#29992;&#20110;&#31038;&#20250;&#25193;&#25955;&#21644;&#22320;&#38663;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hawkes&#36807;&#31243;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#22810;&#20010;&#39046;&#22495;&#30340;&#24207;&#36143;&#20107;&#20214;&#21457;&#29983;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#12290;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#19981;&#20165;&#38590;&#20197;&#25429;&#25417;&#36825;&#31181;&#22797;&#26434;&#30340;&#19981;&#35268;&#21017;&#21160;&#24577;&#65292;&#32780;&#19988;&#36824;&#20250;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#35745;&#31639;&#20107;&#20214;&#30340;&#23545;&#25968;&#20284;&#28982;&#65292;&#22240;&#20026;&#23427;&#20204;&#22823;&#22810;&#22522;&#20110;&#35774;&#35745;&#29992;&#20110;&#35268;&#21017;&#31163;&#25955;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;(CDE)&#30340;Hawkes&#36807;&#31243;&#27010;&#24565;&#65292;&#36890;&#36807;&#37319;&#29992;&#31867;&#20284;&#20110;&#36830;&#32493;RNN&#30340;&#31070;&#32463;CDE&#25216;&#26415;&#12290;&#30001;&#20110;HP-CDE&#19981;&#26029;&#22320;&#35835;&#21462;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#36866;&#24403;&#22320;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#23427;&#20204;&#30340;&#19981;&#22343;&#21248;&#26102;&#38388;&#31354;&#38388;&#65292;&#24182;&#19988;&#23545;&#25968;&#20284;&#28982;&#21487;&#20197;&#20934;&#30830;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Hawkes&#36807;&#31243;&#21644;&#31070;&#32463;CDE&#37117;&#26159;&#22312;&#36830;&#32493;&#30340;&#26102;&#38388;&#22495;&#20013;&#39318;&#20808;&#24320;&#21457;&#30340;&#65292;&#23427;&#20204;&#20855;&#26377;&#30456;&#20284;&#30340;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;HP-CDE&#20855;&#26377;&#36879;&#26126;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#23454;&#38469;&#22330;&#26223;&#65292;&#20363;&#22914;&#31038;&#20250;&#25193;&#25955;&#65292;&#20854;&#20013;&#20107;&#20214;&#20043;&#38388;&#30340;&#38388;&#38548;&#26102;&#38388;&#26159;&#19981;&#35268;&#21017;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#31038;&#20132;&#25193;&#25955;&#21644;&#22320;&#38663;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#24182;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;Hawkes&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first de
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06137</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#31561;&#25928;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;WIRL&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#19982;&#25237;&#24433;&#27425;&#26799;&#24230;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;WIRL&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;&#26368;&#22823;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#23548;&#24341;&#25104;&#26412;&#23398;&#20064;&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
&lt;/p&gt;</description></item><item><title>Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#29575;&#21487;&#36798;99.95&#65285;&#65292;&#26377;&#28508;&#21147;&#29992;&#20110;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#30340;&#36890;&#20449;&#35774;&#22791;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30456;&#20284;&#25163;&#35821;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.05296</link><description>&lt;p&gt;
Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mediapipe and CNNs for Real-Time ASL Gesture Recognition. (arXiv:2305.05296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05296
&lt;/p&gt;
&lt;p&gt;
Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#29575;&#21487;&#36798;99.95&#65285;&#65292;&#26377;&#28508;&#21147;&#29992;&#20110;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#30340;&#36890;&#20449;&#35774;&#22791;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30456;&#20284;&#25163;&#35821;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35782;&#21035;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#36816;&#21160;&#30340;&#23454;&#26102;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Mediapipe&#24211;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;ASL&#25163;&#21183;&#20998;&#31867;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#20197;99.95&#65285;&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#25152;&#26377;ASL&#23383;&#27597;&#65292;&#34920;&#26126;&#23427;&#22312;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#35774;&#35745;&#30340;&#36890;&#20449;&#35774;&#22791;&#20013;&#26377;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20855;&#26377;&#30456;&#20284;&#25163;&#37096;&#36816;&#21160;&#30340;&#25163;&#35821;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#39640;&#21548;&#21147;&#20007;&#22833;&#20154;&#22763;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mediapipe&#21644;CNN&#36827;&#34892;&#23454;&#26102;&#25163;&#35821;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper describes a realtime system for identifying American Sign Language (ASL) movements that employs modern computer vision and machine learning approaches. The suggested method makes use of the Mediapipe library for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture classification. The testing results show that the suggested system can detect all ASL alphabets with an accuracy of 99.95%, indicating its potential for use in communication devices for people with hearing impairments. The proposed approach can also be applied to additional sign languages with similar hand motions, potentially increasing the quality of life for people with hearing loss. Overall, the study demonstrates the effectiveness of using Mediapipe and CNN for real-time sign language recognition, making a significant contribution to the field of computer vision and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2305.05126</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#20869;&#26680;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#28041;&#21450;&#22312;&#21508;&#31181;&#31574;&#21010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32858;&#21512;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24230;&#37327;&#25351;&#26631;&#30340;&#36873;&#25321;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#29702;&#24819;&#24230;&#37327;&#19981;&#26126;&#26174;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#22270;&#29702;&#35770;&#65292;&#24182;&#20419;&#36827;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35825;&#23548;&#19968;&#32452;&#37197;&#22791;&#26377;&#19982;&#19968;&#20123;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#27169;&#22411;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;LLMs&#35013;&#22791;&#20449;&#24687;&#24341;&#23548;&#27169;&#22359;&#26469;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;LLMs&#30340;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#40657;&#30418;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04757</link><description>&lt;p&gt;
&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Augmented Large Language Models with Parametric Knowledge Guiding. (arXiv:2305.04757v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04757
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548;&#30340;&#22686;&#24378;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;LLMs&#35013;&#22791;&#20449;&#24687;&#24341;&#23548;&#27169;&#22359;&#26469;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;LLMs&#30340;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#40657;&#30418;LLMs&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20197;&#20854;&#20986;&#33394;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23545;&#30456;&#20851;&#25968;&#25454;&#30340;&#26377;&#38480;&#25509;&#35302;&#65292;&#23427;&#20204;&#22312;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#30340;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340; LLM &#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#21482;&#33021;&#36890;&#36807; API &#35775;&#38382;, &#36825;&#38459;&#27490;&#20102;&#36827;&#19968;&#27493;&#29992;&#39046;&#22495;&#23450;&#21046;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#21521; LLM &#25152;&#26377;&#32773;&#25552;&#20379;&#31169;&#26377;&#25968;&#25454;&#20250;&#23548;&#33268;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#22411;&#30340;&#24102;&#21442;&#25968;&#30693;&#35782;&#24341;&#23548; (PKG) &#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20026; LLM &#37197;&#22791;&#20102;&#30693;&#35782;&#24341;&#23548;&#27169;&#22359;&#65292;&#20197;&#35775;&#38382;&#30456;&#20851;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464; LLM &#30340;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340; PKG &#22522;&#20110;&#24320;&#28304;&#30340;&#8220;&#30333;&#30418;&#8221;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#31163;&#32447;&#23384;&#20648; LLM &#38656;&#35201;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340; PKG &#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#8220;&#40657;&#30418;&#8221;LLM&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source "white-box" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of "black-box" LLMs on a range o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.00477</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#36739;&#20302;&#65306;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#26469;&#25214;&#21040;&#22909;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#30340;&#29615;&#22659;&#27169;&#22411;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#26159;&#36825;&#26679;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#30001;&#20110;&#20854;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#65288;PSDRL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#21487;&#25193;&#23637;&#30340;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#20854;&#22522;&#20110;&#27169;&#22411;&#30340;&#26412;&#36136;&#29305;&#24449;&#12290;PSDRL&#23558;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19978;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#22522;&#20110;&#20540;&#20989;&#25968;&#36924;&#36817;&#30340;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#23545;Atari&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;PSDRL&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#24341;&#20837;&#25200;&#21160;&#65292;&#25913;&#36827;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#36136;&#20294;&#28151;&#21512;&#27604;&#20363;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#20302;&#21151;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.14762</link><description>&lt;p&gt;
&#21033;&#29992;&#25200;&#21160;&#26469;&#25913;&#21892;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#26679;&#26412;&#20013;&#24341;&#20837;&#25200;&#21160;&#65292;&#25913;&#36827;&#22522;&#20110;&#26680;&#21270;&#26031;&#22374;&#36317;&#30340;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#26041;&#27861;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#21516;&#36136;&#20294;&#28151;&#21512;&#27604;&#20363;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#20302;&#21151;&#29575;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#23454;&#39564;&#35777;&#25454;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#21270;&#26031;&#22374;&#36317;&#65288;KSD&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#29992;&#20110;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#24046;&#24322;&#24230;&#37327;&#12290;&#21363;&#20351;&#30446;&#26631;&#20998;&#24067;&#20855;&#26377;&#26410;&#30693;&#30340;&#26631;&#20934;&#21270;&#22240;&#23376;&#65292;&#20363;&#22914;&#22312;&#36125;&#21494;&#26031;&#20998;&#26512;&#20013;&#65292;&#20063;&#21487;&#20197;&#24212;&#29992;&#23427;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#30446;&#26631;&#20998;&#24067;&#21644;&#26367;&#20195;&#20998;&#24067;&#20855;&#26377;&#30456;&#21516;&#19988;&#30456;&#36317;&#36739;&#36828;&#30340;&#27169;&#24335;&#20294;&#22312;&#28151;&#21512;&#27604;&#20363;&#19978;&#26377;&#25152;&#19981;&#21516;&#26102;&#65292;KSD&#26816;&#39564;&#21487;&#33021;&#20250;&#20986;&#29616;&#20302;&#21151;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#39532;&#23572;&#31185;&#22827;&#36716;&#31227;&#26680;&#23545;&#35266;&#27979;&#26679;&#26412;&#36827;&#34892;&#25200;&#21160;&#65292;&#20351;&#20854;&#30456;&#23545;&#20110;&#30446;&#26631;&#20998;&#24067;&#19981;&#21464;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#22312;&#25200;&#21160;&#26679;&#26412;&#19978;&#20351;&#29992;KSD&#26816;&#39564;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;&#25968;&#20540;&#35777;&#25454;&#34920;&#26126;&#65292;&#20351;&#29992;&#36866;&#24403;&#36873;&#25321;&#30340;&#26680;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#27604;KSD&#26816;&#39564;&#20855;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12550</link><description>&lt;p&gt;
&#35757;&#32451;&#20013;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36890;&#36807;&#32467;&#21512;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163;(&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;)&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23398;&#20064;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20581;&#22766;&#24615;&#30340;&#26377;&#25928;&#23398;&#20064;&#25216;&#26415;&#12290;&#26412;&#30740;&#31350;&#22312;&#26356;&#19968;&#33324;&#30340;&#25200;&#21160;&#33539;&#22260;&#19979;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#23545;&#25239;&#35757;&#32451;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#25506;&#32034;&#34920;&#26126;&#65292;&#23558;&#23545;&#25163;&#21644;&#21453;&#23545;&#25163; (&#24102;&#26377;&#21453;&#23545;&#25163;&#25200;&#21160;&#30340;&#26679;&#26412;) &#32467;&#21512;&#22312;&#35757;&#32451;&#20013;&#65292;&#22312;&#19968;&#20123;&#20856;&#22411;&#30340;&#23398;&#20064;&#22330;&#26223; (&#22914;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#21644;&#19981;&#24179;&#34913;&#23398;&#20064;) &#20013;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#26356;&#22909;&#30340;&#31867;&#21035;&#38388;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;</title><link>http://arxiv.org/abs/2304.11692</link><description>&lt;p&gt;
BN&#19982;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#24341;&#36215;&#26799;&#24230;&#29190;&#28856;&#65292;&#20294;&#34987;&#28608;&#27963;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#25152;&#25269;&#28040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Disharmony Between BN and ReLU Causes Gradient Explosion, but is Offset by the Correlation Between Activations. (arXiv:2304.11692v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;BN&#21644;ReLU&#20043;&#38388;&#30340;&#19981;&#21644;&#35856;&#26159;&#23548;&#33268;&#26799;&#24230;&#29190;&#28856;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#21516;&#26102;&#21457;&#29616;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#21487;&#26367;&#20195;WarmUp&#65292;&#22312;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#20063;&#34920;&#29616;&#19981;&#38169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25209;&#26631;&#20934;&#21270;&#21644;ReLU&#31561;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#22312;&#35757;&#32451;&#21021;&#26399;&#30001;&#20110;&#26102;&#38388;&#26799;&#24230;&#29190;&#28856;&#32780;&#20986;&#29616;&#19981;&#31283;&#23450;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;ReLU&#22914;&#20309;&#27604;&#39044;&#26399;&#26356;&#22810;&#22320;&#20943;&#23569;&#26041;&#24046;&#65292;&#20197;&#21450;&#25209;&#26631;&#20934;&#21270;&#22914;&#20309;&#22312;&#24674;&#22797;&#26399;&#38388;&#25918;&#22823;&#26799;&#24230;&#65292;&#23548;&#33268;&#21069;&#21521;&#20256;&#25773;&#20445;&#25345;&#31283;&#23450;&#32780;&#26799;&#24230;&#29190;&#28856;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#20197;&#21450;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#24863;&#26469;&#33258;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#30340;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#31639;&#27861;&#65292;&#22312;&#22823;&#25209;&#37327;&#35757;&#32451;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#23398;&#20064;&#29575;&#32553;&#25918;&#26041;&#27861;&#65292;&#24182;&#21487;&#26367;&#25442;&#23567;&#25209;&#37327;&#35757;&#32451;&#20013;&#30340;WarmUp&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks based on batch normalization and ReLU-like activation functions can experience instability during the early stages of training due to the high gradient induced by temporal gradient explosion. We explain how ReLU reduces variance more than expected, and how batch normalization amplifies the gradient during recovery, which causes gradient explosion while forward propagation remains stable. Additionally, we discuss how the dynamics of a deep neural network change during training and how the correlation between inputs can alleviate this problem. Lastly, we propose a better adaptive learning rate algorithm inspired by second-order optimization algorithms, which outperforms existing learning rate scaling methods in large batch training and can also replace WarmUp in small batch training.
&lt;/p&gt;</description></item><item><title>Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.10784</link><description>&lt;p&gt;
Eyettention&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#20197;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#30340;&#25195;&#35270;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Eyettention: An Attention-based Dual-Sequence Model for Predicting Human Scanpaths during Reading. (arXiv:2304.10784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10784
&lt;/p&gt;
&lt;p&gt;
Eyettention&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#38405;&#35835;&#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#27169;&#25311;&#38405;&#35835;&#32773;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20855;&#26377;&#20511;&#37492;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#26102;&#30340;&#30524;&#21160;&#25581;&#31034;&#20102;&#38405;&#35835;&#32773;&#30340;&#35748;&#30693;&#36807;&#31243;&#21644;&#25152;&#38405;&#35835;&#25991;&#26412;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#38405;&#35835;&#20013;&#25195;&#35270;&#36335;&#24452;&#30340;&#20998;&#26512;&#24050;&#24341;&#36215;&#21508;&#20010;&#39046;&#22495;&#30340;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#20174;&#35748;&#30693;&#31185;&#23398;&#21040;&#35821;&#35328;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#38405;&#35835;&#26102;&#20154;&#31867;&#30340;&#25195;&#35270;&#36335;&#24452;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#23427;&#20204;&#26159;&#30001;&#21452;&#24207;&#21015;&#32452;&#25104;&#30340;&#65306;&#21333;&#35789;&#25353;&#29031;&#35821;&#35328;&#30340;&#35821;&#27861;&#35268;&#21017;&#25490;&#24207;&#65292;&#32780;&#27880;&#35270;&#21017;&#25353;&#29031;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#12290;&#20154;&#31867;&#24182;&#19981;&#20005;&#26684;&#25353;&#24038;&#21040;&#21491;&#30340;&#39034;&#24207;&#38405;&#35835;&#65292;&#32780;&#26159;&#36339;&#36807;&#25110;&#37325;&#22797;&#27880;&#35270;&#21333;&#35789;&#65292;&#24182;&#20498;&#36864;&#21040;&#20197;&#21069;&#30340;&#21333;&#35789;&#65292;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#23545;&#40784;&#24182;&#19981;&#23481;&#26131;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;Eyettention&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#22788;&#29702;&#35821;&#35328;&#24207;&#21015;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#21452;&#24207;&#21015;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye movements during reading offer insights into both the reader's cognitive processes and the characteristics of the text that is being read. Hence, the analysis of scanpaths in reading have attracted increasing attention across fields, ranging from cognitive science over linguistics to computer science. In particular, eye-tracking-while-reading data has been argued to bear the potential to make machine-learning-based language models exhibit a more human-like linguistic behavior. However, one of the main challenges in modeling human scanpaths in reading is their dual-sequence nature: the words are ordered following the grammatical rules of the language, whereas the fixations are chronologically ordered. As humans do not strictly read from left-to-right, but rather skip or refixate words and regress to previous words, the alignment of the linguistic and the temporal sequence is non-trivial. In this paper, we develop Eyettention, the first dual-sequence model that simultaneously process
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.10770</link><description>&lt;p&gt;
DEIR: &#22522;&#20110;&#21306;&#20998;&#24615;&#27169;&#22411;&#30340;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#65292;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#65292;&#20854;&#26377;&#25928;&#24615;&#20851;&#38190;&#22320;&#24433;&#21709;&#30528;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38754;&#23545;&#31232;&#30095;&#30340;&#22806;&#37096;&#22870;&#21169;&#26102;&#26356;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#35266;&#27979;&#20013;&#20272;&#35745;&#26032;&#39062;&#24615;&#30340;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#26377;&#25928;&#40723;&#21169;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#33021;&#20250;&#24433;&#21709;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#27492;&#19968;&#20010;&#35266;&#27979;&#30340;&#26032;&#39062;&#24615;&#19982;&#25506;&#32034;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#20934;&#30830;&#20272;&#35745;&#25506;&#32034;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DEIR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20174;&#26465;&#20214;&#20114;&#20449;&#24687;&#39033;&#20013;&#29702;&#35770;&#19978;&#23548;&#20986;&#20869;&#22312;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#20027;&#35201;&#19982;&#20195;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#25152;&#36129;&#29486;&#30340;&#26032;&#39062;&#24615;&#25104;&#27604;&#20363;&#65292;&#24182;&#20511;&#21161;&#21306;&#20998;&#24615;&#30340;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;MiniGrid&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#26631;&#20934;&#21644;&#30828;&#26680;&#25506;&#32034;&#28216;&#25103;&#65292;&#22312;&#32467;&#26524;&#19978;DEIR&#27604;&#22522;&#32447;&#23398;&#20064;&#26356;&#24555;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#24212;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.17995</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#65306;&#22522;&#20110;&#29109;&#29305;&#24449;&#30340;&#33041;&#30005;&#20449;&#21495;&#21644;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#20998;&#31163;&#65292;&#29992;&#20110;NNetEn&#35745;&#31639;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Neural Network Entropy (NNetEn): EEG Signals and Chaotic Time Series Separation by Entropy Features, Python Package for NNetEn Calculation. (arXiv:2303.17995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;NNetEn&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#65292;&#24182;&#22312;&#20998;&#31163;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29109;&#27979;&#37327;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#20013;&#26377;&#25928;&#30340;&#29305;&#24449;&#12290;&#20256;&#32479;&#30340;&#29109;&#27979;&#37327;&#26041;&#27861;&#65292;&#20363;&#22914;&#39321;&#20892;&#29109;&#65292;&#20351;&#29992;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#20998;&#31163;&#26102;&#38388;&#24207;&#21015;&#65292;&#38656;&#35201;&#26032;&#30340;&#29109;&#20272;&#35745;&#26041;&#27861;&#26469;&#34920;&#24449;&#31995;&#32479;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#29109;(NNetEn)&#27010;&#24565;&#26159;&#22522;&#20110;&#29305;&#27530;&#25968;&#25454;&#38598;(MNIST-10&#21644;SARS-CoV-2-RBV1)&#30340;&#20998;&#31867;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#19982;&#35760;&#24405;&#22312;LogNNet&#31070;&#32463;&#32593;&#32476;&#20648;&#23618;&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#29109;&#30456;&#20851;&#12290;NNetEn&#20197;&#21407;&#22987;&#26041;&#24335;&#20272;&#35745;&#26102;&#38388;&#24207;&#21015;&#30340;&#28151;&#27788;&#21160;&#24577;&#12290;&#22522;&#20110;NNetEn&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#20998;&#31867;&#24230;&#37327;&#65306;R2&#25928;&#29575;&#21644;&#30382;&#23572;&#36874;&#25928;&#29575;&#12290;NNetEn&#30340;&#25928;&#29575;&#22312;&#20351;&#29992;&#31163;&#25955;&#20998;&#26512;(ANOVA)&#20998;&#31163;&#27491;&#24358;&#26144;&#23556;&#30340;&#20004;&#20010;&#28151;&#27788;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#24471;&#21040;&#39564;&#35777;&#12290;&#23545;&#20110;&#20004;&#20010;&#25509;&#36817;&#30340;&#21160;&#24577;&#26102;&#38388;&#24207;&#21015; (r=1.1918&#21644;r=1.2243)&#65292;F&#27604;&#20540;&#36798;&#21040;&#20102;124&#30340;&#20540;&#65292;&#21453;&#26144;&#20102;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets (MNIST-10 and SARS-CoV-2-RBV1) in relation to the entropy of the time series recorded in the reservoir of the LogNNet neural network. NNetEn estimates the chaotic dynamics of time series in an original way. Based on the NNetEn algorithm, we propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis (ANOVA). For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16971</link><description>&lt;p&gt;
&#22810;&#39033;&#24335;&#20998;&#31867;&#20013;&#30340;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#25972;&#20307;&#25968;&#25454;&#38598;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#20256;&#36882;SJS&#12289;&#20462;&#27491;&#31867;&#21518;&#39564;&#27010;&#29575;&#12289;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#12289;SJS&#19982;&#21327;&#21464;&#37327;&#36716;&#31227;&#20851;&#31995;&#31561;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32852;&#21512;&#20559;&#31227;&#65288;SJS&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#25972;&#20307;&#20559;&#31227;&#30340;&#21487;&#22788;&#29702;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36793;&#38469;&#20998;&#24067;&#20197;&#21450;&#21518;&#39564;&#27010;&#29575;&#21644;&#31867;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#22312;&#27809;&#26377;&#26631;&#31614;&#35266;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#30446;&#26631;&#25968;&#25454;&#38598;&#25311;&#21512;SJS&#21487;&#33021;&#20250;&#20135;&#29983;&#26631;&#31614;&#30340;&#26377;&#25928;&#39044;&#27979;&#21644;&#31867;&#20808;&#39564;&#27010;&#29575;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#29305;&#24449;&#38598;&#20043;&#38388;&#20256;&#36882;SJS&#26041;&#38754;&#25552;&#20379;&#20102;&#26032;&#30340;&#32467;&#26524;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30446;&#26631;&#20998;&#24067;&#30340;&#31867;&#21518;&#39564;&#27010;&#29575;&#30340;&#26465;&#20214;&#20462;&#27491;&#20844;&#24335;&#65292;&#30830;&#23450;&#24615;SJS&#30340;&#21487;&#36776;&#35748;&#24615;&#20197;&#21450;SJS&#21644;&#21327;&#21464;&#37327;&#36716;&#31227;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#29992;&#20110;&#20272;&#35745;SJS&#29305;&#24449;&#30340;&#31639;&#27861;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#20250;&#22952;&#30861;&#23547;&#25214;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#21015;&#34920;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; $b$-ary Littlestone &#32500;&#24230;&#21487;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#25077;&#25026;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#21487;&#20197;&#20351;&#29992;&#25913;&#32534;&#33258; Littlestone &#30340; SOA &#21644; Rosenblatt &#30340;&#24863;&#30693;&#22120;&#31561;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#36824;&#24314;&#31435;&#20102;&#21015;&#34920;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.15383</link><description>&lt;p&gt;
&#22522;&#20110;&#21015;&#34920;&#30340;&#22312;&#32447;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
List Online Classification. (arXiv:2303.15383v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26631;&#31614;&#21015;&#34920;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102; $b$-ary Littlestone &#32500;&#24230;&#21487;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#25077;&#25026;&#30340;&#24773;&#20917;&#19979;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#21487;&#20197;&#20351;&#29992;&#25913;&#32534;&#33258; Littlestone &#30340; SOA &#21644; Rosenblatt &#30340;&#24863;&#30693;&#22120;&#31561;&#31639;&#27861;&#36827;&#34892;&#39044;&#27979;&#65292;&#21516;&#26102;&#36824;&#24314;&#31435;&#20102;&#21015;&#34920;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22810;&#20998;&#31867;&#22312;&#32447;&#39044;&#27979;&#65292;&#20854;&#20013;&#23398;&#20064;&#32773;&#21487;&#20197;&#20351;&#29992;&#22810;&#20010;&#26631;&#31614;&#30340;&#21015;&#34920;&#36827;&#34892;&#39044;&#27979;&#65288;&#19982;&#20256;&#32479;&#35774;&#32622;&#20013;&#20165;&#20351;&#29992;&#19968;&#31181;&#26631;&#31614;&#19981;&#21516;&#65289;&#12290;&#25105;&#20204;&#20351;&#29992; $b$-ary Littlestone &#32500;&#24230;&#34920;&#24449;&#20102;&#35813;&#27169;&#22411;&#20013;&#30340;&#21487;&#23398;&#20064;&#24615;&#12290;&#35813;&#32500;&#24230;&#26159;&#32463;&#20856; Littlestone &#32500;&#24230;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20108;&#36827;&#21046;&#38169;&#35823;&#26641;&#34987;&#26367;&#25442;&#20026; $(k+1)$-ary &#38169;&#35823;&#26641;&#65292;&#20854;&#20013; k &#26159;&#21015;&#34920;&#20013;&#26631;&#31614;&#30340;&#25968;&#37327;&#12290;&#22312;&#25077;&#25026;&#30340;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#27604;&#36739;&#31867;&#20013;&#26159;&#21542;&#21253;&#21547;&#21333;&#26631;&#31614;&#25110;&#22810;&#26631;&#31614;&#20989;&#25968;&#20197;&#21450;&#23427;&#19982;&#31639;&#27861;&#20351;&#29992;&#30340;&#21015;&#34920;&#22823;&#23567;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#36127;&#24724;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#20160;&#20040;&#24773;&#20917;&#19979;&#23454;&#29616;&#36127;&#24724;&#30340;&#23436;&#25972;&#29305;&#24615;&#21270;&#12290;&#20316;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#32463;&#20856;&#31639;&#27861;&#65292;&#22914; Littlestone &#30340; SOA &#21644; Rosenblatt &#30340;&#24863;&#30693;&#22120;&#65292;&#20197;&#20351;&#29992;&#26631;&#31614;&#21015;&#34920;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#20026;&#21487;&#20197;&#36827;&#34892;&#21015;&#34920;&#23398;&#20064;&#30340;&#32452;&#21512;&#32467;&#26524;&#24314;&#31435;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study multiclass online prediction where the learner can predict using a list of multiple labels (as opposed to just one label in the traditional setting). We characterize learnability in this model using the $b$-ary Littlestone dimension. This dimension is a variation of the classical Littlestone dimension with the difference that binary mistake trees are replaced with $(k+1)$-ary mistake trees, where $k$ is the number of labels in the list. In the agnostic setting, we explore different scenarios depending on whether the comparator class consists of single-labeled or multi-labeled functions and its tradeoff with the size of the lists the algorithm uses. We find that it is possible to achieve negative regret in some cases and provide a complete characterization of when this is possible. As part of our work, we adapt classical algorithms such as Littlestone's SOA and Rosenblatt's Perceptron to predict using lists of labels. We also establish combinatorial results for list-learnable c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#29305;&#23450;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hamilton-Jacobi PDEs&#32852;&#31995;&#36215;&#26469;&#65292;&#34920;&#26126;&#24403;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#24212;&#30340;&#22810;&#26102;&#38388;HJ PDEs&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.12928</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#26102;&#38388; Hamilton-Jacobi PDE &#35299;&#20915;&#19968;&#20123;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35770;&#19978;&#23558;&#29305;&#23450;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hamilton-Jacobi PDEs&#32852;&#31995;&#36215;&#26469;&#65292;&#34920;&#26126;&#24403;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#26102;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#23545;&#24212;&#30340;&#22810;&#26102;&#38388;HJ PDEs&#21644;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hamilton-Jacobi &#20559;&#24494;&#20998;&#26041;&#31243;(HJ PDEs)&#19982;&#24191;&#27867;&#39046;&#22495;&#65292;&#22914;&#26368;&#20248;&#25511;&#21046;&#12289;&#24494;&#20998;&#28216;&#25103;&#21644;&#25104;&#20687;&#31185;&#23398;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#36890;&#36807;&#23558;&#26102;&#38388;&#21464;&#37327;&#35270;&#20026;&#26356;&#39640;&#32500;&#30340;&#37327;&#65292;HJ PDEs &#21487;&#20197;&#25193;&#23637;&#21040;&#22810;&#26102;&#38388;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#22312;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#38382;&#39064;&#19982;&#22810;&#26102;&#38388;Hopf&#20844;&#24335;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#35813;&#20844;&#24335;&#23545;&#24212;&#20110;&#26576;&#20123;&#22810;&#26102;&#38388; HJ PDEs &#30340;&#35299;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#24403;&#25105;&#20204;&#35299;&#20915;&#36825;&#20123;&#23398;&#20064;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#20063;&#35299;&#20915;&#20102;&#19968;&#20010;&#22810;&#26102;&#38388; HJ PDE &#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#26576;&#20123;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#36825;&#31181;&#32852;&#31995;&#30340;&#31532;&#19968;&#20010;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#23637;&#20102;&#27491;&#21017;&#21270;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#19982;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120; (LQR) &#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#26694;&#26550;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hamilton-Jacobi partial differential equations (HJ PDEs) have deep connections with a wide range of fields, including optimal control, differential games, and imaging sciences. By considering the time variable to be a higher dimensional quantity, HJ PDEs can be extended to the multi-time case. In this paper, we establish a novel theoretical connection between specific optimization problems arising in machine learning and the multi-time Hopf formula, which corresponds to a representation of the solution to certain multi-time HJ PDEs. Through this connection, we increase the interpretability of the training process of certain machine learning applications by showing that when we solve these learning problems, we also solve a multi-time HJ PDE and, by extension, its corresponding optimal control problem. As a first exploration of this connection, we develop the relation between the regularized linear regression problem and the Linear Quadratic Regulator (LQR). We then leverage our theoret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.07551</link><description>&lt;p&gt;
&#21512;&#24182;&#20915;&#31574;Transformer&#65306;&#22810;&#20219;&#21153;&#31574;&#30053;&#24418;&#25104;&#30340;&#26435;&#37325;&#24179;&#22343;&#21270;
&lt;/p&gt;
&lt;p&gt;
Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#35757;&#32451;&#20110;&#19981;&#21516; MuJoCo &#36816;&#21160;&#38382;&#39064;&#19978;&#30340; Decision Transformer &#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#36890;&#36807;&#20849;&#20139;&#19968;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#35757;&#32451;&#20197;&#21450;&#20849;&#21516;&#20351;&#29992;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#65292;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#20351;&#20195;&#29702;&#30340;&#36807;&#31243;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22522;&#20110;Transformer&#30340;&#36890;&#29992;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#38382;&#39064;&#30340;&#31574;&#30053;&#30340;&#21069;&#26223;&#12290;&#20026;&#20102;&#21019;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#24120;&#38656;&#35201;&#38598;&#20013;&#30340;&#35757;&#32451;&#30446;&#26631;&#12289;&#25968;&#25454;&#21644;&#35745;&#31639;&#12290;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#26356;&#28789;&#27963;&#22320;&#21019;&#24314;&#36890;&#29992;&#31574;&#30053;&#65292;&#36890;&#36807;&#21512;&#24182;&#22810;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#12289;&#21333;&#29420;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#21017;&#36825;&#26679;&#20570;&#23601;&#27604;&#36739;&#26377;&#24847;&#20041;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#21512;&#24182;&#25110;&#24179;&#22343;&#19981;&#21516;MuJoCo&#36816;&#21160;&#38382;&#39064;&#19978;&#35757;&#32451;&#30340;Decision Transformer&#30340;&#23376;&#38598;&#26469;&#36808;&#20986;&#36825;&#20010;&#26041;&#21521;&#30340;&#21021;&#27493;&#27493;&#39588;&#65292;&#24418;&#25104;&#27809;&#26377;&#38598;&#20013;&#35757;&#32451;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24314;&#35758;&#22312;&#21512;&#24182;&#31574;&#30053;&#26102;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22914;&#26524;&#25152;&#26377;&#31574;&#30053;&#37117;&#20174;&#20849;&#21516;&#30340;&#39044;&#35757;&#32451;&#21021;&#22987;&#21270;&#24320;&#22987;&#65292;&#24182;&#22312;&#38382;&#39064;&#29305;&#23450;&#30340;&#24494;&#35843;&#26399;&#38388;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#26041;&#21521;&#30340;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#27665;&#20027;&#21270;&#21644;&#20998;&#21457;&#20855;&#26377;&#19968;&#33324;&#33021;&#21147;&#30340;&#20195;&#29702;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#24066;&#25104;&#24180;&#24739;&#32773;&#20013;&#22810;&#31181;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#21450;&#20854;&#39118;&#38505;&#22240;&#32032;&#65292;&#20854;&#20013;&#24515;&#34880;&#31649;&#30142;&#30149;&#26368;&#20026;&#26222;&#36941;&#12290;&#30007;&#24615;&#21442;&#19982;&#32773;&#36739;&#22899;&#24615;&#26356;&#23481;&#26131;&#24739;&#26377;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20294;&#31958;&#23615;&#30149;&#19981;&#20855;&#26377;&#24615;&#21035;&#20542;&#21521;&#12290;CVD&#21644;DM&#37117;&#20250;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#24739;&#26377;&#32933;&#32982;&#30151;&#30340;&#20303;&#38498;&#30149;&#20154;&#21344;&#20116;&#20998;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2303.04808</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#36798;&#21345;&#24066;&#22522;&#20110;&#21307;&#38498;&#30340;&#27178;&#26029;&#38754;&#30740;&#31350;&#65306;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#21450;&#20027;&#35201;&#39118;&#38505;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prevalence and major risk factors of non-communicable diseases: A Hospital-based Cross-Sectional Study in Dhaka, Bangladesh. (arXiv:2303.04808v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04808
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#24066;&#25104;&#24180;&#24739;&#32773;&#20013;&#22810;&#31181;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#30340;&#24739;&#30149;&#29575;&#21450;&#20854;&#39118;&#38505;&#22240;&#32032;&#65292;&#20854;&#20013;&#24515;&#34880;&#31649;&#30142;&#30149;&#26368;&#20026;&#26222;&#36941;&#12290;&#30007;&#24615;&#21442;&#19982;&#32773;&#36739;&#22899;&#24615;&#26356;&#23481;&#26131;&#24739;&#26377;&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#20294;&#31958;&#23615;&#30149;&#19981;&#20855;&#26377;&#24615;&#21035;&#20542;&#21521;&#12290;CVD&#21644;DM&#37117;&#20250;&#38543;&#30528;&#24180;&#40836;&#30340;&#22686;&#38271;&#32780;&#22686;&#21152;&#12290;&#24739;&#26377;&#32933;&#32982;&#30151;&#30340;&#20303;&#38498;&#30149;&#20154;&#21344;&#20116;&#20998;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#23391;&#21152;&#25289;&#22269;&#36798;&#21345;&#24066;&#23547;&#27714;&#33829;&#20859;&#25351;&#23548;&#30340;&#25104;&#24180;&#24739;&#32773;&#20013;&#22810;&#31181;&#38750;&#20256;&#26579;&#24615;&#30142;&#30149;&#65288;NCD&#65289;&#30340;&#24739;&#30149;&#29575;&#65292;&#20998;&#26512;&#20854;&#39118;&#38505;&#22240;&#32032;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24615;&#21035;&#12289;&#24180;&#40836;&#32452;&#12289;&#32933;&#32982;&#19982;NCD&#65288;&#31958;&#23615;&#30149;&#12289;CKD&#12289;IBS&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#12289;&#24930;&#24615;&#32958;&#33039;&#30142;&#30149;&#12289;&#30002;&#29366;&#33146;&#30142;&#30149;&#65289;&#20043;&#38388;&#26377;&#20851;&#32852;&#12290;NCD&#20013;&#26368;&#24120;&#35265;&#30340;&#26159;&#24515;&#34880;&#31649;&#38382;&#39064;&#65288;CVD&#65289;&#65292;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#21344;83.56%&#12290;CVD&#22312;&#30007;&#24615;&#21442;&#19982;&#32773;&#20013;&#26356;&#20026;&#26222;&#36941;&#12290;&#30456;&#24212;&#22320;&#65292;&#30007;&#24615;&#21442;&#19982;&#32773;&#30340;&#34880;&#21387;&#20998;&#24067;&#27604;&#22899;&#24615;&#26356;&#39640;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31958;&#23615;&#30149;&#24182;&#27809;&#26377;&#24615;&#21035;&#20542;&#21521;&#12290;&#26080;&#35770;CVD&#36824;&#26159;DM&#65292;&#37117;&#20855;&#26377;&#24180;&#40836;&#19978;&#30340;&#36827;&#23637;&#12290;&#24930;&#24615;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#22312;&#20013;&#24180;&#21442;&#19982;&#32773;&#20013;&#27604;&#24180;&#36731;&#25110;&#32769;&#24180;&#20154;&#26356;&#20026;&#24120;&#35265;&#12290;&#22522;&#20110;&#25968;&#25454;&#65292;&#20116;&#20998;&#20043;&#19968;&#30340;&#20303;&#38498;&#24739;&#32773;&#24739;&#26377;&#32933;&#32982;&#30151;&#12290;&#25105;&#20204;&#23545;&#21512;&#24182;&#30151;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;31.5%&#30340;&#20154;&#21475;&#20165;&#24739;&#26377;&#19968;&#31181;NCD&#65292;30.1%&#30340;&#20154;&#24739;&#26377;&#20004;&#31181;&#25110;&#20004;&#31181;&#20197;&#19978;&#30340;NCD&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The study aimed to determine the prevalence of several non-communicable diseases (NCD) and analyze risk factors among adult patients seeking nutritional guidance in Dhaka, Bangladesh. Result: Our study observed the relationships between gender, age groups, obesity, and NCDs (DM, CKD, IBS, CVD, CRD, thyroid). The most frequently reported NCD was cardiovascular issues (CVD), which was present in 83.56% of all participants. CVD was more common in male participants. Consequently, male participants had a higher blood pressure distribution than females. Diabetes mellitus (DM), on the other hand, did not have a gender-based inclination. Both CVD and DM had an age-based progression. Our study showed that chronic respiratory illness was more frequent in middle-aged participants than in younger or elderly individuals. Based on the data, every one in five hospitalized patients was obese. We analyzed the co-morbidities and found that 31.5% of the population has only one NCD, 30.1% has t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#35748;&#35777;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#23545;&#20004;&#31181;&#24120;&#35265;&#30340;&#27745;&#26579;&#31867;&#22411;&#36827;&#34892;&#25269;&#25239;&#65292;&#24182;&#30830;&#20445;&#27867;&#21270;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02251</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#35748;&#35777;&#40065;&#26834;&#31070;&#32463;&#32593;&#32476;&#65306;&#27867;&#21270;&#21644;&#25239;&#27745;&#26579;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robust Neural Networks: Generalization and Corruption Resistance. (arXiv:2303.02251v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02251
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#36890;&#36807;&#35748;&#35777;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#23545;&#20004;&#31181;&#24120;&#35265;&#30340;&#27745;&#26579;&#31867;&#22411;&#36827;&#34892;&#25269;&#25239;&#65292;&#24182;&#30830;&#20445;&#27867;&#21270;&#20445;&#35777;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#30683;&#30462;&#65292;&#20855;&#26377;&#26497;&#39640;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#40065;&#26834;&#24615;&#65288;&#23545;&#8220;&#27745;&#26579;&#8221;&#30340;&#25269;&#25239;&#33021;&#21147;&#65289;&#21487;&#33021;&#19982;&#27867;&#21270;&#23384;&#22312;&#30683;&#30462;&#12290;&#20363;&#22914;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#26088;&#22312;&#20943;&#23569;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#23545;&#23567;&#25968;&#25454;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#36807;&#25311;&#21512;&#26159;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#23613;&#31649;&#22312;&#26631;&#20934;&#35757;&#32451;&#20013;&#20960;&#20046;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#31181;&#22855;&#29305;&#30340;&#8220;&#40065;&#26834;&#36807;&#25311;&#21512;&#8221;&#29616;&#35937;&#30340;&#29702;&#35770;&#35777;&#25454;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#65292;&#23558;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#22320;&#35777;&#26126;&#20102;&#35813;&#25439;&#22833;&#20855;&#26377;&#35748;&#35777;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#65292;&#21487;&#20197;&#25269;&#25239;&#20004;&#31181;&#24120;&#35265;&#30340;&#27745;&#26579;&#31867;&#22411;&#8212;&#8212;&#25968;&#25454;&#36867;&#36991;&#21644;&#25915;&#20987;&#8212;&#8212;&#21516;&#26102;&#30830;&#20445;&#27867;&#21270;&#20445;&#35777;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#23383;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#23436;&#25972;&#40065;&#26834;&#65288;HR&#65289;&#35757;&#32451;&#31243;&#24207;&#20855;&#26377;SOTA&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;HR&#35757;&#32451;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#24182;&#21487;&#20197;&#33258;&#28982;&#22320;&#24212;&#29992;&#20110;GAN&#21644;RL&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work have demonstrated that robustness (to "corruption") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar "robust overfitting" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;Soft Actor-Critic&#31639;&#27861;&#21644;Soft Q-learning&#31639;&#27861;&#22312;&#26368;&#22823;&#29109;&#26694;&#26550;&#19979;&#25910;&#25947;&#20110;&#21516;&#19968;&#35299;&#65292;&#36825;&#19968;&#32467;&#35770;&#23545;&#20248;&#21270;&#31639;&#27861;&#20855;&#26377;&#36739;&#22823;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.01240</link><description>&lt;p&gt;
Soft Actor-Critic&#31639;&#27861;&#30340;&#25910;&#25947;&#28857;
&lt;/p&gt;
&lt;p&gt;
The Point to Which Soft Actor-Critic Converges. (arXiv:2303.01240v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#65292;Soft Actor-Critic&#31639;&#27861;&#21644;Soft Q-learning&#31639;&#27861;&#22312;&#26368;&#22823;&#29109;&#26694;&#26550;&#19979;&#25910;&#25947;&#20110;&#21516;&#19968;&#35299;&#65292;&#36825;&#19968;&#32467;&#35770;&#23545;&#20248;&#21270;&#31639;&#27861;&#20855;&#26377;&#36739;&#22823;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Soft Actor-Critic&#26159;Soft Q-learning&#30340;&#25104;&#21151;&#21518;&#32487;&#32773;&#65292;&#23613;&#31649;&#23427;&#20204;&#37117;&#22788;&#20110;&#26368;&#22823;&#29109;&#26694;&#26550;&#19979;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#25910;&#25947;&#20110;&#30456;&#21516;&#30340;&#35299;&#65292;&#36825;&#19968;&#32467;&#26524;&#38750;&#24120;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#23427;&#23558;&#20248;&#21270;&#20174;&#22256;&#38590;&#30340;&#26041;&#24335;&#36716;&#21270;&#20026;&#20102;&#31616;&#21333;&#30340;&#26041;&#24335;&#12290;&#21516;&#26679;&#30340;&#35777;&#26126;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#27491;&#21017;&#39033;&#65292;&#20363;&#22914;KL&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft actor-critic is a successful successor over soft Q-learning. While lived under maximum entropy framework, their relationship is still unclear. In this paper, we prove that in the limit they converge to the same solution. This is appealing since it translates the optimization from an arduous to an easier way. The same justification can also be applied to other regularizers such as KL divergence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22686;&#24378;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65292;&#26082;&#21487;&#20197;&#21033;&#29992;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#26032;&#30340;&#36807;&#37319;&#26679;&#31574;&#30053;&#26469;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.05918</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#22686;&#24378;&#20915;&#31574;&#26641;&#36827;&#34892;&#39640;&#25928;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Fraud Detection Using Deep Boosting Decision Trees. (arXiv:2302.05918v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#22686;&#24378;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#27450;&#35784;&#26816;&#27979;&#65292;&#26082;&#21487;&#20197;&#21033;&#29992;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#26032;&#30340;&#36807;&#37319;&#26679;&#31574;&#30053;&#26469;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27450;&#35784;&#26816;&#27979;&#26159;&#35782;&#21035;&#12289;&#30417;&#25511;&#21644;&#39044;&#38450;&#22797;&#26434;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#27450;&#35784;&#27963;&#21160;&#12290;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26469;&#22788;&#29702;&#27450;&#35784;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#22686;&#24378;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#22686;&#24378;&#20915;&#31574;&#26641;&#65288;DBDT&#65289;&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#26500;&#24314;&#20102;&#36719;&#20915;&#31574;&#26641;&#65292;&#28982;&#21518;&#23558;&#20854;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#26368;&#32456;&#20998;&#31867;&#65292;&#26082;&#21487;&#20197;&#21033;&#29992;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#36807;&#37319;&#26679;&#31574;&#30053;&#26469;&#32531;&#35299;&#25968;&#25454;&#19981;&#24179;&#34913;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fraud detection is to identify, monitor, and prevent potentially fraudulent activities from complex data. The recent development and success in AI, especially machine learning, provides a new data-driven way to deal with fraud. From a methodological point of view, machine learning based fraud detection can be divided into two categories, i.e., conventional methods (decision tree, boosting...) and deep learning, both of which have significant limitations in terms of the lack of representation learning ability for the former and interpretability for the latter. Furthermore, due to the rarity of detected fraud cases, the associated data is usually imbalanced, which seriously degrades the performance of classification algorithms. In this paper, we propose deep boosting decision trees (DBDT), a novel approach for fraud detection based on gradient boosting and neural networks. In order to combine the advantages of both conventional methods and deep learning, we first construct soft decision 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05881</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#25968;&#20540;&#20808;&#39564;&#30340;&#24191;&#20041;CP&#20998;&#35299;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;GCDTC&#65292;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#21644;&#24191;&#20041;CP&#20998;&#35299;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#65307;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#20010;&#31639;&#27861;SPTC&#65292;&#20316;&#20026;&#35813;&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#34917;&#20840;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#36825;&#19968;&#31867;&#21035;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#23545;&#34917;&#20840;&#24352;&#37327;&#26045;&#21152;&#20302;&#31209;&#32467;&#26500;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#32771;&#34385;&#21040;&#24352;&#37327;&#20803;&#32032;&#30340;&#25968;&#20540;&#20808;&#39564;&#20449;&#24687;&#12290;&#24573;&#30053;&#25968;&#20540;&#20808;&#39564;&#23558;&#23548;&#33268;&#20002;&#22833;&#20851;&#20110;&#25968;&#25454;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#22240;&#27492;&#38459;&#27490;&#31639;&#27861;&#36798;&#21040;&#26368;&#20248;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#26500;&#24314;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26694;&#26550;&#65292;&#21517;&#20026;GCDTC&#65288;&#24191;&#20041;CP&#20998;&#35299;&#24352;&#37327;&#34917;&#20840;&#65289;&#65292;&#20197;&#21033;&#29992;&#25968;&#20540;&#20808;&#39564;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24352;&#37327;&#34917;&#20840;&#31934;&#24230;&#12290;&#22312;&#36825;&#20010;&#26032;&#24341;&#20837;&#30340;&#26694;&#26550;&#20013;&#65292;&#23558;&#24191;&#20041;&#30340;CP&#20998;&#35299;&#24212;&#29992;&#20110;&#20302;&#31209;&#24352;&#37327;&#34917;&#20840;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPTC&#65288;&#24179;&#28369;&#27850;&#26494;&#24352;&#37327;&#34917;&#20840;&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#36127;&#25972;&#25968;&#24352;&#37327;&#34917;&#20840;&#65292;&#20316;&#20026;GCDTC&#26694;&#26550;&#30340;&#19968;&#20010;&#23454;&#29616;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#27604;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#26356;&#20248;&#30340;&#24352;&#37327;&#34917;&#20840;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;DCMDPs&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20381;&#36182;&#21382;&#21490;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;&#20854;&#20013;&#30340;&#36923;&#36753;DCMDPs&#36890;&#36807;&#21033;&#29992;&#32858;&#21512;&#20989;&#25968;&#30830;&#23450;&#19978;&#19979;&#25991;&#36716;&#25442;&#65292;&#25171;&#30772;&#20102;&#23545;&#21382;&#21490;&#38271;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.02061</link><description>&lt;p&gt;
&#21382;&#21490;&#20381;&#36182;&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with History-Dependent Dynamic Contexts. (arXiv:2302.02061v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02061
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;DCMDPs&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20381;&#36182;&#21382;&#21490;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;&#20854;&#20013;&#30340;&#36923;&#36753;DCMDPs&#36890;&#36807;&#21033;&#29992;&#32858;&#21512;&#20989;&#25968;&#30830;&#23450;&#19978;&#19979;&#25991;&#36716;&#25442;&#65292;&#25171;&#30772;&#20102;&#23545;&#21382;&#21490;&#38271;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#22312;&#25512;&#33616;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#19978;&#19979;&#25991;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;DCMDPs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#20381;&#36182;&#21382;&#21490;&#29615;&#22659;&#30340;&#24773;&#20917;&#12290;&#23427;&#25512;&#24191;&#20102;&#19978;&#19979;&#25991;MDP&#26694;&#26550;&#65292;&#20197;&#22788;&#29702;&#38750;&#39532;&#23572;&#21487;&#22827;&#29615;&#22659;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#30528;&#37325;&#20110;&#36923;&#36753;DCMDPs&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#32858;&#21512;&#20989;&#25968;&#30830;&#23450;&#19978;&#19979;&#25991;&#36716;&#25442;&#26469;&#25171;&#30772;&#23545;&#21382;&#21490;&#38271;&#24230;&#30340;&#25351;&#25968;&#20381;&#36182;&#12290;&#36825;&#31181;&#29305;&#27530;&#32467;&#26500;&#20351;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#20986;&#19968;&#31181;&#31867;&#20284;&#20110;&#19978;&#38480;&#32622;&#20449;&#30028;&#31639;&#27861;&#30340;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36951;&#25022;&#30028;&#12290;&#21463;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36923;&#36753;DCMDPs&#65292;&#36825;&#20010;&#31639;&#27861;&#22312;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#20351;&#29992;&#21382;&#21490;&#20381;&#36182;&#29305;&#24449;&#19978;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#25512;&#33616;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65288;&#20351;&#29992;MovieLens&#25968;&#25454;&#38598;&#65289;&#65292;&#20854;&#20013;&#29992;&#25143;&#34892;&#20026;&#21160;&#24577;&#22320;&#38543;&#30528;&#25512;&#33616;&#30340;&#21464;&#21270;&#32780;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features. We demonstrate the efficacy of our approach on a recommendation task (using MovieLens data) where user behavior dynamics evolve in response to recommendations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#21644;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#29702;&#35770;&#24418;&#25104;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31070;&#32463;&#36924;&#36817;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20989;&#25968;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#23569;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#24555;&#36895;&#25429;&#25417;&#21040;&#26080;&#31351;&#22810;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.00328</link><description>&lt;p&gt;
&#23398;&#20064;&#20989;&#25968;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
Learning Functional Transduction. (arXiv:2302.00328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00328
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#21644;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#29702;&#35770;&#24418;&#25104;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31070;&#32463;&#36924;&#36817;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#20989;&#25968;&#31354;&#38388;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#23569;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#24110;&#21161;&#24555;&#36895;&#25429;&#25417;&#21040;&#26080;&#31351;&#22810;&#30340;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#30740;&#31350;&#20998;&#20026;&#20004;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;&#22238;&#24402;&#20219;&#21153;&#65306;&#36716;&#23548;&#26041;&#27861;&#30452;&#25509;&#20174;&#21487;&#29992;&#30340;&#25968;&#25454;&#20013;&#26500;&#24314;&#20272;&#35745;&#65292;&#20294;&#36890;&#24120;&#38382;&#39064;&#19981;&#20855;&#20307;&#12290;&#24402;&#32435;&#26041;&#27861;&#21487;&#20197;&#26356;&#20855;&#20307;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#35745;&#31639;&#23494;&#38598;&#30340;&#35299;&#20915;&#26041;&#26696;&#25628;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#20803;&#23398;&#20064;&#36716;&#23548;&#22238;&#24402;&#21407;&#21017;&#65292;&#36890;&#36807;&#21033;&#29992;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#29702;&#35770;&#24418;&#25104;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#31070;&#32463;&#36924;&#36817;&#22120;&#65292;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#26377;&#38480;&#21644;&#26080;&#38480;&#32500;&#31354;&#38388;&#65288;&#20989;&#25968;&#20540;&#31639;&#23376;&#65289;&#19978;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#26086;&#35757;&#32451;&#23436;&#25104;&#65292;&#36716;&#23548;&#22120;&#20960;&#20046;&#21487;&#20197;&#21363;&#26102;&#22320;&#25429;&#25417;&#21040;&#26080;&#31351;&#22810;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#32473;&#23450;&#23569;&#37327;&#36755;&#20837;&#21644;&#36755;&#20986;&#31034;&#20363;&#65292;&#24182;&#36820;&#22238;&#26032;&#30340;&#22270;&#20687;&#20272;&#35745;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#20803;&#23398;&#30340;&#36716;&#23548;&#26041;&#27861;&#23545;&#20110;&#27169;&#25311;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#30340;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research in machine learning has polarized into two general approaches for regression tasks: Transductive methods construct estimates directly from available data but are usually problem unspecific. Inductive methods can be much more specific but generally require compute-intensive solution searches. In this work, we propose a hybrid approach and show that transductive regression principles can be meta-learned through gradient descent to form efficient in-context neural approximators by leveraging the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS). We apply this approach to function spaces defined over finite and infinite-dimensional spaces (function-valued operators) and show that once trained, the Transducer can almost instantaneously capture an infinity of functional relationships given a few pairs of input and output examples and return new image estimates. We demonstrate the benefit of our meta-learned transductive approach to model complex physical systems influe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#32422;&#26463;&#20989;&#25968;&#20026;&#20984;&#25110;&#24369;&#20984;&#65292;&#22312;&#21482;&#20351;&#29992;&#21333;&#29615;&#36335;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13314</link><description>&lt;p&gt;
&#21333;&#29615;&#36335;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#27714;&#35299;&#38750;&#20809;&#28369;&#24369;&#20984;&#20989;&#25968;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;Oracle&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Oracle Complexity of Single-Loop Switching Subgradient Methods for Non-Smooth Weakly Convex Functional Constrained Optimization. (arXiv:2301.13314v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#29992;&#20110;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#20854;&#20013;&#32422;&#26463;&#20989;&#25968;&#20026;&#20984;&#25110;&#24369;&#20984;&#65292;&#22312;&#21482;&#20351;&#29992;&#21333;&#29615;&#36335;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#24369;&#20984;&#19988;&#32422;&#26463;&#20026;&#20984;&#25110;&#24369;&#20984;&#30340;&#38750;&#20984;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#27714;&#35299;&#27492;&#31867;&#38382;&#39064;&#65292;&#23427;&#26159;&#19968;&#31181;&#30452;&#35266;&#26131;&#23454;&#29616;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#21482;&#26377;&#22312;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#24050;&#30693;&#20854;Oracle&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#39033;&#38024;&#23545;&#38750;&#20984;&#38382;&#39064;&#30340;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;Oracle&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#38024;&#23545;&#30340;&#38382;&#39064;&#26159;&#27714;&#24471;&#20960;&#20046;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#20998;&#21035;&#23545;&#32422;&#26463;&#20026;&#20984;&#21644;&#24369;&#20984;&#30340;&#24773;&#24418;&#36827;&#34892;&#35752;&#35770;&#12290;&#19982;&#29616;&#26377;&#30340;&#21452;&#29615;&#36335;&#26041;&#27861;&#30456;&#27604;&#65292;&#20132;&#26367;&#27425;&#26799;&#24230;&#27861;&#21487;&#24212;&#29992;&#20110;&#38750;&#20809;&#28369;&#38382;&#39064;&#65292;&#20165;&#20351;&#29992;&#21333;&#29615;&#36335;&#21363;&#21487;&#23454;&#29616;&#30456;&#21516;&#30340;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#20869;&#37096;&#36845;&#20195;&#27425;&#25968;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a non-convex constrained optimization problem, where the objective function is weakly convex and the constraint function is either convex or weakly convex. To solve this problem, we consider the classical switching subgradient method, which is an intuitive and easily implementable first-order method whose oracle complexity was only known for convex problems. This paper provides the first analysis on the oracle complexity of the switching subgradient method for finding a nearly stationary point of non-convex problems. Our results are derived separately for convex and weakly convex constraints. Compared to existing approaches, especially the double-loop methods, the switching gradient method can be applied to non-smooth problems and achieves the same complexity using only a single loop, which saves the effort on tuning the number of inner iterations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13287</link><description>&lt;p&gt;
MILO: &#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#21644;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#21644;&#35843;&#20248;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#20043;&#19968;&#26159;&#36890;&#36807;&#36873;&#25321;&#24456;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#19982;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#22522;&#20934;&#30456;&#27604;&#65292;&#29616;&#26377;&#30340;&#26234;&#33021;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#30001;&#20110;&#32791;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#27493;&#39588;&#32780;&#19981;&#20855;&#31454;&#20105;&#21147;&#65292;&#35813;&#27493;&#39588;&#28041;&#21450;&#35745;&#31639;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#26799;&#24230;&#21644;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#23376;&#27169;&#22359;&#30446;&#26631;&#30340;&#36138;&#24515;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#28040;&#38500;&#23545;&#19979;&#28216;&#27169;&#22411;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MILO&#65292;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#23427;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21333;&#19968;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2301.12586</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#35821;&#35328;&#24314;&#27169;&#32479;&#19968;&#20998;&#23376;&#21644;&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Unifying Molecular and Textual Representations via Multi-task Language Modelling. (arXiv:2301.12586v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21333;&#19968;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#65292;&#36890;&#36807;&#20026;&#20998;&#23376;&#35774;&#35745;&#21644;&#21512;&#25104;&#35268;&#21010;&#25552;&#20379;&#29983;&#25104;&#24335;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#26032;&#26041;&#27861;&#26377;&#28508;&#21147;&#25512;&#21160;&#25968;&#25454;&#39537;&#21160;&#30340;&#31185;&#23398;&#21457;&#29616;&#30340;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#20219;&#21153;&#20173;&#28982;&#38656;&#35201;&#19987;&#38376;&#30340;&#27169;&#22411;&#65292;&#23548;&#33268;&#38656;&#35201;&#29305;&#23450;&#38382;&#39064;&#30340;&#24494;&#35843;&#65292;&#24182;&#24573;&#35270;&#20219;&#21153;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#33258;&#28982;&#35821;&#35328;&#21644;&#21270;&#23398;&#34920;&#31034;&#20043;&#38388;&#32570;&#20047;&#32479;&#19968;&#34920;&#31034;&#65292;&#20174;&#32780;&#20351;&#20154;&#26426;&#20132;&#20114;&#21464;&#24471;&#22797;&#26434;&#21644;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#22495;&#12289;&#22810;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#65292;&#26080;&#38656;&#26114;&#36149;&#30340;&#21333;&#19968;&#22495;&#25110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#26435;&#37325;&#20250;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#23376;&#29983;&#25104;&#12289;&#21453;&#21512;&#25104;&#39044;&#27979;&#12289;&#21270;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in neural language models have also been successfully applied to the field of chemistry, offering generative solutions for classical problems in molecular design and synthesis planning. These new methods have the potential to fuel a new era of data-driven automation in scientific discovery. However, specialized models are still typically required for each task, leading to the need for problem-specific fine-tuning and neglecting task interrelations. The main obstacle in this field is the lack of a unified representation between natural language and chemical representations, complicating and limiting human-machine interaction. Here, we propose the first multi-domain, multi-task language model that can solve a wide range of tasks in both the chemical and natural language domains. Our model can handle chemical and natural language concurrently, without requiring expensive pre-training on single domains or task-specific models. Interestingly, sharing weights across domai
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22122;&#22768;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#30340;&#26694;&#26550;ALIM&#65292;&#36890;&#36807;&#26435;&#34913;&#20505;&#36873;&#38598;&#21644;&#27169;&#22411;&#36755;&#20986;&#26469;&#20943;&#23569;&#26816;&#27979;&#35823;&#24046;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;PLL&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12077</link><description>&lt;p&gt;
ALIM: &#20026;&#22122;&#22768;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#35843;&#25972;&#26631;&#31614;&#37325;&#35201;&#24615;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning. (arXiv:2301.12077v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12077
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22122;&#22768;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#30340;&#26694;&#26550;ALIM&#65292;&#36890;&#36807;&#26435;&#34913;&#20505;&#36873;&#38598;&#21644;&#27169;&#22411;&#36755;&#20986;&#26469;&#20943;&#23569;&#26816;&#27979;&#35823;&#24046;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21487;&#20197;&#19982;&#29616;&#26377;PLL&#26041;&#27861;&#38598;&#25104;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22122;&#22768;&#23616;&#37096;&#26631;&#31614;&#23398;&#20064;&#65288;noisy PLL&#65289;&#26159;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#19982;&#35201;&#27714;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#34255;&#22312;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#30340;PLL&#19981;&#21516;&#65292;&#22122;&#22768;PLL&#25918;&#26494;&#20102;&#36825;&#20010;&#38480;&#21046;&#65292;&#24182;&#20801;&#35768;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21487;&#33021;&#19981;&#22312;&#20505;&#36873;&#26631;&#31614;&#38598;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#24037;&#20316;&#35797;&#22270;&#26816;&#27979;&#22122;&#22768;&#26679;&#26412;&#24182;&#20026;&#27599;&#20010;&#22122;&#22768;&#26679;&#26412;&#20272;&#35745;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#26816;&#27979;&#35823;&#24046;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#36825;&#20123;&#38169;&#35823;&#21487;&#20197;&#22312;&#35757;&#32451;&#26399;&#38388;&#31215;&#32047;&#65292;&#24182;&#25345;&#32493;&#24433;&#21709;&#27169;&#22411;&#20248;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#29702;&#35770;&#20445;&#35777;&#30340;&#22122;&#22768;PLL&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#35843;&#25972;&#26631;&#31614;&#37325;&#35201;&#24615;&#26426;&#21046;&#65288;ALIM&#65289;&#8221;&#12290;&#23427;&#26088;&#22312;&#36890;&#36807;&#26435;&#34913;&#21021;&#22987;&#20505;&#36873;&#38598;&#21644;&#27169;&#22411;&#36755;&#20986;&#26469;&#20943;&#23569;&#26816;&#27979;&#38169;&#35823;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;ALIM&#26159;&#19968;&#31181;&#21487;&#19982;&#29616;&#26377;PLL&#26041;&#27861;&#38598;&#25104;&#30340;&#25554;&#20214;&#31574;&#30053;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22122;&#22768;PLL&#20219;&#21153;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noisy partial label learning (noisy PLL) is an important branch of weakly supervised learning. Unlike PLL where the ground-truth label must conceal in the candidate label set, noisy PLL relaxes this constraint and allows the ground-truth label may not be in the candidate label set. To address this challenging problem, most of the existing works attempt to detect noisy samples and estimate the ground-truth label for each noisy sample. However, detection errors are unavoidable. These errors can accumulate during training and continuously affect model optimization. To this end, we propose a novel framework for noisy PLL with theoretical guarantees, called ``Adjusting Label Importance Mechanism (ALIM)''. It aims to reduce the negative impact of detection errors by trading off the initial candidate set and model outputs. ALIM is a plug-in strategy that can be integrated with existing PLL approaches. Experimental results on benchmark datasets demonstrate that our method can achieve state-of-
&lt;/p&gt;</description></item><item><title>MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11259</link><description>&lt;p&gt;
&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#29983;&#25104;&#19982;&#33258;&#25105;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11259
&lt;/p&gt;
&lt;p&gt;
MolGen&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#21644;&#33258;&#25105;&#21453;&#39304;&#30340;&#33539;&#24335;&#65292;&#23454;&#29616;&#20102;&#21270;&#23398;&#26377;&#25928;&#24615;&#12289;&#22810;&#26679;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#31361;&#30772;&#65292;&#22312;&#20998;&#23376;&#29983;&#25104;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#30340;&#29983;&#25104;&#24050;&#32463;&#21463;&#21040;&#26497;&#22823;&#30340;&#20851;&#27880;&#65292;&#20854;&#38761;&#26032;&#20102;&#31185;&#23398;&#23478;&#35774;&#35745;&#20998;&#23376;&#32467;&#26500;&#30340;&#26041;&#24335;&#65292;&#24182;&#20026;&#21270;&#23398;&#21644;&#33647;&#29289;&#35774;&#35745;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#20998;&#23376;&#29983;&#25104;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#29983;&#25104;&#35821;&#27861;&#25110;&#21270;&#23398;&#23384;&#22312;&#32570;&#38519;&#30340;&#20998;&#23376;&#65292;&#29421;&#31364;&#30340;&#39046;&#22495;&#19987;&#27880;&#20197;&#21450;&#30001;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#25110;&#22806;&#37096;&#20998;&#23376;&#25968;&#25454;&#24211;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#22810;&#26679;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MolGen&#65292;&#23427;&#26159;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#20998;&#23376;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#20998;&#23376;&#35821;&#35328;&#27169;&#22411;&#12290;MolGen&#36890;&#36807;&#37325;&#26500;&#19968;&#20159;&#22810;&#20010;&#20998;&#23376;SELFIES&#33719;&#24471;&#20102;&#22266;&#26377;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#39046;&#22495;&#26080;&#20851;&#30340;&#20998;&#23376;&#21069;&#32512;&#35843;&#25972;&#20419;&#36827;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#21453;&#39304;&#33539;&#24335;&#65292;&#21551;&#21457;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#26368;&#32456;&#19979;&#28216;&#30446;&#26631;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#26356;&#31283;&#20581;&#21644;&#39640;&#25928;&#30340;&#20998;&#23376;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MolGen&#22312;&#21270;&#23398;&#26377;&#25928;&#24615;&#65292;&#22810;&#26679;&#24615;&#65292;&#26032;&#39062;&#24615;&#21644;&#22797;&#26434;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
&lt;/p&gt;</description></item><item><title>FedRAP&#26159;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#20849;&#20139;&#39033;&#23884;&#20837;&#21644;&#26412;&#22320;&#20010;&#24615;&#21270;&#35270;&#22270;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#23545;&#25512;&#33616;&#39033;&#30446;&#24863;&#30693;&#30340;&#20010;&#20307;&#24046;&#24322;&#24182;&#19988;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2301.09109</link><description>&lt;p&gt;
&#24102;&#22686;&#37327;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Federated Recommendation with Additive Personalization. (arXiv:2301.09109v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09109
&lt;/p&gt;
&lt;p&gt;
FedRAP&#26159;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#31995;&#32479;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#35757;&#32451;&#20849;&#20139;&#39033;&#23884;&#20837;&#21644;&#26412;&#22320;&#20010;&#24615;&#21270;&#35270;&#22270;&#65292;&#20197;&#25429;&#25417;&#29992;&#25143;&#23545;&#25512;&#33616;&#39033;&#30446;&#24863;&#30693;&#30340;&#20010;&#20307;&#24046;&#24322;&#24182;&#19988;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26500;&#24314;&#25512;&#33616;&#31995;&#32479;&#26159;&#25512;&#21160;&#19979;&#19968;&#20195;&#20114;&#32852;&#32593;&#26381;&#21153;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26032;&#20852;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;FL&#35757;&#32451;&#20849;&#20139;&#39033;&#23884;&#20837;&#65292;&#21516;&#26102;&#22312;&#23458;&#25143;&#31471;&#20445;&#25345;&#29992;&#25143;&#23884;&#20837;&#31169;&#23494;&#24615;&#12290;&#28982;&#32780;&#65292;&#30456;&#21516;&#23884;&#20837;&#23545;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#39033;&#30446;&#19981;&#33021;&#25429;&#25417;&#21040;&#29992;&#25143;&#23545;&#21516;&#19968;&#39033;&#30446;&#24863;&#30693;&#30340;&#20010;&#20307;&#24046;&#24322;&#65292;&#22240;&#27492;&#23548;&#33268;&#20010;&#24615;&#21270;&#24046;&#12290;&#27492;&#22806;&#65292;FL&#20013;&#30340;&#23494;&#38598;&#39033;&#30446;&#23884;&#20837;&#23548;&#33268;&#36890;&#20449;&#25104;&#26412;&#21644;&#24310;&#36831;&#26114;&#36149;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#22686;&#37327;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#25512;&#33616;&#65288;FedRAP&#65289;&#65292;&#23427;&#36890;&#36807;FL&#23398;&#20064;&#39033;&#30446;&#30340;&#20840;&#23616;&#35270;&#22270;&#24182;&#22312;&#27599;&#20010;&#29992;&#25143;&#26412;&#22320;&#23398;&#20064;&#20010;&#24615;&#21270;&#35270;&#22270;&#12290;FedRAP&#36890;&#36807;&#27491;&#21017;&#21270;&#22686;&#21152;&#27491;&#21017;&#21270;&#26435;&#37325;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#23616;&#37096;&#21644;&#20840;&#23616;&#35270;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35838;&#31243;&#34920;&#26469;&#36880;&#28176;&#23398;&#20064;&#26412;&#22320;&#21644;&#20840;&#23616;&#35270;&#22270;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#20419;&#36827;&#20004;&#31181;&#35270;&#22270;&#20043;&#38388;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#20351;&#20840;&#23616;&#35270;&#22270;&#26356;&#31232;&#30095;&#20197;&#33410;&#30465;FL&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building recommendation systems via federated learning (FL) is a new emerging challenge for advancing next-generation Internet service and privacy protection. Existing approaches train shared item embedding by FL while keeping the user embedding private on client side. However, item embedding identical for all clients cannot capture users' individual differences on perceiving the same item and thus leads to poor personalization. Moreover, dense item embedding in FL results in expensive communication cost and latency. To address these challenges, we propose Federated Recommendation with Additive Personalization (FedRAP), which learns a global view of items via FL and a personalized view locally on each user. FedRAP enforces sparsity of the global view to save FL's communication cost and encourages difference between the two views through regularization. We propose an effective curriculum to learn the local and global views progressively with increasing regularization weights. To produce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22826;&#38451;&#36752;&#23556;&#19979;&#21464;&#24418;&#33322;&#22825;&#22120;&#30340;&#23039;&#24577;&#25511;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20851;&#33410;&#35282;&#24230;&#20248;&#21270;&#26041;&#27861;&#21644;&#36890;&#36807;&#20851;&#33410;&#39537;&#21160;&#30340;&#21160;&#37327;&#38459;&#23612;&#25511;&#21046;&#65292;&#20351;&#21464;&#24418;&#33322;&#22825;&#22120;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#36712;&#36947;&#21644;&#23039;&#24577;&#25511;&#21046;&#26041;&#38754;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.08435</link><description>&lt;p&gt;
&#21463;&#22826;&#38451;&#36752;&#23556;&#21147;&#24433;&#21709;&#30340;&#21464;&#24418;&#33322;&#22825;&#22120;&#20307;&#24577;&#20248;&#21270;&#21450;&#20854;&#20851;&#33410;&#39537;&#21160;&#23039;&#24577;&#31283;&#23450;
&lt;/p&gt;
&lt;p&gt;
Optimization of body configuration and joint-driven attitude stabilization for transformable spacecrafts under solar radiation pressure. (arXiv:2301.08435v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22826;&#38451;&#36752;&#23556;&#19979;&#21464;&#24418;&#33322;&#22825;&#22120;&#30340;&#23039;&#24577;&#25511;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#20851;&#33410;&#35282;&#24230;&#20248;&#21270;&#26041;&#27861;&#21644;&#36890;&#36807;&#20851;&#33410;&#39537;&#21160;&#30340;&#21160;&#37327;&#38459;&#23612;&#25511;&#21046;&#65292;&#20351;&#21464;&#24418;&#33322;&#22825;&#22120;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#36712;&#36947;&#21644;&#23039;&#24577;&#25511;&#21046;&#26041;&#38754;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#24070;&#26159;&#26368;&#26377;&#21069;&#36884;&#30340;&#22826;&#31354;&#25506;&#32034;&#31995;&#32479;&#20043;&#19968;&#65292;&#22240;&#20854;&#21487;&#20197;&#21033;&#29992;&#22826;&#38451;&#36752;&#23556;&#21387;&#21147;&#65288;SRP&#65289;&#29702;&#35770;&#19978;&#20855;&#26377;&#26080;&#38480;&#30340;&#27604;&#20914;&#12290;&#36817;&#24180;&#26469;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#8220;&#21487;&#21464;&#24418;&#33322;&#22825;&#22120;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#39537;&#21160;&#20851;&#33410;&#20027;&#21160;&#37325;&#26032;&#37197;&#32622;&#20854;&#26426;&#36523;&#12290;&#22914;&#26524;&#20687;&#22826;&#38451;&#24070;&#19968;&#26679;&#21033;&#29992;&#21464;&#24418;&#33322;&#22825;&#22120;&#65292;&#20854;&#25511;&#21046;&#33258;&#30001;&#24230;&#39640;&#30340;&#20887;&#20313;&#24230;&#23558;&#26497;&#22823;&#22686;&#24378;&#36712;&#36947;&#21644;&#23039;&#24577;&#25511;&#21046;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#20854;&#22823;&#37327;&#30340;&#36755;&#20837;&#23039;&#24577;&#20351;&#25511;&#21046;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20154;&#21592;&#26045;&#21152;&#20102;&#24378;&#21046;&#24615;&#30340;&#38480;&#21046;&#20197;&#38480;&#21046;&#20854;&#28508;&#22312;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#22826;&#38451;&#36752;&#23556;&#19979;&#21464;&#24418;&#33322;&#22825;&#22120;&#30340;&#23039;&#24577;&#25511;&#21046;&#25216;&#26415;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#20851;&#33410;&#35282;&#24230;&#20248;&#21270;&#26041;&#27861;&#20197;&#33719;&#24471;&#20219;&#24847;&#30340;SRP&#21147;&#21644;&#21147;&#30697;&#65307;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#36890;&#36807;&#20851;&#33410;&#39537;&#21160;&#30340;&#21160;&#37327;&#38459;&#23612;&#25511;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20351;&#21464;&#24418;&#33322;&#22825;&#22120;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#22312;&#36712;&#36947;&#21644;&#23039;&#24577;&#25511;&#21046;&#26041;&#38754;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20223;&#30495;&#32467;&#26524;&#35828;&#26126;&#20102;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#21464;&#24418;&#33322;&#22825;&#22120;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A solar sail is one of the most promising space exploration system because of its theoretically infinite specific impulse using solar radiation pressure (SRP). Recently, some researchers proposed "transformable spacecrafts" that can actively reconfigure their body configurations with actuatable joints. The transformable spacecrafts are expected to greatly enhance orbit and attitude control capability due to its high redundancy in control degree of freedom if they are used like solar sails. However, its large number of input poses difficulties in control, and therefore, previous researchers imposed strong constraints to limit its potential control capabilities. This paper addresses novel attitude control techniques for the transformable spacecrafts under SRP. The authors have constructed two proposed methods; one of those is a joint angle optimization to acquire arbitrary SRP force and torque, and the other is a momentum damping control driven by joint angle actuation. Our proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Maxout&#32593;&#32476;&#20851;&#20110;&#36755;&#20837;&#21644;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#36991;&#20813;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.06956</link><description>&lt;p&gt;
Maxout&#32593;&#32476;&#30340;&#26399;&#26395;&#26799;&#24230;&#21450;&#20854;&#23545;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Expected Gradients of Maxout Networks and Consequences to Parameter Initialization. (arXiv:2301.06956v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Maxout&#32593;&#32476;&#20851;&#20110;&#36755;&#20837;&#21644;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#36991;&#20813;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Maxout&#32593;&#32476;&#30456;&#23545;&#20110;&#36755;&#20837;&#21644;&#21442;&#25968;&#30340;&#26799;&#24230;&#65292;&#24182;&#26681;&#25454;&#32593;&#32476;&#32467;&#26500;&#21644;&#21442;&#25968;&#20998;&#24067;&#24471;&#20986;&#26799;&#24230;&#30340;&#30697;&#19978;&#30028;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36755;&#20837;&#36755;&#20986;Jacobian&#30340;&#20998;&#24067;&#21462;&#20915;&#20110;&#36755;&#20837;&#65292;&#36825;&#20351;&#24471;&#31283;&#23450;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#21464;&#24471;&#22797;&#26434;&#12290;&#22522;&#20110;&#26799;&#24230;&#30697;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36991;&#20813;&#22312;&#23485;&#32593;&#32476;&#20013;&#26799;&#24230;&#28040;&#22833;&#21644;&#29190;&#28856;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#31574;&#30053;&#12290;&#22312;&#28145;&#24230;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#31574;&#30053;&#25913;&#21892;&#20102;Maxout&#32593;&#32476;&#30340;SGD&#21644;Adam&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24471;&#21040;&#20102;&#20851;&#20110;&#26399;&#26395;&#32447;&#24615;&#21306;&#22495;&#25968;&#37327;&#12289;&#26399;&#26395;&#26354;&#32447;&#38271;&#24230;&#22833;&#30495;&#21644;NTK&#30340;&#31934;&#32454;&#30028;&#38480;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the gradients of a maxout network with respect to inputs and parameters and obtain bounds for the moments depending on the architecture and the parameter distribution. We observe that the distribution of the input-output Jacobian depends on the input, which complicates a stable parameter initialization. Based on the moments of the gradients, we formulate parameter initialization strategies that avoid vanishing and exploding gradients in wide networks. Experiments with deep fully-connected and convolutional networks show that this strategy improves SGD and Adam training of deep maxout networks. In addition, we obtain refined bounds on the expected number of linear regions, results on the expected curve length distortion, and results on the NTK.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#25552;&#39640;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.06198</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#30340;&#24191;&#20041;&#31070;&#32463;&#23553;&#38381;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Neural Closure Models with Interpretability. (arXiv:2301.06198v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#26469;&#25552;&#39640;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#26159;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25913;&#36827;&#35745;&#31639;&#29289;&#29702;&#23398;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23398;&#20064;&#32467;&#26524;&#22312;&#35299;&#37322;&#24615;&#21644;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35745;&#31639;&#32593;&#26684;&#20998;&#36776;&#29575;&#12289;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#12289;&#39046;&#22495;&#20960;&#20309;&#21644;&#29289;&#29702;&#25110;&#38382;&#39064;&#29305;&#23450;&#21442;&#25968;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32479;&#19968;&#30340;&#31070;&#32463;&#20559;&#24494;&#20998;&#26041;&#31243;&#26041;&#27861;&#21516;&#26102;&#35299;&#20915;&#20102;&#25152;&#26377;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30452;&#25509;&#22312;&#29616;&#26377;/&#20302;&#20445;&#30495;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#24418;&#24335;&#20013;&#21152;&#20837;&#20102;Markovian&#21644;&#38750;Markovian&#31070;&#32463;&#32593;&#32476;&#23553;&#38381;&#21442;&#25968;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#27169;&#22411;&#30340;&#25299;&#23637;&#12290;Markovian&#39033;&#30340;&#35774;&#35745;&#26088;&#22312;&#23454;&#29616;&#20854;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the predictive capability and computational cost of dynamical models is often at the heart of augmenting computational physics with machine learning (ML). However, most learning results are limited in interpretability and generalization over different computational grid resolutions, initial and boundary conditions, domain geometries, and physical or problem-specific parameters. In the present study, we simultaneously address all these challenges by developing the novel and versatile methodology of unified neural partial delay differential equations. We augment existing/low-fidelity dynamical models directly in their partial differential equation (PDE) forms with both Markovian and non-Markovian neural network (NN) closure parameterizations. The melding of the existing models with NNs in the continuous spatiotemporal space followed by numerical discretization automatically allows for the desired generalizability. The Markovian term is designed to enable extraction of its analy
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#33539;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#24191;&#27867;&#32780;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2301.05763</link><description>&lt;p&gt;
&#19968;&#31181;&#20005;&#26684;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#23545;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#20043;&#21487;&#37325;&#22797;&#20877;&#29616;&#24615;&#33267;&#20851;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
A Rigorous Uncertainty-Aware Quantification Framework Is Essential for Reproducible and Replicable Machine Learning Workflows. (arXiv:2301.05763v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05763
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22522;&#20110;&#36125;&#21494;&#26031;&#33539;&#24335;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#25552;&#20379;&#19968;&#20010;&#24191;&#27867;&#32780;&#20005;&#26684;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#21253;&#21547;&#30340;&#32467;&#26524;&#22797;&#29616;&#33021;&#21147;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#12290;&#19968;&#31181;&#33021;&#22815;&#23450;&#37327;&#35780;&#20272;&#24863;&#20852;&#36259;&#37327;&#65288;QoI&#65289;&#22797;&#21046;&#24615;&#30340;&#19981;&#30830;&#23450;&#24230;&#24863;&#30693;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#26377;&#21161;&#20110;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#33719;&#24471;&#30340;&#32467;&#26524;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#36125;&#21494;&#26031;&#33539;&#24335;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22914;&#20309;&#25552;&#20379;&#19968;&#31181;&#24191;&#27867;&#21644;&#20005;&#26684;&#30340;&#26694;&#26550;&#65292;&#23450;&#37327;&#22320;&#35780;&#20272;&#22797;&#26434;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#23558;&#22635;&#34917;&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#22240;&#20026;&#23427;&#23558;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#30830;&#23450;&#26426;&#22120;&#23398;&#20064;&#25110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#39044;&#27979;&#21464;&#24322;&#23545;&#24037;&#20316;&#27969;&#31243;&#20013;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26399;&#26395;&#36825;&#20010;&#26694;&#26550;&#23558;&#26377;&#21161;&#20110;&#35774;&#35745;&#26356;&#20855;&#37325;&#22797;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to replicate predictions by machine learning (ML) or artificial intelligence (AI) models and results in scientific workflows that incorporate such ML/AI predictions is driven by numerous factors. An uncertainty-aware metric that can quantitatively assess the reproducibility of quantities of interest (QoI) would contribute to the trustworthiness of results obtained from scientific workflows involving ML/AI models. In this article, we discuss how uncertainty quantification (UQ) in a Bayesian paradigm can provide a general and rigorous framework for quantifying reproducibility for complex scientific workflows. Such as framework has the potential to fill a critical gap that currently exists in ML/AI for scientific workflows, as it will enable researchers to determine the impact of ML/AI model prediction variability on the predictive outcomes of ML/AI-powered workflows. We expect that the envisioned framework will contribute to the design of more reproducible and trustworthy wor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;Time-To-First-Spike&#65288;TTFS&#65289;&#32422;&#26463;&#26102;&#65292;&#24615;&#33021;&#12289;&#33021;&#32791;&#12289;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#22810;&#20010;&#33033;&#20914;&#30340;&#26494;&#24347;&#29256;&#26412;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#25918;&#23485;&#33033;&#20914;&#32422;&#26463;&#21487;&#20197;&#25552;&#39640;SNN&#30340;&#24615;&#33021;&#12289;&#33021;&#32791;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#21512;&#29702;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.09500</link><description>&lt;p&gt;
&#25506;&#32034;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Exploring Tradeoffs in Spiking Neural Networks. (arXiv:2212.09500v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;Time-To-First-Spike&#65288;TTFS&#65289;&#32422;&#26463;&#26102;&#65292;&#24615;&#33021;&#12289;&#33021;&#32791;&#12289;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#22810;&#20010;&#33033;&#20914;&#30340;&#26494;&#24347;&#29256;&#26412;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#36825;&#20123;&#26435;&#34913;&#12290;&#23454;&#39564;&#35777;&#26126;&#25918;&#23485;&#33033;&#20914;&#32422;&#26463;&#21487;&#20197;&#25552;&#39640;SNN&#30340;&#24615;&#33021;&#12289;&#33021;&#32791;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#21512;&#29702;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;SNN&#30340;&#25928;&#33021;&#19981;&#20165;&#19982;&#24615;&#33021;&#26377;&#20851;&#65292;&#36824;&#19982;&#33021;&#32791;&#12289;&#39044;&#27979;&#36895;&#24230;&#21644;&#23545;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#26377;&#20851;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;Fast&#65286;Deep&#20197;&#21450;&#20854;&#20182;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#31070;&#32463;&#20803;&#26368;&#22810;&#21482;&#33021;&#21457;&#23556;&#19968;&#27425;&#33033;&#20914;&#32780;&#23454;&#29616;&#24555;&#36895;&#21644;&#33410;&#33021;&#35745;&#31639;&#12290;&#31216;&#20026;"Time-To-First-Spike&#65288;TTFS&#65289;"&#65292;&#20294;&#36825;&#31181;&#32422;&#26463;&#22312;&#35768;&#22810;&#26041;&#38754;&#38480;&#21046;&#20102;SNN&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;&#27492;&#32422;&#26463;&#26102;&#24615;&#33021;&#12289;&#33021;&#32791;&#12289;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#26435;&#34913;&#23384;&#22312;&#30340;&#24773;&#20917;&#65292;&#22312;&#20184;&#20986;&#31232;&#30095;&#24230;&#21644;&#39044;&#27979;&#24310;&#36831;&#30340;&#20195;&#20215;&#19979;&#33719;&#24471;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20123;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fast&#65286;Deep&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#20801;&#35768;&#27599;&#20010;&#31070;&#32463;&#20803;&#21457;&#20986;&#22810;&#20010;&#33033;&#20914;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25918;&#26494;&#33033;&#20914;&#32422;&#26463;&#21487;&#20197;&#25552;&#39640;SNN&#30340;&#24615;&#33021;&#12289;&#33021;&#32791;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#21512;&#29702;&#30340;&#39044;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have emerged as a promising alternative to traditional Deep Neural Networks for low-power computing. However, the effectiveness of SNNs is not solely determined by their performance but also by their energy consumption, prediction speed, and robustness to noise. The recent method Fast \&amp; Deep, along with others, achieves fast and energy-efficient computation by constraining neurons to fire at most once. Known as Time-To-First-Spike (TTFS), this constraint however restricts the capabilities of SNNs in many aspects. In this work, we explore the relationships between performance, energy consumption, speed and stability when using this constraint. More precisely, we highlight the existence of tradeoffs where performance and robustness are gained at the cost of sparsity and prediction latency. To improve these tradeoffs, we propose a relaxed version of Fast \&amp; Deep that allows for multiple spikes per neuron. Our experiments show that relaxing the spike constra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;P2T2&#65292;&#21487;&#20197;&#40065;&#26834;&#22320;&#20174;MRI&#25968;&#25454;&#20013;&#20272;&#35745;$T_2$&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#23454;&#36341;&#21644;&#20855;&#26377;&#24322;&#26500;&#37319;&#38598;&#21327;&#35758;&#30340;&#22823;&#35268;&#27169;&#22810;&#26426;&#26500;&#35797;&#39564;&#12290;</title><link>http://arxiv.org/abs/2212.04928</link><description>&lt;p&gt;
P2T2:&#19968;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29289;&#29702;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#23450;&#37327;$T_2$&#21152;&#26435;MRI&#20013;&#40065;&#26834;&#22320;&#20272;&#35745;$T_2$&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
P2T2: a Physically-primed deep-neural-network approach for robust $T_{2}$ distribution estimation from quantitative $T_{2}$-weighted MRI. (arXiv:2212.04928v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;P2T2&#65292;&#21487;&#20197;&#40065;&#26834;&#22320;&#20174;MRI&#25968;&#25454;&#20013;&#20272;&#35745;$T_2$&#20998;&#24067;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#23454;&#36341;&#21644;&#20855;&#26377;&#24322;&#26500;&#37319;&#38598;&#21327;&#35758;&#30340;&#22823;&#35268;&#27169;&#22810;&#26426;&#26500;&#35797;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#22238;&#27874;$T_2$&#21152;&#26435;MRI&#65288;$T_2W$&#65289;&#25968;&#25454;&#20272;&#35745;$T_2$&#24347;&#35947;&#26102;&#38388;&#20998;&#24067;&#21487;&#20197;&#20026;&#35780;&#20272;&#19981;&#21516;&#30149;&#29702;&#24773;&#20917;&#19979;&#30340;&#28814;&#30151;&#12289;&#33073;&#39635;&#38808;&#12289;&#27700;&#32959;&#21644;&#36719;&#39592;&#32452;&#25104;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#21253;&#25324;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#39592;&#20851;&#33410;&#28814;&#21644;&#32959;&#30244;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#20174;MRI&#25968;&#25454;&#20272;&#35745;$T_2$&#20998;&#24067;&#30340;&#22797;&#26434;&#36870;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#23545;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#30340;&#20020;&#24202;&#25968;&#25454;&#19981;&#22815;&#40065;&#26834;&#65292;&#24182;&#19988;&#38750;&#24120;&#25935;&#24863;&#20110;&#22312;&#37319;&#38598;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#22238;&#27874;&#26102;&#38388;&#65288;TE&#65289;&#30340;&#21464;&#21270;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#20020;&#24202;&#23454;&#36341;&#21644;&#20855;&#26377;&#24322;&#26500;&#37319;&#38598;&#21327;&#35758;&#30340;&#22823;&#35268;&#27169;&#22810;&#26426;&#26500;&#35797;&#39564;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#38459;&#30861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$P_2T_2$&#30340;&#29289;&#29702;&#24341;&#23548;&#30340;DNN&#26041;&#27861;&#65292;&#23427;&#23558;&#20449;&#21495;&#34928;&#20943;&#27491;&#21521;&#27169;&#22411;&#19982;MRI&#20449;&#21495;&#21512;&#24182;&#21040;DNN&#32467;&#26500;&#20013;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating $T_2$ relaxation time distributions from multi-echo $T_2$-weighted MRI ($T_2W$) data can provide valuable biomarkers for assessing inflammation, demyelination, edema, and cartilage composition in various pathologies, including neurodegenerative disorders, osteoarthritis, and tumors. Deep neural network (DNN) based methods have been proposed to address the complex inverse problem of estimating $T_2$ distributions from MRI data, but they are not yet robust enough for clinical data with low Signal-to-Noise ratio (SNR) and are highly sensitive to distribution shifts such as variations in echo-times (TE) used during acquisition. Consequently, their application is hindered in clinical practice and large-scale multi-institutional trials with heterogeneous acquisition protocols. We propose a physically-primed DNN approach, called $P_2T_2$, that incorporates the signal decay forward model in addition to the MRI signal into the DNN architecture to improve the accuracy and robustness o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DPP&#30340;&#26041;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#65292;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#20449;&#24687;&#26469;&#26356;&#26032;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02055</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#20351;&#29992;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#26469;&#36827;&#34892;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Neural Networks with Diverse Negative Samples via Decomposed Determinant Point Processes. (arXiv:2212.02055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DPP&#30340;&#26041;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#65292;&#22312;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#20449;&#24687;&#26469;&#26356;&#26032;&#33410;&#28857;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#36890;&#36807;&#20174;&#33410;&#28857;&#21644;&#23427;&#20204;&#30340;&#25299;&#25169;&#20013;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#26469;&#23454;&#29616;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#30340;&#36136;&#37327;-&#22810;&#26679;&#24615;&#20998;&#35299;&#26041;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#30340;&#36127;&#26679;&#26412;&#65292;&#38500;&#36793;&#32536;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#33410;&#28857;&#37117;&#34987;&#32771;&#34385;&#22312;&#20869;&#65292;&#36825;&#20123;&#33410;&#28857;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#30340;&#20449;&#24687;&#26377;&#21033;&#20110;&#34920;&#31034;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph convolutional networks (GCNs) have achieved great success in graph representation learning by extracting high-level features from nodes and their topology. Since GCNs generally follow a message-passing mechanism, each node aggregates information from its first-order neighbour to update its representation. As a result, the representations of nodes with edges between them should be positively correlated and thus can be considered positive samples. However, there are more non-neighbour nodes in the whole graph, which provide diverse and useful information for the representation update. Two non-adjacent nodes usually have different representations, which can be seen as negative samples. Besides the node representations, the structural information of the graph is also crucial for learning. In this paper, we used quality-diversity decomposition in determinant point processes (DPP) to obtain diverse negative samples. When defining a distribution on diverse subsets of all non-neighbourin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#25512;&#29702;&#26041;&#26696;&#65292;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#26469;&#35757;&#32451;&#20004;&#20010;&#23567;&#22411;&#33976;&#39311;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#20998;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#19988;&#22312;&#22810;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#32463;&#24120;&#20248;&#20110;&#37027;&#20123;&#27809;&#26377;&#32463;&#36807;CoT&#25512;&#29702;&#26041;&#27861;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.00193</link><description>&lt;p&gt;
&#25226;&#25512;&#29702;&#33021;&#21147;&#21387;&#32553;&#21040;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Distilling Reasoning Capabilities into Smaller Language Models. (arXiv:2212.00193v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#25226;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36880;&#27493;&#25512;&#29702;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#25512;&#29702;&#26041;&#26696;&#65292;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#26469;&#35757;&#32451;&#20004;&#20010;&#23567;&#22411;&#33976;&#39311;&#27169;&#22411;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#29992;&#26469;&#20998;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#19988;&#22312;&#22810;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#30340;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#65292;&#32463;&#24120;&#20248;&#20110;&#37027;&#20123;&#27809;&#26377;&#32463;&#36807;CoT&#25512;&#29702;&#26041;&#27861;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#27861;&#65288;&#22914;CoT&#65289;&#22312;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;CoT&#26041;&#27861;&#30340;&#25104;&#21151;&#22522;&#26412;&#19978;&#26159;&#19982;&#27169;&#22411;&#22823;&#23567;&#23494;&#20999;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#21313;&#20159;&#32423;&#21442;&#25968;&#35268;&#27169;&#30340;&#27169;&#22411;&#25165;&#33021;&#20351;CoT&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#22823;&#27169;&#22411;&#30340;&#36880;&#27493;CoT&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#23558;&#36825;&#20123;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#23567;&#30340;&#27169;&#22411;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#25512;&#29702;&#26041;&#26696;&#65306;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#65292;&#23427;&#23398;&#20064;&#23558;&#21407;&#22987;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#38382;&#39064;&#65292;&#24182;&#29992;&#23427;&#26469;&#25351;&#23548;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#25105;&#20204;&#20351;&#29992;&#33487;&#26684;&#25289;&#24213;&#24335;CoT&#26469;&#35757;&#32451;&#20004;&#20010;&#23567;&#22411;&#33976;&#39311;&#27169;&#22411;&#30340;&#32452;&#21512;&#65306;&#38382;&#39064;&#20998;&#35299;&#22120;&#21644;&#23376;&#38382;&#39064;&#27714;&#35299;&#22120;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#26032;&#38382;&#39064;&#65292;&#36825;&#20004;&#20010;&#33976;&#39311;&#27169;&#22411;&#20197;&#21516;&#27493;&#30340;&#26041;&#24335;&#24037;&#20316;&#65292;&#20197;&#20998;&#35299;&#21644;&#35299;&#20915;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;GSM8K&#65292;StrategyQA&#21644;SVAMP&#65289;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#33976;&#39311;&#27169;&#22411;&#23398;&#20250;&#20102;&#39640;&#31934;&#24230;&#22320;&#25191;&#34892;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#65292;&#36890;&#24120;&#20248;&#20110;&#27809;&#26377;&#19987;&#38376;&#20351;&#29992;CoT&#25512;&#29702;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.  In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32858;&#31867;&#21644;&#23884;&#20837;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;&#31934;&#20934;&#21307;&#30103;&#20013;&#30340;&#39640;&#32500;&#38382;&#39064;&#21644;&#32858;&#31867;&#38382;&#39064;&#65292;&#32463;&#39564;&#35777;&#35813;&#26041;&#27861;&#36739;&#24403;&#21069;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.16553</link><description>&lt;p&gt;
&#31616;&#21333;&#39640;&#25928;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#31934;&#20934;&#21307;&#30103;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Algorithms for Cluster-Aware Precision Medicine. (arXiv:2211.16553v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32858;&#31867;&#21644;&#23884;&#20837;&#30340;&#31616;&#21333;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;&#31934;&#20934;&#21307;&#30103;&#20013;&#30340;&#39640;&#32500;&#38382;&#39064;&#21644;&#32858;&#31867;&#38382;&#39064;&#65292;&#32463;&#39564;&#35777;&#35813;&#26041;&#27861;&#36739;&#24403;&#21069;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#25968;&#25454;&#39537;&#21160;&#30340;&#20010;&#24615;&#21270;&#35786;&#26029;&#12289;&#39044;&#21518;&#21644;&#27835;&#30103;&#65292;&#20026;&#31934;&#20934;&#21307;&#30103;&#24102;&#26469;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#21464;&#38761;&#12290;&#28982;&#32780;&#65292;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#32858;&#31867;&#32467;&#26500;&#20351;&#24471;&#22312;&#39640;&#32500;&#24230;&#12289;&#38480;&#21046;&#24615;&#35266;&#27979;&#30340;&#31934;&#20934;&#21307;&#30103;&#39046;&#22495;&#20013;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#20026;&#20102;&#21516;&#26102;&#20811;&#26381;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#32852;&#21512;&#32858;&#31867;&#21644;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;&#23884;&#20837;&#26041;&#27861;&#19982;&#20984;&#32858;&#31867;&#24809;&#32602;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#32467;&#21512;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#23884;&#20837;&#26041;&#27861;&#20811;&#26381;&#20102;&#24403;&#21069;&#32852;&#21512;&#23884;&#20837;&#21644;&#32858;&#31867;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#23618;&#27425;&#32858;&#31867;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#12289;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65288;LLE&#65289;&#21644;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#30340;&#31616;&#21333;&#23454;&#29616;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#26696;&#20363;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI-enabled precision medicine promises a transformational improvement in healthcare outcomes by enabling data-driven personalized diagnosis, prognosis, and treatment. However, the well-known "curse of dimensionality" and the clustered structure of biomedical data together interact to present a joint challenge in the high dimensional, limited observation precision medicine regime. To overcome both issues simultaneously we propose a simple and scalable approach to joint clustering and embedding that combines standard embedding methods with a convex clustering penalty in a modular way. This novel, cluster-aware embedding approach overcomes the complexity and limitations of current joint embedding and clustering methods, which we show with straightforward implementations of hierarchically clustered principal component analysis (PCA), locally linear embedding (LLE), and canonical correlation analysis (CCA). Through both numerical experiments and real-world examples, we demonstrate that our 
&lt;/p&gt;</description></item><item><title>&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.16468</link><description>&lt;p&gt;
&#22240;&#26524;&#22270;&#20013;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs. (arXiv:2211.16468v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16468
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#22270;&#20013;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#21069;&#38376;&#35843;&#25972;&#30340;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#26159;&#23454;&#35777;&#31185;&#23398;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24403;&#31995;&#32479;&#20013;&#28041;&#21450;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#22240;&#32032;&#26102;&#65292;&#36825;&#21464;&#24471;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#21069;&#38376;&#35843;&#25972;&#8212;&#8212;&#19968;&#31181;&#32463;&#20856;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#20013;&#20171;&#21464;&#37327;&#65292;&#21363;&#20351;&#23384;&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#28151;&#28102;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#22240;&#26524;&#25928;&#24212;&#12290;&#34429;&#28982;&#21069;&#38376;&#20272;&#35745;&#30340;&#32479;&#35745;&#29305;&#24615;&#24050;&#32463;&#24456;&#22909;&#22320;&#29702;&#35299;&#20102;&#65292;&#20294;&#23427;&#30340;&#31639;&#27861;&#26041;&#38754;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#24471;&#21040;&#25506;&#31350;&#12290;&#26368;&#36817;&#65292;Jeong&#65292;Tian&#21644;Barenboim [NeurIPS 2022]&#25552;&#20986;&#20102;&#19968;&#31181;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#30340;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#20013;&#25214;&#21040;&#28385;&#36275;&#21069;&#38376;&#20934;&#21017;&#30340;&#38598;&#21512;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20026;$O&#65288;n^3&#65288;n+m&#65289;&#65289;$&#65292;&#20854;&#20013;$n$&#34920;&#31034;&#21464;&#37327;&#30340;&#25968;&#37327;&#65292;$m$&#34920;&#31034;&#22240;&#26524;&#22270;&#30340;&#36793;&#30340;&#25968;&#37327;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#32447;&#24615;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#31639;&#27861;&#65292;&#21363;$O&#65288;n+m&#65289;$&#65292;&#29992;&#20110;&#36825;&#39033;&#20219;&#21153;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#28176;&#36817;&#26368;&#20248;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal effect estimation from observational data is a fundamental task in empirical sciences. It becomes particularly challenging when unobserved confounders are involved in a system. This paper focuses on front-door adjustment -- a classic technique which, using observed mediators allows to identify causal effects even in the presence of unobserved confounding. While the statistical properties of the front-door estimation are quite well understood, its algorithmic aspects remained unexplored for a long time. Recently, Jeong, Tian, and Barenboim [NeurIPS 2022] have presented the first polynomial-time algorithm for finding sets satisfying the front-door criterion in a given directed acyclic graph (DAG), with an $O(n^3(n+m))$ run time, where $n$ denotes the number of variables and $m$ the number of edges of the causal graph. In our work, we give the first linear-time, i.e., $O(n+m)$, algorithm for this task, which thus reaches the asymptotically optimal time complexity. This result impli
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#21487;&#20197;&#38544;&#21547;&#22320;&#32534;&#30721;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#26032;&#31034;&#20363;&#26356;&#26032;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#36896;&#21644;&#27604;&#36739;&#24615;&#36136;&#35777;&#26126;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#23398;&#20064;&#22120;&#39044;&#27979;&#22120;&#21644;&#20174;&#26174;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#30456;&#20284;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15661</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19978;&#19979;&#25991;&#23398;&#20064;&#31639;&#27861;&#65311;&#20351;&#29992;&#32447;&#24615;&#27169;&#22411;&#36827;&#34892;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
What learning algorithm is in-context learning? Investigations with linear models. (arXiv:2211.15661v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15661
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#21487;&#20197;&#38544;&#21547;&#22320;&#32534;&#30721;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#26032;&#31034;&#20363;&#26356;&#26032;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#36896;&#21644;&#27604;&#36739;&#24615;&#36136;&#35777;&#26126;&#20102;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#23398;&#20064;&#22120;&#39044;&#27979;&#22120;&#21644;&#20174;&#26174;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#30456;&#20284;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24207;&#21015;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#36716;&#25442;&#22120;&#65292;&#23637;&#29616;&#20102;&#19968;&#31181;&#38750;&#20961;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#23427;&#20204;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#21576;&#29616;&#30340;&#26631;&#35760;&#31034;&#20363;&#24207;&#21015;$(x,f(x))$&#26500;&#24314;&#26032;&#30340;&#39044;&#27979;&#22120;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#30340;&#21442;&#25968;&#26356;&#26032;&#12290;&#25105;&#20204;&#35843;&#26597;&#20551;&#35774;&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#36890;&#36807;&#22312;&#20854;&#28608;&#27963;&#20013;&#32534;&#30721;&#36739;&#23567;&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#26032;&#31034;&#20363;&#26356;&#26032;&#36825;&#20123;&#38544;&#24335;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#26631;&#20934;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20197;&#32447;&#24615;&#22238;&#24402;&#20316;&#20026;&#21407;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#26465;&#36825;&#20010;&#20551;&#35774;&#30340;&#35777;&#25454;&#26469;&#28304;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#36896;&#35777;&#26126;&#20102;&#36716;&#25442;&#22120;&#21487;&#20197;&#22312;&#26799;&#24230;&#19979;&#38477;&#21644;&#38381;&#24418;&#24335;&#30340;&#23725;&#22238;&#24402;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#32447;&#24615;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#35757;&#32451;&#20986;&#26469;&#30340;&#23398;&#20064;&#22120;&#19982;&#26799;&#24230;&#19979;&#38477;&#12289;&#23725;&#22238;&#24402;&#20197;&#21450;&#31934;&#30830;&#26368;&#23567;&#20108;&#20056;&#22238;&#24402;&#25152;&#35745;&#31639;&#30340;&#39044;&#27979;&#22120;&#38750;&#24120;&#30456;&#20284;&#65292;&#22312;&#36716;&#25442;&#22120;&#30340;&#28145;&#24230;&#21644;&#25968;&#25454;&#38598;&#30340;&#22122;&#22768;&#27700;&#24179;&#21464;&#21270;&#26102;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24230;&#37327;&#23398;&#20064;&#22120;&#39044;&#27979;&#22120;&#21644;&#20174;&#26174;&#24335;&#23398;&#20064;&#31639;&#27861;&#20013;&#33719;&#24471;&#30340;&#39044;&#27979;&#22120;&#20043;&#38388;&#30456;&#20284;&#31243;&#24230;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#24230;&#37327;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#21487;&#20197;&#38544;&#21547;&#22320;&#32534;&#30721;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20013;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#20999;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples $(x, f(x))$ presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#65288;CIM&#65289;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#20808;&#39564;&#20449;&#24687;&#26500;&#24314;&#20869;&#22312;&#30446;&#26631;&#65292;&#24182;&#36866;&#24212;&#24615;&#22320;&#24179;&#34913;&#20869;&#22312;&#21644;&#26174;&#24335;&#30446;&#26631;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;CIM&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15205</link><description>&lt;p&gt;
CIM&#65306;&#22522;&#20110;&#32422;&#26463;&#30340;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#24212;&#23545;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control. (arXiv:2211.15205v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32422;&#26463;&#30340;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#65288;CIM&#65289;&#65292;&#23427;&#21033;&#29992;&#20219;&#21153;&#20808;&#39564;&#20449;&#24687;&#26500;&#24314;&#20869;&#22312;&#30446;&#26631;&#65292;&#24182;&#36866;&#24212;&#24615;&#22320;&#24179;&#34913;&#20869;&#22312;&#21644;&#26174;&#24335;&#30446;&#26631;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#65292;CIM&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#22312;&#21160;&#26426;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25506;&#32034;&#25216;&#26415;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#31232;&#30095;&#25110;&#27809;&#26377;&#26174;&#24335;&#22870;&#21169;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20869;&#22312;&#21160;&#26426;&#30340;&#35774;&#35745;&#23384;&#22312;&#20004;&#20010;&#25216;&#26415;&#38590;&#28857;&#65306;1&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#24403;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#20419;&#36827;&#26377;&#25928;&#25506;&#32034;&#65307;2&#65289;&#22914;&#20309;&#19982;&#26174;&#24335;&#22870;&#21169;&#36827;&#34892;&#26377;&#25928;&#25972;&#21512;&#65292;&#20197;&#24110;&#21161;&#25214;&#21040;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#30446;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#20869;&#22312;&#30446;&#26631;&#37117;&#26159;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#35774;&#35745;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21152;&#27861;&#19982;&#26174;&#24335;&#22870;&#21169;&#36827;&#34892;&#25972;&#21512;&#65288;&#25110;&#22312;&#26080;&#22870;&#21169;&#30340;&#39044;&#35757;&#32451;&#20013;&#21333;&#29420;&#20351;&#29992;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#35774;&#35745;&#22312;&#20856;&#22411;&#30340;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#23558;&#20250;&#22833;&#36133;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#20869;&#22312;&#21160;&#26426;&#65288;CIM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#21487;&#33719;&#24471;&#30340;&#20219;&#21153;&#20808;&#39564;&#20449;&#24687;&#26469;&#26500;&#24314;&#32422;&#26463;&#30340;&#20869;&#22312;&#30446;&#26631;&#65292;&#24182;&#21033;&#29992;Lagrangian&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#12289;&#39640;&#25928;&#21644;&#21407;&#21017;&#24615;&#30340;&#26694;&#26550;&#36866;&#24212;&#24615;&#22320;&#24179;&#34913;&#20869;&#22312;&#21644;&#26174;&#24335;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#31232;&#30095;&#22870;&#21169;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;CIM&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#20869;&#22312;&#21160;&#26426;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intrinsic motivation is a promising exploration technique for solving reinforcement learning tasks with sparse or absent extrinsic rewards. There exist two technical challenges in implementing intrinsic motivation: 1) how to design a proper intrinsic objective to facilitate efficient exploration; and 2) how to combine the intrinsic objective with the extrinsic objective to help find better solutions. In the current literature, the intrinsic objectives are all designed in a task-agnostic manner and combined with the extrinsic objective via simple addition (or used by itself for reward-free pre-training). In this work, we show that these designs would fail in typical sparse-reward continuous control tasks. To address the problem, we propose Constrained Intrinsic Motivation (CIM) to leverage readily attainable task priors to construct a constrained intrinsic objective, and at the same time, exploit the Lagrangian method to adaptively balance the intrinsic and extrinsic objectives via a si
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTask&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.15055</link><description>&lt;p&gt;
AdaTask: &#19968;&#31181;&#38754;&#21521;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaTask: A Task-aware Adaptive Learning Rate Approach to Multi-task Learning. (arXiv:2211.15055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15055
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTask&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#27169;&#22411;&#24050;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#27599;&#20010;&#21442;&#25968;&#19978;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#27599;&#20010;&#20219;&#21153;&#23545;&#35813;&#21442;&#25968;&#36827;&#34892;&#30340;&#24635;&#26356;&#26032;&#26469;&#34913;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21183;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#25968;&#34928;&#20943;&#30340;&#24179;&#22343;&#26356;&#26032;&#65288;AU&#65289;&#26469;&#35745;&#31639;&#27599;&#20010;&#20219;&#21153;&#22312;&#35813;&#21442;&#25968;&#19978;&#30340;&#24635;&#26356;&#26032;&#25968;&#12290;&#22522;&#20110;&#36825;&#19968;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;MTL&#26041;&#27861;&#20013;&#30340;&#35768;&#22810;&#21442;&#25968;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#39640;&#30340;&#20849;&#20139;&#23618;&#20013;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#21463;&#21040;&#19968;&#20010;&#25110;&#20960;&#20010;&#20219;&#21153;&#30340;&#25903;&#37197;&#12290;AU&#30340;&#25903;&#37197;&#20027;&#35201;&#26159;&#30001;&#20110;&#19968;&#20010;&#25110;&#20960;&#20010;&#20219;&#21153;&#30340;&#26799;&#24230;&#32047;&#31215;&#23548;&#33268;&#30340;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaTask&#30340;&#20219;&#21153;&#24863;&#30693;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#20197;&#20998;&#31163;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#30340;&#32047;&#31215;&#26799;&#24230;&#65292;&#20174;&#32780;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;AdaTask&#26681;&#25454;AU&#20540;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#19981;&#21516;&#20219;&#21153;&#30340;&#23398;&#20064;&#29575;&#65292;&#20197;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;AdaTask&#65292;&#24182;&#35777;&#26126;&#23427;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) models have demonstrated impressive results in computer vision, natural language processing, and recommender systems. Even though many approaches have been proposed, how well these approaches balance different tasks on each parameter still remains unclear. In this paper, we propose to measure the task dominance degree of a parameter by the total updates of each task on this parameter. Specifically, we compute the total updates by the exponentially decaying Average of the squared Updates (AU) on a parameter from the corresponding task.Based on this novel metric, we observe that many parameters in existing MTL methods, especially those in the higher shared layers, are still dominated by one or several tasks. The dominance of AU is mainly due to the dominance of accumulative gradients from one or several tasks. Motivated by this, we propose a Task-wise Adaptive learning rate approach, AdaTask in short, to separate the \emph{accumulative gradients} and hence the l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#24179;&#31227;&#19981;&#21464;&#25110;&#31561;&#21464;&#20989;&#25968;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#27531;&#24046;&#21644;&#38750;&#27531;&#24046;&#21464;&#20307;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#26222;&#36866;&#36924;&#36817;&#65292;&#21516;&#26102;&#36825;&#20123;&#26465;&#20214;&#26159;&#24517;&#35201;&#30340;&#12290;</title><link>http://arxiv.org/abs/2211.14047</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;&#24615;&#36136;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Universal Approximation Property of Deep Fully Convolutional Neural Networks. (arXiv:2211.14047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#24179;&#31227;&#19981;&#21464;&#25110;&#31561;&#21464;&#20989;&#25968;&#30340;&#36924;&#36817;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#27531;&#24046;&#21644;&#38750;&#27531;&#24046;&#21464;&#20307;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#23454;&#29616;&#26222;&#36866;&#36924;&#36817;&#65292;&#21516;&#26102;&#36825;&#20123;&#26465;&#20214;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21160;&#21147;&#31995;&#32479;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#20840;&#21367;&#31215;&#32593;&#32476;&#23545;&#24179;&#31227;&#19981;&#21464;&#25110;&#31561;&#21464;&#20989;&#25968;&#30340;&#36924;&#36817;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36890;&#36947;&#23485;&#24230;&#20026;&#24120;&#25968;&#26102;&#65292;&#28145;&#24230;&#27531;&#24046;&#20840;&#21367;&#31215;&#32593;&#32476;&#21450;&#20854;&#36830;&#32493;&#23618;&#23545;&#24212;&#30340;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#23545;&#31216;&#20989;&#25968;&#30340;&#26222;&#36866;&#36924;&#36817;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#33267;&#23569;&#27599;&#23618;&#20855;&#26377;2&#20010;&#36890;&#36947;&#21644;&#33267;&#23569;2&#30340;&#21367;&#31215;&#26680;&#23610;&#23544;&#30340;&#38750;&#27531;&#24046;&#21464;&#20307;&#20063;&#33021;&#36798;&#21040;&#30456;&#21516;&#30340;&#36924;&#36817;&#24615;&#33021;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#36825;&#20123;&#35201;&#27714;&#26159;&#24517;&#35201;&#30340;&#65292;&#22240;&#20026;&#36890;&#36947;&#25968;&#37327;&#36739;&#23569;&#25110;&#21367;&#31215;&#26680;&#23610;&#23544;&#36739;&#23567;&#30340;&#32593;&#32476;&#19981;&#33021;&#23454;&#29616;&#26222;&#36866;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the approximation of shift-invariant or equivariant functions by deep fully convolutional networks from the dynamical systems perspective. We prove that deep residual fully convolutional networks and their continuous-layer counterpart can achieve universal approximation of these symmetric functions at constant channel width. Moreover, we show that the same can be achieved by non-residual variants with at least 2 channels in each layer and convolutional kernel size of at least 2. In addition, we show that these requirements are necessary, in the sense that networks with fewer channels or smaller kernels fail to be universal approximators.
&lt;/p&gt;</description></item><item><title>Diffiner&#26159;&#19968;&#31181;&#22522;&#20110;DNN&#30340;&#29983;&#25104;&#32454;&#21270;&#22120;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#32463;&#36807;SE&#26041;&#27861;&#39044;&#22788;&#29702;&#21518;&#30340;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;SE&#26041;&#27861;&#65292;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.17287</link><description>&lt;p&gt;
Diffiner: &#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#22686;&#24378;&#30340;&#22810;&#21151;&#33021;&#25193;&#25955;&#29983;&#25104;&#32454;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Diffiner: A Versatile Diffusion-based Generative Refiner for Speech Enhancement. (arXiv:2210.17287v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17287
&lt;/p&gt;
&lt;p&gt;
Diffiner&#26159;&#19968;&#31181;&#22522;&#20110;DNN&#30340;&#29983;&#25104;&#32454;&#21270;&#22120;&#65292;&#21487;&#29992;&#20110;&#25913;&#21892;&#32463;&#36807;SE&#26041;&#27861;&#39044;&#22788;&#29702;&#21518;&#30340;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;SE&#26041;&#27861;&#65292;&#19988;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#26041;&#27861;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;DNN&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38477;&#20302;&#25152;&#29983;&#25104;&#36755;&#20986;&#30340;&#24863;&#30693;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;DNN&#30340;&#29983;&#25104;&#32454;&#21270;&#22120;&#65292;Diffiner&#65292;&#26088;&#22312;&#25913;&#21892;&#36890;&#36807;SE&#26041;&#27861;&#39044;&#22788;&#29702;&#36807;&#30340;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#20165;&#21253;&#21547;&#28165;&#26224;&#35821;&#38899;&#30340;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#32454;&#21270;&#22120;&#26377;&#25928;&#22320;&#23558;&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#26032;&#29983;&#25104;&#30340;&#28165;&#26224;&#37096;&#20998;&#19982;&#20043;&#21069;&#30340;SE&#26041;&#27861;&#36896;&#25104;&#30340;&#36864;&#21270;&#21644;&#22833;&#30495;&#37096;&#20998;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#20135;&#29983;&#32454;&#21270;&#30340;&#35821;&#38899;&#12290;&#19968;&#26086;&#25105;&#20204;&#30340;&#32454;&#21270;&#22120;&#35757;&#32451;&#20026;&#19968;&#32452;&#28165;&#26224;&#30340;&#35821;&#38899;&#65292;&#23427;&#23601;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;SE&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;SE&#27169;&#22359;&#19987;&#38376;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#32454;&#21270;&#22120;&#21487;&#20197;&#26159;&#30456;&#23545;&#20110;SE&#26041;&#27861;&#30340;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#21518;&#22788;&#29702;&#27169;&#22359;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#30340;&#27169;&#22359;&#21270;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#24863;&#30693;&#35821;&#38899;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep neural network (DNN)-based speech enhancement (SE) methods outperform the previous non-DNN-based ones, they often degrade the perceptual quality of generated outputs. To tackle this problem, we introduce a DNN-based generative refiner, Diffiner, aiming to improve perceptual speech quality pre-processed by an SE method. We train a diffusion-based generative model by utilizing a dataset consisting of clean speech only. Then, our refiner effectively mixes clean parts newly generated via denoising diffusion restoration into the degraded and distorted parts caused by a preceding SE method, resulting in refined speech. Once our refiner is trained on a set of clean speech, it can be applied to various SE methods without additional training specialized for each SE module. Therefore, our refiner can be a versatile post-processing module w.r.t. SE methods and has high potential in terms of modularity. Experimental results show that our method improved perceptual speech quality rega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Masked Autoencoders&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#37325;&#26500;&#34987;&#36861;&#36394;&#38169;&#35823;&#30340;&#21475;&#33108;&#23398;&#35760;&#24405;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;XRMB&#25968;&#25454;&#38598;&#30740;&#31350;&#20013;&#12290;</title><link>http://arxiv.org/abs/2210.15195</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26159;&#21475;&#33108;&#23398;&#20064;&#30340;&#21033;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders Are Articulatory Learners. (arXiv:2210.15195v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Masked Autoencoders&#26041;&#27861;&#65292;&#21487;&#20197;&#31934;&#30830;&#37325;&#26500;&#34987;&#36861;&#36394;&#38169;&#35823;&#30340;&#21475;&#33108;&#23398;&#35760;&#24405;&#65292;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;XRMB&#25968;&#25454;&#38598;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#33108;&#23398;&#35760;&#24405;&#19979;&#19981;&#21516;&#21475;&#33108;&#37096;&#20301;&#30340;&#20301;&#32622;&#21644;&#36816;&#21160;&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#20135;&#29983;&#20197;&#21450;&#24320;&#21457;&#22522;&#20110;&#21475;&#33108;&#23398;&#30340;&#35821;&#38899;&#21512;&#25104;&#22120;&#21644;&#35821;&#38899;&#21453;&#28436;&#31995;&#32479;&#12290;&#23041;&#26031;&#24247;&#26143;&#22823;&#23398;X&#23556;&#32447;&#24494;&#26463;&#65288;XRMB&#65289;&#25968;&#25454;&#38598;&#26159;&#25552;&#20379;&#19982;&#38899;&#39057;&#35760;&#24405;&#21516;&#27493;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290; XRMB&#21475;&#33108;&#23398;&#35760;&#24405;&#20351;&#29992;&#25918;&#32622;&#22312;&#22810;&#20010;&#21475;&#33108;&#37096;&#20301;&#30340;&#39063;&#31890;&#65292;&#21487;&#20197;&#30001;&#24494;&#26463;&#36319;&#36394;&#12290;&#28982;&#32780;&#65292;&#24456;&#22823;&#19968;&#37096;&#20998;&#21475;&#33108;&#23398;&#35760;&#24405;&#34987;&#36319;&#36394;&#38169;&#35823;&#65292;&#19968;&#30452;&#26080;&#27861;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110; Masked Autoencoders &#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#37325;&#26500; XRMB&#25968;&#25454;&#38598;&#30340;47&#20010;&#28436;&#35762;&#32773;&#20013;41&#20010;&#30340;&#34987;&#36861;&#36394;&#38169;&#35823;&#30340;&#21475;&#33108;&#23398;&#35760;&#24405;&#12290;&#24403;&#20843;&#20010;&#21475;&#33108;&#37096;&#20301;&#20013;&#30340;&#19977;&#20010;&#34987;&#35823;&#36861;&#36394;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#37325;&#26500;&#21475;&#33108;&#23398;&#36712;&#36857;&#65292;&#19982;&#30495;&#23454;&#24773;&#20917;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Articulatory recordings track the positions and motion of different articulators along the vocal tract and are widely used to study speech production and to develop speech technologies such as articulatory based speech synthesizers and speech inversion systems. The University of Wisconsin X-Ray microbeam (XRMB) dataset is one of various datasets that provide articulatory recordings synced with audio recordings. The XRMB articulatory recordings employ pellets placed on a number of articulators which can be tracked by the microbeam. However, a significant portion of the articulatory recordings are mistracked, and have been so far unsuable. In this work, we present a deep learning based approach using Masked Autoencoders to accurately reconstruct the mistracked articulatory recordings for 41 out of 47 speakers of the XRMB dataset. Our model is able to reconstruct articulatory trajectories that closely match ground truth, even when three out of eight articulators are mistracked, and retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#19977;&#20010;&#31163;&#25955;PDEs&#30340;&#31070;&#32463;&#23553;&#38381;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20808;&#31163;&#25955;&#21270;&#20877;&#20248;&#21270;&#30340;&#36712;&#36857;&#25311;&#21512;&#26159;&#39318;&#36873;&#65292;&#27604;&#23548;&#25968;&#25311;&#21512;&#26356;&#20934;&#30830;&#12289;&#27604;&#20808;&#20248;&#21270;&#20877;&#31163;&#25955;&#21270;&#26356;&#31283;&#23450;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.14675</link><description>&lt;p&gt;
&#31163;&#25955;PDEs&#30340;&#31070;&#32463;&#23553;&#38381;&#27169;&#22411;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of neural closure models for discretised PDEs. (arXiv:2210.14675v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#19977;&#20010;&#31163;&#25955;PDEs&#30340;&#31070;&#32463;&#23553;&#38381;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20808;&#31163;&#25955;&#21270;&#20877;&#20248;&#21270;&#30340;&#36712;&#36857;&#25311;&#21512;&#26159;&#39318;&#36873;&#65292;&#27604;&#23548;&#25968;&#25311;&#21512;&#26356;&#20934;&#30830;&#12289;&#27604;&#20808;&#20248;&#21270;&#20877;&#31163;&#25955;&#21270;&#26356;&#31283;&#23450;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#23553;&#38381;&#27169;&#22411;&#24050;&#32463;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#29992;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#36924;&#36817;&#22810;&#23610;&#24230;&#31995;&#32479;&#20013;&#23567;&#23610;&#24230;&#30340;&#26041;&#27861;&#12290;&#25439;&#22833;&#20989;&#25968;&#30340;&#36873;&#25321;&#21644;&#30456;&#20851;&#30340;&#35757;&#32451;&#36807;&#31243;&#23545;&#29983;&#25104;&#30340;&#31070;&#32463;&#23553;&#38381;&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#31283;&#23450;&#24615;&#24433;&#21709;&#24040;&#22823;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;"&#23548;&#25968;&#25311;&#21512;"&#12289;&#20808;&#31163;&#25955;&#21270;&#20877;&#20248;&#21270;&#30340;&#36712;&#36857;&#25311;&#21512;&#21644;&#20808;&#20248;&#21270;&#20877;&#31163;&#25955;&#21270;&#30340;&#36712;&#36857;&#25311;&#21512;&#12290;&#23548;&#25968;&#25311;&#21512;&#26159;&#27010;&#24565;&#19978;&#26368;&#31616;&#21333;&#21644;&#35745;&#31639;&#19978;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65288;Kuramoto-Sivashinsky&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65288;Burgers&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36712;&#36857;&#25311;&#21512;&#35745;&#31639;&#19978;&#26356;&#21152;&#26114;&#36149;&#65292;&#20294;&#26356;&#21152;&#40065;&#26834;&#65292;&#22240;&#27492;&#26159;&#39318;&#36873;&#30340;&#26041;&#27861;&#12290;&#22312;&#20004;&#31181;&#36712;&#36857;&#25311;&#21512;&#26041;&#27861;&#20013;&#65292;&#20808;&#31163;&#25955;&#21270;&#20877;&#20248;&#21270;&#30340;&#26041;&#27861;&#27604;&#20808;&#20248;&#21270;&#20877;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#26356;&#20934;&#30830;&#12290;&#34429;&#28982;&#20808;&#20248;&#21270;&#20877;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#26356;&#21152;&#39640;&#25928;&#65292;&#20294;&#31283;&#23450;&#24615;&#36739;&#24046;&#65292;&#21487;&#33021;&#38656;&#35201;&#31934;&#24515;&#35843;&#25972;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural closure models have recently been proposed as a method for efficiently approximating small scales in multiscale systems with neural networks. The choice of loss function and associated training procedure has a large effect on the accuracy and stability of the resulting neural closure model. In this work, we systematically compare three distinct procedures: "derivative fitting", "trajectory fitting" with discretise-then-optimise, and "trajectory fitting" with optimise-then-discretise. Derivative fitting is conceptually the simplest and computationally the most efficient approach and is found to perform reasonably well on one of the test problems (Kuramoto-Sivashinsky) but poorly on the other (Burgers). Trajectory fitting is computationally more expensive but is more robust and is therefore the preferred approach. Of the two trajectory fitting procedures, the discretise-then-optimise approach produces more accurate models than the optimise-then-discretise approach. While the optim
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;</title><link>http://arxiv.org/abs/2210.06015</link><description>&lt;p&gt;
EC-NAS: &#38754;&#21521;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#33021;&#32791;&#24863;&#30693;&#34920;&#26684;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06015
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#32791;&#24863;&#30693;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#34920;&#26684;&#22522;&#20934; EC-NAS&#65292;&#35813;&#22522;&#20934;&#36890;&#36807;&#28155;&#21152;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36873;&#25321;&#12289;&#35757;&#32451;&#21644;&#37096;&#32626;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25152;&#38656;&#30340;&#33021;&#37327;&#28040;&#32791;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#26088;&#22312;&#25903;&#25345;&#35774;&#35745;&#33021;&#25928;&#39640;&#12289;&#35757;&#32451;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#12289;&#36866;&#29992;&#20110;&#23454;&#38469;&#36793;&#32536;/&#31227;&#21160;&#35745;&#31639;&#29615;&#22659;&#24182;&#20855;&#26377;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#33021;&#25928;&#20316;&#20026;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034; (NAS) &#30340;&#19968;&#39033;&#39069;&#22806;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#19981;&#21516;&#26550;&#26500;&#30340;&#33021;&#32791;&#21644;&#30899;&#36275;&#36857;&#20449;&#24687;&#65292;&#25552;&#20379;&#26356;&#26032;&#30340;&#34920;&#26684;&#22522;&#20934; EC-NAS &#20197;&#22312;&#36739;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#35780;&#20272; NAS &#31574;&#30053;&#12290;EC-NAS &#36824;&#21253;&#25324;&#29992;&#20110;&#39044;&#27979;&#33021;&#32791;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#26377;&#21161;&#20110;&#38477;&#20302;&#24635;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
&lt;/p&gt;</description></item><item><title>&#22312;S-MBD&#38382;&#39064;&#20013;,&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23637;&#24320;&#24335;&#21387;&#32553;&#26041;&#27861;&#65292;&#20174;&#23569;&#37327;&#30340;&#26102;&#38388;&#27979;&#37327;&#20013;&#26080;&#25439;&#24674;&#22797;&#31232;&#30095;&#28388;&#27874;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#23637;&#24320;&#24335;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.14165</link><description>&lt;p&gt;
&#38750;&#23637;&#24320;&#24335;&#21387;&#32553;&#30450;&#21453;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Unrolled Compressed Blind-Deconvolution. (arXiv:2209.14165v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14165
&lt;/p&gt;
&lt;p&gt;
&#22312;S-MBD&#38382;&#39064;&#20013;,&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23637;&#24320;&#24335;&#21387;&#32553;&#26041;&#27861;&#65292;&#20174;&#23569;&#37327;&#30340;&#26102;&#38388;&#27979;&#37327;&#20013;&#26080;&#25439;&#24674;&#22797;&#31232;&#30095;&#28388;&#27874;&#22120;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#23637;&#24320;&#24335;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#22810;&#36890;&#36947;&#30450;&#21453;&#21367;&#31215;&#38382;&#39064;(S-MBD)&#32463;&#24120;&#20986;&#29616;&#22312;&#35832;&#22914;&#38647;&#36798;/&#22768;&#32435;/&#36229;&#22768;&#25104;&#20687;&#31561;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#21644;&#23454;&#29616;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;&#26041;&#27861;&#65292;&#20351;&#24471;&#20174;&#26102;&#38388;&#19978;&#30340;&#23569;&#37327;&#27979;&#37327;&#20013;&#23601;&#33021;&#36827;&#34892;&#30450;&#24674;&#22797;&#12290;&#25152;&#25552;&#20986;&#30340;&#21387;&#32553;&#26041;&#27861;&#36890;&#36807;&#20808;&#23558;&#20449;&#21495;&#32463;&#36807;&#28388;&#27874;&#22120;&#20877;&#36827;&#34892;&#23376;&#37319;&#26679;&#27979;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#23454;&#29616;&#25104;&#26412;&#38477;&#20302;&#12290;&#25105;&#20204;&#20174;&#21387;&#32553;&#27979;&#37327;&#20013;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#31232;&#30095;&#28388;&#27874;&#22120;&#30340;&#21487;&#36776;&#35782;&#24615;&#21644;&#24674;&#22797;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20801;&#35768;&#35774;&#35745;&#19968;&#31995;&#21015;&#21387;&#32553;&#28388;&#27874;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#38750;&#23637;&#24320;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#21387;&#32553;&#28388;&#27874;&#22120;&#24182;&#35299;&#20915;S-MBD&#38382;&#39064;&#12290;&#32534;&#30721;&#22120;&#26159;&#19968;&#20010;&#24490;&#29615;&#25512;&#29702;&#32593;&#32476;&#65292;&#23558;&#21387;&#32553;&#27979;&#37327;&#26144;&#23556;&#21040;&#31232;&#30095;&#28388;&#27874;&#22120;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#38750;&#23637;&#24320;&#24335;&#23398;&#20064;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#36873;&#25321;&#26377;&#22122;&#22768;&#30340;&#21333;&#20301;&#20998;&#35299;&#31639;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of sparse multichannel blind deconvolution (S-MBD) arises frequently in many engineering applications such as radar/sonar/ultrasound imaging. To reduce its computational and implementation cost, we propose a compression method that enables blind recovery from much fewer measurements with respect to the full received signal in time. The proposed compression measures the signal through a filter followed by a subsampling, allowing for a significant reduction in implementation cost. We derive theoretical guarantees for the identifiability and recovery of a sparse filter from compressed measurements. Our results allow for the design of a wide class of compression filters. We, then, propose a data-driven unrolled learning framework to learn the compression filter and solve the S-MBD problem. The encoder is a recurrent inference network that maps compressed measurements into an estimate of sparse filters. We demonstrate that our unrolled learning method is more robust to choices o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#30340;&#8220;&#34920;&#31034;&#22797;&#26434;&#24230;&#8221;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#36328;&#22810;&#20010;&#31070;&#32463;&#20803;&#25193;&#25955;&#30340;&#20449;&#24687;&#35775;&#38382;&#38590;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.10438</link><description>&lt;p&gt;
&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#30340;&#31070;&#32463;&#34920;&#31034;&#22797;&#26434;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Measure of the Complexity of Neural Representations based on Partial Information Decomposition. (arXiv:2209.10438v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#30340;&#8220;&#34920;&#31034;&#22797;&#26434;&#24230;&#8221;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#36328;&#22810;&#20010;&#31070;&#32463;&#20803;&#25193;&#25955;&#30340;&#20449;&#24687;&#35775;&#38382;&#38590;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#36890;&#24120;&#26159;&#30001;&#31070;&#32463;&#20803;&#32676;&#32852;&#21512;&#34920;&#31034;&#30340;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#20998;&#31867;&#26631;&#31614;&#30340;&#20114;&#20449;&#24687;&#22914;&#20309;&#22312;&#21333;&#20010;&#31070;&#32463;&#20803;&#20043;&#38388;&#20998;&#37197;&#30340;&#32454;&#33410;&#23578;&#19981;&#28165;&#26970;&#65306;&#34429;&#28982;&#37096;&#20998;&#20114;&#20449;&#24687;&#21482;&#33021;&#20174;&#29305;&#23450;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#20013;&#33719;&#24471;&#65292;&#20294;&#20854;&#20182;&#37096;&#20998;&#21017;&#30001;&#22810;&#20010;&#31070;&#32463;&#20803;&#20887;&#20313;&#25110;&#21327;&#21516;&#25215;&#36733;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#20449;&#24687;&#35770;&#30340;&#37096;&#20998;&#20449;&#24687;&#20998;&#35299;&#26469;&#20998;&#31163;&#36825;&#20123;&#19981;&#21516;&#30340;&#36129;&#29486;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#34920;&#31034;&#22797;&#26434;&#24230;&#8221;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#36328;&#22810;&#20010;&#31070;&#32463;&#20803;&#25193;&#25955;&#30340;&#20449;&#24687;&#35775;&#38382;&#38590;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#20309;&#30452;&#25509;&#35745;&#31639;&#36739;&#23567;&#23618;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#38024;&#23545;&#36739;&#22823;&#23618;&#25552;&#20986;&#20102;&#23376;&#25277;&#26679;&#21644;&#31895;&#31890;&#21270;&#36807;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#23545;&#24212;&#30340;&#19978;&#38480;&#12290;&#22312;MNIST&#21644;CIFAR10&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#22312;&#37327;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#34920;&#31034;&#22797;&#26434;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of "Representational Complexity", which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#32447;&#24615;&#21270;&#21407;&#22987;-&#23545;&#20598;&#65288;ALPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#21644;&#32447;&#24615;&#21270;PD&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#20013;LPD&#26041;&#27861;&#23545;&#20110;&#21407;&#22987;&#20989;&#25968;Lipschitz&#24120;&#25968;&#20381;&#36182;&#24615;&#27425;&#20248;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;SPP&#20855;&#26377;&#21322;&#32447;&#24615;&#32806;&#21512;&#20989;&#25968;&#65292;ALPD&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#65292;&#32780;&#38024;&#23545;&#20855;&#26377;&#19968;&#33324;&#38750;&#32447;&#24615;&#32806;&#21512;&#20989;&#25968;&#30340;SPP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;ALPD&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.04604</link><description>&lt;p&gt;
&#21152;&#36895;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#22788;&#29702;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Accelerated Primal-Dual Methods for Convex-Strongly-Concave Saddle Point Problems. (arXiv:2209.04604v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#30340;&#32447;&#24615;&#21270;&#21407;&#22987;-&#23545;&#20598;&#65288;ALPD&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#21644;&#32447;&#24615;&#21270;PD&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#20013;LPD&#26041;&#27861;&#23545;&#20110;&#21407;&#22987;&#20989;&#25968;Lipschitz&#24120;&#25968;&#20381;&#36182;&#24615;&#27425;&#20248;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;SPP&#20855;&#26377;&#21322;&#32447;&#24615;&#32806;&#21512;&#20989;&#25968;&#65292;ALPD&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#26799;&#24230;&#22797;&#26434;&#24230;&#65292;&#32780;&#38024;&#23545;&#20855;&#26377;&#19968;&#33324;&#38750;&#32447;&#24615;&#32806;&#21512;&#20989;&#25968;&#30340;SPP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#30340;ALPD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#65288;PD&#65289;&#26041;&#27861;&#22788;&#29702;&#38797;&#28857;&#38382;&#39064;&#65288;SPP&#65289;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#21407;&#22987;&#20989;&#25968;&#30340;&#32447;&#24615;&#36924;&#36817;&#20195;&#26367;&#26631;&#20934;&#30340;&#37051;&#22495;&#27493;&#39588;&#65292;&#20174;&#32780;&#24471;&#21040;&#32447;&#24615;&#21270;&#30340;PD&#65288;LPD&#65289;&#26041;&#27861;&#12290;&#23545;&#20110;&#20984;-&#24378;&#20985;&#38797;&#28857;&#38382;&#39064;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;LPD&#26041;&#27861;&#23545;&#20110;&#21407;&#22987;&#20989;&#25968;&#30340;Lipschitz&#24120;&#25968;&#26377;&#19968;&#20010;&#27425;&#20248;&#30340;&#20381;&#36182;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#29305;&#24615;&#19982;LPD&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#21333;&#24490;&#29615;&#21152;&#36895;&#32447;&#24615;&#21270;&#21407;&#22987;-&#23545;&#20598;&#65288;ALPD&#65289;&#26041;&#27861;&#12290;ALPD&#26041;&#27861;&#22312;SPP&#20855;&#26377;&#21322;&#32447;&#24615;&#32806;&#21512;&#20989;&#25968;&#26102;&#23454;&#29616;&#20102;&#26368;&#20248;&#26799;&#24230;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#19968;&#33324;&#38750;&#32447;&#24615;&#32806;&#21512;&#20989;&#25968;&#30340;SPP&#30340;&#19981;&#31934;&#30830;ALPD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#25345;&#20102;&#21407;&#22987;&#37096;&#20998;&#30340;&#26368;&#20248;&#26799;&#24230;&#35780;&#20272;&#65292;&#24182;&#26174;&#33879;&#25913;&#21892;&#20102;&#32806;&#21512;&#39033;&#30340;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#29992;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate a primal-dual (PD) method for the saddle point problem (SPP) that uses a linear approximation of the primal function instead of the standard proximal step, resulting in a linearized PD (LPD) method. For convex-strongly concave SPP, we observe that the LPD method has a suboptimal dependence on the Lipschitz constant of the primal function. To fix this issue, we combine features of Accelerated Gradient Descent with the LPD method resulting in a single-loop Accelerated Linearized Primal-Dual (ALPD) method. ALPD method achieves the optimal gradient complexity when the SPP has a semi-linear coupling function. We also present an inexact ALPD method for SPPs with a general nonlinear coupling function that maintains the optimal gradient evaluations of the primal parts and significantly improves the gradient evaluations of the coupling term compared to the ALPD method. We verify our findings with numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#19977;&#31181;&#26032;&#39564;&#35777;&#22120;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#22312;100&#19975;&#20010;&#26816;&#26597;&#28857;&#30340;&#22823;&#25968;&#25454;&#38598;&#19978;&#19982;&#20854;&#20182;&#20116;&#31181;&#39564;&#35777;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25490;&#21517;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#20010;&#39564;&#35777;&#22120;&#20248;&#20110;&#29616;&#26377;&#30340;&#39564;&#35777;&#22120;&#65292;&#24182;&#19988;&#26368;&#20339;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#26041;&#27861;&#22240;&#20219;&#21153;&#31867;&#22411;&#32780;&#24322;&#12290;</title><link>http://arxiv.org/abs/2208.07360</link><description>&lt;p&gt;
&#19977;&#20010;&#26032;&#30340;&#39564;&#35777;&#22120;&#21450;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Three New Validators and a Large-Scale Benchmark Ranking for Unsupervised Domain Adaptation. (arXiv:2208.07360v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#19977;&#31181;&#26032;&#39564;&#35777;&#22120;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#22312;100&#19975;&#20010;&#26816;&#26597;&#28857;&#30340;&#22823;&#25968;&#25454;&#38598;&#19978;&#19982;&#20854;&#20182;&#20116;&#31181;&#39564;&#35777;&#22120;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#25490;&#21517;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#20010;&#39564;&#35777;&#22120;&#20248;&#20110;&#29616;&#26377;&#30340;&#39564;&#35777;&#22120;&#65292;&#24182;&#19988;&#26368;&#20339;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#26041;&#27861;&#22240;&#20219;&#21153;&#31867;&#22411;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#30340;&#25913;&#21464;&#21487;&#20197;&#23545;&#27169;&#22411;&#20934;&#30830;&#24230;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#36229;&#21442;&#25968;&#35843;&#25972;&#22312;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#36229;&#21442;&#25968;&#35843;&#25972;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36890;&#36807;&#20351;&#29992;&#8220;&#39564;&#35777;&#22120;&#8221;&#35780;&#20272;&#27169;&#22411;&#26816;&#26597;&#28857;&#12290;&#22312;&#26377;&#26631;&#31614;&#30340;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#39564;&#35777;&#22120;&#36890;&#36807;&#35745;&#31639;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#26469;&#35780;&#20272;&#26816;&#26597;&#28857;&#12290;&#30456;&#21453;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#39564;&#35777;&#38598;&#27809;&#26377;&#36825;&#26679;&#30340;&#26631;&#31614;&#12290;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#65292;&#22240;&#27492;&#26080;&#27861;&#35745;&#31639;&#20934;&#30830;&#24615;&#65292;&#22240;&#27492;&#39564;&#35777;&#22120;&#24517;&#39035;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#20160;&#20040;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#36825;&#20010;&#38382;&#39064;&#22312;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#39564;&#35777;&#22120;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#20116;&#20010;&#29616;&#26377;&#30340;&#39564;&#35777;&#22120;&#22312;&#21253;&#21547;100&#19975;&#20010;&#26816;&#26597;&#28857;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#21644;&#25490;&#21517;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20379;&#30340;&#20004;&#20010;&#39564;&#35777;&#22120;&#20248;&#20110;&#29616;&#26377;&#30340;&#39564;&#35777;&#22120;&#65292;&#24182;&#19988;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#26368;&#20339;&#26041;&#27861;&#22240;UDA&#20219;&#21153;&#31867;&#22411;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changes to hyperparameters can have a dramatic effect on model accuracy. Thus, the tuning of hyperparameters plays an important role in optimizing machine-learning models. An integral part of the hyperparameter-tuning process is the evaluation of model checkpoints, which is done through the use of "validators". In a supervised setting, these validators evaluate checkpoints by computing accuracy on a validation set that has labels. In contrast, in an unsupervised setting, the validation set has no such labels. Without any labels, it is impossible to compute accuracy, so validators must estimate accuracy instead. But what is the best approach to estimating accuracy? In this paper, we consider this question in the context of unsupervised domain adaptation (UDA). Specifically, we propose three new validators, and we compare and rank them against five other existing validators, on a large dataset of 1,000,000 checkpoints. Extensive experimental results show that two of our proposed validato
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;</title><link>http://arxiv.org/abs/2207.05631</link><description>&lt;p&gt;
DGPO: &#20351;&#29992;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#21457;&#29616;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37117;&#35797;&#22270;&#23547;&#25214;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#21333;&#20010;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20363;&#22914;&#65292;&#20351;&#26234;&#33021;&#20307;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#26356;&#21152;&#26377;&#36259;&#65292;&#25110;&#32773;&#25552;&#39640;&#31574;&#30053;&#23545;&#24847;&#22806;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#65288;DGPO&#65289;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#22810;&#31181;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#36890;&#36807;&#22312;&#21333;&#27425;&#36816;&#34892;&#20013;&#35757;&#32451;&#20849;&#20139;&#31574;&#30053;&#32593;&#32476;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#30446;&#26631;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#20132;&#26367;&#32422;&#26463;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#22806;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#25512;&#26029;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#36845;&#20195;&#26469;&#26368;&#22823;&#21270;&#24471;&#21040;&#30340;&#19979;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324; Atari &#28216;&#25103;&#21644; Mujoco &#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#31995;&#21015;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20219;&#21153;&#20559;&#32622;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#22810;&#20219;&#21153;&#65288;MTL&#65289;&#20195;&#29702;&#24615;&#33021;&#24046;&#24322;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#37325;&#25918;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65288;3RL&#65289;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#25968;&#25454;&#12289;&#35745;&#31639;&#25110;&#39640;&#32500;&#24230;&#21463;&#38480;&#35774;&#32622;&#19979;&#23588;&#20854;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2205.14495</link><description>&lt;p&gt;
&#26080;&#20219;&#21153;&#20559;&#32622;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65306;&#33719;&#21462;&#27934;&#35265;&#24182;&#20811;&#26381;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Task-Agnostic Continual Reinforcement Learning: Gaining Insights and Overcoming Challenges. (arXiv:2205.14495v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#20219;&#21153;&#20559;&#32622;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#22810;&#20219;&#21153;&#65288;MTL&#65289;&#20195;&#29702;&#24615;&#33021;&#24046;&#24322;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#37325;&#25918;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65288;3RL&#65289;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#25968;&#25454;&#12289;&#35745;&#31639;&#25110;&#39640;&#32500;&#24230;&#21463;&#38480;&#35774;&#32622;&#19979;&#23588;&#20854;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20351;&#27169;&#22411;&#21644;&#20195;&#29702;&#33021;&#22815;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#21516;&#26102;&#35299;&#20915;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#24433;&#21709;&#26080;&#20219;&#21153;&#20559;&#32622;CL&#21644;&#22810;&#20219;&#21153;&#65288;MTL&#65289;&#20195;&#29702;&#24615;&#33021;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20004;&#20010;&#20551;&#35774;&#65306;&#65288;1&#65289;&#26080;&#20219;&#21153;&#20559;&#32622;&#26041;&#27861;&#21487;&#33021;&#22312;&#25968;&#25454;&#12289;&#35745;&#31639;&#25110;&#39640;&#32500;&#24230;&#21463;&#38480;&#35774;&#32622;&#20013;&#25552;&#20379;&#20248;&#21183;&#65292;&#65288;2&#65289;&#26356;&#24555;&#30340;&#36866;&#24212;&#21487;&#33021;&#29305;&#21035;&#26377;&#30410;&#20110;&#36830;&#32493;&#23398;&#20064;&#35774;&#32622;&#65292;&#24110;&#21161;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#37325;&#25918;&#30340;&#24490;&#29615;&#24378;&#21270;&#23398;&#20064;&#65288;3RL&#65289;&#26041;&#27861;&#29992;&#20110;&#26080;&#20219;&#21153;&#20559;&#32622;CL&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20219;&#21153;&#21644;Meta-World&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;3RL&#65292;&#20854;&#20013;&#21253;&#25324;50&#20010;&#29420;&#29305;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;3RL&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#29978;&#33267;&#21487;&#20197;&#36229;&#36807;&#20854;&#22810;&#20219;&#21153;&#31561;&#20215;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) enables the development of models and agents that learn from a sequence of tasks while addressing the limitations of standard deep learning approaches, such as catastrophic forgetting. In this work, we investigate the factors that contribute to the performance differences between task-agnostic CL and multi-task (MTL) agents. We pose two hypotheses: (1) task-agnostic methods might provide advantages in settings with limited data, computation, or high dimensionality, and (2) faster adaptation may be particularly beneficial in continual learning settings, helping to mitigate the effects of catastrophic forgetting. To investigate these hypotheses, we introduce a replay-based recurrent reinforcement learning (3RL) methodology for task-agnostic CL agents. We assess 3RL on a synthetic task and the Meta-World benchmark, which includes 50 unique manipulation tasks. Our results demonstrate that 3RL outperforms baseline methods and can even surpass its multi-task equivalen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;Selectively Adaptive Lasso&#65288;SAL&#65289;&#65292;&#23427;&#22522;&#20110;HAL&#30340;&#29702;&#35770;&#26500;&#24314;&#65292;&#20445;&#30041;&#20102;&#26080;&#32500;&#24230;&#12289;&#38750;&#21442;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#21487;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#23558;&#35768;&#22810;&#22238;&#24402;&#31995;&#25968;&#33258;&#21160;&#35774;&#32622;&#20026;&#38646;&#12290;</title><link>http://arxiv.org/abs/2205.10697</link><description>&lt;p&gt;
Selectively Adaptive Lasso&#36873;&#36866;&#24212;Lasso
&lt;/p&gt;
&lt;p&gt;
The Selectively Adaptive Lasso. (arXiv:2205.10697v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#8212;&#8212;Selectively Adaptive Lasso&#65288;SAL&#65289;&#65292;&#23427;&#22522;&#20110;HAL&#30340;&#29702;&#35770;&#26500;&#24314;&#65292;&#20445;&#30041;&#20102;&#26080;&#32500;&#24230;&#12289;&#38750;&#21442;&#25968;&#25910;&#25947;&#36895;&#29575;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#20063;&#20855;&#26377;&#21487;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#31639;&#27861;&#23558;&#35768;&#22810;&#22238;&#24402;&#31995;&#25968;&#33258;&#21160;&#35774;&#32622;&#20026;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#26041;&#27861;&#33021;&#22815;&#36827;&#34892;&#26080;&#38656;&#36807;&#22810;&#30340;&#21442;&#25968;&#20551;&#35774;&#30340;&#20989;&#25968;&#20272;&#35745;&#12290;&#34429;&#28982;&#23427;&#20204;&#21487;&#20197;&#22312;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22823;&#22810;&#25968;&#32570;&#20047;&#31867;&#21322;&#21442;&#25968;&#26377;&#25928;&#20272;&#35745;&#65288;&#20363;&#22914;&#65292;TMLE&#65292;AIPW&#65289;&#25152;&#38656;&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#12290;&#39640;&#24230;&#33258;&#36866;&#24212;Lasso&#65288;HAL&#65289;&#26159;&#21807;&#19968;&#32463;&#35777;&#26126;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#24847;&#20041;&#19978;&#30340;&#22823;&#31867;&#20989;&#25968;&#30340;&#22238;&#24402;&#26041;&#27861;&#65292;&#19982;&#39044;&#27979;&#21464;&#37327;&#30340;&#32500;&#24230;&#26080;&#20851;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;HAL&#26080;&#27861;&#25193;&#23637;&#35745;&#31639;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;HAL&#29702;&#35770;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#36873;&#25321;&#33258;&#36866;&#24212;Lasso&#65288;SAL&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20445;&#30041;HAL&#30340;&#26080;&#32500;&#24230;&#12289;&#38750;&#21442;&#25968;&#25910;&#25947;&#29575;&#65292;&#20294;&#20063;&#33021;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20123;&#19982;&#23884;&#22871;Donsker&#31867;&#20013;&#30340;&#32463;&#39564;&#25439;&#22833;&#26368;&#23567;&#21270;&#26377;&#20851;&#30340;&#19968;&#33324;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#20855;&#26377;&#31616;&#21333;&#30340;&#20998;&#32452;&#35268;&#21017;&#65292;&#33258;&#21160;&#23558;&#35768;&#22810;&#22238;&#24402;&#31995;&#25968;&#35774;&#20026;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning regression methods allow estimation of functions without unrealistic parametric assumptions. Although they can perform exceptionally in prediction error, most lack theoretical convergence rates necessary for semi-parametric efficient estimation (e.g. TMLE, AIPW) of parameters like average treatment effects. The Highly Adaptive Lasso (HAL) is the only regression method proven to converge quickly enough for a meaningfully large class of functions, independent of the dimensionality of the predictors. Unfortunately, HAL is not computationally scalable. In this paper we build upon the theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new algorithm which retains HAL's dimension-free, nonparametric convergence rate but which also scales computationally to large high-dimensional datasets. To accomplish this, we prove some general theoretical results pertaining to empirical loss minimization in nested Donsker classes. Our resulting algorithm is a form of gradie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23436;&#25972;3D&#36229;&#33539;&#22260;&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#22836;&#37096;CT&#20013;&#20986;&#34880;&#20998;&#21106;&#20219;&#21153;&#30340;&#36828;&#36229;&#33539;&#22260;&#21644;&#25509;&#36817;&#36229;&#33539;&#22260;&#24773;&#20917;&#65292;&#22312;&#36229;&#33539;&#22260;&#30340;&#22788;&#29702;&#20013;&#33021;&#22815;&#40065;&#26834;&#22320;&#35782;&#21035;&#24182;&#25490;&#38500;&#36229;&#33539;&#22260;&#24773;&#20917;&#65292;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36229;&#33539;&#22260;&#24615;&#33021;&#65292;&#33021;&#22815;&#22312;&#36229;&#33539;&#22260;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#38752;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.10650</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#20020;&#24202;&#23433;&#20840;&#20998;&#21106;&#30340;&#36229;&#33539;&#22260;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transformer-based out-of-distribution detection for clinically safe segmentation. (arXiv:2205.10650v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23436;&#25972;3D&#36229;&#33539;&#22260;&#26816;&#27979;&#27169;&#22411;&#65292;&#38024;&#23545;&#22836;&#37096;CT&#20013;&#20986;&#34880;&#20998;&#21106;&#20219;&#21153;&#30340;&#36828;&#36229;&#33539;&#22260;&#21644;&#25509;&#36817;&#36229;&#33539;&#22260;&#24773;&#20917;&#65292;&#22312;&#36229;&#33539;&#22260;&#30340;&#22788;&#29702;&#20013;&#33021;&#22815;&#40065;&#26834;&#22320;&#35782;&#21035;&#24182;&#25490;&#38500;&#36229;&#33539;&#22260;&#24773;&#20917;&#65292;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#36229;&#33539;&#22260;&#24615;&#33021;&#65292;&#33021;&#22815;&#22312;&#36229;&#33539;&#22260;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#38752;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35774;&#32622;&#20013;&#65292;&#23545;&#37096;&#32626;&#30340;&#22270;&#20687;&#22788;&#29702;&#31995;&#32479;&#20855;&#22791;&#40065;&#26834;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#29305;&#21035;&#26159;&#19981;&#33021;&#20570;&#20986;&#33258;&#20449;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26368;&#27969;&#34892;&#30340;&#23433;&#20840;&#22788;&#29702;&#26041;&#27861;&#26159;&#35757;&#32451;&#33021;&#22815;&#25552;&#20379;&#20854;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#30340;&#32593;&#32476;&#65292;&#20294;&#36825;&#20123;&#32593;&#32476;&#24448;&#24448;&#22312;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#33539;&#22260;&#20043;&#22806;&#30340;&#36755;&#20837;&#24773;&#20917;&#19979;&#22833;&#36133;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#34987;&#25552;&#35758;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65307;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26174;&#24335;&#22320;&#37327;&#21270;&#25968;&#25454;&#26679;&#26412;&#30340;&#21487;&#33021;&#24615;&#65292;&#22312;&#25191;&#34892;&#36827;&#19968;&#27493;&#22788;&#29702;&#20043;&#21069;&#36807;&#28388;&#25481;&#20219;&#20309;&#36229;&#33539;&#22260;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22270;&#20687;&#20998;&#21106;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#65292;&#38024;&#23545;&#22836;&#37096;CT&#20013;&#20986;&#34880;&#20998;&#21106;&#20219;&#21153;&#30340;&#36828;&#36229;&#33539;&#22260;( far-OOD )&#21644;&#25509;&#36817;&#36229;&#33539;&#22260;( near-OOD )&#24773;&#20917;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#22312;&#36229;&#33539;&#22260;&#22788;&#29702;&#26102;&#37117;&#20250;&#25552;&#20379;&#33258;&#20449;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#22240;&#27492;&#19981;&#36866;&#21512;&#29992;&#20110;&#23433;&#20840;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#23436;&#25972;3D&#36229;&#33539;&#22260;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#40065;&#26834;&#22320;&#35782;&#21035;&#24182;&#25490;&#38500;&#36229;&#33539;&#22260;&#24773;&#20917;&#12290;&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36229;&#33539;&#22260;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#36229;&#33539;&#22260;&#24773;&#20917;&#19979;&#25552;&#20379;&#21487;&#38752;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a clinical setting it is essential that deployed image processing systems are robust to the full range of inputs they might encounter and, in particular, do not make confidently wrong predictions. The most popular approach to safe processing is to train networks that can provide a measure of their uncertainty, but these tend to fail for inputs that are far outside the training data distribution. Recently, generative modelling approaches have been proposed as an alternative; these can quantify the likelihood of a data sample explicitly, filtering out any out-of-distribution (OOD) samples before further processing is performed. In this work, we focus on image segmentation and evaluate several approaches to network uncertainty in the far-OOD and near-OOD cases for the task of segmenting haemorrhages in head CTs. We find all of these approaches are unsuitable for safe segmentation as they provide confidently wrong predictions when operating OOD. We propose performing full 3D OOD detecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Bamboo&#37197;&#32622;&#31574;&#30053;&#65292;&#22522;&#20110;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.10505</link><description>&lt;p&gt;
Transformer&#37197;&#32622;&#19982;&#35757;&#32451;&#30446;&#26631;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Transformer Configuration and Training Objective. (arXiv:2205.10505v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Bamboo&#37197;&#32622;&#31574;&#30053;&#65292;&#22522;&#20110;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#19978;&#37117;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#22312;&#35768;&#22810;&#27169;&#22411;&#35757;&#32451;&#24773;&#20917;&#19979;&#65292;&#36890;&#24120;&#37319;&#29992;&#20256;&#32479;&#30340;&#37197;&#32622;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#20256;&#32479;&#37197;&#32622;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25552;&#20986;&#20102;Bamboo&#30340;&#37197;&#32622;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#26356;&#28145;&#26356;&#31364;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;Masked&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#65292;&#24182;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;gLaSDI&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#19982;&#23616;&#37096;DI&#27169;&#22411;&#30340;&#32467;&#21512;&#21644;&#33258;&#36866;&#24212;&#36138;&#24515;&#37319;&#26679;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#38477;&#38454;&#24314;&#27169;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#39640;&#25928;&#12289;&#40065;&#26834;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2204.12005</link><description>&lt;p&gt;
gLaSDI: &#21442;&#25968;&#21270;&#29289;&#29702;&#30693;&#35782;&#25351;&#23548;&#30340;&#36138;&#24515;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
gLaSDI: Parametric Physics-informed Greedy Latent Space Dynamics Identification. (arXiv:2204.12005v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;gLaSDI&#65292;&#36890;&#36807;&#33258;&#32534;&#30721;&#22120;&#19982;&#23616;&#37096;DI&#27169;&#22411;&#30340;&#32467;&#21512;&#21644;&#33258;&#36866;&#24212;&#36138;&#24515;&#37319;&#26679;&#31639;&#27861;&#26469;&#23454;&#29616;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#25454;&#39537;&#21160;&#38477;&#38454;&#24314;&#27169;&#65292;&#20855;&#26377;&#31934;&#30830;&#12289;&#39640;&#25928;&#12289;&#40065;&#26834;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#33258;&#36866;&#24212;&#30340;&#29289;&#29702;&#30693;&#35782;&#25351;&#23548;&#30340;&#36138;&#24515;&#28508;&#31354;&#38388;&#21160;&#21147;&#23398;&#35782;&#21035;&#26041;&#27861;&#65288;gLaSDI&#65289;&#65292;&#29992;&#20110;&#39640;&#32500;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#31934;&#30830;&#12289;&#39640;&#25928;&#21644;&#40065;&#26834;&#30340;&#25968;&#25454;&#39537;&#21160;&#38477;&#38454;&#24314;&#27169;&#12290;&#22312;&#25552;&#20986;&#30340;gLaSDI&#26694;&#26550;&#20013;&#65292;&#33258;&#32534;&#30721;&#22120;&#21457;&#29616;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#38750;&#32447;&#24615;&#28508;&#22312;&#34920;&#31034;&#65292;&#32780;&#21160;&#21147;&#23398;&#35782;&#21035;&#65288;DI&#65289;&#27169;&#22411;&#25429;&#33719;&#23616;&#37096;&#28508;&#31354;&#38388;&#21160;&#24577;&#12290;&#37319;&#29992;&#20132;&#20114;&#24335;&#35757;&#32451;&#31639;&#27861;&#23545;&#33258;&#32534;&#30721;&#22120;&#21644;&#23616;&#37096;DI&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#31616;&#21333;&#30340;&#28508;&#31354;&#38388;&#21160;&#24577;&#24182;&#25552;&#39640;&#25968;&#25454;&#39537;&#21160;&#38477;&#38454;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#21644;&#21152;&#36895;&#21442;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#20197;&#24471;&#21040;&#26368;&#20339;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36138;&#24515;&#37319;&#26679;&#31639;&#27861;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#25351;&#23548;&#30340;&#27531;&#24046;&#35823;&#24046;&#25351;&#26631;&#21644;&#38543;&#26426;&#23376;&#38598;&#35780;&#20272;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25628;&#32034;&#26368;&#20248;&#35757;&#32451;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#21033;&#29992;&#26412;&#22320;&#28508;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
A parametric adaptive physics-informed greedy Latent Space Dynamics Identification (gLaSDI) method is proposed for accurate, efficient, and robust data-driven reduced-order modeling of high-dimensional nonlinear dynamical systems. In the proposed gLaSDI framework, an autoencoder discovers intrinsic nonlinear latent representations of high-dimensional data, while dynamics identification (DI) models capture local latent-space dynamics. An interactive training algorithm is adopted for the autoencoder and local DI models, which enables identification of simple latent-space dynamics and enhances accuracy and efficiency of data-driven reduced-order modeling. To maximize and accelerate the exploration of the parameter space for the optimal model performance, an adaptive greedy sampling algorithm integrated with a physics-informed residual-based error indicator and random-subset evaluation is introduced to search for the optimal training samples on the fly. Further, to exploit local latent-spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#30165;&#36857;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#21046;&#36896;&#30165;&#36857;&#30340;&#26041;&#24335;&#36827;&#34892;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#26356;&#21152;&#21487;&#37096;&#32626;&#65292;&#24182;&#22312;&#20844;&#24320;API&#21644;&#20132;&#36890;&#26631;&#24535;&#30340;&#22270;&#20687;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#30772;&#35299;&#29575;&#65292;&#25915;&#20987;&#27425;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2204.09397</link><description>&lt;p&gt;
&#23545;CNN&#20998;&#31867;&#22120;&#30340;&#23545;&#25239;&#24615;&#30165;&#36857;&#25915;&#20987;&#65306;&#21487;&#37096;&#32626;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Adversarial Scratches: Deployable Attacks to CNN Classifiers. (arXiv:2204.09397v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#30165;&#36857;&#25915;&#20987;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#21046;&#36896;&#30165;&#36857;&#30340;&#26041;&#24335;&#36827;&#34892;&#25915;&#20987;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#26356;&#21152;&#21487;&#37096;&#32626;&#65292;&#24182;&#22312;&#20844;&#24320;API&#21644;&#20132;&#36890;&#26631;&#24535;&#30340;&#22270;&#20687;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#30772;&#35299;&#29575;&#65292;&#25915;&#20987;&#27425;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#31034;&#20363;&#26159;&#24212;&#29992;&#20110;&#27169;&#22411;&#36755;&#20837;&#30340;&#23567;&#25200;&#21160;&#65292;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;L0&#40657;&#30418;&#25915;&#20987;&#8212;&#8212;&#23545;&#25239;&#24615;&#30165;&#36857;&#25915;&#20987;&#65306;&#36890;&#36807;&#22312;&#22270;&#20687;&#20013;&#21046;&#36896;&#30165;&#36857;&#30340;&#26041;&#24335;&#36827;&#34892;&#25915;&#20987;&#65292;&#23427;&#27604;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#26356;&#20855;&#26377;&#21487;&#37096;&#32626;&#24615;&#12290;&#23545;&#25239;&#24615;&#30165;&#36857;&#21033;&#29992;Bezier&#26354;&#32447;&#26469;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#24182;&#21487;&#33021;&#23558;&#25915;&#20987;&#32422;&#26463;&#21040;&#29305;&#23450;&#20301;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#27979;&#35797;&#20102;&#23545;&#25239;&#24615;&#30165;&#36857;&#25915;&#20987;&#65292;&#21253;&#25324;&#20844;&#24320;API&#21644;&#20132;&#36890;&#26631;&#24535;&#30340;&#22270;&#20687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#36890;&#24120;&#27604;&#20854;&#20182;&#37096;&#32626;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23454;&#29616;&#26356;&#39640;&#30340;&#30772;&#35299;&#29575;&#65292;&#21516;&#26102;&#38656;&#35201;&#26126;&#26174;&#26356;&#23569;&#30340;&#25915;&#20987;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of work has shown that deep neural networks are susceptible to adversarial examples. These take the form of small perturbations applied to the model's input which lead to incorrect predictions. Unfortunately, most literature focuses on visually imperceivable perturbations to be applied to digital images that often are, by design, impossible to be deployed to physical targets. We present Adversarial Scratches: a novel L0 black-box attack, which takes the form of scratches in images, and which possesses much greater deployability than other state-of-the-art attacks. Adversarial Scratches leverage B\'ezier Curves to reduce the dimension of the search space and possibly constrain the attack to a specific location. We test Adversarial Scratches in several scenarios, including a publicly available API and images of traffic signs. Results show that, often, our attack achieves higher fooling rate than other deployable state-of-the-art methods, while requiring significantly fewer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25512;&#33616;&#65292;&#20998;&#26512;&#20102;&#20854;&#23545;&#20154;&#24037;&#38134;&#34892;&#38388;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#20010;&#20307;&#20449;&#24687;&#21644;&#20844;&#20849;&#25512;&#33616;&#65292;&#37329;&#34701;&#26426;&#26500;&#21046;&#23450;&#20511;&#36151;&#21327;&#35758;&#12290;&#31574;&#30053;&#25512;&#33616;&#33021;&#22815;&#21512;&#29702;&#24341;&#23548;&#37329;&#34701;&#34892;&#20026;&#32773;&#30340;&#20449;&#36151;&#20851;&#31995;&#24418;&#25104;&#65292;&#26680;&#24515;-&#21608;&#36793;&#32593;&#32476;&#32467;&#26500;&#36805;&#36895;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2204.07134</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25512;&#33616;&#23545;&#38134;&#34892;&#38388;&#32593;&#32476;&#31283;&#23450;&#24615;&#30340;&#24433;&#21709;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Policy Recommendation for Interbank Network Stability. (arXiv:2204.07134v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25512;&#33616;&#65292;&#20998;&#26512;&#20102;&#20854;&#23545;&#20154;&#24037;&#38134;&#34892;&#38388;&#24066;&#22330;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#20010;&#20307;&#20449;&#24687;&#21644;&#20844;&#20849;&#25512;&#33616;&#65292;&#37329;&#34701;&#26426;&#26500;&#21046;&#23450;&#20511;&#36151;&#21327;&#35758;&#12290;&#31574;&#30053;&#25512;&#33616;&#33021;&#22815;&#21512;&#29702;&#24341;&#23548;&#37329;&#34701;&#34892;&#20026;&#32773;&#30340;&#20449;&#36151;&#20851;&#31995;&#24418;&#25104;&#65292;&#26680;&#24515;-&#21608;&#36793;&#32593;&#32476;&#32467;&#26500;&#36805;&#36895;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31574;&#30053;&#25512;&#33616;&#23545;&#20154;&#24037;&#38134;&#34892;&#38388;&#24066;&#22330;&#34920;&#29616;&#30340;&#24433;&#21709;&#12290;&#37329;&#34701;&#26426;&#26500;&#26681;&#25454;&#20844;&#20849;&#25512;&#33616;&#21644;&#20010;&#20307;&#20449;&#24687;&#21046;&#23450;&#20511;&#36151;&#21327;&#35758;&#12290;&#21069;&#32773;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26368;&#20248;&#31574;&#30053;&#26469;&#27169;&#25311;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#36866;&#24212;&#24615;&#24182;&#25910;&#38598;&#26377;&#20851;&#32463;&#27982;&#29615;&#22659;&#30340;&#20449;&#24687;&#12290;&#31574;&#30053;&#25512;&#33616;&#30452;&#25509;&#25110;&#38388;&#25509;&#22320;&#24433;&#21709;&#37329;&#34701;&#34892;&#20026;&#32773;&#30340;&#20449;&#36151;&#20851;&#31995;&#24418;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#20302;&#21033;&#29575;&#25110;&#39640;&#27969;&#21160;&#24615;&#20379;&#32473;&#30340;&#26368;&#20248;&#36873;&#25321;&#26469;&#21512;&#29702;&#24341;&#23548;&#65292;&#20877;&#26681;&#25454;&#38134;&#34892;&#20195;&#29702;&#30340;&#36164;&#20135;&#36127;&#20538;&#34920;&#20449;&#24687;&#26469;&#30830;&#35748;&#20182;&#20204;&#22312;&#24066;&#22330;&#20869;&#26368;&#36866;&#23452;&#21521;&#23458;&#25143;&#25552;&#20379;&#30340;&#27969;&#21160;&#24615;&#21644;&#21033;&#29575;&#32452;&#21512;&#12290;&#36890;&#36807;&#23558;&#20844;&#20849;&#21644;&#31169;&#26377;&#20449;&#21495;&#30456;&#32467;&#21512;&#65292;&#37329;&#34701;&#26426;&#26500;&#33021;&#22815;&#36890;&#36807;&#36880;&#28176;&#24314;&#31435;&#26377;&#36873;&#25321;&#24615;&#38468;&#21152;&#30340;&#20449;&#36151;&#36830;&#25509;&#32780;&#21019;&#24314;&#25110;&#20462;&#24314;&#20182;&#20204;&#30340;&#20449;&#36151;&#32593;&#32476;&#65292;&#20174;&#32780;&#29983;&#25104;&#21160;&#24577;&#32593;&#32476;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26680;&#24515;-&#21608;&#36793;&#32593;&#32476;&#32467;&#26500;&#20855;&#26377;&#36805;&#36895;&#31283;&#23450;&#30340;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the effect of a policy recommendation on the performance of an artificial interbank market. Financial institutions stipulate lending agreements following a public recommendation and their individual information. The former is modeled by a reinforcement learning optimal policy that maximizes the system's fitness and gathers information on the economic environment. The policy recommendation directs economic actors to create credit relationships through the optimal choice between a low interest rate or a high liquidity supply. The latter, based on the agents' balance sheet, allows determining the liquidity supply and interest rate that the banks optimally offer their clients within the market. Thanks to the combination between the public and the private signal, financial institutions create or cut their credit connections over time via a preferential attachment evolving procedure able to generate a dynamic network. Our results show that the emergence of a core-pe
&lt;/p&gt;</description></item><item><title>PyDTS&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#29983;&#23384;&#25968;&#25454;&#21322;&#21442;&#25968;&#31454;&#20105;&#39118;&#38505;&#27169;&#22411;&#30340;Python&#21253;&#65292;&#25903;&#25345;&#21253;&#25324;LASSO&#21644;&#24377;&#24615;&#32593;&#31561;&#27491;&#21017;&#21270;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.05731</link><description>&lt;p&gt;
PyDTS&#65306;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#31454;&#20105;&#39118;&#38505;&#65288;&#27491;&#21017;&#21270;&#65289;&#22238;&#24402;&#30340; Python &#21253;
&lt;/p&gt;
&lt;p&gt;
PyDTS: A Python Package for Discrete-Time Survival (Regularized) Regression with Competing Risks. (arXiv:2204.05731v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05731
&lt;/p&gt;
&lt;p&gt;
PyDTS&#26159;&#19968;&#20010;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#29983;&#23384;&#25968;&#25454;&#21322;&#21442;&#25968;&#31454;&#20105;&#39118;&#38505;&#27169;&#22411;&#30340;Python&#21253;&#65292;&#25903;&#25345;&#21253;&#25324;LASSO&#21644;&#24377;&#24615;&#32593;&#31561;&#27491;&#21017;&#21270;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#33267;&#20107;&#20214;&#20998;&#26512;&#65288;&#29983;&#23384;&#20998;&#26512;&#65289;&#29992;&#20110;&#21709;&#24212;&#26102;&#38388;&#26159;&#25351;&#39044;&#23450;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#12290;&#30001;&#20110;&#26102;&#38388;&#26412;&#36523;&#26159;&#31163;&#25955;&#30340;&#25110;&#30001;&#20110;&#23558;&#22833;&#36133;&#26102;&#38388;&#20998;&#32452;&#20026;&#38388;&#38548;&#25110;&#33293;&#20837;&#27979;&#37327;&#65292;&#22240;&#27492;&#26102;&#38388;&#33267;&#20107;&#20214;&#25968;&#25454;&#26377;&#26102;&#26159;&#31163;&#25955;&#30340;&#12290;&#27492;&#22806;&#65292;&#20010;&#20307;&#30340;&#22833;&#36133;&#21487;&#33021;&#26159;&#20960;&#31181;&#19981;&#21516;&#30340;&#22833;&#36133;&#31867;&#22411;&#20043;&#19968;&#65292;&#31216;&#20026;&#31454;&#20105;&#39118;&#38505;&#65288;&#20107;&#20214;&#65289;&#12290;&#22823;&#22810;&#25968;&#29983;&#23384;&#22238;&#24402;&#20998;&#26512;&#30340;&#26041;&#27861;&#21644;&#36719;&#20214;&#21253;&#20551;&#23450;&#26102;&#38388;&#26159;&#22312;&#36830;&#32493;&#23610;&#24230;&#19978;&#27979;&#37327;&#30340;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#23558;&#26631;&#20934;&#30340;&#36830;&#32493;&#26102;&#38388;&#27169;&#22411;&#24212;&#29992;&#20110;&#31163;&#25955;&#26102;&#38388;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#30340;&#20272;&#35745;&#22120;&#23384;&#22312;&#20559;&#24046;&#12290;&#20171;&#32461;&#20102; Python &#21253; PyDTS&#65292;&#29992;&#20110;&#27169;&#25311;&#65292;&#20272;&#35745;&#21644;&#35780;&#20272;&#31163;&#25955;&#26102;&#38388;&#29983;&#23384;&#25968;&#25454;&#30340;&#21322;&#21442;&#25968;&#31454;&#20105;&#39118;&#38505;&#27169;&#22411;&#12290;&#35813;&#21253;&#23454;&#29616;&#20102;&#24555;&#36895;&#36807;&#31243;&#65292;&#20351;&#26377;&#25928;&#22320;&#21253;&#25324;&#27491;&#21017;&#21270;&#22238;&#24402;&#26041;&#27861;&#65292;&#22914; LASSO &#21644;&#24377;&#24615;&#32593;&#32476;&#31561;&#12290;&#19968;&#20010;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Time-to-event analysis (survival analysis) is used when the response of interest is the time until a pre-specified event occurs. Time-to-event data are sometimes discrete either because time itself is discrete or due to grouping of failure times into intervals or rounding off measurements. In addition, the failure of an individual could be one of several distinct failure types, known as competing risks (events). Most methods and software packages for survival regression analysis assume that time is measured on a continuous scale. It is well-known that naively applying standard continuous-time models with discrete-time data may result in biased estimators of the discrete-time models. The Python package PyDTS, for simulating, estimating and evaluating semi-parametric competing-risks models for discrete-time survival data, is introduced. The package implements a fast procedure that enables including regularized regression methods, such as LASSO and elastic net, among others. A simulation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#36817;&#20284;&#20110;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#21644;&#24067;&#23572;&#20989;&#25968;&#65292;&#19988;&#22312;&#32473;&#23450;&#31867;&#26465;&#20214;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#35823;&#24046;&#19982;&#26368;&#20248;&#35823;&#24046;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2108.06339</link><description>&lt;p&gt;
&#38543;&#26426;&#25237;&#24433;&#20998;&#31867;&#30340;&#26368;&#20248;&#24615;&#21644;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimality and complexity of classification by random projection. (arXiv:2108.06339v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#32452;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#21487;&#20197;&#36817;&#20284;&#20110;&#20219;&#24847;&#36830;&#32493;&#20989;&#25968;&#21644;&#24067;&#23572;&#20989;&#25968;&#65292;&#19988;&#22312;&#32473;&#23450;&#31867;&#26465;&#20214;&#23494;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#20854;&#35823;&#24046;&#19982;&#26368;&#20248;&#35823;&#24046;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#19982;&#36873;&#25321;&#20998;&#31867;&#22120;&#30340;&#20989;&#25968;&#38598;&#30340;&#22797;&#26434;&#24230;&#26377;&#20851;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#32452;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;&#36890;&#36807;&#38543;&#26426;&#19968;&#32500;&#29305;&#24449;&#20570;&#38408;&#20540;&#22788;&#29702;&#12290;&#35813;&#29305;&#24449;&#36890;&#36807;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#30001;&#39640;&#27425;&#21333;&#39033;&#24335;&#21442;&#25968;&#21270;&#30340;&#26356;&#39640;&#32500;&#31354;&#38388;&#20013;&#21518;&#22312;&#38543;&#26426;&#30452;&#32447;&#19978;&#36827;&#34892;&#25237;&#24433;&#32780;&#24471;&#21040;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25193;&#23637;&#30340;&#25968;&#25454;&#34987;&#25237;&#24433;n&#27425;&#65292;&#24182;&#20174;&#36825;n&#20010;&#20013;&#36873;&#20986;&#34920;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#31867;&#22411;&#30340;&#20998;&#31867;&#22120;&#26159;&#26497;&#20854;&#28789;&#27963;&#30340;&#65292;&#22240;&#20026;&#23427;&#26377;&#21487;&#33021;&#36817;&#20284;&#20110;&#20219;&#20309;&#22312;&#32039;&#33268;&#38598;&#19978;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#20197;&#21450;&#23558;&#25903;&#25745;&#38598;&#25286;&#20998;&#20026;&#21487;&#27979;&#23376;&#38598;&#30340;&#20219;&#20309;&#24067;&#23572;&#20989;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#22914;&#26524;&#32473;&#23450;&#31867;&#26465;&#20214;&#23494;&#24230;&#30340;&#23436;&#20840;&#30693;&#35782;&#65292;&#21017;&#36825;&#20123;&#20302;&#22797;&#26434;&#24230;&#20998;&#31867;&#22120;&#30340;&#35823;&#24046;&#23558;&#22312;k&#21644;n&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#25910;&#25947;&#21040;&#26368;&#20248;&#65288;&#36125;&#21494;&#26031;&#65289;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization error of a classifier is related to the complexity of the set of functions among which the classifier is chosen. We study a family of low-complexity classifiers consisting of thresholding a random one-dimensional feature. The feature is obtained by projecting the data on a random line after embedding it into a higher-dimensional space parametrized by monomials of order up to k. More specifically, the extended data is projected n-times and the best classifier among those n, based on its performance on training data, is chosen. We show that this type of classifier is extremely flexible, as it is likely to approximate, to an arbitrary precision, any continuous function on a compact set as well as any boolean function on a compact set that splits the support into measurable subsets. In particular, given full knowledge of the class conditional densities, the error of these low-complexity classifiers would converge to the optimal (Bayes) error as k and n go to infinity. On
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36866;&#37327;&#32423;&#21035;&#30340;&#36882;&#22686;&#35745;&#31639;&#26469;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;Epistemic&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20351;&#24471;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#22823;&#24133;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#22823;&#22411;&#38598;&#25104;&#27169;&#22411;&#65292;&#20026;&#27169;&#22411;&#32852;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25509;&#21475;&#12290;</title><link>http://arxiv.org/abs/2107.08924</link><description>&lt;p&gt;
&#35748;&#30693;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Epistemic Neural Networks. (arXiv:2107.08924v8 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36866;&#37327;&#32423;&#21035;&#30340;&#36882;&#22686;&#35745;&#31639;&#26469;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#19981;&#30830;&#23450;&#24615;&#30340;Epistemic&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20351;&#24471;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#35745;&#31639;&#25104;&#26412;&#22823;&#24133;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#22823;&#22411;&#38598;&#25104;&#27169;&#22411;&#65292;&#20026;&#27169;&#22411;&#32852;&#21512;&#39044;&#27979;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25509;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20381;&#36182;&#20110;&#26234;&#33021;&#20307;&#23545;&#20854;&#19981;&#30693;&#36947;&#30340;&#20107;&#29289;&#30340;&#20102;&#35299;&#12290;&#26234;&#33021;&#20307;&#39044;&#27979;&#22810;&#20010;&#36755;&#20837;&#26631;&#31614;&#30340;&#36136;&#37327;&#21487;&#20197;&#35780;&#20272;&#20854;&#23545;&#36825;&#31181;&#33021;&#21147;&#30340;&#25484;&#25569;&#31243;&#24230;&#12290;&#38598;&#25104;&#24335;&#26041;&#27861;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#26377;&#25928;&#30340;&#39044;&#27979;&#65292;&#20294;&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#38598;&#25104;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#20174;&#32780;&#21487;&#33021;&#20250;&#21464;&#24471;&#31105;&#27490;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Epinet&#65306;&#19968;&#31181;&#21487;&#20197;&#21152;&#24378;&#20219;&#20309;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#65288;&#21253;&#25324;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#36866;&#37327;&#32423;&#21035;&#30340;&#36882;&#22686;&#35745;&#31639;&#35757;&#32451;&#26469;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#29992;Epinet&#65292;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#35745;&#31639;&#25104;&#26412;&#22823;&#24133;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#32988;&#36807;&#30001;&#25968;&#30334;&#20010;&#25110;&#26356;&#22810;&#31890;&#23376;&#32452;&#25104;&#30340;&#22823;&#22411;&#38598;&#25104;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#31526;&#21512;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#32479;&#26694;&#26550;&#12290;&#20026;&#20102;&#36866;&#24212;&#36229;&#36234;BNN&#30340;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20363;&#22914;Epinet&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20316;&#20026;&#20135;&#29983;&#32852;&#21512;&#39044;&#27979;&#27169;&#22411;&#30340;&#25509;&#21475;&#30340;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. In principle, ensemble-based approaches produce effective joint predictions, but the computational costs of training large ensembles can become prohibitive. We introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks. To accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as an interface for models that produce joint predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#37322;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#21487;&#35757;&#32451;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; - &#21464;&#24322;&#24615;&#65292;&#19982; MLP &#30340;&#28608;&#27963;&#25968;&#37327;&#21576;&#27491;&#30456;&#20851;&#65292;&#19982;"&#22349;&#22604;&#21040;&#24120;&#25968;"&#29616;&#35937;&#21576;&#36127;&#30456;&#20851;&#65292;&#26159; MLP &#21487;&#35757;&#32451;&#24615;&#30340;&#19968;&#20010;&#20934;&#30830;&#39044;&#27979;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2105.08911</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#24322;&#24615;&#35299;&#37322;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#21487;&#35757;&#32451;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-layer Perceptron Trainability Explained via Variability. (arXiv:2105.08911v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#37322;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#21487;&#35757;&#32451;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; - &#21464;&#24322;&#24615;&#65292;&#19982; MLP &#30340;&#28608;&#27963;&#25968;&#37327;&#21576;&#27491;&#30456;&#20851;&#65292;&#19982;"&#22349;&#22604;&#21040;&#24120;&#25968;"&#29616;&#35937;&#21576;&#36127;&#30456;&#20851;&#65292;&#26159; MLP &#21487;&#35757;&#32451;&#24615;&#30340;&#19968;&#20010;&#20934;&#30830;&#39044;&#27979;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#30340;&#35768;&#22810;&#22522;&#26412;&#26041;&#38754;&#20173;&#26410;&#23436;&#20840;&#29702;&#35299;&#65292;&#21253;&#25324;DNN&#30340;&#21487;&#35757;&#32451;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36776;&#21035;&#22312;&#30456;&#20284;&#26465;&#20214;&#19979;&#65292;&#20160;&#20040;&#20351;&#19968;&#20010;DNN&#27169;&#22411;&#27604;&#21478;&#19968;&#20010;&#26356;&#23481;&#26131;&#35757;&#32451;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24102;&#26377;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120; (MLP) &#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565; - &#21464;&#24322;&#24615;&#65292;&#20197;&#24110;&#21161;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#22909;&#22788;&#20197;&#21450;&#22312;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340; MLP &#26102;&#25152;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#21464;&#24322;&#24615;&#20195;&#34920;&#20102;&#25968;&#25454;&#31354;&#38388;&#20013;&#19982;&#33391;&#22909;&#32553;&#25918;&#30340;&#38543;&#26426;&#26435;&#37325;&#30456;&#20851;&#30340;&#22320;&#24418;&#27169;&#24335;&#30340;&#20016;&#23500;&#24615;&#12290;&#25105;&#20204;&#20174;&#23454;&#35777;&#19978;&#35777;&#26126;&#65292;&#21464;&#24322;&#24615;&#19982;&#28608;&#27963;&#25968;&#37327;&#21576;&#27491;&#30456;&#20851;&#65292;&#19982;"&#22349;&#22604;&#21040;&#24120;&#25968;"&#29616;&#35937;&#21576;&#36127;&#30456;&#20851;&#65292;&#21518;&#32773;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#26799;&#24230;&#28040;&#22833;&#29616;&#35937;&#30456;&#20851;&#20294;&#24182;&#19981;&#23436;&#20840;&#30456;&#21516;&#12290;&#22312;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21464;&#24322;&#24615;&#26159; MLP &#21487;&#35757;&#32451;&#24615;&#30340;&#19968;&#20010;&#20934;&#30830;&#39044;&#27979;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous successes of deep neural networks (DNNs) in various applications, many fundamental aspects of deep learning remain incompletely understood, including DNN trainability. In a trainability study, one aims to discern what makes one DNN model easier to train than another under comparable conditions. In particular, our study focuses on multi-layer perceptron (MLP) models equipped with the same number of parameters. We introduce a new notion called variability to help explain the benefits of deep learning and the difficulties in training very deep MLPs. Simply put, variability of a neural network represents the richness of landscape patterns in the data space with respect to well-scaled random weights. We empirically show that variability is positively correlated to the number of activations and negatively correlated to a phenomenon called "Collapse to Constant", which is related but not identical to the well-known vanishing gradient phenomenon. Experiments on a small s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;&#37096;&#20998;&#22810;&#35270;&#35282;&#23398;&#20064;&#65292;&#26088;&#22312;&#20811;&#26381;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#35270;&#35282;&#32570;&#22833;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2105.02046</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#37096;&#20998;&#22810;&#35270;&#35282;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Partial Multi-view Learning. (arXiv:2105.02046v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.02046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;&#37096;&#20998;&#22810;&#35270;&#35282;&#23398;&#20064;&#65292;&#26088;&#22312;&#20811;&#26381;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#35270;&#35282;&#32570;&#22833;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25968;&#25454;&#24448;&#24448;&#20855;&#26377;&#22810;&#20010;&#35270;&#35282;&#65292;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#35270;&#35282;&#30340;&#20449;&#24687;&#23545;&#20110;&#20351;&#25968;&#25454;&#26356;&#20855;&#20195;&#34920;&#24615;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20013;&#30340;&#21508;&#31181;&#38480;&#21046;&#21644;&#22833;&#35823;&#65292;&#30495;&#23454;&#25968;&#25454;&#36973;&#21463;&#35270;&#35282;&#32570;&#22833;&#21644;&#25968;&#25454;&#31232;&#32570;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#20849;&#23384;&#20351;&#24471;&#23454;&#29616;&#27169;&#24335;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#24456;&#23569;&#26377;&#36866;&#24403;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#26356;&#22810;&#20851;&#27880;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#23569;&#26679;&#26412;&#37096;&#20998;&#22810;&#35270;&#35282;&#23398;&#20064;&#65292;&#19987;&#27880;&#20110;&#20811;&#26381;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#35270;&#35282;&#32570;&#22833;&#38382;&#39064;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#27492;&#20219;&#21153;&#30340;&#25361;&#25112;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;i&#65289;&#38590;&#20197;&#22312;&#32570;&#22833;&#35270;&#35282;&#30340;&#24178;&#25200;&#19979;&#20811;&#26381;&#25968;&#25454;&#31232;&#32570;&#30340;&#24433;&#21709;&#65307;&#65288;ii&#65289;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#21152;&#21095;&#20102;&#20449;&#24687;&#30340;&#31232;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often the case that data are with multiple views in real-world applications. Fully exploring the information of each view is significant for making data more representative. However, due to various limitations and failures in data collection and pre-processing, it is inevitable for real data to suffer from view missing and data scarcity. The coexistence of these two issues makes it more challenging to achieve the pattern classification task. Currently, to our best knowledge, few appropriate methods can well-handle these two issues simultaneously. Aiming to draw more attention from the community to this challenge, we propose a new task in this paper, called few-shot partial multi-view learning, which focuses on overcoming the negative impact of the view-missing issue in the low-data regime. The challenges of this task are twofold: (i) it is difficult to overcome the impact of data scarcity under the interference of missing views; (ii) the limited number of data exacerbates informa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;ODENet&#21644;&#19968;&#31867;ResNet&#65292;&#8220;&#23485;&#24230;&#20026;n+m&#30340;ODENet&#21487;&#20197;&#36924;&#36817;${\rm \mathbb{R}^n}$&#19978;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#8221;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#26576;&#20010;&#35843;&#25972;&#21464;&#37327;&#30340;&#26799;&#24230;&#24182;&#29992;&#20110;&#26500;&#24314;ODENet&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;MNIST&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2101.10229</link><description>&lt;p&gt;
&#19968;&#20010;ODENet&#21644;ResNet&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#65306;&#25968;&#23398;&#20998;&#26512;&#19982;&#25968;&#20540;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Universal Approximation Properties for an ODENet and a ResNet: Mathematical Analysis and Numerical Experiments. (arXiv:2101.10229v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;ODENet&#21644;&#19968;&#31867;ResNet&#65292;&#8220;&#23485;&#24230;&#20026;n+m&#30340;ODENet&#21487;&#20197;&#36924;&#36817;${\rm \mathbb{R}^n}$&#19978;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#8221;&#65292;&#21516;&#26102;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#26576;&#20010;&#35843;&#25972;&#21464;&#37327;&#30340;&#26799;&#24230;&#24182;&#29992;&#20110;&#26500;&#24314;ODENet&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;MNIST&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;ODENet&#21644;&#19968;&#31867;ResNet&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;(UAP)&#65292;&#23427;&#20204;&#26159;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#31616;&#21270;&#25968;&#23398;&#27169;&#22411;&#12290; UAP&#21487;&#20197;&#38472;&#36848;&#22914;&#19979;:&#35774;$n$&#21644;$m$&#20998;&#21035;&#20026;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20986;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#24182;&#20551;&#35774;$m\leq n$&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#38750;&#22810;&#39033;&#24335;&#36830;&#32493;&#28608;&#27963;&#20989;&#25968;&#30340;&#23485;&#24230;&#20026;$n+m$&#30340;ODENet&#21487;&#20197;&#36924;&#36817;$\mathbb {R} ^ n$&#19978;&#32039;&#33268;&#23376;&#38598;&#19978;&#30340;&#20219;&#20309;&#36830;&#32493;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#36235;&#20110;&#26080;&#38480;&#26102;&#65292;ResNet&#20855;&#26377;&#30456;&#21516;&#30340;&#24615;&#36136;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26126;&#30830;&#25512;&#23548;&#20102;&#25439;&#22833;&#20989;&#25968;&#23545;&#26576;&#20010;&#35843;&#25972;&#21464;&#37327;&#30340;&#26799;&#24230;&#12290; &#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#26500;&#24314;ODENet&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#23637;&#31034;&#27492;&#31639;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;MNIST&#19978;&#30340;&#22238;&#24402;&#38382;&#39064;&#12289;&#20108;&#20998;&#31867;&#21644;&#22810;&#39033;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a universal approximation property (UAP) for a class of ODENet and a class of ResNet, which are simplified mathematical models for deep learning systems with skip connections. The UAP can be stated as follows. Let $n$ and $m$ be the dimension of input and output data, and assume $m\leq n$. Then we show that ODENet of width $n+m$ with any non-polynomial continuous activation function can approximate any continuous function on a compact subset on $\mathbb{R}^n$. We also show that ResNet has the same property as the depth tends to infinity. Furthermore, we derive the gradient of a loss function explicitly with respect to a certain tuning variable. We use this to construct a learning algorithm for ODENet. To demonstrate the usefulness of this algorithm, we apply it to a regression problem, a binary classification, and a multinomial classification in MNIST.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#22797;&#39318;&#20215;&#25293;&#21334;&#30340;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#30340;&#21453;&#39304;&#32467;&#26500;&#21644;&#25903;&#20184;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2003.09795</link><description>&lt;p&gt;
&#37325;&#22797;&#39318;&#20215;&#25293;&#21334;&#20013;&#30340;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal No-regret Learning in Repeated First-price Auctions. (arXiv:2003.09795v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.09795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#22797;&#39318;&#20215;&#25293;&#21334;&#30340;&#26368;&#20248;&#26080;&#24724;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#23450;&#30340;&#21453;&#39304;&#32467;&#26500;&#21644;&#25903;&#20184;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37325;&#22797;&#39318;&#20215;&#25293;&#21334;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#65292;&#25293;&#21334;&#32773;&#21482;&#22312;&#27599;&#27425;&#25293;&#21334;&#32467;&#26463;&#21518;&#30475;&#21040;&#26368;&#39640;&#30340;&#20986;&#20215;&#65292;&#20026;&#20102;&#26368;&#22823;&#21270;&#25910;&#30410;&#65292;&#22905;&#24517;&#39035;&#36827;&#34892;&#36866;&#24212;&#24615;&#20986;&#20215;&#12290;&#28982;&#32780;&#65292;&#25293;&#21334;&#32773;&#21482;&#33021;&#38754;&#23545;&#34987;&#23457;&#26597;&#30340;&#21453;&#39304;&#65292;&#22914;&#26524;&#22905;&#36194;&#24471;&#20986;&#20215;&#65292;&#23601;&#26080;&#27861;&#35266;&#23519;&#21040;&#20854;&#20182;&#31454;&#26631;&#32773;&#30340;&#26368;&#39640;&#20986;&#20215;&#65292;&#32780;&#20854;&#20182;&#31454;&#26631;&#32773;&#30340;&#26368;&#39640;&#20986;&#20215;&#26159;&#20174;&#26410;&#30693;&#30340;&#20998;&#24067;&#20013;\textit{iid}&#25277;&#21462;&#30340;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39318;&#20215;&#25293;&#21334;&#30340;&#20004;&#20010;&#32467;&#26500;&#24615;&#36136;&#65292;&#21363;&#29305;&#23450;&#30340;&#21453;&#39304;&#32467;&#26500;&#21644;&#25903;&#20184;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;$\widetilde{O}(\sqrt{T})$&#36951;&#25022;&#30028;&#38480;&#12290;&#39318;&#20215;&#25293;&#21334;&#20013;&#30340;&#21453;&#39304;&#26426;&#21046;&#32467;&#21512;&#20102;&#36328;&#34892;&#21160;(&#20986;&#20215;)&#30340;&#22270;&#24418;&#21453;&#39304;&#12289;&#36328;&#19978;&#19979;&#25991;(&#31169;&#20154;&#20215;&#20540;)&#30340;&#20132;&#21449;&#23398;&#20064;&#20197;&#21450;&#23545;&#19978;&#19979;&#25991;&#30340;&#37096;&#20998;&#25490;&#24207;&#65292;&#25105;&#20204;&#23558;&#20854;&#25512;&#24191;&#20026;&#37096;&#20998;&#25490;&#24207;&#24773;&#22659;&#36172;&#21338;&#26426;&#12290;&#36890;&#36807;&#23637;&#31034;&#25439;&#22833;&#20989;&#25968;&#19982;&#20248;&#21270;&#32467;&#26500;&#20043;&#38388;&#30340;&#22855;&#24618;&#20998;&#31163;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#27492;&#26694;&#26550;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#21518;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#26377;&#25928;&#21464;&#20307;&#65292;&#31216;&#20026;&#8220;&#24102;&#26377;&#22522;&#20110;&#26041;&#24046;&#30340;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#35843;&#29992;&#27491;&#21017;&#21270;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#8221;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21033;&#29992;&#20102;&#23545;&#38382;&#39064;&#26412;&#36136;&#32467;&#26500;&#21644;Kwon-Singer&#23450;&#29702;&#30340;&#20180;&#32454;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online learning in repeated first-price auctions where a bidder, only observing the winning bid at the end of each auction, learns to adaptively bid in order to maximize her cumulative payoff. To achieve this goal, the bidder faces a censored feedback: if she wins the bid, then she is not able to observe the highest bid of the other bidders, which we assume is \textit{iid} drawn from an unknown distribution. In this paper, we develop the first learning algorithm that achieves a near-optimal $\widetilde{O}(\sqrt{T})$ regret bound, by exploiting two structural properties of first-price auctions, i.e. the specific feedback structure and payoff function.  The feedback in first-price auctions combines the graph feedback across actions (bids), the cross learning across contexts (private values), and a partial order over the contexts; we generalize it as the partially ordered contextual bandits. We establish both strengths and weaknesses of this framework, by showing a curious separa
&lt;/p&gt;</description></item><item><title>Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2002.01368</link><description>&lt;p&gt;
&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#25193;&#23637;&#31867;&#21035;&#30340;&#24320;&#25918;&#38598;&#23398;&#20064;&#65288;Open-LACU&#65289;
&lt;/p&gt;
&lt;p&gt;
Open-set learning with augmented category by exploiting unlabeled data (Open-LACU). (arXiv:2002.01368v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.01368
&lt;/p&gt;
&lt;p&gt;
Open-LACU&#26159;&#19968;&#31181;&#26032;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#31574;&#30053;&#65292;&#23427;&#21487;&#20197;&#23558;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#19981;&#21516;&#30340;&#32972;&#26223;&#21644;&#26410;&#30693;&#31867;&#21035;&#26469;&#25552;&#39640;&#35757;&#32451;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;&#24320;&#25918;&#24335;&#35782;&#21035;&#65288;OSR&#65289;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#35768;&#22810;&#23581;&#35797;&#20197;&#21512;&#25104;&#21333;&#20010;&#35757;&#32451;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#27599;&#27425;&#23581;&#35797;&#37117;&#36829;&#21453;&#20102;&#24320;&#25918;&#38598;&#23450;&#20041;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#22312;&#26410;&#26631;&#35760;&#30340;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#26032;&#39062;&#30340;&#31867;&#21035;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20854;&#20013;&#20998;&#31867;&#22120;&#33021;&#22815;&#22312;&#35266;&#23519;&#21040;&#30340;&#21644;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#20043;&#38388;&#36827;&#34892;&#25512;&#24191;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#32972;&#26223;&#31867;&#21035;&#21644;&#26410;&#35266;&#23519;&#21040;&#26032;&#39062;&#31867;&#21035;&#30340;&#26410;&#30693;&#31867;&#21035;&#12290;&#36890;&#36807;&#20998;&#31867;&#36825;&#20004;&#31181;&#26032;&#39062;&#31867;&#21035;&#30340;&#26041;&#24335;&#65292;Open-LACU&#33021;&#22815;&#25552;&#39640;&#35757;&#32451;&#30340;&#25104;&#26412;&#25928;&#30410;&#24615;&#65292;&#24182;&#30830;&#20445;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#39062;&#31867;&#21035;&#26102;&#36827;&#34892;&#23433;&#20840;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several efforts have been made to synthesize semi-supervised learning (SSL) and open set recognition (OSR) within a single training policy. However, each attempt violated the definition of an open set by incorporating novel categories within the unlabeled training set. Although such \textit{observed} novel categories are undoubtedly prevalent in application-grade datasets, they should not be conflated with the OSR-defined \textit{unobserved} novel categories, which only emerge during testing. This study proposes a new learning policy wherein classifiers generalize between observed and unobserved novel categories. Specifically, our open-set learning with augmented category by exploiting unlabeled data (Open-LACU) policy defines a background category for observed novel categories and an unknown category for unobserved novel categories. By separating these novel category types, Open-LACU promotes cost-efficient training by eliminating the need to label every category and ensures safe clas
&lt;/p&gt;</description></item></channel></rss>