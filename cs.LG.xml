<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02499</link><description>&lt;p&gt;
&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Generalized Policies for Fully Observable Non-Deterministic Planning Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25193;&#23637;&#20102;&#23398;&#20064;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#35745;&#21010;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;&#22312;&#19968;&#20123; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#31574;&#30053;&#20195;&#34920;&#35299;&#20915;&#22823;&#37327;&#35745;&#21010;&#38382;&#39064;&#30340;&#21453;&#24212;&#24615;&#31574;&#30053;&#65292;&#20363;&#22914;&#20174;&#32473;&#23450;&#39046;&#22495;&#20013;&#26080;&#38480;&#21487;&#35299;&#23454;&#20363;&#30340;&#38598;&#21512;&#12290; &#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#31995;&#21015;&#23567;&#35757;&#32451;&#23454;&#20363;&#20013;&#23398;&#20064;&#36825;&#31181;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#32463;&#20856;&#39046;&#22495;&#12290; &#26412;&#25991;&#25193;&#23637;&#20102;&#23398;&#20064;&#38754;&#21521;&#23436;&#20840;&#21487;&#35266;&#23519;&#12289;&#38750;&#30830;&#23450;&#24615;&#65288;FOND&#65289;&#39046;&#22495;&#30340;&#27867;&#21270;&#31574;&#30053;&#30340;&#20844;&#24335;&#21644;&#23548;&#33268;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015; FOND &#35745;&#21010;&#22522;&#20934;&#39046;&#22495;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#39046;&#22495;&#20013;&#20135;&#29983;&#30340;&#27867;&#21270;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#27491;&#30830;&#24615;&#12290; &#23398;&#20064; FOND &#35745;&#21010;&#30340;&#27867;&#21270;&#31574;&#30053;&#26041;&#27861;&#23454;&#38469;&#19978;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#19968;&#31181;&#25628;&#32034;&#32467;&#26524;&#30340;&#21478;&#19968;&#31181; FOND &#35745;&#21010;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#26159;&#22312;&#32473;&#23450;&#29366;&#24577;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26159;&#22312;&#30001;&#24517;&#39035;&#23398;&#20064;&#30340;&#29305;&#24449;&#23450;&#20041;&#30340;&#25277;&#35937;&#31354;&#38388;&#20013;&#25628;&#32034;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02499v1 Announce Type: new  Abstract: General policies represent reactive strategies for solving large families of planning problems like the infinite collection of solvable instances from a given domain. Methods for learning such policies from a collection of small training instances have been developed successfully for classical domains. In this work, we extend the formulations and the resulting combinatorial methods for learning general policies over fully observable, non-deterministic (FOND) domains. We also evaluate the resulting approach experimentally over a number of benchmark domains in FOND planning, present the general policies that result in some of these domains, and prove their correctness. The method for learning general policies for FOND planning can actually be seen as an alternative FOND planning method that searches for solutions, not in the given state space but in an abstract space defined by features that must be learned as well.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2404.01714</link><description>&lt;p&gt;
&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#30340;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization Algorithm for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;CG-like-Adam&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#24182;&#22312;&#25910;&#25947;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#21152;&#24555;&#22521;&#35757;&#36895;&#24230;&#24182;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#20849;&#36717;&#26799;&#24230;&#20462;&#27491;&#20026;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#36890;&#29992;Adam&#20013;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CG-like-Adam&#30340;&#26032;&#20248;&#21270;&#31639;&#27861;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#29992;Adam&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#30697;&#20272;&#35745;&#22343;&#30001;&#20849;&#36717;&#26799;&#24230;&#26679;&#24335;&#26367;&#25442;&#12290;&#25910;&#25947;&#20998;&#26512;&#22788;&#29702;&#20102;&#19968;&#38454;&#30697;&#20272;&#35745;&#30340;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#31995;&#25968;&#20026;&#24120;&#25968;&#19988;&#19968;&#38454;&#30697;&#20272;&#35745;&#26080;&#20559;&#30340;&#24773;&#20917;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20102;&#22522;&#20110;CIFAR10/100&#25968;&#25454;&#38598;&#30340;&#25152;&#25552;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01714v1 Announce Type: cross  Abstract: Training deep neural networks is a challenging task. In order to speed up training and enhance the performance of deep neural networks, we rectify the vanilla conjugate gradient as conjugate-gradient-like and incorporate it into the generic Adam, and thus propose a new optimization algorithm named CG-like-Adam for deep learning. Specifically, both the first-order and the second-order moment estimation of generic Adam are replaced by the conjugate-gradient-like. Convergence analysis handles the cases where the exponential moving average coefficient of the first-order moment estimation is constant and the first-order moment estimation is unbiased. Numerical experiments show the superiority of the proposed algorithm based on the CIFAR10/100 dataset.
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;</title><link>https://arxiv.org/abs/2403.14421</link><description>&lt;p&gt;
&#23558;&#25193;&#25955;&#27169;&#22411;&#35843;&#25972;&#21040;&#31169;&#26377;&#39046;&#22495;&#32780;&#26080;&#38656;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14421
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14421v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25991;&#25688;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#23384;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#35760;&#24518;&#21270;&#38382;&#39064;&#65292;&#21487;&#33021;&#20250;&#22797;&#21046;&#20986;&#19982;&#20854;&#35757;&#32451;&#22270;&#20687;&#20960;&#20046;&#23436;&#20840;&#30456;&#21516;&#30340;&#21103;&#26412;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#24076;&#26395;&#30475;&#21040;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26679;&#26412;&#24182;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#38544;&#31169;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#22312;&#23569;&#37327;&#20844;&#20849;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#19968;&#20010;DP&#26816;&#32034;&#26426;&#21046;&#65292;&#20197;&#20174;&#31169;&#26377;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#26816;&#32034;&#30340;&#26679;&#26412;&#26469;&#22686;&#24378;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;\emph{&#24046;&#20998;&#38544;&#31169;&#26816;&#32034;&#22686;&#24378;&#25193;&#25955;&#27169;&#22411;}&#65288;DP-RDM&#65289;&#22312;&#36866;&#24212;&#21478;&#19968;&#20010;&#39046;&#22495;&#26102;&#26080;&#38656;&#23545;&#26816;&#32034;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#26679;&#26412;&#65292;&#21516;&#26102;&#28385;&#36275;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#12290;&#20363;&#22914;&#65292;&#22312;&#35780;&#20272;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14421v1 Announce Type: new  Abstract: Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our \emph{differentially private retrieval-augmented diffusion model} (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.14297</link><description>&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#24212;&#29992;&#28041;&#21450;&#22797;&#26434;&#21644;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#36890;&#24120;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#26222;&#36941;&#20551;&#35774;&#25968;&#25454;&#28304;&#23558;&#25345;&#32493;&#21487;&#29992;&#12290;&#19981;&#21516;&#24773;&#20917;&#21487;&#33021;&#24433;&#21709;EO&#25968;&#25454;&#28304;&#30340;&#21487;&#29992;&#24615;&#65292;&#22914;&#22122;&#22768;&#12289;&#20113;&#23618;&#25110;&#21355;&#26143;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#32570;&#22833;&#26102;&#38388;&#24615;&#21644;&#38745;&#24577;EO&#25968;&#25454;&#28304;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#32570;&#22833;&#25968;&#25454;&#26102;&#33258;&#28982;&#26356;&#21152;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#38598;&#25104;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20809;&#23398;&#35270;&#35282;&#22312;&#21333;&#29420;&#32570;&#22833;&#26102;&#26159;&#26368;&#20851;&#38190;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14297v1 Announce Type: cross  Abstract: Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.
&lt;/p&gt;</description></item><item><title>STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.12418</link><description>&lt;p&gt;
STG-Mamba: &#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12418
&lt;/p&gt;
&lt;p&gt;
STG-Mamba &#26159;&#39318;&#20010;&#21033;&#29992;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#26102;&#31354;&#22270;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Graph&#65288;STG&#65289;&#25968;&#25454;&#20855;&#26377;&#21160;&#24577;&#24615;&#12289;&#24322;&#36136;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#29305;&#28857;&#65292;&#23548;&#33268;&#26102;&#31354;&#22270;&#23398;&#20064;&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#38598;&#20013;&#20110;&#27169;&#25311;STG&#32593;&#32476;&#20013;&#33410;&#28857;&#20010;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;&#38543;&#26102;&#38388;&#23384;&#22312;&#30340;STG&#31995;&#32479;&#26412;&#36136;&#29305;&#24449;&#30340;&#24314;&#27169;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#29616;&#20195;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSMs&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#31934;&#24515;&#25506;&#32034;&#20102;STG&#31995;&#32479;&#22312;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#21160;&#24577;&#29366;&#24577;&#28436;&#21464;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Spatial-Temporal Graph Mamba&#65288;STG-Mamba&#65289;&#65292;&#20316;&#20026;&#39318;&#20010;&#21033;&#29992;&#24378;&#22823;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;STG&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#23558;STG&#32593;&#32476;&#35270;&#20026;&#31995;&#32479;&#65292;&#24182;&#37319;&#29992;&#22270;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22359;&#65288;GS3B&#65289;&#31934;&#30830;&#34920;&#24449;STG&#30340;&#21160;&#24577;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12418v1 Announce Type: cross  Abstract: Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Graph Selective State Space Block (GS3B) to precisely characterize the dynamic evolution of ST
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08901</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#21457;&#29616;&#21487;&#20449;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#30340;&#25112;&#30053;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08901
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#37319;&#29992;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24320;&#21457;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#39640;&#20445;&#30495;&#20223;&#30495;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;&#27169;&#22411;&#20013;&#30340;&#24191;&#27867;&#25972;&#21512;&#65292;&#20984;&#26174;&#20102;&#31283;&#20581;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#20445;&#20195;&#29702;&#27169;&#22411;&#21487;&#21487;&#38752;&#22320;&#29992;&#20110;&#37325;&#35201;&#20915;&#31574;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20195;&#29702;&#27169;&#22411;&#30340;Occam Plausibility Algorithm&#65288;OPAL-surrogate&#65289;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#65292;&#20197;&#22312;&#22823;&#37327;&#28508;&#22312;&#27169;&#22411;&#65288;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#31867;&#21035;&#20197;&#21450;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#65289;&#20013;&#25581;&#31034;&#20855;&#26377;&#39044;&#27979;&#33021;&#21147;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#26694;&#26550;&#22522;&#20110;&#23618;&#27425;&#36125;&#21494;&#26031;&#25512;&#29702;&#65292;&#24182;&#37319;&#29992;&#27169;&#22411;&#39564;&#35777;&#27979;&#35797;&#26469;&#35780;&#20272;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#20195;&#29702;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#39044;&#27979;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21407;&#21017;&#65292;OPAL-surrogate&#24341;&#20837;&#20102;&#19968;&#31181;&#31995;&#32479;&#24615;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08901v1 Announce Type: cross  Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and
&lt;/p&gt;</description></item><item><title>C2P-GCN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22270;&#24418;&#25104;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#24418;&#25104;&#34917;&#19969;&#32423;&#22270;&#65292;&#31532;&#20108;&#38454;&#27573;&#24418;&#25104;&#22270;&#20687;&#32423;&#22270;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.04962</link><description>&lt;p&gt;
C2P-GCN&#65306;&#29992;&#20110;&#32467;&#30452;&#32928;&#30284;&#20998;&#32423;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
C2P-GCN: Cell-to-Patch Graph Convolutional Network for Colorectal Cancer Grading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04962
&lt;/p&gt;
&lt;p&gt;
C2P-GCN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22270;&#24418;&#25104;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#24418;&#25104;&#34917;&#19969;&#32423;&#22270;&#65292;&#31532;&#20108;&#38454;&#27573;&#24418;&#25104;&#22270;&#20687;&#32423;&#22270;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#25429;&#25417;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#32534;&#30721;&#32452;&#32455;/&#22120;&#23448;&#32467;&#26500;&#20449;&#24687;&#65292;&#36880;&#28176;&#25104;&#20026;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#23398;&#22270;&#20687;&#20998;&#32423;&#30340;&#39318;&#36873;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#22270;&#30340;&#25216;&#26415;&#28041;&#21450;&#23558;&#25972;&#24352;&#24187;&#28783;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#20998;&#25104;&#26356;&#23567;&#25110;&#20013;&#31561;&#22823;&#23567;&#30340;&#34917;&#19969;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#34917;&#19969;&#19978;&#26500;&#24314;&#22270;&#20197;&#30452;&#25509;&#29992;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#25972;&#20010;WSI&#20013;&#23384;&#22312;&#30340;&#32452;&#32455;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20381;&#36182;&#20110;&#26469;&#33258;&#22823;&#37327;&#22270;&#20687;&#34917;&#19969;&#30340;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32454;&#32990;&#21040;&#34917;&#19969;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;C2P-GCN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#22270;&#24418;&#25104;&#30340;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#23427;&#22522;&#20110;WSI&#19978;&#27599;&#20010;&#34917;&#19969;&#19978;&#30340;&#32454;&#32990;&#32452;&#32455;&#24418;&#25104;&#19968;&#20010;&#34917;&#19969;&#32423;&#22270;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#23427;&#22522;&#20110;WSI&#20013;&#27599;&#20010;&#34917;&#19969;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#37327;&#24418;&#25104;&#19968;&#20010;&#22270;&#20687;&#32423;&#22270;&#65292;&#23558;&#27599;&#20010;&#34917;&#19969;&#35270;&#20026;&#22270;&#30340;&#33410;&#28857;&#12290;&#36825;&#31181;&#22270;&#34920;&#31034;&#26159;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04962v1 Announce Type: cross  Abstract: Graph-based learning approaches, due to their ability to encode tissue/organ structure information, are increasingly favored for grading colorectal cancer histology images. Recent graph-based techniques involve dividing whole slide images (WSIs) into smaller or medium-sized patches, and then building graphs on each patch for direct use in training. This method, however, fails to capture the tissue structure information present in an entire WSI and relies on training from a significantly large dataset of image patches. In this paper, we propose a novel cell-to-patch graph convolutional network (C2P-GCN), which is a two-stage graph formation-based approach. In the first stage, it forms a patch-level graph based on the cell organization on each patch of a WSI. In the second stage, it forms an image-level graph based on a similarity measure between patches of a WSI considering each patch as a node of a graph. This graph representation is t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#25512;&#36831;&#23545;&#20154;&#32676;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#21069;&#25152;&#26410;&#35265;&#30340;&#19987;&#23478;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#38754;&#23545;&#22256;&#38590;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.02683</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#23545;&#20154;&#32676;&#30340;&#23398;&#20064;&#65306;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to Defer to a Population: A Meta-Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23398;&#20064;&#25512;&#36831;&#23545;&#20154;&#32676;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#21069;&#25152;&#26410;&#35265;&#30340;&#19987;&#23478;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#38754;&#23545;&#22256;&#38590;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02683v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26694;&#26550;&#20801;&#35768;&#33258;&#20027;&#31995;&#32479;&#36890;&#36807;&#23558;&#22256;&#38590;&#20915;&#31574;&#22996;&#25176;&#32473;&#20154;&#31867;&#19987;&#23478;&#26469;&#20445;&#25345;&#23433;&#20840;&#21644;&#20581;&#22766;&#12290;&#25152;&#26377;&#29616;&#26377;&#30340;&#20851;&#20110;L2D&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#27599;&#20010;&#19987;&#23478;&#37117;&#21487;&#20197;&#24456;&#22909;&#22320;&#30830;&#23450;&#65292;&#24182;&#19988;&#22914;&#26524;&#20219;&#20309;&#19987;&#23478;&#21457;&#29983;&#21464;&#21270;&#65292;&#31995;&#32479;&#24212;&#35813;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20943;&#36731;&#20102;&#36825;&#19968;&#38480;&#21046;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;L2D&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#24212;&#23545;&#21069;&#25152;&#26410;&#35265;&#30340;&#19987;&#23478;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#32771;&#34385;&#20102;&#22522;&#20110;&#20248;&#21270;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#21464;&#20307;&#12290;&#32473;&#23450;&#19968;&#20010;&#23567;&#30340;&#19978;&#19979;&#25991;&#38598;&#26469;&#25551;&#36848;&#24403;&#21069;&#21487;&#29992;&#30340;&#19987;&#23478;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#24555;&#36895;&#35843;&#25972;&#23427;&#30340;&#25512;&#36831;&#31574;&#30053;&#12290;&#23545;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#23547;&#25214;&#19978;&#19979;&#25991;&#38598;&#20013;&#19982;&#32473;&#23450;&#27979;&#35797;&#28857;&#30456;&#20284;&#30340;&#28857;&#65292;&#20174;&#32780;&#26356;&#31934;&#30830;&#22320;&#35780;&#20272;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02683v1 Announce Type: new  Abstract: The learning to defer (L2D) framework allows autonomous systems to be safe and robust by allocating difficult decisions to a human expert. All existing work on L2D assumes that each expert is well-identified, and if any expert were to change, the system should be re-trained. In this work, we alleviate this constraint, formulating an L2D system that can cope with never-before-seen experts at test-time. We accomplish this by using meta-learning, considering both optimization- and model-based variants. Given a small context set to characterize the currently available expert, our framework can quickly adapt its deferral policy. For the model-based approach, we employ an attention mechanism that is able to look for points in the context set that are similar to a given test point, leading to an even more precise assessment of the expert's abilities. In the experiments, we validate our methods on image recognition, traffic sign detection, and s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;SciML&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22312;UQ&#20219;&#21153;&#20013;&#21033;&#29992;SciML&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.15115</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained polynomial chaos expansion for scientific machine learning and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15115
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#26041;&#27861;&#65292;&#23558;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#19982;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#26377;&#25928;&#22320;&#23454;&#29616;SciML&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#22312;UQ&#20219;&#21153;&#20013;&#21033;&#29992;SciML&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#32422;&#26463;&#30340;&#22810;&#39033;&#24335;&#28151;&#27788;&#23637;&#24320;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#25191;&#34892;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#20219;&#21153;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#29420;&#29305;&#30340;&#33021;&#21147;&#65306;&#23558;SciML&#19982;UQ&#26080;&#32541;&#38598;&#25104;&#65292;&#20174;&#32780;&#33021;&#22815;&#26377;&#25928;&#22320;&#37327;&#21270;SciML&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21033;&#29992;SciML&#26469;&#25913;&#21892;UQ&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;&#35813;&#26367;&#20195;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32435;&#20837;&#22810;&#31181;&#29289;&#29702;&#32422;&#26463;&#65292;&#22914;&#25903;&#37197;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#21450;&#20854;&#30456;&#20851;&#30340;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#32422;&#26463;&#65292;&#19981;&#31561;&#24335;&#22411;&#32422;&#26463;&#65288;&#22914;&#21333;&#35843;&#24615;&#65292;&#20984;&#24615;&#65292;&#38750;&#36127;&#24615;&#31561;&#65289;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#39069;&#22806;&#20808;&#39564;&#20449;&#24687;&#20197;&#36741;&#21161;&#26377;&#38480;&#25968;&#25454;&#12290;&#36825;&#30830;&#20445;&#20102;&#29289;&#29702;&#19978;&#21512;&#29702;&#30340;&#39044;&#27979;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#26114;&#36149;&#35745;&#31639;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15115v1 Announce Type: cross  Abstract: We present a novel physics-constrained polynomial chaos expansion as a surrogate modeling method capable of performing both scientific machine learning (SciML) and uncertainty quantification (UQ) tasks. The proposed method possesses a unique capability: it seamlessly integrates SciML into UQ and vice versa, which allows it to quantify the uncertainties in SciML tasks effectively and leverage SciML for improved uncertainty assessment during UQ-related tasks. The proposed surrogate model can effectively incorporate a variety of physical constraints, such as governing partial differential equations (PDEs) with associated initial and boundary conditions constraints, inequality-type constraints (e.g., monotonicity, convexity, non-negativity, among others), and additional a priori information in the training process to supplement limited data. This ensures physically realistic predictions and significantly reduces the need for expensive comp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.14528</link><description>&lt;p&gt;
ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ACE : Off-Policy Actor-Critic with Causality-Aware Entropy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACE&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#65292;&#26377;&#25928;&#35780;&#20272;&#19981;&#21516;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20998;&#26512;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24341;&#20837;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#22312;&#22810;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24573;&#35270;&#20102;&#31574;&#30053;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#21407;&#22987;&#34892;&#20026;&#30340;&#21464;&#21270;&#37325;&#35201;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#21160;&#20316;&#32500;&#24230;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35780;&#20272;&#35757;&#32451;&#36807;&#31243;&#20013;&#21508;&#31181;&#21407;&#22987;&#34892;&#20026;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#24863;&#30693;&#29109;&#39033;&#65292;&#26377;&#25928;&#22320;&#35782;&#21035;&#24182;&#20248;&#20808;&#22788;&#29702;&#20855;&#26377;&#39640;&#28508;&#22312;&#24433;&#21709;&#30340;&#34892;&#21160;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#38450;&#27490;&#23545;&#29305;&#23450;&#21407;&#22987;&#34892;&#20026;&#36807;&#24230;&#20851;&#27880;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26799;&#24230;&#20241;&#30496;&#29616;&#35937;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20241;&#30496;&#24341;&#23548;&#22797;&#20301;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;ACE&#65306;&#20855;&#26377;&#22240;&#26524;&#24863;&#30693;&#29109;&#27491;&#21017;&#21270;&#30340;&#31163;&#31574;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65292;&#22312;&#36328;7&#20010;&#39046;&#22495;&#30340;29&#20010;&#19981;&#21516;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#65292;&#30456;&#36739;&#20110;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#22522;&#32447;&#65292;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14528v1 Announce Type: cross  Abstract: The varying significance of distinct primitive behaviors during the policy learning process has been overlooked by prior model-free RL algorithms. Leveraging this insight, we explore the causal relationship between different action dimensions and rewards to evaluate the significance of various primitive behaviors during training. We introduce a causality-aware entropy term that effectively identifies and prioritizes actions with high potential impacts for efficient exploration. Furthermore, to prevent excessive focus on specific primitive behaviors, we analyze the gradient dormancy phenomenon and introduce a dormancy-guided reset mechanism to further enhance the efficacy of our method. Our proposed algorithm, ACE: Off-policy Actor-critic with Causality-aware Entropy regularization, demonstrates a substantial performance advantage across 29 diverse continuous control tasks spanning 7 domains compared to model-free RL baselines, which un
&lt;/p&gt;</description></item><item><title>HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10228</link><description>&lt;p&gt;
HyperAgent&#65306;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#19988;&#21487;&#35777;&#26126;&#29992;&#20110;&#22797;&#26434;&#29615;&#22659;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyperAgent: A Simple, Scalable, Efficient and Provable Reinforcement Learning Framework for Complex Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10228
&lt;/p&gt;
&lt;p&gt;
HyperAgent&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#33021;&#22815;&#23454;&#29616;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#26159;&#39318;&#20010;&#36798;&#21040;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#38656;&#35201;&#31616;&#21333;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#12289;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#19981;&#26029;&#31215;&#32047;&#30340;&#20132;&#20114;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#36229;&#27169;&#22411;&#12289;&#32034;&#24341;&#25277;&#26679;&#26041;&#26696;&#21644;&#22686;&#37327;&#26356;&#26032;&#26426;&#21046;&#30340;RL&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#20215;&#20540;&#20989;&#25968;&#36924;&#36817;&#20013;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#39034;&#24207;&#21518;&#39564;&#36924;&#36817;&#21644;&#25968;&#25454;&#39640;&#25928;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#36229;&#36234;&#20102;&#20849;&#36717;&#24615;&#12290;HyperAgent&#30340;&#23454;&#29616;&#31616;&#21333;&#65292;&#21482;&#38656;&#35201;&#22312;DDQN&#20013;&#28155;&#21152;&#19968;&#20010;&#27169;&#22359;&#21644;&#19968;&#34892;&#39069;&#22806;&#20195;&#30721;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;HyperAgent&#22312;&#22823;&#35268;&#27169;&#28145;&#24230;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#25968;&#25454;&#36824;&#26159;&#35745;&#31639;&#26041;&#38754;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#25552;&#21319;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#22312;&#23454;&#38469;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#20013;&#65292;HyperAgent&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#21487;&#35777;&#26126;&#21487;&#25193;&#23637;&#30340;&#27599;&#27493;&#35745;&#31639;&#22797;&#26434;&#24230;&#20197;&#21450;&#27425;&#32447;&#24615;&#21518;&#24724;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10228v1 Announce Type: cross  Abstract: To solve complex tasks under resource constraints, reinforcement learning (RL) agents need to be simple, efficient, and scalable with (1) large state space and (2) increasingly accumulated data of interactions. We propose the HyperAgent, a RL framework with hypermodel, index sampling schemes and incremental update mechanism, enabling computation-efficient sequential posterior approximation and data-efficient action selection under general value function approximation beyond conjugacy. The implementation of \HyperAgent is simple as it only adds one module and one line of code additional to DDQN. Practically, HyperAgent demonstrates its robust performance in large-scale deep RL benchmarks with significant efficiency gain in terms of both data and computation. Theoretically, among the practically scalable algorithms, HyperAgent is the first method to achieve provably scalable per-step computational complexity as well as sublinear regret u
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08674</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#20154;&#31867;&#35838;&#31243;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08674
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#35268;&#21017;&#32467;&#26500;&#21644;&#35757;&#32451;&#20013;&#25152;&#20351;&#29992;&#30340;&#31034;&#20363;&#35838;&#31243;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#30001;&#31616;&#27905;&#35268;&#21017;&#25511;&#21046;&#30340;&#20219;&#21153;&#20013;&#65292;&#24403;&#30456;&#20851;&#31034;&#20363;&#22312;&#22810;&#27425;&#35797;&#39564;&#20013;&#34987;&#20998;&#32452;&#26102;&#65292;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#65307;&#20294;&#22312;&#32570;&#20047;&#36825;&#26679;&#30340;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#20132;&#38169;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#27809;&#26377;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#65288;ICL&#65289;&#22312;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33258;&#21457;&#20135;&#29983;&#20102;&#21516;&#26679;&#30340;&#26435;&#34913;&#12290;ICL&#26159;&#36890;&#36807;&#20869;&#23618;&#24490;&#29615;&#31639;&#27861;&#22312;&#28608;&#27963;&#21160;&#21147;&#23398;&#20013;&#23454;&#29616;&#30340;&#19968;&#31181;&#8220;&#19978;&#19979;&#25991;&#20869;&#23398;&#20064;&#8221;&#65288;in-context learning&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26435;&#37325;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20803;&#23398;&#20064;&#21464;&#21387;&#22120;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#22312;&#28041;&#21450;&#35268;&#21017;&#32467;&#26500;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20154;&#31867;&#25152;&#31034;&#30340;&#20998;&#32452;&#20248;&#21183;&#65292;&#32780;&#21516;&#26102;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#21017;&#22797;&#21046;&#20102;&#20154;&#31867;&#22312;&#32570;&#23569;&#36825;&#26679;&#32467;&#26500;&#30340;&#20219;&#21153;&#19978;&#25152;&#35266;&#23519;&#21040;&#30340;&#20132;&#38169;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#21333;&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#36755;&#20837;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07899</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#30340;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A systematic investigation of learnability from single child linguistic input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07899
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#21333;&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#36755;&#20837;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#29983;&#25104;&#35821;&#35328;&#36830;&#36143;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102; remarkable proficiency&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#19982;&#20154;&#31867;&#35821;&#35328;&#21487;&#23398;&#20064;&#24615;&#30340;&#30456;&#20851;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#19982;&#20799;&#31461;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#36755;&#20837;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#24046;&#36317;&#12290;LMs&#36890;&#24120;&#22312;&#25968;&#37327;&#32423;&#19978;&#26356;&#22823;&#19988;&#26412;&#36136;&#19982;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#19981;&#21516;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#21333;&#20010;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#30340;&#23376;&#38598;&#19978;&#35757;&#32451;LMs&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#35757;&#32451;&#30340;LMs&#21487;&#20197;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#20799;&#31461;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;LSTMs&#21644;&#26356;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#26816;&#39564;&#20174;&#21333;&#19968;&#20799;&#31461;&#36755;&#20837;&#21487;&#23398;&#20064;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#8230;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall
&lt;/p&gt;</description></item><item><title>Hydragen&#26159;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#21069;&#32512;&#30340;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#20998;&#35299;&#20026;&#20849;&#20139;&#21069;&#32512;&#21644;&#21807;&#19968;&#21518;&#32512;&#65292;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#31471;&#21040;&#31471;LLM&#21534;&#21520;&#37327;&#22810;&#36798;32&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05099</link><description>&lt;p&gt;
Hydragen&#65306;&#20849;&#20139;&#21069;&#32512;&#30340;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hydragen: High-Throughput LLM Inference with Shared Prefixes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05099
&lt;/p&gt;
&lt;p&gt;
Hydragen&#26159;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#21069;&#32512;&#30340;&#39640;&#21534;&#21520;&#37327;LLM&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#20998;&#35299;&#20026;&#20849;&#20139;&#21069;&#32512;&#21644;&#21807;&#19968;&#21518;&#32512;&#65292;&#26469;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#31471;&#21040;&#31471;LLM&#21534;&#21520;&#37327;&#22810;&#36798;32&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29616;&#22312;&#24050;&#32463;&#37096;&#32626;&#21040;&#25968;&#20159;&#29992;&#25143;&#19978;&#12290;LLM&#25512;&#29702;&#36890;&#24120;&#22312;&#20849;&#20139;&#21069;&#32512;&#30340;&#24207;&#21015;&#25209;&#27425;&#19978;&#25191;&#34892;&#65292;&#20363;&#22914;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#31995;&#32479;&#25552;&#31034;&#12290;&#22312;&#36825;&#31181;&#22823;&#25209;&#37327;&#35774;&#32622;&#19979;&#65292;&#35299;&#30721;&#21487;&#33021;&#20250;&#21463;&#21040;&#27880;&#24847;&#25805;&#20316;&#30340;&#29942;&#39048;&#65292;&#35813;&#25805;&#20316;&#20174;&#20869;&#23384;&#20013;&#35835;&#21462;&#22823;&#22411;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#65292;&#24182;&#20026;&#25209;&#27425;&#20013;&#30340;&#27599;&#20010;&#24207;&#21015;&#35745;&#31639;&#20302;&#25928;&#30340;&#30697;&#38453;-&#21521;&#37327;&#20056;&#31215;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Hydragen&#65292;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#21069;&#32512;&#30340;&#30828;&#20214;&#24863;&#30693;&#31934;&#30830;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#12290;Hydragen&#23558;&#27880;&#24847;&#21147;&#20998;&#21035;&#35745;&#31639;&#22312;&#20849;&#20139;&#21069;&#32512;&#21644;&#21807;&#19968;&#21518;&#32512;&#19978;&#12290;&#36825;&#31181;&#20998;&#35299;&#36890;&#36807;&#22312;&#24207;&#21015;&#20043;&#38388;&#25209;&#37327;&#26597;&#35810;&#19968;&#36215;&#20943;&#23569;&#20887;&#20313;&#20869;&#23384;&#35835;&#21462;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#21069;&#32512;&#27880;&#24847;&#21147;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#20351;&#29992;&#30828;&#20214;&#21451;&#22909;&#30340;&#30697;&#38453;&#20056;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#31471;&#21040;&#31471;&#30340;LLM&#21534;&#21520;&#37327;&#25552;&#39640;&#22810;&#36798;32&#20493;&#65292;&#36229;&#36807;&#31454;&#20105;&#22522;&#32447;&#65292;&#24182;&#19988;&#38543;&#30528;&#25209;&#27425;&#22823;&#23567;&#21644;&#20849;&#20139;&#21069;&#32512;&#30340;&#38271;&#24230;&#22686;&#21152;&#65292;&#36895;&#24230;&#25552;&#39640;&#30340;&#24133;&#24230;&#20063;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large language models (LLMs) are now deployed to hundreds of millions of users. LLM inference is commonly performed on batches of sequences that share a prefix, such as few-shot examples or a chatbot system prompt. Decoding in this large-batch setting can be bottlenecked by the attention operation, which reads large key-value (KV) caches from memory and computes inefficient matrix-vector products for every sequence in the batch. In this work, we introduce Hydragen, a hardware-aware exact implementation of attention with shared prefixes. Hydragen computes attention over the shared prefix and unique suffixes separately. This decomposition enables efficient prefix attention by batching queries together across sequences, reducing redundant memory reads and enabling the use of hardware-friendly matrix multiplications. Our method can improve end-to-end LLM throughput by up to 32x against competitive baselines, with speedup growing with the batch size and shared prefix lengt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stable Audio&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#21644;&#26465;&#20214;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29983;&#25104;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Stable Audio&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#30340;&#38899;&#20048;&#65292;&#24182;&#22312;&#24615;&#33021;&#35780;&#20272;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.04825</link><description>&lt;p&gt;
&#24555;&#36895;&#23450;&#26102;&#26465;&#20214;&#19979;&#30340;&#28508;&#22312;&#38899;&#39057;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Fast Timing-Conditioned Latent Audio Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Stable Audio&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#21644;&#26465;&#20214;&#21270;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#29983;&#25104;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Stable Audio&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#30340;&#38899;&#20048;&#65292;&#24182;&#22312;&#24615;&#33021;&#35780;&#20272;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#38271;&#31687;44.1kHz&#31435;&#20307;&#22768;&#38899;&#39057;&#21487;&#33021;&#23545;&#35745;&#31639;&#35201;&#27714;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#24182;&#27809;&#26377;&#35299;&#20915;&#38899;&#20048;&#21644;&#38899;&#25928;&#22312;&#25345;&#32493;&#26102;&#38388;&#19978;&#30340;&#33258;&#28982;&#21464;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20197;&#39640;&#25928;&#26041;&#24335;&#29983;&#25104;&#38271;&#31687;&#12289;&#21487;&#21464;&#38271;&#24230;&#30340;44.1kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#21644;&#38899;&#25928;&#12290;Stable Audio&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#65292;&#20854;&#28508;&#22312;&#24615;&#36136;&#30001;&#19968;&#20010;&#20840;&#21367;&#31215;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23450;&#20041;&#12290;&#23427;&#19981;&#20165;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#36824;&#22522;&#20110;&#26102;&#38388;&#23884;&#20837;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#29983;&#25104;&#30340;&#38899;&#20048;&#21644;&#38899;&#25928;&#30340;&#20869;&#23481;&#21644;&#38271;&#24230;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#22312;A100 GPU&#19978;&#65292;Stable Audio&#33021;&#22815;&#22312;8&#31186;&#20869;&#20197;44.1kHz&#30340;&#36895;&#24230;&#28210;&#26579;&#38271;&#36798;95&#31186;&#30340;&#31435;&#20307;&#22768;&#20449;&#21495;&#12290;&#23613;&#31649;&#35745;&#31639;&#25928;&#29575;&#39640;&#19988;&#25512;&#29702;&#36895;&#24230;&#24555;&#65292;&#20294;&#23427;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#25991;&#26412;-&#38899;&#20048;&#21644;&#38899;&#39057;&#22522;&#20934;&#20013;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#65292;&#24182;&#19988;&#19982;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#32467;&#26500;&#21644;&#31435;&#20307;&#22768;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating long-form 44.1kHz stereo audio from text prompts can be computationally demanding. Further, most previous works do not tackle that music and sound effects naturally vary in their duration. Our research focuses on the efficient generation of long-form, variable-length stereo music and sounds at 44.1kHz using text prompts with a generative model. Stable Audio is based on latent diffusion, with its latent defined by a fully-convolutional variational autoencoder. It is conditioned on text prompts as well as timing embeddings, allowing for fine control over both the content and length of the generated music and sounds. Stable Audio is capable of rendering stereo signals of up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute efficiency and fast inference, it is one of the best in two public text-to-music and -audio benchmarks and, differently from state-of-the-art models, can generate music with structure and stereo sounds.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#26799;&#24230;&#32500;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04005</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#19981;&#30830;&#23450;&#24615;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Bayesian Uncertainty for Gradient Aggregation in Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#27010;&#29575;&#20998;&#24067;&#26469;&#37327;&#21270;&#26799;&#24230;&#32500;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#26085;&#30410;&#31361;&#20986;&#65292;&#38656;&#35201;&#21516;&#26102;&#25191;&#34892;&#22810;&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#38656;&#27714;&#20063;&#22312;&#22686;&#38271;&#12290;&#20026;&#27599;&#20010;&#20219;&#21153;&#36816;&#34892;&#19987;&#29992;&#27169;&#22411;&#22312;&#35745;&#31639;&#19978;&#21313;&#20998;&#26114;&#36149;&#65292;&#22240;&#27492;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#20852;&#36259;&#20063;&#36234;&#26469;&#36234;&#22823;&#12290;MTL&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#33021;&#39640;&#25928;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#30340;&#21333;&#20010;&#27169;&#22411;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#20219;&#21153;&#30340;&#21333;&#19968;&#26799;&#24230;&#24182;&#23558;&#23427;&#20204;&#32858;&#21512;&#36215;&#26469;&#20197;&#33719;&#24471;&#32467;&#21512;&#30340;&#26356;&#26032;&#26041;&#21521;&#26469;&#20248;&#21270;MTL&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#19968;&#20010;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#21363;&#26799;&#24230;&#32500;&#24230;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#20026;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#25918;&#32622;&#19968;&#20010;&#27010;&#29575;&#20998;&#24067;&#65292;&#36825;&#21448;&#24341;&#36215;&#20102;&#20219;&#21153;&#26799;&#24230;&#30340;&#20998;&#24067;&#12290;&#36825;&#20123;&#39069;&#22806;&#30340;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#20351;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#27599;&#20010;&#26799;&#24230;&#32500;&#24230;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#22312;&#32858;&#21512;&#23427;&#20204;&#26102;&#23558;&#20854;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning becomes more prominent there is a growing demand to perform several inference tasks in parallel. Running a dedicated model for each task is computationally expensive and therefore there is a great interest in multi-task learning (MTL). MTL aims at learning a single model that solves several tasks efficiently. Optimizing MTL models is often achieved by computing a single gradient per task and aggregating them for obtaining a combined update direction. However, these approaches do not consider an important aspect, the sensitivity in the gradient dimensions. Here, we introduce a novel gradient aggregation approach using Bayesian inference. We place a probability distribution over the task-specific parameters, which in turn induce a distribution over the gradients of the tasks. This additional valuable information allows us to quantify the uncertainty in each of the gradients dimensions, which can then be factored in when aggregating them. We empirically demonstrate the
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#20250;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#65292;&#30740;&#31350;&#21457;&#29616;&#24120;&#35265;&#19988;&#20855;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02868</link><description>&lt;p&gt;
&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#26263;&#22320;&#37324;&#26159;&#19968;&#31181;&#36951;&#24536;&#32531;&#35299;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Reinforcement Learning Models is Secretly a Forgetting Mitigation Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02868
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#20250;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#65292;&#30740;&#31350;&#21457;&#29616;&#24120;&#35265;&#19988;&#20855;&#26377;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36890;&#36807;&#20351;&#29992;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#25216;&#26415;&#65292;&#20801;&#35768;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36716;&#31227;&#33021;&#21147;&#65292;&#26368;&#36817;&#22522;&#30784;&#27169;&#22411;&#30340;&#25104;&#21151;&#24212;&#29992;&#23601;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#32454;&#35843;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20174;&#21160;&#20316;&#21644;&#35266;&#23519;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30340;&#35282;&#24230;&#65292;&#23558;&#32454;&#35843;&#38454;&#27573;&#26410;&#35775;&#38382;&#21040;&#30340;&#19979;&#28216;&#20219;&#21153;&#29366;&#24577;&#23376;&#31354;&#38388;&#20013;&#30340;&#39044;&#35757;&#32451;&#33021;&#21147;&#36951;&#24536;&#38382;&#39064;&#20316;&#20026;&#23548;&#33268;&#36716;&#31227;&#25928;&#26524;&#24046;&#30340;&#19968;&#20010;&#20855;&#20307;&#21407;&#22240;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#12290;&#27169;&#22411;&#22312;&#36825;&#20010;&#26410;&#35775;&#38382;&#21040;&#30340;&#29366;&#24577;&#23376;&#31354;&#38388;&#20013;&#30340;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#39044;&#35757;&#32451;&#20351;&#20854;&#22833;&#21435;&#20102;&#26399;&#26395;&#30340;&#36716;&#31227;&#20248;&#21183;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#35813;&#38382;&#39064;&#21457;&#29983;&#30340;&#26465;&#20214;&#65292;&#34920;&#26126;&#23427;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#26159;&#28798;&#38590;&#24615;&#30340;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;NetHack&#21644;Montezuma's Revenge&#29615;&#22659;&#36827;&#34892;&#35814;&#32454;&#30340;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#30340;&#30693;&#35782;&#20445;&#30041;&#25216;&#26415;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#20805;&#20998;&#21033;&#29992;&#32454;&#35843;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is a widespread technique that allows practitioners to transfer pre-trained capabilities, as recently showcased by the successful applications of foundation models. However, fine-tuning reinforcement learning (RL) models remains a challenge. This work conceptualizes one specific cause of poor transfer, accentuated in the RL setting by the interplay between actions and observations: forgetting of pre-trained capabilities. Namely, a model deteriorates on the state subspace of the downstream task not visited in the initial phase of fine-tuning, on which the model behaved well due to pre-training. This way, we lose the anticipated transfer benefits. We identify conditions when this problem occurs, showing that it is common and, in many cases, catastrophic. Through a detailed empirical analysis of the challenging NetHack and Montezuma's Revenge environments, we show that standard knowledge retention techniques mitigate the problem and thus allow us to take full advantage of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35270;&#35273;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02382</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35270;&#35273;&#35843;&#25972;&#20013;&#25552;&#31034;&#35789;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Power of Prompt for Visual Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35270;&#35273;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25552;&#31034;&#35843;&#25972;&#65288;VPT&#65289;&#26159;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#35789;&#26469;&#23450;&#21046;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;VPT&#21450;&#20854;&#21464;&#31181;&#32463;&#24120;&#36935;&#21040;&#35832;&#22914;&#25552;&#31034;&#21021;&#22987;&#21270;&#12289;&#25552;&#31034;&#38271;&#24230;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#20013;&#24615;&#33021;&#19981;&#20339;&#31561;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#25104;&#21151;&#30340;&#19978;&#19979;&#25991;&#36866;&#24212;&#12290;&#26412;&#30740;&#31350;&#20174;&#25506;&#32034;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#31034;&#35789;&#19982;&#34917;&#19969;&#20196;&#29260;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#28436;&#21464;&#24320;&#22987;&#12290;&#21463;&#21040;&#25552;&#31034;&#20196;&#29260;&#19982;&#34917;&#19969;&#20196;&#29260;&#20043;&#38388;&#24448;&#24448;&#20855;&#26377;&#39640;&#20114;&#20449;&#24687;&#30340;&#35266;&#23519;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19979;&#28216;&#20196;&#29260;&#21407;&#22411;&#23545;&#25552;&#31034;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#35813;&#31574;&#30053;&#24615;&#21021;&#22987;&#21270;&#26126;&#26174;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;VPT&#65292;&#26080;&#38656;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20248;&#21270;&#20196;&#29260;&#26500;&#36896;&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#27969;&#31243;&#20445;&#25345;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#35814;&#23613;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual prompt tuning (VPT) is a promising solution incorporating learnable prompt tokens to customize pre-trained models for downstream tasks. However, VPT and its variants often encounter challenges like prompt initialization, prompt length, and subpar performance in self-supervised pretraining, hindering successful contextual adaptation. This study commences by exploring the correlation evolvement between prompts and patch tokens during proficient training. Inspired by the observation that the prompt tokens tend to share high mutual information with patch tokens, we propose initializing prompts with downstream token prototypes. The strategic initialization, a stand-in for the previous initialization, substantially improves performance in fine-tuning. To refine further, we optimize token construction with a streamlined pipeline that maintains excellent performance with almost no increase in computational expenses compared to VPT. Exhaustive experiments show our proposed approach outpe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#31934;&#31616;&#20316;&#20026;&#26426;&#22120;&#36951;&#24536;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#25345;&#20037;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#20102;&#36817;&#20284;&#36951;&#24536;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00195</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#31616;&#39537;&#21160;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation Driven Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00195
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#38598;&#31934;&#31616;&#20316;&#20026;&#26426;&#22120;&#36951;&#24536;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#35299;&#20915;&#20102;&#25345;&#20037;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#25913;&#36827;&#20102;&#36817;&#20284;&#36951;&#24536;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30417;&#31649;&#35201;&#27714;&#21644;&#27880;&#37325;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#24403;&#21069;&#30340;&#36235;&#21183;&#24378;&#35843;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#24536;&#35760;&#26679;&#26412;&#30340;&#34917;&#38598;&#26469;&#36951;&#24536;&#35757;&#32451;&#25968;&#25454;&#30340;&#26420;&#32032;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#35745;&#31639;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25361;&#25112;&#24050;&#32463;&#36890;&#36807;&#19968;&#31995;&#21015;&#23646;&#20110;&#26426;&#22120;&#36951;&#24536;&#33539;&#30068;&#30340;&#25216;&#26415;&#24471;&#21040;&#20102;&#26377;&#25928;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#25345;&#20037;&#24615;&#35745;&#31639;&#25361;&#25112;&#19982;&#26410;&#36951;&#24536;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21644;&#38544;&#31169;&#24615;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#25105;&#20204;&#23558;&#20854;&#24402;&#22240;&#20110;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#35282;&#24230;&#25913;&#36827;&#36817;&#20284;&#36951;&#24536;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#24037;&#20316;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24341;&#20837;&#25968;&#25454;&#38598;&#31934;&#31616;&#20316;&#20026;&#26426;&#22120;&#36951;&#24536;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#31934;&#31616;&#25216;&#26415;&#21644;&#21019;&#26032;&#30340;&#36951;&#24536;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend in data regulation requirements and privacy-preserving machine learning has emphasized the importance of machine unlearning. The naive approach to unlearning training data by retraining over the complement of the forget samples is susceptible to computational challenges. These challenges have been effectively addressed through a collection of techniques falling under the umbrella of machine unlearning. However, there still exists a lack of sufficiency in handling persistent computational challenges in harmony with the utility and privacy of unlearned model. We attribute this to the lack of work on improving the computational complexity of approximate unlearning from the perspective of the training dataset. In this paper, we aim to fill this gap by introducing dataset condensation as an essential component of machine unlearning in the context of image classification. To achieve this goal, we propose new dataset condensation techniques and an innovative unlearning schem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00152</link><description>&lt;p&gt;
&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;: &#20174;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36861;&#27714;&#65292;&#21040;&#24213;&#26159;&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DeNNs&#65289;&#21644;&#20855;&#26377;&#26377;&#38480;&#38544;&#34255;&#23618;&#30340;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;WeNNs&#65289;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21487;&#20197;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21253;&#25324;&#26679;&#26412;&#28857;&#30340;&#25968;&#37327;&#65292;&#31070;&#32463;&#32593;&#32476;&#20869;&#30340;&#21442;&#25968;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26356;&#22810;&#30340;&#21442;&#25968;&#20542;&#21521;&#20110;&#36873;&#25321;WeNNs&#65292;&#32780;&#26356;&#22810;&#30340;&#26679;&#26412;&#28857;&#21644;&#26356;&#39640;&#30340;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#20542;&#21521;&#20110;&#36873;&#25321;DeNNs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;Ritz&#21644;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.17173</link><description>&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#32534;&#30721;&#22120;&#23454;&#29616;&#38646;-shot&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reinforcement Learning via Function Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#38590;&#28857;&#22312;&#20110;&#23547;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#34920;&#31034;&#26469;&#34920;&#36798;&#24403;&#21069;&#20219;&#21153;&#65292;&#20197;&#20415;&#20195;&#29702;&#31243;&#24207;&#29702;&#35299;&#23427;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#22870;&#21169;&#20989;&#25968;&#25110;&#36716;&#31227;&#20989;&#25968;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#19968;&#20010;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#26377;&#20851;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#36801;&#31227;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;RL&#31639;&#27861;&#19982;&#20989;&#25968;&#32534;&#30721;&#22120;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;RL&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#30340;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.15502</link><description>&lt;p&gt;
&#36873;&#21462;&#23436;&#20840;&#38543;&#26426;&#30340;&#20114;&#34917;&#26631;&#31614;&#26159;&#22810;&#31867;&#21035;&#20998;&#31867;&#30340;&#23454;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The Selected-completely-at-random Complementary Label is a Practical Weak Supervision for Multi-class Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#30340;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#65292;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#20197;&#21450;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#26159;&#19968;&#20010;&#24369;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#20851;&#32852;&#30528;&#19968;&#20010;&#25110;&#22810;&#20010;&#20114;&#34917;&#26631;&#31614;&#65292;&#25351;&#31034;&#20854;&#19981;&#23646;&#20110;&#30340;&#31867;&#21035;&#12290;&#29616;&#26377;&#30340;&#19968;&#33268;&#26041;&#27861;&#20381;&#36182;&#20110;&#22343;&#21248;&#20998;&#24067;&#20551;&#35774;&#26469;&#27169;&#25311;&#20114;&#34917;&#26631;&#31614;&#30340;&#29983;&#25104;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#19968;&#20010;&#26222;&#36890;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#26469;&#20272;&#35745;&#38750;&#22343;&#21248;&#24773;&#20917;&#19979;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#36825;&#20004;&#20010;&#26465;&#20214;&#21487;&#33021;&#19981;&#20250;&#34987;&#28385;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#36825;&#20123;&#26465;&#20214;&#12290;&#21463;&#21040;PU&#23398;&#20064;&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23436;&#20840;&#38543;&#26426;&#36873;&#25321;&#20551;&#35774;&#30340;&#26080;&#20559;&#39118;&#38505;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39118;&#38505;&#26657;&#27491;&#26041;&#27861;&#26469;&#35299;&#20915;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20114;&#34917;&#26631;&#31614;&#23398;&#20064;&#21487;&#20197;&#34987;&#34920;&#36798;&#20026;&#19968;&#32452;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15502v2 Announce Type: replace  Abstract: Complementary-label learning is a weakly supervised learning problem in which each training example is associated with one or multiple complementary labels indicating the classes to which it does not belong. Existing consistent approaches have relied on the uniform distribution assumption to model the generation of complementary labels, or on an ordinary-label training set to estimate the transition matrix in non-uniform cases. However, either condition may not be satisfied in real-world scenarios. In this paper, we propose a novel consistent approach that does not rely on these conditions. Inspired by the positive-unlabeled (PU) learning literature, we propose an unbiased risk estimator based on the Selected Completely At Random assumption for complementary-label learning. We then introduce a risk-correction approach to address overfitting problems. Furthermore, we find that complementary-label learning can be expressed as a set of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Tabsyn&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;Tabsyn&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#36895;&#24230;&#31561;&#20851;&#38190;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#32479;&#19968;&#31354;&#38388;&#24182;&#25429;&#25417;&#21015;&#38388;&#20851;&#31995;&#65292;&#20248;&#21270;&#28508;&#22312;&#23884;&#20837;&#30340;&#20998;&#24067;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#29983;&#25104;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>https://arxiv.org/abs/2310.09656</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#28151;&#21512;&#31867;&#22411;&#30340;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Tabsyn&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;Tabsyn&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#36895;&#24230;&#31561;&#20851;&#38190;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#32479;&#19968;&#31354;&#38388;&#24182;&#25429;&#25417;&#21015;&#38388;&#20851;&#31995;&#65292;&#20248;&#21270;&#28508;&#22312;&#23884;&#20837;&#30340;&#20998;&#24067;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#29983;&#25104;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#39046;&#22495;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#22810;&#26679;&#30340;&#20998;&#24067;&#21644;&#34701;&#21512;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#34920;&#26684;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Tabsyn&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;Tabsyn&#30340;&#20027;&#35201;&#20248;&#21183;&#21253;&#25324;&#65306;&#65288;1&#65289;&#36890;&#29992;&#24615;&#65306;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21333;&#19968;&#30340;&#32479;&#19968;&#31354;&#38388;&#65292;&#24182;&#26126;&#30830;&#25429;&#25417;&#21015;&#38388;&#20851;&#31995;&#65307;&#65288;2&#65289;&#36136;&#37327;&#65306;&#36890;&#36807;&#20248;&#21270;&#28508;&#22312;&#23884;&#20837;&#30340;&#20998;&#24067;&#20197;&#22686;&#24378;&#21518;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65307;&#65288;3&#65289;&#36895;&#24230;&#65306;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#21453;&#21521;&#27493;&#39588;&#21644;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;&#23545;&#20110;&#20845;&#20010;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Tabsyn&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces Tabsyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms exist
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.02129</link><description>&lt;p&gt;
&#25581;&#31034;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Pitfalls of Knowledge Editing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#32534;&#36753;&#30340;&#28508;&#22312;&#38519;&#38449;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;&#30693;&#35782;&#20914;&#31361;&#21644;&#30693;&#35782;&#25197;&#26354;&#26159;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#24050;&#32463;&#36716;&#21521;&#24320;&#21457;&#32534;&#36753;LLMs&#20869;&#22312;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20173;&#26377;&#19968;&#20010;&#38452;&#20113;&#24748;&#22312;&#22836;&#39030;&#19978; - &#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#35302;&#21457;&#34676;&#34678;&#25928;&#24212;&#65311;&#22240;&#20026;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#30693;&#35782;&#32534;&#36753;&#26159;&#21542;&#20250;&#24341;&#20837;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#30340;&#21103;&#20316;&#29992;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#20102;&#19982;LLMs&#30693;&#35782;&#32534;&#36753;&#30456;&#20851;&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#30693;&#35782;&#20914;&#31361;&#65306;&#32534;&#36753;&#36923;&#36753;&#20914;&#31361;&#30340;&#20107;&#23454;&#32452;&#21487;&#33021;&#20250;&#25918;&#22823;LLMs&#22266;&#26377;&#30340;&#19981;&#19968;&#33268;&#24615; - &#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#24573;&#30053;&#30340;&#19968;&#20010;&#26041;&#38754;&#12290;&#65288;2&#65289;&#30693;&#35782;&#25197;&#26354;&#65306;&#20026;&#20102;&#32534;&#36753;&#20107;&#23454;&#30693;&#35782;&#32780;&#26356;&#25913;&#21442;&#25968;&#21487;&#33021;&#20250;&#19981;&#21487;&#36870;&#22320;&#25197;&#26354;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
&lt;/p&gt;</description></item><item><title>OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;</title><link>http://arxiv.org/abs/2401.16445</link><description>&lt;p&gt;
OMPGPT: &#19968;&#31181;&#29992;&#20110;OpenMP&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OMPGPT: A Generative Pre-trained Transformer Model for OpenMP. (arXiv:2401.16445v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16445
&lt;/p&gt;
&lt;p&gt;
OMPGPT&#26159;&#19968;&#31181;&#20026;&#20102;OpenMP pragma&#29983;&#25104;&#32780;&#35774;&#35745;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#31574;&#30053;chain-of-OMP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#31561;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#38543;&#30528;&#36825;&#19968;&#36235;&#21183;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;StarCoder&#12289;WizardCoder&#21644;CodeLlama&#31561;&#65292;&#24050;&#32463;&#28044;&#29616;&#20986;&#26469;&#65292;&#22312;&#22823;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#22266;&#26377;&#30340;&#21407;&#22240;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#23436;&#25104;&#21644;&#27880;&#37322;&#29983;&#25104;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#20197;&#21450;&#23545;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#19968;&#33324;&#25903;&#25345;&#12290;&#34429;&#28982;&#20195;&#30721;LLMs&#30340;&#36890;&#29992;&#33021;&#21147;&#23545;&#35768;&#22810;&#31243;&#24207;&#21592;&#26469;&#35828;&#24456;&#26377;&#29992;&#65292;&#20294;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20855;&#26377;&#26356;&#31364;&#30340;&#38656;&#27714;&#38598;&#65292;&#20351;&#24471;&#26356;&#23567;&#12289;&#26356;&#20855;&#39046;&#22495;&#29305;&#23450;&#30340;LM&#25104;&#20026;&#19968;&#20010;&#26356;&#26126;&#26234;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OMPGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#31934;&#24515;&#35774;&#35745;&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;OpenMP pragma&#29983;&#25104;&#26041;&#38754;&#30340;&#22266;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#24182;&#25913;&#36827;&#20102;&#26469;&#33258;NLP&#39046;&#22495;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#38142;&#24335;OMP&#65288;chain-of-OMP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), as epitomized by models like ChatGPT, have revolutionized the field of natural language processing (NLP). Along with this trend, code-based large language models such as StarCoder, WizardCoder, and CodeLlama have emerged, trained extensively on vast repositories of code data. Yet, inherent in their design, these models primarily focus on generative tasks like code generation, code completion, and comment generation, and general support for multiple programming languages. While the generic abilities of code LLMs are useful for many programmers, the area of high-performance computing (HPC) has a narrower set of requirements that make a smaller and more domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel model meticulously designed to harness the inherent strengths of language models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt engineering techniques from the NLP domain to create chain-of-OMP, an innovative strat
&lt;/p&gt;</description></item><item><title>CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2401.14109</link><description>&lt;p&gt;
CompactifAI: &#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14109
&lt;/p&gt;
&lt;p&gt;
CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;LlaMA&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22914;&#24040;&#22823;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#12289;&#36739;&#22823;&#30340;&#33021;&#28304;&#38656;&#27714;&#20197;&#21450;&#29616;&#22330;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#22914;&#21098;&#26525;&#12289;&#33976;&#39311;&#21644;&#20302;&#31209;&#36924;&#36817;&#20027;&#35201;&#20851;&#27880;&#20943;&#23569;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#26377;&#25928;&#25968;&#37327;&#65292;&#32780;&#37327;&#21270;&#26041;&#27861;&#21017;&#20391;&#37325;&#20110;&#38477;&#20302;&#21333;&#20010;&#26435;&#37325;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#31070;&#32463;&#20803;&#25968;&#30446;&#19981;&#21464;&#12290;&#34429;&#28982;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#27809;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#35748;&#20026;&#25130;&#26029;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#26159;&#19968;&#31181;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#21387;&#32553;&#26041;&#27861;CompactifAI&#65292;&#23427;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03233</link><description>&lt;p&gt;
&#22522;&#20110;Split Learning&#30340;&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning (SL)&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#22522;&#20110;&#32908;&#30005;&#30340;&#20551;&#32930;&#25511;&#21046;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SL&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20551;&#32930;&#35774;&#22791;&#22312;&#22788;&#29702;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;SL&#30340;&#21487;&#34892;&#24615;&#28304;&#20110;&#20854;&#22266;&#26377;&#30340;&#27169;&#22411;&#20998;&#21106;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#25191;&#34892;&#36739;&#23567;&#30340;&#27169;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#19981;&#24688;&#24403;&#30340;&#20999;&#23618;&#20250;&#38459;&#30861;SL&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25913;&#21892;&#20551;&#32930;&#25511;&#21046;&#30340;&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.01783</link><description>&lt;p&gt;
&#20351;&#29992;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#30340;&#36229;&#27874;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22312;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#20013;&#36816;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#26367;&#25442;&#25968;&#20540;&#36890;&#37327;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#29992;&#20110;&#25968;&#20540;&#35299;PDE&#65292;&#26368;&#36817;&#21457;&#23637;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22914;PINN&#21644;&#31070;&#32463;&#31639;&#23376;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#32570;&#28857;&#65292;&#26377;&#24456;&#22810;&#31181;&#31867;&#22411;&#30340;&#30740;&#31350;&#23558;&#20256;&#32479;&#30340;&#25968;&#20540;&#26041;&#26696;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#29992;&#31070;&#32463;&#32593;&#32476;&#26367;&#20195;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#19968;&#23567;&#37096;&#20998;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36229;&#22768;&#21476;&#20856;&#23432;&#24658;&#23450;&#24459;&#65292;&#23558;&#25968;&#20540;&#26041;&#26696;&#20013;&#30340;&#25968;&#20540;&#36890;&#37327;&#26367;&#25442;&#20026;&#31070;&#32463;&#31639;&#23376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#36896;&#20102;&#21463;&#25968;&#20540;&#26041;&#26696;&#21551;&#21457;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;FNO&#36924;&#36817;&#25968;&#20540;&#36890;&#37327;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#19982;&#21407;&#22987;&#26041;&#27861;&#30340;&#27604;&#36739;&#20855;&#26377;&#25968;&#20540;&#26041;&#26696;&#21644;FNO&#30340;&#20248;&#21183;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#21644;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#23545;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.02828</link><description>&lt;p&gt;
&#38543;&#26426;&#36924;&#36817;&#30340;&#25910;&#25947;&#36895;&#24230;&#65306;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Convergence Rates for Stochastic Approximation: Biased Noise with Unbounded Variance, and Applications. (arXiv:2312.02828v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24102;&#26377;&#26080;&#30028;&#26041;&#24046;&#30340;&#26377;&#20559;&#22122;&#22768;&#23545;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#20171;&#32461;&#20102;&#35813;&#31639;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
1951&#24180;&#32599;&#23486;&#26031;&#21644;&#33707;&#27931;&#24341;&#20837;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#31639;&#27861;&#24050;&#32463;&#25104;&#20026;&#35299;&#26041;&#31243;$\mathbf{f}({\boldsymbol{\theta}}) = \mathbf{0}$&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#24403;&#21482;&#26377;$\mathbf{f}(\cdot)$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#21487;&#29992;&#26102;&#12290;&#22914;&#26524;&#23545;&#20110;&#26576;&#20010;&#20989;&#25968;$J(\cdot)$&#65292;$\mathbf{f}({\boldsymbol{\theta}}) = \nabla J({\boldsymbol{\theta}})$&#65292;&#37027;&#20040;SA&#20063;&#21487;&#20197;&#29992;&#26469;&#23547;&#25214;$J(\cdot)$&#30340;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;$t$&#65292;&#24403;&#21069;&#30340;&#29468;&#27979;${\boldsymbol{\theta}}_t$&#36890;&#36807;&#24418;&#24335;&#20026;$\mathbf{f}({\boldsymbol{\theta}}_t) + {\boldsymbol{\xi}}_{t+1}$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#26356;&#26032;&#20026;${\boldsymbol{\theta}}_{t+1}$&#12290;&#22312;&#35768;&#22810;&#25991;&#29486;&#20013;&#65292;&#20551;&#35774;&#35823;&#24046;&#39033;${\boldsymbol{\xi}}_{t+1}$&#30340;&#26465;&#20214;&#22343;&#20540;&#20026;&#38646;&#65292;&#21644;/&#25110;&#32773;&#23427;&#30340;&#26465;&#20214;&#26041;&#24046;&#38543;$t$&#65288;&#32780;&#19981;&#26159;${\boldsymbol{\theta}}_t$&#65289;&#34987;&#38480;&#21046;&#12290;&#22810;&#24180;&#26469;&#65292;SA&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20854;&#20013;&#19968;&#20010;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Stochastic Approximation (SA) algorithm introduced by Robbins and Monro in 1951 has been a standard method for solving equations of the form $\mathbf{f}({\boldsymbol {\theta}}) = \mathbf{0}$, when only noisy measurements of $\mathbf{f}(\cdot)$ are available. If $\mathbf{f}({\boldsymbol {\theta}}) = \nabla J({\boldsymbol {\theta}})$ for some function $J(\cdot)$, then SA can also be used to find a stationary point of $J(\cdot)$. At each time $t$, the current guess ${\boldsymbol {\theta}}_t$ is updated to ${\boldsymbol {\theta}}_{t+1}$ using a noisy measurement of the form $\mathbf{f}({\boldsymbol {\theta}}_t) + {\boldsymbol {\xi}}_{t+1}$. In much of the literature, it is assumed that the error term ${\boldsymbol {\xi}}_{t+1}$ has zero conditional mean, and/or that its conditional variance is bounded as a function of $t$ (though not necessarily with respect to ${\boldsymbol {\theta}}_t$). Over the years, SA has been applied to a variety of areas, out of which the focus in this paper i
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.08644</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#24314;&#27169;&#20013;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Mass-Conserving-Perceptron for Machine Learning-Based Modeling of Geoscientific Systems. (arXiv:2310.08644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120;&#65288;MCP&#65289;&#29992;&#20110;&#23558;&#29289;&#29702;-&#27010;&#24565;&#27169;&#22411;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#24314;&#27169;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#21644;&#36136;&#37327;&#20445;&#25345;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25968;&#21313;&#24180;&#26469;&#33268;&#21147;&#20110;&#26500;&#24314;&#29992;&#20110;&#39044;&#27979;&#22320;&#29699;&#31185;&#23398;&#31995;&#32479;&#26102;&#38388;&#24207;&#21015;&#28436;&#21270;&#30340;&#29289;&#29702;-&#27010;&#24565; (PC) &#27169;&#22411;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064; (ML) &#30340;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;ML&#22522;&#30784;&#27169;&#22411;&#20013;&#25552;&#21462;&#29289;&#29702;&#29702;&#35299;&#30340;&#22256;&#38590;&#20351;&#24471;&#20854;&#22312;&#22686;&#24378;&#23545;&#31995;&#32479;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#31185;&#23398;&#30693;&#35782;&#26041;&#38754;&#30340;&#24212;&#29992;&#21464;&#24471;&#22797;&#26434;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35299;&#29289;&#29702;&#24615;&#30340;&#36136;&#37327;&#20445;&#25345;&#24863;&#30693;&#22120; (MCP) &#20316;&#20026;&#24357;&#21512;PC&#27169;&#22411;&#21644;ML&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;MCP&#21033;&#29992;PC&#27169;&#22411;&#21644;GRNNs&#32972;&#21518;&#30340;&#26377;&#21521;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#21516;&#26500;&#24615;&#65292;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#26126;&#30830;&#34920;&#31034;&#29289;&#29702;&#36807;&#31243;&#30340;&#36136;&#37327;&#20445;&#25345;&#24615;&#36136;&#65292;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#21644;&#29616;&#25104;&#30340;ML&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#36825;&#31181;&#36807;&#31243;&#30340;&#21151;&#33021;&#24615;&#65288;&#21487;&#35299;&#37322;&#24615;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although decades of effort have been devoted to building Physical-Conceptual (PC) models for predicting the time-series evolution of geoscientific systems, recent work shows that Machine Learning (ML) based Gated Recurrent Neural Network technology can be used to develop models that are much more accurate. However, the difficulty of extracting physical understanding from ML-based models complicates their utility for enhancing scientific knowledge regarding system structure and function. Here, we propose a physically-interpretable Mass Conserving Perceptron (MCP) as a way to bridge the gap between PC-based and ML-based modeling approaches. The MCP exploits the inherent isomorphism between the directed graph structures underlying both PC models and GRNNs to explicitly represent the mass-conserving nature of physical processes while enabling the functional nature of such processes to be directly learned (in an interpretable manner) from available data using off-the-shelf ML technology. As
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.07805</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20301;&#38543;&#26426;&#26725;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#30456;&#20301;&#31354;&#38388;&#20013;&#26500;&#24314;&#36335;&#24452;&#27979;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#26159;&#29992;&#20110;&#36830;&#32493;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;DMs&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#65288;&#21363;&#20301;&#32622;&#31354;&#38388;&#65289;&#20013;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21453;&#28436;&#26469;&#24037;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20301;&#31354;&#38388;&#21160;&#21147;&#23398;&#30340;&#26032;&#22411;&#29983;&#25104;&#24314;&#27169;&#26694;&#26550;&#65292;&#20854;&#20013;&#30456;&#20301;&#31354;&#38388;&#34987;&#23450;&#20041;&#20026;&#19968;&#20010;&#21253;&#25324;&#20301;&#32622;&#21644;&#36895;&#24230;&#30340;&#22686;&#24378;&#31354;&#38388;&#12290;&#21033;&#29992;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#30456;&#20301;&#31354;&#38388;&#20013;&#30340;&#36335;&#24452;&#27979;&#24230;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#37319;&#26679;&#12290;&#19982;DMs&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21160;&#21147;&#20256;&#25773;&#30340;&#26089;&#26399;&#38454;&#27573;&#23601;&#33021;&#22815;&#29983;&#25104;&#36924;&#30495;&#30340;&#25968;&#25454;&#28857;&#12290;&#36825;&#31181;&#26089;&#26399;&#39044;&#27979;&#20026;&#36890;&#36807;&#27839;&#36712;&#36857;&#21033;&#29992;&#39069;&#22806;&#30340;&#36895;&#24230;&#20449;&#24687;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#26631;&#20934;&#22270;&#20687;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#23567;&#20989;&#25968;&#35780;&#20272;&#25968;&#37327;&#30340;&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.03031</link><description>&lt;p&gt;
ChatGPT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#26377;&#22810;&#26222;&#36941;&#65311;&#8212;&#8212; &#25506;&#32034;&#24503;&#35821;&#21644;&#33521;&#35821;ChatGPT&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#20998;&#26512;&#24182;&#25506;&#32034;&#20102;&#24503;&#35821;&#21644;&#33521;&#35821;&#30340;ChatGPT&#22238;&#24212;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#23545;&#31995;&#32479;&#22810;&#27425;&#25552;&#20379;&#30456;&#21516;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#24212;&#23384;&#22312;&#24046;&#24322;&#12290;&#20351;&#29992;ChatGPT&#26469;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#24037;&#20316;&#25991;&#26412;&#38750;&#24120;&#26377;&#29992;&#65292;&#20294;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#31995;&#32479;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#30340;&#25512;&#20986;&#65292;OpenAI&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20379;&#20855;&#26377;&#26377;&#38480;IT&#19987;&#19994;&#30693;&#35782;&#30340;&#29992;&#25143;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#32972;&#26223;&#30340;&#29992;&#25143;&#21487;&#33021;&#32570;&#20047;&#23545;LLM&#30340;&#36866;&#24403;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#32570;&#20047;&#23545;&#20854;&#22266;&#26377;&#38480;&#21046;&#30340;&#24847;&#35782;&#65292;&#23558;&#25509;&#21463;&#31995;&#32479;&#36755;&#20986;&#30340;&#34920;&#38754;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#36755;&#20837;&#25552;&#31034;&#21644;&#29983;&#25104;&#30340;&#22238;&#24212;&#65292;&#20197;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#29992;&#25143;&#22312;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;ChatGPT&#22312;&#33521;&#35821;&#21644;&#24503;&#35821;&#20013;&#30340;&#21453;&#24212;&#65292;&#24182;&#25552;&#20379;&#20102;&#22899;&#24615;&#12289;&#30007;&#24615;&#25110;&#20013;&#31435;&#35282;&#24230;&#30340;&#25351;&#20196;&#26102;&#65292;&#22238;&#22797;&#30340;&#26159;&#21542;&#26377;&#24046;&#24322;&#12290;&#36890;&#36807;&#28145;&#20837;&#35843;&#26597;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36873;&#25321;&#30340;&#25552;&#31034;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#22312;&#30456;&#21516;&#26041;&#24335;&#19979;&#22810;&#27425;&#25552;&#20379;&#25351;&#20196;&#26102;&#22238;&#24212;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#24110;&#21161;&#38750;IT&#29992;&#25143;&#25776;&#20889;&#26085;&#24120;&#24037;&#20316;&#25991;&#26412;&#65292;ChatGPT&#30830;&#23454;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#28982;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#35201;&#24847;&#35782;&#21040;&#65292;&#24403;&#22788;&#29702;&#31995;&#32479;&#36755;&#20986;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#20805;&#20998;&#32771;&#34385;&#21040;&#20854;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman VAEs&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;GANs&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#35889;&#24037;&#20855;&#23545;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#20540;&#26045;&#21152;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#23545;&#23450;&#24615;&#34892;&#20026;&#21644;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.02619</link><description>&lt;p&gt;
&#36890;&#36807;Koopman VAEs&#29983;&#25104;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Regular and Irregular Time Series Data via Koopman VAEs. (arXiv:2310.02619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Koopman VAEs&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;GANs&#35757;&#32451;&#19981;&#31283;&#23450;&#21644;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#35889;&#24037;&#20855;&#23545;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#20540;&#26045;&#21152;&#32422;&#26463;&#65292;&#23454;&#29616;&#20102;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#23545;&#23450;&#24615;&#34892;&#20026;&#21644;&#31283;&#23450;&#24615;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30495;&#23454;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#20110;&#35768;&#22810;&#24037;&#31243;&#21644;&#31185;&#23398;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;GANs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24120;&#24120;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#21487;&#33021;&#20986;&#29616;&#27169;&#24335;&#23849;&#28291;&#30340;&#38382;&#39064;&#12290;&#32780;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#34987;&#35748;&#20026;&#23545;&#36825;&#20123;&#38382;&#39064;&#26356;&#20855;&#40065;&#26834;&#24615;&#65292;&#20294;&#21364;&#24456;&#23569;&#34987;&#32771;&#34385;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26032;&#22411;&#27169;&#22411;&#20808;&#39564;&#30340;&#29983;&#25104;&#26694;&#26550;Koopman VAE&#65288;KVAE&#65289;&#65292;&#21487;&#20197;&#20026;&#35268;&#21017;&#21644;&#38750;&#35268;&#21017;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#21463;Koopman&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#26144;&#23556;&#26469;&#34920;&#31034;&#28508;&#22312;&#26465;&#20214;&#20808;&#39564;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29983;&#25104;&#24314;&#27169;&#30340;&#20004;&#20010;&#26399;&#26395;&#29305;&#24615;&#65306;&#65288;i&#65289;&#36890;&#36807;&#21033;&#29992;&#35889;&#24037;&#20855;&#23545;&#32447;&#24615;&#26144;&#23556;&#30340;&#29305;&#24449;&#20540;&#26045;&#21152;&#32422;&#26463;&#65292;&#21487;&#20197;&#23454;&#29616;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#65307;&#65288;ii&#65289;&#30740;&#31350;&#23450;&#24615;&#34892;&#20026;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating realistic time series data is important for many engineering and scientific applications. Existing work tackles this problem using generative adversarial networks (GANs). However, GANs are often unstable during training, and they can suffer from mode collapse. While variational autoencoders (VAEs) are known to be more robust to these issues, they are (surprisingly) less often considered for time series generation. In this work, we introduce Koopman VAE (KVAE), a new generative framework that is based on a novel design for the model prior, and that can be optimized for either regular and irregular training data. Inspired by Koopman theory, we represent the latent conditional prior dynamics using a linear map. Our approach enhances generative modeling with two desired features: (i) incorporating domain knowledge can be achieved by leverageing spectral tools that prescribe constraints on the eigenvalues of the linear map; and (ii) studying the qualitative behavior and stablity 
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>G4SATBench&#26159;&#31532;&#19968;&#20010;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;SAT&#27714;&#35299;&#22120;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;SAT&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#31639;&#27861;&#23545;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#36890;&#36807;&#19982;&#25628;&#32034;&#22411;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#36824;&#25581;&#31034;&#20102;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.16941</link><description>&lt;p&gt;
G4SATBench: &#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#35780;&#20272;&#21644;&#25913;&#36827;SAT&#27714;&#35299;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks. (arXiv:2309.16941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16941
&lt;/p&gt;
&lt;p&gt;
G4SATBench&#26159;&#31532;&#19968;&#20010;&#20026;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;SAT&#27714;&#35299;&#22120;&#25552;&#20379;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#32780;&#22810;&#26679;&#30340;SAT&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#31639;&#27861;&#23545;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;&#36890;&#36807;&#19982;&#25628;&#32034;&#22411;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#23427;&#36824;&#25581;&#31034;&#20102;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#20854;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#20316;&#20026;&#35299;&#20915;&#24067;&#23572;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;(SAT)&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#20986;&#29616;&#20102;&#65292;&#20026;&#20256;&#32479;&#30340;&#22238;&#28335;&#25110;&#23616;&#37096;&#25628;&#32034;SAT&#27714;&#35299;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#30340;&#25991;&#29486;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#25968;&#25454;&#38598;&#21644;&#20844;&#27491;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#21644;&#27604;&#36739;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;G4SATBench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#24314;&#31435;&#20840;&#38754;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;&#22312;G4SATBench&#20013;&#65292;&#25105;&#20204;&#31934;&#24515;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;7&#20010;&#38382;&#39064;&#21644;3&#20010;&#38590;&#24230;&#32423;&#21035;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;SAT&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#12289;&#35757;&#32451;&#30446;&#26631;&#21644;&#25512;&#29702;&#31639;&#27861;&#19979;&#23545;&#24191;&#27867;&#30340;GNN&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#20102;&#25506;&#32034;&#22522;&#20110;GNN&#30340;SAT&#27714;&#35299;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#29702;&#35299;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#36824;&#23558;&#23427;&#20204;&#30340;&#27714;&#35299;&#36807;&#31243;&#19982;&#22522;&#20110;&#25628;&#32034;&#30340;SAT&#27714;&#35299;&#22120;&#20013;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers
&lt;/p&gt;</description></item><item><title>&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#26159;&#21033;&#29992;&#21487;&#36716;&#31227;&#24615;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#36719;&#27979;&#37327;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#26469;&#22686;&#24378;&#36719;&#27979;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#20010;&#23454;&#29616;&#30340;&#36827;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15828</link><description>&lt;p&gt;
&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#20801;&#35768;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-unit soft sensing permits few-shot learning. (arXiv:2309.15828v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15828
&lt;/p&gt;
&lt;p&gt;
&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#26159;&#21033;&#29992;&#21487;&#36716;&#31227;&#24615;&#23398;&#20064;&#31639;&#27861;&#25913;&#36827;&#36719;&#27979;&#37327;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#26469;&#22686;&#24378;&#36719;&#27979;&#37327;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#36866;&#29992;&#20110;&#20855;&#26377;&#22810;&#20010;&#23454;&#29616;&#30340;&#36827;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#20855;&#26377;&#21487;&#36716;&#31227;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;&#36719;&#27979;&#37327;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#24403;&#19968;&#20010;&#36719;&#27979;&#37327;&#36890;&#36807;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#26469;&#23398;&#20064;&#26102;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#21152;&#24378;&#12290;&#21487;&#36716;&#31227;&#24615;&#30340;&#26377;&#29992;&#24615;&#21462;&#20915;&#20110;&#25152;&#35774;&#35745;&#30340;&#23398;&#20064;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#36719;&#27979;&#37327;&#35201;&#24212;&#29992;&#20110;&#26377;&#22810;&#20010;&#23454;&#29616;&#30340;&#36827;&#31243;&#65288;&#20363;&#22914;&#65292;&#26377;&#22810;&#20010;&#21487;&#29992;&#25968;&#25454;&#30340;&#31995;&#32479;&#25110;&#35774;&#22791;&#65289;&#26102;&#65292;&#23588;&#20854;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23454;&#29616;&#37117;&#25552;&#20379;&#19968;&#20010;&#36719;&#27979;&#37327;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#19988;&#21512;&#29702;&#22320;&#26399;&#26395;&#36825;&#20123;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#24212;&#29992;&#21487;&#36716;&#31227;&#24615;&#23548;&#33268;&#20102;&#25105;&#20204;&#25152;&#31216;&#30340;&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#65292;&#20854;&#20013;&#36719;&#27979;&#37327;&#36890;&#36807;&#20174;&#25152;&#26377;&#23454;&#29616;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#26469;&#24314;&#27169;&#19968;&#20010;&#36827;&#31243;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#21333;&#20803;&#36719;&#27979;&#37327;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#23427;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#20998;&#23618;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;...
&lt;/p&gt;
&lt;p&gt;
Recent literature has explored various ways to improve soft sensors using learning algorithms with transferability. Broadly put, the performance of a soft sensor may be strengthened when it is learned by solving multiple tasks. The usefulness of transferability depends on how strongly related the devised learning tasks are. A particularly relevant case for transferability, is when a soft sensor is to be developed for a process of which there are many realizations, e.g. system or device with many implementations from which data is available. Then, each realization presents a soft sensor learning task, and it is reasonable to expect that the different tasks are strongly related. Applying transferability in this setting leads to what we call multi-unit soft sensing, where a soft sensor models a process by learning from data from all of its realizations.  This paper explores the learning abilities of a multi-unit soft sensor, which is formulated as a hierarchical model and implemented usin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2309.15366</link><description>&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#65306;&#29983;&#29289;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Density Estimation via Measure Transport: Outlook for Applications in the Biological Sciences. (arXiv:2309.15366v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15366
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#36827;&#34892;&#23494;&#24230;&#20272;&#35745;&#22312;&#29983;&#29289;&#31185;&#23398;&#20013;&#20855;&#26377;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#24230;&#20256;&#36882;&#26041;&#27861;&#30340;&#19968;&#20010;&#20248;&#21183;&#26159;&#20854;&#20801;&#35768;&#23545;&#26681;&#25454;&#24191;&#27867;&#27010;&#29575;&#27979;&#24230;&#20998;&#24067;&#30340;&#25968;&#25454;&#36827;&#34892;&#32479;&#19968;&#30340;&#22788;&#29702;&#21644;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#27979;&#24230;&#20256;&#36882;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#19977;&#35282;&#20256;&#36882;&#26144;&#23556;&#30340;&#20351;&#29992;&#65292;&#20316;&#20026;&#25903;&#25345;&#29983;&#29289;&#31185;&#23398;&#30740;&#31350;&#30340;&#24037;&#20316;&#27969;&#30340;&#19968;&#37096;&#20998;&#12290;&#31232;&#30095;&#25968;&#25454;&#22330;&#26223;&#22312;&#36752;&#23556;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#31232;&#30095;&#20256;&#36882;&#26144;&#23556;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35745;&#31639;&#19968;&#31995;&#21015;&#65288;&#31232;&#30095;&#30340;&#65289;&#33258;&#36866;&#24212;&#20256;&#36882;&#26144;&#23556;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#36825;&#20123;&#26144;&#23556;&#26159;&#22312;&#38543;&#26426;&#36873;&#25321;&#30340;&#19968;&#31995;&#21015;&#21487;&#29992;&#25968;&#25454;&#26679;&#26412;&#23376;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#32771;&#34385;&#30340;&#36752;&#23556;&#29983;&#29289;&#23398;&#24212;&#29992;&#20013;&#65292;&#27492;&#26041;&#27861;&#20026;&#29983;&#25104;&#20551;&#35774;&#25552;&#20379;&#20102;&#19968;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
One among several advantages of measure transport methods is that they allow for a unified framework for processing and analysis of data distributed according to a wide class of probability measures. Within this context, we present results from computational studies aimed at assessing the potential of measure transport techniques, specifically, the use of triangular transport maps, as part of a workflow intended to support research in the biological sciences. Scarce data scenarios, which are common in domains such as radiation biology, are of particular interest. We find that when data is scarce, sparse transport maps are advantageous. In particular, statistics gathered from computing series of (sparse) adaptive transport maps, trained on a series of randomly chosen subsets of the set of available data samples, leads to uncovering information hidden in the data. As a result, in the radiation biology application considered here, this approach provides a tool for generating hypotheses ab
&lt;/p&gt;</description></item><item><title>MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.14236</link><description>&lt;p&gt;
MoDem-V2: &#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#35270;&#35273;-&#36816;&#21160;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation. (arXiv:2309.14236v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14236
&lt;/p&gt;
&lt;p&gt;
MoDem-V2&#26159;&#19968;&#20010;&#33021;&#22815;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#30452;&#25509;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#25805;&#20316;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#26426;&#36733;&#20256;&#24863;&#22120;&#30452;&#25509;&#24863;&#30693;&#19990;&#30028;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#23398;&#20064;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#21407;&#22987;&#20687;&#32032;&#30340;&#38544;&#24335;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#28040;&#38500;&#29615;&#22659;&#35013;&#32622;&#30340;&#38656;&#27714;&#65292;&#20294;&#20165;&#20165;&#20381;&#38752;&#31232;&#30095;&#30340;&#35270;&#35273;&#22870;&#21169;&#20449;&#21495;&#22312;&#25509;&#35302;&#20016;&#23500;&#30340;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#26174;&#33879;&#21152;&#21095;&#20102;&#25506;&#32034;&#30340;&#38590;&#24230;&#12290;&#22240;&#27492;&#65292;&#36825;&#31181;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#36890;&#24120;&#23616;&#38480;&#20110;&#27169;&#25311;&#25110;&#20005;&#26684;&#24037;&#31243;&#21270;&#30340;&#29615;&#22659;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#26126;&#30830;&#30340;&#29366;&#24577;&#20272;&#35745;&#21644;&#31264;&#23494;&#22870;&#21169;&#30340;&#25351;&#23548;&#19979;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20195;&#29702;&#30340;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#19981;&#23433;&#20840;&#34892;&#20026;&#21644;&#37325;&#22823;&#23433;&#20840;&#25925;&#38556;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#31163;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;MoDem-V2&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#30452;&#25509;&#22312;&#38750;&#20202;&#22120;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#20013;&#23398;&#20064;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#12290;&#22312;&#26368;&#26032;&#30340;&#31639;&#27861;&#36827;&#23637;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#65292;
&lt;/p&gt;
&lt;p&gt;
Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration. The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FETSGAN&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21305;&#37197;&#29305;&#24449;&#31354;&#38388;&#21644;&#20302;&#32500;&#24230;&#37319;&#26679;&#31354;&#38388;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#30830;&#20445;&#21512;&#25104;&#26679;&#26412;&#30340;&#26102;&#38388;&#20998;&#24067;&#19981;&#20250;&#23849;&#28291;&#12290;</title><link>http://arxiv.org/abs/2308.15730</link><description>&lt;p&gt;
&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully Embedded Time-Series Generative Adversarial Networks. (arXiv:2308.15730v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15730
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FETSGAN&#65289;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#35757;&#32451;&#21305;&#37197;&#29305;&#24449;&#31354;&#38388;&#21644;&#20302;&#32500;&#24230;&#37319;&#26679;&#31354;&#38388;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#30830;&#20445;&#21512;&#25104;&#26679;&#26412;&#30340;&#26102;&#38388;&#20998;&#24067;&#19981;&#20250;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24212;&#20135;&#29983;&#19982;&#25152;&#24314;&#27169;&#25968;&#25454;&#30340;&#28508;&#22312;&#20998;&#24067;&#30456;&#31526;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#38024;&#23545;&#23454;&#20540;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#21516;&#26102;&#25429;&#33719;&#25968;&#25454;&#30340;&#38745;&#24577;&#20998;&#24067;&#20197;&#21450;&#20219;&#20309;&#28508;&#22312;&#26102;&#38388;&#33539;&#22260;&#30340;&#23436;&#25972;&#26102;&#38388;&#20998;&#24067;&#12290;&#36825;&#20010;&#26102;&#38388;&#35201;&#32032;&#20135;&#29983;&#20102;&#19968;&#20010;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#38480;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#25110;&#26131;&#20110;&#21457;&#29983;&#27169;&#24335;&#23849;&#28291;&#12290;&#22312;&#23436;&#20840;&#23884;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FETSGAN&#65289;&#20013;&#65292;&#25972;&#20010;&#24207;&#21015;&#36890;&#36807;&#19968;&#20010;seq2seq&#39118;&#26684;&#30340;&#23545;&#25239;&#24615;&#33258;&#32534;&#30721;&#22120;&#65288;AAE&#65289;&#30452;&#25509;&#36716;&#25442;&#20026;&#29983;&#25104;&#22120;&#30340;&#37319;&#26679;&#31354;&#38388;&#65292;&#20854;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#29992;&#20110;&#22312;&#29305;&#24449;&#31354;&#38388;&#21644;&#36739;&#20302;&#32500;&#24230;&#30340;&#37319;&#26679;&#31354;&#38388;&#21305;&#37197;&#35757;&#32451;&#20998;&#24067;&#12290;&#36825;&#20010;&#39069;&#22806;&#30340;&#32422;&#26463;&#26465;&#20214;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#26679;&#26412;&#30340;&#26102;&#38388;&#20998;&#24067;&#19981;&#20250;&#23849;&#28291;&#30340;&#26494;&#25955;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#36229;&#36807;&#38408;&#20540;&#65288;FAT&#65289;&#25805;&#20316;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GANs) should produce synthetic data that fits the underlying distribution of the data being modeled. For real valued time-series data, this implies the need to simultaneously capture the static distribution of the data, but also the full temporal distribution of the data for any potential time horizon. This temporal element produces a more complex problem that can potentially leave current solutions under-constrained, unstable during training, or prone to varying degrees of mode collapse. In FETSGAN, entire sequences are translated directly to the generator's sampling space using a seq2seq style adversarial auto encoder (AAE), where adversarial training is used to match the training distribution in both the feature space and the lower dimensional sampling space. This additional constraint provides a loose assurance that the temporal distribution of the synthetic samples will not collapse. In addition, the First Above Threshold (FAT) operator is introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;</title><link>http://arxiv.org/abs/2308.08487</link><description>&lt;p&gt;
&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Temporal Interest Network for Click-Through Rate Prediction. (arXiv:2308.08487v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#65292;&#29992;&#20110;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#20197;&#39044;&#27979;&#28857;&#20987;&#29575;&#30340;&#25928;&#26524;&#21644;&#24050;&#26377;&#26041;&#27861;&#23545;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#23398;&#20064;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#34892;&#20026;&#30340;&#21382;&#21490;&#26159;&#39044;&#27979;&#28857;&#20987;&#29575;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#19982;&#30446;&#26631;&#39033;&#30446;&#20855;&#26377;&#24378;&#28872;&#30340;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;&#34429;&#28982;&#24050;&#26377;&#25991;&#29486;&#20998;&#21035;&#30740;&#31350;&#20102;&#36825;&#20123;&#30456;&#20851;&#24615;&#65292;&#20294;&#23578;&#26410;&#20998;&#26512;&#23427;&#20204;&#30340;&#32452;&#21512;&#65292;&#21363;&#34892;&#20026;&#35821;&#20041;&#12289;&#30446;&#26631;&#35821;&#20041;&#12289;&#34892;&#20026;&#26102;&#38388;&#21644;&#30446;&#26631;&#26102;&#38388;&#30340;&#22235;&#37325;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#30456;&#20851;&#24615;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#23398;&#20064;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#31243;&#24230;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#27979;&#37327;&#20102;&#22235;&#37325;&#30456;&#20851;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#30452;&#35266;&#32780;&#24378;&#22823;&#30340;&#22235;&#37325;&#27169;&#24335;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20960;&#31181;&#20195;&#34920;&#24615;&#30340;&#29992;&#25143;&#34892;&#20026;&#26041;&#27861;&#30340;&#23398;&#20064;&#30456;&#20851;&#24615;&#65292;&#20294;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#37117;&#27809;&#26377;&#23398;&#20064;&#21040;&#36825;&#26679;&#30340;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#26102;&#38388;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26102;&#38388;&#20852;&#36259;&#32593;&#32476;&#65288;TIN&#65289;&#26469;&#25429;&#25417;&#34892;&#20026;&#19982;&#30446;&#26631;&#20043;&#38388;&#30340;&#22235;&#37325;&#35821;&#20041;&#21644;&#26102;&#38388;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of user behaviors constitutes one of the most significant characteristics in predicting the click-through rate (CTR), owing to their strong semantic and temporal correlation with the target item. While the literature has individually examined each of these correlations, research has yet to analyze them in combination, that is, the quadruple correlation of (behavior semantics, target semantics, behavior temporal, and target temporal). The effect of this correlation on performance and the extent to which existing methods learn it remain unknown. To address this gap, we empirically measure the quadruple correlation and observe intuitive yet robust quadruple patterns. We measure the learned correlation of several representative user behavior methods, but to our surprise, none of them learn such a pattern, especially the temporal one.  In this paper, we propose the Temporal Interest Network (TIN) to capture the quadruple semantic and temporal correlation between behaviors and th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.10352</link><description>&lt;p&gt;
&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Properties of Discrete Sliced Wasserstein Losses. (arXiv:2307.10352v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#25955;&#20999;&#21106;Wasserstein&#25439;&#22833;&#30340;&#24615;&#36136;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#20197;&#21450;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#21106;Wasserstein&#65288;SW&#65289;&#36317;&#31163;&#24050;&#25104;&#20026;&#27604;&#36739;&#27010;&#29575;&#27979;&#24230;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#27861;&#12290;&#24191;&#27867;&#24212;&#29992;&#21253;&#25324;&#22270;&#20687;&#22788;&#29702;&#12289;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#29983;&#25104;&#24314;&#27169;&#65292;&#24120;&#24120;&#38656;&#35201;&#20248;&#21270;&#19968;&#20123;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;SW&#65292;&#35813;&#21442;&#25968;&#20805;&#24403;&#31163;&#25955;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#25439;&#22833;&#20989;&#25968;&#65288;&#22240;&#20026;&#20855;&#26377;&#23494;&#24230;&#30340;&#27979;&#24230;&#22312;&#25968;&#20540;&#19978;&#26159;&#26080;&#27861;&#23454;&#29616;&#30340;&#65289;&#12290;&#25152;&#26377;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#37117;&#23384;&#22312;&#30456;&#21516;&#30340;&#23376;&#38382;&#39064;&#65292;&#21363;&#26368;&#23567;&#21270;&#20999;&#21106;Wasserstein&#33021;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;$\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$&#30340;&#23646;&#24615;&#65292;&#21363;&#20004;&#20010;&#20855;&#26377;&#19982;&#19968;&#20010;&#27979;&#24230;&#30340;&#25903;&#25745;&#30456;&#21516;&#25968;&#37327;&#30340;&#31163;&#25955;&#22343;&#21248;&#27979;&#24230;&#20043;&#38388;&#30340;SW&#36317;&#31163;&#20316;&#20026;&#25903;&#25745;$Y \in \mathbb{R}^{n \times d}$&#20989;&#25968;&#30340;&#33021;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#33021;&#37327;&#30340;&#27491;&#21017;&#24615;&#21644;&#20248;&#21270;&#24615;&#36136;&#65292;&#20197;&#21450;&#20854;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#36817;&#20284;$\mathcal{E}_p$&#65288;&#20351;&#29992;SW&#20013;&#30340;&#26399;&#26395;&#20272;&#35745;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sliced Wasserstein (SW) distance has become a popular alternative to the Wasserstein distance for comparing probability measures. Widespread applications include image processing, domain adaptation and generative modelling, where it is common to optimise some parameters in order to minimise SW, which serves as a loss function between discrete probability measures (since measures admitting densities are numerically unattainable). All these optimisation problems bear the same sub-problem, which is minimising the Sliced Wasserstein energy. In this paper we study the properties of $\mathcal{E}: Y \longmapsto \mathrm{SW}_2^2(\gamma_Y, \gamma_Z)$, i.e. the SW distance between two uniform discrete measures with the same amount of points as a function of the support $Y \in \mathbb{R}^{n \times d}$ of one of the measures. We investigate the regularity and optimisation properties of this energy, as well as its Monte-Carlo approximation $\mathcal{E}_p$ (estimating the expectation in SW using 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23384;&#20648;&#36890;&#36947;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#26469;&#22686;&#21152;&#36890;&#36947;&#23481;&#37327;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#40657;&#30418;&#35775;&#38382;&#23454;&#29616;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2307.08811</link><description>&lt;p&gt;
DeepMem: &#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29992;&#20316;&#23384;&#20648;&#36890;&#36947;&#21450;&#20854;&#65288;&#35823;&#29992;&#65289;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DeepMem: ML Models as storage channels and their (mis-)applications. (arXiv:2307.08811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#23384;&#20648;&#36890;&#36947;&#30340;&#26032;&#35270;&#35282;&#65292;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#26469;&#22686;&#21152;&#36890;&#36947;&#23481;&#37327;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#26102;&#23884;&#20837;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#40657;&#30418;&#35775;&#38382;&#23454;&#29616;&#20449;&#24687;&#30340;&#23384;&#20648;&#21644;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20026;&#20102;&#25903;&#25345;&#36890;&#29992;&#24615;&#21644;&#36991;&#20813;&#36807;&#25311;&#21512;&#32780;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#21442;&#25968;&#26082;&#21487;&#20197;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#38544;&#34255;&#19968;&#20010;&#27169;&#22411;&#65289;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#26377;&#30410;&#30446;&#30340;&#65288;&#20363;&#22914;&#65292;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20449;&#24687;&#35770;&#35270;&#35282;&#65307;&#25105;&#20204;&#23558;ML&#27169;&#22411;&#35270;&#20026;&#19968;&#20010;&#23384;&#20648;&#36890;&#36947;&#65292;&#20854;&#23481;&#37327;&#38543;&#30528;&#36807;&#24230;&#21442;&#25968;&#21270;&#32780;&#22686;&#21152;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#21457;&#36865;&#26041;&#65292;&#22312;&#35757;&#32451;&#26102;&#23558;&#20219;&#24847;&#20449;&#24687;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#25509;&#25910;&#26041;&#21487;&#20197;&#36890;&#36807;&#23545;&#37096;&#32626;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#25105;&#20204;&#26681;&#25454;&#21487;&#29992;&#21442;&#25968;&#30340;&#25968;&#37327;&#25512;&#23548;&#20986;&#36890;&#36947;&#23481;&#37327;&#30340;&#19978;&#30028;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#40657;&#30418;&#20889;&#20837;&#21644;&#35835;&#21462;&#21407;&#35821;&#65292;&#20801;&#35768;&#25915;&#20987;&#32773;&#65306;&#65288;i&#65289;&#36890;&#36807;&#22312;&#21457;&#23556;&#26426;&#31471;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#24335;&#20197;&#20248;&#21270;&#22320;&#23558;&#25968;&#25454;&#23384;&#20648;&#22312;&#27169;&#22411;&#20013;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35835;&#21462;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are overparameterized to support generality and avoid overfitting. Prior works have shown that these additional parameters can be used for both malicious (e.g., hiding a model covertly within a trained model) and beneficial purposes (e.g., watermarking a model). In this paper, we propose a novel information theoretic perspective of the problem; we consider the ML model as a storage channel with a capacity that increases with overparameterization. Specifically, we consider a sender that embeds arbitrary information in the model at training time, which can be extracted by a receiver with a black-box access to the deployed model. We derive an upper bound on the capacity of the channel based on the number of available parameters. We then explore black-box write and read primitives that allow the attacker to: (i) store data in an optimized way within the model by augmenting the training data at the transmitter side, and (ii) to read it by querying the model afte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35273;&#27169;&#31946;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;IF-BLS&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35757;&#32451;&#28857;&#20998;&#37197;&#27169;&#31946;&#38582;&#23646;&#24230;&#26469;&#20943;&#23567;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.08713</link><description>&lt;p&gt;
&#30452;&#35273;&#27169;&#31946;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65306;&#22686;&#24378;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Intuitionistic Fuzzy Broad Learning System: Enhancing Robustness Against Noise and Outliers. (arXiv:2307.08713v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35273;&#27169;&#31946;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;IF-BLS&#65289;&#65292;&#29992;&#20110;&#22686;&#24378;&#23545;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#35757;&#32451;&#28857;&#20998;&#37197;&#27169;&#31946;&#38582;&#23646;&#24230;&#26469;&#20943;&#23567;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#31867;&#39046;&#22495;&#65292;&#24191;&#27867;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#23427;&#21033;&#29992;&#36880;&#23618;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#23427;&#21253;&#25324;&#29305;&#24449;&#23398;&#20064;&#21644;&#22686;&#24378;&#37096;&#20998;&#65292;&#20849;&#21516;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#22797;&#26434;&#30340;&#29305;&#24449;&#12290;&#20256;&#32479;&#30340;BLS&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#21516;&#31561;&#37325;&#35201;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#22788;&#29702;&#24102;&#26377;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#26102;&#19981;&#22815;&#31283;&#20581;&#21644;&#26377;&#25928;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#31946;BLS&#65288;F-BLS&#65289;&#27169;&#22411;&#65292;&#23427;&#20026;&#27599;&#20010;&#35757;&#32451;&#28857;&#20998;&#37197;&#27169;&#31946;&#38582;&#23646;&#24230;&#65292;&#20197;&#20943;&#23567;&#22122;&#22768;&#21644;&#24322;&#24120;&#20540;&#30340;&#24433;&#21709;&#12290;&#22312;&#20998;&#37197;&#38582;&#23646;&#24230;&#30340;&#36807;&#31243;&#20013;&#65292;F-BLS&#27169;&#22411;&#20165;&#32771;&#34385;&#26679;&#26412;&#21040;&#31867;&#21035;&#20013;&#24515;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#23646;&#20110;&#31867;&#21035;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#35273;&#27169;&#31946;&#29702;&#35770;&#30340;&#26032;&#22411;BLS&#65288;IF-BLS&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;IF-BLS&#21033;&#29992;&#22522;&#20110;&#27169;&#31946;&#38582;&#23646;&#24230;&#21644;&#38750;&#38582;&#23646;&#24230;&#30340;&#30452;&#35273;&#27169;&#31946;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of data classification, broad learning system (BLS) has proven to be a potent tool that utilizes a layer-by-layer feed-forward neural network. It consists of feature learning and enhancement segments, working together to extract intricate features from input data. The traditional BLS treats all samples as equally significant, which makes it less robust and less effective for real-world datasets with noises and outliers. To address this issue, we propose the fuzzy BLS (F-BLS) model, which assigns a fuzzy membership value to each training point to reduce the influence of noises and outliers. In assigning the membership value, the F-BLS model solely considers the distance from samples to the class center in the original feature space without incorporating the extent of non-belongingness to a class. We further propose a novel BLS based on intuitionistic fuzzy theory (IF-BLS). The proposed IF-BLS utilizes intuitionistic fuzzy numbers based on fuzzy membership and non-membership
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIGEST&#30340;&#24555;&#36895;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#24322;&#27493;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;Gossip&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#24605;&#24819;&#65292;&#24182;&#19987;&#27880;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#25955;&#23398;&#20064;&#20013;&#36739;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07652</link><description>&lt;p&gt;
DIGEST: &#24555;&#36895;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#23398;&#20064;&#19982;&#26412;&#22320;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIGEST&#30340;&#24555;&#36895;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#24322;&#27493;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#32467;&#21512;Gossip&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#24605;&#24819;&#65292;&#24182;&#19987;&#27880;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#65292;&#23454;&#29616;&#20102;&#22312;&#20998;&#25955;&#23398;&#20064;&#20013;&#36739;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#36739;&#24555;&#30340;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#24191;&#27867;&#32771;&#34385;&#30340;&#20998;&#25955;&#23398;&#20064;&#31639;&#27861;&#26159;Gossip&#21644;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#23398;&#20064;&#12290;Gossip&#31639;&#27861;&#65288;&#21516;&#27493;&#21644;&#24322;&#27493;&#29256;&#26412;&#65289;&#23384;&#22312;&#36739;&#39640;&#30340;&#36890;&#20449;&#25104;&#26412;&#65292;&#32780;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#30340;&#23398;&#20064;&#21017;&#20250;&#22686;&#21152;&#25910;&#25947;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#36895;&#21644;&#36890;&#20449;&#26377;&#25928;&#30340;&#24322;&#27493;&#20998;&#25955;&#23398;&#20064;&#26426;&#21046;DIGEST&#65292;&#21033;&#29992;&#20102;Gossip&#21644;&#38543;&#26426;&#28216;&#36208;&#30340;&#24605;&#24819;&#65292;&#24182;&#19987;&#27880;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#12290;DIGEST&#26159;&#19968;&#20010;&#22522;&#20110;&#26412;&#22320;SGD&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#25955;&#31639;&#27861;&#65292;&#23427;&#26368;&#21021;&#26159;&#20026;&#36890;&#20449;&#39640;&#25928;&#30340;&#38598;&#20013;&#24335;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#21333;&#27969;&#21644;&#22810;&#27969;&#30340;DIGEST&#65292;&#24403;&#27969;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#36890;&#20449;&#24320;&#38144;&#21487;&#33021;&#20250;&#22686;&#21152;&#65292;&#24182;&#19988;&#26377;&#19968;&#31181;&#25910;&#25947;&#21644;&#36890;&#20449;&#24320;&#38144;&#30340;&#26435;&#34913;&#21487;&#20197;&#21033;&#29992;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#27969;&#21644;&#22810;&#27969;DIGEST&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20004;&#31181;&#31639;&#27861;&#37117;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two widely considered decentralized learning algorithms are Gossip and random walk-based learning. Gossip algorithms (both synchronous and asynchronous versions) suffer from high communication cost, while random-walk based learning experiences increased convergence time. In this paper, we design a fast and communication-efficient asynchronous decentralized learning mechanism DIGEST by taking advantage of both Gossip and random-walk ideas, and focusing on stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized algorithm building on local-SGD algorithms, which are originally designed for communication efficient centralized learning. We design both single-stream and multi-stream DIGEST, where the communication overhead may increase when the number of streams increases, and there is a convergence and communication overhead trade-off which can be leveraged. We analyze the convergence of single- and multi-stream DIGEST, and prove that both algorithms approach to the optima
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;</title><link>http://arxiv.org/abs/2307.05639</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27963;&#36291;&#23376;&#31354;&#38388;&#24182;&#21457;&#29616;&#37325;&#35201;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#31934;&#24230;&#30697;&#38453;&#65292;&#20174;&#35757;&#32451;&#23436;&#25104;&#21518;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#29992;&#20449;&#24687;&#65292;&#21253;&#25324;&#27963;&#36291;&#23376;&#31354;&#38388;&#30340;&#26041;&#21521;&#21644;&#36755;&#20837;&#21464;&#37327;&#37325;&#35201;&#24615;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#19968;&#20010;&#26082;&#33021;&#36798;&#21040;&#24378;&#22823;&#39044;&#27979;&#24615;&#33021;&#65292;&#21448;&#33021;&#34987;&#20154;&#31867;&#35299;&#37322;&#30340;&#27169;&#22411;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#22256;&#38590;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#30001;&#20110;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#20914;&#31361;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#20026;&#20854;&#39640;&#26031;&#26680;&#28155;&#21152;&#21487;&#23398;&#20064;&#30340;&#31934;&#24230;&#30697;&#38453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35757;&#32451;&#23436;&#25104;&#21518;&#21487;&#20197;&#20174;&#31934;&#24230;&#30697;&#38453;&#30340;&#35889;&#20013;&#25552;&#21462;&#23453;&#36149;&#30340;&#20449;&#24687;&#12290;&#29305;&#21035;&#26159;&#65292;&#29305;&#24449;&#21521;&#37327;&#35299;&#37322;&#20102;&#27169;&#22411;&#26368;&#25935;&#24863;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#27963;&#36291;&#23376;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#30417;&#30563;&#38477;&#32500;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#29305;&#24449;&#21521;&#37327;&#20984;&#26174;&#20102;&#36755;&#20837;&#21644;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#32477;&#23545;&#21464;&#21270;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22522;&#20110;&#20854;&#23545;&#39044;&#27979;&#30340;&#37325;&#35201;&#24615;&#25552;&#21462;&#36755;&#20837;&#21464;&#37327;&#30340;&#25490;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing a model that achieves a strong predictive performance and at the same time is interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the Radial Basis Function Neural Network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.17815</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20449;&#24515;&#39044;&#27979;&#23454;&#29616;&#20855;&#22791;&#24418;&#24335;&#23433;&#20840;&#20445;&#35777;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction. (arXiv:2306.17815v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#38646;&#38454;&#20248;&#21270;&#26159;&#37329;&#34701;&#12289;&#29289;&#29702;&#21644;&#24037;&#31243;&#31561;&#39046;&#22495;&#24212;&#29992;&#30340;&#26680;&#24515;&#22522;&#26412;&#25805;&#20316;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#35265;&#24418;&#24335;&#20013;&#65292;&#35774;&#35745;&#32773;&#39034;&#24207;&#23581;&#35797;&#20505;&#36873;&#35299;&#65292;&#24182;&#20174;&#31995;&#32479;&#20013;&#25509;&#25910;&#21040;&#20851;&#20110;&#27599;&#20010;&#23581;&#35797;&#20540;&#30340;&#22122;&#22768;&#21453;&#39304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#36824;&#25552;&#20379;&#20102;&#26377;&#20851;&#23581;&#35797;&#35299;&#30340;&#23433;&#20840;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#19988;&#20248;&#21270;&#22120;&#34987;&#38480;&#21046;&#22312;&#25972;&#20010;&#20248;&#21270;&#36807;&#31243;&#20013;&#23581;&#35797;&#30340;&#19981;&#23433;&#20840;&#35299;&#30340;&#25968;&#37327;&#19978;&#12290;&#22312;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#30340;&#26041;&#27861;&#19978;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;SAFEOPT&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#21482;&#35201;&#28385;&#36275;&#23545;&#23433;&#20840;&#32422;&#26463;&#20989;&#25968;&#30340;&#20005;&#26684;&#20551;&#35774;&#65292;&#23601;&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#27010;&#29575;&#22312;&#21453;&#39304;&#22122;&#22768;&#19978;&#36991;&#20813;&#36873;&#25321;&#20219;&#20309;&#19981;&#23433;&#20840;&#30340;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;BO&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#32422;&#26463;&#20989;&#25968;&#30340;&#29305;&#24615;&#22914;&#20309;&#65292;&#37117;&#33021;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box zero-th order optimization is a central primitive for applications in fields as diverse as finance, physics, and engineering. In a common formulation of this problem, a designer sequentially attempts candidate solutions, receiving noisy feedback on the value of each attempt from the system. In this paper, we study scenarios in which feedback is also provided on the safety of the attempted solution, and the optimizer is constrained to limit the number of unsafe solutions that are tried throughout the optimization process. Focusing on methods based on Bayesian optimization (BO), prior art has introduced an optimization scheme -- referred to as SAFEOPT -- that is guaranteed not to select any unsafe solution with a controllable probability over feedback noise as long as strict assumptions on the safety constraint function are met. In this paper, a novel BO-based approach is introduced that satisfies safety requirements irrespective of properties of the constraint function. This s
&lt;/p&gt;</description></item><item><title>PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05515</link><description>&lt;p&gt;
&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05515
&lt;/p&gt;
&lt;p&gt;
PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#24050;&#25104;&#20026;&#24212;&#23545;&#21442;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;pFL&#19981;&#26159;&#23398;&#20064;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26159;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20010;&#20307;&#27169;&#22411;&#65292;&#21516;&#26102;&#20173;&#28982;&#21033;&#29992;&#20854;&#20182;&#23458;&#25143;&#31471;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeFLL&#65292;&#36825;&#26159;&#19968;&#31181;&#26681;&#26893;&#20110;&#32456;&#36523;&#23398;&#20064;&#30340;&#26032;&#22411;pFL&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19988;&#22312;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#20063;&#34920;&#29616;&#33391;&#22909;&#12290;PeFLL&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#26469;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#23884;&#20837;&#32593;&#32476;&#23398;&#20064;&#20197;&#19968;&#31181;&#21453;&#26144;&#23427;&#20204;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#28508;&#22312;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#34920;&#31034;&#23458;&#25143;&#31471;&#12290;&#36229;&#32593;&#32476;&#23398;&#20064;&#20174;&#36825;&#20010;&#28508;&#22312;&#31354;&#38388;&#21040;&#21487;&#33021;&#30340;&#23458;&#25143;&#27169;&#22411;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PeFLL&#20135;&#29983;&#20102;&#26356;&#39640;&#20934;&#30830;&#29575;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (pFL) has emerged as a popular approach to dealing with the challenge of statistical heterogeneity between the data distributions of the participating clients. Instead of learning a single global model, pFL aims to learn an individual model for each client while still making use of the data available at other clients. In this work, we present PeFLL, a new pFL approach rooted in lifelong learning that performs well not only on clients present during its training phase, but also on any that may emerge in the future. PeFLL learns to output client specific models by jointly training an embedding network and a hypernetwork. The embedding network learns to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork learns a mapping from this latent space to the space of possible client models. We demonstrate experimentally that PeFLL produces models of superior accuracy compared to previous methods, es
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24230;&#30456;&#20284;&#30340;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#65292;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#20851;&#31995;&#19981;&#28165;&#26224;&#32780;&#24341;&#36215;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.05023</link><description>&lt;p&gt;
&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#20013;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Posterior Collapse in Linear Conditional and Hierarchical Variational Autoencoders. (arXiv:2306.05023v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24230;&#30456;&#20284;&#30340;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#21644;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#65292;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#23545;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#20851;&#31995;&#19981;&#28165;&#26224;&#32780;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#65292;&#21518;&#39564;&#23849;&#28291;&#29616;&#35937;&#25351;&#30340;&#26159;&#21464;&#20998;&#21518;&#39564;&#20998;&#24067;&#19982;&#20808;&#39564;&#20998;&#24067;&#30340;&#30456;&#20284;&#24230;&#36807;&#39640;&#65292;&#23548;&#33268;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#28508;&#22312;&#21464;&#37327;&#20445;&#23384;&#30340;&#36755;&#20837;&#25968;&#25454;&#20449;&#24687;&#36739;&#23569;&#65292;&#26080;&#27861;&#20026;&#35299;&#30721;&#22120;&#30340;&#25968;&#25454;&#37325;&#24314;&#36807;&#31243;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#12290;&#23613;&#31649;&#35813;&#29616;&#35937;&#19968;&#30452;&#26159;VAEs&#24615;&#33021;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20294;&#26159;&#23545;&#20110;&#21518;&#39564;&#23849;&#28291;&#30340;&#29702;&#35770;&#21364;&#30456;&#23545;&#34180;&#24369;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#26631;&#20934;&#30340;VAEs&#20013;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20004;&#31867;&#37325;&#35201;&#32780;&#24120;&#35265;&#21448;&#36739;&#23569;&#30740;&#31350;&#30340;VAEs&#36827;&#34892;&#38750;&#24179;&#20961;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#21363;&#20855;&#26377;&#20004;&#20010;&#28508;&#22312;&#21464;&#37327;&#23618;&#27425;&#30340;&#32447;&#24615;&#26465;&#20214;VAE&#21644;&#20998;&#23618;VAE&#65292;&#25552;&#21319;&#20102;&#23545;&#21518;&#39564;&#23849;&#28291;&#30340;&#29702;&#35770;&#35748;&#35782;&#65292;&#35777;&#26126;&#20102;&#20854;&#25104;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
The posterior collapse phenomenon in variational autoencoders (VAEs), where the variational posterior distribution closely matches the prior distribution, can hinder the quality of the learned latent variables. As a consequence of posterior collapse, the latent variables extracted by the encoder in VAEs preserve less information from the input data and thus fail to produce meaningful representations as input to the reconstruction process in the decoder. While this phenomenon has been an actively addressed topic related to VAEs performance, the theory for posterior collapse remains underdeveloped, especially beyond the standard VAEs. In this work, we advance the theoretical understanding of posterior collapse to two important and prevalent yet less studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically, via a non-trivial theoretical analysis of linear conditional VAEs and hierarchical VAEs with two levels of latent, we prove that the cause of posterior collapses i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2306.03481</link><description>&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#32416;&#32544;&#25968;&#25454;&#30340;&#36716;&#25442;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transition role of entangled data in quantum machine learning. (arXiv:2306.03481v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#20855;&#26377;&#21452;&#37325;&#25928;&#24212;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#21644;&#20943;&#23567;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#20026;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32416;&#32544;&#20316;&#20026;&#22686;&#24378;&#37327;&#23376;&#35745;&#31639;&#30340;&#36164;&#28304;&#65292;&#24050;&#32463;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23558;&#32416;&#32544;&#34701;&#20837;&#21040;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25805;&#20316;&#25110;&#27979;&#37327;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#65292;&#21516;&#26102;&#22312;&#36798;&#21040;&#25351;&#23450;&#39044;&#27979;&#35823;&#24046;&#38408;&#20540;&#26102;&#21462;&#24471;&#20102;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#32416;&#32544;&#31243;&#24230;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#30446;&#21069;&#20173;&#32570;&#20047;&#20998;&#26512;&#24615;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#23398;&#20064;&#37327;&#23376;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#32416;&#32544;&#25968;&#25454;&#65292;&#24314;&#31435;&#20102;&#37327;&#23376;&#19981;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#12290;&#19982;&#20197;&#24448;&#21457;&#29616;&#30340;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#23545;&#39044;&#27979;&#35823;&#24046;&#30340;&#24433;&#21709;&#21576;&#29616;&#20986;&#21452;&#37325;&#25928;&#24212;&#65292;&#21462;&#20915;&#20110;&#20801;&#35768;&#30340;&#27979;&#37327;&#27425;&#25968;&#12290;&#22312;&#26377;&#20805;&#20998;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#32416;&#32544;&#24230;&#21487;&#20197;&#25345;&#32493;&#38477;&#20302;&#39044;&#27979;&#35823;&#24046;&#65292;&#25110;&#20943;&#23569;&#36798;&#21040;&#32473;&#23450;&#35823;&#24046;&#38408;&#20540;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12290;&#26412;&#30740;&#31350;&#38416;&#26126;&#20102;&#32416;&#32544;&#25968;&#25454;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36716;&#25442;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entanglement serves as the resource to empower quantum computing. Recent progress has highlighted its positive impact on learning quantum dynamics, wherein the integration of entanglement into quantum operations or measurements of quantum machine learning (QML) models leads to substantial reductions in training data size, surpassing a specified prediction error threshold. However, an analytical understanding of how the entanglement degree in data affects model performance remains elusive. In this study, we address this knowledge gap by establishing a quantum no-free-lunch (NFL) theorem for learning quantum dynamics using entangled data. Contrary to previous findings, we prove that the impact of entangled data on prediction error exhibits a dual effect, depending on the number of permitted measurements. With a sufficient number of measurements, increasing the entanglement of training data consistently reduces the prediction error or decreases the required size of the training data to ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.02865</link><description>&lt;p&gt;
&#25235;&#20303;&#24847;&#22806;&#25910;&#33719;&#65306;&#22312;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#20215;&#20540;(arXiv:2306.02865v2 [cs.LG]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; BEE &#25805;&#20316;&#31526;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#30340;&#25104;&#21151;&#32463;&#39564;&#65292;&#24182;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#31574;&#30053;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20013; Q &#20540;&#39640;&#20272;&#19982;&#20302;&#20272;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39640;&#36136;&#37327;&#30340; Q &#20540;&#20989;&#25968;&#22312;&#35768;&#22810;&#29616;&#20195;&#31163;&#32447;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#31639;&#27861;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#35299;&#20915;&#37319;&#29992;&#20989;&#25968;&#36924;&#36817;&#22120;&#21644;&#31163;&#32447;&#23398;&#20064;&#25152;&#23548;&#33268;&#30340;&#20540;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;&#19982;&#36825;&#31181;&#26222;&#36941;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040; Q &#20540;&#22312; RL &#35757;&#32451;&#36807;&#31243;&#30340;&#21518;&#26399;&#23454;&#38469;&#19978;&#34987;&#20302;&#20272;&#20102;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#36125;&#23572;&#26364;&#26356;&#26032;&#20013;&#65292;&#24403;&#21069;&#31574;&#30053;&#20351;&#29992;&#27604;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#26356;&#20248;&#30340;&#21160;&#20316;&#26679;&#26412;&#24046;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20010;&#38271;&#26399;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#21487;&#33021;&#38459;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#65292;&#38477;&#20302;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#22312;&#20445;&#25345;&#25506;&#32034;&#20048;&#35266;&#24615;&#30340;&#21516;&#26102;&#65292;&#32467;&#21512;&#20805;&#20998;&#21033;&#29992;&#36807;&#21435;&#25104;&#21151;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#21033;&#29992;&#21644;&#25506;&#32034; (BEE) &#25805;&#20316;&#31526;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21382;&#21490;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#21160;&#20316;&#21644;&#24403;&#21069;&#31574;&#30053;&#29983;&#25104;&#30340;&#21160;&#20316;&#26469;&#26356;&#26032; Q &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;SimFWI&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#27493;&#39588;&#20998;&#21035;&#26159;&#20998;&#21035;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#23398;&#20064;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#21487;&#26497;&#22823;&#22320;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#20219;&#21153;&#65292;&#24182;&#36830;&#25509;&#22810;&#20010;FWI&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.13314</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
Simplifying Full Waveform Inversion via Domain-Independent Self-Supervised Learning. (arXiv:2305.13314v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#39046;&#22495;&#26080;&#20851;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;SimFWI&#31639;&#27861;&#65292;&#20854;&#20004;&#20010;&#27493;&#39588;&#20998;&#21035;&#26159;&#20998;&#21035;&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#23398;&#20064;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#28982;&#21518;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#21487;&#26497;&#22823;&#22320;&#31616;&#21270;&#20840;&#27874;&#24418;&#21453;&#28436;&#20219;&#21153;&#65292;&#24182;&#36830;&#25509;&#22810;&#20010;FWI&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#29289;&#29702;&#23398;&#20013;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#20013;&#24212;&#29992;&#24471;&#21040;&#25104;&#21151;&#65292;&#29992;&#20110;&#39044;&#27979;&#22320;&#38663;&#25968;&#25454;&#20013;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#29616;&#35937;&#65306;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21508;&#33258;&#30340;&#39046;&#22495;&#20013;&#20998;&#21035;&#35757;&#32451;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35266;&#23519;&#21040;&#36328;&#39046;&#22495;&#30340;&#32447;&#24615;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SimFWI&#65292;&#19968;&#20010;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#30340;&#26032;&#33539;&#24335;&#65306;(a)&#36890;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#36974;&#30422;&#22270;&#20687;&#24314;&#27169;&#20998;&#21035;&#23398;&#20064;&#22320;&#38663;&#32534;&#30721;&#22120;&#21644;&#36895;&#24230;&#35299;&#30721;&#22120;&#65307;(b)&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#32447;&#24615;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Geophysics has witnessed success in applying deep learning to one of its core problems: full waveform inversion (FWI) to predict subsurface velocity maps from seismic data. It is treated as an image-to-image translation problem, jointly training an encoder for seismic data and a decoder for the velocity map from seismic-velocity pairs. In this paper, we report a surprising phenomenon: when training an encoder and decoder separately in their own domains via self-supervised learning, a linear relationship is observed across domains in the latent spaces. Moreover, this phenomenon connects multiple FWI datasets in an elegant manner: these datasets can share the self-learned encoder and decoder with different linear mappings.  Based on these findings, we develop SimFWI, a new paradigm that includes two steps: (a) learning a seismic encoder and a velocity decoder separately by masked image modeling over multiple datasets; (b) learning a linear mapping per dataset. Experimental results show t
&lt;/p&gt;</description></item><item><title>MAGDiff&#26159;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#27861;&#65292;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#25552;&#21462;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#21327;&#21464;&#25968;&#25454;&#38598;&#36716;&#31227;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#22270;&#26469;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26679;&#26412; Kolmogorov-Smirnov &#27979;&#35797;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.13271</link><description>&lt;p&gt;
MAGDiff: &#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#22270;&#26816;&#27979;&#21327;&#21464;&#25968;&#25454;&#38598;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
MAGDiff: Covariate Data Set Shift Detection via Activation Graphs of Deep Neural Networks. (arXiv:2305.13271v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13271
&lt;/p&gt;
&lt;p&gt;
MAGDiff&#26159;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#27861;&#65292;&#21487;&#20197;&#20174;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#25552;&#21462;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#21327;&#21464;&#25968;&#25454;&#38598;&#36716;&#31227;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#22270;&#26469;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26679;&#26412; Kolmogorov-Smirnov &#27979;&#35797;&#36827;&#34892;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#20294;&#20687;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#20204;&#21463;&#21040;&#25968;&#25454;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#19982;&#23454;&#38469;&#24212;&#29992;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#23384;&#22312;&#24046;&#24322;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#21463;&#21040;&#20005;&#37325;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; MAGDiff &#30340;&#26032;&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#32473;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#21327;&#21464;&#25968;&#25454;&#38598;&#36716;&#31227;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#19987;&#38376;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#26032;&#27169;&#22411;&#12290;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#26159;&#36890;&#36807;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#28608;&#27963;&#22270;&#26469;&#35745;&#31639;&#30340;&#65292;&#23545;&#20110;&#23646;&#20110;&#35757;&#32451;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25968;&#25454;&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#32479;&#35745;&#37327;&#65292;&#29992;&#20110;&#26816;&#27979;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#30340;&#20004;&#20010;&#26679;&#26412;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#20004;&#20010;&#26679;&#26412; Kolmogorov-Smirnov&#65288;KS&#65289;&#27979;&#35797;&#30340;&#32479;&#35745;&#21151;&#29575;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their successful application to a variety of tasks, neural networks remain limited, like other machine learning methods, by their sensitivity to shifts in the data: their performance can be severely impacted by differences in distribution between the data on which they were trained and that on which they are deployed. In this article, we propose a new family of representations, called MAGDiff, that we extract from any given neural network classifier and that allows for efficient covariate data shift detection without the need to train a new model dedicated to this task. These representations are computed by comparing the activation graphs of the neural network for samples belonging to the training distribution and to the target distribution, and yield powerful data- and task-adapted statistics for the two-sample tests commonly used for data set shift detection. We demonstrate this empirically by measuring the statistical powers of two-sample Kolmogorov-Smirnov (KS) tests on sev
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13138</link><description>&lt;p&gt;
&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#30340;&#26356;&#26032;&#31561;&#20215;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Update Equivalence Framework for Decision-Time Planning. (arXiv:2304.13138v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13138
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#65292;&#20351;&#24471;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#22312;&#26356;&#22823;&#33539;&#22260;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#20915;&#31574;&#29615;&#22659;&#20013;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26827;&#31867;&#28216;&#25103;&#31561;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#20013;&#65292;&#21363;&#26102;&#20462;&#27491;&#65288;&#25110;&#26500;&#24314;&#65289;&#31574;&#30053;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26159;&#23454;&#29616;&#36229;&#20154;&#31867;&#34920;&#29616;&#30340;&#20851;&#38190;&#12290;&#19968;&#20123;&#30740;&#31350;&#23558;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#25193;&#23637;&#21040;&#26356;&#26222;&#36941;&#30340;&#19981;&#23436;&#20840;&#20449;&#24687;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25169;&#20811;&#20013;&#30340;&#36229;&#20154;&#31867;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#32771;&#34385;&#38543;&#30528;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#30340;&#22686;&#21152;&#32780;&#24555;&#36895;&#22686;&#38271;&#30340;&#23376;&#28216;&#25103;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38750;&#20844;&#20849;&#20449;&#24687;&#37327;&#36739;&#22823;&#26102;&#19981;&#36215;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26356;&#26032;&#31561;&#20215;&#32780;&#19981;&#26159;&#23376;&#28216;&#25103;&#27010;&#24565;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#27169;&#25311;&#21516;&#27493;&#23398;&#20064;&#31639;&#27861;&#30340;&#26356;&#26032;&#12290;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#24341;&#20837;&#19968;&#31995;&#21015;&#21407;&#21017;&#19978;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#19981;&#20381;&#36182;&#20110;&#20844;&#20849;&#20449;&#24687;&#65292;&#24182;&#20026;&#26032;&#30340;&#19968;&#20010;&#31995;&#21015;&#30340;&#20915;&#31574;&#26102;&#38388;&#35268;&#21010;&#31639;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of revising (or constructing) a policy immediately prior to execution -- known as decision-time planning -- is key to achieving superhuman performance in perfect-information settings like chess and Go. A recent line of work has extended decision-time planning to more general imperfect-information settings, leading to superhuman performance in poker. However, these methods requires considering subgames whose sizes grow quickly in the amount of non-public information, making them unhelpful when the amount of non-public information is large. Motivated by this issue, we introduce an alternative framework for decision-time planning that is not based on subgames but rather on the notion of update equivalence. In this framework, decision-time planning algorithms simulate updates of synchronous learning algorithms. This framework enables us to introduce a new family of principled decision-time planning algorithms that do not rely on public information, opening the door to sound and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#22312;&#25932;&#23545;&#35774;&#32622;&#20013;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36798;&#21040;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2304.05219</link><description>&lt;p&gt;
BanditQ - &#22312;&#25932;&#23545;&#29615;&#22659;&#20013;&#20445;&#35777;&#29992;&#25143;&#27599;&#27425;&#22870;&#21169;&#30340;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#22312;&#25932;&#23545;&#35774;&#32622;&#20013;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36798;&#21040;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#22914;Hedge&#22312;&#35774;&#35745;&#19978;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23581;&#35797;&#23613;&#21487;&#33021;&#22810;&#22320;&#29609;&#26368;&#20855;&#22238;&#25253;&#30340;&#33218;&#32780;&#24573;&#30053;&#27425;&#20248;&#33218;&#65292;&#20197;&#23454;&#29616;&#20122;&#32447;&#24615;&#36951;&#25022;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20855;&#26377;&#23545;&#25152;&#26377;&#33218;&#32047;&#31215;&#22870;&#21169;&#36895;&#29575;&#19979;&#30028;&#30340;&#25932;&#23545;&#35774;&#32622;&#20013;&#65292;&#20197;&#20844;&#24179;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;&#25490;&#38431;&#35770;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#23427;&#22312;&#20840;&#20449;&#24687;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;BanditQ&#30340;&#35774;&#35745;&#21644;&#20998;&#26512;&#28041;&#21450;&#28508;&#22312;&#20989;&#25968;&#26041;&#27861;&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#24182;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.05215</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Billion-scale Foundation Model for Remote Sensing Images. (arXiv:2304.05215v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#35813;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#24433;&#21709;&#65292;&#23454;&#39564;&#26174;&#31034;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20808;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#39044;&#35757;&#32451;&#26041;&#27861;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#20197;&#21450;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#26368;&#36817;&#65292;&#36965;&#24863;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#20851;&#27880;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#22312;&#26059;&#36716;&#30446;&#26631;&#26816;&#27979;&#21644;&#35821;&#20041;&#20998;&#21106;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#24615;&#33021;&#30340;&#24433;&#21709;&#26469;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#21442;&#25968;&#65288;&#21253;&#25324;86M&#12289;605.26M&#12289;1.3B&#21644;2.4B&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#30830;&#23450;&#21442;&#25968;&#22686;&#21152;&#26159;&#21542;&#20250;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#30340;&#21313;&#20159;&#32423;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;10&#20159;&#20010;&#36965;&#24863;&#22270;&#20687;&#30340;&#26032;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the potential of foundation models in visual tasks has garnered significant attention, pretraining these models before downstream tasks has become a crucial step. The three key factors in pretraining foundation models are the pretraining method, the size of the pretraining dataset, and the number of model parameters. Recently, research in the remote sensing field has focused primarily on the pretraining method and the size of the dataset, with limited emphasis on the number of model parameters. This paper addresses this gap by examining the effect of increasing the number of model parameters on the performance of foundation models in downstream tasks such as rotated object detection and semantic segmentation. We pretrained foundation models with varying numbers of parameters, including 86M, 605.26M, 1.3B, and 2.4B, to determine whether performance in downstream tasks improved with an increase in parameters. To the best of our knowledge, this is the first billion-scale foundation mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.13462</link><description>&lt;p&gt;
&#21033;&#29992;&#37327;&#23376;&#20960;&#20309;&#36827;&#34892;&#23398;&#20064;&#24186;&#27491;&#21464;&#25442;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20351;&#29992;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;&#26469;&#35780;&#20272;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#21435;&#38500;&#23545;&#31216;&#24615;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#26032;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24341;&#20837;&#25968;&#25454;&#30340;&#37327;&#23376;&#36153;&#33293;&#23572;&#20449;&#24687;&#24230;&#37327;(DQFIM)&#26469;&#30830;&#23450;&#27169;&#22411;&#20309;&#26102;&#33021;&#22815;&#27867;&#21270;&#12290;&#23545;&#20110;&#24186;&#27491;&#21464;&#25442;&#30340;&#21487;&#21464;&#23398;&#20064;&#65292;DQFIM&#37327;&#21270;&#20102;&#25104;&#21151;&#35757;&#32451;&#21644;&#27867;&#21270;&#25152;&#38656;&#30340;&#30005;&#36335;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#24212;&#29992;DQFIM&#26469;&#35299;&#37322;&#20309;&#26102;&#24658;&#23450;&#25968;&#37327;&#30340;&#35757;&#32451;&#29366;&#24577;&#21644;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#21442;&#25968;&#36275;&#20197;&#23454;&#29616;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#21024;&#38500;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26174;&#31034;&#65292;&#20351;&#29992;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#27604;&#20351;&#29992;&#30456;&#21516;&#20998;&#24067;&#30340;&#33021;&#21147;&#26356;&#20248;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#25552;&#39640;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.08854</link><description>&lt;p&gt;
&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08854
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#21644;&#25512;&#26029;&#65307;&#21363;&#22312;&#27599;&#20010;&#26102;&#26399;&#65288;&#20063;&#31216;&#20026;&#24773;&#33410;&#65289;&#20197;&#39034;&#24207;&#26041;&#24335;&#19982;&#21333;&#20010;&#21463;&#35797;&#21333;&#20803;&#22810;&#27425;&#20132;&#20114;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#25910;&#38598;&#25968;&#25454;&#21518;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#24182;&#20272;&#35745;&#32467;&#26500;&#21442;&#25968;&#65292;&#22914;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20449;&#29992;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#31532;&#19968;&#20010;&#26102;&#26399;&#30340;&#34892;&#21160;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#65289;&#12290;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#21487;&#20197;&#26500;&#25104;&#30697;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#19981;&#26159;&#24635;&#20307;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#22120;&#65292;&#22312;&#38745;&#24577;&#25968;&#25454;&#24773;&#20917;&#19979;&#23548;&#33268;&#20102;$Z$-&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20272;&#35745;&#37327;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#19981;&#33021;&#28176;&#36817;&#27491;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#26469;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#65292;&#36825;&#26159;&#30001;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We consider estimation and inference with data collected from episodic reinforcement learning (RL) algorithms; i.e. adaptive experimentation algorithms that at each period (aka episode) interact multiple times in a sequential manner with a single treated unit. Our goal is to be able to evaluate counterfactual adaptive policies after data collection and to estimate structural parameters such as dynamic treatment effects, which can be used for credit assignment (e.g. what was the effect of the first period action on the final outcome). Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to $Z$-estimation approaches in the case of static data. However, such estimators fail to be asymptotically normal in the case of adaptive data collection. We propose a re-weighted $Z$-estimation approach with carefully designed adaptive weights to stabilize the episode-varying estimation variance, which results from the non
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053; AA ++&#65292;&#33021;&#22815;&#22312;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2301.13748</link><description>&lt;p&gt;
&#21407;&#22411;&#20998;&#26512;++&#65306;&#37325;&#26032;&#24605;&#32771;&#21021;&#22987;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Archetypal Analysis++: Rethinking the Initialization Strategy. (arXiv:2301.13748v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053; AA ++&#65292;&#33021;&#22815;&#22312;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#20998;&#26512;&#26159;&#19968;&#31181;&#24102;&#26377;&#20984;&#24615;&#32422;&#26463;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#12290;&#30001;&#20110;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#22909;&#30340;&#21021;&#22987;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#26159;&#32463;&#24120;&#20351;&#29992;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#35201;&#20040;&#20135;&#29983;&#27425;&#20248;&#30340;&#36215;&#22987;&#28857;&#65292;&#35201;&#20040;&#23481;&#26131;&#38519;&#20837;&#19981;&#33391;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21407;&#22411;&#20998;&#26512;++&#65288;AA ++&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#21407;&#22411;&#20998;&#26512;&#30340;&#27010;&#29575;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#23427;&#26681;&#25454;&#28857;&#23545;&#30446;&#26631;&#30340;&#24433;&#21709;&#39034;&#24207;&#22320;&#36827;&#34892;&#37319;&#26679;&#65292;&#31867;&#20284;&#20110;$k$-means++&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35748;&#20026;$k$-means++&#24050;&#36817;&#36924;&#36817;&#20102;&#25152;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;$k$-means++&#30340;&#39640;&#25928;&#33945;&#29305;&#21345;&#32599;&#36817;&#20284;&#26041;&#27861;&#24212;&#29992;&#20110;AA++&#12290;&#22312;&#23545;13&#20010;&#19981;&#21516;&#22823;&#23567;&#21644;&#32500;&#24230;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#24182;&#32771;&#34385;&#20004;&#20010;&#39044;&#22788;&#29702;&#31574;&#30053;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;AA++&#20960;&#20046;&#24635;&#26159;&#20248;&#20110;&#25152;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Archetypal analysis is a matrix factorization method with convexity constraints. Due to local minima, a good initialization is essential, but frequently used initialization methods yield either sub-optimal starting points or are prone to get stuck in poor local minima. In this paper, we propose archetypal analysis++ (AA++), a probabilistic initialization strategy for archetypal analysis that sequentially samples points based on their influence on the objective, similar to $k$-means++. In fact, we argue that $k$-means++ already approximates the proposed initialization method. Furthermore, we suggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++. In an extensive empirical evaluation of 13 real-world data sets of varying sizes and dimensionalities and considering two pre-processing strategies, we show that AA++ nearly always outperforms all baselines, including the most frequently used ones.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;Text-to-SQL&#27169;&#22411;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#28431;&#27934;&#33021;&#22815;&#34987;&#24694;&#24847;&#21033;&#29992;&#20135;&#29983;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#21830;&#19994;&#24212;&#29992;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#24847;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2211.15363</link><description>&lt;p&gt;
&#20851;&#20110;Text-to-SQL&#27169;&#22411;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
On the Security Vulnerabilities of Text-to-SQL Models. (arXiv:2211.15363v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15363
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;Text-to-SQL&#27169;&#22411;&#23384;&#22312;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#28431;&#27934;&#33021;&#22815;&#34987;&#24694;&#24847;&#21033;&#29992;&#20135;&#29983;&#25915;&#20987;&#65292;&#36890;&#36807;&#23545;&#21830;&#19994;&#24212;&#29992;&#21644;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#35813;&#30740;&#31350;&#24847;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#65292;&#20294;&#36825;&#20123;&#24369;&#28857;&#26159;&#21542;&#21487;&#33021;&#23548;&#33268;&#36719;&#20214;&#23433;&#20840;&#23041;&#32961;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23545;&#24120;&#29992;&#20110;&#21019;&#24314;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#24211;&#25509;&#21475;&#30340;Text-to-SQL&#31995;&#32479;&#36827;&#34892;&#20102;&#28431;&#27934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20845;&#20010;&#21830;&#19994;&#24212;&#29992;&#20013;&#30340;Text-to-SQL&#27169;&#22359;&#21487;&#20197;&#34987;&#25805;&#32437;&#20197;&#20135;&#29983;&#24694;&#24847;&#20195;&#30721;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#25968;&#25454;&#27844;&#28431;&#21644;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;NLP&#27169;&#22411;&#21487;&#20197;&#34987;&#21033;&#29992;&#20026;&#25915;&#20987;&#21521;&#37327;&#30340;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#22235;&#20010;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#23545;Text-to-SQL&#31995;&#32479;&#36827;&#34892;&#30452;&#25509;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#36798;&#21040;100&#65285;&#30340;&#25104;&#21151;&#29575;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24341;&#36215;&#23398;&#26415;&#30028;&#23545;&#19982;NLP&#31639;&#27861;&#30456;&#20851;&#30340;&#28508;&#22312;&#36719;&#20214;&#23433;&#20840;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although it has been demonstrated that Natural Language Processing (NLP) algorithms are vulnerable to deliberate attacks, the question of whether such weaknesses can lead to software security threats is under-explored. To bridge this gap, we conducted vulnerability tests on Text-to-SQL systems that are commonly used to create natural language interfaces to databases. We showed that the Text-to-SQL modules within six commercial applications can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service attacks. This is the first demonstration that NLP models can be exploited as attack vectors in the wild. In addition, experiments using four open-source language models verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance. The aim of this work is to draw the community's attention to potential software security issues associated with NLP algorithms and encourage explor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.00979</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Wasserstein Shortest-path Filtration Kernels on Graphs. (arXiv:2206.00979v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#30340;&#26032;&#22411;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26680;&#24515;&#30340;&#20449;&#24687;&#20002;&#22833;&#21644;&#32570;&#20047;&#22810;&#20010;&#23610;&#24230;&#32771;&#34385;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65288;SP&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#22270;&#26680;&#24515;&#20043;&#19968;&#12290;&#23427;&#23558;&#22270;&#20998;&#35299;&#20026;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#22270;&#20013;&#26368;&#30701;&#36335;&#24452;&#30340;&#39057;&#29575;&#12290;&#28982;&#32780;&#65292;SP&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#26368;&#30701;&#36335;&#24452;&#30340;&#19977;&#20803;&#34920;&#31034;&#22833;&#21435;&#20102;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;SP&#27604;&#36739;&#22270;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#22270;&#32467;&#26500;&#30340;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#29366;&#32467;&#26500;&#12289;&#29615;&#29366;&#32467;&#26500;&#21644;&#26143;&#29366;&#32467;&#26500;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#30701;&#36335;&#24452;&#22270;&#26680;&#24515;&#65292;&#31216;&#20026;&#22810;&#23610;&#24230;Wasserstein&#26368;&#30701;&#36335;&#24452;&#36807;&#28388;&#22270;&#26680;&#24515;&#65288;MWSPF&#65289;&#12290;&#23427;&#20351;&#29992;&#20197;&#27599;&#20010;&#39030;&#28857;&#20026;&#26681;&#30340;&#26576;&#20010;&#28145;&#24230;&#30340;BFS&#26641;&#26469;&#38480;&#21046;&#32771;&#34385;&#26368;&#30701;&#36335;&#24452;&#30340;&#26368;&#22823;&#38271;&#24230;&#65292;&#32771;&#34385;&#21040;&#23567;&#19990;&#30028;&#29305;&#24615;&#12290;&#23427;&#32771;&#34385;&#20102;&#26368;&#30701;&#36335;&#24452;&#20013;&#25152;&#26377;&#39030;&#28857;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26041;&#20415;&#22312;&#22810;&#20010;&#19981;&#21516;&#23610;&#24230;&#19978;&#27604;&#36739;&#22270;&#65292;&#23427;&#20174;&#39030;&#28857;&#21644;
&lt;/p&gt;
&lt;p&gt;
The traditional shortest-path graph kernel (SP) is one of the most popular graph kernels. It decomposes graphs into shortest paths and computes their frequencies in each graph. However, SP has two main challenges: Firstly, the triplet representation of the shortest path loses information. Secondly, SP compares graphs without considering the multiple different scales of the graph structure which is common in real-world graphs, e.g., the chain-, ring-, and star-structures in social networks. To overcome these two challenges, we develop a novel shortest-path graph kernel called the Multi-scale Wasserstein Shortest-Path Filtration graph kernel (MWSPF). It uses a BFS tree of a certain depth rooted at each vertex to restrict the maximum length of the shortest path considering the small world property. It considers the labels of all the vertices in the shortest path. To facilitate the comparison of graphs at multiple different scales, it augments graphs from both the aspects of the vertex and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#20851;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#31532;&#19968;&#20221;&#32479;&#19968;&#35843;&#26597;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#31227;&#21160;&#20581;&#24247;&#20013;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#36825;&#20004;&#20010;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#21644;&#19981;&#21516;&#20043;&#22788;&#38656;&#35201;&#32771;&#34385;&#65292;&#24182;&#19988;&#36825;&#37324;&#23384;&#22312;&#24040;&#22823;&#30340;&#21512;&#20316;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2203.02605</link><description>&lt;p&gt;
&#29616;&#20195;&#29983;&#29289;&#32479;&#35745;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#26500;&#24314;&#26368;&#20248;&#33258;&#36866;&#24212;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions. (arXiv:2203.02605v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.02605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#31532;&#19968;&#20221;&#32479;&#19968;&#35843;&#26597;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#31227;&#21160;&#20581;&#24247;&#20013;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#36825;&#20004;&#20010;&#39046;&#22495;&#20013;&#37117;&#20855;&#26377;&#24456;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#21644;&#19981;&#21516;&#20043;&#22788;&#38656;&#35201;&#32771;&#34385;&#65292;&#24182;&#19988;&#36825;&#37324;&#23384;&#22312;&#24040;&#22823;&#30340;&#21512;&#20316;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#24207;&#21015;&#24615;&#20915;&#31574;&#20013;&#21344;&#25454;&#20102;&#37325;&#35201;&#22320;&#20301;&#65292;&#25104;&#20026;&#20132;&#20184;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;AIs&#65289;&#30340;&#36234;&#26469;&#36234;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#65292;&#20294;&#20854;&#29616;&#23454;&#24212;&#29992;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#65292;&#37096;&#20998;&#26159;&#30001;&#20110;&#26041;&#27861;&#35770;&#21644;&#24212;&#29992;&#31038;&#21306;&#20043;&#38388;&#30340;&#21327;&#21516;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#23398;&#20064;AIs&#30340;RL&#26041;&#27861;&#30340;&#31532;&#19968;&#20221;&#32479;&#19968;&#35843;&#26597;&#65292;&#21033;&#29992;RL&#30340;&#36890;&#29992;&#26041;&#27861;&#35770;&#20254;&#26469;&#26725;&#25509;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#21644;&#31227;&#21160;&#20581;&#24247;&#20013;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#36825;&#20004;&#20010;AI&#39046;&#22495;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;AI&#39046;&#22495;&#20043;&#38388;&#30340;&#24322;&#21516;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#23545;&#20351;&#29992;RL&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#24049;&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#35774;&#35745;&#26696;&#20363;&#30740;&#31350;&#30340;&#32463;&#39564;&#65292;&#35828;&#26126;&#20102;&#22312;AIs&#39046;&#22495;&#20013;&#65292;&#32479;&#35745;&#23398;&#12289;RL&#21644;&#21307;&#30103;&#30740;&#31350;&#20154;&#21592;&#20043;&#38388;&#30340;&#24040;&#22823;&#21512;&#20316;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, reinforcement learning (RL) has acquired a prominent position in the space of health-related sequential decision-making, becoming an increasingly popular tool for delivering adaptive interventions (AIs). However, despite potential benefits, its real-life application is still limited, partly due to a poor synergy between the methodological and the applied communities. In this work, we provide the first unified survey on RL methods for learning AIs, using the common methodological umbrella of RL to bridge the two AI areas of dynamic treatment regimes and just-in-time adaptive interventions in mobile health. We outline similarities and differences between these two AI domains and discuss their implications for using RL. Finally, we leverage our experience in designing case studies in both areas to illustrate the tremendous collaboration opportunities between statistical, RL, and healthcare researchers in the space of AIs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;</title><link>http://arxiv.org/abs/2202.03356</link><description>&lt;p&gt;
&#39640;&#25928;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#39640;&#25928;&#30340;&#30452;&#36830;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#33410;&#28857;&#24310;&#36831;&#21644;&#24102;&#23485;&#26435;&#34913;&#65292;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26500;&#24314;&#36866;&#29992;&#20110;&#38598;&#20307;&#36890;&#20449;&#30340;&#39640;&#25928;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#38024;&#23545;&#33410;&#28857;&#24310;&#36831;&#19982;&#24102;&#23485;&#26435;&#34913;&#20248;&#21270;&#30340;&#30452;&#36830;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20010;&#31639;&#27861;&#26694;&#26550;&#20174;&#23567;&#30340;&#22522;&#30784;&#25299;&#25169;&#32467;&#26500;&#21644;&#30456;&#20851;&#30340;&#36890;&#20449;&#36827;&#24230;&#24320;&#22987;&#65292;&#24182;&#20351;&#29992;&#19968;&#32452;&#21487;&#20197;&#36845;&#20195;&#24212;&#29992;&#30340;&#25216;&#26415;&#26469;&#27966;&#29983;&#26356;&#22823;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;&#36825;&#20123;&#34893;&#29983;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#26102;&#38388;&#34920;&#21487;&#20197;&#19982;&#25193;&#23637;&#19968;&#36215;&#21512;&#25104;&#65292;&#20063;&#21487;&#20197;&#20351;&#29992;&#20248;&#21270;&#20844;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#25105;&#20204;&#20026;&#32473;&#23450;&#30340;&#38598;&#32676;&#22823;&#23567;&#21644;&#24230;&#25968;&#21512;&#25104;&#35768;&#22810;&#19981;&#21516;&#30340;&#25299;&#25169;&#32467;&#26500;&#21644;&#26102;&#38388;&#34920;&#65292;&#28982;&#21518;&#20026;&#32473;&#23450;&#30340;&#24037;&#20316;&#36127;&#36733;&#30830;&#23450;&#36866;&#24403;&#30340;&#25299;&#25169;&#21644;&#26102;&#38388;&#34920;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;&#34917;&#19969;&#38754;&#26495;&#37197;&#32622;&#25152;&#38656;&#25299;&#25169;&#32467;&#26500;&#30340;12&#33410;&#28857;&#20809;&#23398;&#23454;&#39564;&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22686;&#21152;&#20102;&#22522;&#20110;&#20998;&#26512;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#29992;&#20110;&#26356;&#22823;&#30340;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21015;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#25104;&#26412;&#31995;&#25968;&#21644;&#24341;&#20837;&#39069;&#22806;&#32422;&#26463;&#35299;&#20915;&#20102;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#23616;&#37096;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2104.10751</link><description>&lt;p&gt;
&#20998;&#31867;&#35268;&#21017;&#29983;&#25104;&#65306;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rule Generation for Classification: Scalability, Interpretability, and Fairness. (arXiv:2104.10751v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.10751
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#21015;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#37197;&#25104;&#26412;&#31995;&#25968;&#21644;&#24341;&#20837;&#39069;&#22806;&#32422;&#26463;&#35299;&#20915;&#20102;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#23616;&#37096;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#20248;&#21270;&#26041;&#27861;&#65292;&#20855;&#26377;&#32422;&#26463;&#26465;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#21015;&#29983;&#25104;&#32447;&#24615;&#35268;&#21010;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#21040;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#25152;&#24471;&#23450;&#20215;&#23376;&#38382;&#39064;&#34987;&#35777;&#26126;&#26159;NP&#38590;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#19968;&#20010;&#20195;&#29702;&#23450;&#20215;&#23376;&#38382;&#39064;&#20197;&#21152;&#36895;&#12290;&#35813;&#26041;&#27861;&#36820;&#22238;&#19968;&#32452;&#35268;&#21017;&#20197;&#21450;&#23427;&#20204;&#30340;&#26368;&#20248;&#26435;&#37325;&#65292;&#25351;&#31034;&#27599;&#20010;&#35268;&#21017;&#23545;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#35268;&#21017;&#20998;&#37197;&#25104;&#26412;&#31995;&#25968;&#21644;&#24341;&#20837;&#39069;&#22806;&#32422;&#26463;&#26469;&#35299;&#20915;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#23616;&#37096;&#35299;&#37322;&#24615;&#65292;&#24182;&#23558;&#20844;&#24179;&#24615;&#30340;&#19968;&#33324;&#20998;&#31163;&#20934;&#21017;&#25512;&#24191;&#21040;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#21644;&#31867;&#21035;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#35814;&#32454;&#38416;&#36848;&#20854;&#19981;&#21516;&#26041;&#38754;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23398;&#20064;&#26041;&#27861;&#22312;&#23616;&#37096;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#33391;&#22909;&#30340;&#24179;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new rule-based optimization method for classification with constraints. The proposed method leverages column generation for linear programming, and hence, is scalable to large datasets. The resulting pricing subproblem is shown to be NP-Hard. We recourse to a decision tree-based heuristic and solve a proxy pricing subproblem for acceleration. The method returns a set of rules along with their optimal weights indicating the importance of each rule for learning. We address interpretability and fairness by assigning cost coefficients to the rules and introducing additional constraints. In particular, we focus on local interpretability and generalize separation criterion in fairness to multiple sensitive attributes and classes. We test the performance of the proposed methodology on a collection of datasets and present a case study to elaborate on its different aspects. The proposed rule-based learning method exhibits a good compromise between local interpretability and fairn
&lt;/p&gt;</description></item></channel></rss>