<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.10167</link><description>&lt;p&gt;
&#24191;&#20041;&#21010;&#20998;&#23616;&#37096;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#20957;&#32858;&#27010;&#24565;&#65292;&#26500;&#24314;&#22312;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#22522;&#30784;&#19978;&#65292;&#25193;&#23637;&#20102;&#26089;&#26399;&#32467;&#26524;&#24182;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#30340;&#31038;&#21306;&#21457;&#29616;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#36817;&#30001;Berenhaut&#12289;Moore&#21644;Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]&#25552;&#20986;&#30340;&#20957;&#32858;&#27010;&#24565;&#30340;&#27010;&#25324;&#12290;&#25152;&#25552;&#20986;&#30340;&#34920;&#36848;&#22522;&#20110;&#20998;&#21306;&#23616;&#37096;&#28145;&#24230;&#30340;&#25216;&#26415;&#24182;&#25552;&#28860;&#20102;&#20004;&#20010;&#20851;&#38190;&#27010;&#29575;&#27010;&#24565;&#65306;&#23616;&#37096;&#30456;&#20851;&#24615;&#21644;&#25903;&#25345;&#20998;&#21106;&#12290;&#26089;&#26399;&#32467;&#26524;&#22312;&#26032;&#30340;&#32972;&#26223;&#19979;&#24471;&#21040;&#25193;&#23637;&#65292;&#24182;&#21253;&#25324;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#20013;&#25581;&#31034;&#31038;&#21306;&#30340;&#24212;&#29992;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.10158</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#32508;&#36848;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-centric Artificial Intelligence: A Survey. (arXiv:2303.10158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#30340;&#24517;&#35201;&#24615;&#24182;&#20174;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#21644;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#35813;&#32508;&#36848;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#24182;&#35752;&#35770;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#20960;&#20046;&#25152;&#26377;&#39046;&#22495;&#20135;&#29983;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#20043;&#19968;&#26159;&#21487;&#29992;&#20110;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20016;&#23500;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#25968;&#25454;&#22312;AI&#20013;&#30340;&#20316;&#29992;&#24471;&#21040;&#20102;&#26174;&#33879;&#25918;&#22823;&#65292;&#24341;&#21457;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#36825;&#19968;&#26032;&#20852;&#27010;&#24565;&#30340;&#20986;&#29616;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#27880;&#24847;&#21147;&#36880;&#28176;&#20174;&#25512;&#36827;&#27169;&#22411;&#35774;&#35745;&#36716;&#21521;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25968;&#25454;&#20013;&#24515;AI&#30340;&#24517;&#35201;&#24615;&#65292;&#38543;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#12289;&#25512;&#29702;&#25968;&#25454;&#24320;&#21457;&#21644;&#25968;&#25454;&#32500;&#25252;&#19977;&#20010;&#19968;&#33324;&#24615;&#25968;&#25454;&#20013;&#24515;&#30446;&#26631;&#20197;&#21450;&#20195;&#34920;&#24615;&#26041;&#27861;&#30340;&#20840;&#38754;&#35270;&#35282;&#36827;&#34892;&#20102;&#20171;&#32461;&#12290;&#25105;&#20204;&#36824;&#20174;&#33258;&#21160;&#21270;&#21644;&#21327;&#20316;&#30340;&#35282;&#24230;&#32452;&#32455;&#20102;&#29616;&#26377;&#25991;&#29486;&#65292;&#35752;&#35770;&#20102;&#25361;&#25112;&#65292;&#24182;&#21015;&#20986;&#20102;&#21508;&#31181;&#20219;&#21153;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#26159;&#31532;&#19968;&#20221;&#25552;&#20379;&#36328;&#36234;&#21508;&#20010;&#38454;&#27573;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20840;&#29699;&#35270;&#35282;&#30340;&#32508;&#21512;&#24615;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#35266;&#36951;&#20256;&#31639;&#27861;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#25311;DNA&#30002;&#22522;&#21270;&#65292;&#36890;&#36807;&#22686;&#24378;/&#27785;&#40664;&#22522;&#22240;&#30340;&#27010;&#24565;&#25552;&#39640;&#20102;&#36951;&#20256;&#31639;&#27861;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10154</link><description>&lt;p&gt;
&#34920;&#35266;&#36951;&#20256;&#31639;&#27861;&#65306;&#33258;&#25105;&#24378;&#21270;&#27880;&#24847;&#26426;&#21046;&#35843;&#33410;&#26579;&#33394;&#20307;&#34920;&#36798;
&lt;/p&gt;
&lt;p&gt;
Epigenetics Algorithms: Self-Reinforcement-Attention mechanism to regulate chromosomes expression. (arXiv:2303.10154v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#35266;&#36951;&#20256;&#31639;&#27861;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#25311;DNA&#30002;&#22522;&#21270;&#65292;&#36890;&#36807;&#22686;&#24378;/&#27785;&#40664;&#22522;&#22240;&#30340;&#27010;&#24565;&#25552;&#39640;&#20102;&#36951;&#20256;&#31639;&#27861;&#25928;&#26524;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#26159;&#29983;&#29289;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#19968;&#20010;&#33879;&#21517;&#20363;&#23376;&#12290;&#23427;&#20204;&#36890;&#36807;&#24314;&#27169;&#22810;&#20010;&#25805;&#20316;&#31526;&#65292;&#22914;&#31361;&#21464;&#12289;&#20132;&#21449;&#21644;&#36873;&#25321;&#65292;&#26469;&#27169;&#20223;&#33258;&#28982;&#36873;&#25321;&#12290;&#26368;&#36817;&#21457;&#29616;&#20102;&#8220;&#22312;&#36951;&#20256;&#22522;&#30784;&#20043;&#19978;&#8221;&#25110;&#8220;&#38500;&#36951;&#20256;&#22522;&#30784;&#20043;&#22806;&#8221;&#21457;&#29983;&#30340;&#34920;&#35266;&#36951;&#20256;&#35843;&#25511;&#36807;&#31243;&#65292;&#23427;&#20204;&#28041;&#21450;&#24433;&#21709;&#21644;&#25913;&#21892;&#22522;&#22240;&#34920;&#36798;&#30340;&#21464;&#21270;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#24314;&#27169;&#34920;&#35266;&#36951;&#20256;&#25805;&#20316;&#31526;&#26469;&#25913;&#21892;&#36951;&#20256;&#31639;&#27861;&#65288;GAs&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#35266;&#36951;&#20256;&#31639;&#27861;&#65292;&#27169;&#25311;&#20102;&#34987;&#31216;&#20026;DNA&#30002;&#22522;&#21270;&#30340;&#34920;&#35266;&#36951;&#20256;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#34920;&#35266;&#36951;&#20256;&#31639;&#27861;&#30340;&#21019;&#26032;&#20043;&#22788;&#20027;&#35201;&#22312;&#20110;&#21033;&#29992;&#20102;&#27880;&#24847;&#26426;&#21046;&#21644;&#28145;&#24230;&#23398;&#20064;&#65292;&#36825;&#19982;&#22686;&#24378;/&#27785;&#40664;&#22522;&#22240;&#30340;&#27010;&#24565;&#30456;&#22865;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29702;&#35770;&#35770;&#35777;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#34920;&#35266;&#36951;&#20256;&#31639;&#27861;&#35299;&#20915;&#26356;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#31616;&#21333;&#36951;&#20256;&#31639;&#27861;&#25152;&#19981;&#33021;&#20570;&#21040;&#30340;&#65292;&#20363;&#22914;&#65292;&#38754;&#23545;&#29305;&#23450;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#34920;&#29616;&#20986;&#20102;&#26356;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Genetic algorithms are a well-known example of bio-inspired heuristic methods. They mimic natural selection by modeling several operators such as mutation, crossover, and selection. Recent discoveries about Epigenetics regulation processes that occur "on top of" or "in addition to" the genetic basis for inheritance involve changes that affect and improve gene expression. They raise the question of improving genetic algorithms (GAs) by modeling epigenetics operators. This paper proposes a new epigenetics algorithm that mimics the epigenetics phenomenon known as DNA methylation. The novelty of our epigenetics algorithms lies primarily in taking advantage of attention mechanisms and deep learning, which fits well with the genes enhancing/silencing concept. The paper develops theoretical arguments and presents empirical studies to exhibit the capability of the proposed epigenetics algorithms to solve more complex problems efficiently than has been possible with simple GAs; for example, fac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#26356;&#26032;&#21040;&#25968;&#25454;&#27604;&#29575;&#65288;UTD&#65289;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#23567;&#35268;&#27169;&#30340;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#36830;&#32493;&#25910;&#38598;&#30340;&#32463;&#39564;&#19978;&#26816;&#27979;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DreamerV2&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#19982;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10144</link><description>&lt;p&gt;
&#21160;&#24577;&#26356;&#26032;&#25968;&#25454;&#27604;&#29575;&#65306;&#20943;&#23569;&#19990;&#30028;&#27169;&#22411;&#36807;&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Update-to-Data Ratio: Minimizing World Model Overfitting. (arXiv:2303.10144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10144
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#26356;&#26032;&#21040;&#25968;&#25454;&#27604;&#29575;&#65288;UTD&#65289;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#23567;&#35268;&#27169;&#30340;&#26410;&#29992;&#20110;&#35757;&#32451;&#30340;&#36830;&#32493;&#25910;&#38598;&#30340;&#32463;&#39564;&#19978;&#26816;&#27979;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DreamerV2&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#19982;&#24191;&#27867;&#30340;&#36229;&#21442;&#25968;&#25628;&#32034;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#30340;&#24773;&#22659;&#19979;&#65292;&#22522;&#20110;&#39564;&#35777;&#38598;&#34920;&#29616;&#30340;&#26089;&#26399;&#20572;&#27490;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25214;&#21040;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#21363;&#20351;&#22312;&#35832;&#22914;&#19990;&#30028;&#27169;&#22411;&#23398;&#20064;&#20043;&#31867;&#30340;&#30417;&#30563;&#23376;&#38382;&#39064;&#20013;&#65292;&#20063;&#19981;&#33021;&#24212;&#29992;&#26089;&#26399;&#20572;&#27490;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#22312;&#19981;&#26029;&#28436;&#21464;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#26410;&#21442;&#19982;&#35757;&#32451;&#30340;&#19968;&#23567;&#37096;&#20998;&#36830;&#32493;&#25910;&#38598;&#30340;&#32463;&#39564;&#19978;&#26816;&#27979;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#26469;&#21160;&#24577;&#35843;&#25972;&#35757;&#32451;&#20013;&#30340;&#26356;&#26032;&#21040;&#25968;&#25454;&#27604;&#29575;&#65288;UTD&#65289;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;DreamerV2&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;DeepMind Control Suite&#21644;Atari $100$k&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;DreamerV2&#20013;&#30340;&#40664;&#35748;&#35774;&#32622;&#30456;&#27604;&#65292;&#36890;&#36807;&#35843;&#25972;UTD&#27604;&#29575;&#26469;&#24179;&#34913;&#27424;&#25311;&#21512;&#19982;&#36807;&#25311;&#21512;&#30340;&#25928;&#26524;&#26356;&#22909;&#65292;&#32780;&#19988;&#20855;&#26377;&#19982;&#24191;&#27867;&#36229;&#21442;&#25968;&#25628;&#32034;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari $100$k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#20998;&#23376;&#22270;&#24418;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20986;&#33021;&#22815;&#31934;&#30830;&#12289;&#24555;&#36895;&#35780;&#20272;&#21644;&#36866;&#29992;&#20110;&#19981;&#21516;&#20998;&#23376;&#30340;&#23494;&#24230;&#39044;&#27979;&#21644;&#31283;&#23450;&#24615;&#25490;&#21517;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#29992;&#20110;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#30340;&#26230;&#20307;&#25490;&#21517;&#24037;&#20855;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#28789;&#27963;&#37096;&#32626;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.10140</link><description>&lt;p&gt;
&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#20013;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Geometric Deep Learning for Molecular Crystal Structure Prediction. (arXiv:2303.10140v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#20998;&#23376;&#22270;&#24418;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#20986;&#33021;&#22815;&#31934;&#30830;&#12289;&#24555;&#36895;&#35780;&#20272;&#21644;&#36866;&#29992;&#20110;&#19981;&#21516;&#20998;&#23376;&#30340;&#23494;&#24230;&#39044;&#27979;&#21644;&#31283;&#23450;&#24615;&#25490;&#21517;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#29992;&#20110;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24320;&#21457;&#30340;&#26230;&#20307;&#25490;&#21517;&#24037;&#20855;&#20855;&#26377;&#20302;&#25104;&#26412;&#12289;&#28789;&#27963;&#37096;&#32626;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20998;&#23376;&#22270;&#24418;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#65292;&#24320;&#21457;&#21644;&#27979;&#35797;&#20102;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#65292;&#21152;&#36895;&#20102;&#20998;&#23376;&#26230;&#20307;&#32467;&#26500;&#25490;&#21517;&#21644;&#26230;&#20307;&#24615;&#36136;&#39044;&#27979;&#12290;&#21033;&#29992;&#22522;&#20110;&#22270;&#24418;&#30340;&#23398;&#20064;&#21644;&#22823;&#22411;&#20998;&#23376;&#26230;&#20307;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#23494;&#24230;&#39044;&#27979;&#21644;&#31283;&#23450;&#24615;&#25490;&#21517;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#31934;&#30830;&#12289;&#35780;&#20272;&#36895;&#24230;&#24555;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#19981;&#21516;&#22823;&#23567;&#21644;&#25104;&#20998;&#30340;&#20998;&#23376;&#12290;&#25105;&#20204;&#30340;&#23494;&#24230;&#39044;&#27979;&#27169;&#22411;MolXtalNet-D&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#22823;&#32780;&#22810;&#26679;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#22343;&#26041;&#26681;&#35823;&#24046;&#23567;&#20110;2%&#12290;&#25105;&#20204;&#30340;&#26230;&#20307;&#25490;&#21517;&#24037;&#20855;MolXtalNet-S&#33021;&#22815;&#27491;&#30830;&#21306;&#20998;&#23454;&#39564;&#26679;&#21697;&#21644;&#21512;&#25104;&#29983;&#25104;&#30340;&#20551;&#26679;&#21697;&#65292;&#24182;&#36890;&#36807;&#23545;&#21073;&#26725;&#32467;&#26500;&#25968;&#25454;&#24211;&#30450;&#27979;&#35797;5&#21644;6&#30340;&#25552;&#20132;&#20998;&#26512;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26032;&#24037;&#20855;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#19988;&#28789;&#27963;&#65292;&#21487;&#20197;&#22312;&#29616;&#26377;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#27969;&#31243;&#20013;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop and test new machine learning strategies for accelerating molecular crystal structure ranking and crystal property prediction using tools from geometric deep learning on molecular graphs. Leveraging developments in graph-based learning and the availability of large molecular crystal datasets, we train models for density prediction and stability ranking which are accurate, fast to evaluate, and applicable to molecules of widely varying size and composition. Our density prediction model, MolXtalNet-D, achieves state of the art performance, with lower than 2% mean absolute error on a large and diverse test dataset. Our crystal ranking tool, MolXtalNet-S, correctly discriminates experimental samples from synthetically generated fakes and is further validated through analysis of the submissions to the Cambridge Structural Database Blind Tests 5 and 6. Our new tools are computationally cheap and flexible enough to be deployed within an existing crystal structure prediction pipelin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2303.10139</link><description>&lt;p&gt;
Distill n' Explain&#65306;&#20351;&#29992;&#31616;&#21333;&#26367;&#20195;&#27169;&#22411;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Distill n' Explain: explaining graph neural networks using simple surrogates. (arXiv:2303.10139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#23398;&#20064;&#31616;&#21333;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#33410;&#28857;&#39044;&#27979;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#25214;&#21040;&#20445;&#25345;&#39044;&#27979;&#30340;&#22270;&#23376;&#32467;&#26500;&#12290;&#36825;&#36890;&#24120;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#30001;&#20110;GNN&#30340;&#22797;&#26434;&#24615;&#65288;&#20363;&#22914;&#65292;&#23618;&#25968;&#65289;&#32780;&#23548;&#33268;&#35299;&#37322;&#30340;&#25104;&#26412;&#19978;&#21319;&#12290;&#22240;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;Distill n' Explain (DnX)&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;DnX&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#26469;&#23398;&#20064;&#26367;&#20195;&#30340;GNN&#12290;&#28982;&#21518;&#65292;DnX&#36890;&#36807;&#35299;&#20915;&#31616;&#21333;&#30340;&#20984;&#35268;&#21010;&#26469;&#25552;&#21462;&#33410;&#28857;&#25110;&#36793;&#32423;&#21035;&#30340;&#35299;&#37322;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;FastDnX&#65292;&#36825;&#26159;DnX&#30340;&#26356;&#24555;&#29256;&#26412;&#65292;&#23427;&#21033;&#29992;&#20102;&#25105;&#20204;&#26367;&#20195;&#27169;&#22411;&#30340;&#32447;&#24615;&#20998;&#35299;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;DnX&#21644;FastDnX&#36890;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GNN&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#36816;&#34892;&#36895;&#24230;&#24555;&#24471;&#22810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#20102;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining node predictions in graph neural networks (GNNs) often boils down to finding graph substructures that preserve predictions. Finding these structures usually implies back-propagating through the GNN, bonding the complexity (e.g., number of layers) of the GNN to the cost of explaining it. This naturally begs the question: Can we break this bond by explaining a simpler surrogate GNN? To answer the question, we propose Distill n' Explain (DnX). First, DnX learns a surrogate GNN via knowledge distillation. Then, DnX extracts node or edge-level explanations by solving a simple convex program. We also propose FastDnX, a faster version of DnX that leverages the linear decomposition of our surrogate model. Experiments show that DnX and FastDnX often outperform state-of-the-art GNN explainers while being orders of magnitude faster. Additionally, we support our empirical findings with theoretical results linking the quality of the surrogate model (i.e., distillation error) to the faith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolWriter&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#29305;&#23450;&#30340;&#31243;&#24207;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36716;&#25442;&#34920;&#26684;&#65292;&#20197;&#25552;&#39640;&#34920;&#26684;&#38382;&#31572;&#65288;TQA&#65289;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#24037;&#20855;&#36890;&#36807;&#29983;&#25104;&#34892;&#36807;&#28388;&#24037;&#20855;&#65292;&#25913;&#36827;&#20102;WikiTableQuestions&#21644;WikiSQL&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#38543;&#30528;&#34920;&#26684;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31243;&#24207;&#21270;&#24037;&#20855;&#19982;&#31070;&#32463;&#32452;&#20214;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.10138</link><description>&lt;p&gt;
&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#38382;&#39064;&#29305;&#23450;&#24037;&#20855;&#21512;&#25104;&#65306;&#29983;&#25104;&#12289;&#36716;&#25442;&#12289;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Generate, Transform, Answer: Question Specific Tool Synthesis for Tabular Data. (arXiv:2303.10138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToolWriter&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#29305;&#23450;&#30340;&#31243;&#24207;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36716;&#25442;&#34920;&#26684;&#65292;&#20197;&#25552;&#39640;&#34920;&#26684;&#38382;&#31572;&#65288;TQA&#65289;&#30340;&#24615;&#33021;&#12290;&#36825;&#20010;&#24037;&#20855;&#36890;&#36807;&#29983;&#25104;&#34892;&#36807;&#28388;&#24037;&#20855;&#65292;&#25913;&#36827;&#20102;WikiTableQuestions&#21644;WikiSQL&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#38543;&#30528;&#34920;&#26684;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#20256;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#34920;&#26684;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#20449;&#24687;&#20002;&#22833;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31243;&#24207;&#21270;&#24037;&#20855;&#19982;&#31070;&#32463;&#32452;&#20214;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#38382;&#31572;&#65288;TQA&#65289;&#23545;&#20110;&#31070;&#32463;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33258;&#28982;&#35821;&#35328;&#19982;&#22823;&#37327;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#32852;&#21512;&#25512;&#29702;&#12290;&#19982;&#20154;&#31867;&#20351;&#29992;&#31243;&#24207;&#21270;&#24037;&#20855;&#65288;&#22914;&#36807;&#28388;&#22120;&#65289;&#22312;&#22788;&#29702;&#20043;&#21069;&#36716;&#25442;&#25968;&#25454;&#19981;&#21516;&#65292;TQA&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#22788;&#29702;&#34920;&#26684;&#65292;&#38543;&#30528;&#34920;&#26684;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#20250;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ToolWriter&#65292;&#29992;&#20110;&#29983;&#25104;&#26597;&#35810;&#29305;&#23450;&#30340;&#31243;&#24207;&#65292;&#24182;&#22312;&#38656;&#35201;&#26102;&#26816;&#27979;&#20309;&#26102;&#23558;&#20854;&#24212;&#29992;&#20110;&#36716;&#25442;&#34920;&#26684;&#65292;&#24182;&#23558;&#20854;&#19982;TQA&#27169;&#22411;&#30340;&#33021;&#21147;&#23545;&#40784;&#12290;&#23558;ToolWriter&#19987;&#27880;&#20110;&#29983;&#25104;&#34892;&#36807;&#28388;&#24037;&#20855;&#65292;&#25913;&#36827;&#20102;WikiTableQuestions&#21644;WikiSQL&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#20986;&#29616;&#22312;&#38271;&#34920;&#26684;&#19978;&#12290;&#36890;&#36807;&#35843;&#26597;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23558;&#31243;&#24207;&#21270;&#24037;&#20855;&#19982;&#31070;&#32463;&#32452;&#20214;&#30456;&#32467;&#21512;&#20197;&#25805;&#20316;&#22823;&#37327;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular question answering (TQA) presents a challenging setting for neural systems by requiring joint reasoning of natural language with large amounts of semi-structured data. Unlike humans who use programmatic tools like filters to transform data before processing, language models in TQA process tables directly, resulting in information loss as table size increases. In this paper we propose ToolWriter to generate query specific programs and detect when to apply them to transform tables and align them with the TQA model's capabilities. Focusing ToolWriter to generate row-filtering tools improves the state-of-the-art for WikiTableQuestions and WikiSQL with the most performance gained on long tables. By investigating headroom, our work highlights the broader potential for programmatic tools combined with neural components to manipulate large amounts of structured data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#21147;&#26144;&#23556;&#24202;&#22443;&#22270;&#20687;&#30340;&#20307;&#37325;&#25552;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#26469;&#25552;&#21462;&#28145;&#24230;&#29305;&#24449;&#21644;&#23039;&#21183;&#29305;&#24449;&#20197;&#21450;&#25366;&#25496;&#23039;&#21183;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.10136</link><description>&lt;p&gt;
MassNet&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#24352;&#21387;&#21147;&#22270;&#20687;&#20307;&#37325;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MassNet: A Deep Learning Approach for Body Weight Extraction from A Single Pressure Image. (arXiv:2303.10136v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21387;&#21147;&#26144;&#23556;&#24202;&#22443;&#22270;&#20687;&#30340;&#20307;&#37325;&#25552;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#26469;&#25552;&#21462;&#28145;&#24230;&#29305;&#24449;&#21644;&#23039;&#21183;&#29305;&#24449;&#20197;&#21450;&#25366;&#25496;&#23039;&#21183;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#37325;&#37327;&#20316;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#29983;&#29702;&#29305;&#24449;&#65292;&#22312;&#36523;&#20307;&#31649;&#29702;&#12289;&#24247;&#22797;&#21644;&#38024;&#23545;&#29305;&#23450;&#24739;&#32773;&#30340;&#33647;&#29289;&#21058;&#37327;&#31561;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#20855;&#26377;&#30456;&#24403;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992; 2D/3D&#12289;&#28145;&#24230;&#25110;&#32418;&#22806;&#22270;&#20687;&#65292;&#38754;&#20020;&#30528;&#29031;&#26126;&#12289;&#36974;&#25377;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#21387;&#21147;&#26144;&#23556;&#24202;&#22443;&#26159;&#19968;&#31181;&#38750;&#20405;&#20837;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#33719;&#21462;&#19982;&#36538;&#30528;&#30340;&#20154;&#30340;&#20307;&#37325;&#24378;&#30456;&#20851;&#30340;&#24202;&#38754;&#21387;&#21147;&#20998;&#24067;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21253;&#25324;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#20998;&#21035;&#25552;&#21462;&#28145;&#24230;&#29305;&#24449;&#21644;&#23039;&#21183;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#23558;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#19982;&#28145;&#24230;&#29305;&#24449;&#20998;&#25903;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#24110;&#21161;&#25366;&#25496;&#27599;&#19968;&#20010;&#21333;&#29420;&#20027;&#39064;&#30340;&#19981;&#21516;&#23039;&#21183;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#22240;&#32032;&#12290;&#28982;&#21518;&#23558;&#20004;&#32452;&#29305;&#24449;&#36830;&#25509;&#36215;&#26469;&#65292;&#23436;&#25104;&#20307;&#37325;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Body weight, as an essential physiological trait, is of considerable significance in many applications like body management, rehabilitation, and drug dosing for patient-specific treatments. Previous works on the body weight estimation task are mainly vision-based, using 2D/3D, depth, or infrared images, facing problems in illumination, occlusions, and especially privacy issues. The pressure mapping mattress is a non-invasive and privacy-preserving tool to obtain the pressure distribution image over the bed surface, which strongly correlates with the body weight of the lying person. To extract the body weight from this image, we propose a deep learning-based model, including a dual-branch network to extract the deep features and pose features respectively. A contrastive learning module is also combined with the deep-feature branch to help mine the mutual factors across different postures of every single subject. The two groups of features are then concatenated for the body weight regres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2303.10135</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#25928;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Efficient and Feasible Robotic Assembly Sequence Planning via Graph Representation Learning. (arXiv:2303.10135v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;GRACE&#27169;&#22411;&#21487;&#20197;&#20174;&#35013;&#37197;&#22270;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#39044;&#27979;&#21487;&#34892;&#30340;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#20154;&#35013;&#37197;&#24207;&#21015;&#35268;&#21010;&#65288;RASP&#65289;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#20195;&#21046;&#36896;&#19994;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#24212;&#21147;&#65292;&#38543;&#30528;&#23545;&#26356;&#22823;&#37327;&#21270;&#29983;&#20135;&#38656;&#27714;&#30340;&#19981;&#26029;&#22686;&#38271;&#12290;&#23454;&#29616;&#36825;&#31181;&#33258;&#21160;&#21270;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#20174;&#19981;&#26029;&#22686;&#21152;&#30340;&#28508;&#22312;&#24207;&#21015;&#20013;&#39640;&#25928;&#22320;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#65292;&#36827;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#35013;&#37197;&#36824;&#38656;&#35201;&#25104;&#26412;&#26114;&#36149;&#30340;&#21487;&#34892;&#24615;&#26816;&#26597;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#25324;&#20135;&#21697;&#35013;&#37197;&#22270;&#30340;&#22270;&#24418;&#26041;&#27861;&#21644;&#19968;&#20010;&#21517;&#20026;GRACE&#30340;&#31574;&#30053;&#26550;&#26500;&#65292;&#29992;&#20110;&#35013;&#37197;&#24207;&#21015;&#29983;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;GRACE&#20174;&#22270;&#24418;&#36755;&#20837;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#24182;&#36880;&#27493;&#39044;&#27979;&#35013;&#37197;&#24207;&#21015;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26681;&#25454;&#22312;&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#39044;&#27979;&#38109;&#22411;&#26448;&#20135;&#21697;&#21464;&#20307;&#30340;&#21487;&#34892;&#35013;&#37197;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Robotic Assembly Sequence Planning (RASP) can significantly improve productivity and resilience in modern manufacturing along with the growing need for greater product customization. One of the main challenges in realizing such automation resides in efficiently finding solutions from a growing number of potential sequences for increasingly complex assemblies. Besides, costly feasibility checks are always required for the robotic system. To address this, we propose a holistic graphical approach including a graph representation called Assembly Graph for product assemblies and a policy architecture, Graph Assembly Processing Network, dubbed GRACE for assembly sequence generation. Secondly, we use GRACE to extract meaningful information from the graph input and predict assembly sequences in a step-by-step manner. In experiments, we show that our approach can predict feasible assembly sequences across product variants of aluminum profiles based on data collected in simulation of a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35843;&#26597;&#20102;56&#39033;&#19982;&#36719;&#20214;&#24320;&#21457;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#24615;&#21035;&#20195;&#35789;&#19982;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26126;&#26174;&#19981;&#21516;&#12290;&#20854;&#20013;&#65292;&#21482;&#26377;6%&#30340;&#38656;&#27714;&#25910;&#38598;&#20219;&#21153;&#19982;&#20195;&#35789;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#27979;&#35797;&#20219;&#21153;&#21017;&#22312;100%&#30340;&#24773;&#20917;&#19979;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#24110;&#21161;&#20182;&#20154;&#30340;&#20219;&#21153;&#26377;91%&#30340;&#30456;&#20851;&#24615;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2303.10131</link><description>&lt;p&gt;
&#22905;&#25910;&#38598;&#38656;&#27714;&#65292;&#20182;&#36827;&#34892;&#27979;&#35797;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36719;&#20214;&#24037;&#31243;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models. (arXiv:2303.10131v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35843;&#26597;&#20102;56&#39033;&#19982;&#36719;&#20214;&#24320;&#21457;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21457;&#29616;&#24615;&#21035;&#20195;&#35789;&#19982;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#26126;&#26174;&#19981;&#21516;&#12290;&#20854;&#20013;&#65292;&#21482;&#26377;6%&#30340;&#38656;&#27714;&#25910;&#38598;&#20219;&#21153;&#19982;&#20195;&#35789;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#27979;&#35797;&#20219;&#21153;&#21017;&#22312;100%&#30340;&#24773;&#20917;&#19979;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#24110;&#21161;&#20182;&#20154;&#30340;&#20219;&#21153;&#26377;91%&#30340;&#30456;&#20851;&#24615;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#27604;&#22914;&#23558;&#25216;&#26415;&#35282;&#33394;&#19982;&#30007;&#24615;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#65292;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#23427;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#35843;&#26597;&#19982;&#36719;&#20214;&#24320;&#21457;&#30456;&#20851;&#30340;56&#39033;&#20219;&#21153;&#65288;&#22914;&#20998;&#37197;GitHub&#38382;&#39064;&#21644;&#27979;&#35797;&#65289;&#65292;&#20197;&#20102;&#35299;&#23884;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#25152;&#20135;&#29983;&#30340;&#24433;&#21709;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20174;&#33521;&#35821;&#31995;&#32479;&#22320;&#32763;&#35793;&#25104;&#26080;&#24615;&#21035;&#35821;&#35328;&#65292;&#28982;&#21518;&#20877;&#32763;&#35793;&#22238;&#33521;&#35821;&#65292;&#24182;&#35843;&#26597;&#19982;&#27599;&#20010;&#20219;&#21153;&#30456;&#20851;&#30340;&#20195;&#35789;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#25490;&#21015;&#20013;&#21453;&#22797;&#32763;&#35793;&#27599;&#20010;&#20219;&#21153;100&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19981;&#21516;&#20219;&#21153;&#19982;&#24615;&#21035;&#20195;&#35789;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21482;&#26377;6%&#30340;&#38656;&#27714;&#25910;&#38598;&#20219;&#21153;&#19982;&#20195;&#35789;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#27979;&#35797;&#20219;&#21153;&#21017;&#22312;100%&#30340;&#24773;&#20917;&#19979;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#12290;&#27492;&#22806;&#65292;&#28041;&#21450;&#24110;&#21161;&#20182;&#20154;&#30340;&#20219;&#21153;&#26377;91%&#30340;&#30456;&#20851;&#24615;&#19982;&#8220;&#20182;&#8221;&#30456;&#20851;&#32852;&#65292;&#32780;&#25191;&#34892;&#21516;&#26679;&#20219;&#21153;&#30340;&#22899;&#24615;&#21017;&#24456;&#23481;&#26131;&#34987;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning GitHub issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, requirements elicitation was associated with the pronoun "he" in only 6% of cases, while testing was associated with "he" in 100% of cases. Additionally, tasks related to helping others had a 91% association with "he" while the same association fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2303.10112</link><description>&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;:&#32508;&#36848;&#21644;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Temporal Data: An Overview and New Perspectives. (arXiv:2303.10112v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#36827;&#34892;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25968;&#25454;&#20195;&#34920;&#30528;&#22797;&#26434;&#31995;&#32479;&#30340;&#26102;&#38388;&#39034;&#24207;&#35266;&#27979;&#65292;&#21487;&#20197;&#34987;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#29983;&#25104;&#65292;&#20363;&#22914;&#24037;&#19994;&#12289;&#21307;&#30103;&#21644;&#37329;&#34701;&#12290;&#20998;&#26512;&#36825;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#20854;&#20013;&#65292;&#20174;&#26102;&#38388;&#25968;&#25454;&#20013;&#23398;&#20064;&#22240;&#26524;&#20851;&#31995;&#30340;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26377;&#36259;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#24037;&#20316;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#25968;&#25454;&#26159;&#21542;&#34987;&#26657;&#20934;&#26469;&#20998;&#20026;&#20004;&#20010;&#39640;&#24230;&#30456;&#20851;&#30340;&#31867;&#21035;&#65292;&#21363;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#21644;&#20107;&#20214;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#35843;&#26597;&#20165;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#22240;&#26524;&#21457;&#29616;&#65292;&#24573;&#30053;&#20102;&#31532;&#20108;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35828;&#26126;&#20102;&#36825;&#20004;&#20010;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32508;&#21512;&#32771;&#34385;&#36825;&#20004;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal data, representing chronological observations of complex systems, has always been a typical data structure that can be widely generated by many domains, such as industry, medicine and finance. Analyzing this type of data is extremely valuable for various applications. Thus, different temporal data analysis tasks, eg, classification, clustering and prediction, have been proposed in the past decades. Among them, causal discovery, learning the causal relations from temporal data, is considered an interesting yet critical task and has attracted much research attention. Existing casual discovery works can be divided into two highly correlated categories according to whether the temporal data is calibrated, ie, multivariate time series casual discovery, and event sequence casual discovery. However, most previous surveys are only focused on the time series casual discovery and ignore the second category. In this paper, we specify the correlation between the two categories and provide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#22270;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#22686;&#24378;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20004;&#20010;&#26032;&#30446;&#26631;&#36827;&#34892;&#21435;&#22122;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#27604;14&#31181;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.10108</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#26631;&#31614;&#22270;&#25968;&#25454;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data-Centric Learning from Unlabeled Graphs with Diffusion Model. (arXiv:2303.10108v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26080;&#26631;&#31614;&#22270;&#20013;&#25552;&#21462;&#30693;&#35782;&#24182;&#22686;&#24378;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#20004;&#20010;&#26032;&#30446;&#26631;&#36827;&#34892;&#21435;&#22122;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#25928;&#26524;&#27604;14&#31181;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#21644;&#20247;&#22810;&#12290;&#34429;&#28982;&#27599;&#20010;&#20219;&#21153;&#21482;&#25552;&#20379;&#23569;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#65292;&#20294;&#26080;&#26631;&#31614;&#22270;&#24050;&#20174;&#21508;&#31181;&#28192;&#36947;&#24182;&#22823;&#35268;&#27169;&#25910;&#38598;&#12290;&#20256;&#32479;&#26041;&#27861;&#26159;&#22312;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#26080;&#26631;&#31614;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#30693;&#35782;&#21487;&#33021;&#26080;&#27861;&#19982;&#39044;&#27979;&#20219;&#21153;&#38656;&#35201;&#30340;&#30693;&#35782;&#30456;&#21563;&#21512;&#25110;&#26377;&#26102;&#20250;&#20135;&#29983;&#20914;&#31361;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#22270;&#20013;&#25552;&#21462;&#30693;&#35782;&#20316;&#20026;&#29305;&#23450;&#26377;&#29992;&#25968;&#25454;&#28857;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#27599;&#20010;&#23646;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#26080;&#26631;&#31614;&#22270;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#26032;&#30340;&#30446;&#26631;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#22270;&#24418;&#31034;&#20363;&#21450;&#20854;&#26631;&#31614;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#27604;14&#31181;&#29616;&#26377;&#26041;&#27861;&#22312;fi&#19978;&#34920;&#29616;&#26174;&#30528;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph property prediction tasks are important and numerous. While each task offers a small size of labeled examples, unlabeled graphs have been collected from various sources and at a large scale. A conventional approach is training a model with the unlabeled graphs on self-supervised tasks and then fine-tuning the model on the prediction tasks. However, the self-supervised task knowledge could not be aligned or sometimes conflicted with what the predictions needed. In this paper, we propose to extract the knowledge underlying the large set of unlabeled graphs as a specific set of useful data points to augment each property prediction model. We use a diffusion model to fully utilize the unlabeled graphs and design two new objectives to guide the model's denoising process with each task's labeled data to generate task-specific graph examples and their labels. Experiments demonstrate that our data-centric approach performs significantly better than fourteen existing various methods on fi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.10093</link><description>&lt;p&gt;
&#25552;&#39640;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Role of Context in Region-Word Alignment for Object Detection. (arXiv:2303.10093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#19978;&#19979;&#25991;&#22312;&#30446;&#26631;&#26816;&#27979;&#21306;&#22495;-&#35789;&#23545;&#40784;&#20013;&#20316;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#25552;&#39640;&#20102;&#23646;&#24615;&#30340;&#20316;&#29992;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#23398;&#20064;&#22270;&#20687;-&#26631;&#27880;&#37197;&#23545;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#21306;&#22495;-&#35789;&#23545;&#40784;&#65292;&#25512;&#21160;&#20102;&#24320;&#25918;&#35789;&#27719;&#30446;&#26631;&#26816;&#27979;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21306;&#22495;-&#35789;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20165;&#38024;&#23545;&#30446;&#26631;&#21517;&#35789;&#22312;&#26816;&#27979;&#20013;&#20351;&#29992;&#65292;&#20854;&#20182;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23646;&#24615;&#65292;&#23545;&#26816;&#27979;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#19979;&#28216;&#30446;&#26631;&#26816;&#27979;&#65292;&#24182;&#25552;&#35758;&#22686;&#24378;&#19978;&#19979;&#25991;&#30340;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#31574;&#30053;&#24615;&#22320;&#23558;&#25509;&#22320;&#39044;&#35757;&#32451;&#30446;&#26631;&#24773;&#22659;&#21270;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23646;&#24615;&#20316;&#20026;&#29305;&#21035;&#26377;&#29992;&#30340;&#30446;&#26631;&#19978;&#19979;&#25991;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#36127;&#37319;&#26679;&#31574;&#30053;&#65292;&#20197;&#22686;&#21152;&#23545;&#23427;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#19982;&#21306;&#22495;-&#35789;&#39044;&#35757;&#32451;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21319;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25991;&#26412;-&#21306;&#22495;&#21487;&#35270;&#21270;&#26174;&#31034;&#23646;&#24615;&#25935;&#24863;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language pretraining to learn a fine-grained, region-word alignment between image-caption pairs has propelled progress in open-vocabulary object detection. We observe that region-word alignment methods are typically used in detection with respect to only object nouns, and the impact of other rich context in captions, such as attributes, is unclear. In this study, we explore how language context affects downstream object detection and propose to enhance the role of context. In particular, we show how to strategically contextualize the grounding pretraining objective for improved alignment. We further hone in on attributes as especially useful object context and propose a novel adjective and noun-based negative sampling strategy for increasing their focus in contrastive learning. Overall, our methods enhance object detection when compared to the state-of-the-art in region-word pretraining. We also highlight the fine-grained utility of an attribute-sensitive model through text-regi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#65292;&#22522;&#20110;&#32422;&#26463;&#20256;&#36755;&#24230;&#37327;&#65292;&#21033;&#29992;&#32463;&#39564;&#20284;&#28982;&#32467;&#21512;&#20808;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#40065;&#26834;&#25512;&#26029;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#20013;&#24515;&#20998;&#24067;&#21442;&#25968;&#30340;&#25512;&#26029;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.10085</link><description>&lt;p&gt;
&#36890;&#36807;&#32422;&#26463;&#20256;&#36755;&#24230;&#37327;&#23454;&#29616;&#40065;&#26834;&#27010;&#29575;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Robust probabilistic inference via a constrained transport metric. (arXiv:2303.10085v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#40065;&#26834;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#65292;&#22522;&#20110;&#32422;&#26463;&#20256;&#36755;&#24230;&#37327;&#65292;&#21033;&#29992;&#32463;&#39564;&#20284;&#28982;&#32467;&#21512;&#20808;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#40065;&#26834;&#25512;&#26029;&#38382;&#39064;&#65292;&#23454;&#29616;&#23545;&#20013;&#24515;&#20998;&#24067;&#21442;&#25968;&#30340;&#25512;&#26029;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#36125;&#21494;&#26031;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#19988;&#24120;&#24120;&#19981;&#21487;&#35299;&#37322;&#30340;&#21442;&#25968;&#27169;&#22411;&#30340;&#26497;&#38480;&#26500;&#24314;&#32780;&#25104;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20542;&#26012;&#30340;&#32463;&#39564;&#20284;&#28982;&#30340;&#26500;&#36896;&#65292;&#32467;&#21512;&#19968;&#31181;&#26032;&#22411;&#30340;Wasserstein&#24230;&#37327;&#65292;&#38598;&#20013;&#22312;&#29305;&#23450;&#21442;&#25968;&#26063;&#38468;&#36817;&#65292;&#28982;&#21518;&#32467;&#21512;&#27169;&#22411;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#40065;&#26834;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#40065;&#26834;&#25512;&#26029;&#38382;&#39064;&#20013;&#25214;&#21040;&#24212;&#29992;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#23384;&#22312;&#24322;&#24120;&#20540;&#30340;&#24773;&#20917;&#19979;&#23545;&#19982;&#20013;&#24515;&#20998;&#24067;&#30456;&#20851;&#30340;&#21442;&#25968;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20256;&#36755;&#24230;&#37327;&#20855;&#26377;&#24456;&#39640;&#30340;&#35745;&#31639;&#31616;&#20415;&#24615;&#65292;&#21033;&#29992;&#20102;&#31163;&#25955;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#30340;Sinkhorn&#27491;&#21017;&#21270;&#65292;&#24182;&#26412;&#36136;&#19978;&#21487;&#20197;&#24182;&#34892;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible Bayesian models are typically constructed using limits of large parametric models with a multitude of parameters that are often uninterpretable. In this article, we offer a novel alternative by constructing an exponentially tilted empirical likelihood carefully designed to concentrate near a parametric family of distributions of choice with respect to a novel variant of the Wasserstein metric, which is then combined with a prior distribution on model parameters to obtain a robustified posterior. The proposed approach finds applications in a wide variety of robust inference problems, where we intend to perform inference on the parameters associated with the centering distribution in presence of outliers. Our proposed transport metric enjoys great computational simplicity, exploiting the Sinkhorn regularization for discrete optimal transport problems, and being inherently parallelizable. We demonstrate superior performance of our methodology when compared against state-of-the-ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#24230;&#35843;&#35856;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#31946;&#39046;&#22495;&#26377;&#25928;&#22320;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.10078</link><description>&lt;p&gt;
&#27169;&#31946;&#35843;&#35856;&#65306;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fuzziness-tuned: Improving the Transferability of Adversarial Examples. (arXiv:2303.10078v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#24230;&#35843;&#35856;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#36890;&#36807;&#20943;&#23569;&#27169;&#31946;&#39046;&#22495;&#26377;&#25928;&#22320;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#20855;&#26377;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25239;&#25915;&#20987;&#30340;&#21457;&#23637;&#65292;&#23545;&#25239;&#26679;&#26412;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#38024;&#23545;&#25552;&#39640;&#23545;&#25239;&#26679;&#26412;&#21487;&#36801;&#31227;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#20570;&#20986;&#20102;&#21487;&#35266;&#30340;&#21162;&#21147;&#65292;&#20294;&#22312;&#20302;&#25915;&#20987;&#21147;&#24230;(&#20363;&#22914;&#65292;&#25915;&#20987;&#24378;&#24230;&#1013;=8/255)&#65292;&#22522;&#20110;&#36801;&#31227;&#30340;&#25915;&#20987;&#22312;&#20195;&#29702;&#27169;&#22411;&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#36828;&#39640;&#20110;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#26412;&#25991;&#39318;&#20808;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#20195;&#29702;&#27169;&#22411;&#21644;&#21463;&#23475;&#32773;&#27169;&#22411;&#20043;&#38388;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24040;&#22823;&#24046;&#24322;&#26159;&#30001;&#19968;&#20010;&#29305;&#27530;&#21306;&#22495;&#65288;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#31216;&#20043;&#20026;&#27169;&#31946;&#39046;&#22495;&#65289;&#30340;&#23384;&#22312;&#23548;&#33268;&#30340;&#65292;&#35813;&#21306;&#22495;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#34987;&#20195;&#29702;&#27169;&#22411;&#38169;&#35823;&#20998;&#31867;&#65292;&#32780;&#34987;&#21463;&#23475;&#32773;&#27169;&#22411;&#27491;&#30830;&#20998;&#31867;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#28040;&#38500;&#36825;&#31181;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#24040;&#22823;&#24046;&#24322;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#24230;&#35843;&#35856;&#26041;&#27861;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#27169;&#31946;&#24230;&#8221;&#30340;&#26032;&#21442;&#25968;&#26469;&#25511;&#21046;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#25200;&#21160;&#31243;&#24230;&#65292;&#36825;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#27169;&#31946;&#39046;&#22495;&#24182;&#20419;&#36827;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#22522;&#20110;&#36801;&#31227;&#30340;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of adversarial attacks, adversairal examples have been widely used to enhance the robustness of the training models on deep neural networks. Although considerable efforts of adversarial attacks on improving the transferability of adversarial examples have been developed, the attack success rate of the transfer-based attacks on the surrogate model is much higher than that on victim model under the low attack strength (e.g., the attack strength $\epsilon=8/255$). In this paper, we first systematically investigated this issue and found that the enormous difference of attack success rates between the surrogate model and victim model is caused by the existence of a special area (known as fuzzy domain in our paper), in which the adversarial examples in the area are classified wrongly by the surrogate model while correctly by the victim model. Then, to eliminate such enormous difference of attack success rates for improving the transferability of generated adversarial exa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#21512;&#20316;&#23398;&#20064;&#20013;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21512;&#25104;&#30340;ETF&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#25152;&#26377;&#23458;&#25143;&#31471;&#33021;&#22815;&#23398;&#20064;&#21040;&#32479;&#19968;&#30340;&#26368;&#20248;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.10058</link><description>&lt;p&gt;
&#19981;&#24597;&#20998;&#31867;&#22120;&#20559;&#24046;&#65306;&#20197;&#31070;&#32463;&#23849;&#28291;&#20026;&#28789;&#24863;&#30340;&#21512;&#20316;&#23398;&#20064;&#20013;&#20351;&#29992;&#21512;&#25104;&#21644;&#22266;&#23450;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier. (arXiv:2303.10058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#35299;&#20915;&#21512;&#20316;&#23398;&#20064;&#20013;&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#21512;&#25104;&#30340;ETF&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#25152;&#26377;&#23458;&#25143;&#31471;&#33021;&#22815;&#23398;&#20064;&#21040;&#32479;&#19968;&#30340;&#26368;&#20248;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#26500;&#24615;&#26159;&#22256;&#25200;&#21512;&#20316;&#23398;&#20064;&#24615;&#33021;&#30340;&#20869;&#22312;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#26412;&#22320;&#27169;&#22411;&#30340;&#20559;&#32622;&#20998;&#31867;&#22120;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#20197;&#21069;&#30340;&#23581;&#35797;&#21033;&#29992;FL&#35757;&#32451;&#21518;&#36827;&#34892;&#20998;&#31867;&#22120;&#26657;&#20934;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#26410;&#33021;&#25913;&#21892;&#35757;&#32451;&#26102;&#20998;&#31867;&#22120;&#20559;&#24046;&#23548;&#33268;&#30340;&#24046;&#21155;&#29305;&#24449;&#34920;&#31034;&#12290;&#35299;&#20915;FL&#20013;&#20998;&#31867;&#22120;&#20559;&#24046;&#22256;&#22659;&#38656;&#35201;&#20805;&#20998;&#29702;&#35299;&#20998;&#31867;&#22120;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#31070;&#32463;&#23849;&#28291;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#22312;&#23436;&#32654;&#30340;&#35757;&#32451;&#22330;&#26223;&#19979;&#65292;&#20998;&#31867;&#22120;&#21644;&#29305;&#24449;&#21407;&#22411;&#23849;&#28291;&#20026;&#19968;&#31181;&#31216;&#20026;simplex equiangular tight frame(ETF)&#30340;&#26368;&#20248;&#32467;&#26500;&#12290;&#22522;&#20110;&#36825;&#31181;&#31070;&#32463;&#23849;&#28291;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;FL&#20998;&#31867;&#22120;&#20559;&#24046;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#21512;&#25104;&#21644;&#22266;&#23450;&#30340;ETF&#20998;&#31867;&#22120;&#12290;&#26368;&#20248;&#20998;&#31867;&#22120;&#32467;&#26500;&#20351;&#24471;&#25152;&#26377;&#23458;&#25143;&#31471;&#29978;&#33267;&#22312;&#26497;&#31471;&#24322;&#26500;&#25968;&#25454;&#19979;&#20063;&#33021;&#23398;&#21040;&#32479;&#19968;&#30340;&#21644;&#26368;&#20248;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity is an inherent challenge that hinders the performance of federated learning (FL). Recent studies have identified the biased classifiers of local models as the key bottleneck. Previous attempts have used classifier calibration after FL training, but this approach falls short in improving the poor feature representations caused by training-time classifier biases. Resolving the classifier bias dilemma in FL requires a full understanding of the mechanisms behind the classifier. Recent advances in neural collapse have shown that the classifiers and feature prototypes under perfect training scenarios collapse into an optimal structure called simplex equiangular tight frame (ETF). Building on this neural collapse insight, we propose a solution to the FL's classifier bias problem by utilizing a synthetic and fixed ETF classifier during training. The optimal classifier structure enables all clients to learn unified and optimal feature representations even under extremely hete
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26377;&#25928;&#22320;&#20272;&#35745;&#25104;&#20687;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102; PET &#25104;&#20687;&#21442;&#25968;&#20272;&#35745;&#30340;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2303.10057</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21518;&#39564;&#20272;&#35745;&#65306; PET &#21160;&#24577;&#27169;&#22411;&#20013;&#20998;&#21306;&#24314;&#27169;&#30340;&#27169;&#25311;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Posterior Estimation Using Deep Learning: A Simulation Study of Compartmental Modeling in Dynamic PET. (arXiv:2303.10057v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26377;&#25928;&#22320;&#20272;&#35745;&#25104;&#20687;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102; PET &#25104;&#20687;&#21442;&#25968;&#20272;&#35745;&#30340;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#21487;&#20026;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#34987;&#35270;&#20026;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26377;&#25928;&#22320;&#20272;&#35745;&#25104;&#20687;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#36825;&#20123;&#21518;&#39564;&#20998;&#24067;&#21487;&#20197;&#29992;&#26469;&#25512;&#23548;&#20986;&#26368;&#21487;&#33021;&#30340;&#21442;&#25968;&#20197;&#21450;&#23427;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110; PET &#21160;&#24577;&#33041;&#25104;&#20687;&#27169;&#25311;&#30740;&#31350;&#20013;&#65292;&#20351;&#29992;&#19968;&#20010;&#22522;&#20110;&#21442;&#32771;&#21306;&#22495;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: In medical imaging, images are usually treated as deterministic, while their uncertainties are largely underexplored. Purpose: This work aims at using deep learning to efficiently estimate posterior distributions of imaging parameters, which in turn can be used to derive the most probable parameters as well as their uncertainties. Methods: Our deep learning-based approaches are based on a variational Bayesian inference framework, which is implemented using two different deep neural networks based on conditional variational auto-encoder (CVAE), CVAE-dual-encoder and CVAE-dual-decoder. The conventional CVAE framework, i.e., CVAE-vanilla, can be regarded as a simplified case of these two neural networks. We applied these approaches to a simulation study of dynamic brain PET imaging using a reference region-based kinetic model. Results: In the simulation study, we estimated posterior distributions of PET kinetic parameters given a measurement of time-activity curve. Our propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#31574;&#30053;&#36845;&#20195;&#26426;&#21046;&#30340;&#32676;&#20307;&#36816;&#21160;&#25511;&#21046;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#19979;&#24341;&#23548;&#20195;&#29702;&#20154;&#36981;&#24490;&#21629;&#20196;&#29983;&#25104;&#22120;&#65292;&#24182;&#22312;&#32447;&#35843;&#25972;&#25351;&#23548;&#31574;&#30053;&#20197;&#36866;&#24212;&#23454;&#26102;&#21160;&#24577;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2303.10035</link><description>&lt;p&gt;
&#32676;&#20307;&#36816;&#21160;&#25511;&#21046;&#30340;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Iteration Approach for Flock Motion Control. (arXiv:2303.10035v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#31574;&#30053;&#36845;&#20195;&#26426;&#21046;&#30340;&#32676;&#20307;&#36816;&#21160;&#25511;&#21046;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26102;&#38388;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#19979;&#24341;&#23548;&#20195;&#29702;&#20154;&#36981;&#24490;&#21629;&#20196;&#29983;&#25104;&#22120;&#65292;&#24182;&#22312;&#32447;&#35843;&#25972;&#25351;&#23548;&#31574;&#30053;&#20197;&#36866;&#24212;&#23454;&#26102;&#21160;&#24577;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#36816;&#21160;&#25511;&#21046;&#26088;&#22312;&#31649;&#29702;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#26412;&#22320;&#21644;&#22242;&#38431;&#30446;&#26631;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#20914;&#31361;&#12290;&#25972;&#20010;&#25511;&#21046;&#36807;&#31243;&#25351;&#23548;&#20195;&#29702;&#65292;&#24182;&#30417;&#25511;&#32676;&#20307;&#20957;&#32858;&#24615;&#21644;&#23450;&#20301;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#19982;&#32676;&#20307;&#21160;&#24577;&#21644;&#24418;&#25104;&#30456;&#20851;&#30340;&#26410;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#22522;&#30784;&#26426;&#21046;&#30340;&#25928;&#26524;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21508;&#31181;&#25511;&#21046;&#35774;&#35745;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#23427;&#20204;&#33021;&#22815;&#22810;&#24555;&#22320;&#36866;&#24212;&#23454;&#26102;&#19981;&#21516;&#30340;&#21160;&#24577;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26080;&#27169;&#22411;&#31574;&#30053;&#36845;&#20195;&#26426;&#21046;&#65292;&#20197;&#24341;&#23548;&#19968;&#32676;&#20195;&#29702;&#20154;&#36981;&#24490;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22270;&#24418;&#25299;&#25169;&#19979;&#30340;&#29420;&#31435;&#21629;&#20196;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#20301;&#32622;&#30456;&#37051;&#20381;&#36182;&#20989;&#25968;&#30830;&#23450;&#20219;&#24847;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#36830;&#25509;&#24378;&#24230;&#25110;&#22270;&#36793;&#32536;&#26435;&#37325;&#12290;&#37319;&#29992;&#22312;&#32447;&#36882;&#24402;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#26469;&#35843;&#25972;&#25351;&#23548;&#31574;&#30053;&#65292;&#32780;&#19981;&#24517;&#20102;&#35299;&#20195;&#29702;&#25110;&#21629;&#20196;&#29983;&#25104;&#22120;&#30340;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The flocking motion control is concerned with managing the possible conflicts between local and team objectives of multi-agent systems. The overall control process guides the agents while monitoring the flock-cohesiveness and localization. The underlying mechanisms may degrade due to overlooking the unmodeled uncertainties associated with the flock dynamics and formation. On another side, the efficiencies of the various control designs rely on how quickly they can adapt to different dynamic situations in real-time. An online model-free policy iteration mechanism is developed here to guide a flock of agents to follow an independent command generator over a time-varying graph topology. The strength of connectivity between any two agents or the graph edge weight is decided using a position adjacency dependent function. An online recursive least squares approach is adopted to tune the guidance strategies without knowing the dynamics of the agents or those of the command generator. It is co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#26680;&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#31283;&#20581;&#24615;&#36793;&#30028;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#38543;&#26426;&#22122;&#22768;&#65292;&#21487;&#20197;&#22312;&#32500;&#24230;&#26080;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#19979;&#65292;&#22312;&#38543;&#26426;&#22122;&#22768;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25200;&#21160;&#19979;&#31283;&#23450;&#22320;&#24674;&#22797;&#20004;&#20010;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2303.10030</link><description>&lt;p&gt;
&#38543;&#26426;&#30450;&#21453;&#21367;&#31215;&#22312;&#26680;&#33539;&#25968;&#26368;&#23567;&#21270;&#20013;&#23545;&#25239;&#24615;&#22122;&#22768;&#30340;&#31283;&#20581;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How robust is randomized blind deconvolution via nuclear norm minimization against adversarial noise?. (arXiv:2303.10030v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#26680;&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#31283;&#20581;&#24615;&#36793;&#30028;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#38543;&#26426;&#22122;&#22768;&#65292;&#21487;&#20197;&#22312;&#32500;&#24230;&#26080;&#20851;&#30340;&#35823;&#24046;&#36793;&#30028;&#19979;&#65292;&#22312;&#38543;&#26426;&#22122;&#22768;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25200;&#21160;&#19979;&#31283;&#23450;&#22320;&#24674;&#22797;&#20004;&#20010;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20174;&#21367;&#31215;&#20013;&#24674;&#22797;&#20004;&#20010;&#26410;&#30693;&#20449;&#21495;&#30340;&#38590;&#39064;&#8212;&#8212;&#30450;&#21453;&#21367;&#31215;&#12290;&#23558;&#30450;&#21453;&#21367;&#31215;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#20302;&#31209;&#24674;&#22797;&#38382;&#39064;&#65292;&#30001;&#20110;&#26680;&#33539;&#25968;&#26368;&#23567;&#21270;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#25104;&#21151;&#65292;&#36807;&#21435;&#21313;&#24180;&#20013;&#24050;&#32463;&#20135;&#29983;&#20102;&#22810;&#37325;&#29702;&#35770;&#24674;&#22797;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#21367;&#31215;&#21463;&#21040;&#21152;&#24615;&#26377;&#30028;&#22122;&#22768;&#30340;&#24178;&#25200;&#65292;&#24674;&#22797;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#20173;&#28982;&#36828;&#26410;&#34987;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26680;&#33539;&#25968;&#26368;&#23567;&#21270;&#30340;&#31283;&#20581;&#24615;&#36793;&#30028;&#36827;&#34892;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#38543;&#26426;&#22122;&#22768;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#22122;&#22768;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25200;&#21160;&#19979;&#65292;&#26680;&#33539;&#25968;&#26368;&#23567;&#21270;&#21487;&#20197;&#31283;&#23450;&#22320;&#24674;&#22797;&#20004;&#20010;&#20449;&#21495;&#65292;&#24182;&#19988;&#20855;&#26377;&#26080;&#20851;&#32500;&#24230;&#30340;&#35823;&#24046;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of recovering two unknown signals from their convolution, which is commonly referred to as blind deconvolution. Reformulation of blind deconvolution as a low-rank recovery problem has led to multiple theoretical recovery guarantees in the past decade due to the success of the nuclear norm minimization heuristic. In particular, in the absence of noise, exact recovery has been established for sufficiently incoherent signals contained in lower-dimensional subspaces. However, if the convolution is corrupted by additive bounded noise, the stability of the recovery problem remains much less understood. In particular, existing reconstruction bounds involve large dimension factors and therefore fail to explain the empirical evidence for dimension-independent robustness of nuclear norm minimization. Recently, theoretical evidence has emerged for ill-posed behavior of low-rank matrix recovery for sufficiently small noise levels. In this work, we develop improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;-&#36229;&#24179;&#38754;&#26680;&#20989;&#25968;&#26063;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#31215;&#26497;&#23398;&#20064;&#20013;&#20351;&#29992;&#65292;&#20197;&#24314;&#27169;&#38750;&#24179;&#31283;&#24615;&#21644;&#38750;&#32447;&#24615;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.10022</link><description>&lt;p&gt;
&#23618;&#27425;-&#36229;&#24179;&#38754;&#26680;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#31215;&#26497;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical-Hyperplane Kernels for Actively Learning Gaussian Process Models of Nonstationary Systems. (arXiv:2303.10022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;-&#36229;&#24179;&#38754;&#26680;&#20989;&#25968;&#26063;&#65292;&#33021;&#22815;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#31215;&#26497;&#23398;&#20064;&#20013;&#20351;&#29992;&#65292;&#20197;&#24314;&#27169;&#38750;&#24179;&#31283;&#24615;&#21644;&#38750;&#32447;&#24615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#35745;&#31639;&#26426;&#27169;&#25311;&#21644;&#29289;&#29702;&#26426;&#22120;&#30340;&#31934;&#30830;&#20195;&#29702;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#38271;&#26102;&#38388;&#25110;&#26114;&#36149;&#30340;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#24314;&#27169;&#30340;&#29289;&#29702;&#20381;&#36182;&#20851;&#31995;&#34920;&#29616;&#20986;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#24615;&#12290;&#22240;&#27492;&#65292;&#29992;&#20110;&#20135;&#29983;&#20195;&#29702;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#36890;&#36807;&#25552;&#20379;&#20445;&#25345;&#26597;&#35810;&#25968;&#37327;&#23569;&#30340;&#26041;&#26696;&#65288;&#20363;&#22914;&#20351;&#29992;&#31215;&#26497;&#23398;&#20064;&#65289;&#65292;&#24182;&#33021;&#22815;&#25429;&#33719;&#31995;&#32479;&#30340;&#38750;&#32447;&#24615;&#21644;&#38750;&#24179;&#31283;&#29305;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#19968;&#31181;&#24314;&#27169;&#38750;&#24179;&#31283;&#24615;&#30340;&#26041;&#27861;&#26159;&#24341;&#20837;&#36755;&#20837;&#20998;&#21306;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#26031;&#36807;&#31243;&#30340;&#31215;&#26497;&#23398;&#20064;&#20013;&#34987;&#35777;&#26126;&#26159;&#26377;&#20248;&#21183;&#30340;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20551;&#23450;&#24050;&#30693;&#20998;&#21306;&#65292;&#38656;&#35201;&#24341;&#20837;&#22797;&#26434;&#30340;&#25277;&#26679;&#26041;&#26696;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#38750;&#24120;&#31616;&#21333;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#26680;&#20989;&#25968;&#26063;&#65292;&#23427;&#21253;&#25324;&#19968;&#20010;&#21487;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#23398;&#20064;&#30340;&#20998;&#21306;&#65292;&#24182;&#20351;&#29992;&#26356;&#28789;&#27963;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning precise surrogate models of complex computer simulations and physical machines often require long-lasting or expensive experiments. Furthermore, the modeled physical dependencies exhibit nonlinear and nonstationary behavior. Machine learning methods that are used to produce the surrogate model should therefore address these problems by providing a scheme to keep the number of queries small, e.g. by using active learning and be able to capture the nonlinear and nonstationary properties of the system. One way of modeling the nonstationarity is to induce input-partitioning, a principle that has proven to be advantageous in active learning for Gaussian processes. However, these methods either assume a known partitioning, need to introduce complex sampling schemes or rely on very simple geometries. In this work, we present a simple, yet powerful kernel family that incorporates a partitioning that: i) is learnable via gradient-based methods, ii) uses a geometry that is more flexible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.10019</link><description>&lt;p&gt;
&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#21450;&#20854;&#22312;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#27010;&#29575;CRPS&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26085;&#21069;&#30005;&#20215;&#39044;&#27979;&#20013;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#20998;&#20301;&#25968;&#21644;&#21327;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#27010;&#29575;&#39044;&#27979;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24179;&#28369;&#36807;&#31243;&#20801;&#35768;&#22312;&#32447;&#23398;&#20064;&#12290;&#36890;&#36807;&#32500;&#25968;&#38477;&#20302;&#21644;&#32602;&#20989;&#25968;&#24179;&#28369;&#31561;&#20004;&#31181;&#24179;&#28369;&#26041;&#27861;&#26469;&#23558;&#26631;&#20934;CRPS&#23398;&#20064;&#26694;&#26550;&#25512;&#24191;&#21040;&#22810;&#20803;&#32500;&#24230;&#20013;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#39044;&#27979;&#26085;&#21069;&#30005;&#20215;&#65292;&#30456;&#27604;&#20110;&#32479;&#19968;&#32452;&#21512;&#65292;&#22312;CRPS&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09989</link><description>&lt;p&gt;
&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#25214;&#21040;&#33021;&#21147;&#21306;&#22495;
&lt;/p&gt;
&lt;p&gt;
Finding Competence Regions in Domain Generalization. (arXiv:2303.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#30340;&#40664;&#40664;&#22833;&#36133;&#38382;&#39064;&#12290;&#36890;&#36807;&#39044;&#27979;&#21487;&#20449;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#20197;&#35782;&#21035;&#33021;&#21147;&#21306;&#22495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#34913;&#37327;&#26080;&#33021;&#65292;&#22686;&#21152;&#26080;&#33021;&#24471;&#20998;&#20250;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#23398;&#20064;&#25298;&#32477;&#8221;&#26694;&#26550;&#26469;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#20013;&#40664;&#40664;&#22833;&#36133;&#30340;&#38382;&#39064;&#65292;&#21363;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#28201;&#21644;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27169;&#22411;&#20272;&#35745;&#30340;&#33021;&#21147;&#39044;&#31034;&#30528;&#21487;&#20449;&#21709;&#24212;&#26102;&#25509;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#25298;&#32477;&#36229;&#20986;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#21487;&#20449;&#24230;&#36890;&#36807;&#19982;&#20998;&#31867;&#22120;&#24615;&#33021;&#23494;&#20999;&#30456;&#20851;&#30340;&#20195;&#29702;&#26080;&#33021;&#20998;&#25968;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#20998;&#31867;&#30340;&#26080;&#33021;&#24471;&#20998;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#24378;&#35843;&#20102;&#25298;&#32477;&#29575;&#19982;&#20934;&#30830;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#19982;&#20808;&#21069;&#30340;&#24037;&#20316;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#26631;&#20934;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#65292;&#24182;&#32771;&#34385;&#22312;&#38381;&#21512;&#21644;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#19979;&#36890;&#36807;&#19981;&#21516;&#30340;&#23398;&#20064;&#34920;&#31034;&#26469;&#34913;&#37327;&#26080;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#26080;&#33021;&#20998;&#25968;&#30830;&#23454;&#39044;&#31034;&#30528;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#26174;&#30528;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We propose a "learning to reject" framework to address the problem of silent failures in Domain Generalization (DG), where the test distribution differs from the training distribution. Assuming a mild distribution shift, we wish to accept out-of-distribution (OOD) data whenever a model's estimated competence foresees trustworthy responses, instead of rejecting OOD data outright. Trustworthiness is then predicted via a proxy incompetence score that is tightly linked to the performance of a classifier. We present a comprehensive experimental evaluation of incompetence scores for classification and highlight the resulting trade-offs between rejection rate and accuracy gain. For comparability with prior work, we focus on standard DG benchmarks and consider the effect of measuring incompetence via different learned representations in a closed versus an open world setting. Our results suggest that increasing incompetence scores are indeed predictive of reduced accuracy, leading to significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#25968;&#25454;&#23398;&#20064;&#39134;&#34892;&#22120;&#34892;&#20026;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09981</link><description>&lt;p&gt;
&#20174;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#20013;&#25512;&#26029;&#32456;&#31471;&#31354;&#22495;&#20132;&#36890;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures. (arXiv:2303.09981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#25968;&#25454;&#23398;&#20064;&#39134;&#34892;&#22120;&#34892;&#20026;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#33322;&#31354;&#22120;&#36712;&#36857;&#27169;&#22411;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#65288;ATM&#65289;&#31995;&#32479;&#35774;&#35745;&#21644;&#39564;&#35777;&#24456;&#26377;&#29992;&#12290;&#20202;&#34920;&#39134;&#34892;&#35268;&#21017;&#65288;IFR&#65289;&#19979;&#25805;&#20316;&#30340;&#39134;&#34892;&#22120;&#27169;&#22411;&#38656;&#35201;&#25429;&#25417;&#39134;&#34892;&#22120;&#25353;&#29031;&#26631;&#20934;&#39134;&#34892;&#31243;&#24207;&#30340;&#22266;&#26377;&#21464;&#24322;&#24615;&#12290;&#39134;&#34892;&#22120;&#34892;&#20026;&#30340;&#21464;&#24322;&#24615;&#22312;&#19981;&#21516;&#30340;&#39134;&#34892;&#38454;&#27573;&#20043;&#38388;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#31243;&#24207;&#25968;&#25454;&#21644;&#20174;&#38647;&#36798;&#30417;&#35270;&#25968;&#25454;&#25910;&#38598;&#30340;&#39134;&#34892;&#36712;&#36857;&#20013;&#23398;&#20064;&#21464;&#24322;&#24615;&#12290; &#23545;&#20110;&#27599;&#20010;&#27573;&#33853;&#65292;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23398;&#20064;&#39134;&#34892;&#22120;&#36712;&#36857;&#19982;&#20854;&#31243;&#24207;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#32473;&#23450;&#26032;&#30340;&#31243;&#24207;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#26679;&#19968;&#31995;&#21015;&#20559;&#24046;&#65292;&#24182;&#20351;&#29992;&#20559;&#24046;&#21644;&#31243;&#24207;&#37325;&#26500;&#39134;&#34892;&#22120;&#36712;&#36857;&#26469;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#25429;&#25417;&#39134;&#34892;&#22120;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#25104;&#23545;&#27169;&#22411;&#26469;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic aircraft trajectory models are useful in the design and validation of air traffic management (ATM) systems. Models of aircraft operated under instrument flight rules (IFR) require capturing the variability inherent in how aircraft follow standard flight procedures. The variability in aircraft behavior varies among flight stages. In this paper, we propose a probabilistic model that can learn the variability from the procedural data and flight tracks collected from radar surveillance data. For each segment, a Gaussian mixture model is used to learn the deviations of aircraft trajectories from their procedures. Given new procedures, we can generate synthetic trajectories by sampling a series of deviations from the trained Gaussian distributions and reconstructing the aircraft trajectory using the deviations and the procedures. We extend this method to capture pairwise correlations between aircraft and show how a pairwise model can be used to generate traffic involving an arbitra
&lt;/p&gt;</description></item><item><title>MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09975</link><description>&lt;p&gt;
MedNeXt&#65306;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#21464;&#21387;&#22120;&#39537;&#21160;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation. (arXiv:2303.09975v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09975
&lt;/p&gt;
&lt;p&gt;
MedNeXt&#26159;&#19968;&#20010;&#23450;&#21046;&#21270;&#30340;&#29616;&#20195;&#21270;&#21487;&#25193;&#23637;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#25361;&#25112;&#12290;&#35813;&#32593;&#32476;&#21253;&#21547;&#65306;&#23436;&#20840;ConvNeXt 3D&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12289;&#27531;&#24046;ConvNeXt&#19978;&#19979;&#37319;&#26679;&#22359;&#21644;&#19968;&#31181;&#26032;&#30340;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20351;&#29992;&#22522;&#20110; Transformer &#30340;&#26550;&#26500;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#26159;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20854;&#24615;&#33021;&#36828;&#19981;&#22914;&#33258;&#28982;&#22270;&#20687;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#39640;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#35757;&#32451;&#21040;&#39640;&#24615;&#33021;&#27700;&#24179;&#12290;&#26368;&#36817;&#65292;ConvNeXt &#26550;&#26500;&#23581;&#35797;&#36890;&#36807;&#38236;&#20687;&#21464;&#21387;&#22120;&#22359;&#26469;&#29616;&#20195;&#21270;&#26631;&#20934;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#36825;&#19968;&#26550;&#26500;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#29616;&#20195;&#21270;&#19988;&#21487;&#25193;&#23637;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#21307;&#23398;&#29615;&#22659;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837; MedNeXt&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21464;&#21387;&#22120;&#21551;&#21457;&#30340;&#22823;&#26680;&#20998;&#21106;&#32593;&#32476;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;1&#65289;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#23436;&#20840; ConvNeXt 3D &#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#32593;&#32476;&#65292;2&#65289;&#27531;&#24046; ConvNeXt &#19978;&#19979;&#37319;&#26679;&#22359;&#65292;&#20197;&#22312;&#21508;&#20010;&#23610;&#24230;&#19978;&#20445;&#30041;&#35821;&#20041;&#20449;&#24687;&#65292;3&#65289;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#19978;&#37319;&#26679;&#23567;&#26680;&#26469;&#36845;&#20195;&#22686;&#21152;&#26680;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been exploding interest in embracing Transformer-based architectures for medical image segmentation. However, the lack of large-scale annotated medical datasets make achieving performances equivalent to those in natural images challenging. Convolutional networks, in contrast, have higher inductive biases and consequently, are easily trainable to high performance. Recently, the ConvNeXt architecture attempted to modernize the standard ConvNet by mirroring Transformer blocks. In this work, we improve upon this to design a modernized and scalable convolutional architecture customized to challenges of data-scarce medical settings. We introduce MedNeXt, a Transformer-inspired large kernel segmentation network which introduces - 1) A fully ConvNeXt 3D Encoder-Decoder Network for medical image segmentation, 2) Residual ConvNeXt up and downsampling blocks to preserve semantic richness across scales, 3) A novel technique to iteratively increase kernel sizes by upsampling small kernel 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37051;&#22495;&#24179;&#22343;&#65288;NA&#65289;&#30340;&#24322;&#24120;&#20998;&#25968;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#21450;&#20854;k&#20010;&#26368;&#36817;&#37051;&#23621;&#30340;&#20998;&#25968;&#30456;&#32467;&#21512;&#26469;&#20462;&#25913;&#20854;&#24322;&#24120;&#20998;&#25968;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#23545;&#35937;&#20855;&#26377;&#27604;&#21407;&#22987;&#20998;&#25968;&#26356;&#30456;&#20284;&#30340;&#24322;&#24120;&#20998;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;NA&#23558;&#25152;&#26377;&#27979;&#35797;&#30340;10&#20010;&#22522;&#32447;&#26816;&#27979;&#22120;&#24179;&#22343;&#25913;&#36827;&#20102;13&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.09972</link><description>&lt;p&gt;
&#37051;&#22495;&#24179;&#22343;&#27861;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Neighborhood Averaging for Improving Outlier Detectors. (arXiv:2303.09972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37051;&#22495;&#24179;&#22343;&#65288;NA&#65289;&#30340;&#24322;&#24120;&#20998;&#25968;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#23545;&#35937;&#21450;&#20854;k&#20010;&#26368;&#36817;&#37051;&#23621;&#30340;&#20998;&#25968;&#30456;&#32467;&#21512;&#26469;&#20462;&#25913;&#20854;&#24322;&#24120;&#20998;&#25968;&#65292;&#20351;&#24471;&#30456;&#20284;&#30340;&#23545;&#35937;&#20855;&#26377;&#27604;&#21407;&#22987;&#20998;&#25968;&#26356;&#30456;&#20284;&#30340;&#24322;&#24120;&#20998;&#25968;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;NA&#23558;&#25152;&#26377;&#27979;&#35797;&#30340;10&#20010;&#22522;&#32447;&#26816;&#27979;&#22120;&#24179;&#22343;&#25913;&#36827;&#20102;13&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#30456;&#20284;&#30340;&#23545;&#35937;&#24212;&#35813;&#26377;&#30456;&#20284;&#30340;&#24322;&#24120;&#20998;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#37117;&#29420;&#31435;&#22320;&#20026;&#27599;&#20010;&#23545;&#35937;&#35745;&#31639;&#24322;&#24120;&#20998;&#25968;&#65292;&#32780;&#19981;&#32771;&#34385;&#20854;&#20182;&#23545;&#35937;&#30340;&#24322;&#24120;&#20998;&#25968;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#30456;&#20284;&#30340;&#23545;&#35937;&#20855;&#26377;&#30456;&#20284;&#30340;&#24322;&#24120;&#20998;&#25968;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#25552;&#20986;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37051;&#22495;&#24179;&#22343;&#65288;NA&#65289;&#30340;&#24322;&#24120;&#20998;&#25968;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#23427;&#20851;&#27880;&#23545;&#35937;&#21450;&#20854;&#37051;&#23621;&#65292;&#24182;&#20445;&#35777;&#23427;&#20204;&#20855;&#26377;&#27604;&#21407;&#22987;&#20998;&#25968;&#26356;&#30456;&#20284;&#30340;&#24322;&#24120;&#20998;&#25968;&#12290;&#32473;&#23450;&#20219;&#20309;&#24322;&#24120;&#26816;&#27979;&#22120;&#20013;&#30340;&#23545;&#35937;&#21450;&#20854;&#24322;&#24120;&#20998;&#25968;&#65292;NA&#36890;&#36807;&#23558;&#20854;&#19982;&#20854;k&#20010;&#26368;&#36817;&#37051;&#23621;&#30340;&#20998;&#25968;&#30456;&#32467;&#21512;&#65292;&#20462;&#25913;&#20854;&#24322;&#24120;&#20998;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#21517;&#30340;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#35777;&#26126;&#20102;NA&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65292;NA&#23558;&#25152;&#26377;&#27979;&#35797;&#30340;10&#20010;&#22522;&#32447;&#26816;&#27979;&#22120;&#24179;&#22343;&#25913;&#36827;&#20102;13&#65285;&#65288;&#20174;0.70&#21040;0.79 AUC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We hypothesize that similar objects should have similar outlier scores. To our knowledge, all existing outlier detectors calculate the outlier score for each object independently regardless of the outlier scores of the other objects. Therefore, they do not guarantee that similar objects have similar outlier scores. To verify our proposed hypothesis, we propose an outlier score post-processing technique for outlier detectors, called neighborhood averaging(NA), which pays attention to objects and their neighbors and guarantees them to have more similar outlier scores than their original scores. Given an object and its outlier score from any outlier detector, NA modifies its outlier score by combining it with its k nearest neighbors' scores. We demonstrate the effectivity of NA by using the well-known k-nearest neighbors (k-NN). Experimental results show that NA improves all 10 tested baseline detectors by 13% (from 0.70 to 0.79 AUC) on average evaluated on nine real-world datasets. Moreo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#23454;&#29616;&#20102;&#38543;&#26426;&#36830;&#32493;&#36138;&#24515;&#31639;&#27861;&#30340; $(1-1/e) \approx 63\%$ &#30340;&#36817;&#20284;&#27604;&#25928;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.09960</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#30340;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Submodular Maximization via Polynomial Estimators. (arXiv:2303.09960v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#23454;&#29616;&#20102;&#38543;&#26426;&#36830;&#32493;&#36138;&#24515;&#31639;&#27861;&#30340; $(1-1/e) \approx 63\%$ &#30340;&#36817;&#20284;&#27604;&#25928;&#26524;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22312;&#32447;&#23398;&#20064;&#12289;&#22242;&#38431;&#32452;&#24314;&#12289;&#35774;&#26045;&#36873;&#22336;&#12289;&#24433;&#21709;&#26368;&#22823;&#21270;&#12289;&#20027;&#21160;&#23398;&#20064;&#21644;&#24863;&#30693;&#30446;&#26631;&#20989;&#25968;&#31561;&#39046;&#22495;&#20013;&#33258;&#28982;&#32780;&#28982;&#20986;&#29616;&#30340;&#24102;&#26377;&#19968;&#33324;&#25311;&#38453;&#32422;&#26463;&#30340;&#38543;&#26426;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#38024;&#23545;&#19968;&#31867;&#30001;&#26410;&#30693;&#20998;&#24067;&#19979;&#30340;&#23376;&#27169;&#20989;&#25968;&#26399;&#26395;&#23450;&#20041;&#30340;&#38543;&#26426;&#26368;&#22823;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#36825;&#31181;&#21333;&#35843;&#20989;&#25968;&#65292;&#20351;&#29992;&#22810;&#39033;&#24335;&#26799;&#24230;&#20272;&#31639;&#22120;&#30340;&#38543;&#26426;&#36830;&#32493;&#36138;&#24515;&#31639;&#27861;&#21487;&#20197;&#36798;&#21040; $(1-1/e) \approx 63\%$ &#30340;&#36817;&#20284;&#27604;&#65288;&#26399;&#26395;&#20540;&#65289;&#65292;&#21516;&#26102;&#25105;&#20204;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#22810;&#39033;&#24335;&#20272;&#31639;&#22120;&#20195;&#26367;&#20808;&#21069;&#37319;&#29992;&#37319;&#26679;&#30340;&#31639;&#27861;&#33021;&#22815;&#28040;&#38500;&#19968;&#20123;&#38543;&#26426;&#22240;&#32032;&#24182;&#26174;&#33879;&#38477;&#20302;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study stochastic submodular maximization problems with general matroid constraints, that naturally arise in online learning, team formation, facility location, influence maximization, active learning and sensing objective functions. In other words, we focus on maximizing submodular functions that are defined as expectations over a class of submodular functions with an unknown distribution. We show that for monotone functions of this form, the stochastic continuous greedy algorithm attains an approximation ratio (in expectation) arbitrarily close to $(1-1/e) \approx 63\%$ using a polynomial estimation of the gradient. We argue that using this polynomial estimator instead of the prior art that uses sampling eliminates a source of randomness and experimentally reduces execution time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;ansatz&#65292;&#23558;&#35745;&#31639;&#24265;&#20215;&#30340;&#21704;&#29305;&#37324;-&#31119;&#20811;&#36712;&#36947;&#26144;&#23556;&#21040;&#39640;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#36712;&#36947;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#36328;&#21270;&#21512;&#29289;&#21644;&#20960;&#20309;&#32467;&#26500;&#30340;&#21333;&#20010;&#27874;&#20989;&#25968;&#65292;&#24182;&#36827;&#34892;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.09949</link><description>&lt;p&gt;
&#20026;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26500;&#24314;&#19968;&#20010;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a Foundation Model for Neural Network Wavefunctions. (arXiv:2303.09949v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;ansatz&#65292;&#23558;&#35745;&#31639;&#24265;&#20215;&#30340;&#21704;&#29305;&#37324;-&#31119;&#20811;&#36712;&#36947;&#26144;&#23556;&#21040;&#39640;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#36712;&#36947;&#12290;&#23427;&#33021;&#22815;&#23398;&#20064;&#36328;&#21270;&#21512;&#29289;&#21644;&#20960;&#20309;&#32467;&#26500;&#30340;&#21333;&#20010;&#27874;&#20989;&#25968;&#65292;&#24182;&#36827;&#34892;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#21464;&#20998;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#26469;&#27714;&#35299;&#30005;&#23376;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#39640;&#31934;&#24230;&#21644;&#24378;&#22823;&#30340;&#27874;&#20989;&#25968;ansatz&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25104;&#21151;&#19988;&#20855;&#26377;&#20248;&#33391;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#22312;&#35745;&#31639;&#19978;&#36807;&#20110;&#26114;&#36149;&#20197;&#20415;&#24191;&#27867;&#37319;&#29992;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#26159;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#31995;&#32479;&#20174;&#22836;&#24320;&#22987;&#20248;&#21270;&#27874;&#20989;&#25968;&#65292;&#22240;&#27492;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;ansatz&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#26080;&#20851;&#32852;&#30340;&#12289;&#35745;&#31639;&#24265;&#20215;&#30340;&#21704;&#29305;&#37324;-&#31119;&#20811;&#36712;&#36947;&#26144;&#23556;&#21040;&#30456;&#20851;&#30340;&#39640;&#31934;&#24230;&#31070;&#32463;&#32593;&#32476;&#36712;&#36947;&#12290;&#36825;&#31181;ansatz&#26412;&#36136;&#19978;&#33021;&#22815;&#23398;&#20064;&#36328;&#22810;&#31181;&#21270;&#21512;&#29289;&#21644;&#20960;&#20309;&#32467;&#26500;&#30340;&#21333;&#20010;&#27874;&#20989;&#25968;&#65292;&#36825;&#19968;&#28857;&#25105;&#20204;&#36890;&#36807;&#25104;&#21151;&#23558;&#38024;&#23545;&#23567;&#20998;&#23376;&#29255;&#27573;&#39044;&#35757;&#32451;&#30340;&#27874;&#20989;&#25968;&#27169;&#22411;&#36716;&#31227;&#21040;&#36739;&#22823;&#21270;&#21512;&#29289;&#30340;&#23454;&#39564;&#35777;&#25454;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#35777;&#25454;&#26469;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#24191;&#20041;&#27874;&#20989;&#25968;&#30340;&#24191;&#27867;&#39044;&#35757;&#32451;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have become a highly accurate and powerful wavefunction ansatz in combination with variational Monte Carlo methods for solving the electronic Schr\"odinger equation. However, despite their success and favorable scaling, these methods are still computationally too costly for wide adoption. A significant obstacle is the requirement to optimize the wavefunction from scratch for each new system, thus requiring long optimization. In this work, we propose a novel neural network ansatz, which effectively maps uncorrelated, computationally cheap Hartree-Fock orbitals, to correlated, high-accuracy neural network orbitals. This ansatz is inherently capable of learning a single wavefunction across multiple compounds and geometries, as we demonstrate by successfully transferring a wavefunction model pre-trained on smaller fragments to larger compounds. Furthermore, we provide ample experimental evidence to support the idea that extensive pre-training of a such a generalized wa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#31946;&#24378;&#21270;&#23398;&#20064;&#21327;&#20316;&#26041;&#27861;&#29992;&#20110;&#32676;&#38598;&#31995;&#32479;&#30340;&#33258;&#20027;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#36319;&#38543;&#39046;&#34966;&#12289;&#36991;&#20813;&#30896;&#25758;&#21644;&#36798;&#21040;&#32676;&#20307;&#36895;&#24230;&#20849;&#35782;&#19977;&#20010;&#30446;&#26631;&#65292;&#20855;&#26377;&#24377;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.09946</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#27169;&#31946;&#24378;&#21270;&#23398;&#20064;&#21327;&#20316;&#26041;&#27861;&#29992;&#20110;&#32676;&#38598;&#31995;&#32479;&#30340;&#33258;&#20027;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Fuzzy Reinforcement Learning Cooperative Approach for the Autonomous Control of Flock Systems. (arXiv:2303.09946v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09946
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#27169;&#31946;&#24378;&#21270;&#23398;&#20064;&#21327;&#20316;&#26041;&#27861;&#29992;&#20110;&#32676;&#38598;&#31995;&#32479;&#30340;&#33258;&#20027;&#25511;&#21046;&#12290;&#35813;&#26041;&#27861;&#38024;&#23545;&#36319;&#38543;&#39046;&#34966;&#12289;&#36991;&#20813;&#30896;&#25758;&#21644;&#36798;&#21040;&#32676;&#20307;&#36895;&#24230;&#20849;&#35782;&#19977;&#20010;&#30446;&#26631;&#65292;&#20855;&#26377;&#24377;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#38598;&#24341;&#23548;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32467;&#26500;&#65292;&#38656;&#35201;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20248;&#21270;&#30446;&#26631;&#12290;&#36825;&#36890;&#24120;&#38656;&#35201;&#19981;&#21516;&#30340;&#25511;&#21046;&#26041;&#27861;&#26469;&#22788;&#29702;&#21508;&#31181;&#30446;&#26631;&#65292;&#20363;&#22914;&#24341;&#23548;&#12289;&#36991;&#30896;&#21644;&#20957;&#32858;&#12290;&#29305;&#21035;&#26159;&#24341;&#23548;&#26041;&#26696;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#22797;&#26434;&#30340;&#36319;&#36394;&#35823;&#24046;&#21160;&#21147;&#23398;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#24179;&#34913;&#26465;&#20214;&#33719;&#21462;&#30340;&#32447;&#24615;&#21453;&#39304;&#31574;&#30053;&#30340;&#25216;&#26415;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#22312;&#19981;&#30830;&#23450;&#30340;&#21160;&#24577;&#29615;&#22659;&#19979;&#19981;&#21327;&#35843;&#25110;&#32773;&#20026;&#19981;&#31283;&#23450;&#29366;&#24577;&#12290;&#39044;&#35843;&#35856;&#30340;&#27169;&#31946;&#25512;&#29702;&#32467;&#26500;&#32570;&#20047;&#22312;&#36825;&#26679;&#30340;&#26410;&#24314;&#27169;&#26465;&#20214;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#20998;&#24067;&#24335;&#25216;&#26415;&#65292;&#29992;&#20110;&#32676;&#38598;&#31995;&#32479;&#30340;&#33258;&#20027;&#25511;&#21046;&#12290;&#23427;&#30456;&#23545;&#28789;&#27963;&#30340;&#32467;&#26500;&#22522;&#20110;&#21516;&#26102;&#38024;&#23545;&#19968;&#20123;&#30446;&#26631;&#30340;&#22312;&#32447;&#27169;&#31946;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#65292;&#21363;&#36319;&#38543;&#39046;&#34966;&#12289;&#36991;&#20813;&#30896;&#25758;&#21644;&#36798;&#21040;&#32676;&#20307;&#36895;&#24230;&#20849;&#35782;&#12290;&#38500;&#20102;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#24178;&#25200;&#38754;&#21069;&#30340;&#24377;&#24615;&#20043;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;&#23545;&#21517;&#20041;&#26465;&#20214;&#30340;&#40065;&#26834;&#21644;&#24555;&#36895;&#25910;&#25947;&#24615;&#36136;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#22330;&#26223;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#36319;&#36394;&#31934;&#24230;&#21644;&#30896;&#25758;&#36991;&#20813;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The flock-guidance problem enjoys a challenging structure where multiple optimization objectives are solved simultaneously. This usually necessitates different control approaches to tackle various objectives, such as guidance, collision avoidance, and cohesion. The guidance schemes, in particular, have long suffered from complex tracking-error dynamics. Furthermore, techniques that are based on linear feedback strategies obtained at equilibrium conditions either may not hold or degrade when applied to uncertain dynamic environments. Pre-tuned fuzzy inference architectures lack robustness under such unmodeled conditions. This work introduces an adaptive distributed technique for the autonomous control of flock systems. Its relatively flexible structure is based on online fuzzy reinforcement learning schemes which simultaneously target a number of objectives; namely, following a leader, avoiding collision, and reaching a flock velocity consensus. In addition to its resilience in the face
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;GPU&#21152;&#36895;&#30340;&#30450;&#23545;&#25509;&#31574;&#30053;DSDP&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#25913;&#36827;&#20102;&#30450;&#23545;&#25509;&#30340;&#24615;&#33021;&#12290;DSDP&#21487;&#20197;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#32467;&#21512;&#20301;&#28857;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#21021;&#22987;&#20301;&#32622;&#65292;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#26500;&#35937;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2303.09916</link><description>&lt;p&gt;
DSDP&#65306;&#19968;&#31181;&#30001;GPU&#21152;&#36895;&#30340;&#30450;&#23545;&#25509;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
DSDP: A Blind Docking Strategy Accelerated by GPUs. (arXiv:2303.09916v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;GPU&#21152;&#36895;&#30340;&#30450;&#23545;&#25509;&#31574;&#30053;DSDP&#65292;&#20854;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#25913;&#36827;&#20102;&#30450;&#23545;&#25509;&#30340;&#24615;&#33021;&#12290;DSDP&#21487;&#20197;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#32467;&#21512;&#20301;&#28857;&#65292;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#21021;&#22987;&#20301;&#32622;&#65292;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#26500;&#35937;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#31579;&#36873;&#65292;&#21253;&#25324;&#20998;&#23376;&#23545;&#25509;&#65292;&#22312;&#33647;&#29289;&#30740;&#21457;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#35768;&#22810;&#20256;&#32479;&#30340;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#23436;&#25104;&#23545;&#25509;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23545;&#25509;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#65292;&#24182;&#19988;&#23427;&#20204;&#22312;&#30450;&#23545;&#25509;&#20013;&#30340;&#24615;&#33021;&#26377;&#24453;&#25913;&#36827;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#23545;&#25509;&#36816;&#34892;&#26102;&#38388;&#26174;&#30528;&#32553;&#30701;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#20301;&#28857;&#21644;&#23545;&#25509;&#23039;&#24577;(DSDP)&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#30450;&#23545;&#25509;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#20256;&#32479;&#30340;&#30450;&#23545;&#25509;&#65292;&#25972;&#20010;&#34507;&#30333;&#36136;&#34987;&#19968;&#20010;&#31435;&#26041;&#20307;&#35206;&#30422;&#65292;&#37197;&#20307;&#30340;&#21021;&#22987;&#20301;&#32622;&#22312;&#31435;&#26041;&#20307;&#20013;&#38543;&#26426;&#29983;&#25104;&#12290;&#30456;&#21453;&#65292;DSDP&#21487;&#20197;&#39044;&#27979;&#34507;&#30333;&#36136;&#30340;&#32467;&#21512;&#20301;&#28857;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#21021;&#22987;&#20301;&#32622;&#65292;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#26500;&#35937;&#37319;&#26679;&#12290;DSDP&#30340;&#23545;&#25509;&#20219;&#21153;&#21033;&#29992;&#24471;&#20998;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Virtual screening, including molecular docking, plays an essential role in drug discovery. Many traditional and machine-learning based methods are available to fulfil the docking task. The traditional docking methods are normally extensively time-consuming, and their performance in blind docking remains to be improved. Although the runtime of docking based on machine learning is significantly decreased, their accuracy is still limited. In this study, we take the advantage of both traditional and machine-learning based methods, and present a method Deep Site and Docking Pose (DSDP) to improve the performance of blind docking. For the traditional blind docking, the entire protein is covered by a cube, and the initial positions of ligands are randomly generated in the cube. In contract, DSDP can predict the binding site of proteins and provide an accurate searching space and initial positions for the further conformational sampling. The docking task of DSDP makes use of the score function
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#39044;&#27979;&#34880;&#31958;&#24773;&#20917;&#21450;&#20854;&#20998;&#31867;&#65292;&#36755;&#20986;&#39044;&#35686;&#28040;&#24687;&#20197;&#35753;&#30149;&#20154;&#26356;&#26089;&#36827;&#34892;&#27835;&#30103;&#12290;&#22312;&#39044;&#27979;&#21453;&#24377;&#24615;&#39640;&#34880;&#31958;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#20934;&#24230;&#21644;&#20934;&#30830;&#24230;&#65292;&#20026;&#39044;&#38450;&#39640;&#34880;&#31958;&#25552;&#20379;&#20102;&#21021;&#27493;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.09913</link><description>&lt;p&gt;
&#30701;&#31687;&#35770;&#25991;: &#22522;&#30784;&#35843;&#25972;: &#36235;&#21183;&#39044;&#27979;&#35686;&#25253;&#21644;&#35843;&#25972;&#22522;&#30784;&#36895;&#29575;&#20197;&#39044;&#38450;&#39640;&#34880;&#31958;
&lt;/p&gt;
&lt;p&gt;
Short: Basal-Adjust: Trend Prediction Alerts and Adjusted Basal Rates for Hyperglycemia Prevention. (arXiv:2303.09913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09913
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#39044;&#27979;&#34880;&#31958;&#24773;&#20917;&#21450;&#20854;&#20998;&#31867;&#65292;&#36755;&#20986;&#39044;&#35686;&#28040;&#24687;&#20197;&#35753;&#30149;&#20154;&#26356;&#26089;&#36827;&#34892;&#27835;&#30103;&#12290;&#22312;&#39044;&#27979;&#21453;&#24377;&#24615;&#39640;&#34880;&#31958;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#39044;&#27979;&#31934;&#20934;&#24230;&#21644;&#20934;&#30830;&#24230;&#65292;&#20026;&#39044;&#38450;&#39640;&#34880;&#31958;&#25552;&#20379;&#20102;&#21021;&#27493;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31958;&#23615;&#30149;&#27835;&#30103;&#26041;&#38754;&#65292;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#33008;&#33146;&#31995;&#32479;&#65288;APS&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23578;&#23384;&#22312;&#20110;&#22788;&#29702;&#19981;&#23433;&#20840;&#34880;&#31958;&#65288;BG&#65289;&#27700;&#24179;&#26102;&#25928;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#21453;&#24377;&#24615;&#39640;&#34880;&#31958;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;BG&#24773;&#22659;&#20998;&#31867;&#65292;&#20135;&#29983;&#39044;&#35686;&#20449;&#24687;&#20197;&#35753;&#30149;&#20154;&#26356;&#26089;&#12289;&#26356;&#30452;&#35266;&#22320;&#36827;&#34892;&#27835;&#30103;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20302;&#34880;&#31958;&#21644;&#39640;&#34880;&#31958;&#39044;&#27979;&#36890;&#30693;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#38024;&#23545;BG&#24773;&#22659;&#30340;&#29305;&#23450;&#35686;&#25253;&#20449;&#24687;&#20197;&#21450;&#39044;&#38450;&#21453;&#24377;&#24615;&#39640;&#34880;&#31958;&#30340;&#31934;&#30830;&#22522;&#30784;&#24314;&#35758;&#30340;&#21021;&#27493;&#25514;&#26045;&#12290;&#22312;DCLP3&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#23454;&#29616;&#20102;&#23545;&#20110;&#30149;&#20154;&#35686;&#25253;&#30340;&#21453;&#24377;&#24615;&#39640;&#20107;&#20214;&#30340;&#39044;&#27979;&#31934;&#24230;&gt; 98&#65285;&#21644;&#31934;&#24230;&gt; 79&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant advancements in type 1 diabetes treatment have been made in the development of state-of-the-art Artificial Pancreas Systems (APS). However, lapses currently exist in the timely treatment of unsafe blood glucose (BG) levels, especially in the case of rebound hyperglycemia. We propose a machine learning (ML) method for predictive BG scenario categorization that outputs messages alerting the patient to upcoming BG trends to allow for earlier, educated treatment. In addition to standard notifications of predicted hypoglycemia and hyperglycemia, we introduce BG scenario-specific alert messages and the preliminary steps toward precise basal suggestions for the prevention of rebound hyperglycemia. Experimental evaluation on the DCLP3 clinical dataset achieves &gt;98% accuracy and &gt;79% precision for predicting rebound high events for patient alerts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#27573;&#26354;&#29575;&#30340;&#39640;&#24230;&#38750;&#24179;&#20961;&#32500;&#24230;&#35268;&#32422;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#21033;&#29992;&#27492;&#25351;&#26631;&#26469;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#32500;&#24230;&#35268;&#32422;&#31639;&#27861;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2303.09909</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#27573;&#26354;&#29575;&#30340;&#38477;&#32500;&#35780;&#20272;&#26694;&#26550;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An evaluation framework for dimensionality reduction through sectional curvature. (arXiv:2303.09909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#27573;&#26354;&#29575;&#30340;&#39640;&#24230;&#38750;&#24179;&#20961;&#32500;&#24230;&#35268;&#32422;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#21033;&#29992;&#27492;&#25351;&#26631;&#26469;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#32500;&#24230;&#35268;&#32422;&#31639;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#23450;&#20041;&#19978;&#32570;&#20047;&#22522;&#20934;&#65292;&#36825;&#32473;&#35780;&#20272;&#27492;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#35774;&#35745;&#25351;&#26631;&#24102;&#26469;&#20102;&#37325;&#22823;&#22256;&#38590;&#12290;&#19982;&#30417;&#30563;&#23398;&#20064;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#30417;&#30563;&#23398;&#20064;&#30340;&#36136;&#37327;&#25351;&#26631;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#32780;&#22312;&#38477;&#32500;&#39046;&#22495;&#20013;&#65292;&#21482;&#25552;&#20986;&#20102;&#19968;&#20123;&#36807;&#20110;&#31616;&#21270;&#30340;&#25351;&#26631;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;&#19968;&#31181;&#26497;&#20854;&#38750;&#24179;&#20961;&#30340;&#38477;&#32500;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65292;&#35813;&#25351;&#26631;&#22522;&#20110;&#26469;&#33258;&#40654;&#26364;&#20960;&#20309;&#30340;&#20998;&#27573;&#26354;&#29575;&#34892;&#20026;&#12290;&#20026;&#20102;&#27979;&#35797;&#20854;&#21487;&#34892;&#24615;&#65292;&#35813;&#25351;&#26631;&#24050;&#34987;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#38477;&#32500;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;&#31639;&#27861;&#35780;&#20272;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#20195;&#34920;&#24615;&#65292;&#21033;&#29992;&#24179;&#38754;&#26354;&#32447;&#30340;&#26354;&#29575;&#29305;&#24615;&#65292;&#26500;&#36896;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#21270;&#38382;&#39064;&#23454;&#20363;&#29983;&#25104;&#22120;&#24418;&#24335;&#30340;&#20989;&#25968;&#29983;&#25104;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised machine learning lacks ground truth by definition. This poses a major difficulty when designing metrics to evaluate the performance of such algorithms. In sharp contrast with supervised learning, for which plenty of quality metrics have been studied in the literature, in the field of dimensionality reduction only a few over-simplistic metrics has been proposed. In this work, we aim to introduce the first highly non-trivial dimensionality reduction performance metric. This metric is based on the sectional curvature behaviour arising from Riemannian geometry. To test its feasibility, this metric has been used to evaluate the performance of the most commonly used dimension reduction algorithms in the state of the art. Furthermore, to make the evaluation of the algorithms robust and representative, using curvature properties of planar curves, a new parameterized problem instance generator has been constructed in the form of a function generator. Experimental results are consis
&lt;/p&gt;</description></item><item><title>&#20171;&#35266;&#38598;&#20307;&#36816;&#21160;&#30340;&#38543;&#26426;&#29305;&#24449;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#21160;&#21147;&#23398;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#32773;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#30740;&#31350;&#30456;&#20114;&#20316;&#29992;&#20010;&#20307;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#37492;&#23450;&#21644;&#20998;&#26512;&#65292;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#31209;&#24207;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2303.09906</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#38543;&#26426;&#24314;&#27169;&#25506;&#32034;&#38598;&#20307;&#36816;&#21160;&#30340;&#20171;&#35266;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Discovering mesoscopic descriptions of collective movement with neural stochastic modelling. (arXiv:2303.09906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09906
&lt;/p&gt;
&lt;p&gt;
&#20171;&#35266;&#38598;&#20307;&#36816;&#21160;&#30340;&#38543;&#26426;&#29305;&#24449;&#23545;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#21160;&#21147;&#23398;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#32773;&#21033;&#29992;&#29289;&#29702;&#21551;&#21457;&#12289;&#31070;&#32463;&#32593;&#32476;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#30740;&#31350;&#30456;&#20114;&#20316;&#29992;&#20010;&#20307;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#37492;&#23450;&#21644;&#20998;&#26512;&#65292;&#20026;&#36825;&#20123;&#31995;&#32479;&#30340;&#31209;&#24207;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#20307;&#36816;&#21160;&#26159;&#33258;&#28982;&#30028;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#65292;&#21551;&#21457;&#24037;&#31243;&#24072;&#12289;&#29289;&#29702;&#23398;&#23478;&#21644;&#25968;&#23398;&#23478;&#24320;&#21457;&#25968;&#23398;&#27169;&#22411;&#21644;&#29983;&#29289;&#21551;&#21457;&#35774;&#35745;&#12290;&#23567;&#33267;&#20013;&#31561;&#32676;&#20307;&#35268;&#27169;&#65288;&#32422;10-1000&#20010;&#20010;&#20307;&#65292;&#20063;&#31216;&#8220;&#20171;&#35266;&#23610;&#24230;&#8221;&#30340;&#38598;&#20307;&#36816;&#21160;&#65292;&#30001;&#20110;&#38543;&#26426;&#24615;&#32780;&#26174;&#31034;&#20986;&#38750;&#24179;&#20961;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#22312;&#20171;&#35266;&#23610;&#24230;&#38598;&#20307;&#29616;&#35937;&#30340;&#30740;&#31350;&#20013;&#65292;&#34920;&#24449;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#29305;&#24449;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#65292;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#34920;&#24449;&#30456;&#20114;&#20316;&#29992;&#20010;&#20307;&#30340;&#38543;&#26426;&#32676;&#20307;&#21160;&#21147;&#23398;&#65292;&#36890;&#36807;&#25511;&#21046;&#32676;&#20307;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#32676;&#20307;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#21033;&#29992;&#28418;&#31227;&#21644;&#25193;&#25955;&#22330;&#37492;&#23450;&#31995;&#32479;&#30340;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#26041;&#38754;&#30340;&#21160;&#21147;&#23398;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#31209;&#24207;&#24615;&#36136;&#20570;&#20986;&#26032;&#39062;&#30340;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collective motion is an ubiquitous phenomenon in nature, inspiring engineers, physicists and mathematicians to develop mathematical models and bio-inspired designs. Collective motion at small to medium group sizes ($\sim$10-1000 individuals, also called the `mesoscale'), can show nontrivial features due to stochasticity. Therefore, characterizing both the deterministic and stochastic aspects of the dynamics is crucial in the study of mesoscale collective phenomena. Here, we use a physics-inspired, neural-network based approach to characterize the stochastic group dynamics of interacting individuals, through a stochastic differential equation (SDE) that governs the collective dynamics of the group. We apply this technique on both synthetic and real-world datasets, and identify the deterministic and stochastic aspects of the dynamics using drift and diffusion fields, enabling us to make novel inferences about the nature of order in these systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;DeepMVC&#65292;&#29992;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20851;&#38190;&#35266;&#23519;&#65292;&#21457;&#29616;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#34920;&#31034;&#20250;&#23545;&#32858;&#31867;&#21487;&#20998;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35282;&#25968;&#37327;&#36739;&#22810;&#26102;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#32570;&#38519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;DeepMVC&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.09877</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#23545;&#27604;&#23545;&#40784;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Self-supervision and Contrastive Alignment in Deep Multi-view Clustering. (arXiv:2303.09877v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#26694;&#26550;DeepMVC&#65292;&#29992;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20851;&#38190;&#35266;&#23519;&#65292;&#21457;&#29616;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#34920;&#31034;&#20250;&#23545;&#32858;&#31867;&#21487;&#20998;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35282;&#25968;&#37327;&#36739;&#22810;&#26102;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#32570;&#38519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#30340;&#22522;&#20110;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;DeepMVC&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#36817;&#24180;&#26469;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#20013;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#26041;&#27861;&#22312;&#21457;&#23637;&#26041;&#38754;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#20250;&#25302;&#24930;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;DeepMVC&#65292;&#29992;&#20110;&#28145;&#24230;&#22810;&#35270;&#35282;&#32858;&#31867;&#65292;&#20854;&#20013;&#21253;&#25324;&#35768;&#22810;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#23545;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24433;&#21709;&#36827;&#34892;&#20851;&#38190;&#35266;&#23519;&#65292;&#29305;&#21035;&#26159;&#23545;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23545;&#40784;&#34920;&#31034;&#30340;&#32570;&#28857;&#30340;&#35266;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#27604;&#23545;&#40784;&#21487;&#33021;&#20250;&#23545;&#32858;&#31867;&#21487;&#20998;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;&#24403;&#35270;&#35282;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24433;&#21709;&#20250;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#26032;&#30340;DeepMVC&#23454;&#20363;&#65292;&#20855;&#26377;&#26032;&#24418;&#24335;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#65288;i&#65289;&#19982;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#19968;&#33268;&#65292;&#23545;&#27604;&#23545;&#40784;&#20250;&#38477;&#20302;&#20855;&#26377;&#35768;&#22810;&#35270;&#35282;&#30340;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65307;&#65288;ii&#65289;&#25152;&#26377;&#26041;&#27861;&#37117;&#21463;&#30410;&#20110;&#19968;&#23450;&#31243;&#24230;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a central component in recent approaches to deep multi-view clustering (MVC). However, we find large variations in the development of self-supervision-based methods for deep MVC, potentially slowing the progress of the field. To address this, we present DeepMVC, a unified framework for deep MVC that includes many recent methods as instances. We leverage our framework to make key observations about the effect of self-supervision, and in particular, drawbacks of aligning representations with contrastive learning. Further, we prove that contrastive alignment can negatively influence cluster separability, and that this effect becomes worse when the number of views increases. Motivated by our findings, we develop several new DeepMVC instances with new forms of self-supervision. We conduct extensive experiments and find that (i) in line with our theoretical findings, contrastive alignments decreases performance on datasets with many views; (ii) all methods benefit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09874</link><description>&lt;p&gt;
&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;20&#19990;&#32426;50&#24180;&#20195;&#65292;&#38669;&#21202;&#26031;&#24052;&#27931;&#21644;&#24343;&#38647;&#24503;&#38463;&#29305;&#32435;&#22827;&#25552;&#20986;&#20102;&#24863;&#23448;&#31995;&#32479;&#21644;&#23427;&#20204;&#22914;&#20309;&#36866;&#24212;&#29615;&#22659;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#26089;&#26399;&#35270;&#35273;&#30340;&#36827;&#21270;&#26159;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20256;&#36882;&#20851;&#20110;&#36755;&#20837;&#20449;&#21495;&#30340;&#20449;&#24687;&#12290;&#25353;&#29031;&#39321;&#20892;&#30340;&#23450;&#20041;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#36890;&#36807;&#33258;&#28982;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#27010;&#29575;&#26469;&#25551;&#36848;&#30340;&#12290;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20197;&#21069;&#26080;&#27861;&#30452;&#25509;&#20934;&#30830;&#22320;&#39044;&#27979;&#22270;&#20687;&#30340;&#27010;&#29575;&#12290;&#23613;&#31649;&#36825;&#31181;&#24819;&#27861;&#30340;&#25506;&#32034;&#26159;&#38388;&#25509;&#30340;&#65292;&#20027;&#35201;&#22522;&#20110;&#22270;&#20687;&#23494;&#24230;&#30340;&#36807;&#24230;&#31616;&#21270;&#27169;&#22411;&#25110;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#37325;&#29616;&#21508;&#31181;&#29983;&#29702;&#21644;&#24515;&#29702;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#30830;&#23450;&#30693;&#35273;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#24847;&#35265;&#30456;&#20851;&#24615;&#24456;&#39640;&#30340;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#30340;&#20195;&#29702;&#65292;&#20197;&#21450;&#19968;&#20010;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#30452;&#25509;&#20272;&#35745;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;Barlow&#21644;Attneave&#29702;&#35770;&#39044;&#27979;&#30340;&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#30693;&#35273;&#20043;&#38388;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#26469;&#35828;&#26126;&#36825;&#19968;&#21457;&#29616;&#65292;&#36825;&#26159;&#36890;&#36807;&#35270;&#35273;&#25628;&#32034;&#23454;&#39564;&#27979;&#37327;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09863</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#30340;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#65306;&#24191;&#20041;&#35823;&#24046;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Nonparametric Estimation of Intrinsic Data Structures by Chart Autoencoders: Generalization Error and Robustness. (arXiv:2303.09863v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#28145;&#24230;&#38750;&#21442;&#25968;&#20272;&#35745;&#20869;&#37096;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24191;&#20041;&#35823;&#24046;&#20445;&#35777;&#21644;&#21435;&#22122;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#32534;&#30721;&#22120;&#22312;&#23398;&#20064;&#39640;&#32500;&#25968;&#25454;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#26041;&#38754;&#24050;&#32463;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#25104;&#21151;&#12290;&#20551;&#35774;&#25968;&#25454;&#22312;&#20302;&#32500;&#27969;&#24418;&#38468;&#36817;&#37319;&#26679;&#65292;&#25105;&#20204;&#37319;&#29992;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#25968;&#25454;&#32534;&#30721;&#20026;&#19968;&#32452;&#22270;&#34920;&#19978;&#30340;&#20302;&#32500;&#28508;&#22312;&#29305;&#24449;&#65292;&#20174;&#32780;&#20445;&#30041;&#20102;&#25968;&#25454;&#27969;&#24418;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#20026;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#30340;&#24191;&#20041;&#35823;&#24046;&#24314;&#31435;&#20102;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#19988;&#36890;&#36807;&#32771;&#34385;$d$&#32500;&#27969;&#24418;&#19978;$n$&#20010;&#24102;&#22122;&#22768;&#35757;&#32451;&#26679;&#26412;&#21450;&#20854;&#26080;&#22122;&#22768;&#23545;&#24212;&#29289;&#26469;&#23637;&#31034;&#23427;&#20204;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#36890;&#36807;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#33021;&#22815;&#26377;&#25928;&#22320;&#21435;&#22122;&#36755;&#20837;&#25968;&#25454;&#21644;&#27491;&#24577;&#20998;&#24067;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#32593;&#32476;&#26550;&#26500;&#19979;&#65292;&#22270;&#34920;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#33268;&#20026;$\displaystyle n^{-\frac{2}{d+2}}\log^4 n$&#38454;&#30340;&#24179;&#26041;&#24191;&#20041;&#35823;&#24046;&#65292;&#35813;&#35823;&#24046;&#21462;&#20915;&#20110;&#27969;&#24418;&#30340;&#20869;&#22312;&#32500;&#24230;&#65292;&#24182;&#19988;&#20165;&#24369;&#20381;&#36182;&#20110;&#26679;&#26412;&#25968;&#37327;$n$&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoencoders have demonstrated remarkable success in learning low-dimensional latent features of high-dimensional data across various applications. Assuming that data are sampled near a low-dimensional manifold, we employ chart autoencoders, which encode data into low-dimensional latent features on a collection of charts, preserving the topology and geometry of the data manifold. Our paper establishes statistical guarantees on the generalization error of chart autoencoders, and we demonstrate their denoising capabilities by considering $n$ noisy training samples, along with their noise-free counterparts, on a $d$-dimensional manifold. By training autoencoders, we show that chart autoencoders can effectively denoise the input data with normal noise. We prove that, under proper network architectures, chart autoencoders achieve a squared generalization error in the order of $\displaystyle n^{-\frac{2}{d+2}}\log^4 n$, which depends on the intrinsic dimension of the manifold and only weakly
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#36335;&#24452;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#20805;&#20998;&#21033;&#29992;&#22270;&#20687;&#21464;&#24418;&#22120;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#27169;&#20223;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09857</link><description>&lt;p&gt;
&#22270;&#20687;&#21040;&#35270;&#39057;&#36716;&#25442;&#30340;&#21452;&#36335;&#24452;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Dual-path Adaptation from Image to Video Transformers. (arXiv:2303.09857v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#36335;&#24452;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#20805;&#20998;&#21033;&#29992;&#22270;&#20687;&#21464;&#24418;&#22120;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31934;&#30830;&#27169;&#20223;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#22320;&#23558;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;ViT&#21644;Swin&#65289;&#30340;&#20248;&#36234;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#26041;&#27861;&#65292;&#20165;&#20351;&#29992;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#20197;&#21069;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#65292;&#21516;&#26102;&#32771;&#34385;&#31354;&#38388;&#21644;&#26102;&#38388;&#24314;&#27169;&#65292;&#37319;&#29992;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32479;&#19968;&#27169;&#22359;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22270;&#20687;&#21464;&#24418;&#22120;&#30340;&#20195;&#34920;&#24615;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#35270;&#39057;&#27169;&#22411;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#21452;&#36335;&#24452;&#65288;&#20004;&#20010;&#27969;&#65289;&#26550;&#26500;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#36335;&#24452;&#33258;&#36866;&#24212;&#65292;&#20998;&#20026;&#31354;&#38388;&#21644;&#26102;&#38388;&#33258;&#36866;&#24212;&#36335;&#24452;&#65292;&#24182;&#22312;&#27599;&#20010;&#21464;&#24418;&#22120;&#22359;&#20013;&#37319;&#29992;&#36731;&#37327;&#32423;&#29942;&#39048;&#36866;&#37197;&#22120;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#26102;&#38388;&#21160;&#24577;&#24314;&#27169;&#65292;&#25105;&#20204;&#23558;&#36830;&#32493;&#30340;&#24103;&#32452;&#21512;&#25104;&#31867;&#20284;&#20110;&#32593;&#26684;&#30340;&#24103;&#38598;&#65292;&#24182;&#31934;&#30830;&#27169;&#20223;&#35270;&#35273;&#21464;&#25442;&#22120;&#30340;&#33021;&#21147;&#65292;&#21363;&#25512;&#26029;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#32479;&#19968;&#30340;&#35270;&#35282;&#24191;&#27867;&#22320;&#30740;&#31350;&#20102;&#22810;&#20010;&#22522;&#32447;&#22312;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we efficiently transfer the surpassing representation power of the vision foundation models, such as ViT and Swin, for video understanding with only a few trainable parameters. Previous adaptation methods have simultaneously considered spatial and temporal modeling with a unified learnable module but still suffered from fully leveraging the representative capabilities of image transformers. We argue that the popular dual-path (two-stream) architecture in video models can mitigate this problem. We propose a novel DualPath adaptation separated into spatial and temporal adaptation paths, where a lightweight bottleneck adapter is employed in each transformer block. Especially for temporal dynamic modeling, we incorporate consecutive frames into a grid-like frameset to precisely imitate vision transformers' capability that extrapolates relationships between tokens. In addition, we extensively investigate the multiple baselines from a unified perspective in video understanding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;GADFormer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#36712;&#36857;&#19978;&#25191;&#34892;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;&#65292;&#30456;&#27604;&#20854;&#20182;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;GADFormer&#34920;&#29616;&#26356;&#20248;&#65292;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;GADFormer&#22312;&#26816;&#27979;&#24322;&#24120;&#32676;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2303.09841</link><description>&lt;p&gt;
GADFormer:&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36712;&#36857;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GADFormer: An Attention-based Model for Group Anomaly Detection on Trajectories. (arXiv:2303.09841v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#27169;&#22411;&#30340;GADFormer&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#36712;&#36857;&#19978;&#25191;&#34892;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;&#65292;&#30456;&#27604;&#20854;&#20182;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;GADFormer&#34920;&#29616;&#26356;&#20248;&#65292;&#22312;&#22810;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#34920;&#26126;GADFormer&#22312;&#26816;&#27979;&#24322;&#24120;&#32676;&#20307;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;(GAD)&#21487;&#20197;&#25581;&#31034;&#30001;&#22810;&#20010;&#25104;&#21592;&#23454;&#20363;&#32452;&#25104;&#30340;&#32676;&#20307;&#20013;&#24322;&#24120;&#34892;&#20026;&#65292;&#28982;&#32780;&#38543;&#30528;&#32676;&#20307;&#25104;&#21592;&#25968;&#37327;&#21644;&#24322;&#26500;&#24615;&#30340;&#22686;&#21152;&#65292;&#23454;&#38469;&#19978;&#30340;&#24322;&#24120;&#32676;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#26816;&#27979;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#35774;&#23450;&#19979;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#20307;&#31995;&#32467;&#26500;&#30340;GAD&#19987;&#29992;&#27169;&#22411;GADFormer&#65292;&#23427;&#33021;&#22815;&#22312;&#36712;&#36857;&#19978;&#25191;&#34892;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32676;&#20307;&#24322;&#24120;&#26816;&#27979;,&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#22312;&#26816;&#27979;&#39640;&#31934;&#24230;&#29575;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group Anomaly Detection (GAD) reveals anomalous behavior among groups consisting of multiple member instances, which are, individually considered, not necessarily anomalous. This task is of major importance across multiple disciplines, in which also sequences like trajectories can be considered as a group. However, with increasing amount and heterogenity of group members, actual abnormal groups get harder to detect, especially in an unsupervised or semi-supervised setting. Recurrent Neural Networks are well established deep sequence models, but recent works have shown that their performance can decrease with increasing sequence lengths. Hence, we introduce with this paper GADFormer, a GAD specific BERT architecture, capable to perform attention-based Group Anomaly Detection on trajectories in an unsupervised and semi-supervised setting. We show formally and experimentally how trajectory outlier detection can be realized as an attention-based Group Anomaly Detection problem. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;</title><link>http://arxiv.org/abs/2303.09823</link><description>&lt;p&gt;
Transformers&#21644;Ensemble&#26041;&#27861;&#65306;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#21152;CERIST NLP&#25361;&#25112;&#36187;2022&#20013;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#23454;&#39564;&#36807;&#31243;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;6&#20010;Transformer&#27169;&#22411;&#21450;&#20854;&#32452;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;2&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#35757;&#32451;&#38598;&#19978;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#20026;F1&#20998;&#25968;&#20026;0.60&#65292;&#20934;&#30830;&#24615;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.09780</link><description>&lt;p&gt;
Mpox-AISM&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36229;&#32423;&#30417;&#27979;&#20197;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Mpox-AISM: AI-Mediated Super Monitoring for Forestalling Monkeypox Spread. (arXiv:2303.09780v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21450;&#26102;&#12289;&#20415;&#25463;&#21644;&#20934;&#30830;&#35786;&#26029;&#26089;&#26399;&#24739;&#32773;&#26159;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#12289;&#23454;&#26102;&#30340;&#22312;&#32447;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65292;&#29992;&#20110;&#26500;&#24314;&#20302;&#25104;&#26412;&#12289;&#26041;&#20415;&#12289;&#21450;&#26102;&#21644;&#26080;&#19987;&#19994;&#30693;&#35782;&#30340;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#32452;&#35013;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20171;&#23548;&#30340;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65288;Mpox-AISM&#65289;&#65292;&#26681;&#25454;&#25968;&#25454;&#38598;&#29305;&#24449;&#21644;&#29492;&#30168;&#28436;&#21464;&#36235;&#21183;&#20197;&#21450;&#19982;&#39640;&#30456;&#20284;&#24230;&#30340;&#20854;&#20182;&#19971;&#31181;&#30382;&#32932;&#30149;&#30340;&#19987;&#19994;&#20998;&#31867;&#65292;&#22240;&#27492;&#36825;&#20123;&#21151;&#33021;&#19982;&#21512;&#29702;&#30340;&#31243;&#24207;&#30028;&#38754;&#21644;&#38408;&#20540;&#35774;&#32622;&#30830;&#20445;&#20102;&#20854;&#28789;&#25935;&#24230;&#36229;&#36807;95.9&#65285;&#65292;&#29305;&#24322;&#24230;&#20960;&#20046;&#36798;&#21040;100&#65285;&#12290;&#22240;&#27492;&#65292;&#22312;&#20114;&#32852;&#32593;&#21644;&#36890;&#35759;&#32456;&#31471;&#30340;&#20113;&#26381;&#21153;&#30340;&#24110;&#21161;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#28508;&#22312;&#22320;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#29492;&#30168;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge on forestalling monkeypox (Mpox) spread is the timely, convenient and accurate diagnosis for earlystage infected individuals. Here, we propose a remote and realtime online visualization strategy, called "Super Monitoring" to construct a low cost, convenient, timely and unspecialized diagnosis of early-stage Mpox. Such AI-mediated "Super Monitoring" (Mpox-AISM) invokes a framework assembled by deep learning, data augmentation and self-supervised learning, as well as professionally classifies four subtypes according to dataset characteristics and evolution trend of Mpox and seven other types of dermatopathya with high similarity, hence these features together with reasonable program interface and threshold setting ensure that its Recall (Sensitivity) was beyond 95.9% and the specificity was almost 100%. As a result, with the help of cloud service on Internet and communication terminal, this strategy can be potentially utilized for the real-time detection of earlystage Mpox 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;SE-GSL&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#29109;&#21644;&#32534;&#30721;&#26641;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#23884;&#20837;&#20449;&#24687;&#20869;&#23481;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#22768;&#24930;&#26500;&#24314;&#26368;&#20248;&#32534;&#30721;&#26641;&#30340;&#26041;&#26696;&#12290;&#35813;&#26694;&#26550;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#33410;&#28857;&#32467;&#26500;&#29109;&#20998;&#24067;&#26469;&#24674;&#22797;&#22270;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.09778</link><description>&lt;p&gt;
SE-GSL&#65306;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#29109;&#20248;&#21270;&#23454;&#29616;&#36890;&#29992;&#26377;&#25928;&#22270;&#32467;&#26500;&#23398;&#20064;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
SE-GSL: A General and Effective Graph Structure Learning Framework through Structural Entropy Optimization. (arXiv:2303.09778v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;SE-GSL&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#29109;&#21644;&#32534;&#30721;&#26641;&#20013;&#30340;&#23618;&#27425;&#32467;&#26500;&#26469;&#26368;&#22823;&#21270;&#23884;&#20837;&#20449;&#24687;&#20869;&#23481;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#22768;&#24930;&#26500;&#24314;&#26368;&#20248;&#32534;&#30721;&#26641;&#30340;&#26041;&#26696;&#12290;&#35813;&#26694;&#26550;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#33410;&#28857;&#32467;&#26500;&#29109;&#20998;&#24067;&#26469;&#24674;&#22797;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#23398;&#20064;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290; &#28982;&#32780;&#65292;&#23427;&#23481;&#26131;&#21463;&#21040;&#20302;&#36136;&#37327;&#21644;&#19981;&#21487;&#38752;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#26159;&#24120;&#24577;&#32780;&#19981;&#26159;&#20363;&#22806;&#12290;&#29616;&#26377;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#20173;&#28982;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#29109;&#21644;&#32534;&#30721;&#26641;&#20013;&#25277;&#35937;&#30340;&#22270;&#23618;&#27425;&#32467;&#26500;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GSL&#26694;&#26550;SE-GSL&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#32500;&#32467;&#26500;&#29109;&#26469;&#26368;&#22823;&#21270;&#23884;&#20837;&#20449;&#24687;&#20869;&#23481;&#65292;&#24403;&#36741;&#21161;&#37051;&#22495;&#23646;&#24615;&#34987;&#34701;&#21512;&#20197;&#22686;&#24378;&#21407;&#22987;&#22270;&#26102;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#26368;&#20248;&#32534;&#30721;&#26641;&#30340;&#26032;&#26041;&#26696;&#65292;&#20197;&#22312;&#20998;&#23618;&#25277;&#35937;&#20013;&#26368;&#23567;&#21270;&#22270;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#38899;&#65292;&#21516;&#26102;&#30830;&#20445;&#36866;&#24403;&#30340;&#31038;&#21306;&#21010;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#33410;&#28857;&#32467;&#26500;&#29109;&#20998;&#24067;&#26469;&#24674;&#22797;&#22270;&#32467;&#26500;&#12290;&#23427;&#22686;&#21152;&#20102;&#26356;&#22823;&#19981;&#30830;&#23450;&#24615;&#30340;&#33410;&#28857;&#20043;&#38388;&#30340;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are de facto solutions to structural data learning. However, it is susceptible to low-quality and unreliable structure, which has been a norm rather than an exception in real-world graphs. Existing graph structure learning (GSL) frameworks still lack robustness and interpretability. This paper proposes a general GSL framework, SE-GSL, through structural entropy and the graph hierarchy abstracted in the encoding tree. Particularly, we exploit the one-dimensional structural entropy to maximize embedded information content when auxiliary neighbourhood attributes are fused to enhance the original graph. A new scheme of constructing optimal encoding trees is proposed to minimize the uncertainty and noises in the graph whilst assuring proper community partition in hierarchical abstraction. We present a novel sample-based mechanism for restoring the graph structure via node structural entropy distribution. It increases the connectivity among nodes with larger unce
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#22238;&#24402;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#36716;&#21270;&#20026;QUBO&#65292;&#23558;&#20915;&#31574;&#26641;&#30340;&#20915;&#31574;&#35268;&#21017;&#25193;&#23637;&#21040;&#22810;&#32500;&#36793;&#30028;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#20915;&#31574;&#26641;&#30340;&#20934;&#30830;&#24230;&#12290;&#20351;&#29992;&#36864;&#28779;&#26426;&#22120;&#35299;&#20915;&#36825;&#19968;&#25193;&#23637;&#36890;&#24120;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.09772</link><description>&lt;p&gt;
QUBO&#20915;&#31574;&#26641;&#65306;&#36864;&#28779;&#26426;&#22120;&#25193;&#23637;&#20915;&#31574;&#26641;&#20998;&#35010;
&lt;/p&gt;
&lt;p&gt;
QUBO Decision Tree: Annealing Machine Extends Decision Tree Splitting. (arXiv:2303.09772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#22238;&#24402;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35757;&#32451;&#36807;&#31243;&#36716;&#21270;&#20026;QUBO&#65292;&#23558;&#20915;&#31574;&#26641;&#30340;&#20915;&#31574;&#35268;&#21017;&#25193;&#23637;&#21040;&#22810;&#32500;&#36793;&#30028;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#20915;&#31574;&#26641;&#30340;&#20934;&#30830;&#24230;&#12290;&#20351;&#29992;&#36864;&#28779;&#26426;&#22120;&#35299;&#20915;&#36825;&#19968;&#25193;&#23637;&#36890;&#24120;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#20540;&#20248;&#21270;&#65288;QUBO&#65289;&#25193;&#23637;&#22238;&#24402;&#26641;&#30340;&#26041;&#27861;&#12290;&#22238;&#24402;&#26641;&#26159;&#38750;&#24120;&#27969;&#34892;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#35757;&#32451;&#34920;&#26684;&#25968;&#25454;&#38598;&#65292;&#20294;&#20854;&#20934;&#30830;&#24230;&#19981;&#36275;&#65292;&#22240;&#20026;&#20915;&#31574;&#35268;&#21017;&#36807;&#20110;&#31616;&#21333;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;QUBO&#36716;&#25442;&#35757;&#32451;&#36807;&#31243;&#65292;&#23558;&#20915;&#31574;&#26641;&#30340;&#20915;&#31574;&#35268;&#21017;&#25193;&#23637;&#21040;&#22810;&#32500;&#36793;&#30028;&#12290;&#36825;&#31181;&#25193;&#23637;&#36890;&#24120;&#30001;&#20110;&#35745;&#31639;&#38480;&#21046;&#32780;&#26080;&#27861;&#23454;&#29616;&#65292;&#28982;&#32780;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#36864;&#28779;&#26426;&#22120;&#33021;&#22815;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an extension of regression trees by quadratic unconstrained binary optimization (QUBO). Regression trees are very popular prediction models that are trainable with tabular datasets, but their accuracy is insufficient because the decision rules are too simple. The proposed method extends the decision rules in decision trees to multi-dimensional boundaries. Such an extension is generally unimplementable because of computational limitations, however, the proposed method transforms the training process to QUBO, which enables an annealing machine to solve this problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (DDAE) &#26159;&#21542;&#33021;&#36890;&#36807;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#35757;&#32451;&#33719;&#21462;&#24378;&#26377;&#21147;&#30340;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#65292;&#32467;&#26524;&#34920;&#26126;DDAE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#23545;&#20110;&#33258;&#30417;&#30563;&#29983;&#25104;&#21644;&#36776;&#21035;&#24615;&#23398;&#20064;&#26159;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;95.9&#65285;&#21644;50.0&#65285;&#30340;&#32447;&#24615;&#25506;&#27979;&#31934;&#24230;&#65292;&#19982;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2303.09769</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;&#26159;&#32479;&#19968;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Autoencoders are Unified Self-supervised Learners. (arXiv:2303.09769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120; (DDAE) &#26159;&#21542;&#33021;&#36890;&#36807;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#35757;&#32451;&#33719;&#21462;&#24378;&#26377;&#21147;&#30340;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#65292;&#32467;&#26524;&#34920;&#26126;DDAE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;&#65292;&#23545;&#20110;&#33258;&#30417;&#30563;&#29983;&#25104;&#21644;&#36776;&#21035;&#24615;&#23398;&#20064;&#26159;&#36890;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;95.9&#65285;&#21644;50.0&#65285;&#30340;&#32447;&#24615;&#25506;&#27979;&#31934;&#24230;&#65292;&#19982;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#31867;&#20284;&#20110;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#39044;&#35757;&#32451;&#33719;&#21462;&#20998;&#31867;&#30340;&#36776;&#21035;&#24615;&#34920;&#31034;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32593;&#32476;&#65292;&#21363;&#21435;&#22122;&#25193;&#25955;&#33258;&#32534;&#30721;&#22120;(DDAE)&#26159;&#32479;&#19968;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22120;:&#36890;&#36807;&#22312;&#26080;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;DDAE&#24050;&#32463;&#22312;&#20013;&#38388;&#23618;&#23398;&#20064;&#21040;&#20102;&#24378;&#26377;&#21147;&#30340;&#32447;&#24615;&#21487;&#20998;&#34920;&#31034;&#65292;&#32780;&#26080;&#38656;&#36741;&#21161;&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#20351;&#25193;&#25955;&#39044;&#35757;&#32451;&#25104;&#20026;&#33258;&#30417;&#30563;&#29983;&#25104;&#21644;&#36776;&#21035;&#24615;&#23398;&#20064;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22810;&#31867;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#32447;&#24615;&#25506;&#27979;&#21644;&#24494;&#35843;&#35780;&#20272;&#12290;&#25105;&#20204;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#22312;CIFAR-10&#21644;Tiny-ImageNet&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;95.9&#65285;&#21644;50.0&#65285;&#30340;&#32447;&#24615;&#25506;&#27979;&#31934;&#24230;&#65292;&#19982;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#21644;&#23545;&#27604;&#23398;&#20064;&#39318;&#27425;&#21487;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#20174;Image&#19978;&#30340;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inspired by recent advances in diffusion models, which are reminiscent of denoising autoencoders, we investigate whether they can acquire discriminative representations for classification via generative pre-training. This paper shows that the networks in diffusion models, namely denoising diffusion autoencoders (DDAE), are unified self-supervised learners: by pre-training on unconditional image generation, DDAE has already learned strongly linear-separable representations at its intermediate layers without auxiliary encoders, thus making diffusion pre-training emerge as a general approach for self-supervised generative and discriminative learning. To verify this, we perform linear probe and fine-tuning evaluations on multi-class datasets. Our diffusion-based approach achieves 95.9% and 50.0% linear probe accuracies on CIFAR-10 and Tiny-ImageNet, respectively, and is comparable to masked autoencoders and contrastive learning for the first time. Additionally, transfer learning from Image
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26377;&#20851;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.09767</link><description>&lt;p&gt;
&#20851;&#20110;&#25968;&#25454;&#30340;&#19968;&#20999;&#65306;&#23545;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26377;&#20851;&#25968;&#25454;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;&#65292;&#31995;&#32479;&#22320;&#24635;&#32467;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#21644;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#26679;&#26412;&#26159;&#25915;&#20987;&#32773;&#26377;&#24847;&#35774;&#35745;&#29992;&#20110;&#28151;&#28102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#20415;&#20854;&#29359;&#38169;&#30340;&#36755;&#20837;&#12290;&#36825;&#20123;&#26679;&#26412;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#30340;&#36866;&#29992;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28041;&#21450;&#29983;&#21629;&#21644;&#23433;&#20840;&#30340;&#39046;&#22495;&#65292;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23545;&#25239;&#40065;&#26834;&#24615;&#39046;&#22495;&#30740;&#31350;&#23545;&#25239;&#25915;&#20987;&#26426;&#21046;&#21644;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#26377;&#20851;&#27169;&#22411;&#20351;&#29992;&#30340;&#25968;&#25454;&#23545;&#20854;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#24433;&#21709;&#30340;&#25991;&#29486;&#12290;&#23427;&#31995;&#32479;&#22320;&#35782;&#21035;&#21644;&#24635;&#32467;&#20102;&#36825;&#20010;&#39046;&#22495;&#20869;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#30693;&#35782;&#30340;&#24046;&#36317;&#21644;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine-learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews literature that focuses on the effects of data used by a model on the model's adversarial robustness. It systematically identifies and summarizes the state-of-the-art research in this area and further discusses gaps of knowledge and promising future research directions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;SIMP&#31561;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#20316;&#20026;&#31934;&#21046;&#26426;&#21046;&#25972;&#21512;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25299;&#25169;&#32467;&#26500;&#20013;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20986;&#36739;&#20256;&#32479;&#25299;&#25169;&#20248;&#21270;&#21644;&#20854;&#20182;&#23398;&#20064;&#27169;&#22411;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09760</link><description>&lt;p&gt;
&#25193;&#25955;&#26368;&#20248;&#25299;&#25169;&#32467;&#26500;&#65306;&#19968;&#31181;&#29983;&#25104;&#24335;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diffusing the Optimal Topology: A Generative Optimization Approach. (arXiv:2303.09760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09760
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;SIMP&#31561;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#20316;&#20026;&#31934;&#21046;&#26426;&#21046;&#25972;&#21512;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25299;&#25169;&#32467;&#26500;&#20013;&#12290;&#35813;&#26041;&#27861;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#29616;&#20986;&#36739;&#20256;&#32479;&#25299;&#25169;&#20248;&#21270;&#21644;&#20854;&#20182;&#23398;&#20064;&#27169;&#22411;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25299;&#25169;&#20248;&#21270;&#26088;&#22312;&#23547;&#25214;&#22312;&#28385;&#36275;&#19968;&#31995;&#21015;&#32422;&#26463;&#26465;&#20214;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#31995;&#32479;&#24615;&#33021;&#30340;&#26368;&#20339;&#35774;&#35745;&#12290;&#20256;&#32479;&#30340;&#36845;&#20195;&#20248;&#21270;&#26041;&#27861;&#22914;SIMP&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#22797;&#26434;&#25110;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#25299;&#25169;&#20248;&#21270;&#36807;&#31243;&#65292;&#20294;&#26159;&#24403;&#38754;&#20020;&#36229;&#20986;&#20998;&#24067;&#32422;&#26463;&#37197;&#32622;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#29983;&#25104;&#20855;&#26377;&#28014;&#21160;&#26448;&#26009;&#21644;&#20302;&#24615;&#33021;&#30340;&#35774;&#35745;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#26465;&#20214;&#32422;&#26463;&#21644;&#29289;&#29702;&#22330;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#24191;&#27867;&#30340;&#39044;&#22788;&#29702;&#21644;&#20195;&#29702;&#27169;&#22411;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23558;&#20687;SIMP&#36825;&#26679;&#30340;&#32463;&#20856;&#20248;&#21270;&#20316;&#20026;&#25299;&#25169;&#32467;&#26500;&#30340;&#31934;&#21046;&#26426;&#21046;&#25972;&#21512;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25299;&#25169;&#32467;&#26500;&#20013;&#12290;&#25105;&#20204;&#36824;&#23558;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20854;&#19982;&#20256;&#32479;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#21644;&#20854;&#20182;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#30456;&#27604;&#30340;&#20248;&#33391;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology Optimization seeks to find the best design that satisfies a set of constraints while maximizing system performance. Traditional iterative optimization methods like SIMP can be computationally expensive and get stuck in local minima, limiting their applicability to complex or large-scale problems. Learning-based approaches have been developed to accelerate the topology optimization process, but these methods can generate designs with floating material and low performance when challenged with out-of-distribution constraint configurations. Recently, deep generative models, such as Generative Adversarial Networks and Diffusion Models, conditioned on constraints and physics fields have shown promise, but they require extensive pre-processing and surrogate models for improving performance. To address these issues, we propose a Generative Optimization method that integrates classic optimization like SIMP as a refining mechanism for the topology generated by a deep generative model. W
&lt;/p&gt;</description></item><item><title>CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09752</link><description>&lt;p&gt;
CoLT5: &#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;&#24555;&#36895;&#38271;&#36317;&#31163;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09752
&lt;/p&gt;
&lt;p&gt;
CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#20294;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#25991;&#26723;&#24456;&#26114;&#36149;&#8212;&#8212;&#36825;&#19981;&#20165;&#26159;&#22240;&#20026;&#20108;&#27425;&#27880;&#24847;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#21069;&#39304;&#21644;&#25237;&#24433;&#23618;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#26631;&#35760;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoLT5&#65292;&#19968;&#31181;&#38271;&#36755;&#20837;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#35745;&#31639;&#26469;&#21033;&#29992;&#27492;&#30452;&#35273;&#65292;&#22312;&#21069;&#39304;&#21644;&#27880;&#24847;&#23618;&#20013;&#20026;&#37325;&#35201;&#26631;&#35760;&#25552;&#20379;&#26356;&#22810;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoLT5&#27604;LongT5&#34920;&#29616;&#26356;&#24378;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26356;&#24555;&#65292;&#22312;&#38271;&#36755;&#20837;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;SOTA&#12290;&#27492;&#22806;&#65292;CoLT5&#33021;&#22815;&#26377;&#25928;&#19988;&#21487;&#25511;&#22320;&#21033;&#29992;&#26497;&#38271;&#30340;&#36755;&#20837;&#65292;&#23637;&#31034;&#20102;&#39640;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#30340;&#24378;&#22823;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;C2IR&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22266;&#23450;&#27169;&#22411;&#30340;&#22270;&#20687;&#21360;&#35937;&#26469;&#24674;&#22797;&#31867;&#26465;&#20214;&#30340;&#29305;&#24449;&#32479;&#35745;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#36798;&#21040;&#20102;&#19982;&#20840;&#25968;&#25454;&#35775;&#38382;&#30340;&#26816;&#27979;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09746</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#26465;&#20214;&#21360;&#35937;&#20877;&#29616;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-distribution Examples via Class-conditional Impressions Reappearing. (arXiv:2303.09746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26080;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;C2IR&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22266;&#23450;&#27169;&#22411;&#30340;&#22270;&#20687;&#21360;&#35937;&#26469;&#24674;&#22797;&#31867;&#26465;&#20214;&#30340;&#29305;&#24449;&#32479;&#35745;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#36798;&#21040;&#20102;&#19982;&#20840;&#25968;&#25454;&#35775;&#38382;&#30340;&#26816;&#27979;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#25552;&#39640;&#26631;&#20934;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#20174;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#20013;&#21306;&#20998;&#24322;&#24120;&#36755;&#20837;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#26159;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#30001;&#20110;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36741;&#21161;&#25968;&#25454;&#24448;&#24448;&#19981;&#20999;&#23454;&#38469;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#33258;&#28982;&#25968;&#25454;&#35757;&#32451;&#30340;&#25968;&#25454;&#26080;&#20851;&#26041;&#27861;&#65292;&#31216;&#20026;&#31867;&#26465;&#20214;&#21360;&#35937;&#20877;&#29616;&#65288;C2IR&#65289;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22266;&#23450;&#27169;&#22411;&#30340;&#22270;&#20687;&#21360;&#35937;&#26469;&#24674;&#22797;&#31867;&#26465;&#20214;&#30340;&#29305;&#24449;&#32479;&#35745;&#20449;&#24687;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#65292;&#20272;&#35745;&#23618;&#32423;&#21035;&#30340;&#31867;&#26465;&#20214;&#20559;&#24046;&#65292;&#24182;&#36890;&#36807;&#27979;&#37327;&#22522;&#20110;&#26799;&#24230;&#30340;&#37325;&#35201;&#24615;&#65288;MGI&#65289;&#33719;&#21462;&#23618;&#26435;&#37325;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;C2IR&#20248;&#20110;&#20854;&#20182;&#21518;&#32493;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#20840;&#35775;&#38382;&#65288;ID&#21644;OOD&#65289;&#26816;&#27979;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection aims at enhancing standard deep neural networks to distinguish anomalous inputs from original training data. Previous progress has introduced various approaches where the in-distribution training data and even several OOD examples are prerequisites. However, due to privacy and security, auxiliary data tends to be impractical in a real-world scenario. In this paper, we propose a data-free method without training on natural data, called Class-Conditional Impressions Reappearing (C2IR), which utilizes image impressions from the fixed model to recover class-conditional feature statistics. Based on that, we introduce Integral Probability Metrics to estimate layer-wise class-conditional deviations and obtain layer weights by Measuring Gradient-based Importance (MGI). The experiments verify the effectiveness of our method and indicate that C2IR outperforms other post-hoc methods and reaches comparable performance to the full access (ID and OOD) detection me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20803;&#26641;&#19978;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.09705</link><description>&lt;p&gt;
&#22312;&#20803;&#26641;&#19978;&#25209;&#37327;&#26356;&#26032;&#21518;&#39564;&#26641;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Updating of a Posterior Tree Distribution over a Meta-Tree. (arXiv:2303.09705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20803;&#26641;&#19978;&#35745;&#31639;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#21069;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#19981;&#21487;&#35266;&#23519;&#30340;&#26641;&#21644;&#19968;&#20010;&#24207;&#21015;&#26356;&#26032;&#26041;&#27861;&#34920;&#31034;&#30340;&#27010;&#29575;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#35745;&#31639;&#19968;&#32452;&#26641;&#19978;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#35813;&#38598;&#21512;&#31216;&#20026;&#20803;&#26641;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#25209;&#37327;&#26356;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previously, we proposed a probabilistic data generation model represented by an unobservable tree and a sequential updating method to calculate a posterior distribution over a set of trees. The set is called a meta-tree. In this paper, we propose a more efficient batch updating method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;LSTM&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#29992;&#20110;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#21487;&#20197;&#20174;&#20004;&#20010;&#26041;&#21521;&#21457;&#29616;&#38544;&#34255;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09703</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#21452;&#21521;LSTM&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#8212;&#8212;&#20197;&#39118;&#33021;&#25968;&#25454;&#38598;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Bi-LSTM Autoencoder Framework for Anomaly Detection -- A Case Study of a Wind Power Dataset. (arXiv:2303.09703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;LSTM&#33258;&#32534;&#30721;&#22120;&#26694;&#26550;&#29992;&#20110;&#21457;&#29616;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#65292;&#21487;&#20197;&#20174;&#20004;&#20010;&#26041;&#21521;&#21457;&#29616;&#38544;&#34255;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#25351;&#20559;&#31163;&#27491;&#24120;&#21644;&#22343;&#19968;&#20107;&#20214;&#30340;&#25968;&#25454;&#28857;&#25110;&#20107;&#20214;&#65292;&#21253;&#25324;&#27450;&#35784;&#27963;&#21160;&#12289;&#32593;&#32476;&#28183;&#36879;&#12289;&#35774;&#22791;&#25925;&#38556;&#12289;&#24037;&#33402;&#21464;&#21270;&#25110;&#20854;&#20182;&#37325;&#35201;&#20294;&#19981;&#39057;&#32321;&#21457;&#29983;&#30340;&#20107;&#20214;&#12290;&#21450;&#26102;&#21457;&#29616;&#27492;&#31867;&#20107;&#20214;&#21487;&#20197;&#36991;&#20813;&#28508;&#22312;&#30340;&#36130;&#21153;&#12289;&#20449;&#24687;&#21644;&#20154;&#21147;&#36164;&#28304;&#25439;&#22833;&#12290;&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#25552;&#39640;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#65292;&#24322;&#24120;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20854;&#20013;&#65292;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#30001;&#20110;&#26102;&#38388;&#32500;&#24230;&#24102;&#26469;&#30340;&#39069;&#22806;&#22797;&#26434;&#24615;&#32780;&#26368;&#36817;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;(Bi-LSTM)&#32467;&#26500;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#12290;Bi-LSTM&#32593;&#32476;&#30001;&#20004;&#20010;&#21333;&#21521;LSTM&#32593;&#32476;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#20004;&#20010;&#26041;&#21521;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#21457;&#29616;&#38544;&#34255;&#30340;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomalies refer to data points or events that deviate from normal and homogeneous events, which can include fraudulent activities, network infiltrations, equipment malfunctions, process changes, or other significant but infrequent events. Prompt detection of such events can prevent potential losses in terms of finances, information, and human resources. With the advancement of computational capabilities and the availability of large datasets, anomaly detection has become a major area of research. Among these, anomaly detection in time series has gained more attention recently due to the added complexity imposed by the time dimension. This study presents a novel framework for time series anomaly detection using a combination of Bidirectional Long Short Term Memory (Bi-LSTM) architecture and Autoencoder. The Bi-LSTM network, which comprises two unidirectional LSTM networks, can analyze the time series data from both directions and thus effectively discover the long-term dependencies hidd
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#38142;&#25509;&#25512;&#33616;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23545;&#31038;&#20132;&#32593;&#32476;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#38142;&#25509;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#24310;&#36831;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.09700</link><description>&lt;p&gt;
&#25512;&#33616;&#38142;&#25509;&#30340;&#24310;&#26102;&#21644;&#38388;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Delayed and Indirect Impacts of Link Recommendations. (arXiv:2303.09700v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#38142;&#25509;&#25512;&#33616;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23545;&#31038;&#20132;&#32593;&#32476;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#25512;&#33616;&#38142;&#25509;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#24310;&#36831;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#38142;&#25509;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#24456;&#38590;&#35780;&#20272;&#65292;&#36804;&#20170;&#20026;&#27490;&#30740;&#31350;&#21463;&#38480;&#20110;&#26377;&#38480;&#30340;&#29615;&#22659;&#12290;&#35266;&#23519;&#24615;&#30740;&#31350;&#21463;&#38480;&#20110;&#23427;&#20204;&#25152;&#33021;&#22238;&#31572;&#30340;&#22240;&#26524;&#38382;&#39064;&#30340;&#31181;&#31867;&#65292;&#22825;&#30495;&#30340; A/B &#27979;&#35797;&#24120;&#24120;&#20250;&#30001;&#20110;&#26410;&#32771;&#34385;&#21040;&#30340;&#32593;&#32476;&#24178;&#25200;&#32780;&#23548;&#33268;&#20559;&#35265;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#32463;&#24120;&#23616;&#38480;&#20110;&#38745;&#24577;&#32593;&#32476;&#27169;&#22411;&#65292;&#19981;&#32771;&#34385;&#38142;&#25509;&#25512;&#33616;&#21644;&#26377;&#26426;&#32593;&#32476;&#28436;&#21270;&#20043;&#38388;&#30340;&#28508;&#22312;&#21453;&#39304;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#25512;&#33616;&#23545;&#31038;&#20132;&#32593;&#32476;&#30340;&#24433;&#21709;&#12290;&#37319;&#29992;&#27169;&#25311;&#26041;&#27861;&#65292;&#32771;&#34385;&#19968;&#20010;&#26174;&#24335;&#30340;&#21160;&#24577;&#24418;&#25104;&#27169;&#22411;&#8212;&#8212;&#33879;&#21517;&#30340;Jackson-Rogers&#27169;&#22411;&#30340;&#25193;&#23637;&#8212;&#8212;&#24182;&#30740;&#31350;&#38142;&#25509;&#25512;&#33616;&#22914;&#20309;&#38543;&#26102;&#38388;&#24433;&#21709;&#32593;&#32476;&#28436;&#21270;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#38142;&#25509;&#25512;&#33616;&#23545;&#32593;&#32476;&#32467;&#26500;&#23646;&#24615;&#26377;&#20196;&#20154;&#24778;&#35766;&#30340;&#24310;&#36831;&#21644;&#38388;&#25509;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impacts of link recommendations on social networks are challenging to evaluate, and so far they have been studied in limited settings. Observational studies are restricted in the kinds of causal questions they can answer and naive A/B tests often lead to biased evaluations due to unaccounted network interference. Furthermore, evaluations in simulation settings are often limited to static network models that do not take into account the potential feedback loops between link recommendation and organic network evolution. To this end, we study the impacts of recommendations on social networks in dynamic settings. Adopting a simulation-based approach, we consider an explicit dynamic formation model -- an extension of the celebrated Jackson-Rogers model -- and investigate how link recommendations affect network evolution over time. Empirically, we find that link recommendations have surprising delayed and indirect effects on the structural properties of networks. Specifically, we find th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#39044;&#27979;&#31163;&#25955;&#26102;&#38388;&#20116;&#31181;&#26412;&#22320;&#20998;&#23700;&#65292;&#22312;&#32463;&#27982;&#12289;&#29983;&#24577;&#12289;&#29983;&#29702;&#23398;&#31561;&#26041;&#38754;&#30340;&#35797;&#39564;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#20248;&#31168;&#34920;&#29616;&#65292;&#26159;&#25552;&#21069;&#35686;&#21578;&#20851;&#38190;&#36716;&#21464;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09669</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#31163;&#25955;&#26102;&#38388;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Predicting discrete-time bifurcations with deep learning. (arXiv:2303.09669v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#39044;&#27979;&#31163;&#25955;&#26102;&#38388;&#20116;&#31181;&#26412;&#22320;&#20998;&#23700;&#65292;&#22312;&#32463;&#27982;&#12289;&#29983;&#24577;&#12289;&#29983;&#29702;&#23398;&#31561;&#26041;&#38754;&#30340;&#35797;&#39564;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#20248;&#31168;&#34920;&#29616;&#65292;&#26159;&#25552;&#21069;&#35686;&#21578;&#20851;&#38190;&#36716;&#21464;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#21644;&#20154;&#36896;&#31995;&#32479;&#23481;&#26131;&#21457;&#29983;&#20851;&#38190;&#36716;&#21464;-&#21160;&#21147;&#23398;&#30340;&#31361;&#28982;&#21644;&#28508;&#22312;&#30340;&#30772;&#22351;&#24615;&#21464;&#21270;&#12290;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#36890;&#36807;&#20174;&#22823;&#35268;&#27169;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#20998;&#23700;&#65288;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65289;&#30340;&#36890;&#29992;&#29305;&#24449;&#65292;&#20026;&#20851;&#38190;&#36716;&#21464;&#25552;&#20379;&#25552;&#21069;&#35686;&#21578;&#20449;&#21495;&#65288;EWS&#65289;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20998;&#31867;&#22120;&#21482;&#34987;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#36830;&#32493;&#26102;&#38388;&#20998;&#27495;&#65292;&#32780;&#24573;&#30053;&#20102;&#31163;&#25955;&#26102;&#38388;&#20998;&#27495;&#29420;&#29305;&#30340;&#20016;&#23500;&#21160;&#24577;&#29305;&#24449;&#12290;&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#35757;&#32451;&#25552;&#20379; EWS &#30340;&#20116;&#31181;&#31163;&#25955;&#26102;&#38388;&#12289;&#20849;&#32500;&#24230;1&#30340;&#26412;&#22320;&#20998;&#23700;&#12290;&#25105;&#20204;&#22312;&#29983;&#29702;&#23398;&#12289;&#32463;&#27982;&#23398;&#21644;&#29983;&#24577;&#23398;&#20013;&#20351;&#29992;&#30340;&#31163;&#25955;&#26102;&#38388;&#27169;&#22411;&#30340;&#27169;&#25311;&#25968;&#25454;&#20197;&#21450;&#32463;&#21382;&#20102;&#20493;&#22686;&#20998;&#23700;&#30340;&#40481;&#24515;&#32858;&#38598;&#30340;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#24191;&#27867;&#30340;&#22122;&#22768;&#24378;&#24230;&#21644;&#25509;&#36817;&#20998;&#23700;&#30340;&#36895;&#29575;&#33539;&#22260;&#20869;&#65292;&#20998;&#31867;&#22120;&#20248;&#20110;&#24120;&#29992;&#30340; EWS&#12290;&#23427;&#20063;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#20998;&#23700;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural and man-made systems are prone to critical transitions -- abrupt and potentially devastating changes in dynamics. Deep learning classifiers can provide an early warning signal (EWS) for critical transitions by learning generic features of bifurcations (dynamical instabilities) from large simulated training data sets. So far, classifiers have only been trained to predict continuous-time bifurcations, ignoring rich dynamics unique to discrete-time bifurcations. Here, we train a deep learning classifier to provide an EWS for the five local discrete-time bifurcations of codimension-1. We test the classifier on simulation data from discrete-time models used in physiology, economics and ecology, as well as experimental data of spontaneously beating chick-heart aggregates that undergo a period-doubling bifurcation. The classifier outperforms commonly used EWS under a wide range of noise intensities and rates of approach to the bifurcation. It also predicts the correct bifurcation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36127;&#36131;&#20219;&#30340;&#32676;&#20307;&#20998;&#26512;&#35774;&#35745;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;TribalGram&#24037;&#20855;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#23545;&#32676;&#20307;&#20998;&#26512;&#36827;&#34892;&#25512;&#26029;&#35780;&#20272;&#12289;&#27169;&#22411;&#35299;&#37322;&#12289;&#25968;&#25454;&#21327;&#20316;&#21644;&#29702;&#35299;&#65292;&#20197;&#22686;&#24378;&#23545;&#20998;&#26512;&#32676;&#20307;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#24182;&#38450;&#27490;&#22240;&#21051;&#26495;&#21360;&#35937;&#21644;&#36807;&#24230;&#27010;&#25324;&#23548;&#33268;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;</title><link>http://arxiv.org/abs/2303.09664</link><description>&lt;p&gt;
&#29992;TribalGram&#23545;&#32676;&#20307;&#38388;&#24046;&#24322;&#36827;&#34892;&#20851;&#38190;&#26816;&#26597;&#65306;&#37096;&#33853;&#20998;&#26512;&#30340;&#25209;&#21028;&#24615;&#35270;&#23519;
&lt;/p&gt;
&lt;p&gt;
Tribe or Not? Critical Inspection of Group Differences Using TribalGram. (arXiv:2303.09664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#36127;&#36131;&#20219;&#30340;&#32676;&#20307;&#20998;&#26512;&#35774;&#35745;&#25351;&#21335;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;TribalGram&#24037;&#20855;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#23545;&#32676;&#20307;&#20998;&#26512;&#36827;&#34892;&#25512;&#26029;&#35780;&#20272;&#12289;&#27169;&#22411;&#35299;&#37322;&#12289;&#25968;&#25454;&#21327;&#20316;&#21644;&#29702;&#35299;&#65292;&#20197;&#22686;&#24378;&#23545;&#20998;&#26512;&#32676;&#20307;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#24182;&#38450;&#27490;&#22240;&#21051;&#26495;&#21360;&#35937;&#21644;&#36807;&#24230;&#27010;&#25324;&#23548;&#33268;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#25366;&#25496;&#25216;&#26415;&#30340;&#20852;&#36215;&#65292;&#32676;&#20307;&#20998;&#26512;&#21644;&#32676;&#20307;&#23618;&#38754;&#30340;&#20998;&#26512;&#22312;&#21253;&#25324;&#25919;&#31574;&#21046;&#23450;&#21644;&#30452;&#25509;&#33829;&#38144;&#22312;&#20869;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#33021;&#20026;&#19968;&#20010;&#32676;&#20307;&#30340;&#20849;&#21516;&#29305;&#24449;&#25552;&#20379;&#35265;&#35299;&#65307;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#32676;&#20307;&#23618;&#38754;&#30340;&#20998;&#26512;&#21487;&#33021;&#20250;&#23548;&#33268;&#21051;&#26495;&#21360;&#35937;&#21644;&#31995;&#32479;&#24615;&#21387;&#36843;&#12290;&#20998;&#26512;&#24037;&#20855;&#22914;&#20309;&#20419;&#36827;&#26356;&#21152;&#26377;&#24847;&#35782;&#30340;&#32676;&#20307;&#20998;&#26512;&#36807;&#31243;&#65311;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#36127;&#36131;&#20219;&#30340;&#32676;&#20307;&#20998;&#26512;&#35774;&#35745;&#25351;&#21335;&#65292;&#20197;&#38416;&#26126;&#32676;&#20307;&#24046;&#24322;&#30340;&#38656;&#35201;&#21644;&#38450;&#27490;&#23545;&#32676;&#20307;&#30340;&#36807;&#24230;&#27010;&#25324;&#12290;&#36981;&#24490;&#36825;&#20123;&#35774;&#35745;&#25351;&#21335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;TribalGram&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#21487;&#35270;&#21270;&#26469;&#25552;&#20379;&#25512;&#26029;&#35780;&#20272;&#12289;&#27169;&#22411;&#35299;&#37322;&#12289;&#25968;&#25454;&#21327;&#20316;&#21644;&#29702;&#35299;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20855;&#12290;&#36890;&#36807;&#19982;&#39046;&#22495;&#19987;&#23478;&#30340;&#35775;&#35848;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#21644;&#24037;&#20855;&#22914;&#20309;&#24102;&#26469;&#23545;&#20998;&#26512;&#32676;&#20307;&#30340;&#26356;&#28145;&#20837;&#29702;&#35299;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#21051;&#26495;&#21360;&#35937;&#21644;&#36807;&#24230;&#27010;&#25324;&#32780;&#24102;&#26469;&#30340;&#28508;&#22312;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group's shared characteristics; in others, the group-level analysis can lead to problems including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram, a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer under
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;MIMO&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#25569;&#25163;&#31574;&#30053;&#21644;&#22810;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#20840;&#23616;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#29123;&#26009;&#28040;&#32791;&#12289;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09658</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Energy Management of Multi-mode Plug-in Hybrid Electric Vehicle using Multi-agent Deep Reinforcement Learning. (arXiv:2303.09658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;&#33021;&#37327;&#31649;&#29702;&#30340;MIMO&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#25569;&#25163;&#31574;&#30053;&#21644;&#22810;&#30446;&#26631;&#20989;&#25968;&#20248;&#21270;&#20840;&#23616;&#25511;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#29123;&#26009;&#28040;&#32791;&#12289;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#20986;&#29616;&#30340;&#22810;&#27169;&#24335;&#25554;&#30005;&#28151;&#21512;&#21160;&#21147;&#27773;&#36710;(PHEV)&#25216;&#26415;&#26159;&#20943;&#23569;&#30899;&#25490;&#25918;&#30340;&#36884;&#24452;&#20043;&#19968;&#65292;&#20854;&#33021;&#37327;&#31649;&#29702;&#38656;&#35201;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;(MIMO)&#25511;&#21046;&#12290;&#30446;&#21069;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;MIMO&#25511;&#21046;&#35299;&#32806;&#20026;&#21333;&#36755;&#20986;(MISO)&#25511;&#21046;&#65292;&#24182;&#19988;&#21482;&#33021;&#23454;&#29616;&#20854;&#23616;&#37096;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#20840;&#23616;&#20248;&#21270;&#22810;&#27169;&#24335;&#36710;&#36742;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#30340;&#22810;&#27169;&#24335;PHEV&#33021;&#37327;&#31649;&#29702;&#30340;MIMO&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#30456;&#20851;&#27604;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25569;&#25163;&#31574;&#30053;&#65292;&#20351;&#24471;&#20004;&#20010;&#23398;&#20064;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;MADRL&#26694;&#26550;&#19979;&#20351;&#29992;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;(DDPG)&#31639;&#27861;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#24433;&#21709;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#22240;&#32032;&#36827;&#34892;&#28789;&#25935;&#24230;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;DDPG&#26234;&#33021;&#20307;&#30340;&#32479;&#19968;&#35774;&#32622;&#12290;&#25569;&#25163;&#31574;&#30053;&#30340;&#26368;&#20248;&#24037;&#20316;&#27169;&#24335;&#36890;&#36807;&#22810;&#30446;&#26631;&#20989;&#25968;&#24471;&#21040;&#65292;&#32771;&#34385;&#29123;&#26009;&#28040;&#32791;&#12289;&#30005;&#27744;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#36829;&#35268;&#12290;&#22522;&#20110;&#30828;&#20214;&#22312;&#29615;(HIL)&#20223;&#30495;&#22120;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;MADRL&#33021;&#37327;&#31649;&#29702;&#26041;&#27861;&#22312;&#29123;&#26009;&#28040;&#32791;&#12289;SOC&#21464;&#21270;&#21644;&#21151;&#29575;&#38480;&#21046;&#36829;&#35268;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21644;&#21333;&#20010;&#26234;&#33021;&#20307;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recently emerging multi-mode plug-in hybrid electric vehicle (PHEV) technology is one of the pathways making contributions to decarbonization, and its energy management requires multiple-input and multiple-output (MIMO) control. At the present, the existing methods usually decouple the MIMO control into single-output (MISO) control and can only achieve its local optimal performance. To optimize the multi-mode vehicle globally, this paper studies a MIMO control method for energy management of the multi-mode PHEV based on multi-agent deep reinforcement learning (MADRL). By introducing a relevance ratio, a hand-shaking strategy is proposed to enable two learning agents to work collaboratively under the MADRL framework using the deep deterministic policy gradient (DDPG) algorithm. Unified settings for the DDPG agents are obtained through a sensitivity analysis of the influencing factors to the learning performance. The optimal working mode for the hand-shaking strategy is attained thro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ESCAPE&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#26426;&#20132;&#20114;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#24182;&#20462;&#27491;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.09657</link><description>&lt;p&gt;
ESCAPE&#65306;&#36890;&#36807;&#20132;&#20114;&#24335;&#35270;&#35273;&#20998;&#26512;&#28040;&#38500;&#26426;&#22120;&#30340;&#30450;&#21306;&#31995;&#32479;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
ESCAPE: Countering Systematic Errors from Machine's Blind Spots via Interactive Visual Analysis. (arXiv:2303.09657v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ESCAPE&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#21033;&#29992;&#20154;&#26426;&#20132;&#20114;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#24182;&#20462;&#27491;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#31995;&#32479;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27169;&#22411;&#23398;&#20064;&#25512;&#24191;&#25968;&#25454;&#21644;&#30446;&#26631;&#31867;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#22810;&#22320;&#35266;&#23519;&#21040;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#24456;&#23481;&#26131;&#22312;AI&#24212;&#29992;&#31243;&#24207;&#20013;&#23548;&#33268;&#31995;&#32479;&#38169;&#35823;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;AI&#30450;&#21306;&#12290;&#24403;&#27169;&#22411;&#20351;&#29992;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65288;&#20363;&#22914;&#65292;&#29483;/&#29399;&#20998;&#31867;&#65289;&#65292;&#20854;&#20013;&#37325;&#35201;&#27169;&#24335;&#65288;&#20363;&#22914;&#65292;&#40657;&#29483;&#65289;&#32570;&#22833;&#25110;&#21608;&#36793;/&#19981;&#33391;&#27169;&#24335;&#65288;&#20363;&#22914;&#65292;&#33609;&#22320;&#32972;&#26223;&#30340;&#29399;&#65289;&#20250;&#35823;&#23548;&#21040;&#26576;&#20010;&#31867;&#26102;&#65292;&#20250;&#20986;&#29616;&#36825;&#20123;&#30450;&#21306;&#12290;&#29978;&#33267;&#26356;&#22797;&#26434;&#30340;&#25216;&#26415;&#20063;&#19981;&#33021;&#20445;&#35777;&#25429;&#33719;&#12289;&#29702;&#35299;&#21644;&#38450;&#27490;&#20266;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ESCAPE&#30340;&#35270;&#35273;&#20998;&#26512;&#31995;&#32479;&#65292;&#23427;&#20419;&#36827;&#20102;&#28040;&#38500;&#31995;&#32479;&#35823;&#24046;&#30340;&#20154;&#26426;&#20132;&#20114;&#24037;&#20316;&#27969;&#31243;&#12290;&#36890;&#36807;&#20801;&#35768;&#20154;&#31867;&#29992;&#25143;&#36731;&#26494;&#26816;&#26597;&#34394;&#20551;&#20851;&#32852;&#65292;&#35813;&#31995;&#32479;&#26377;&#21161;&#20110;&#29992;&#25143;&#33258;&#21457;&#22320;&#35782;&#21035;&#19982;&#38169;&#35823;&#20998;&#31867;&#30456;&#20851;&#30340;&#27010;&#24565;&#24182;&#35780;&#20272;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification models learn to generalize the associations between data samples and their target classes. However, researchers have increasingly observed that machine learning practice easily leads to systematic errors in AI applications, a phenomenon referred to as AI blindspots. Such blindspots arise when a model is trained with training samples (e.g., cat/dog classification) where important patterns (e.g., black cats) are missing or periphery/undesirable patterns (e.g., dogs with grass background) are misleading towards a certain class. Even more sophisticated techniques cannot guarantee to capture, reason about, and prevent the spurious associations. In this work, we propose ESCAPE, a visual analytic system that promotes a human-in-the-loop workflow for countering systematic errors. By allowing human users to easily inspect spurious associations, the system facilitates users to spontaneously recognize concepts associated misclassifications and evaluate mitigation strategies that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#22270;&#20687;&#37325;&#24314;&#32593;&#32476;&#35757;&#32451;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#19988;&#22312;&#32570;&#23569;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#30340;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2303.09642</link><description>&lt;p&gt;
SUD$^2$:&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#37325;&#24314;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SUD$^2$: Supervision by Denoising Diffusion Models for Image Reconstruction. (arXiv:2303.09642v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#22270;&#20687;&#37325;&#24314;&#32593;&#32476;&#35757;&#32451;&#30340;&#24191;&#20041;&#26694;&#26550;&#65292;&#24182;&#19988;&#22312;&#32570;&#23569;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#35757;&#32451;&#30340;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22270;&#20687;&#21453;&#38382;&#39064;&#65288;&#22914;&#22270;&#20687;&#20462;&#22797;&#21644;&#21435;&#38654;&#65289;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#21069;&#21521;&#27169;&#22411;&#26159;&#26410;&#30693;&#30340;&#65292;&#25110;&#32773;&#20381;&#36182;&#20110;&#26410;&#30693;&#30340;&#28508;&#22312;&#21442;&#25968;&#12290;&#34429;&#28982;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#25104;&#23545;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#36825;&#26679;&#30340;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22270;&#20687;&#37325;&#24314;&#32593;&#32476;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#20687;&#21435;&#22122;&#31639;&#27861;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#22312;&#32570;&#23569;&#25104;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#30417;&#30563;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many imaging inverse problems$\unicode{x2014}$such as image-dependent in-painting and dehazing$\unicode{x2014}$are challenging because their forward models are unknown or depend on unknown latent parameters. While one can solve such problems by training a neural network with vast quantities of paired training data, such paired training data is often unavailable. In this paper, we propose a generalized framework for training image reconstruction networks when paired training data is scarce. In particular, we demonstrate the ability of image denoising algorithms and, by extension, denoising diffusion models to supervise network training in the absence of paired training data.
&lt;/p&gt;</description></item><item><title>CTGCN&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#25216;&#26415;&#20811;&#26381;&#35745;&#31639;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#22240;&#26524;&#24615;&#25552;&#39640;&#20102;&#23545;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.09634</link><description>&lt;p&gt;
&#22240;&#26524;&#26102;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CTGCN&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Temporal Graph Convolutional Neural Networks (CTGCN). (arXiv:2303.09634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09634
&lt;/p&gt;
&lt;p&gt;
CTGCN&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26426;&#21046;&#65292;&#21033;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#25216;&#26415;&#20811;&#26381;&#35745;&#31639;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#22240;&#26524;&#24615;&#25552;&#39640;&#20102;&#23545;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22823;&#35268;&#27169;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20351;&#29992;&#22270;&#24418;&#32467;&#26500;&#36827;&#34892;&#20248;&#38597;&#34920;&#31034;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#21487;&#20280;&#32553;&#24615;&#36890;&#24120;&#21463;&#21040;&#24212;&#29992;&#25152;&#38656;&#30340;&#39046;&#22495;&#30693;&#35782;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#26102;&#38388;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CTGCN&#65289;&#12290;&#25105;&#20204;&#30340;CTGCN&#26550;&#26500;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#26426;&#21046;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#28508;&#22312;&#30340;&#22240;&#26524;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#28857;&#22312;&#20110;&#20854;&#33021;&#22815;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#25216;&#26415;&#26469;&#20811;&#26381;&#35745;&#31639;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#24182;&#19988;&#20351;&#29992;&#22240;&#26524;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#26356;&#39640;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;CTGCN&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23558;&#22240;&#26524;&#24615;&#38598;&#25104;&#21040;TGCN&#26550;&#26500;&#20013;&#21487;&#20197;&#27604;&#20856;&#22411;&#30340;TGCN&#26041;&#27861;&#25552;&#39640;&#26368;&#22810;40&#65285;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36866;&#24212;&#24615;&#24378;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many large-scale applications can be elegantly represented using graph structures. Their scalability, however, is often limited by the domain knowledge required to apply them. To address this problem, we propose a novel Causal Temporal Graph Convolutional Neural Network (CTGCN). Our CTGCN architecture is based on a causal discovery mechanism, and is capable of discovering the underlying causal processes. The major advantages of our approach stem from its ability to overcome computational scalability problems with a divide and conquer technique, and from the greater explainability of predictions made using a causal model. We evaluate the scalability of our CTGCN on two datasets to demonstrate that our method is applicable to large scale problems, and show that the integration of causality into the TGCN architecture improves prediction performance up to 40% over typical TGCN approach. Our results are obtained without requiring additional domain knowledge, making our approach adaptable to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;&#36229;&#38477;&#32500;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38477;&#32500;&#65292;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#35299;&#24555;&#29031;&#30340;&#23376;&#37319;&#26679;&#29256;&#26412;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#20811;&#26381;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;2D Burgers&#38382;&#39064;&#19978;&#28436;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09630</link><description>&lt;p&gt;
&#39640;&#25928;&#20934;&#30830;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38477;&#32500;&#30340;&#36229;&#38477;&#32500;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hyper-Reduced Autoencoders for Efficient and Accurate Nonlinear Model Reductions. (arXiv:2303.09630v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09630
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;&#36229;&#38477;&#32500;&#33258;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#38750;&#32447;&#24615;&#27169;&#22411;&#38477;&#32500;&#65292;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#35299;&#24555;&#29031;&#30340;&#23376;&#37319;&#26679;&#29256;&#26412;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#20811;&#26381;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;2D Burgers&#38382;&#39064;&#19978;&#28436;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25237;&#24433;&#27169;&#22411;&#38477;&#32500;&#22312;&#38750;&#32447;&#24615;&#27969;&#24418;&#19978;&#34987;&#25552;&#20986;&#26469;&#29992;&#20110;&#24930;&#36895;&#19979;&#38477;Kolmogorov n-width&#38382;&#39064;&#30340;&#27169;&#22411;&#38477;&#32500;&#65292;&#22914;&#27969;&#31243;&#20027;&#23548;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20986;&#27604;&#20256;&#32479;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#38477;&#32500;&#27169;&#22411;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#35299;&#24555;&#29031;&#19978;&#35757;&#32451;&#32593;&#32476;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#22312;&#39640;&#20445;&#30495;&#24230;&#30340;&#35299;&#24555;&#29031;&#30340;&#23376;&#37319;&#26679;&#29256;&#26412;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#20811;&#26381;&#36825;&#20010;&#32570;&#28857;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#37325;&#24515;&#25554;&#20540;&#30340;&#36229;&#38477;&#32500;&#21644;Gappy-POD&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#20195;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;2D Burgers&#38382;&#39064;&#19978;&#28436;&#31034;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projection-based model order reduction on nonlinear manifolds has been recently proposed for problems with slowly decaying Kolmogorov n-width such as advection-dominated ones. These methods often use neural networks for manifold learning and showcase improved accuracy over traditional linear subspace-reduced order models. A disadvantage of the previously proposed methods is the potential high computational costs of training the networks on high-fidelity solution snapshots. In this work, we propose and analyze a novel method that overcomes this disadvantage by training a neural network only on subsampled versions of the high-fidelity solution snapshots. This method coupled with collocation-based hyper-reduction and Gappy-POD allows for efficient and accurate surrogate models. We demonstrate the validity of our approach on a 2d Burgers problem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#21608;&#26399;&#24615;MDP&#20013;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;PUCRL2&#65292;PUCRLB&#65292;U-PUCRL2&#21644;U-PUCRLB&#12290;PUCRLB&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#20854;&#36951;&#25022;&#38543;&#21608;&#26399;$N$&#30340;&#21464;&#21270;&#20026;$O(\sqrt{N})$&#12290;</title><link>http://arxiv.org/abs/2303.09629</link><description>&lt;p&gt;
&#21608;&#26399;&#24615;MDP&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Periodic MDP. (arXiv:2303.09629v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#21608;&#26399;&#24615;MDP&#20013;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#22235;&#31181;&#31639;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;PUCRL2&#65292;PUCRLB&#65292;U-PUCRL2&#21644;U-PUCRLB&#12290;PUCRLB&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#20854;&#36951;&#25022;&#38543;&#21608;&#26399;$N$&#30340;&#21464;&#21270;&#20026;$O(\sqrt{N})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21608;&#26399;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29305;&#27530;&#30340;&#38750;&#24179;&#31283;MDP&#65292;&#20854;&#20013;&#29366;&#24577;&#36716;&#31227;&#27010;&#29575;&#21644;&#22870;&#21169;&#20989;&#25968;&#37117;&#20250;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#22312;&#24179;&#22343;&#22870;&#21169;&#26368;&#22823;&#21270;&#30340;&#35774;&#32622;&#19979;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#28155;&#21152;&#21608;&#26399;&#32034;&#24341;&#26469;&#23558;&#38382;&#39064;&#24402;&#32467;&#20026;&#38745;&#24577;MDP&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21608;&#26399;&#24615;&#19978;&#32622;&#20449;&#21306;&#38388;&#24378;&#21270;&#23398;&#20064;-2&#65288;PUCRL2&#65289;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PUCRL2&#30340;&#36951;&#25022;&#20540;&#38543;&#21608;&#26399;$N$&#32447;&#24615;&#21464;&#21270;&#65292;&#24182;&#19988;&#38543;&#30528;&#26102;&#38480;&#38271;&#24230;$T$&#21576;$\mathcal{O}(\sqrt{Tlog T})$&#30340;&#21464;&#21270;&#12290;&#21033;&#29992;&#22686;&#24191;MDP&#30340;&#36716;&#31227;&#30697;&#38453;&#30340;&#31232;&#30095;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21478;&#19968;&#20010;&#31639;&#27861;PUCRLB&#65292;&#23427;&#22312;&#36951;&#25022;&#65288;&#21608;&#26399;&#30340;$O(\sqrt{N})$&#20381;&#36182;&#20851;&#31995;&#65289;&#21644;&#32463;&#39564;&#34920;&#29616;&#26041;&#38754;&#37117;&#27604;PUCRL2&#26356;&#20986;&#33394;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;U-PUCRL2&#21644;U-PUCRLB&#65292;&#29992;&#20110;&#29615;&#22659;&#20013;&#25193;&#23637;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#21608;&#26399;&#26410;&#30693;&#65292;&#20294;&#24050;&#30693;&#19968;&#32452;&#20505;&#36873;&#21608;&#26399;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
We study learning in periodic Markov Decision Process (MDP), a special type of non-stationary MDP where both the state transition probabilities and reward functions vary periodically, under the average reward maximization setting. We formulate the problem as a stationary MDP by augmenting the state space with the period index, and propose a periodic upper confidence bound reinforcement learning-2 (PUCRL2) algorithm. We show that the regret of PUCRL2 varies linearly with the period $N$ and as $\mathcal{O}(\sqrt{Tlog T})$ with the horizon length $T$. Utilizing the information about the sparsity of transition matrix of augmented MDP, we propose another algorithm PUCRLB which enhances upon PUCRL2, both in terms of regret ($O(\sqrt{N})$ dependency on period) and empirical performance. Finally, we propose two other algorithms U-PUCRL2 and U-PUCRLB for extended uncertainty in the environment in which the period is unknown but a set of candidate periods are known. Numerical results demonstrate
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20174;&#28216;&#25103;&#20013;&#39640;&#25928;&#23398;&#20064;&#39640;&#23618;&#27425;&#35745;&#21010;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#38271;&#26399;&#22797;&#26434;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#20182;&#20204;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#28216;&#25103;&#25968;&#25454;&#23398;&#20064;&#22522;&#20110;&#29289;&#20307;&#30340;&#31163;&#25955;&#34892;&#20026;&#20808;&#39564;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#39640;&#23618;&#27425;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#23427;&#20351;&#29992;&#20808;&#39564;&#26469;&#25351;&#23548;&#35268;&#21010;&#21644;&#26500;&#24314;&#22797;&#26434;&#38271;&#26399;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.09628</link><description>&lt;p&gt;
&#20174;&#28216;&#25103;&#20013;&#39640;&#25928;&#23398;&#20064;&#39640;&#23618;&#27425;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of High Level Plans from Play. (arXiv:2303.09628v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20174;&#28216;&#25103;&#20013;&#39640;&#25928;&#23398;&#20064;&#39640;&#23618;&#27425;&#35745;&#21010;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#38271;&#26399;&#22797;&#26434;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#20182;&#20204;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#28216;&#25103;&#25968;&#25454;&#23398;&#20064;&#22522;&#20110;&#29289;&#20307;&#30340;&#31163;&#25955;&#34892;&#20026;&#20808;&#39564;&#65292;&#28982;&#21518;&#35774;&#35745;&#19968;&#20010;&#39640;&#23618;&#27425;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#23427;&#20351;&#29992;&#20808;&#39564;&#26469;&#25351;&#23548;&#35268;&#21010;&#21644;&#26500;&#24314;&#22797;&#26434;&#38271;&#26399;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#21040;&#31934;&#32454;&#30340;&#29615;&#22659;&#20132;&#20114;&#20197;&#21450;&#35268;&#21010;&#38271;&#26399;&#30446;&#26631;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#22312;&#39640;&#32500;&#24230;&#29615;&#22659;&#19979;&#35268;&#21010;&#31471;&#21040;&#31471;&#26102;&#26174;&#31034;&#20986;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#25506;&#32034;&#20302;&#25928;&#20197;&#21450;&#38271;&#26399;&#35268;&#21010;&#30340;&#20449;&#29992;&#20998;&#37197;&#22797;&#26434;&#24615;&#32780;&#21463;&#21040;&#26681;&#26412;&#24615;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#28216;&#25103;&#20013;&#39640;&#25928;&#23398;&#20064;&#39640;&#23618;&#27425;&#35745;&#21010;&#65288;ELF-P&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26725;&#25509;&#21160;&#20316;&#35268;&#21010;&#21644;&#28145;&#24230;RL&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#38271;&#26399;&#22797;&#26434;&#30340;&#25805;&#20316;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#28216;&#25103;&#25968;&#25454;&#23398;&#20064;&#22522;&#20110;&#29289;&#20307;&#30340;&#31163;&#25955;&#34892;&#20026;&#20808;&#39564;&#65292;&#24314;&#27169;&#23427;&#20204;&#22312;&#24403;&#21069;&#24773;&#22659;&#19979;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#39640;&#23618;&#27425;&#30340;&#30446;&#26631;&#26465;&#20214;&#31574;&#30053;&#65292;&#23427;&#65288;1&#65289;&#20351;&#29992;&#22522;&#20803;&#20316;&#20026;&#26500;&#24314;&#22797;&#26434;&#38271;&#26399;&#20219;&#21153;&#30340;&#22522;&#30784;&#65292;&#65288;2&#65289;&#21033;&#29992;&#31163;&#25955;&#30340;&#34892;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world robotic manipulation tasks remain an elusive challenge, since they involve both fine-grained environment interaction, as well as the ability to plan for long-horizon goals. Although deep reinforcement learning (RL) methods have shown encouraging results when planning end-to-end in high-dimensional environments, they remain fundamentally limited by poor sample efficiency due to inefficient exploration, and by the complexity of credit assignment over long horizons. In this work, we present Efficient Learning of High-Level Plans from Play (ELF-P), a framework for robotic learning that bridges motion planning and deep RL to achieve long-horizon complex manipulation tasks. We leverage task-agnostic play data to learn a discrete behavioral prior over object-centric primitives, modeling their feasibility given the current context. We then design a high-level goal-conditioned policy which (1) uses primitives as building blocks to scaffold complex long-horizon tasks and (2) leverages
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09618</link><description>&lt;p&gt;
HIVE&#65306;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20551;&#35774;&#65292;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65292;&#20854;&#36755;&#20986;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#32534;&#36753;&#25351;&#20196;&#65292;&#21516;&#26679;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#20854;&#36755;&#20986;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#27491;&#30830;&#25351;&#20196;&#21644;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#65288;HIVE&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#32534;&#36753;&#30340;&#22270;&#20687;&#19978;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20197;&#25429;&#25417;&#22522;&#30784;&#29992;&#25143;&#20559;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#21487;&#25193;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#22870;&#21169;&#20540;&#34701;&#20837;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38480;&#21046;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;1M&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;3.6K&#22870;&#21169;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#22870;&#21169;&#23398;&#20064;&#65292;&#20197;&#21450;1K&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#38382;&#39064;&#31867;&#30340;&#20998;&#25955;&#24335;&#40654;&#26364;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;Kronecker&#20056;&#31215;&#36924;&#36817;RFIM&#65292;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#36924;&#36817;&#12290;&#35813;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;$\mathcal{O}(1/K)$&#30340;&#26368;&#20339;&#24050;&#30693;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.09611</link><description>&lt;p&gt;
&#20855;&#26377;Kronecker&#20056;&#31215;&#36924;&#36817;&#30340;&#20998;&#25955;&#24335;&#40654;&#26364;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized Riemannian natural gradient methods with Kronecker-product approximations. (arXiv:2303.09611v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#38382;&#39064;&#31867;&#30340;&#20998;&#25955;&#24335;&#40654;&#26364;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36890;&#36807;Kronecker&#20056;&#31215;&#36924;&#36817;RFIM&#65292;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#36924;&#36817;&#12290;&#35813;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;$\mathcal{O}(1/K)$&#30340;&#26368;&#20339;&#24050;&#30693;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20108;&#38454;&#20449;&#24687;&#36924;&#36817;&#65292;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#24050;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#40654;&#26364;&#27969;&#24418;&#19978;&#30340;&#22823;&#35268;&#27169;&#20998;&#25955;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#65292;&#20854;&#20013;&#30001;&#26412;&#22320;&#25968;&#25454;&#38598;&#23450;&#20041;&#30340;&#26412;&#22320;&#30446;&#26631;&#20989;&#25968;&#26159;&#23545;&#25968;&#27010;&#29575;&#31867;&#22411;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#40654;&#26364;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;(RFIM)&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#40654;&#26364;&#33258;&#28982;&#26799;&#24230;&#19979;&#38477;(DRNGD)&#26041;&#27861;&#12290;&#20026;&#20102;&#20811;&#26381;&#39640;&#32500;RFIM&#30340;&#36890;&#20449;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#32467;&#26500;&#21270;&#38382;&#39064;&#31867;&#65292;&#20854;&#20013;RFIM&#21487;&#20197;&#36890;&#36807;&#20004;&#20010;&#20302;&#32500;&#30697;&#38453;&#30340;Kronecker&#31215;&#36924;&#36817;&#12290;&#36890;&#36807;&#22312;Kronecker&#22240;&#23376;&#19978;&#25191;&#34892;&#36890;&#20449;&#65292;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#33719;&#24471;RFIM&#30340;&#39640;&#36136;&#37327;&#36924;&#36817;&#12290;&#25105;&#20204;&#35777;&#26126;DRNGD&#25910;&#25947;&#36895;&#24230;&#36798;&#21040;$\mathcal{O}(1/K)$&#30340;&#26368;&#20339;&#24050;&#30693;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a computationally efficient approximation of the second-order information, natural gradient methods have been successful in solving large-scale structured optimization problems. We study the natural gradient methods for the large-scale decentralized optimization problems on Riemannian manifolds, where the local objective function defined by the local dataset is of a log-probability type. By utilizing the structure of the Riemannian Fisher information matrix (RFIM), we present an efficient decentralized Riemannian natural gradient descent (DRNGD) method. To overcome the communication issue of the high-dimension RFIM, we consider a class of structured problems for which the RFIM can be approximated by a Kronecker product of two low-dimension matrices. By performing the communications over the Kronecker factors, a high-quality approximation of the RFIM can be obtained in a low cost. We prove that DRNGD converges to a stationary point with the best-known rate of $\mathcal{O}(1/K)$. Nu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;AI&#20276;&#20387;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#36712;&#36857;&#21487;&#35270;&#21270;&#25552;&#20379;&#23545;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.09601</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#24515;&#29702;&#27835;&#30103;AI&#20276;&#20387;&#19982;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Psychotherapy AI Companion with Reinforcement Learning Recommendations and Interpretable Policy Dynamics. (arXiv:2303.09601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;AI&#20276;&#20387;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#36712;&#36857;&#21487;&#35270;&#21270;&#25552;&#20379;&#23545;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Deep Reinforcement Learning&#65288;DRL&#65289;&#29983;&#25104;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#25512;&#33616;&#30340;&#24378;&#21270;&#23398;&#20064;&#24515;&#29702;&#27835;&#30103;AI&#20276;&#20387;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#31934;&#31070;&#30142;&#30149;&#65288;&#28966;&#34385;&#30151;&#65292;&#25233;&#37057;&#30151;&#65292;&#31934;&#31070;&#20998;&#35010;&#30151;&#21644;&#33258;&#26432;&#30149;&#20363;&#65289;&#20351;&#29992;&#22810;&#30446;&#26631;&#31574;&#30053;&#29983;&#25104;&#22120;&#36827;&#34892;&#29983;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#19977;&#20010;&#19981;&#21516;&#30340;&#24037;&#20316;&#32852;&#30431;&#35780;&#20998;&#26631;&#20934;&#65288;&#20219;&#21153;&#65292;&#20851;&#31995;&#21644;&#30446;&#26631;&#65289;&#26469;&#26816;&#39564;&#25512;&#33616;&#20027;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#33021;&#22815;&#30456;&#23545;&#36739;&#22909;&#22320;&#25429;&#33719;&#30495;&#23454;&#25968;&#25454;&#65288;&#27835;&#30103;&#24072;&#35752;&#35770;&#30340;&#21382;&#21490;&#20027;&#39064;&#65289;&#65292;&#26368;&#20339;&#27169;&#22411;&#30340;&#34920;&#29616;&#22240;&#30142;&#30149;&#21644;&#35780;&#32423;&#26631;&#20934;&#32780;&#24322;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#27934;&#35265;&#65292;&#25105;&#20204;&#22312;2D&#20027;&#25104;&#20998;&#20998;&#26512;&#31354;&#38388;&#21644;&#36716;&#31227;&#30697;&#38453;&#20013;&#21487;&#35270;&#21270;&#31574;&#30053;&#36712;&#36857;&#12290;&#36825;&#20123;&#21487;&#35270;&#21270;&#21576;&#29616;&#20102;&#22312;&#19981;&#21516;&#22870;&#21169;&#20449;&#21495;&#21644;&#19981;&#21516;&#20020;&#24202;&#35786;&#26029;&#19979;&#35757;&#32451;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#26412;&#31995;&#32479;&#22312;&#29983;&#25104;&#22810;&#30446;&#26631;&#31574;&#30053;&#24515;&#29702;&#27835;&#30103;&#20027;&#39064;&#26041;&#38754;&#30340;&#25104;&#21151;&#34920;&#29616;&#20026;&#24320;&#21457;&#33021;&#22815;&#24110;&#21161;&#27835;&#30103;&#24072;&#25552;&#20379;&#20010;&#24615;&#21270;&#27835;&#30103;&#30340;AI&#20276;&#20387;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a Reinforcement Learning Psychotherapy AI Companion that generates topic recommendations for therapists based on patient responses. The system uses Deep Reinforcement Learning (DRL) to generate multi-objective policies for four different psychiatric conditions: anxiety, depression, schizophrenia, and suicidal cases. We present our experimental results on the accuracy of recommended topics using three different scales of working alliance ratings: task, bond, and goal. We show that the system is able to capture the real data (historical topics discussed by the therapists) relatively well, and that the best performing models vary by disorder and rating scale. To gain interpretable insights into the learned policies, we visualize policy trajectories in a 2D principal component analysis space and transition matrices. These visualizations reveal distinct patterns in the policies trained with different reward signals and trained on different clinical diagnoses. Our system's succe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38454;&#27573;&#12289;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#21450;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#12290;&#20851;&#38190;&#30340;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#23558;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#32447;&#24615;&#29305;&#24449;&#65292;&#20197;&#26041;&#20415;&#26816;&#26597;&#21644;&#29702;&#35299;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#35813;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.09590</link><description>&lt;p&gt;
&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Visual Analytics of Multivariate Networks with Representation Learning and Composite Variable Construction. (arXiv:2303.09590v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65292;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38454;&#27573;&#12289;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#21450;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#12290;&#20851;&#38190;&#30340;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#23558;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#32447;&#24615;&#29305;&#24449;&#65292;&#20197;&#26041;&#20415;&#26816;&#26597;&#21644;&#29702;&#35299;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#35813;&#24037;&#20316;&#27969;&#31243;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#21457;&#29616;&#12290;&#21457;&#25496;&#21644;&#29702;&#35299;&#22810;&#21464;&#37327;&#32593;&#32476;&#20013;&#30340;&#20851;&#31995;&#24182;&#19981;&#26159;&#19968;&#39033;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30740;&#31350;&#22810;&#21464;&#37327;&#32593;&#32476;&#20197;&#25552;&#21462;&#32593;&#32476;&#19981;&#21516;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#20043;&#38388;&#20851;&#32852;&#30340;&#35270;&#35273;&#20998;&#26512;&#24037;&#20316;&#27969;&#31243;&#65288;&#20363;&#22914;&#65292;&#20160;&#20040;&#26159;&#22312;&#31038;&#20132;&#32593;&#32476;&#23494;&#24230;&#26041;&#38754;&#19982;&#19981;&#21516;&#23646;&#24615;&#30340;&#32452;&#21512;&#20851;&#31995;&#65289;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#21253;&#25324;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#26681;&#25454;&#25152;&#36873;&#36755;&#20837;&#21644;&#36755;&#20986;&#23646;&#24615;&#26469;&#23545;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#38477;&#32500;&#21644;&#20248;&#21270;&#38454;&#27573;&#20197;&#20135;&#29983;&#19968;&#20010;&#31616;&#21270;&#30340;&#32467;&#26524;&#38598;&#21512;&#20197;&#20415;&#26816;&#26597;&#65292;&#26368;&#21518;&#36890;&#36807;&#29992;&#25143;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#25509;&#21475;&#36827;&#34892;&#35299;&#37322;&#38454;&#27573;&#30340;&#25805;&#20316;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#37096;&#20998;&#26159;&#32452;&#21512;&#21464;&#37327;&#26500;&#24314;&#27493;&#39588;&#65292;&#35813;&#27493;&#39588;&#23558;&#30001;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#37325;&#22609;&#20026;&#30452;&#35266;&#35299;&#37322;&#30340;&#32447;&#24615;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22823;&#22411;&#32452;&#32455;&#21592;&#24037;&#20043;&#38388;&#30340;&#30005;&#23376;&#37038;&#20214;&#36890;&#20449;&#25968;&#25454;&#38598;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#24037;&#20316;&#27969;&#31243;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate networks are commonly found in real-world data-driven applications. Uncovering and understanding the relations of interest in multivariate networks is not a trivial task. This paper presents a visual analytics workflow for studying multivariate networks to extract associations between different structural and semantic characteristics of the networks (e.g., what are the combinations of attributes largely relating to the density of a social network?). The workflow consists of a neural-network-based learning phase to classify the data based on the chosen input and output attributes, a dimensionality reduction and optimization phase to produce a simplified set of results for examination, and finally an interpreting phase conducted by the user through an interactive visualization interface. A key part of our design is a composite variable construction step that remodels nonlinear features obtained by neural networks into linear features that are intuitive to interpret. We demon
&lt;/p&gt;</description></item><item><title>TypeT5&#26159;&#19968;&#31181;&#21033;&#29992;CodeT5&#36827;&#34892;&#38745;&#24577;&#20998;&#26512;&#30340;&#26032;&#22411;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#32597;&#35265;&#21644;&#22797;&#26434;&#31867;&#22411;&#26102;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#31867;&#22411;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2303.09564</link><description>&lt;p&gt;
TypeT5: &#22522;&#20110;&#38745;&#24577;&#20998;&#26512;&#30340;Seq2seq&#31867;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
TypeT5: Seq2seq Type Inference using Static Analysis. (arXiv:2303.09564v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09564
&lt;/p&gt;
&lt;p&gt;
TypeT5&#26159;&#19968;&#31181;&#21033;&#29992;CodeT5&#36827;&#34892;&#38745;&#24577;&#20998;&#26512;&#30340;&#26032;&#22411;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39044;&#27979;&#32597;&#35265;&#21644;&#22797;&#26434;&#31867;&#22411;&#26102;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#31867;&#22411;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39044;&#27979;Python&#21644;JavaScript&#31243;&#24207;&#20013;&#32570;&#23569;&#30340;&#31867;&#22411;&#27880;&#37322;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#26368;&#24120;&#35265;&#30340;&#31867;&#22411;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#22312;&#32597;&#35265;&#25110;&#22797;&#26434;&#31867;&#22411;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31867;&#22411;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#31867;&#22411;&#39044;&#27979;&#35270;&#20026;&#20195;&#30721;&#22635;&#20805;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;CodeT5&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#30340;&#26368;&#20808;&#36827;&#30340;seq2seq&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#38745;&#24577;&#20998;&#26512;&#20026;&#27599;&#20010;&#20195;&#30721;&#20803;&#32032;&#26500;&#36896;&#21160;&#24577;&#19978;&#19979;&#25991;&#65292;&#20854;&#31867;&#22411;&#31614;&#21517;&#30001;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#35299;&#30721;&#26041;&#26696;&#65292;&#23558;&#20808;&#21069;&#30340;&#31867;&#22411;&#39044;&#27979;&#32435;&#20837;&#27169;&#22411;&#30340;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#65292;&#20801;&#35768;&#30456;&#20851;&#20195;&#30721;&#20803;&#32032;&#20043;&#38388;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TypeT5&#26041;&#27861;&#19981;&#20165;&#22312;&#25972;&#20307;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65288;&#29305;&#21035;&#26159;&#22312;&#32597;&#35265;&#21644;&#22797;&#26434;&#31867;&#22411;&#26041;&#38754;&#65289;&#65292;&#32780;&#19988;&#20135;&#29983;&#30340;&#32467;&#26524;&#26356;&#20855;&#36830;&#36143;&#24615;&#65292;&#31867;&#22411;&#38169;&#35823;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model. We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors -- while
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2303.09350</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation by learning using privileged information. (arXiv:2303.09350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#31639;&#27861;&#65292;&#20197;&#22312;&#23398;&#20064;&#20013;&#25918;&#23485;&#20551;&#35774;&#26465;&#20214;&#24182;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#20943;&#23569;&#38169;&#35823;&#26469;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#31561;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21482;&#22312;&#24378;&#20551;&#35774;&#26465;&#20214;&#19979;&#24471;&#20197;&#23454;&#29616;&#65292;&#22914;&#21327;&#21464;&#37327;&#31227;&#20301;&#21644;&#36755;&#20837;&#39046;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#12290;&#21518;&#32773;&#22312;&#39640;&#32500;&#24212;&#29992;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#65292;&#27604;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#22312;&#38754;&#23545;&#36825;&#31181;&#25361;&#25112;&#26102;&#65292;&#22270;&#20687;&#20998;&#31867;&#20173;&#28982;&#26159;&#31639;&#27861;&#24320;&#21457;&#30340;&#28789;&#24863;&#21644;&#22522;&#20934;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#33719;&#21462;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#26679;&#26412;&#30340;&#26377;&#20851;&#20449;&#24687;&#33021;&#22815;&#24110;&#21161;&#25918;&#23485;&#36825;&#20123;&#20551;&#35774;&#65292;&#24182;&#22312;&#23398;&#20064;&#20013;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#20195;&#20215;&#26159;&#25910;&#38598;&#26356;&#20016;&#23500;&#30340;&#21464;&#37327;&#38598;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#21033;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65288;DALUPI&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#22810;&#26631;&#31614;&#22270;&#20687;&#20998;&#31867;&#30340;&#23454;&#29992;&#31471;&#21040;&#31471;&#31639;&#27861;&#65292;&#21463;&#21040;&#25105;&#20204;&#20998;&#26512;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#21253;&#25324;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#20837;&#29305;&#26435;&#20449;&#24687;&#21487;&#20197;&#20943;&#23569;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Successful unsupervised domain adaptation (UDA) is guaranteed only under strong assumptions such as covariate shift and overlap between input domains. The latter is often violated in high-dimensional applications such as image classification which, despite this challenge, continues to serve as inspiration and benchmark for algorithm development. In this work, we show that access to side information about examples from the source and target domains can help relax these assumptions and increase sample efficiency in learning, at the cost of collecting a richer variable set. We call this domain adaptation by learning using privileged information (DALUPI). Tailored for this task, we propose a simple two-stage learning algorithm inspired by our analysis and a practical end-to-end algorithm for multi-label image classification. In a suite of experiments, including an application to medical image analysis, we demonstrate that incorporating privileged information in learning can reduce errors i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.09065</link><description>&lt;p&gt;
&#22522;&#20110;t-SPN&#21644;&#28388;&#27874;&#30340;&#32454;&#32990;&#20998;&#31867;&#30340;&#26368;&#22823;&#38388;&#38548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Maximum Margin Learning of t-SPNs for Cell Classification with Filtering. (arXiv:2303.09065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;t-SPN&#31639;&#27861;&#21644;&#28388;&#27874;&#25216;&#26415;&#30340;&#32454;&#32990;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#21644;L2&#27491;&#21017;&#21270;&#65292;&#35813;&#26041;&#27861;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#27010;&#29575;&#20307;&#31995;&#32467;&#26500;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;&#26641;&#24418;&#27714;&#21644;&#20135;&#21697;&#32593;&#32476;(t-SPN)&#65292;&#29992;&#20110;&#32454;&#32990;&#20998;&#31867;&#12290;&#26500;&#24314;t-SPN&#30340;&#30446;&#30340;&#26159;&#34920;&#31034;&#26410;&#24402;&#19968;&#21270;&#27010;&#29575;&#20316;&#20026;&#26368;&#30456;&#20284;&#30340;&#32454;&#32990;&#31867;&#21035;&#30340;&#26465;&#20214;&#27010;&#29575;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#36793;&#32536;&#26469;&#23398;&#20064;&#26500;&#24314;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#65292;&#35813;&#36793;&#32536;&#26159;&#30495;&#23454;&#26631;&#31614;&#21644;&#26368;&#26377;&#31454;&#20105;&#21147;&#30340;&#38169;&#35823;&#26631;&#31614;&#20043;&#38388;&#30340;&#26465;&#20214;&#27010;&#29575;&#24046;&#12290;&#20026;&#20102;&#22686;&#24378;&#20307;&#31995;&#32467;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#32771;&#34385;&#20102;L2&#27491;&#21017;&#21270;&#65288;REG&#65289;&#21644;&#26368;&#22823;&#38388;&#38548;&#65288;MM&#65289;&#26631;&#20934;&#12290;&#20026;&#20102;&#31361;&#20986;&#32454;&#32990;&#29305;&#24449;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#30340;&#26377;&#25928;&#24615;&#65306;&#29702;&#24819;&#39640;&#36890;&#28388;&#27874;&#21644;&#25289;&#26222;&#25289;&#26031;&#28388;&#27874;(Log)&#12290;&#22312;HEp-2&#21644;Feulgen&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#22522;&#20110;&#26368;&#22823;&#38388;&#38548;&#20934;&#21017;&#19982;&#27491;&#21017;&#21270;&#23398;&#20064;&#30340;t-SPN&#20307;&#31995;&#32467;&#26500;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
An algorithm based on a deep probabilistic architecture referred to as a tree-structured sum-product network (t-SPN) is considered for cell classification. The t-SPN is constructed such that the unnormalized probability is represented as conditional probabilities of a subset of most similar cell classes. The constructed t-SPN architecture is learned by maximizing the margin, which is the difference in the conditional probability between the true and the most competitive false label. To enhance the generalization ability of the architecture, L2-regularization (REG) is considered along with the maximum margin (MM) criterion in the learning process. To highlight cell features, this paper investigates the effectiveness of two generic high-pass filters: ideal high-pass filtering and the Laplacian of Gaussian (LOG) filtering. On both HEp-2 and Feulgen benchmark datasets, the t-SPN architecture learned based on the max-margin criterion with regularization produced the highest accuracy rate co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#26500;&#24314;&#21152;&#26435;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#24403;&#21069;&#21516;&#31867;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#26356;&#20026;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2303.08874</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Bayesian Quadrature for Neural Ensemble Search. (arXiv:2303.08874v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#26500;&#24314;&#21152;&#26435;&#38598;&#25104;&#31070;&#32463;&#32593;&#32476;&#65292;&#30456;&#27604;&#24403;&#21069;&#21516;&#31867;&#26041;&#27861;&#65292;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#26356;&#20026;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#26102;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#26500;&#24314;&#22343;&#31561;&#21152;&#26435;&#30340;&#38598;&#25104;&#65292;&#36825;&#21487;&#33021;&#23481;&#26131;&#21463;&#21040;&#36739;&#24369;&#26550;&#26500;&#30340;&#22833;&#25928;&#27169;&#24335;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#38598;&#25104;&#35270;&#20026;&#36817;&#20284;&#36793;&#32536;&#21270;&#26550;&#26500;&#65292;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#31215;&#20998;&#30340;&#24037;&#20855;&#26500;&#24314;&#38598;&#25104;&#26041;&#27861;&#8212;&#8212;&#36825;&#20123;&#24037;&#20855;&#38750;&#24120;&#36866;&#21512;&#25506;&#32034;&#26550;&#26500;&#20284;&#28982;&#34920;&#38754;&#26377;&#20998;&#25955;&#12289;&#29421;&#31364;&#23792;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#38598;&#25104;&#30001;&#20307;&#29616;&#20854;&#24615;&#33021;&#30340;&#26550;&#26500;&#21152;&#26435;&#26435;&#37325;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#8212;&#8212;&#22312;&#27979;&#35797;&#20284;&#28982;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#26399;&#26395;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#8212;&#8212;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#65292;&#24182;&#36890;&#36807;&#21066;&#20943;&#30740;&#31350;&#39564;&#35777;&#20854;&#21508;&#25104;&#20998;&#30340;&#29420;&#31435;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensembling can improve the performance of Neural Networks, but existing approaches struggle when the architecture likelihood surface has dispersed, narrow peaks. Furthermore, existing methods construct equally weighted ensembles, and this is likely to be vulnerable to the failure modes of the weaker architectures. By viewing ensembling as approximately marginalising over architectures we construct ensembles using the tools of Bayesian Quadrature -tools which are well suited to the exploration of likelihood surfaces with dispersed, narrow peaks. Additionally, the resulting ensembles consist of architectures weighted commensurate with their performance. We show empirically -- in terms of test likelihood, accuracy, and expected calibration error -that our method outperforms state-of-the-art baselines, and verify via ablation studies that its components do so independently.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#30740;&#31350;&#20102;DEPINN&#22312;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#21644;&#25552;&#39640;&#20808;&#39564;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#27492;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.08455</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
On the uncertainty analysis of the data-enabled physics-informed neural network for solving neutron diffusion eigenvalue problem. (arXiv:2303.08455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#24102;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#30740;&#31350;&#20102;DEPINN&#22312;&#27714;&#35299;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26041;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#29992;&#20110;&#20943;&#23569;&#22122;&#22768;&#24433;&#21709;&#21644;&#25552;&#39640;&#20808;&#39564;&#25968;&#25454;&#21033;&#29992;&#29575;&#65292;&#27492;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24037;&#31243;&#23454;&#39564;&#20013;&#65292;&#36890;&#36807;&#25506;&#27979;&#22120;&#33719;&#24471;&#30340;&#25968;&#25454;&#19981;&#21487;&#36991;&#20813;&#22320;&#24102;&#26377;&#22122;&#22768;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#20808;&#39564;&#25968;&#25454;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#22122;&#22768;&#26102;&#65292;&#24050;&#32463;&#25552;&#20986;&#30340;&#25968;&#25454;&#39537;&#21160;&#30340;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#65288;DEPINN&#65289;&#22312;&#35745;&#31639;&#20013;&#23376;&#25193;&#25955;&#26412;&#24449;&#20540;&#38382;&#39064;&#26102;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20943;&#23569;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#22122;&#22768;&#20808;&#39564;&#25968;&#25454;&#30340;&#21033;&#29992;&#29575;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21019;&#26032;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20123;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#20540;&#32467;&#26524;&#65292;&#26412;&#25991;&#22312;&#20004;&#20010;&#20856;&#22411;&#30340;&#22522;&#20934;&#38382;&#39064;&#19978;&#26816;&#39564;&#20102;DEPINN&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#21306;&#38388;&#25439;&#22833;&#20989;&#25968;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#25991;&#30830;&#35748;&#20102;&#25913;&#36827;&#30340;DEPINN&#22312;&#26680;&#21453;&#24212;&#22534;&#29289;&#29702;&#23454;&#38469;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical engineering experiments, the data obtained through detectors are inevitably noisy. For the already proposed data-enabled physics-informed neural network (DEPINN) \citep{DEPINN}, we investigate the performance of DEPINN in calculating the neutron diffusion eigenvalue problem from several perspectives when the prior data contain different scales of noise. Further, in order to reduce the effect of noise and improve the utilization of the noisy prior data, we propose innovative interval loss functions and give some rigorous mathematical proofs. The robustness of DEPINN is examined on two typical benchmark problems through a large number of numerical results, and the effectiveness of the proposed interval loss function is demonstrated by comparison. This paper confirms the feasibility of the improved DEPINN for practical engineering applications in nuclear reactor physics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#37096;&#20998;&#21644;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#32452;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.08068</link><description>&lt;p&gt;
&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#39118;&#26684;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Style Feature Extraction Using Contrastive Conditioned Variational Autoencoders with Mutual Information Constraints. (arXiv:2303.08068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20114;&#20449;&#24687;&#32422;&#26463;&#19979;&#30340;&#23545;&#27604;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#36827;&#34892;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#30001;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#37096;&#20998;&#21644;&#19968;&#20010;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#32454;&#31890;&#24230;&#29305;&#24449;&#65288;&#22914;&#39118;&#26684;&#65289;&#38750;&#24120;&#37325;&#35201;&#12290;&#26080;&#30417;&#30563;&#26041;&#27861;&#65288;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65289;&#21487;&#20197;&#25552;&#21462;&#39118;&#26684;&#65292;&#20294;&#25552;&#21462;&#30340;&#39118;&#26684;&#36890;&#24120;&#19982;&#20854;&#20182;&#29305;&#24449;&#28151;&#21512;&#12290;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20998;&#31867;&#26631;&#31614;&#26469;&#25351;&#23548;VAEs&#25552;&#21462;&#39118;&#26684;&#65292;&#21363;&#26465;&#20214;VAEs&#65288;CVAEs&#65289;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20165;&#25552;&#21462;&#39118;&#26684;&#30340;&#26041;&#27861;&#23578;&#26410;&#24314;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#22522;&#20110;CVAE&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20165;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22823;&#33268;&#30001;&#20004;&#20010;&#24182;&#34892;&#37096;&#20998;&#32452;&#25104;; &#25552;&#21462;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#37096;&#20998;&#65292;&#20197;&#21450;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#30340;CVAE&#37096;&#20998;&#12290;CL&#27169;&#22411;&#36890;&#24120;&#20197;&#26080;&#38656;&#25968;&#25454;&#25193;&#20805;&#30340;&#33258;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#19982;&#26679;&#24335;&#26080;&#20851;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#35270;&#20026;&#26679;&#24335;&#20013;&#30340;&#25200;&#21160;&#12290;&#20197;&#25552;&#21462;&#30340;&#39118;&#26684;&#26080;&#20851;&#29305;&#24449;&#20026;&#26465;&#20214;&#65292;CVAE&#23398;&#20064;&#20165;&#25552;&#21462;&#39118;&#26684;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#20808;&#35757;&#32451;CL&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;CL&#27169;&#22411;&#25351;&#23548;CVAE&#30340;&#35757;&#32451;&#12290;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is crucial to extract fine-grained features such as styles from unlabeled data in data analysis. Unsupervised methods, such as variational autoencoders (VAEs), can extract styles, but the extracted styles are usually mixed with other features. We can isolate the styles using VAEs conditioned by class labels, known as conditional VAEs (CVAEs). However, methods to extract only styles using unlabeled data are not established. In this paper, we construct a CVAE-based method that extracts style features using only unlabeled data. The proposed model roughly consists of two parallel parts; a contrastive learning (CL) part that extracts style-independent features and a CVAE part that extracts style features. CL models generally learn representations independent of data augmentation, which can be seen as a perturbation in styles, in a self-supervised way. Taking the style-independent features as a condition, the CVAE learns to extract only styles. In the training procedure, a CL model is tra
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30896;&#25758;&#20132;&#21449;&#29109;&#30340;&#33258;&#26631;&#27880;&#20998;&#31867;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#35813;&#25439;&#22833;&#26041;&#27861;&#30340;EM-like&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#26631;&#27880;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07321</link><description>&lt;p&gt;
&#33258;&#26631;&#27880;&#20998;&#31867;&#20013;&#30340;&#30896;&#25758;&#20132;&#21449;&#29109;&#25439;&#22833;&#21450;EM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Collision Cross-entropy and EM Algorithm for Self-labeled Classification. (arXiv:2303.07321v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30896;&#25758;&#20132;&#21449;&#29109;&#30340;&#33258;&#26631;&#27880;&#20998;&#31867;&#25439;&#22833;&#26367;&#20195;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#35813;&#25439;&#22833;&#26041;&#27861;&#30340;EM-like&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#33258;&#26631;&#27880;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21518;&#39564;&#27169;&#22411;&#30340;&#33258;&#26631;&#27880;&#20998;&#31867;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#30896;&#25758;&#20132;&#21449;&#29109;&#8221;&#20316;&#20026;&#39321;&#20892;&#20132;&#21449;&#29109;&#30340;&#19968;&#20010;&#20581;&#22766;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;EM-like&#31639;&#27861;&#26469;&#36890;&#36807;&#20132;&#26367;&#25311;&#21512;&#21518;&#39564;&#27010;&#29575;y&#21644;&#26356;&#26032;&#27169;&#22411;&#39044;&#27979;&#26469;&#20248;&#21270;&#25105;&#20204;&#30340;&#25439;&#22833;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#30896;&#25758;&#25439;&#22833;&#22312;&#21508;&#31181;&#33258;&#26631;&#27880;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#25110;&#33267;&#23569;&#19982;&#29616;&#26377;&#25439;&#22833;&#30456;&#21305;&#37197;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#30446;&#26631;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose "collision cross-entropy" as a robust alternative to the Shannon's cross-entropy in the context of self-labeled classification with posterior models. Assuming unlabeled data, self-labeling works by estimating latent pseudo-labels, categorical distributions y, that optimize some discriminative clustering criteria, e.g. "decisiveness" and "fairness". All existing self-labeled losses incorporate Shannon's cross-entropy term targeting the model prediction, softmax, at the estimated distribution y. In fact, softmax is trained to mimic the uncertainty in y exactly. Instead, we propose the negative log-likelihood of "collision" to maximize the probability of equality between two random variables represented by distributions softmax and y. We show that our loss satisfies some properties of a generalized cross-entropy. Interestingly, it agrees with the Shannon's cross-entropy for one-hot pseudo-labels y, but the training from softer labels weakens. For example, if y is a uniform dist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#29992;&#33021;&#37327;&#20998;&#25968;&#21028;&#26029;&#26410;&#26631;&#35760;&#26679;&#26412;&#26159;&#8220;&#20869;&#20998;&#24067;&#8221;&#36824;&#26159;&#8220;&#22806;&#20998;&#24067;&#8221;&#65292;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#25104;&#20026;&#20869;&#37096;&#20998;&#24067;&#24182;&#23545;&#35757;&#32451;&#20135;&#29983;&#36129;&#29486;&#65292;&#32467;&#21512;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2303.07269</link><description>&lt;p&gt;
InPL: &#20266;&#26631;&#31614;&#39318;&#20808;&#26631;&#35760;&#20869;&#28857;&#30340;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
InPL: Pseudo-labeling the Inliers First for Imbalanced Semi-supervised Learning. (arXiv:2303.07269v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07269
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#29992;&#33021;&#37327;&#20998;&#25968;&#21028;&#26029;&#26410;&#26631;&#35760;&#26679;&#26412;&#26159;&#8220;&#20869;&#20998;&#24067;&#8221;&#36824;&#26159;&#8220;&#22806;&#20998;&#24067;&#8221;&#65292;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#25104;&#20026;&#20869;&#37096;&#20998;&#24067;&#24182;&#23545;&#35757;&#32451;&#20135;&#29983;&#36129;&#29486;&#65292;&#32467;&#21512;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#19981;&#24179;&#34913;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#20808;&#36827;&#26041;&#27861;&#20381;&#36182;&#20110;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#20266;&#26631;&#31614;&#21644;&#19968;&#33268;&#24615;&#35268;&#21017;&#12290;&#20026;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#65292;&#36890;&#24120;&#37319;&#29992;&#39640;&#32622;&#20449;&#24230;&#38408;&#20540;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#65292;&#23545;&#20110;&#36828;&#31163;&#35757;&#32451;&#25968;&#25454;&#30340;&#26679;&#26412;&#65292;&#22522;&#20110;softmax&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#21487;&#20197;&#20219;&#24847;&#39640;&#65292;&#22240;&#27492;&#21363;&#20351;&#23545;&#20110;&#39640;&#32622;&#20449;&#24230;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65292;&#20854;&#20266;&#26631;&#31614;&#20173;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#26631;&#31614;&#30340;&#26032;&#35270;&#35282;&#65292;&#29992;&#20110;&#19981;&#24179;&#34913;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#32780;&#26159;&#25552;&#20986;&#34913;&#37327;&#19968;&#20010;&#26410;&#26631;&#35760; &#26679;&#26412;&#24456;&#21487;&#33021;&#23646;&#20110;&#8220;&#20869;&#20998;&#24067;&#8221;&#65288;&#21363;&#25509;&#36817;&#24403;&#21069;&#35757;&#32451;&#25968;&#25454;&#65289;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#30830;&#23450;&#19968;&#20010;&#26410;&#26631;&#35760;&#26679;&#26412;&#26159;&#8220;&#20869;&#20998;&#24067;&#8221;&#36824;&#26159;&#8220;&#22806;&#20998;&#24067;&#8221;&#65292;&#25105;&#20204;&#37319;&#29992;&#26469;&#33258;&#8220;&#22806;&#20998;&#24067;&#26816;&#27979;&#8221;&#25991;&#29486;&#20013;&#30340;&#33021;&#37327;&#20998;&#25968;&#12290;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#25104;&#20026;&#20869;&#37096;&#20998;&#24067;&#24182;&#23545;&#35757;&#32451;&#20135;&#29983;&#36129;&#29486;&#65292;&#32467;&#21512;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent state-of-the-art methods in imbalanced semi-supervised learning (SSL) rely on confidence-based pseudo-labeling with consistency regularization. To obtain high-quality pseudo-labels, a high confidence threshold is typically adopted. However, it has been shown that softmax-based confidence scores in deep networks can be arbitrarily high for samples far from the training data, and thus, the pseudo-labels for even high-confidence unlabeled samples may still be unreliable. In this work, we present a new perspective of pseudo-labeling for imbalanced SSL. Without relying on model confidence, we propose to measure whether an unlabeled sample is likely to be ``in-distribution''; i.e., close to the current training data. To decide whether an unlabeled sample is ``in-distribution'' or ``out-of-distribution'', we adopt the energy score from out-of-distribution detection literature. As training progresses and more unlabeled samples become in-distribution and contribute to training, the combi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;UNET&#27169;&#22411;&#32467;&#21512;&#20174;COVID-19&#24739;&#32773;&#20013;&#25552;&#21462;&#24863;&#26579;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#23558;&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;COVID-19&#37325;&#30151;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2303.07130</link><description>&lt;p&gt;
&#22522;&#20110;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;COVID-19&#37325;&#30151;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing COVID-19 Severity Analysis through Ensemble Methods. (arXiv:2303.07130v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;UNET&#27169;&#22411;&#32467;&#21512;&#20174;COVID-19&#24739;&#32773;&#20013;&#25552;&#21462;&#24863;&#26579;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#23558;&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;COVID-19&#37325;&#30151;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;(CT)&#25552;&#20379;&#20102;&#32954;&#37096;&#30340;&#35814;&#32454;&#22270;&#20687;&#65292;&#20801;&#35768;&#20020;&#24202;&#21307;&#29983;&#35266;&#23519;COVID-19&#25152;&#36896;&#25104;&#30340;&#25439;&#20260;&#31243;&#24230;&#12290;&#22522;&#20110;CT&#30340;&#32954;&#37096;&#21463;&#32047;&#31243;&#24230;&#35780;&#20998;(CTSS)&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;CT&#25195;&#25551;&#20013;&#35266;&#23519;&#21040;&#30340;&#32954;&#37096;&#21463;&#32047;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#27969;&#31243;&#65292;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;UNET&#27169;&#22411;&#32467;&#21512;&#20174;COVID-19&#24739;&#32773;&#20013;&#25552;&#21462;&#24863;&#26579;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#65306;&#26497;&#31471;&#26799;&#24230;&#25552;&#21319;&#12289;&#26497;&#24230;&#38543;&#26426;&#21270;&#26641;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#23558;&#24863;&#26579;&#30340;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#12290;&#35813;&#31995;&#32479;&#22312;AI-Enabled&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30740;&#35752;&#20250;&#21644;COVID-19&#35786;&#26029;&#22823;&#36187;(AI-MIA-COV19D)&#30340;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#20102;64\%&#30340;&#23439;F1&#24471;&#20998;&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;&#23558;&#39046;&#22495;&#30693;&#35782;&#19982;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#31934;&#30830;COVID-19&#35786;&#26029;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computed Tomography (CT) scans provide a detailed image of the lungs, allowing clinicians to observe the extent of damage caused by COVID-19. The CT severity score (CTSS) based scoring method is used to identify the extent of lung involvement observed on a CT scan. This paper presents a domain knowledge-based pipeline for extracting regions of infection in COVID-19 patients using a combination of image-processing algorithms and a pre-trained UNET model. The severity of the infection is then classified into different categories using an ensemble of three machine-learning models: Extreme Gradient Boosting, Extremely Randomized Trees, and Support Vector Machine. The proposed system was evaluated on a validation dataset in the AI-Enabled Medical Image Analysis Workshop and COVID-19 Diagnosis Competition (AI-MIA-COV19D) and achieved a macro F1 score of 64\%. These results demonstrate the potential of combining domain knowledge with machine learning techniques for accurate COVID-19 diagnosis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#36755;&#20837;&#30340;Transformer&#32534;&#30721;&#22120;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PAN&#65289;&#29992;&#20110;&#30140;&#30171;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#33258;&#21160;&#30140;&#30171;&#24378;&#24230;&#20998;&#32423;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#22810;&#23610;&#24230;&#21367;&#31215;&#32593;&#32476; (MSCN)&#12289;&#21387;&#32553;&#19982;&#28608;&#21457;&#27531;&#20313;&#32593;&#32476;(SEResNet)&#21644;Transformer&#32534;&#30721;&#22120;&#31561;&#29305;&#24449;&#25552;&#21462;&#26550;&#26500;&#65292;&#20026;&#23458;&#35266;&#19988;&#33258;&#21160;&#30340;&#30140;&#30171;&#24378;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2303.06845</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#30340;&#22810;&#23610;&#24230;&#28145;&#24230;&#23398;&#20064;&#30140;&#30171;&#20998;&#31867;&#30340;Transformer&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformer Encoder with Multiscale Deep Learning for Pain Classification Using Physiological Signals. (arXiv:2303.06845v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#36755;&#20837;&#30340;Transformer&#32534;&#30721;&#22120;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65288;PAN&#65289;&#29992;&#20110;&#30140;&#30171;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#33258;&#21160;&#30140;&#30171;&#24378;&#24230;&#20998;&#32423;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#22810;&#23610;&#24230;&#21367;&#31215;&#32593;&#32476; (MSCN)&#12289;&#21387;&#32553;&#19982;&#28608;&#21457;&#27531;&#20313;&#32593;&#32476;(SEResNet)&#21644;Transformer&#32534;&#30721;&#22120;&#31561;&#29305;&#24449;&#25552;&#21462;&#26550;&#26500;&#65292;&#20026;&#23458;&#35266;&#19988;&#33258;&#21160;&#30340;&#30140;&#30171;&#24378;&#24230;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30140;&#30171;&#26159;&#19990;&#30028;&#33539;&#22260;&#20869;&#30340;&#19968;&#20010;&#20005;&#37325;&#20581;&#24247;&#38382;&#39064;&#65292;&#23427;&#24433;&#21709;&#30528;&#22823;&#37096;&#20998;&#20154;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#31649;&#29702;&#21644;&#27835;&#30103;&#30140;&#30171;&#65292;&#38656;&#35201;&#20934;&#30830;&#20998;&#31867;&#21644;&#35780;&#20272;&#30140;&#30171;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30140;&#30171;&#26159;&#20027;&#35266;&#24863;&#21463;&#39537;&#21160;&#30340;&#20307;&#39564;&#65292;&#36825;&#21487;&#33021;&#26377;&#19968;&#23450;&#30340;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#30340;&#27979;&#37327;&#30140;&#30171;&#24378;&#24230;&#30340;&#25216;&#26415;&#65292;&#20363;&#22914;&#33258;&#25105;&#25253;&#21578;&#37327;&#34920;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#19988;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#23458;&#35266;&#21644;&#33258;&#21160;&#30340;&#30140;&#30171;&#24378;&#24230;&#35780;&#20272;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PainAttnNet&#65288;PAN&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#29702;&#20449;&#21495;&#36755;&#20837;&#30340;&#29992;&#20110;&#20998;&#31867;&#30140;&#30171;&#24378;&#24230;&#30340;&#26032;&#22411;Transformer&#32534;&#30721;&#22120;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30001;&#19977;&#20010;&#29305;&#24449;&#25552;&#21462;&#26550;&#26500;&#32452;&#25104;&#65306;&#22810;&#23610;&#24230;&#21367;&#31215;&#32593;&#32476;&#65288;MSCN&#65289;&#12289;&#21387;&#32553;&#19982;&#28608;&#21457;&#27531;&#20313;&#32593;&#32476;&#65288;SEResNet&#65289;&#21644;Transformer&#32534;&#30721;&#22120;&#22359;&#12290;&#26681;&#25454;&#30140;&#30171;&#21050;&#28608;&#65292;MSCN&#25552;&#21462;&#30701;&#26102;&#21644;&#38271;&#26102;&#31383;&#21475;&#20449;&#24687;&#65292;&#20197;&#21450;&#24207;&#21015;&#29305;&#24449;&#65292;SEResNet&#36827;&#19968;&#27493;&#20943;&#23569;&#36947;&#27425;&#65292;&#28982;&#21518;&#20256;&#36882;&#21040;Transformer&#32534;&#30721;&#22120;&#36827;&#34892;&#30140;&#30171;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PAN&#21487;&#20026;&#33258;&#21160;&#30140;&#30171;&#24378;&#24230;&#20998;&#32423;&#25552;&#20379;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pain is a serious worldwide health problem that affects a vast proportion of the population. For efficient pain management and treatment, accurate classification and evaluation of pain severity are necessary. However, this can be challenging as pain is a subjective sensation-driven experience. Traditional techniques for measuring pain intensity, e.g. self-report scales, are susceptible to bias and unreliable in some instances. Consequently, there is a need for more objective and automatic pain intensity assessment strategies. In this paper, we develop PainAttnNet (PAN), a novel transfomer-encoder deep-learning framework for classifying pain intensities with physiological signals as input. The proposed approach is comprised of three feature extraction architectures: multiscale convolutional networks (MSCN), a squeeze-and-excitation residual network (SEResNet), and a transformer encoder block. On the basis of pain stimuli, MSCN extracts short- and long-window information as well as seque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06280</link><description>&lt;p&gt;
&#25506;&#31350;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Investigating Stateful Defenses Against Black-Box Adversarial Examples. (arXiv:2303.06280v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#40657;&#30418;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;82.2&#65285;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;76.5&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates stateful defenses against black-box adversarial examples and proposes a new stateful defense model that achieves 82.2% accuracy on the CIFAR10 dataset and 76.5% accuracy on the ImageNet dataset.
&lt;/p&gt;
&lt;p&gt;
&#38450;&#24481;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#20813;&#21463;&#30333;&#30418;&#23545;&#25239;&#25915;&#20987;&#24050;&#34987;&#35777;&#26126;&#26497;&#20026;&#22256;&#38590;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#65292;&#35797;&#22270;&#38450;&#24481;&#26356;&#21463;&#38480;&#21046;&#30340;&#40657;&#30418;&#25915;&#20987;&#32773;&#12290;&#36825;&#20123;&#38450;&#24481;&#36890;&#36807;&#36319;&#36394;&#20256;&#20837;&#27169;&#22411;&#26597;&#35810;&#30340;&#21382;&#21490;&#35760;&#24405;&#65292;&#24182;&#25298;&#32477;&#37027;&#20123;&#21487;&#30097;&#22320;&#30456;&#20284;&#30340;&#26597;&#35810;&#26469;&#25805;&#20316;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;Blacklight&#26159;&#22312;USENIX Security '22&#19978;&#25552;&#20986;&#30340;&#65292;&#22768;&#31216;&#21487;&#20197;&#38450;&#27490;&#20960;&#20046;100&#65285;&#30340;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#25915;&#20987;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#35843;&#25972;&#29616;&#26377;&#40657;&#30418;&#25915;&#20987;&#30340;&#21442;&#25968;&#65292;&#26174;&#33879;&#38477;&#20302;&#21463;Blacklight&#20445;&#25252;&#30340;&#20998;&#31867;&#22120;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;CIFAR10&#19978;&#20174;82.2&#65285;&#38477;&#33267;6.4&#65285;&#65289;&#12290;&#21463;&#21040;&#36825;&#19968;&#24778;&#20154;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#29366;&#24577;&#38450;&#24481;&#30340;&#31995;&#32479;&#21270;&#65292;&#20197;&#20102;&#35299;&#20026;&#20160;&#20040;&#29616;&#26377;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#20250;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#29366;&#24577;&#38450;&#24481;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;CIFAR10&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;82.2&#65285;&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#20026;76.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defending machine-learning (ML) models against white-box adversarial attacks has proven to be extremely difficult. Instead, recent work has proposed stateful defenses in an attempt to defend against a more restricted black-box attacker. These defenses operate by tracking a history of incoming model queries, and rejecting those that are suspiciously similar. The current state-of-the-art stateful defense Blacklight was proposed at USENIX Security '22 and claims to prevent nearly 100% of attacks on both the CIFAR10 and ImageNet datasets. In this paper, we observe that an attacker can significantly reduce the accuracy of a Blacklight-protected classifier (e.g., from 82.2% to 6.4% on CIFAR10) by simply adjusting the parameters of an existing black-box attack. Motivated by this surprising observation, since existing attacks were evaluated by the Blacklight authors, we provide a systematization of stateful defenses to understand why existing stateful defense models fail. Finally, we propose a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05617</link><description>&lt;p&gt;
KGNv2: &#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#21512;&#25104;&#20013;&#30340;&#23610;&#24230;&#21644;&#23039;&#24577;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input. (arXiv:2303.05617v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20851;&#38190;&#28857;&#20174;2D/2.5D&#36755;&#20837;&#20013;&#36827;&#34892;&#12290;&#22312;&#21069;&#26399;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#25235;&#21462;&#26816;&#27979;&#22120;&#24050;&#32463;&#35777;&#26126;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#24425;&#33394;&#22270;&#20687;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35273;&#20449;&#24687;&#24357;&#34917;&#20102;&#22024;&#26434;&#30340;&#28145;&#24230;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#20934;&#30830;&#39044;&#27979;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#28857;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#37325;&#26032;&#35774;&#35745;&#20102;&#20851;&#38190;&#28857;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#20943;&#36731;&#20851;&#38190;&#28857;&#39044;&#27979;&#22122;&#22768;&#23545;&#36879;&#35270;n&#28857;(PnP)&#31639;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#27604;&#22522;&#32447;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#26159;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#35937;&#19978;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#30495;&#23454;&#29289;&#20307;&#19978;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#23545;&#27604;&#35843;&#25972;&#65292;&#29992;&#20110;&#23398;&#20064; Markov &#36716;&#31227;&#26680;&#65292;&#24182;&#22312;&#22122;&#22768;&#26680;&#30340;&#24110;&#21161;&#19979;&#36827;&#34892;&#25968;&#25454;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#35813;&#31639;&#27861;&#21487;&#20197;&#23545;&#25968;&#25454;&#27969;&#24418;&#36827;&#34892;&#23616;&#37096;&#25506;&#32034;&#24182;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.05497</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#35843;&#25972;&#23398;&#20064;&#31283;&#23450;&#30340; Markov &#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Stationary Markov Processes with Contrastive Adjustment. (arXiv:2303.05497v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#23545;&#27604;&#35843;&#25972;&#65292;&#29992;&#20110;&#23398;&#20064; Markov &#36716;&#31227;&#26680;&#65292;&#24182;&#22312;&#22122;&#22768;&#26680;&#30340;&#24110;&#21161;&#19979;&#36827;&#34892;&#25968;&#25454;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#35813;&#31639;&#27861;&#21487;&#20197;&#23545;&#25968;&#25454;&#27969;&#24418;&#36827;&#34892;&#23616;&#37096;&#25506;&#32034;&#24182;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#19981;&#26029;&#25913;&#36827;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#31216;&#20026;&#23545;&#27604;&#35843;&#25972;&#65292;&#29992;&#20110;&#23398;&#20064; Markov &#36716;&#31227;&#26680;&#65292;&#20351;&#20854;&#31283;&#24577;&#20998;&#24067;&#19982;&#25968;&#25454;&#20998;&#24067;&#30456;&#21305;&#37197;&#12290;&#23545;&#27604;&#35843;&#25972;&#19981;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#36716;&#31227;&#20998;&#24067;&#23478;&#26063;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#36830;&#32493;&#21644;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#25968;&#25454;&#12290;&#21463;&#21040;&#26368;&#36817;&#20851;&#20110;&#22122;&#22768;&#36864;&#28779;&#37319;&#26679;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#36716;&#31227;&#31639;&#23376;&#65292;&#21363;&#22122;&#22768;&#26680;&#65292;&#21487;&#20197;&#22312;&#20132;&#25442;&#36895;&#24230;&#19982;&#26679;&#26412;&#20445;&#30495;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#23545;&#27604;&#35843;&#25972;&#22312;&#20154;&#26426;&#35774;&#35745;&#27969;&#31243;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#25152;&#23398;&#30340; Markov &#38142;&#30340;&#19981;&#21464;&#24615;&#20351;&#24471;&#21487;&#20197;&#23545;&#25968;&#25454;&#27969;&#24418;&#36827;&#34892;&#23616;&#37096;&#25506;&#32034;&#65292;&#24182;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#23545;&#27604;&#35843;&#25972;&#35757;&#32451;&#30340;&#22122;&#22768;&#26680;&#30340;&#24615;&#33021;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#22312;&#21508;&#31181;&#22270;&#20687;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#26377; promising &#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new optimization algorithm, termed contrastive adjustment, for learning Markov transition kernels whose stationary distribution matches the data distribution. Contrastive adjustment is not restricted to a particular family of transition distributions and can be used to model data in both continuous and discrete state spaces. Inspired by recent work on noise-annealed sampling, we propose a particular transition operator, the noise kernel, that can trade mixing speed for sample fidelity. We show that contrastive adjustment is highly valuable in human-computer design processes, as the stationarity of the learned Markov chain enables local exploration of the data manifold and makes it possible to iteratively refine outputs by human feedback. We compare the performance of noise kernels trained with contrastive adjustment to current state-of-the-art generative models and demonstrate promising results on a variety of image synthesis tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.03770</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised Domain Adaptation. (arXiv:2303.03770v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#30340;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#36866;&#24212;&#30446;&#26631;&#22495;&#65292;&#20854;&#20851;&#38190;&#26159;&#36890;&#36807;&#20272;&#35745;&#20266;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25351;&#23548;&#20854;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#20197;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#20551;&#23450;&#22312;&#36866;&#24212;&#36807;&#31243;&#20013;&#21516;&#26102;&#21487;&#29992;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#28304;&#33258;&#36866;&#24212;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UDA&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;UDA&#30340;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#65292;&#22312;&#35813;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#27809;&#26377;&#35775;&#38382;&#28304;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;SF-UDA&#35774;&#32622;&#65292;&#22522;&#20110;&#25439;&#22833;&#37325;&#26032;&#21152;&#26435;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#23545;&#20266;&#26631;&#31614;&#30340;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;&#35813;&#20998;&#31867;&#25439;&#22833;&#22522;&#20110;&#20272;&#35745;&#20854;&#19981;&#30830;&#23450;&#24615;&#26469;&#37325;&#26032;&#21152;&#26435;&#65292;&#20197;&#25351;&#23548;&#20266;&#26631;&#31614;&#30340;&#36827;&#19968;&#27493;&#31934;&#21270;&#65292;&#24182;&#36890;&#36807;&#32858;&#38598;&#30456;&#37051;&#26679;&#26412;&#30340;&#30693;&#35782;&#26469;&#36880;&#27493;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#26694;&#26550;&#26469;&#20316;&#20026;&#30446;&#26631;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;&#30693;&#35782;&#30340;&#32858;&#21512;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#26679;&#26412;&#23545;&#25490;&#38500;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#38500;&#30001;&#20849;&#20139;&#30456;&#21516;&#29305;&#24449;&#30340;&#26679;&#26412;&#26500;&#25104;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard Unsupervised Domain Adaptation (UDA) methods assume the availability of both source and target data during the adaptation. In this work, we investigate Source-free Unsupervised Domain Adaptation (SF-UDA), a specific case of UDA where a model is adapted to a target domain without access to source data. We propose a novel approach for the SF-UDA setting based on a loss reweighting strategy that brings robustness against the noise that inevitably affects the pseudo-labels. The classification loss is reweighted based on the reliability of the pseudo-labels that is measured by estimating their uncertainty. Guided by such reweighting strategy, the pseudo-labels are progressively refined by aggregating knowledge from neighbouring samples. Furthermore, a self-supervised contrastive framework is leveraged as a target space regulariser to enhance such knowledge aggregation. A novel negative pairs exclusion strategy is proposed to identify and exclude negative pairs made of samples shari
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#36866;&#24212;&#24178;&#39044;&#26469;&#21152;&#24378;&#30111;&#30142;&#30417;&#27979;&#21644;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#25913;&#21892;&#21307;&#25252;&#36136;&#37327;&#65292;&#25552;&#39640;&#26816;&#27979;&#29575;&#21644;&#20844;&#20849;&#21355;&#29983;&#65292;&#20943;&#23569;&#33647;&#21697;&#30701;&#32570;&#21644;&#20026;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.02075</link><description>&lt;p&gt;
&#20840;&#29699;&#20581;&#24247;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#65306;&#20197;&#30111;&#30142;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adaptive Interventions for Global Health: A Case Study of Malaria. (arXiv:2303.02075v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02075
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#21644;&#26426;&#22120;&#23398;&#20064;&#33258;&#36866;&#24212;&#24178;&#39044;&#26469;&#21152;&#24378;&#30111;&#30142;&#30417;&#27979;&#21644;&#27835;&#30103;&#20381;&#20174;&#24615;&#65292;&#25913;&#21892;&#21307;&#25252;&#36136;&#37327;&#65292;&#25552;&#39640;&#26816;&#27979;&#29575;&#21644;&#20844;&#20849;&#21355;&#29983;&#65292;&#20943;&#23569;&#33647;&#21697;&#30701;&#32570;&#21644;&#20026;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30111;&#30142;&#26159;&#21487;&#20197;&#39044;&#38450;&#12289;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#30142;&#30149;&#65307;&#20294;&#27599;&#24180;&#20173;&#26377;&#36229;&#36807;&#20004;&#20159;&#20010;&#30149;&#20363;&#21644;&#20004;&#19975;&#20010;&#21487;&#39044;&#38450;&#27515;&#20129;&#12290;&#23588;&#20854;&#22312;&#25746;&#21704;&#25289;&#20197;&#21335;&#38750;&#27954;&#30340;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#65292;&#30111;&#30142;&#20173;&#28982;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#20844;&#20849;&#21355;&#29983;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#31227;&#21160;&#20581;&#24247;&#24212;&#29992;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#24178;&#39044;&#65292;&#21487;&#20197;&#21152;&#24378;&#30111;&#30142;&#30417;&#27979;&#21644;&#27835;&#30103;&#30340;&#20381;&#20174;&#24615;&#65292;&#22686;&#21152;&#26816;&#27979;&#65292;&#34913;&#37327;&#25552;&#20379;&#32773;&#30340;&#25216;&#33021;&#21644;&#25252;&#29702;&#36136;&#37327;&#65292;&#36890;&#36807;&#25903;&#25345;&#19968;&#32447;&#24037;&#20316;&#20154;&#21592;&#21644;&#24739;&#32773;&#65288;&#22914;&#23481;&#37327;&#24314;&#35774;&#21644;&#40723;&#21169;&#34892;&#20026;&#21464;&#21270;&#65292;&#22914;&#20351;&#29992;&#34442;&#24080;&#65289;&#25913;&#21892;&#20844;&#20849;&#21355;&#29983;&#65292;&#20943;&#23569;&#33647;&#24215;&#21644;&#35786;&#25152;&#30340;&#27979;&#35797;&#24211;&#23384;&#30701;&#32570;&#24182;&#20026;&#25919;&#31574;&#24178;&#39044;&#25552;&#20379;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Malaria can be prevented, diagnosed, and treated; however, every year, there are more than 200 million cases and 200.000 preventable deaths. Malaria remains a pressing public health concern in low- and middle-income countries, especially in sub-Saharan Africa. We describe how by means of mobile health applications, machine-learning-based adaptive interventions can strengthen malaria surveillance and treatment adherence, increase testing, measure provider skills and quality of care, improve public health by supporting front-line workers and patients (e.g., by capacity building and encouraging behavioral changes, like using bed nets), reduce test stockouts in pharmacies and clinics and informing public health for policy intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36328;&#27169;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#22320;&#20272;&#35745;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#65292;&#24182;&#22312;&#36816;&#21160;&#20998;&#21106;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31561;&#23376;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.00462</link><description>&lt;p&gt;
&#38544;&#34255;&#30340;&#23453;&#30707;&#65306;&#20351;&#29992;&#36328;&#27169;&#24577;&#30417;&#30563;&#30340;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hidden Gems: 4D Radar Scene Flow Learning Using Cross-Modal Supervision. (arXiv:2303.00462v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36328;&#27169;&#24577;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#22320;&#20272;&#35745;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#65292;&#24182;&#22312;&#36816;&#21160;&#20998;&#21106;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#31561;&#23376;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#36328;&#27169;&#24577;&#23398;&#20064;&#36827;&#34892;4D&#38647;&#36798;&#22522;&#30784;&#22330;&#26223;&#27969;&#37327;&#20272;&#35745;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#29616;&#20195;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#21516;&#19968;&#20301;&#32622;&#30340;&#20256;&#24863;&#22120;&#20887;&#20313;&#30340;&#21551;&#21457;&#12290;&#36825;&#31181;&#20887;&#20313;&#38544;&#21547;&#22320;&#20026;&#38647;&#36798;&#22330;&#26223;&#27969;&#20272;&#35745;&#25552;&#20379;&#20102;&#21508;&#31181;&#24418;&#24335;&#30340;&#30417;&#30563;&#32447;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#38024;&#23545;&#24050;&#30830;&#23450;&#30340;&#36328;&#27169;&#24577;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#20351;&#29992;&#22810;&#20010;&#36328;&#27169;&#24577;&#32422;&#26463;&#26426;&#20250;&#26377;&#25928;&#22320;&#36827;&#34892;&#22330;&#26223;&#27969;&#37327;&#20272;&#35745;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#36328;&#27169;&#24577;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#25512;&#26029;&#26356;&#20934;&#30830;&#30340;4D&#38647;&#36798;&#22330;&#26223;&#27969;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#23545;&#20004;&#20010;&#23376;&#20219;&#21153;-&#36816;&#21160;&#20998;&#21106;&#21644;&#33258;&#25105;&#36816;&#21160;&#20272;&#35745;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#23558;&#22312;https://github.com/Toytiny/CMFlow&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a novel approach to 4D radar-based scene flow estimation via cross-modal learning. Our approach is motivated by the co-located sensing redundancy in modern autonomous vehicles. Such redundancy implicitly provides various forms of supervision cues to the radar scene flow estimation. Specifically, we introduce a multi-task model architecture for the identified cross-modal learning problem and propose loss functions to opportunistically engage scene flow estimation using multiple cross-modal constraints for effective model training. Extensive experiments show the state-of-the-art performance of our method and demonstrate the effectiveness of cross-modal supervised learning to infer more accurate 4D radar scene flow. We also show its usefulness to two subtasks - motion segmentation and ego-motion estimation. Our source code will be available on https://github.com/Toytiny/CMFlow.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#22312;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.00055</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#26102;&#38388;&#23610;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning time-scales in two-layers neural networks. (arXiv:2303.00055v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#22312;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#20855;&#26377;&#22810;&#20010;&#24341;&#20154;&#27880;&#24847;&#30340;&#29305;&#28857;&#12290;&#23588;&#20854;&#26159;&#65292;&#22312;&#22823;&#25209;&#37327;&#25968;&#25454;&#24179;&#22343;&#21518;&#65292;&#32463;&#39564;&#39118;&#38505;&#30340;&#19979;&#38477;&#36895;&#29575;&#26159;&#38750;&#21333;&#35843;&#30340;&#12290;&#20960;&#20046;&#27809;&#26377;&#36827;&#23637;&#30340;&#38271;&#21608;&#26399;&#21644;&#24555;&#36895;&#19979;&#38477;&#30340;&#38388;&#38548;&#20132;&#26367;&#20986;&#29616;&#12290;&#36825;&#20123;&#36830;&#32493;&#30340;&#23398;&#20064;&#38454;&#27573;&#24448;&#24448;&#22312;&#38750;&#24120;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#36827;&#34892;&#12290;&#26368;&#21518;&#65292;&#22312;&#26089;&#26399;&#38454;&#27573;&#23398;&#20064;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#8220;&#31616;&#21333;&#30340;&#8221;&#25110;&#8220;&#26131;&#20110;&#23398;&#20064;&#30340;&#8221;&#65292;&#23613;&#31649;&#20197;&#38590;&#20197;&#24418;&#24335;&#21270;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#31526;&#21512;&#21333;&#25351;&#25968;&#27169;&#22411;&#30340;&#39640;&#32500;&#23485;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#65292;&#22312;&#19968;&#31995;&#21015;&#26032;&#30340;&#20005;&#23494;&#32467;&#26524;&#12289;&#38750;&#20005;&#23494;&#25968;&#23398;&#25512;&#23548;&#21644;&#25968;&#20540;&#23454;&#39564;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20379;&#20102;&#23545;&#32593;&#32476;&#23398;&#20064;&#21160;&#24577;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#29305;&#21035;&#25351;&#20986;&#65292;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#29575;&#21442;&#25968;&#21270;&#28165;&#26224;&#30340;&#38454;&#27573;&#36716;&#25442;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#38271;&#21608;&#26399;&#30340;&#20986;&#29616;&#21644;&#28040;&#22833;&#26377;&#20851;&#12290;&#25105;&#20204;&#36824;&#20026;&#26089;&#26399;&#23398;&#20064;&#26102;&#25152;&#23398;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#35268;&#33539;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based learning in multi-layer neural networks displays a number of striking features. In particular, the decrease rate of empirical risk is non-monotone even after averaging over large batches. Long plateaus in which one observes barely any progress alternate with intervals of rapid decrease. These successive phases of learning often take place on very different time scales. Finally, models learnt in an early phase are typically `simpler' or `easier to learn' although in a way that is difficult to formalize.  Although theoretical explanations of these phenomena have been put forward, each of them captures at best certain specific regimes. In this paper, we study the gradient flow dynamics of a wide two-layer neural network in high-dimension, when data are distributed according to a single-index model (i.e., the target function depends on a one-dimensional projection of the covariates). Based on a mixture of new rigorous results, non-rigorous mathematical derivations, and numer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#27979;&#37327;&#21464;&#37327;&#30340;&#25968;&#25454;&#30340;&#20869;&#26680;&#26041;&#27861;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#32473;&#20986;&#20102;&#26497;&#38480;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2302.14446</link><description>&lt;p&gt;
&#22343;&#22330;&#26497;&#38480;&#20013;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Reproducing kernel Hilbert spaces in the mean field limit. (arXiv:2302.14446v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20316;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#27979;&#37327;&#21464;&#37327;&#30340;&#25968;&#25454;&#30340;&#20869;&#26680;&#26041;&#27861;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#32473;&#20986;&#20102;&#26497;&#38480;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#26680;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21463;&#27426;&#36814;&#21644;&#25104;&#21151;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#23427;&#20204;&#26377;&#19968;&#20010;&#25104;&#29087;&#30340;&#29702;&#35770;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#25903;&#25345;&#12290;&#20174;&#25968;&#23398;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#20869;&#26680;&#30340;&#27010;&#24565;&#21644;&#20869;&#26680;&#29983;&#25104;&#30340;&#20989;&#25968;&#31354;&#38388;&#65292;&#21363;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#12290;&#21463;&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#23398;&#20064;&#26041;&#27861;&#26368;&#36817;&#30340;&#21457;&#23637;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20316;&#29992;&#20110;&#20855;&#26377;&#35768;&#22810;&#27979;&#37327;&#21464;&#37327;&#30340;&#25968;&#25454;&#30340;&#20869;&#26680;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20869;&#26680;&#30340;&#20005;&#26684;&#22343;&#22330;&#26497;&#38480;&#65292;&#24182;&#25552;&#20379;&#20102;&#26497;&#38480;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20960;&#20010;&#20869;&#26680;&#30340;&#31034;&#20363;&#65292;&#36825;&#20123;&#20869;&#26680;&#20801;&#35768;&#20005;&#26684;&#30340;&#22343;&#22330;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel methods, being supported by a well-developed theory and coming with efficient algorithms, are among the most popular and successful machine learning techniques. From a mathematical point of view, these methods rest on the concept of kernels and function spaces generated by kernels, so called reproducing kernel Hilbert spaces. Motivated by recent developments of learning approaches in the context of interacting particle systems, we investigate kernel methods acting on data with many measurement variables. We show the rigorous mean field limit of kernels and provide a detailed analysis of the limiting reproducing kernel Hilbert space. Furthermore, several examples of kernels, that allow a rigorous mean field limit, are presented.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.13356</link><description>&lt;p&gt;
&#34920;&#29616;&#19981;&#36275;&#20197;&#20026;&#30408;&#65292;&#28145;&#31350;Rashomon&#30340;&#22235;&#37325;&#22863;
&lt;/p&gt;
&lt;p&gt;
Performance is not enough: a story of the Rashomon's quartet. (arXiv:2302.13356v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20854;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24314;&#27169;&#36890;&#24120;&#34987;&#31616;&#21270;&#20026;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#26469;&#20248;&#21270;&#36873;&#23450;&#30340;&#24615;&#33021;&#24230;&#37327;&#12290;&#20294;&#22914;&#26524;&#31532;&#20108;&#20248;&#27169;&#22411;&#33021;&#22815;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#24335;&#21516;&#26679;&#25551;&#36848;&#25968;&#25454;&#21602;&#65311;&#31532;&#19977;&#20010;&#27169;&#22411;&#21602;&#65311;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#20250;&#23398;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#25968;&#25454;&#20851;&#31995;&#21527;&#65311;&#21463;&#21040;Anscombe&#22235;&#37325;&#22863;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Rashomon&#30340;&#22235;&#37325;&#22863;&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340;&#22235;&#20010;&#27169;&#22411;&#20855;&#26377;&#20960;&#20046;&#30456;&#21516;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#35270;&#21270;&#25581;&#31034;&#20102;&#26497;&#20854;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#32467;&#26500;&#12290;&#24341;&#20837;&#30340;&#31616;&#21333;&#31034;&#20363;&#26088;&#22312;&#36827;&#19968;&#27493;&#20419;&#36827;&#21487;&#35270;&#21270;&#20316;&#20026;&#27604;&#36739;&#39044;&#27979;&#27169;&#22411;&#36229;&#36234;&#24615;&#33021;&#30340;&#24517;&#35201;&#24037;&#20855;&#12290;&#25105;&#20204;&#38656;&#35201;&#24320;&#21457;&#23500;&#26377;&#27934;&#23519;&#21147;&#30340;&#25216;&#26415;&#26469;&#35299;&#37322;&#27169;&#22411;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive modelling is often reduced to finding the best model that optimizes a selected performance measure. But what if the second-best model describes the data equally well but in a completely different way? What about the third? Is it possible that the most effective models learn completely different relationships in the data? Inspired by Anscombe's quartet, this paper introduces Rashomon's quartet, a synthetic dataset for which four models from different classes have practically identical predictive performance. However, their visualization reveals drastically distinct ways of understanding the correlation structure in data. The introduced simple illustrative example aims to further facilitate visualization as a mandatory tool to compare predictive models beyond their performance. We need to develop insightful techniques for the explanatory analysis of model sets.
&lt;/p&gt;</description></item><item><title>EAGLE&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#21387;&#21147;&#21644;&#36895;&#24230;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2302.10803</link><description>&lt;p&gt;
Eagle: &#22522;&#20110;&#32593;&#26684;&#21464;&#25442;&#22120;&#30340;&#28237;&#27969;&#27969;&#20307;&#21160;&#21147;&#23398;&#22823;&#35268;&#27169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Eagle: Large-Scale Learning of Turbulent Fluid Dynamics with Mesh Transformers. (arXiv:2302.10803v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10803
&lt;/p&gt;
&lt;p&gt;
EAGLE&#24341;&#20837;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#26032;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#21464;&#25442;&#22120;&#65292;&#33021;&#22815;&#39044;&#27979;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#20307;&#21160;&#21147;&#23398;&#25968;&#25454;&#38598;&#20013;&#30340;&#21387;&#21147;&#21644;&#36895;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#36890;&#36807;&#27169;&#25311;&#21644;&#35745;&#31639;&#25968;&#20540;&#27169;&#22411;&#35299;&#20915;Navier-Stokes&#26041;&#31243;&#26469;&#20272;&#35745;&#27969;&#20307;&#21160;&#21147;&#23398;&#65292;&#21363;&#20351;&#22312;&#39640;&#31471;&#30828;&#20214;&#19978;&#20063;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#36825;&#26159;&#19968;&#20010;&#26497;&#20854;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#21464;&#31181;&#30340;&#26041;&#27861;&#24050;&#32463;&#24320;&#22987;&#23581;&#35797;&#35299;&#20915;&#27492;&#31867;&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#37117;&#21482;&#38024;&#23545;&#20960;&#20309;&#24418;&#29366;&#22266;&#23450;&#30340;&#38745;&#24577;&#22330;&#26223;&#20013;&#30340;&#38745;&#24577;&#23545;&#35937;&#36827;&#34892;&#20102;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#35797;&#22270;&#36229;&#36234;&#29616;&#26377;&#24037;&#20316;&#30340;&#22797;&#26434;&#24230;&#65292;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EAGLE&#65292;&#19968;&#20010;&#21253;&#21547;1.1&#30334;&#19975;&#20010;&#20108;&#32500;&#32593;&#26684;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#32593;&#26684;&#26159;&#30001;&#19968;&#20010;&#31227;&#21160;&#27969;&#20307;&#28304;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#65292;&#20854;&#30456;&#20114;&#20316;&#29992;&#23548;&#33268;&#20102;&#38750;&#32447;&#24615;&#22330;&#26223;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;600&#20010;&#19981;&#21516;&#22330;&#26223;&#12290;&#20026;&#20102;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;EAGLE&#25968;&#25454;&#38598;&#36827;&#34892;&#26410;&#26469;&#30340;&#21387;&#21147;&#21644;&#36895;&#24230;&#39044;&#27979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#32593;&#26684;&#21464;&#25442;&#22120;&#65292;&#23427;&#21033;&#29992;&#20102;&#33410;&#28857;&#32858;&#31867;&#12289;&#22270;&#27744;&#21270;&#21644;&#20840;&#23616;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating fluid dynamics is classically done through the simulation and integration of numerical models solving the Navier-Stokes equations, which is computationally complex and time-consuming even on high-end hardware. This is a notoriously hard problem to solve, which has recently been addressed with machine learning, in particular graph neural networks (GNN) and variants trained and evaluated on datasets of static objects in static scenes with fixed geometry. We attempt to go beyond existing work in complexity and introduce a new model, method and benchmark. We propose EAGLE, a large-scale dataset of 1.1 million 2D meshes resulting from simulations of unsteady fluid dynamics caused by a moving flow source interacting with nonlinear scene structure, comprised of 600 different scenes of three different types. To perform future forecasting of pressure and velocity on the challenging EAGLE dataset, we introduce a new mesh transformer. It leverages node clustering, graph pooling and glo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32858;&#31867;&#20559;&#26012;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#21644;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#37096;&#35757;&#32451;&#27491;&#21017;&#21270;&#26041;&#27861;</title><link>http://arxiv.org/abs/2302.10413</link><description>&lt;p&gt;
CADIS&#65306;&#37319;&#29992;&#32858;&#31867;&#32858;&#21512;&#21644;&#30693;&#35782;&#33976;&#39311;&#27491;&#21017;&#21270;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#32858;&#31867;&#20559;&#26012;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
CADIS: Handling Cluster-skewed Non-IID Data in Federated Learning with Clustered Aggregation and Knowledge DIStilled Regularization. (arXiv:2302.10413v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32858;&#31867;&#20559;&#26012;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#21644;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#23616;&#37096;&#35757;&#32451;&#27491;&#21017;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#36793;&#32536;&#35774;&#22791;&#33021;&#22815;&#21327;&#20316;&#22320;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#19981;&#26292;&#38706;&#23427;&#20204;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65288;IID&#65289;&#26102;&#65292;&#21363;&#30001;&#36890;&#24120;&#19981;&#29420;&#31435;&#19988;&#21516;&#20998;&#24067;&#30340;&#23458;&#25143;&#31471;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#30340;&#19968;&#31181;&#26032;&#22411;&#38750;IID&#25968;&#25454;&#65292;&#31216;&#20026;&#32858;&#31867;&#20559;&#26012;&#38750;IID&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#31181;&#25968;&#25454;&#29616;&#35937;&#26159;&#25351;&#23458;&#25143;&#31471;&#21487;&#20197;&#34987;&#20998;&#25104;&#20855;&#26377;&#30456;&#20284;&#25968;&#25454;&#20998;&#24067;&#30340;&#32676;&#32452;&#12290;&#36890;&#36807;&#23545;&#20998;&#31867;&#27169;&#22411;&#30340;&#27425;&#32423;&#23618;&#34892;&#20026;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24230;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#20004;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#30456;&#20284;&#24230;&#65292;&#21516;&#26102;&#19981;&#36829;&#21453;&#20854;&#38544;&#31169;&#26435;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#21512;&#26041;&#26696;&#65292;&#30830;&#20445;&#19981;&#21516;&#32676;&#32452;&#20043;&#38388;&#30340;&#24179;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#22411;&#23616;&#37096;&#35757;&#32451;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Federated learning enables edge devices to train a global model collaboratively without exposing their data. Despite achieving outstanding advantages in computing efficiency and privacy protection, federated learning faces a significant challenge when dealing with non-IID data, i.e., data generated by clients that are typically not independent and identically distributed. In this paper, we tackle a new type of Non-IID data, called cluster-skewed non-IID, discovered in actual data sets. The cluster-skewed non-IID is a phenomenon in which clients can be grouped into clusters with similar data distributions. By performing an in-depth analysis of the behavior of a classification model's penultimate layer, we introduce a metric that quantifies the similarity between two clients' data distributions without violating their privacy. We then propose an aggregation scheme that guarantees equality between clusters. In addition, we offer a novel local training regularization based on the knowledge
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#22522;&#20110;&#39063;&#31890;&#21270;&#31070;&#32463;&#20803;&#32423;&#21035;&#21487;&#35299;&#37322;&#24615;&#30340; AI &#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#38745;&#33033;&#33639;&#20809;&#25104;&#20687;&#20013;&#35780;&#20272;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36827;&#31243;&#65292;&#39564;&#35777;&#35270;&#32593;&#33180;&#34880;&#31649;&#31995;&#32479;&#20026; AD &#35780;&#20272;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2302.03008</link><description>&lt;p&gt;
LAVA&#65306;&#22522;&#20110;&#39063;&#31890;&#21270;&#31070;&#32463;&#20803;&#32423;&#21035;&#21487;&#35299;&#37322;&#24615;&#30340;&#38745;&#33033;&#33639;&#20809;&#25104;&#20687;&#35786;&#26029;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340; AI
&lt;/p&gt;
&lt;p&gt;
LAVA: Granular Neuron-Level Explainable AI for Alzheimer's Disease Assessment from Fundus Images. (arXiv:2302.03008v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03008
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#22522;&#20110;&#39063;&#31890;&#21270;&#31070;&#32463;&#20803;&#32423;&#21035;&#21487;&#35299;&#37322;&#24615;&#30340; AI &#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#38745;&#33033;&#33639;&#20809;&#25104;&#20687;&#20013;&#35780;&#20272;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#36827;&#31243;&#65292;&#39564;&#35777;&#35270;&#32593;&#33180;&#34880;&#31649;&#31995;&#32479;&#20026; AD &#35780;&#20272;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#28176;&#36827;&#24615;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#26159;&#30196;&#21574;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26089;&#26399;&#35786;&#26029;&#23545;&#24739;&#32773;&#20174;&#28508;&#22312;&#30340;&#24178;&#39044;&#21644;&#27835;&#30103;&#20013;&#33719;&#30410;&#33267;&#20851;&#37325;&#35201;&#12290;&#35270;&#32593;&#33180;&#22240;&#20854;&#19982;&#22823;&#33041;&#30340;&#35299;&#21078;&#32852;&#31995;&#65292;&#34987;&#20551;&#23450;&#20026; AD &#26816;&#27979;&#30340;&#35786;&#26029;&#37096;&#20301;&#12290;&#28982;&#32780;&#30456;&#20851;&#30340; AI &#27169;&#22411;&#23578;&#26410;&#25552;&#20379;&#20851;&#20110;&#20915;&#31574;&#30340;&#21512;&#29702;&#35299;&#37322;&#65292;&#20063;&#26080;&#27861;&#25512;&#26029;&#30142;&#30149;&#36827;&#23637;&#30340;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615; AI &#26694;&#26550;&#65292;&#31216;&#20026; Granular Neuron-level Explainer&#65288;LAVA&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#35299;&#37322;&#21407;&#22411;&#65292;&#21487;&#20197;&#25506;&#27979;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#20197;&#30452;&#25509;&#20174;&#35270;&#32593;&#33180;&#25104;&#20687;&#20013;&#35780;&#20272; AD &#36827;&#31243;&#12290;&#26412;&#26041;&#27861;&#24212;&#29992;&#20110;&#39564;&#35777;&#35270;&#32593;&#33180;&#34880;&#31649;&#31995;&#32479;&#20026; AD &#35780;&#20272;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#21644;&#35786;&#26029;&#25163;&#27573;&#12290;&#36890;&#36807; UK Biobank &#35748;&#30693;&#27979;&#35797;&#21644;&#34880;&#31649;&#37492;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD) is a progressive neurodegenerative disease and the leading cause of dementia. Early diagnosis is critical for patients to benefit from potential intervention and treatment. The retina has been hypothesized as a diagnostic site for AD detection owing to its anatomical connection with the brain. Developed AI models for this purpose have yet to provide a rational explanation about the decision and neither infer the stage of disease's progression. Along this direction, we propose a novel model-agnostic explainable-AI framework, called Granular Neuron-level Explainer (LAVA), an interpretation prototype that probes into intermediate layers of the Convolutional Neural Network (CNN) models to assess the AD continuum directly from the retinal imaging without longitudinal or clinical evaluation. This method is applied to validate the retinal vasculature as a biomarker and diagnostic modality for Alzheimer's Disease (AD) evaluation. UK Biobank cognitive tests and vascular
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;FES&#25511;&#21046;&#20013;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32908;&#32905;&#30130;&#21171;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.04005</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25511;&#21046;&#30340;&#34920;&#38754;&#32908;&#32905;&#30005;&#21050;&#28608;&#33218;&#37096;&#36816;&#21160;&#24674;&#22797;&#65306;&#21033;&#29992;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#25511;&#21046;&#36880;&#28176;&#21152;&#37325;&#30340;&#32908;&#32905;&#30130;&#21171;&#65288;arXiv:2301.04005v2 [eess.SY] &#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards AI-controlled FES-restoration of arm movements: Controlling for progressive muscular fatigue with Gaussian state-space models. (arXiv:2301.04005v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;FES&#25511;&#21046;&#20013;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32908;&#32905;&#30130;&#21171;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#21160;&#38556;&#30861;&#20250;&#38480;&#21046;&#20010;&#20307;&#23436;&#25104;&#26085;&#24120;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#34920;&#38754;&#32908;&#32905;&#30005;&#21050;&#28608;&#65288;FES&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#38750;&#20405;&#20837;&#24335;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24674;&#22797;&#22833;&#21435;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;FES&#35825;&#23548;&#26399;&#26395;&#30340;&#36816;&#21160;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#24037;&#31243;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#30001;&#20154;&#31867;&#33218;&#37096;&#31070;&#32463;&#26426;&#26800;&#22797;&#26434;&#24615;&#21644;&#20010;&#20307;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#21463;&#35797;&#32773;&#21644;&#22330;&#26223;&#21046;&#23450;&#23450;&#21046;&#21270;&#30340;&#25511;&#21046;&#35268;&#21017;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;RL&#25511;&#21046;FES&#30340;&#19968;&#20010;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#26159;&#36880;&#28176;&#21464;&#21270;&#30340;&#32908;&#32905;&#30130;&#21171;&#65292;&#23427;&#20316;&#20026;&#21050;&#28608;&#30340;&#26410;&#30693;&#20989;&#25968;&#19981;&#26029;&#25913;&#21464;&#65292;&#25171;&#30772;&#20102;RL&#30340;&#39532;&#23572;&#21487;&#22827;&#20551;&#35774;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32908;&#32905;&#30130;&#21171;&#38382;&#39064;&#65292;&#20351;&#25105;&#20204;&#30340;RL&#25511;&#21046;&#22120;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25511;&#21046;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#21033;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#29366;&#24577;&#31354;&#38388;&#30340;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GSSM&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reaching disability limits an individual's ability in performing daily tasks. Surface Functional Electrical Stimulation (FES) offers a non-invasive solution to restore the lost abilities. However, inducing desired movements using FES is still an open engineering problem. This problem is accentuated by the complexities of human arms' neuromechanics and the variations across individuals. Reinforcement Learning (RL) emerges as a promising approach to govern customised control rules for different subjects and settings. Yet, one remaining challenge of using RL to control FES is unobservable muscle fatigue that progressively changes as an unknown function of the stimulation, breaking the Markovian assumption of RL. In this work, we present a method to address the unobservable muscle fatigue issue, allowing our RL controller to achieve higher control performances. Our method is based on a Gaussian State-Space Model (GSSM) that utilizes recurrent neural networks to learn Markovian state-spaces
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#25511;&#21046;FES&#23454;&#29616;&#25152;&#38656;&#36816;&#21160;&#26041;&#21521;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26426;&#26800;&#23398;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;FES&#25511;&#21046;&#65292;&#20197;&#24674;&#22797;&#19978;&#32930;&#36816;&#21160;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.04004</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#26426;&#26800;&#23398;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#19978;&#32930;&#36816;&#21160;&#30340;FES&#24674;&#22797;&#30340;AI&#25511;&#21046;&#65306;3D&#20280;&#25163;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Towards AI-controlled FES-restoration of arm movements: neuromechanics-based reinforcement learning for 3-D reaching. (arXiv:2301.04004v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#33268;&#21147;&#20110;&#35299;&#20915;&#25511;&#21046;FES&#23454;&#29616;&#25152;&#38656;&#36816;&#21160;&#26041;&#21521;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#26426;&#26800;&#23398;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#34892;FES&#25511;&#21046;&#65292;&#20197;&#24674;&#22797;&#19978;&#32930;&#36816;&#21160;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#30251;&#21644;&#19978;&#32930;&#30340;&#36816;&#21160;&#38556;&#30861;&#24433;&#21709;&#20102;&#29983;&#27963;&#30340;&#36136;&#37327;&#12290;&#21151;&#33021;&#24615;&#30005;&#21050;&#28608;&#65288;FES&#65289;&#21487;&#20197;&#24674;&#22797;&#22833;&#21435;&#30340;&#32908;&#32905;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25511;&#21046;FES&#20197;&#23454;&#29616;&#25152;&#38656;&#30340;&#36816;&#21160;&#26041;&#21521;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#36890;&#36807;&#26500;&#24314;&#31070;&#32463;&#26426;&#26800;&#27169;&#22411;&#26469;&#23547;&#25214;&#19968;&#33324;&#26041;&#27861;&#26469;&#35843;&#33410;&#25511;&#21046;&#35268;&#21017;&#65292;&#20197;&#23436;&#25104;&#19981;&#21516;&#20219;&#21153;&#21644;&#21463;&#35797;&#32773;&#30340;&#24037;&#31243;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reaching disabilities affect the quality of life. Functional Electrical Stimulation (FES) can restore lost motor functions. Yet, there remain challenges in controlling FES to induce desired movements. Neuromechanical models are valuable tools for developing FES control methods. However, focusing on the upper extremity areas, several existing models are either overly simplified or too computationally demanding for control purposes. Besides the model-related issues, finding a general method for governing the control rules for different tasks and subjects remains an engineering challenge. Here, we present our approach toward FES-based restoration of arm movements to address those fundamental issues in controlling FES. Firstly, we present our surface-FES-oriented neuromechanical models of human arms built using well-accepted, open-source software. The models are designed to capture significant dynamics in FES controls with minimal computational cost. Our models are customisable and can be 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;INVALIDATOR&#65292;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#21644;&#35821;&#27861;&#25512;&#29702;&#33258;&#21160;&#35780;&#20272;&#30001;APR&#29983;&#25104;&#30340;&#34917;&#19969;&#30340;&#27491;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;INVALIDATOR&#21033;&#29992;&#31243;&#24207;&#19981;&#21464;&#37327;&#26469;&#25512;&#29702;&#31243;&#24207;&#30340;&#35821;&#20041;&#24182;&#25429;&#33719;&#31243;&#24207;&#30340;&#35821;&#27861;&#65292;&#28982;&#21518;&#21028;&#26029;APR&#29983;&#25104;&#30340;&#34917;&#19969;&#26159;&#21542;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;INVALIDATOR&#22312;&#26816;&#27979;&#36807;&#24230;&#25311;&#21512;&#34917;&#19969;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.01113</link><description>&lt;p&gt;
Invalidator: &#36890;&#36807;&#35821;&#20041;&#21644;&#35821;&#27861;&#25512;&#29702;&#23454;&#29616;&#33258;&#21160;&#20462;&#22797;&#31243;&#24207;&#30340;&#27491;&#30830;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning. (arXiv:2301.01113v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;INVALIDATOR&#65292;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#21644;&#35821;&#27861;&#25512;&#29702;&#33258;&#21160;&#35780;&#20272;&#30001;APR&#29983;&#25104;&#30340;&#34917;&#19969;&#30340;&#27491;&#30830;&#24615;&#30340;&#25216;&#26415;&#12290;INVALIDATOR&#21033;&#29992;&#31243;&#24207;&#19981;&#21464;&#37327;&#26469;&#25512;&#29702;&#31243;&#24207;&#30340;&#35821;&#20041;&#24182;&#25429;&#33719;&#31243;&#24207;&#30340;&#35821;&#27861;&#65292;&#28982;&#21518;&#21028;&#26029;APR&#29983;&#25104;&#30340;&#34917;&#19969;&#26159;&#21542;&#36807;&#24230;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;INVALIDATOR&#22312;&#26816;&#27979;&#36807;&#24230;&#25311;&#21512;&#34917;&#19969;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#38754;&#20020;&#27979;&#35797;&#36807;&#24230;&#25311;&#21512;&#30340;&#25361;&#25112;&#65292;&#21363;&#29983;&#25104;&#30340;&#34917;&#19969;&#36890;&#36807;&#39564;&#35777;&#27979;&#35797;&#20294;&#26410;&#33021;&#25512;&#24191;&#12290;&#30446;&#21069;&#30340;&#32570;&#38519;&#20462;&#22797;&#35780;&#20272;&#26041;&#27861;&#21253;&#25324;&#29983;&#25104;&#26032;&#27979;&#35797;&#25110;&#25163;&#21160;&#26816;&#26597;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#25110;&#20855;&#26377;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;INVALIDATOR&#65292;&#36890;&#36807;&#35821;&#20041;&#21644;&#35821;&#27861;&#25512;&#29702;&#33258;&#21160;&#35780;&#20272;&#30001;APR&#29983;&#25104;&#30340;&#34917;&#19969;&#30340;&#27491;&#30830;&#24615;&#12290;INVALIDATOR&#21033;&#29992;&#31243;&#24207;&#19981;&#21464;&#37327;&#25512;&#29702;&#31243;&#24207;&#35821;&#20041;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20174;&#22823;&#22411;&#20195;&#30721;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#35821;&#35328;&#35821;&#27861;&#26469;&#25429;&#33719;&#31243;&#24207;&#35821;&#27861;&#12290;&#32473;&#23450;&#26377;&#32570;&#38519;&#30340;&#31243;&#24207;&#21644;&#24320;&#21457;&#20154;&#21592;&#20462;&#22797;&#21518;&#30340;&#31243;&#24207;&#65292;INVALIDATOR&#25512;&#26029;&#20986;&#20004;&#20010;&#31243;&#24207;&#20013;&#21487;&#33021;&#30340;&#19981;&#21464;&#37327;&#12290;&#28982;&#21518;&#65292;INVALIDATOR&#30830;&#23450;APR&#29983;&#25104;&#30340;&#34917;&#19969;&#36807;&#24230;&#25311;&#21512;&#65292;&#22914;&#26524;&#65306;&#65288;1&#65289;&#23427;&#36829;&#21453;&#20102;&#27491;&#30830;&#30340;&#35268;&#33539;&#25110;&#65288;2&#65289;&#20445;&#30041;&#20102;&#21407;&#26469;&#26377;&#32570;&#38519;&#31243;&#24207;&#30340;&#38169;&#35823;&#34892;&#20026;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#26681;&#25454;&#20854;&#31243;&#24207;&#20998;&#26512;&#26080;&#27861;&#30830;&#23450;&#36807;&#24230;&#25311;&#21512;&#34917;&#19969;&#30340;&#24773;&#20917;&#19979;&#65292;INVALIDATOR&#20250;&#22238;&#36864;&#21040;&#29983;&#25104;&#20854;&#20182;&#27979;&#35797;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#30495;&#23454;&#19990;&#30028;&#30340;&#38169;&#35823;&#22522;&#20934;&#19978;&#35780;&#20272;INVALIDATOR&#65292;&#24182;&#26174;&#31034;&#23427;&#22312;&#26816;&#27979;&#36807;&#24230;&#25311;&#21512;&#34917;&#19969;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated program repair (APR) faces the challenge of test overfitting, where generated patches pass validation tests but fail to generalize. Existing methods for patch assessment involve generating new tests or manual inspection, which can be time-consuming or biased. In this paper, we propose a novel technique, INVALIDATOR, to automatically assess the correctness of APR-generated patches via semantic and syntactic reasoning. INVALIDATOR leverages program invariants to reason about program semantics while also capturing program syntax through language semantics learned from a large code corpus using a pre-trained language model. Given a buggy program and the developer-patched program, INVALIDATOR infers likely invariants on both programs. Then, INVALIDATOR determines that an APR-generated patch overfits if: (1) it violates correct specifications or (2) maintains erroneous behaviors from the original buggy program. In case our approach fails to determine an overfitting patch based on i
&lt;/p&gt;</description></item><item><title>SIRL&#26159;&#22522;&#20110;&#20154;&#25552;&#20379;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#20154;&#35782;&#21035;&#21644;&#38548;&#31163;&#22240;&#26524;&#29305;&#24449;&#24182;&#29983;&#25104;&#36866;&#24403;&#34892;&#20026;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2301.00810</link><description>&lt;p&gt;
SIRL: &#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#38544;&#24335;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SIRL: Similarity-based Implicit Representation Learning. (arXiv:2301.00810v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00810
&lt;/p&gt;
&lt;p&gt;
SIRL&#26159;&#22522;&#20110;&#20154;&#25552;&#20379;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#26426;&#22120;&#20154;&#35782;&#21035;&#21644;&#38548;&#31163;&#22240;&#26524;&#29305;&#24449;&#24182;&#29983;&#25104;&#36866;&#24403;&#34892;&#20026;&#65292;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#20351;&#29992;&#39640;&#23481;&#37327;&#27169;&#22411;&#20197;&#21407;&#22987;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#26102;&#65292;&#20182;&#20204;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#20219;&#21153;&#30340;&#8220;&#29305;&#24449;&#8221;&#34920;&#31034;&#21450;&#22914;&#20309;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#21512;&#25104;&#19968;&#20010;&#30446;&#26631;&#12290;&#22914;&#26524;&#20182;&#20204;&#23581;&#35797;&#20174;&#29992;&#20110;&#25945;&#25480;&#23436;&#25972;&#22870;&#21169;&#20989;&#25968;&#30340;&#36755;&#20837;&#20013;&#21516;&#26102;&#23398;&#20064;&#20004;&#32773;&#65292;&#24456;&#23481;&#26131;&#20135;&#29983;&#21253;&#21547;&#25968;&#25454;&#20013;&#20551;&#30456;&#20851;&#24615;&#30340;&#34920;&#31034;&#65292;&#23548;&#33268;&#19981;&#33021;&#25512;&#24191;&#21040;&#26032;&#30340;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#35782;&#21035;&#21644;&#38548;&#31163;&#20154;&#20204;&#23454;&#38469;&#20851;&#24515;&#21644;&#20351;&#29992;&#30340;&#22240;&#26524;&#29305;&#24449;&#65292;&#24403;&#34920;&#31034;&#29366;&#24577;&#21644;&#34892;&#20026;&#26102;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35810;&#38382;&#29992;&#25143;&#35748;&#20026;&#30456;&#20284;&#30340;&#34892;&#20026;&#26469;&#35843;&#25972;&#36825;&#31181;&#34920;&#31034;&#65306;&#22914;&#26524;&#20851;&#38190;&#29305;&#24449;&#30456;&#20284;&#65292;&#36825;&#20123;&#34892;&#20026;&#23558;&#30456;&#20284;&#65292;&#21363;&#20351;&#20302;&#23618;&#34892;&#20026;&#26377;&#25152;&#19981;&#21516;&#65307;&#30456;&#21453;&#65292;&#22914;&#26524;&#21363;&#20351;&#26377;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#19981;&#21516;&#65292;&#37027;&#20040;&#36825;&#20123;&#34892;&#20026;&#23601;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#27491;&#26159;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#23398;&#20064;&#20219;&#21153;&#30446;&#26631;&#24182;&#29983;&#25104;&#36866;&#24403;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#30456;&#20284;&#24230;&#30340;&#38544;&#24335;&#34920;&#31034;&#23398;&#20064;&#65288;SIRL&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30001;&#20154;&#25552;&#20379;&#30340;&#30456;&#20284;&#24615;&#21028;&#26029;&#26469;&#23398;&#20064;&#38544;&#24335;&#20219;&#21153;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#35780;&#20272;SIRL&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#20854;&#20182;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
When robots learn reward functions using high capacity models that take raw state directly as input, they need to both learn a representation for what matters in the task -- the task ``features" -- as well as how to combine these features into a single objective. If they try to do both at once from input designed to teach the full reward function, it is easy to end up with a representation that contains spurious correlations in the data, which fails to generalize to new settings. Instead, our ultimate goal is to enable robots to identify and isolate the causal features that people actually care about and use when they represent states and behavior. Our idea is that we can tune into this representation by asking users what behaviors they consider similar: behaviors will be similar if the features that matter are similar, even if low-level behavior is different; conversely, behaviors will be different if even one of the features that matter differs. This, in turn, is what enables the rob
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#30340;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#19981;&#38656;&#35201;&#37197;&#23545;&#27880;&#37322;&#65292;&#21516;&#26102;&#20351;&#29992;WLI&#21644;NBI&#20004;&#20010;&#39046;&#22495;&#26469;&#23454;&#29616;&#33152;&#33009;&#32452;&#32455;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.11375</link><description>&lt;p&gt;
&#22810;&#21306;&#22495;&#20869;&#31397;&#38236;&#22270;&#20687;&#20013;&#22522;&#20110;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#33152;&#33009;&#32452;&#32455;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Bladder Tissue Classification in Multi-Domain Endoscopic Images. (arXiv:2212.11375v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11375
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#39046;&#22495;&#30340;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65292;&#21487;&#20197;&#19981;&#38656;&#35201;&#37197;&#23545;&#27880;&#37322;&#65292;&#21516;&#26102;&#20351;&#29992;WLI&#21644;NBI&#20004;&#20010;&#39046;&#22495;&#26469;&#23454;&#29616;&#33152;&#33009;&#32452;&#32455;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#22312;&#32463;&#36755;&#23615;&#31649;&#33152;&#33009;&#32959;&#30244;&#20999;&#38500;(TURBT)&#25163;&#26415;&#20013;&#65292;&#20934;&#30830;&#30340;&#33152;&#33009;&#32452;&#32455;&#35270;&#35273;&#20998;&#31867;&#23545;&#20110;&#25913;&#21892;&#26089;&#26399;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;TURBT&#25163;&#26415;&#20013;&#65292;&#20250;&#20351;&#29992;&#30333;&#20809;&#25104;&#20687;&#65288;WLI&#65289;&#21644;&#31364;&#24102;&#25104;&#20687;&#65288;NBI&#65289;&#25216;&#26415;&#26469;&#26816;&#27979;&#30149;&#21464;&#12290;&#27599;&#31181;&#25104;&#20687;&#25216;&#26415;&#37117;&#25552;&#20379;&#20102;&#19981;&#21516;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#20351;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#35782;&#21035;&#21644;&#20998;&#31867;&#30284;&#24615;&#30149;&#21464;&#12290;&#20351;&#29992;&#21516;&#26102;&#20351;&#29992;&#36825;&#20004;&#20010;&#25104;&#20687;&#25216;&#26415;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#20869;&#31397;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#21482;&#26377;WLI&#39046;&#22495;&#20013;&#26377;&#27880;&#37322;&#21487;&#29992;&#65292;&#21516;&#26102;&#20869;&#31397;&#38236;&#22270;&#20687;&#23545;&#24212;&#20110;&#38750;&#37197;&#23545;&#25968;&#25454;&#26102;&#65288;&#21363;&#20004;&#31181;&#25104;&#20687;&#25216;&#26415;&#20043;&#38388;&#27809;&#26377;&#31934;&#30830;&#30340;&#22270;&#20687;&#23545;&#24212;&#65289;&#65292;&#35782;&#21035;&#33152;&#33009;&#32452;&#32455;&#20998;&#31867;&#20173;&#28982;&#26159;&#20010;&#25361;&#25112;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26041;&#27861;&#65292;&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#22312;&#26377;&#26631;&#31614;WLI&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#32593;&#32476;&#65307;&#19968;&#20010;&#24490;&#29615;&#19968;&#33268;&#24615;GAN&#29992;&#20110;&#20174;&#19968;&#31181;&#39046;&#22495;&#32763;&#35793;&#21040;&#21478;&#19968;&#20010;&#39046;&#22495;&#65307;&#20197;&#21450;&#19968;&#20010;&#20351;&#29992;&#20004;&#20010;&#39046;&#22495;&#26469;&#25191;&#34892;&#26368;&#32456;&#20998;&#31867;&#30340;&#23398;&#29983;&#32593;&#32476;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#21644;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#22312;&#38750;&#37197;&#23545;&#25968;&#25454;&#19978;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#19968;&#20010;&#30001;497&#24352;&#33152;&#33009;&#32452;&#32455;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#21333;&#39046;&#22495;&#21644;&#22810;&#39046;&#22495;&#20998;&#31867;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#22343;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#32467;&#35770;&#65306;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#39046;&#22495;&#36716;&#25442;&#25216;&#26415;&#26469;&#25913;&#21892;&#20869;&#31397;&#38236;&#22270;&#20687;&#20013;&#30340;&#33152;&#33009;&#32452;&#32455;&#20998;&#31867;&#30340;&#28508;&#21147;&#65292;&#32780;&#19981;&#38656;&#35201;&#37197;&#23545;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Accurate visual classification of bladder tissue during Trans-Urethral Resection of Bladder Tumor (TURBT) procedures is essential to improve early cancer diagnosis and treatment. During TURBT interventions, White Light Imaging (WLI) and Narrow Band Imaging (NBI) techniques are used for lesion detection. Each imaging technique provides diverse visual information that allows clinicians to identify and classify cancerous lesions. Computer vision methods that use both imaging techniques could improve endoscopic diagnosis. We address the challenge of tissue classification when annotations are available only in one domain, in our case WLI, and the endoscopic images correspond to an unpaired dataset, i.e. there is no exact equivalent for every image in both NBI and WLI domains. Method: We propose a semi-surprised Generative Adversarial Network (GAN)-based method composed of three main components: a teacher network trained on the labeled WLI data; a cycle-consistency GAN to perform 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;CausalEGM&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#30340;&#35299;&#32806;&#20197;&#21450;&#23558;&#28151;&#28102;&#21464;&#37327;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.05925</link><description>&lt;p&gt;
CausalEGM: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#33324;&#24615;&#22240;&#26524;&#25512;&#26029;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CausalEGM: a general causal inference framework by encoding generative modeling. (arXiv:2212.05925v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05925
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;CausalEGM&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#36827;&#34892;&#22240;&#26524;&#25928;&#24212;&#30340;&#35299;&#32806;&#20197;&#21450;&#23558;&#28151;&#28102;&#21464;&#37327;&#26144;&#23556;&#21040;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29702;&#35299;&#21644;&#34920;&#24449;&#22240;&#26524;&#25928;&#24212;&#24050;&#32463;&#25104;&#20026;&#35266;&#23519;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20294;&#24403;&#28151;&#28102;&#21464;&#37327;&#20855;&#26377;&#39640;&#32500;&#24615;&#26102;&#65292;&#36825;&#31181;&#38382;&#39064;&#24456;&#20855;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#32534;&#30721;&#29983;&#25104;&#24314;&#27169;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#36890;&#29992;&#26694;&#26550;"CausalEGM"&#65292;&#21487;&#24212;&#29992;&#20110;&#20108;&#20803;&#21644;&#36830;&#32493;&#30340;&#27835;&#30103;&#35774;&#32622;&#12290;&#22312;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#39640;&#32500;&#28151;&#28102;&#21464;&#37327;&#31354;&#38388;&#21644;&#24050;&#30693;&#23494;&#24230;&#65288;&#20363;&#22914;&#22810;&#20803;&#27491;&#24577;&#20998;&#24067;&#65289;&#30340;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#36716;&#25442;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;CausalEGM&#21516;&#26102;&#23558;&#28151;&#28102;&#21464;&#37327;&#23545;&#27835;&#30103;&#21644;&#32467;&#26524;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#35299;&#32806;&#65292;&#24182;&#26144;&#23556;&#28151;&#28102;&#21464;&#37327;&#21040;&#20302;&#32500;&#28508;&#21464;&#37327;&#31354;&#38388;&#12290;&#36890;&#36807;&#23545;&#20302;&#32500;&#28508;&#29305;&#24449;&#30340;&#35843;&#33410;&#65292;CausalEGM&#21487;&#20197;&#20272;&#35745;&#27599;&#20010;&#20010;&#20307;&#30340;&#22240;&#26524;&#25928;&#24212;&#25110;&#20154;&#32676;&#20013;&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Although understanding and characterizing causal effects have become essential in observational studies, it is challenging when the confounders are high-dimensional. In this article, we develop a general framework $\textit{CausalEGM}$ for estimating causal effects by encoding generative modeling, which can be applied in both binary and continuous treatment settings. Under the potential outcome framework with unconfoundedness, we establish a bidirectional transformation between the high-dimensional confounders space and a low-dimensional latent space where the density is known (e.g., multivariate normal distribution). Through this, CausalEGM simultaneously decouples the dependencies of confounders on both treatment and outcome and maps the confounders to the low-dimensional latent space. By conditioning on the low-dimensional latent features, CausalEGM can estimate the causal effect for each individual or the average causal effect within a population. Our theoretical analysis shows that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#21453;&#21521;&#20256;&#25773;&#21644;&#22810;&#20010;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21457;&#29616;&#24403;&#26410;&#25552;&#20379;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29983;&#29289;&#31639;&#27861;&#27604;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;</title><link>http://arxiv.org/abs/2212.04614</link><description>&lt;p&gt;
&#29983;&#29289;&#21551;&#21457;&#24335;&#23398;&#20064;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#22909;&#65311;&#29983;&#29289;&#23398;&#20064;&#19982;&#21453;&#21521;&#20256;&#25773;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is Bio-Inspired Learning Better than Backprop? Benchmarking Bio Learning vs. Backprop. (arXiv:2212.04614v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#27604;&#20102;&#21453;&#21521;&#20256;&#25773;&#21644;&#22810;&#20010;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#21457;&#29616;&#24403;&#26410;&#25552;&#20379;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29983;&#29289;&#31639;&#27861;&#27604;&#21453;&#21521;&#20256;&#25773;&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#21453;&#21521;&#20256;&#25773;&#65288;BP&#65289;&#19981;&#34987;&#35748;&#20026;&#26159;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#20197;&#26469;&#65292;&#29983;&#29289;&#21551;&#21457;&#24335;&#23398;&#20064;&#36817;&#24180;&#26469;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#23427;&#20204;&#37117;&#27604;BP&#26356;&#31526;&#21512;&#29983;&#29289;&#23398;&#21407;&#29702;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20811;&#26381;BP&#30340;&#29983;&#29289;&#23398;&#19981;&#21512;&#29702;&#24615;&#65292;&#20351;&#29992;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#24378;&#28872;&#21160;&#26426;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;BP&#21644;&#22810;&#20010;&#29983;&#29289;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#20840;&#38754;&#27604;&#36739;&#65292;&#20197;&#22238;&#31572;&#29983;&#29289;&#23398;&#20064;&#26159;&#21542;&#27604;BP&#25552;&#20379;&#39069;&#22806;&#30340;&#22909;&#22788;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35774;&#35745;&#36873;&#25321;&#26469;&#27979;&#35797;&#29983;&#29289;&#31639;&#27861;&#65292;&#22914;&#20165;&#20351;&#29992;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#12289;&#35757;&#32451;&#26102;&#36164;&#28304;&#32422;&#26463;&#12289;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31232;&#30095;&#21270;&#20197;&#21450;&#21521;&#36755;&#20837;&#26679;&#26412;&#28155;&#21152;&#22122;&#22768;&#12290;&#36890;&#36807;&#36825;&#20123;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#29983;&#29289;&#31639;&#27861;&#36229;&#36807;BP&#30340;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#22312;&#26410;&#25552;&#20379;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#26102;&#65292;&#29983;&#29289;&#31639;&#27861;&#27604;BP&#34920;&#29616;&#35201;&#22909;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bio-inspired learning has been gaining popularity recently given that Backpropagation (BP) is not considered biologically plausible. Many algorithms have been proposed in the literature which are all more biologically plausible than BP. However, apart from overcoming the biological implausibility of BP, a strong motivation for using Bio-inspired algorithms remains lacking. In this study, we undertake a holistic comparison of BP vs. multiple Bio-inspired algorithms to answer the question of whether Bio-learning offers additional benefits over BP. We test Bio-algorithms under different design choices such as access to only partial training data, resource constraints in terms of the number of training epochs, sparsification of the neural network parameters and addition of noise to input samples. Through these experiments, we notably find two key advantages of Bio-algorithms over BP. Firstly, Bio-algorithms perform much better than BP when the entire training dataset is not supplied. Four 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#36873;&#25321;&#27169;&#22411;&#65292;&#20351;&#29992;&#20856;&#22411;&#30340;&#27010;&#24565;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#37322;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#22823;&#37096;&#20998;&#28608;&#27963;&#19981;&#21516;&#27010;&#24565;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#20856;&#22411;&#37096;&#20998;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#20026;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2212.03396</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#20856;&#22411;&#37096;&#20998;&#20197;&#35299;&#37322;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling. (arXiv:2212.03396v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03396
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#36873;&#25321;&#27169;&#22411;&#65292;&#20351;&#29992;&#20856;&#22411;&#30340;&#27010;&#24565;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#37322;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#36873;&#25321;&#22823;&#37096;&#20998;&#28608;&#27963;&#19981;&#21516;&#27010;&#24565;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#20856;&#22411;&#37096;&#20998;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#20026;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#65292;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#35299;&#37322;&#26041;&#27861;&#36890;&#36807;&#23558;&#26679;&#26412;&#19982;&#21442;&#32771;&#38598;&#20013;&#30340;&#20856;&#22411;&#20195;&#34920;&#36827;&#34892;&#30456;&#20284;&#24230;&#27604;&#36739;&#65292;&#25552;&#20379;&#20102;&#27169;&#22411;&#39044;&#27979;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#22312;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#39046;&#22495;&#65292;&#21407;&#22411;&#30340;&#30456;&#20284;&#24615;&#35745;&#31639;&#36890;&#24120;&#22522;&#20110;&#32534;&#30721;&#34920;&#31034;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#36882;&#24402;&#30340;&#20989;&#25968;&#65292;&#21407;&#22411;&#35299;&#37322;&#19982;&#21407;&#22987;&#36755;&#20837;&#20043;&#38388;&#36890;&#24120;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35299;&#37322;&#36873;&#25321;&#27169;&#22411;&#65288;SESM&#65289;&#65292;&#23427;&#20351;&#29992;&#20856;&#22411;&#30340;&#27010;&#24565;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#37322;&#20854;&#33258;&#36523;&#30340;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#24605;&#24819;&#65292;&#36890;&#36807;&#36873;&#25321;&#22823;&#37096;&#20998;&#28608;&#27963;&#19981;&#21516;&#27010;&#24565;&#30340;&#23376;&#24207;&#21015;&#20316;&#20026;&#20856;&#22411;&#37096;&#20998;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#29992;&#25143;&#21487;&#20197;&#23558;&#20854;&#19982;&#36873;&#25321;&#33258;&#19981;&#21516;&#31034;&#20363;&#36755;&#20837;&#30340;&#23376;&#24207;&#21015;&#36827;&#34892;&#27604;&#36739;&#20197;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#12290;&#20026;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#65292;&#21253;&#25324;&#22810;&#26679;&#24615;&#65292;&#31283;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prototype-based interpretability methods provide intuitive explanations of model prediction by comparing samples to a reference set of memorized exemplars or typical representatives in terms of similarity. In the field of sequential data modeling, similarity calculations of prototypes are usually based on encoded representation vectors. However, due to highly recursive functions, there is usually a non-negligible disparity between the prototype-based explanations and the original input. In this work, we propose a Self-Explaining Selective Model (SESM) that uses a linear combination of prototypical concepts to explain its own predictions. The model employs the idea of case-based reasoning by selecting sub-sequences of the input that mostly activate different concepts as prototypical parts, which users can compare to sub-sequences selected from different example inputs to understand model decisions. For better interpretability, we design multiple constraints including diversity, stabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;(PAT)&#65292;&#23427;&#37319;&#29992;&#20102;&#26032;&#30340;&#22278;&#29615;&#26680;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16961</link><description>&lt;p&gt;
&#24102;&#26377;&#22278;&#29615;&#26680;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pattern Attention Transformer with Doughnut Kernel. (arXiv:2211.16961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;(PAT)&#65292;&#23427;&#37319;&#29992;&#20102;&#26032;&#30340;&#22278;&#29615;&#26680;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;Pattern Attention Transformer&#65288;PAT&#65289;&#65292;&#35813;&#20307;&#31995;&#32467;&#26500;&#30001;&#26032;&#30340;&#22278;&#29615;&#26680;&#32452;&#25104;&#12290;&#19982;NLP&#39046;&#22495;&#30340;&#26631;&#35760;&#19981;&#21516;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;Transformer&#35299;&#20915;&#20102;&#22788;&#29702;&#22270;&#20687;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;&#22312;ViT&#20013;&#65292;&#22270;&#20687;&#34987;&#20999;&#25104;&#26041;&#24418;&#30340;&#34917;&#19969;&#12290;&#20316;&#20026;ViT&#30340;&#21518;&#32493;&#65292;Swin Transformer&#25552;&#20986;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#31227;&#20301;&#27493;&#39588;&#20197;&#20943;&#23569;&#22266;&#23450;&#36793;&#30028;&#30340;&#23384;&#22312;&#65292;&#36825;&#20063;&#23548;&#33268;&#8220;&#20004;&#20010;&#36830;&#25509;&#30340;Swin Transformer&#22359;&#8221;&#25104;&#20026;&#27169;&#22411;&#30340;&#26368;&#23567;&#21333;&#20301;&#12290;&#32487;&#25215;&#20102;&#34917;&#19969;/&#31383;&#21475;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#30340;&#22278;&#29615;&#26680;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#34917;&#19969;&#30340;&#35774;&#35745;&#12290;&#23427;&#29992;&#20256;&#24863;&#22120;&#21644;&#26356;&#26032;&#20004;&#31181;&#21306;&#22495;&#20195;&#26367;&#20102;&#32447;&#22411;&#36793;&#30028;&#65292;&#36825;&#26159;&#22522;&#20110;&#33258;&#25105;&#20851;&#27880;&#30340;&#29702;&#35299;&#65288;&#31216;&#20026;QKVA&#32593;&#26684;&#65289;&#12290;&#22278;&#29615;&#26680;&#36824;&#24102;&#26469;&#20102;&#19968;&#20010;&#20851;&#20110;&#26680;&#24418;&#29366;&#30340;&#26032;&#35805;&#39064;&#65292;&#36229;&#36234;&#20102;&#26041;&#24418;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#24615;&#33021;&#65292;PAT&#34987;&#35774;&#35745;&#20026;&#30001;&#23450;&#26399;&#20843;&#36793;&#24418;&#24418;&#29366;&#30340;Transformer&#22359;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this paper a new architecture, the Pattern Attention Transformer (PAT), that is composed of the new doughnut kernel. Compared with tokens in the NLP field, Transformer in computer vision has the problem of handling the high resolution of pixels in images. In ViT, an image is cut into square-shaped patches. As the follow-up of ViT, Swin Transformer proposes an additional step of shifting to decrease the existence of fixed boundaries, which also incurs 'two connected Swin Transformer blocks' as the minimum unit of the model. Inheriting the patch/window idea, our doughnut kernel enhances the design of patches further. It replaces the line-cut boundaries with two types of areas: sensor and updating, which is based on the comprehension of self-attention (named QKVA grid). The doughnut kernel also brings a new topic about the shape of kernels beyond square. To verify its performance on image classification, PAT is designed with Transformer blocks of regular octagon shape doughn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38548;&#31163;&#24863;&#30693;&#22312;&#32447;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#34394;&#25311;&#32593;&#32476;&#29615;&#22659;&#20013;&#22810;&#20010;VN&#20849;&#23384;&#21487;&#33021;&#23548;&#33268;&#38548;&#31163;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.14158</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38548;&#31163;&#24863;&#30693;&#22312;&#32447;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Isolation-Aware Online Virtual Network Embedding via Deep Reinforcement Learning. (arXiv:2211.14158v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#38548;&#31163;&#24863;&#30693;&#22312;&#32447;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#34394;&#25311;&#32593;&#32476;&#29615;&#22659;&#20013;&#22810;&#20010;VN&#20849;&#23384;&#21487;&#33021;&#23548;&#33268;&#38548;&#31163;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21270;&#25216;&#26415;&#26159;&#29616;&#20195;ICT&#22522;&#30784;&#35774;&#26045;&#30340;&#22522;&#30784;&#65292;&#23427;&#20351;&#26381;&#21153;&#25552;&#20379;&#21830;&#33021;&#22815;&#21019;&#24314;&#25903;&#25345;&#21508;&#31181;&#26234;&#33021;&#22478;&#24066;&#24212;&#29992;&#30340;&#19987;&#29992;&#34394;&#25311;&#32593;&#32476;&#65288;VN&#65289;&#12290;&#36825;&#20123;VN&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#25968;&#25454;&#65292;&#38656;&#35201;&#20005;&#26684;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;&#34394;&#25311;&#21270;&#32593;&#32476;&#29615;&#22659;&#20013;&#65292;&#22810;&#20010;VN&#21487;&#33021;&#20849;&#23384;&#20110;&#21516;&#19968;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#19978;&#65292;&#22914;&#26524;&#38548;&#31163;&#19981;&#24403;&#65292;&#21017;&#21487;&#33021;&#30456;&#20114;&#24178;&#25200;&#25110;&#25552;&#20379;&#26410;&#32463;&#25480;&#26435;&#30340;&#35775;&#38382;&#12290;&#21069;&#32773;&#20250;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#21518;&#32773;&#20250;&#30772;&#22351;VN&#30340;&#23433;&#20840;&#24615;&#12290;&#24403;&#29305;&#23450;VN&#36829;&#21453;&#38548;&#31163;&#35201;&#27714;&#26102;&#65292;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#21830;&#30340;&#26381;&#21153;&#20445;&#38556;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#35299;&#20915;&#38548;&#31163;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#22312;&#34394;&#25311;&#32593;&#32476;&#23884;&#20837;&#65288;VNE&#65289;&#26399;&#38388;&#36827;&#34892;&#38548;&#31163;&#65292;&#21363;&#23558;VN&#20998;&#37197;&#21040;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#19978;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtualization technologies are the foundation of modern ICT infrastructure, enabling service providers to create dedicated virtual networks (VNs) that can support a wide range of smart city applications. These VNs continuously generate massive amounts of data, necessitating stringent reliability and security requirements. In virtualized network environments, however, multiple VNs may coexist on the same physical infrastructure and, if not properly isolated, may interfere with or provide unauthorized access to one another. The former causes performance degradation, while the latter compromises the security of VNs. Service assurance for infrastructure providers becomes significantly more complicated when a specific VN violates the isolation requirement.  In an effort to address the isolation issue, this paper proposes isolation during virtual network embedding (VNE), the procedure of allocating VNs onto physical infrastructure. We define a simple abstracted concept of isolation levels t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2211.05207</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26469;&#35782;&#21035;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#19979;&#30340;&#33041;&#30005;&#22270;&#22270;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Machine Learning System to Identify EEG Patterns on the Ictal-Interictal-Injury Continuum. (arXiv:2211.05207v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05207
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#21644;&#19977;&#31181;&#35299;&#37322;&#26041;&#27861;&#65292;&#36825;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#65292;&#20154;&#20204;&#21628;&#21505;&#22312;&#29992;&#20110;&#20020;&#24202;&#24037;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#22686;&#21152;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;ICU&#33041;&#30005;&#30417;&#27979;&#20013;&#24120;&#35265;&#30340;6&#31181;&#33041;&#27874;&#22270;&#26696;&#65288;&#30315;&#30187;&#12289;LPD&#12289;GPD&#12289;LRDA&#12289;GRDA&#12289;&#20854;&#20182;&#65289;&#30340;&#23384;&#22312;&#12290;&#27599;&#20010;&#39044;&#27979;&#37117;&#37197;&#26377;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#20511;&#21161;&#20110;&#19987;&#38376;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#27492;&#26032;&#22411;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#20102;&#19968;&#32452;&#21407;&#22411;&#31034;&#20363;&#65288;&#8220;&#21407;&#22411;&#8221;&#65289;&#65292;&#24182;&#36890;&#36807;&#23558;&#26032;&#30340;EEG&#29255;&#27573;&#19982;&#36825;&#20123;&#21407;&#22411;&#36827;&#34892;&#27604;&#36739;&#26469;&#20570;&#20986;&#20915;&#31574;&#12290;&#36825;&#20123;&#21407;&#22411;&#21487;&#20197;&#26159;&#21333;&#31867;&#65288;&#20165;&#19982;&#19968;&#20010;&#31867;&#30456;&#20851;&#65289;&#25110;&#21452;&#31867;&#65288;&#19982;&#20004;&#20010;&#31867;&#30456;&#20851;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;&#27169;&#22411;&#35299;&#37322;&#26041;&#27861;&#65306;1&#65289;&#20351;&#29992;&#20840;&#23616;&#32467;&#26500;&#20445;&#25345;&#26041;&#27861;&#65292;&#23558;1275&#32500;cEEG&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#20108;&#32500;&#31354;&#38388;&#20013;&#65292;&#21487;&#35270;&#21270;&#30315;&#30187;-&#38388;&#38553;-&#25439;&#20260;&#36830;&#32493;&#29366;&#24577;&#65292;&#20174;&#32780;&#28145;&#20837;&#20102;&#35299;&#20854;&#39640;&#32500;&#32467;&#26500;&#12290;2&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#20154;&#31867;&#19987;&#23478;&#33021;&#22815;&#26597;&#35810;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#20197;&#33258;&#28982;&#35821;&#35328;&#25509;&#25910;&#32463;&#36807;&#19987;&#23478;&#39564;&#35777;&#30340;&#35299;&#37322;&#12290;3&#65289;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#26576;&#20010;&#20915;&#31574;&#30340;&#36755;&#20837;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#65292;&#20801;&#35768;&#35814;&#32454;&#26816;&#26597;&#36755;&#20837;&#21644;&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35299;&#37322;&#24615;&#27169;&#22411;&#20998;&#31867;EEG&#22270;&#26696;&#21644;&#25552;&#20379;&#19987;&#23478;&#21451;&#22909;&#30340;&#35299;&#37322;&#30340;&#23454;&#29992;&#24615;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#23545;&#20110;&#24314;&#31435;AI&#30340;&#20449;&#20219;&#21644;&#20020;&#24202;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many medical subfields, there is a call for greater interpretability in the machine learning systems used for clinical work. In this paper, we design an interpretable deep learning model to predict the presence of 6 types of brainwave patterns (Seizure, LPD, GPD, LRDA, GRDA, other) commonly encountered in ICU EEG monitoring. Each prediction is accompanied by a high-quality explanation delivered with the assistance of a specialized user interface. This novel model architecture learns a set of prototypical examples (``prototypes'') and makes decisions by comparing a new EEG segment to these prototypes. These prototypes are either single-class (affiliated with only one class) or dual-class (affiliated with two classes).  We present three main ways of interpreting the model: 1) Using global-structure preserving methods, we map the 1275-dimensional cEEG latent features to a 2D space to visualize the ictal-interictal-injury continuum and gain insight into its high-dimensional structure. 2
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#36880;&#23618;&#20013;&#38388;&#34920;&#31034;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#32534;&#30721;&#22768;&#23398;&#12289;&#35821;&#38899;&#21644;&#21333;&#35789;&#32423;&#23646;&#24615;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#30456;&#20851;&#12290;&#36890;&#36807;&#27604;&#36739;&#23646;&#24615;&#36235;&#21183;&#21644;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;CCA&#36235;&#21183;&#20026;&#36873;&#25321;&#23618;&#27425;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2211.03929</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#36880;&#23618;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative layer-wise analysis of self-supervised speech models. (arXiv:2211.03929v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#36880;&#23618;&#20013;&#38388;&#34920;&#31034;&#65292;&#21457;&#29616;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#32534;&#30721;&#22768;&#23398;&#12289;&#35821;&#38899;&#21644;&#21333;&#35789;&#32423;&#23646;&#24615;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24046;&#24322;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#30456;&#20851;&#12290;&#36890;&#36807;&#27604;&#36739;&#23646;&#24615;&#36235;&#21183;&#21644;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;CCA&#36235;&#21183;&#20026;&#36873;&#25321;&#23618;&#27425;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#19981;&#21516;&#39044;&#35757;&#32451;&#30446;&#26631;&#12289;&#36755;&#20837;&#24418;&#24335;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#34987;&#25552;&#20986;&#12290;&#23613;&#31649;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#20173;&#28982;&#23545;&#36825;&#20123;&#27169;&#22411;&#32534;&#30721;&#30340;&#23646;&#24615;&#21450;&#20854;&#24046;&#24322;&#20102;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;(CCA)&#30340;&#36731;&#37327;&#32423;&#20998;&#26512;&#24037;&#20855;&#65292;&#26816;&#26597;&#20102;&#22810;&#20010;&#26368;&#36817;&#27169;&#22411;&#30340;&#20013;&#38388;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#21333;&#20010;&#23618;&#27425;&#20013;&#32534;&#30721;&#30340;&#22768;&#23398;&#12289;&#35821;&#38899;&#21644;&#21333;&#35789;&#32423;&#23646;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#23646;&#24615;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#23618;&#27425;&#28436;&#21464;&#26041;&#24335;&#19981;&#21516;&#65292;&#19988;&#21464;&#21270;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#36873;&#25321;&#30456;&#20851;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#23646;&#24615;&#36235;&#21183;&#21644;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;&#21457;&#29616;CCA&#36235;&#21183;&#20026;&#36873;&#25321;&#23618;&#27425;&#25552;&#20379;&#20102;&#21487;&#38752;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models, varying in their pre-training objective, input modality, and pre-training data, have been proposed in the last few years. Despite impressive successes on downstream tasks, we still have a limited understanding of the properties encoded by the models and the differences across models. In this work, we examine the intermediate representations for a variety of recent models. Specifically, we measure acoustic, phonetic, and word-level properties encoded in individual layers, using a lightweight analysis tool based on canonical correlation analysis (CCA). We find that these properties evolve across layers differently depending on the model, and the variations relate to the choice of pre-training objective. We further investigate the utility of our analyses for downstream tasks by comparing the property trends with performance on speech recognition and spoken language understanding tasks. We discover that CCA trends provide reliable guidance to choose laye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29992;&#20363;&#20998;&#26512;&#25506;&#35752;&#20102;&#22522;&#20110;AI&#25216;&#26415;&#30340;&#36131;&#20219;&#21046;&#24230;&#19979;&#35777;&#26126;&#36127;&#25285;&#30340;&#25361;&#25112;&#21644;&#35268;&#21017;&#25913;&#38761;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2211.01817</link><description>&lt;p&gt;
AI&#26102;&#20195;&#30340;&#36131;&#20219;&#21046;&#24230;&#65306;&#22522;&#20110;&#29992;&#20363;&#30340;&#35777;&#26126;&#36127;&#25285;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Liability regimes in the age of AI: a use-case driven analysis of the burden of proof. (arXiv:2211.01817v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29992;&#20363;&#20998;&#26512;&#25506;&#35752;&#20102;&#22522;&#20110;AI&#25216;&#26415;&#30340;&#36131;&#20219;&#21046;&#24230;&#19979;&#35777;&#26126;&#36127;&#25285;&#30340;&#25361;&#25112;&#21644;&#35268;&#21017;&#25913;&#38761;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#30340;&#26032;&#20852;&#25216;&#26415;&#26377;&#21487;&#33021;&#20026;&#25105;&#20204;&#30340;&#31038;&#20250;&#24102;&#26469;&#39072;&#35206;&#24615;&#30340;&#36716;&#22411;&#65292;&#24182;&#25512;&#36827;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#30340;&#22810;&#39033;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65289;&#26159;&#19968;&#27425;&#30495;&#27491;&#30340;&#38761;&#21629;&#12290;&#20294;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36825;&#20123;&#26041;&#27861;&#23398;&#22266;&#26377;&#30340;&#26576;&#20123;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#33021;&#23545;&#23433;&#20840;&#21644;&#22522;&#26412;&#26435;&#21033;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#12290;&#23613;&#31649;&#22312;&#37319;&#29992;&#36807;&#31243;&#20013;&#26377;&#26426;&#21046;&#26469;&#26368;&#23567;&#21270;&#36825;&#20123;&#39118;&#38505;&#65288;&#20363;&#22914;&#23433;&#20840;&#35268;&#23450;&#65289;&#65292;&#20294;&#36825;&#24182;&#19981;&#25490;&#38500;&#25439;&#23475;&#21457;&#29983;&#30340;&#21487;&#33021;&#65292;&#22914;&#26524;&#36825;&#31181;&#24773;&#20917;&#21457;&#29983;&#65292;&#21463;&#23475;&#32773;&#24212;&#35813;&#33021;&#22815;&#23547;&#27714;&#34917;&#20607;&#12290;&#22240;&#27492;&#65292;&#36131;&#20219;&#21046;&#24230;&#23558;&#22312;&#30830;&#20445;&#20351;&#29992;&#25110;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#30340;&#21463;&#23475;&#32773;&#30340;&#22522;&#26412;&#20445;&#25252;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20351;AI&#31995;&#32479;&#22266;&#26377;&#39118;&#38505;&#30340;&#30456;&#21516;&#29305;&#24449;&#65292;&#20363;&#22914;&#32570;&#20047;&#22240;&#26524;&#20851;&#31995;&#12289;&#19981;&#36879;&#26126;&#12289;&#19981;&#21487;&#39044;&#27979;&#25110;&#20182;&#20204;&#33258;&#25105;&#21644;&#33258;&#36866;&#24212;&#30340;&#26412;&#36136;&#65292;&#20063;&#20351;&#24471;&#20256;&#32479;&#30340;&#36131;&#20219;&#35268;&#21017;&#38590;&#20197;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#29992;&#20363;&#21450;&#20854;&#28508;&#22312;&#21361;&#23475;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#29992;&#20363;&#30340;&#36131;&#20219;&#35268;&#21017;&#35777;&#26126;&#36127;&#25285;&#20998;&#26512;&#65292;&#35782;&#21035;&#20102;&#24403;&#21069;&#36131;&#20219;&#35268;&#21017;&#22312;AI&#25216;&#26415;&#39046;&#22495;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25506;&#35752;&#20102;&#25913;&#38761;&#36825;&#20123;&#35268;&#21017;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
New emerging technologies powered by Artificial Intelligence (AI) have the potential to disruptively transform our societies for the better. In particular, data-driven learning approaches (i.e., Machine Learning (ML)) have been a true revolution in the advancement of multiple technologies in various application domains. But at the same time there is growing concern about certain intrinsic characteristics of these methodologies that carry potential risks to both safety and fundamental rights. Although there are mechanisms in the adoption process to minimize these risks (e.g., safety regulations), these do not exclude the possibility of harm occurring, and if this happens, victims should be able to seek compensation. Liability regimes will therefore play a key role in ensuring basic protection for victims using or interacting with these systems. However, the same characteristics that make AI systems inherently risky, such as lack of causality, opacity, unpredictability or their self and 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.00313</link><description>&lt;p&gt;
RGMIM: &#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
RGMIM: Region-Guided Masked Image Modeling for COVID-19 Detection. (arXiv:2211.00313v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;COVID-19&#26816;&#27979;&#30340;&#26032;&#39062;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#27491;&#22312;&#24555;&#36895;&#25512;&#36827;&#21307;&#23398;&#39046;&#22495;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#12290;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#25513;&#30422;&#20102;&#19968;&#32452;&#36755;&#20837;&#20687;&#32032;&#24182;&#35797;&#22270;&#39044;&#27979;&#36974;&#30422;&#30340;&#20687;&#32032;&#12290;&#20256;&#32479;&#30340;MIM&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38543;&#26426;&#25513;&#33180;&#31574;&#30053;&#12290;&#19982;&#26222;&#36890;&#22270;&#20687;&#30456;&#27604;&#65292;&#21307;&#23398;&#22270;&#20687;&#24448;&#24448;&#20855;&#26377;&#29992;&#20110;&#30142;&#30149;&#26816;&#27979;&#30340;&#23567;&#21306;&#22495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#19987;&#27880;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#33258;&#21160;COVID-19&#35782;&#21035;&#26041;&#38754;&#36827;&#34892;&#35780;&#20272;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21306;&#22495;&#24341;&#23548;&#30340;&#25513;&#33180;&#22270;&#20687;&#24314;&#27169;&#26041;&#27861;&#65288;RGMIM&#65289;&#29992;&#20110;COVID-19&#26816;&#27979;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25513;&#33180;&#31574;&#30053;&#65292;&#21033;&#29992;&#32954;&#25513;&#27169;&#20449;&#24687;&#26469;&#35782;&#21035;&#26377;&#25928;&#21306;&#22495;&#65292;&#20197;&#23398;&#20064;&#26356;&#26377;&#29992;&#30340;COVID-19&#26816;&#27979;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20116;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65288;MAE&#65292;SKD&#65292;Cross&#65292;BYOL&#21644;SimSiam&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: Self-supervised learning is rapidly advancing computer-aided diagnosis in the medical field. Masked image modeling (MIM) is one of the self-supervised learning methods that masks a subset of input pixels and attempts to predict the masked pixels. Traditional MIM methods often employ a random masking strategy. In comparison to ordinary images, medical images often have a small region of interest for disease detection. Consequently, we focus on fixing the problem in this work, which is evaluated by automatic COVID-19 identification. Methods: In this study, we propose a novel region-guided masked image modeling method (RGMIM) for COVID-19 detection in this paper. In our method, we devise a new masking strategy that employed lung mask information to identify valid regions to learn more useful information for COVID-19 detection. The proposed method was contrasted with five self-supervised learning techniques (MAE, SKD, Cross, BYOL, and, SimSiam). We present a quantitative evaluatio
&lt;/p&gt;</description></item><item><title>&#22312;&#20247;&#21253;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.16380</link><description>&lt;p&gt;
&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#19981;&#31934;&#30830;&#12289;&#20247;&#21253;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Approach for Noisy, Crowdsourced Datasets Utilizing Ensemble Modeling, Normalized Distributions of Annotations, and Entropic Measures of Uncertainty. (arXiv:2210.16380v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16380
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#21253;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#24314;&#27169;&#12289;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#21644;&#29109;&#27979;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#31934;&#30830;&#30340;&#12289;&#20247;&#21253;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20998;&#31867;&#23545;&#20110;&#26368;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35828;&#37117;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20004;&#20010;&#38382;&#39064;&#20351;&#24471;&#36825;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#26356;&#21152;&#22797;&#26434;&#65292;&#21363;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;AL-ALL&#21644;AL-PUB&#25968;&#25454;&#38598;--&#21253;&#21547;&#26469;&#33258;&#21476;&#24076;&#33098;&#32440;&#33609;&#30340;&#22270;&#20687;&#30340;&#32039;&#23494;&#35009;&#21098;&#30340;&#21333;&#20010;&#23383;&#31526;--&#21463;&#21040;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#20005;&#37325;&#24433;&#21709;&#12290;&#23558;&#38598;&#21512;&#24314;&#27169;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#26631;&#31614;&#19981;&#30830;&#23450;&#30340;&#22270;&#20687;&#65292;&#24182;&#37327;&#21270;&#36825;&#20123;&#26679;&#26412;&#30340;&#21487;&#20449;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#30001;&#20960;&#20046;&#30456;&#21516;&#30340;ResNets&#32452;&#25104;&#30340;&#22534;&#21472;&#27867;&#21270;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65306;&#19968;&#20010;&#21033;&#29992;&#31232;&#30095;&#20132;&#21449;&#29109;&#65288;CXE&#65289;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;Kullback-Liebler&#25955;&#24230;&#65288;KLD&#65289;&#12290;&#20004;&#20010;&#32593;&#32476;&#37117;&#20351;&#29992;&#20174;&#20247;&#21253;&#19968;&#33268;&#24615;&#20013;&#24471;&#20986;&#30340;&#26631;&#31614;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#32593;&#32476;&#65292;KLD&#26159;&#30456;&#23545;&#20110;&#25152;&#25552;&#20986;&#30340;&#27880;&#37322;&#30340;&#24402;&#19968;&#21270;&#20998;&#24067;&#65288;NDA&#65289;&#35745;&#31639;&#30340;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#38598;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#24212;&#29992;k-&#36817;&#37051;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing classification on noisy, crowdsourced image datasets can prove challenging even for the best neural networks. Two issues which complicate the problem on such datasets are class imbalance and ground-truth uncertainty in labeling. The AL-ALL and AL-PUB datasets -- consisting of tightly cropped, individual characters from images of ancient Greek papyri -- are strongly affected by both issues. The application of ensemble modeling to such datasets can help identify images where the ground-truth is questionable and quantify the trustworthiness of those samples. As such, we apply stacked generalization consisting of nearly identical ResNets with different loss functions: one utilizing sparse cross-entropy (CXE) and the other Kullback-Liebler Divergence (KLD). Both networks use labels drawn from the crowdsourced consensus. For the second network, the KLD is calculated with respect to the proposed Normalized Distribution of Annotations (NDA). For our ensemble model, we apply a k-near
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#31572;&#26696;&#25512;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#38382;&#39064;&#25552;&#31034;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.12353</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#31572;&#26696;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Multiple Choice Question Answering. (arXiv:2210.12353v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#31572;&#26696;&#25512;&#26029;&#65292;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#38382;&#39064;&#25552;&#31034;&#26041;&#24335;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-3&#22312;&#22810;&#39033;&#36873;&#25321;&#39064;&#31572;&#26696;&#25512;&#26029;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#33853;&#21518;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26356;&#33258;&#28982;&#30340;&#38382;&#39064;&#25552;&#31034;&#26041;&#24335;&#65292;&#21363;&#23558;&#38382;&#39064;&#21644;&#31572;&#26696;&#36873;&#39033;&#32852;&#21512;&#21576;&#29616;&#32473;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20415;&#27169;&#22411;&#33021;&#22815;&#26126;&#30830;&#27604;&#36739;&#31572;&#26696;&#36873;&#39033;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#38477;&#20302;&#26631;&#35760;&#21270;&#26041;&#26696;&#21644;&#31572;&#26696;&#36873;&#39033;&#34920;&#31034;&#23545;&#31572;&#26696;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g., "A") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that re
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2210.04470</link><description>&lt;p&gt;
&#28436;&#21592;&#35780;&#35770;&#25110;&#35780;&#35770;&#28436;&#21592;&#65311;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor-Critic or Critic-Actor? A Tale of Two Time Scales. (arXiv:2210.04470v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#65292;&#23427;&#22312;&#24555;&#36895;&#21644;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#20215;&#20540;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#35813;&#31639;&#27861;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#26684;&#30340;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30340;&#26631;&#20934;&#20844;&#24335;&#65292;&#23558;&#20854;&#35270;&#20026;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#30340;&#38543;&#26426;&#36924;&#36817;&#65292;&#20854;&#20013;&#20215;&#20540;&#20989;&#25968;&#22312;&#24555;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#65292;&#31574;&#30053;&#22312;&#24930;&#36895;&#26102;&#38388;&#23610;&#24230;&#19978;&#35745;&#31639;&#12290;&#36825;&#27169;&#25311;&#20102;&#31574;&#30053;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#26102;&#38388;&#23610;&#24230;&#30340;&#21453;&#36716;&#23454;&#38469;&#19978;&#20250;&#27169;&#25311;&#20540;&#36845;&#20195;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#21512;&#27861;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#24182;&#36890;&#36807;&#24102;&#26377;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#22120;&#30340;&#20989;&#25968;&#36924;&#36817;&#27979;&#35797;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#35780;&#35770;&#28436;&#21592;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#19982;&#28436;&#21592;&#35780;&#35770;&#31639;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit the standard formulation of tabular actor-critic algorithm as a two time-scale stochastic approximation with value function computed on a faster time-scale and policy computed on a slower time-scale. This emulates policy iteration. We begin by observing that reversal of the time scales will in fact emulate value iteration and is a legitimate algorithm. We provide a proof of convergence and compare the two empirically with and without function approximation (with both linear and nonlinear function approximators) and observe that our proposed critic-actor algorithm performs on par with actor-critic in terms of both accuracy and computational effort.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#33976;&#39311;&#20316;&#20026;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Vision Transformer&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.02871</link><description>&lt;p&gt;
&#33258;&#33976;&#39311;&#22312;Transformer&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Distillation for Further Pre-training of Transformers. (arXiv:2210.02871v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#33976;&#39311;&#20316;&#20026;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;Vision Transformer&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#39033;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#37327;&#26080;&#26631;&#27880;&#25968;&#25454;&#19978;&#23545;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#22312;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#25104;&#21151;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#23384;&#22312;&#22823;&#30340;&#25968;&#25454;&#39046;&#22495;&#19978;&#30340;&#24046;&#24322;&#65292;&#21017;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21069;&#20154;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20294;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#20165;&#20851;&#27880;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#23545;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#26102;&#65292;Vision Transformer&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#33976;&#39311;&#20316;&#20026;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#27491;&#21017;&#21270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#30446;&#26631;&#26410;&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21021;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20174;&#36827;&#19968;&#27493;&#35757;&#32451;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#21021;&#22987;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#35270;&#35273;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#33976;&#39311;&#26041;&#27861;&#22987;&#32456;&#25552;&#39640;&#20102;&#21021;&#22987;&#39044;&#35757;&#32451;Vision Transformer&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training a large transformer model on a massive amount of unlabeled data and fine-tuning it on labeled datasets for diverse downstream tasks has proven to be a successful strategy, for a variety of vision and natural language processing tasks. However, direct fine-tuning of the pre-trained model may be suboptimal if there exist large discrepancies across data domains for pre-training and fine-tuning. To tackle this issue, several previous studies have proposed further pre-training strategies, where we continue to pre-train the model on the target unlabeled dataset before fine-tuning. However, all of them solely focus on language models and we empirically find that a Vision Transformer is vulnerable to overfitting as we continue to pretrain the model on target unlabeled data. In order to tackle this limitation, we propose self-distillation as a regularization for a further pre-training stage. Specifically, we first further pre-train the initial pre-trained model on the target unlabe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13476</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;:&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels. (arXiv:2209.13476v4 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13476
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;MOAN&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#26497;&#24230;&#26377;&#38480;&#30340;&#26631;&#31614;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30740;&#31350;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#32972;&#26223;&#19979;&#20165;&#20973;&#20511;&#20960;&#20010;&#26631;&#31614;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#23454;&#20307;&#21306;&#20998;&#21644;&#19981;&#21464;&#26144;&#23556;&#19978;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#19977;&#20010;&#24120;&#35265;&#29942;&#39048;&#65306;(1)&#23614;&#37096;&#20998;&#24067;&#65306;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#36890;&#24120;&#36981;&#24490;&#38544;&#21547;&#30340;&#38271;&#23614;&#31867;&#20998;&#24067;&#12290;&#30450;&#30446;&#21033;&#29992;&#25152;&#26377;&#35757;&#32451;&#20687;&#32032;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#23548;&#33268;&#24615;&#33021;&#24694;&#21270;&#65307;(2)&#19968;&#33268;&#24615;&#65306;&#30001;&#20110;&#19981;&#21516;&#35299;&#21078;&#29305;&#24449;&#20043;&#38388;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#20998;&#21106;&#27169;&#22411;&#26159;&#21542;&#23398;&#20250;&#20102;&#26377;&#24847;&#20041;&#19988;&#19968;&#33268;&#30340;&#35299;&#21078;&#29305;&#24449;&#20173;&#19981;&#28165;&#26970;&#65307;&#20197;&#21450;(3)&#22810;&#26679;&#24615;&#65306;&#25972;&#20010;&#25968;&#25454;&#38598;&#20869;&#37096;&#20999;&#29255;&#30340;&#30456;&#20851;&#24615;&#21463;&#21040;&#30340;&#20851;&#27880;&#26174;&#33879;&#36739;&#23569;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#23547;&#25214;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#38598;&#26412;&#36523;&#30340;&#31574;&#30053;&#26041;&#27861;&#65292;&#20174;&#19981;&#21516;&#30340;&#35299;&#21078;&#35270;&#22270;&#20013;&#21457;&#29616;&#30456;&#20284;&#20294;&#19981;&#21516;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MOAN&#65292;&#29992;&#20110;&#26497;&#24230;&#26377;&#38480;&#26631;&#31614;&#19979;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12290; MOAN&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#27969;&#27700;&#32447;&#32452;&#25104;&#65306;&#21327;&#21516;&#37051;&#22495;&#25366;&#25496;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#12290;&#21069;&#32773;&#25552;&#21462;&#20849;&#21516;&#20381;&#36182;&#30340;&#20687;&#32032;&#21306;&#22495;&#65292;&#32780;&#21518;&#32773;&#24378;&#21046;&#32593;&#32476;&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35299;&#21078;&#23398;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MOAN&#22312;Dice&#35780;&#20998;&#21644;&#20854;&#20182;&#35780;&#20272;&#25351;&#26631;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we i
&lt;/p&gt;</description></item><item><title>AirTrack&#26159;&#19968;&#20010;&#29992;&#20110;&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#23454;&#26102;&#35270;&#35273;&#26816;&#27979;&#21644;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#26816;&#27979;&#21644;&#36319;&#36394;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;Amazon AOT&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#21516;&#26102;&#65292;&#22810;&#27425;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#39134;&#34892;&#27979;&#35797;&#22343;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12849</link><description>&lt;p&gt;
AirTrack&#65306;&#29992;&#20110;&#38271;&#31243;&#39134;&#26426;&#26816;&#27979;&#21644;&#36319;&#36394;&#30340;&#26426;&#36733;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AirTrack: Onboard Deep Learning Framework for Long-Range Aircraft Detection and Tracking. (arXiv:2209.12849v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12849
&lt;/p&gt;
&lt;p&gt;
AirTrack&#26159;&#19968;&#20010;&#29992;&#20110;&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#23454;&#26102;&#35270;&#35273;&#26816;&#27979;&#21644;&#36319;&#36394;&#26694;&#26550;&#65292;&#20351;&#29992;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20197;&#25552;&#39640;&#26816;&#27979;&#21644;&#36319;&#36394;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;Amazon AOT&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;&#21516;&#26102;&#65292;&#22810;&#27425;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#39134;&#34892;&#27979;&#35797;&#22343;&#26174;&#31034;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#31995;&#32479;&#23433;&#20840;&#36816;&#34892;&#38656;&#35201;&#20855;&#22791;&#26816;&#27979;&#21644;&#36991;&#20813;&#30896;&#25758;&#65288;DAA&#65289;&#33021;&#21147;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;AirTrack&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#30340;&#12289;&#20165;&#20351;&#29992;&#35270;&#35273;&#20449;&#24687;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#26694;&#26550;&#65292;&#32771;&#34385;&#20102;&#23567;&#22411;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#22823;&#23567;&#12289;&#37325;&#37327;&#21644;&#21151;&#32791;&#65288;SWaP&#65289;&#38480;&#21046;&#12290;&#37492;&#20110;&#36828;&#31243;&#39134;&#26426;&#30340;&#20302;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20840;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#23545;&#40784;&#36830;&#32493;&#22270;&#20687;&#20197;&#28040;&#38500;&#33258;&#25105;&#36816;&#21160;&#12290;&#23545;&#40784;&#21518;&#30340;&#22270;&#20687;&#22312;&#32423;&#32852;&#30340;&#20027;&#35201;&#21644;&#27425;&#35201;&#20998;&#31867;&#22120;&#20013;&#20351;&#29992;&#65292;&#20197;&#25552;&#39640;&#22810;&#20010;&#25351;&#26631;&#30340;&#26816;&#27979;&#21644;&#36319;&#36394;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AirTrack&#22312;Amazon&#31354;&#20013;&#29289;&#20307;&#36319;&#36394;&#65288;AOT&#65289;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#22810;&#27425;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#30340;&#39134;&#34892;&#27979;&#35797;&#65292;&#21253;&#25324;&#19982;&#36890;&#29992;&#33322;&#31354;&#20132;&#36890;&#20114;&#21160;&#30340;Cessna 182&#21644;Bell&#30452;&#21319;&#26426;&#21521;&#21463;&#25511;&#39134;&#34892;&#26080;&#20154;&#26426;&#38752;&#36817;&#30340;&#38468;&#36817;&#30896;&#25758;&#39134;&#34892;&#27979;&#35797;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#28385;&#36275;&#26032;&#24341;&#20837;&#30340;A&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detect-and-Avoid (DAA) capabilities are critical for safe operations of unmanned aircraft systems (UAS). This paper introduces, AirTrack, a real-time vision-only detect and tracking framework that respects the size, weight, and power (SWaP) constraints of sUAS systems. Given the low Signal-to-Noise ratios (SNR) of far away aircraft, we propose using full resolution images in a deep learning framework that aligns successive images to remove ego-motion. The aligned images are then used downstream in cascaded primary and secondary classifiers to improve detection and tracking performance on multiple metrics. We show that AirTrack outperforms state-of-the art baselines on the Amazon Airborne Object Tracking (AOT) Dataset. Multiple real world flight tests with a Cessna 182 interacting with general aviation traffic and additional near-collision flight tests with a Bell helicopter flying towards a UAS in a controlled setting showcase that the proposed approach satisfies the newly introduced A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#21333;&#30446;RGB-D&#36755;&#20837;&#19979;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;PnP&#31639;&#27861;&#24674;&#22797;SE(3)&#23039;&#24577;&#12290;&#26426;&#22120;&#20154;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25235;&#21462;&#28857;&#25552;&#26696;&#30340;&#31934;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#24182;&#20855;&#26377;&#24456;&#24378;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.08752</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#21333;&#30446;RGB-D&#36755;&#20837;&#19979;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input. (arXiv:2209.08752v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#21333;&#30446;RGB-D&#36755;&#20837;&#19979;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;PnP&#31639;&#27861;&#24674;&#22797;SE(3)&#23039;&#24577;&#12290;&#26426;&#22120;&#20154;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25235;&#21462;&#28857;&#25552;&#26696;&#30340;&#31934;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#24182;&#20855;&#26377;&#24456;&#24378;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24050;&#32463;&#22312;&#28857;&#20113;&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#28857;&#38598;&#26080;&#24207;&#24615;&#32780;&#23548;&#33268;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;RGB-D&#36755;&#20837;&#20013;&#29983;&#25104;&#25235;&#21462;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Keypoint-GraspNet&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#26816;&#27979;&#22841;&#25345;&#22120;&#20851;&#38190;&#28857;&#30340;&#25237;&#24433;&#65292;&#28982;&#21518;&#20351;&#29992;PnP&#31639;&#27861;&#24674;&#22797;SE(3)&#23039;&#24577;&#12290;&#22522;&#20110;&#19977;&#32500;&#24418;&#20307;&#21644;&#25235;&#21462;&#23478;&#26063;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#26816;&#39564;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;&#22522;&#20110;&#24230;&#37327;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25235;&#21462;&#28857;&#25552;&#26696;&#30340;&#31934;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#26426;&#22120;&#20154;&#23454;&#39564;&#23637;&#31034;&#20102;&#39640;&#25104;&#21151;&#29575;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great success has been achieved in the 6-DoF grasp learning from the point cloud input, yet the computational cost due to the point set orderlessness remains a concern. Alternatively, we explore the grasp generation from the RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects the projection of the gripper keypoints in the image space and then recover the SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive shape and the grasp family is constructed to examine our idea. Metric-based evaluation reveals that our method outperforms the baselines in terms of the grasp proposal accuracy, diversity, and the time cost. Finally, robot experiments show high success rate, demonstrating the potential of the idea in the real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#21629;&#21517;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#20351;&#29992;&#26469;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#24182;&#20998;&#37197;&#26497;&#24615;&#20998;&#25968;&#36827;&#34892;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2209.08110</link><description>&lt;p&gt;
&#22312;Twitter&#19978;&#26816;&#27979;&#21629;&#21517;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#25919;&#27835;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Detecting Political Biases of Named Entities and Hashtags on Twitter. (arXiv:2209.08110v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;Twitter&#19978;&#30340;&#21629;&#21517;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#20351;&#29992;&#26469;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#24182;&#20998;&#37197;&#26497;&#24615;&#20998;&#25968;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#30340;&#24847;&#35782;&#24418;&#24577;&#20998;&#27495;&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#21464;&#24471;&#26085;&#30410;&#31361;&#20986;&#12290;&#22240;&#27492;&#65292;&#26377;&#24456;&#22810;&#20851;&#20110;&#25919;&#27835;&#26497;&#21270;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#35768;&#22810;&#26368;&#36817;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#26816;&#27979;&#25991;&#26412;&#35821;&#26009;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#21487;&#20197;&#23581;&#35797;&#25551;&#36848;&#21644;&#21306;&#20998;&#35813;&#25991;&#26412;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#65288;&#21363;&#20805;&#24403;&#21517;&#35789;&#30340;&#21517;&#35789;&#21644;&#30701;&#35821;&#65289;&#21644;&#26631;&#31614;&#36890;&#24120;&#37117;&#25658;&#24102;&#26377;&#20851;&#25919;&#27835;&#35266;&#28857;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25968;&#25454;&#20013;&#30340;&#25919;&#27835;&#26497;&#24615;&#65292;&#24182;&#36890;&#36807;&#26126;&#30830;&#22320;&#20026;&#23454;&#20307;&#21644;&#26631;&#31614;&#20998;&#37197;&#26497;&#24615;&#20998;&#25968;&#26469;&#37327;&#21270;&#36825;&#20123;&#26497;&#24615;&#12290;&#23613;&#31649;&#36825;&#20010;&#24819;&#27861;&#24456;&#30452;&#35266;&#65292;&#20294;&#26159;&#20197;&#21487;&#20449;&#30340;&#25968;&#37327;&#26041;&#24335;&#25191;&#34892;&#36825;&#26679;&#30340;&#25512;&#26029;&#24456;&#22256;&#38590;&#12290;&#20027;&#35201;&#25361;&#25112;&#21253;&#25324;&#23454;&#20307;&#21644;&#26631;&#31614;&#30340;&#25968;&#37327;&#36739;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ideological divisions in the United States have become increasingly prominent in daily communication. Accordingly, there has been much research on political polarization, including many recent efforts that take a computational perspective. By detecting political biases in a corpus of text, one can attempt to describe and discern the polarity of that text. Intuitively, the named entities (i.e., the nouns and the phrases that act as nouns) and hashtags in text often carry information about political views. For example, people who use the term "pro-choice" are likely to be liberal, whereas people who use the term "pro-life" are likely to be conservative. In this paper, we seek to reveal political polarities in social-media text data and to quantify these polarities by explicitly assigning a polarity score to entities and hashtags. Although this idea is straightforward, it is difficult to perform such inference in a trustworthy quantitative way. Key challenges include the small number of k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#26696;&#65292;&#29992;&#20110;&#20855;&#26377;&#36755;&#20837;&#38480;&#21046;&#30340;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#65292;&#26080;&#38656;&#20808;&#39564;&#21442;&#25968;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2209.07040</link><description>&lt;p&gt;
&#24102;&#26377;&#36755;&#20837;&#38480;&#21046;&#30340;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#23398;&#20064;&#33258;&#36866;&#24212;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning-Based Adaptive Control for Stochastic Linear Systems with Input Constraints. (arXiv:2209.07040v3 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#25511;&#21046;&#26041;&#26696;&#65292;&#29992;&#20110;&#20855;&#26377;&#36755;&#20837;&#38480;&#21046;&#30340;&#38543;&#26426;&#32447;&#24615;&#31995;&#32479;&#65292;&#26080;&#38656;&#20808;&#39564;&#21442;&#25968;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#31561;&#20215;&#26041;&#26696;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#25511;&#21046;&#21463;&#21152;&#24615;&#12289;&#29420;&#31435;&#21516;&#20998;&#24067;&#39640;&#26031;&#25200;&#21160;&#21644;&#26377;&#30028;&#25511;&#21046;&#36755;&#20837;&#32422;&#26463;&#30340;&#26631;&#37327;&#32447;&#24615;&#31995;&#32479;&#65292;&#26080;&#38656;&#20808;&#30693;&#36947;&#31995;&#32479;&#21442;&#25968;&#36793;&#30028;&#25110;&#25511;&#21046;&#26041;&#21521;&#12290; &#20551;&#35774;&#31995;&#32479;&#26368;&#24046;&#24773;&#20917;&#19979;&#26159;&#36793;&#32536;&#31283;&#23450;&#30340;&#65292;&#35777;&#26126;&#20102;&#38381;&#29615;&#31995;&#32479;&#29366;&#24577;&#30340;&#24179;&#26041;&#22343;&#20540;&#26377;&#30028;&#24615;&#12290;&#26368;&#21518;&#65292;&#25552;&#20379;&#20102;&#25968;&#20540;&#20363;&#23376;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a certainty-equivalence scheme for adaptive control of scalar linear systems subject to additive, i.i.d. Gaussian disturbances and bounded control input constraints, without requiring prior knowledge of the bounds of the system parameters, nor the control direction. Assuming that the system is at-worst marginally stable, mean square boundedness of the closed-loop system states is proven. Lastly, numerical examples are presented to illustrate our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Treeformer&#65292;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#20998;&#23618;&#23548;&#33322;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35745;&#31639;&#27880;&#24847;&#21147;&#12290;&#19982;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#27861;&#30456;&#27604;&#65292;Treeformer &#21487;&#20197;&#23558;&#26816;&#32034;&#25104;&#26412;&#20174;&#32447;&#24615;&#38477;&#20026;&#36817;&#20284;&#23545;&#25968;&#32423;&#21035;&#65292;&#24182;&#25552;&#20379;&#20004;&#31181;&#26377;&#25928;&#30340;&#20851;&#27880;&#23618;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#36807;&#38271;&#30340;&#24212;&#29992;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2208.09015</link><description>&lt;p&gt;
Treeformer: &#31264;&#23494;&#26799;&#24230;&#26641;&#23454;&#29616;&#39640;&#25928;&#27880;&#24847;&#21147;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Treeformer: Dense Gradient Trees for Efficient Attention Computation. (arXiv:2208.09015v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Treeformer&#65292;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#20998;&#23618;&#23548;&#33322;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#35745;&#31639;&#27880;&#24847;&#21147;&#12290;&#19982;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#27861;&#30456;&#27604;&#65292;Treeformer &#21487;&#20197;&#23558;&#26816;&#32034;&#25104;&#26412;&#20174;&#32447;&#24615;&#38477;&#20026;&#36817;&#20284;&#23545;&#25968;&#32423;&#21035;&#65292;&#24182;&#25552;&#20379;&#20004;&#31181;&#26377;&#25928;&#30340;&#20851;&#27880;&#23618;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#22788;&#29702;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#36807;&#38271;&#30340;&#24212;&#29992;&#65292;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#25512;&#29702;&#21644;&#35757;&#32451;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#25104;&#20108;&#27425;&#20851;&#31995;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#22914;&#32593;&#39029;&#32763;&#35793;&#21644;&#26597;&#35810;-&#22238;&#31572;&#31561;&#36895;&#24230;&#35201;&#27714;&#36739;&#39640;&#30340;&#24212;&#29992;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#21152;&#36895;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#36817;&#26399;&#25552;&#20986;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#22914;&#24378;&#21046;&#20351;&#29992;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#32467;&#26500;&#65292;&#22914;&#31232;&#30095;&#12289;&#20302;&#31209;&#12289;&#20351;&#29992;&#26680;&#20989;&#25968;&#36924;&#36817;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#27880;&#24847;&#21147;&#35745;&#31639;&#30475;&#20316;&#26368;&#36817;&#37051;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#20998;&#23618;&#23548;&#33322;&#65292;&#23558;&#27599;&#20010;&#26597;&#35810;&#26631;&#35760;&#30340;&#26816;&#32034;&#25104;&#26412;&#20174;&#32447;&#24615;&#38477;&#20026;&#36817;&#20284;&#23545;&#25968;&#32423;&#21035;&#12290;&#22522;&#20110;&#36825;&#31181;&#20998;&#23618;&#23548;&#33322;&#65292;&#25105;&#20204;&#35774;&#35745;&#20986;Treeformer&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#20004;&#31181;&#26377;&#25928;&#30340;&#20851;&#27880;&#23618;-- TF-Attention&#21644;TC-Attention&#12290;TF-Attention&#20197;&#32454;&#31890;&#24230;&#26041;&#24335;&#35745;&#31639;&#20854;&#20013;&#30340;&#20851;&#27880;&#65292;&#32780;TC-Attention&#26159;&#19968;&#31181;&#26356;&#31895;&#31890;&#24230;&#30340;&#20851;&#27880;&#23618;&#65292;&#20063;&#30830;&#20445;&#26799;&#24230;&#26159;&#8220;&#23494;&#38598;&#8221;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are "dense". To optimize such challe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#20174;&#19968;&#32452;&#38750;&#31169;&#26377;&#22238;&#24402;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#39640;Tukey&#28145;&#24230;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38656;&#25968;&#25454;&#36793;&#30028;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25968;&#25454;&#20016;&#23500;&#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2208.07353</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Easy Differentially Private Linear Regression. (arXiv:2208.07353v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#22238;&#24402;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#20174;&#19968;&#32452;&#38750;&#31169;&#26377;&#22238;&#24402;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#39640;Tukey&#28145;&#24230;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#26080;&#38656;&#25968;&#25454;&#36793;&#30028;&#21644;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25968;&#25454;&#20016;&#23500;&#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#22238;&#24402;&#26159;&#32479;&#35745;&#20998;&#26512;&#30340;&#22522;&#30784;&#24037;&#20855;&#12290;&#36825;&#20419;&#20351;&#24320;&#21457;&#20986;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#30340;&#32447;&#24615;&#22238;&#24402;&#26041;&#27861;&#65292;&#20174;&#32780;&#20445;&#35777;&#23398;&#20064;&#30340;&#27169;&#22411;&#27844;&#38706;&#30340;&#20010;&#20154;&#25968;&#25454;&#20449;&#24687;&#24456;&#23569;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#35299;&#20915;&#26041;&#26696;&#20551;&#23450;&#26368;&#32456;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#25351;&#23450;&#22909;&#30340;&#25968;&#25454;&#36793;&#30028;&#21644;&#36229;&#21442;&#25968;&#65292;&#20294;&#20004;&#32773;&#37117;&#23384;&#22312;&#23454;&#38469;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#25351;&#25968;&#26426;&#21046;&#20174;&#19968;&#32452;&#38750;&#31169;&#26377;&#22238;&#24402;&#27169;&#22411;&#20013;&#36873;&#25321;&#20855;&#26377;&#39640; Tukey &#28145;&#24230;&#30340;&#27169;&#22411;&#12290;&#32473;&#23450;&#29992;&#20110;&#35757;&#32451; $m$ &#20010;&#27169;&#22411;&#30340; $n$ &#20010; $d$ &#32500;&#25968;&#25454;&#26679;&#26412;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#30340; Tukey &#28145;&#24230;&#26500;&#36896;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31867;&#27604;&#65292;&#23427;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026; $O(d^2n + dm\log(m))$&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#31639;&#27861;&#22312;&#26080;&#25968;&#25454;&#36793;&#30028;&#25110;&#36229;&#21442;&#25968;&#36873;&#25321;&#35201;&#27714;&#30340;&#25968;&#25454;&#20016;&#23500;&#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#24378;&#22823;&#30340;&#23454;&#35777;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear regression is a fundamental tool for statistical analysis. This has motivated the development of linear regression methods that also satisfy differential privacy and thus guarantee that the learned model reveals little about any one data point used to construct it. However, existing differentially private solutions assume that the end user can easily specify good data bounds and hyperparameters. Both present significant practical obstacles. In this paper, we study an algorithm which uses the exponential mechanism to select a model with high Tukey depth from a collection of non-private regression models. Given $n$ samples of $d$-dimensional data used to train $m$ models, we construct an efficient analogue using an approximate Tukey depth that runs in time $O(d^2n + dm\log(m))$. We find that this algorithm obtains strong empirical performance in the data-rich setting with no data bounds or hyperparameter selection required.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#28176;&#36827;&#30340;AMP&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#39640;&#32500;&#32479;&#35745;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;AMP&#29702;&#35770;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;AMP&#22312;&#29420;&#31435;&#21021;&#22987;&#21270;&#21644;&#35889;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2208.03313</link><description>&lt;p&gt;
&#19981;&#23545;&#31216;&#27169;&#22411;&#20013;&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#30340;&#38750;&#28176;&#36817;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Non-Asymptotic Framework for Approximate Message Passing in Spiked Models. (arXiv:2208.03313v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#28176;&#36827;&#30340;AMP&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#39640;&#32500;&#32479;&#35745;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;AMP&#29702;&#35770;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#39044;&#27979;AMP&#22312;&#29420;&#31435;&#21021;&#22987;&#21270;&#21644;&#35889;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;&#30340;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#20449;&#24687;&#20256;&#36882;&#65288;AMP&#65289;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#36845;&#20195;&#33539;&#24335;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39640;&#32500;&#32479;&#35745;&#38382;&#39064;&#30340;&#27714;&#35299;&#20013;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;AMP&#29702;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#39640;&#32500;&#28176;&#36817;&#24615;&#26041;&#38754;&#65292;&#26410;&#33021;&#39044;&#27979;&#24403;&#36845;&#20195;&#27425;&#25968;&#36229;&#36807;$o\big(\frac{\log n}{\log\log n}\big)$&#65288;&#20854;&#20013;$n$&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65289;&#26102;AMP&#30340;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#38181;&#24418;&#30697;&#38453;&#20272;&#35745;&#20013;&#30340;AMP&#12290;&#25105;&#20204;&#22522;&#20110;&#26032;&#30340;AMP&#26356;&#26032;&#20998;&#35299;&#21644;&#21487;&#25511;&#27531;&#24046;&#39033;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#34920;&#24449;&#29420;&#31435;&#21021;&#22987;&#21270;&#24773;&#20917;&#19979;AMP&#30340;&#26377;&#38480;&#26679;&#26412;&#34892;&#20026;&#65292;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#21040;&#21253;&#25324;&#35889;&#21021;&#22987;&#21270;&#30340;&#24773;&#20917;&#12290;&#20316;&#20026;&#36825;&#31181;&#20998;&#26512;&#26041;&#27861;&#30340;&#20004;&#20010;&#20855;&#20307;&#24212;&#29992;&#65292;&#24403;&#35299;&#20915;$\mathbb{Z}_2$&#21516;&#27493;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#39044;&#27979;&#20102;&#35889;&#21021;&#22987;&#21270;AMP&#30340;&#34892;&#20026;&#65292;&#26368;&#22810;&#21487;&#20197;&#36827;&#34892;$O\big(\frac{n}{\mathrm{poly}\log n}\big)$&#20010;&#36845;&#20195;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate message passing (AMP) emerges as an effective iterative paradigm for solving high-dimensional statistical problems. However, prior AMP theory -which focused mostly on high-dimensional asymptotics -- fell short of predicting the AMP dynamics when the number of iterations surpasses $o\big(\frac{\log n}{\log\log n}\big)$ (with $n$ the problem dimension). To address this inadequacy, this paper develops a non-asymptotic framework for understanding AMP in spiked matrix estimation. Built upon new decomposition of AMP updates and controllable residual terms, we lay out an analysis recipe to characterize the finite-sample behavior of AMP in the presence of an independent initialization, which is further generalized to allow for spectral initialization. As two concrete consequences of the proposed analysis recipe: (i) when solving $\mathbb{Z}_2$ synchronization, we predict the behavior of spectrally initialized AMP for up to $O\big(\frac{n}{\mathrm{poly}\log n}\big)$ iterations, sh
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65306;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#65288;Weakly Supervised Detection Transformer&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#25968;&#30334;&#31181;&#26032;&#22411;&#29289;&#21697;&#30340;WSOD&#24494;&#35843;&#20013;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2207.05205</link><description>&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#25193;&#23637;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scaling Novel Object Detection with Weakly Supervised Detection Transformers. (arXiv:2207.05205v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#65306;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#65288;Weakly Supervised Detection Transformer&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#36716;&#31227;&#33267;&#25968;&#30334;&#31181;&#26032;&#22411;&#29289;&#21697;&#30340;WSOD&#24494;&#35843;&#20013;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#22810;&#23454;&#20363;&#23398;&#20064;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#22823;&#35268;&#27169;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#24494;&#35843;&#29616;&#26377;&#27169;&#22411;&#20197;&#20415;&#26816;&#27979;&#26032;&#22411;&#29289;&#21697;&#65292;&#20294;&#26631;&#27880;&#36793;&#30028;&#26694;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#24369;&#30417;&#30563;&#30446;&#26631;&#26816;&#27979;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#26469;&#35757;&#32451;&#29289;&#21697;&#26816;&#27979;&#22120;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24369;&#30417;&#30563;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#23567;&#35268;&#27169;&#25968;&#25454;&#65292;&#24182;&#38656;&#35201;&#22810;&#27425;&#35757;&#32451;&#21644;&#25913;&#36827;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24369;&#30417;&#30563;&#26816;&#27979;&#21464;&#24418;&#22120;&#65292;&#23427;&#33021;&#22815;&#23558;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#36716;&#31227;&#33267;&#25968;&#30334;&#31181;&#26032;&#22411;&#29289;&#21697;&#30340;WSOD&#24494;&#35843;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#39044;&#35757;&#32451;&#30693;&#35782;&#26469;&#25913;&#36827;&#22312;WSOD&#26041;&#27861;&#20013;&#24120;&#29992;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;MIL&#65289;&#26694;&#26550;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#26032;&#22411;&#29289;&#21697;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#25193;&#23637;&#30740;&#31350;&#8230;
&lt;/p&gt;
&lt;p&gt;
A critical object detection task is finetuning an existing model to detect novel objects, but the standard workflow requires bounding box annotations which are time-consuming and expensive to collect. Weakly supervised object detection (WSOD) offers an appealing alternative, where object detectors can be trained using image-level labels. However, the practical application of current WSOD models is limited, as they only operate at small data scales and require multiple rounds of training and refinement. To address this, we propose the Weakly Supervised Detection Transformer, which enables efficient knowledge transfer from a large-scale pretraining dataset to WSOD finetuning on hundreds of novel objects. Additionally, we leverage pretrained knowledge to improve the multiple instance learning (MIL) framework often used in WSOD methods. Our experiments show that our approach outperforms previous state-of-the-art models on large-scale novel object detection datasets, and our scaling study r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23545;&#31867;&#22686;&#37327;&#23398;&#20064;&#36827;&#34892;&#23458;&#35266;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#39564;&#20998;&#26512;&#20102;CIL&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26368;&#26032;&#31639;&#27861;&#20248;&#20808;&#32771;&#34385;&#39640;&#31283;&#23450;&#24615;&#65292;&#20294;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.08101</link><description>&lt;p&gt;
&#20174;&#34920;&#31034;&#23398;&#20064;&#30340;&#35282;&#24230;&#25506;&#32034;&#26356;&#23458;&#35266;&#30340;&#35780;&#20215;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards More Objective Evaluation of Class Incremental Learning: Representation Learning Perspective. (arXiv:2206.08101v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23545;&#31867;&#22686;&#37327;&#23398;&#20064;&#36827;&#34892;&#23458;&#35266;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#39564;&#20998;&#26512;&#20102;CIL&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26368;&#26032;&#31639;&#27861;&#20248;&#20808;&#32771;&#34385;&#39640;&#31283;&#23450;&#24615;&#65292;&#20294;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26159;&#25351;&#22312;&#19981;&#24536;&#35760;&#24050;&#32463;&#23398;&#20064;&#30340;&#31867;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#26029;&#22320;&#20174;&#22686;&#37327;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#30340;&#23545;&#35937;&#31867;&#21035;&#30340;&#36807;&#31243;&#12290;&#34429;&#28982;&#35780;&#20272;CIL&#31639;&#27861;&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22522;&#20110;&#25152;&#26377;&#24050;&#23398;&#20064;&#31867;&#21035;&#30340;&#24179;&#22343;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#26368;&#22823;&#21270;&#20934;&#30830;&#29575;&#24182;&#19981;&#19968;&#23450;&#33021;&#23548;&#33268;&#26377;&#25928;&#30340;CIL&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#20351;&#29992;&#21508;&#31181;&#35780;&#20272;&#21327;&#35758;&#23454;&#39564;&#20998;&#26512;CIL&#31639;&#27861;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#20248;&#20808;&#32771;&#34385;&#39640;&#31283;&#23450;&#24615;&#65292;&#19988;&#27809;&#26377;&#26174;&#33879;&#25913;&#21464;&#23398;&#20064;&#30340;&#34920;&#31034;&#65292;&#26377;&#26102;&#29978;&#33267;&#23398;&#20064;&#20102;&#27604;&#26420;&#32032;&#22522;&#32447;&#26356;&#24046;&#30340;&#34920;&#31034;&#12290;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#39640;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#23398;&#20064;&#20102;&#26356;&#25509;&#36817;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#31532;&#19968;&#20010;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#22522;&#27169;&#22411;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is the process of continually learning new object classes from incremental data while not forgetting past learned classes. While the common method for evaluating CIL algorithms is based on average test accuracy for all learned classes, we argue that maximizing accuracy alone does not necessarily lead to effective CIL algorithms. In this paper, we experimentally analyze neural network models trained by CIL algorithms using various evaluation protocols in representation learning and propose a new analysis method. Our experiments show that most state-of-the-art algorithms prioritize high stability and do not significantly change the learned representation, and sometimes even learn a representation of lower quality than a naive baseline. However, we observe that these algorithms can still achieve high test accuracy because they learn a classifier that is closer to the optimal classifier. We also found that the base model learned in the first task varies in 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#37319;&#29992;&#26799;&#24230;&#36319;&#36394;&#21644;&#32593;&#32476;&#32423;&#26041;&#24046;&#20943;&#23569;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#27599;&#20010;&#33410;&#28857;&#36319;&#36394;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#23616;&#37096;&#30446;&#26631;&#20989;&#25968;&#24378;&#20984;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20943;&#23569;&#39044;&#26399;&#28151;&#21512;&#30697;&#38453;&#30340;&#28201;&#21644;&#36830;&#25509;&#26465;&#20214;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2206.04113</link><description>&lt;p&gt;
&#35774;&#22791;&#37319;&#26679;&#19979;&#30340;&#25512;&#25289;&#24335;&#20998;&#24067;&#24335;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Push--Pull with Device Sampling. (arXiv:2206.04113v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04113
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#37319;&#29992;&#26799;&#24230;&#36319;&#36394;&#21644;&#32593;&#32476;&#32423;&#26041;&#24046;&#20943;&#23569;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#23454;&#29616;&#27599;&#20010;&#33410;&#28857;&#36319;&#36394;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#23616;&#37096;&#30446;&#26631;&#20989;&#25968;&#24378;&#20984;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20943;&#23569;&#39044;&#26399;&#28151;&#21512;&#30697;&#38453;&#30340;&#28201;&#21644;&#36830;&#25509;&#26465;&#20214;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20123;&#20195;&#29702;&#20197;&#20132;&#25442;&#24182;&#36890;&#36807;&#22522;&#30784;&#36890;&#20449;&#22270;&#20943;&#23569;&#20854;&#26412;&#22320;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#24049;&#25918;&#22312;&#24322;&#27493;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;&#27599;&#27425;&#36845;&#20195;&#21482;&#26377;&#38543;&#26426;&#37096;&#20998;&#33410;&#28857;&#25191;&#34892;&#35745;&#31639;&#65292;&#32780;&#20449;&#24687;&#20132;&#25442;&#21487;&#20197;&#22312;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#20197;&#38750;&#23545;&#31216;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#23558;&#26799;&#24230;&#36319;&#36394;&#19982;&#32593;&#32476;&#32423;&#21035;&#30340;&#26041;&#24046;&#20943;&#23569;&#30456;&#32467;&#21512;&#65288;&#19982;&#27599;&#20010;&#33410;&#28857;&#20869;&#30340;&#26041;&#24046;&#20943;&#23569;&#30456;&#23545;&#27604;&#65289;&#12290;&#36825;&#20351;&#27599;&#20010;&#33410;&#28857;&#33021;&#22815;&#36319;&#36394;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#24403;&#23616;&#37096;&#30446;&#26631;&#20989;&#25968;&#24378;&#20984;&#26102;&#65292;&#35813;&#31639;&#27861;&#32447;&#24615;&#25910;&#25947;&#65292;&#22312;&#39044;&#26399;&#28151;&#21512;&#30697;&#38453;&#30340;&#28201;&#21644;&#36830;&#25509;&#26465;&#20214;&#19979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#35201;&#27714;&#28151;&#21512;&#30697;&#38453;&#20855;&#26377;&#21452;&#37325;&#38543;&#26426;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;...
&lt;/p&gt;
&lt;p&gt;
We consider decentralized optimization problems in which a number of agents collaborate to minimize the average of their local functions by exchanging over an underlying communication graph. Specifically, we place ourselves in an asynchronous model where only a random portion of nodes perform computation at each iteration, while the information exchange can be conducted between all the nodes and in an asymmetric fashion. For this setting, we propose an algorithm that combines gradient tracking with a network-level variance reduction (in contrast to variance reduction within each node). This enables each node to track the average of the gradients of the objective functions. Our theoretical analysis shows that the algorithm converges linearly, when the local objective functions are strongly convex, under mild connectivity conditions on the expected mixing matrices. In particular, our result does not require the mixing matrices to be doubly stochastic. In the experiments, we investigate a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#35299;&#31354;&#38388;&#20960;&#20309;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#23558;&#27714;&#35299;&#22120;&#35270;&#20026;&#36127;&#24658;&#31561;&#21464;&#25442;&#65292;&#20197;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#23547;&#25214;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36991;&#20813;&#20351;&#29992;&#36890;&#36807;&#25237;&#24433;&#24179;&#28369;&#27714;&#35299;&#22120;&#12289;&#23558;&#27714;&#35299;&#22120;&#26494;&#24347;&#20026;&#36830;&#32493;&#38382;&#39064;&#25110;&#32773;&#36890;&#36807;&#25216;&#26415;&#23545;&#25439;&#22833;&#38754;&#35980;&#36827;&#34892;&#25554;&#20540;&#31561;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#31163;&#25955;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#22270;&#31561;&#24773;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.15213</link><description>&lt;p&gt;
&#32452;&#21512;&#31639;&#27861;&#30340;&#21453;&#21521;&#20256;&#25773;&#65306;&#20351;&#29992;&#25237;&#24433;&#36827;&#34892;&#30340;&#24658;&#31561;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through Combinatorial Algorithms: Identity with Projection Works. (arXiv:2205.15213v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#35299;&#31354;&#38388;&#20960;&#20309;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#23558;&#27714;&#35299;&#22120;&#35270;&#20026;&#36127;&#24658;&#31561;&#21464;&#25442;&#65292;&#20197;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#23547;&#25214;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36991;&#20813;&#20351;&#29992;&#36890;&#36807;&#25237;&#24433;&#24179;&#28369;&#27714;&#35299;&#22120;&#12289;&#23558;&#27714;&#35299;&#22120;&#26494;&#24347;&#20026;&#36830;&#32493;&#38382;&#39064;&#25110;&#32773;&#36890;&#36807;&#25216;&#26415;&#23545;&#25439;&#22833;&#38754;&#35980;&#36827;&#34892;&#25554;&#20540;&#31561;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;&#31163;&#25955;&#27714;&#35299;&#22120;&#21644;&#28145;&#24230;&#22270;&#31561;&#24773;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31163;&#25955;&#27714;&#35299;&#22120;&#23884;&#20837;&#21487;&#24494;&#20998;&#23618;&#20013;&#65292;&#36171;&#20104;&#20102;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#32452;&#21512;&#34920;&#36798;&#33021;&#21147;&#21644;&#31163;&#25955;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#27714;&#35299;&#22120;&#30340;&#23548;&#25968;&#20026;&#38646;&#25110;&#26410;&#23450;&#20041;&#65292;&#22240;&#27492;&#23547;&#25214;&#26377;&#24847;&#20041;&#30340;&#26367;&#20195;&#26041;&#26696;&#23545;&#20110;&#26377;&#25928;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#20316;&#21697;&#20381;&#36182;&#20110;&#20351;&#29992;&#36755;&#20837;&#25200;&#21160;&#24179;&#28369;&#27714;&#35299;&#22120;&#12289;&#23558;&#27714;&#35299;&#22120;&#26494;&#24347;&#20026;&#36830;&#32493;&#38382;&#39064;&#25110;&#32773;&#36890;&#36807;&#25216;&#26415;&#23545;&#25439;&#22833;&#38754;&#35980;&#36827;&#34892;&#25554;&#20540;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#27714;&#35299;&#22120;&#35843;&#29992;&#12289;&#24341;&#20837;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#25110;&#32773;&#29306;&#29298;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#25955;&#35299;&#31354;&#38388;&#20960;&#20309;&#30340;&#21407;&#21017;&#26041;&#27861;&#65292;&#20197;&#22312;&#21453;&#21521;&#20256;&#25773;&#26102;&#23558;&#27714;&#35299;&#22120;&#35270;&#20026;&#36127;&#24658;&#31561;&#21464;&#25442;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#35777;&#26126;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#12289;&#26080;&#36229;&#21442;&#25968;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35768;&#22810;&#23454;&#39564;&#20013;&#19982;&#20043;&#21069;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#31454;&#20105;&#65292;&#20363;&#22914;&#22312;&#31163;&#25955;&#37319;&#26679;&#22120;&#12289;&#28145;&#24230;&#22270;&#20013;&#36827;&#34892;&#30340;&#21453;&#21521;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding discrete solvers as differentiable layers has given modern deep learning architectures combinatorial expressivity and discrete reasoning capabilities. The derivative of these solvers is zero or undefined, therefore a meaningful replacement is crucial for effective gradient-based learning. Prior works rely on smoothing the solver with input perturbations, relaxing the solver to continuous problems, or interpolating the loss landscape with techniques that typically require additional solver calls, introduce extra hyper-parameters, or compromise performance. We propose a principled approach to exploit the geometry of the discrete solution space to treat the solver as a negative identity on the backward pass and further provide a theoretical justification. Our experiments demonstrate that such a straightforward hyper-parameter-free approach is able to compete with previous more complex methods on numerous experiments such as backpropagation through discrete samplers, deep graph m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#23637;&#24320;&#22810;&#23610;&#24230;&#32593;&#32476;&#32467;&#26500;&#29992;&#20110;&#21152;&#36895;MRI&#37325;&#24314;&#65292;&#35813;&#32467;&#26500;&#23558;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#22823;&#22359;&#22270;&#20687;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.08213</link><description>&lt;p&gt;
HUMUS-Net: &#28151;&#21512;&#23637;&#24320;&#22810;&#23610;&#24230;&#32593;&#32476;&#32467;&#26500;&#29992;&#20110;&#21152;&#36895;MRI&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction. (arXiv:2203.08213v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#23637;&#24320;&#22810;&#23610;&#24230;&#32593;&#32476;&#32467;&#26500;&#29992;&#20110;&#21152;&#36895;MRI&#37325;&#24314;&#65292;&#35813;&#32467;&#26500;&#23558;&#21464;&#21387;&#22120;&#21644;&#21367;&#31215;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22788;&#29702;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#22823;&#22359;&#22270;&#20687;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21152;&#36895;MRI&#37325;&#24314;&#20013;&#65292;&#38656;&#35201;&#20174;&#19968;&#32452;&#27424;&#37319;&#26679;&#21644;&#22122;&#22768;&#27979;&#37327;&#20013;&#24674;&#22797;&#24739;&#32773;&#30340;&#35299;&#21078;&#32467;&#26500;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25104;&#21151;&#35299;&#20915;&#36825;&#20010;&#36870;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#20135;&#29983;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26550;&#26500;&#20005;&#37325;&#20381;&#36182;&#20110;&#21367;&#31215;&#65292;&#36825;&#20123;&#21367;&#31215;&#26159;&#20869;&#23481;&#26080;&#20851;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#27169;&#25311;&#22270;&#20687;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;&#21464;&#21387;&#22120;&#20316;&#20026;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26680;&#24515;&#65292;&#24050;&#32463;&#25104;&#20026;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#30340;&#24378;&#22823;&#26500;&#24314;&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22411;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#21106;&#20026;&#38750;&#37325;&#21472;&#30340;&#22359;&#65292;&#24182;&#23558;&#22359;&#23884;&#20837;&#21040;&#36739;&#20302;&#32500;&#30340;&#20196;&#29260;&#20013;&#65292;&#21033;&#29992;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#19981;&#21463;&#21367;&#31215;&#26550;&#26500;&#30340;&#21069;&#36848;&#32570;&#28857;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#36755;&#20837;&#22270;&#20687;&#20998;&#36776;&#29575;&#36739;&#39640;&#19988;&#38656;&#35201;&#23558;&#22270;&#20687;&#20998;&#21106;&#20026;&#36739;&#22823;&#30340;&#22359;&#26102;&#65292;&#21464;&#21387;&#22120;&#20250;&#20135;&#29983;&#26497;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a lar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;Transformer&#27169;&#22359;&#32593;&#32476;&#65288;TMN&#65289;&#65292;&#23427;&#26159;&#30001;Transformer&#27169;&#22359;&#32452;&#25104;&#30340;&#26032;&#22411;NMN&#65292;&#33021;&#22815;&#21462;&#24471;&#22312;VQA&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#24615;&#33021;&#65292;&#38024;&#23545;&#23376;&#20219;&#21153;&#30340;&#26032;&#32452;&#21512;&#27604;&#26631;&#20934;Transformer&#25552;&#39640;&#20102;30%&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2201.11316</link><description>&lt;p&gt;
Transformer&#27169;&#22359;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#31995;&#32479;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Transformer Module Networks for Systematic Generalization in Visual Question Answering. (arXiv:2201.11316v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.11316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Transformer&#27169;&#22359;&#32593;&#32476;&#65288;TMN&#65289;&#65292;&#23427;&#26159;&#30001;Transformer&#27169;&#22359;&#32452;&#25104;&#30340;&#26032;&#22411;NMN&#65292;&#33021;&#22815;&#21462;&#24471;&#22312;VQA&#20219;&#21153;&#20013;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#24615;&#33021;&#65292;&#38024;&#23545;&#23376;&#20219;&#21153;&#30340;&#26032;&#32452;&#21512;&#27604;&#26631;&#20934;Transformer&#25552;&#39640;&#20102;30%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#31995;&#32479;&#21270;&#27867;&#21270;&#33021;&#21147;&#65292;&#21363;&#22788;&#29702;&#24050;&#30693;&#27010;&#24565;&#30340;&#26032;&#32452;&#21512;&#65292;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21457;&#29616;&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;&#65288;NMN&#65289;&#21487;&#20197;&#21462;&#24471;&#26356;&#22909;&#25110;&#19982;&#20256;&#32479;&#30340;Transformer&#30456;&#20284;&#30340;&#31995;&#32479;&#21270;&#27867;&#21270;&#24615;&#33021;&#65292;&#21363;&#20351;NMN&#30340;&#27169;&#22359;&#26159;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;Transformer&#22312;NMNs&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22359;&#21270;&#22914;&#20309;&#20026;Transformer&#24102;&#26469;&#30410;&#22788;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;NMN&#65292;&#21363;Transformer&#27169;&#22359;&#32593;&#32476;&#65288;TMN&#65289;&#26159;&#30001;Transformer&#27169;&#22359;&#32452;&#25104;&#30340;&#32452;&#21512;&#12290;&#22312;&#19977;&#20010;VQA&#25968;&#25454;&#38598;&#20013;&#65292;TMN&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#27867;&#21270;&#24615;&#33021;&#65292;&#38024;&#23545;&#23376;&#20219;&#21153;&#30340;&#26032;&#32452;&#21512;&#27604;&#26631;&#20934;Transformer&#25552;&#39640;&#20102;30%&#20197;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#20165;&#27169;&#22359;&#32452;&#21512;&#32780;&#19988;&#27599;&#20010;&#32452;&#20214;&#30340;&#19987;&#19994;&#21270;&#37117;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers achieve great performance on Visual Question Answering (VQA). However, their systematic generalization capabilities, i.e., handling novel combinations of known concepts, is unclear. We reveal that Neural Module Networks (NMNs), i.e., question-specific compositions of modules that tackle a sub-task, achieve better or similar systematic generalization performance than the conventional Transformers, even though NMNs' modules are CNN-based. In order to address this shortcoming of Transformers with respect to NMNs, in this paper we investigate whether and how modularity can bring benefits to Transformers. Namely, we introduce Transformer Module Network (TMN), a novel NMN based on compositions of Transformer modules. TMNs achieve state-of-the-art systematic generalization performance in three VQA datasets, improving more than 30% over standard Transformers for novel compositions of sub-tasks. We show that not only the module composition but also the module specialization for eac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;GraphPart&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#27880;&#37322;&#39044;&#31639;&#32422;&#26463;&#19979;&#24615;&#33021;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2201.09391</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#21306;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Partition-Based Active Learning for Graph Neural Networks. (arXiv:2201.09391v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#21306;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;GraphPart&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#27880;&#37322;&#39044;&#31639;&#32422;&#26463;&#19979;&#24615;&#33021;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20027;&#21160;&#23398;&#20064;&#35774;&#32622;&#19979;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#21306;&#30340;GNN&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;GraphPart&#12290;GraphPart&#39318;&#20808;&#23558;&#22270;&#20998;&#25104;&#19981;&#30456;&#20132;&#30340;&#23376;&#22270;&#65292;&#28982;&#21518;&#22312;&#27599;&#20010;&#23376;&#22270;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#33410;&#28857;&#36827;&#34892;&#26597;&#35810;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22270;&#21644;&#33410;&#28857;&#29305;&#24449;&#19979;&#30340;&#29616;&#23454;&#24179;&#28369;&#20551;&#35774;&#30340;&#20998;&#31867;&#35823;&#24046;&#30340;&#26032;&#39062;&#20998;&#26512;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27880;&#37322;&#39044;&#31639;&#32422;&#26463;&#19979;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;GNN&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#19981;&#20250;&#24341;&#20837;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#35757;&#32451;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#27809;&#26377;&#26631;&#35760;&#39564;&#35777;&#38598;&#30340;&#20027;&#21160;&#23398;&#20064;&#35774;&#32622;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of semi-supervised learning with Graph Neural Networks (GNNs) in an active learning setup. We propose GraphPart, a novel partition-based active learning approach for GNNs. GraphPart first splits the graph into disjoint partitions and then selects representative nodes within each partition to query. The proposed method is motivated by a novel analysis of the classification error under realistic smoothness assumptions over the graph and the node features. Extensive experiments on multiple benchmark datasets demonstrate that the proposed method outperforms existing active learning methods for GNNs under a wide range of annotation budget constraints. In addition, the proposed method does not introduce additional hyperparameters, which is crucial for model training, especially in the active learning setting where a labeled validation set may not be available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#20559;&#35265;&#20256;&#36882;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25216;&#26415;&#35774;&#35745;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#20250;&#25918;&#22823;&#21644;&#20256;&#25773;&#21487;&#38752;&#24615;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2201.07677</link><description>&lt;p&gt;
&#24494;&#22411;&#12289;&#22987;&#32456;&#22312;&#32447;&#19988;&#26131;&#30862;: &#35774;&#35745;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#20256;&#36882;&#19982;&#22312;&#32447;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiny, always-on and fragile: Bias propagation through design choices in on-device machine learning workflows. (arXiv:2201.07677v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#20559;&#35265;&#20256;&#36882;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#25216;&#26415;&#35774;&#35745;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#20250;&#25918;&#22823;&#21644;&#20256;&#25773;&#21487;&#38752;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#21313;&#20159;&#20010;&#20998;&#24067;&#24335;&#12289;&#24322;&#26500;&#30340; IOT &#35774;&#22791;&#65292;&#22312;&#20010;&#20154;&#25968;&#25454;&#19978;&#37096;&#32626;&#30340;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#65292;&#29992;&#20110;&#31169;&#23494;&#12289;&#24555;&#36895;&#19988;&#31163;&#32447;&#25512;&#29702;&#12290;&#36793;&#32536;&#31471;&#30340;&#26426;&#22120;&#23398;&#20064;&#39640;&#24230;&#20381;&#36182;&#19978;&#19979;&#25991;&#65292;&#23545;&#29992;&#25143;&#12289;&#29992;&#27861;&#12289;&#30828;&#20214;&#21644;&#29615;&#22659;&#23646;&#24615;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#20559;&#35265;&#30340;&#20542;&#21521;&#20351;&#24471;&#30740;&#31350;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20559;&#35265;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#26159;&#23545;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#20013;&#20559;&#35265;&#30740;&#31350;&#30340;&#39318;&#27425;&#25506;&#32034;&#65292;&#20026;&#26500;&#24314;&#26356;&#20844;&#24179;&#30340;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#22880;&#23450;&#20102;&#37325;&#35201;&#22522;&#30784;&#12290;&#26412;&#25991;&#36890;&#36807;&#36719;&#20214;&#24037;&#31243;&#35282;&#24230;&#35843;&#26597;&#20102;&#36793;&#32536;&#31471;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#27969;&#31243;&#20013;&#20559;&#35265;&#20256;&#36882;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#21487;&#38752;&#24615;&#20559;&#35265;&#30830;&#23450;&#20026;&#19981;&#20844;&#24179;&#24615;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#21487;&#38752;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#25216;&#26415;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#25918;&#22823;&#21644;&#20256;&#25773;&#21487;&#38752;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#20250;&#23545;&#21487;&#38752;&#24615;&#20559;&#35265;&#30340;&#20256;&#25773;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Billions of distributed, heterogeneous and resource constrained IoT devices deploy on-device machine learning (ML) for private, fast and offline inference on personal data. On-device ML is highly context dependent, and sensitive to user, usage, hardware and environment attributes. This sensitivity and the propensity towards bias in ML makes it important to study bias in on-device settings. Our study is one of the first investigations of bias in this emerging domain, and lays important foundations for building fairer on-device ML. We apply a software engineering lens, investigating the propagation of bias through design choices in on-device ML workflows. We first identify reliability bias as a source of unfairness and propose a measure to quantify it. We then conduct empirical experiments for a keyword spotting task to show how complex and interacting technical design choices amplify and propagate reliability bias. Our results validate that design choices made during model training, lik
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340; K-image &#28151;&#21512;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#20808;&#39564;&#20998;&#24067;&#30340;&#26829;&#23376;&#20998;&#25286;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20581;&#22766;&#12289;&#24191;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.04248</link><description>&lt;p&gt;
&#35770;&#20998;&#31867;&#38382;&#39064;&#20013;&#22270;&#20687;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340; K-image &#25193;&#23637;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Observations on K-image Expansion of Image-Mixing Augmentation for Classification. (arXiv:2110.04248v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340; K-image &#28151;&#21512;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#20808;&#39564;&#20998;&#24067;&#30340;&#26829;&#23376;&#20998;&#25286;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#20581;&#22766;&#12289;&#24191;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#28151;&#21512;&#22686;&#24378;&#25216;&#26415;&#65288;&#20363;&#22914; Mixup &#21644; CutMix&#65289;&#24050;&#25104;&#20026;&#22270;&#20687;&#20998;&#31867;&#35757;&#32451;&#25216;&#26415;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#24182;&#26410;&#38416;&#26126;&#28151;&#21512;&#22270;&#20687;&#25968;&#30446;&#30340;&#38382;&#39064;&#65306;&#21482;&#26377;&#26420;&#32032;&#30340; K-image &#25193;&#23637;&#24050;&#34987;&#35777;&#26126;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#29380;&#21033;&#20811;&#38647;&#20808;&#39564;&#20998;&#24067;&#30340;&#26829;&#23376;&#20998;&#25286;&#36807;&#31243;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; K-image &#28151;&#21512;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340; K-image &#25193;&#23637;&#22686;&#24378;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#21452;&#22270;&#20687;&#28151;&#21512;&#22686;&#24378;&#26041;&#27861;&#65306;(1) &#20135;&#29983;&#26356;&#20581;&#22766;&#21644;&#24191;&#20041;&#30340;&#20998;&#31867;&#22120;&#65307;(2) &#26356;&#20248;&#33391;&#30340;&#25439;&#22833;&#26223;&#35266;&#24418;&#29366;&#65307;(3) &#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27010;&#29575;&#27169;&#22411;&#21487;&#20197;&#34913;&#37327;&#27599;&#20010;&#26679;&#26412;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#29616;&#25628;&#32034;&#26102;&#38388;&#20943;&#23569;&#20102; 7 &#20493;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-mixing augmentations (e.g., Mixup and CutMix), which typically involve mixing two images, have become the de-facto training techniques for image classification. Despite their huge success in image classification, the number of images to be mixed has not been elucidated in the literature: only the naive K-image expansion has been shown to lead to performance degradation. This study derives a new K-image mixing augmentation based on the stick-breaking process under Dirichlet prior distribution. We demonstrate the superiority of our K-image expansion augmentation over conventional two-image mixing augmentation methods through extensive experiments and analyses: (1) more robust and generalized classifiers; (2) a more desirable loss landscape shape; (3) better adversarial robustness. Moreover, we show that our probabilistic model can measure the sample-wise uncertainty and boost the efficiency for network architecture search by achieving a 7-fold reduction in the search time. Code wil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#32593;&#32476;DeLaN&#65292;&#32467;&#21512;&#29289;&#29702;&#23398;&#21407;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#21512;&#29702;&#24615;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2110.01894</link><description>&lt;p&gt;
&#32467;&#21512;&#29289;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#23398;&#20064;&#36830;&#32493;&#26102;&#38388;&#21160;&#21147;&#23398;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Combining Physics and Deep Learning to learn Continuous-Time Dynamics Models. (arXiv:2110.01894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#32593;&#32476;DeLaN&#65292;&#32467;&#21512;&#29289;&#29702;&#23398;&#21407;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#21512;&#29702;&#24615;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#20294;&#28145;&#24230;&#32593;&#32476;&#30340;&#19981;&#36275;&#20043;&#19968;&#22312;&#20110;&#36825;&#20123;&#32593;&#32476;&#26159;&#40657;&#30418;&#23376;&#34920;&#31034;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#23398;&#20064;&#21040;&#30340;&#36817;&#20284;&#20540;&#24573;&#30053;&#20102;&#29616;&#26377;&#30340;&#29289;&#29702;&#25110;&#26426;&#22120;&#20154;&#23398;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#20123;&#40657;&#30418;&#23376;&#27169;&#22411;&#24182;&#19981;&#29702;&#24819;&#65292;&#22240;&#20026;&#22522;&#26412;&#21407;&#29702;&#24050;&#32463;&#34987;&#24456;&#22909;&#22320;&#29702;&#35299;&#65292;&#26631;&#20934;&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#23398;&#20064;&#36829;&#21453;&#36825;&#20123;&#21407;&#29702;&#30340;&#21160;&#21147;&#23398;&#12290;&#20026;&#20102;&#20351;&#29992;&#28145;&#24230;&#32593;&#32476;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#21512;&#29702;&#24615;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#23558;&#29289;&#29702;&#23398;&#30340;&#22522;&#26412;&#21407;&#29702;&#19982;&#28145;&#24230;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23558;Lagrangian&#21147;&#23398;&#32435;&#20837;&#27169;&#22411;&#23398;&#20064;&#20013;&#65292;&#20351;&#24471;&#25152;&#26377;&#36817;&#20284;&#27169;&#22411;&#37117;&#36981;&#24490;&#29289;&#29702;&#23450;&#24459;&#24182;&#20445;&#23384;&#33021;&#37327;&#12290;Deep Lagrangian Networks (DeLaN) &#20351;&#29992;&#20004;&#20010;&#32593;&#32476;&#21442;&#25968;&#21270;&#31995;&#32479;&#33021;&#37327;&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#27431;&#25289;-&#25289;&#26684;&#26391;&#26085;&#24494;&#20998;&#26041;&#31243;&#30340;&#24179;&#26041;&#27531;&#24046;&#26469;&#33719;&#24471;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has been widely used within learning algorithms for robotics. One disadvantage of deep networks is that these networks are black-box representations. Therefore, the learned approximations ignore the existing knowledge of physics or robotics. Especially for learning dynamics models, these black-box models are not desirable as the underlying principles are well understood and the standard deep networks can learn dynamics that violate these principles. To learn dynamics models with deep networks that guarantee physically plausible dynamics, we introduce physics-inspired deep networks that combine first principles from physics with deep learning. We incorporate Lagrangian mechanics within the model learning such that all approximated models adhere to the laws of physics and conserve energy. Deep Lagrangian Networks (DeLaN) parametrize the system energy using two networks. The parameters are obtained by minimizing the squared residual of the Euler-Lagrange differential equatio
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2109.01636</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.01636
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#23454;&#39564;&#34920;&#26126;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;NER&#26041;&#27861;&#21487;&#25552;&#39640;NER&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;NER&#20219;&#21153;&#38754;&#20020;&#30340;&#26368;&#22823;&#22256;&#38590;&#26159;&#21363;&#20351;&#22312;NE&#31867;&#22411;&#21644;&#25991;&#26723;&#19981;&#29087;&#24713;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#38656;&#35201;&#20445;&#25345;&#21487;&#26816;&#27979;&#24615;&#12290;&#24847;&#35782;&#21040;&#29305;&#23450;&#24615;&#20449;&#24687;&#21487;&#33021;&#21253;&#21547;&#21333;&#35789;&#30340;&#28508;&#22312;&#21547;&#20041;&#24182;&#29983;&#25104;&#35789;&#23884;&#20837;&#30340;&#35821;&#20041;&#30456;&#20851;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20998;&#24067;&#24863;&#30693;&#35789;&#23884;&#20837;&#65292;&#24182;&#23454;&#26045;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;NER&#26694;&#26550;&#20013;&#30340;&#20998;&#24067;&#20449;&#24687;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#23558;&#35789;&#30340;&#29305;&#24322;&#24615;&#34701;&#20837;&#29616;&#26377;&#30340;NER&#26041;&#27861;&#20013;&#65292;NER&#30340;&#24615;&#33021;&#23558;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the fast development of Deep Learning techniques, Named Entity Recognition (NER) is becoming more and more important in the information extraction task. The greatest difficulty that the NER task faces is to keep the detectability even when types of NE and documents are unfamiliar. Realizing that the specificity information may contain potential meanings of a word and generate semantic-related features for word embedding, we develop a distribution-aware word embedding and implement three different methods to make use of the distribution information in a NER framework. And the result shows that the performance of NER will be improved if the word specificity is incorporated into existing NER methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#31070;&#32463;ODE&#21644;&#28145;&#24230;&#21487;&#20998;&#21367;&#31215;&#30456;&#32467;&#21512;&#30340;DNN&#27169;&#22411;dsODENet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36793;&#32536;&#39046;&#22495;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;FPGA&#35774;&#35745;&#65292;&#21487;&#37096;&#32626;dsODENet&#12290;</title><link>http://arxiv.org/abs/2107.12824</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#21487;&#20998;&#21367;&#31215;&#31070;&#32463;ODE&#30340;&#20302;&#25104;&#26412;FPGA&#36793;&#32536;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Low-Cost Neural ODE with Depthwise Separable Convolution for Edge Domain Adaptation on FPGAs. (arXiv:2107.12824v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#31070;&#32463;ODE&#21644;&#28145;&#24230;&#21487;&#20998;&#21367;&#31215;&#30456;&#32467;&#21512;&#30340;DNN&#27169;&#22411;dsODENet&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#36793;&#32536;&#39046;&#22495;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;FPGA&#35774;&#35745;&#65292;&#21487;&#37096;&#32626;dsODENet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#29615;&#22659;&#20013;&#65292;&#39640;&#24615;&#33021;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#31995;&#32479;&#38656;&#27714;&#37327;&#24456;&#39640;&#12290;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23558;DNN&#37096;&#32626;&#21040;&#35745;&#31639;&#36164;&#28304;&#26377;&#20005;&#26684;&#38480;&#21046;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#32467;&#21512;&#26368;&#36817;&#25552;&#20986;&#30340;&#21442;&#25968;&#32553;&#20943;&#25216;&#26415;&#65306;&#31070;&#32463;ODE&#65288;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65289;&#21644;DSC&#65288;&#28145;&#24230;&#21487;&#20998;&#21367;&#31215;&#65289;&#65292;&#23548;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#31934;&#24230;&#39640;&#30340;DNN&#27169;&#22411;&#65292;&#31216;&#20026;dsODENet&#12290;&#31070;&#32463;ODE&#21033;&#29992;ResNet&#21644;ODE&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#23618;&#20043;&#38388;&#20849;&#20139;&#22823;&#37096;&#20998;&#26435;&#37325;&#21442;&#25968;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#20869;&#23384;&#28040;&#32791;&#12290;&#26412;&#25991;&#23558;dsODENet&#24212;&#29992;&#20110;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#22495;&#33258;&#36866;&#24212;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;dsODENet&#30340;&#36164;&#28304;&#39640;&#25928;&#30340;&#22522;&#20110;FPGA&#30340;&#35774;&#35745;&#65292;&#20854;&#20013;&#38500;&#20102;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#23618;&#30340;&#25152;&#26377;&#21442;&#25968;&#21644;&#29305;&#24449;&#22270;&#22343;&#21487;&#26144;&#23556;&#21040;&#33455;&#29255;&#19978;&#30340;&#20869;&#23384;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-performance deep neural network (DNN)-based systems are in high demand in edge environments. Due to its high computational complexity, it is challenging to deploy DNNs on edge devices with strict limitations on computational resources. In this paper, we derive a compact while highly-accurate DNN model, termed dsODENet, by combining recently-proposed parameter reduction techniques: Neural ODE (Ordinary Differential Equation) and DSC (Depthwise Separable Convolution). Neural ODE exploits a similarity between ResNet and ODE, and shares most of weight parameters among multiple layers, which greatly reduces the memory consumption. We apply dsODENet to a domain adaptation as a practical use case with image classification datasets. We also propose a resource-efficient FPGA-based design for dsODENet, where all the parameters and feature maps except for pre- and post-processing layers can be mapped onto on-chip memories. It is implemented on Xilinx ZCU104 board and evaluated in terms of do
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#31454;&#20105;&#27604;&#29575;&#33021;&#22815;&#20809;&#28369;&#22320;&#38543;&#35823;&#24046;&#20943;&#23567;&#32780;&#20174;&#23545;&#25968;&#32423;&#21035;&#38477;&#20302;&#21040;&#24120;&#25968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2107.08277</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning Augmented Online Facility Location. (arXiv:2107.08277v3 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.08277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#31454;&#20105;&#27604;&#29575;&#33021;&#22815;&#20809;&#28369;&#22320;&#38543;&#35823;&#24046;&#20943;&#23567;&#32780;&#20174;&#23545;&#25968;&#32423;&#21035;&#38477;&#20302;&#21040;&#24120;&#25968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#35774;&#26045;&#36873;&#22336;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#38656;&#27714;&#36880;&#19968;&#21040;&#36798;&#65292;&#24182;&#19988;&#24517;&#39035;&#22312;&#21040;&#36798;&#26102;&#23558;&#20854;&#65288;&#26080;&#27861;&#25764;&#38144;&#22320;&#65289;&#20998;&#37197;&#32473;&#19968;&#20010;&#24320;&#25918;&#30340;&#35774;&#26045;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#26410;&#26469;&#38656;&#27714;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35813;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#23545;&#26368;&#20248;&#35774;&#26045;&#20301;&#32622;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#31454;&#20105;&#27604;&#29575;&#20174;&#38656;&#27714;&#25968;&#37327;&#30340;&#23545;&#25968;&#32423;&#21035;&#24179;&#28369;&#22320;&#38477;&#20302;&#21040;&#24120;&#25968;&#32423;&#21035;&#65292;&#24403;&#38169;&#35823;&#65288;&#21363;&#39044;&#27979;&#20301;&#32622;&#19982;&#26368;&#20248;&#35774;&#26045;&#20301;&#32622;&#20043;&#38388;&#30340;&#24635;&#36317;&#31163;&#65289;&#21521;&#38646;&#38477;&#20302;&#26102;&#12290;&#25105;&#20204;&#37197;&#21512;&#31639;&#27861;&#30340;&#38477;&#20302;&#30028;&#65292;&#24314;&#31435;&#20102;&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#29575;&#23545;&#35823;&#24046;&#30340;&#20381;&#36182;&#20851;&#31995;&#26159;&#26368;&#20248;&#30340;&#65292;&#24120;&#25968;&#25509;&#36817;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the research agenda initiated by Munoz &amp; Vassilvitskii [1] and Lykouris &amp; Vassilvitskii [2] on learning-augmented online algorithms for classical online optimization problems, in this work, we consider the Online Facility Location problem under this framework. In Online Facility Location (OFL), demands arrive one-by-one in a metric space and must be (irrevocably) assigned to an open facility upon arrival, without any knowledge about future demands.  We present an online algorithm for OFL that exploits potentially imperfect predictions on the locations of the optimal facilities. We prove that the competitive ratio decreases smoothly from sublogarithmic in the number of demands to constant, as the error, i.e., the total distance of the predicted locations to the optimal facility locations, decreases towards zero. We complement our analysis with a matching lower bound establishing that the dependence of the algorithm's competitive ratio on the error is optimal, up to constant fa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#26679;&#26412;&#22823;&#23567;&#19979;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2005.12900</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#31361;&#30772;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#22823;&#23567;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Breaking the Sample Size Barrier in Model-Based Reinforcement Learning with a Generative Model. (arXiv:2005.12900v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#22312;&#36739;&#23567;&#30340;&#26679;&#26412;&#22823;&#23567;&#19979;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30528;&#30524;&#20110;&#22312;&#26377;&#29983;&#25104;&#27169;&#22411;&#65288;&#25110;&#27169;&#25311;&#22120;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#22686;&#24378;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#32771;&#34385;&#24102;&#26377;&#25240;&#25187;&#30340;&#26080;&#38480;&#26102;&#38388;&#27493;&#38271;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#20854;&#29366;&#24577;&#31354;&#38388;&#20026;$\mathcal{S}$&#65292;&#21160;&#20316;&#31354;&#38388;&#20026;$\mathcal{A}$&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#26159;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#32479;&#35745;&#31934;&#24230;&#20043;&#38388;&#26435;&#34913;&#30340;&#23436;&#25972;&#22270;&#26223;&#23578;&#26410;&#30830;&#23450;&#12290;&#29305;&#21035;&#26159;&#65292;&#25152;&#26377;&#30340;&#20808;&#21069;&#32467;&#26524;&#37117;&#21463;&#21040;&#20005;&#37325;&#30340;&#26679;&#26412;&#22823;&#23567;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#20204;&#22768;&#31216;&#30340;&#32479;&#35745;&#20445;&#35777;&#20165;&#22312;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;&#33267;&#23569;$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$&#26102;&#25165;&#25104;&#31435;&#12290;&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#20004;&#20010;&#31639;&#27861;&#8212;&#8212;&#25200;&#21160;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#21644;&#20445;&#23432;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#8212;&#8212;&#22312;&#26679;&#26412;&#22823;&#23567;&#36229;&#36807;$\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$&#30340;&#24773;&#20917;&#19979;&#23601;&#33021;&#35777;&#26126;&#23427;&#20204;&#30340;&#26497;&#23567;&#21270;&#26368;&#22823;&#31639;&#27861;&#20248;&#21270;&#24615;&#33021;&#65288;&#20960;&#20046;&#31526;&#21512;&#19968;&#20123;&#23545;&#25968;&#22240;&#23376;&#65289;&#12290;&#38500;&#20102;&#26080;&#38480;&#26102;&#38388;&#27493;&#38271;M&#35299;&#20915;&#26041;&#26696;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26377;&#38480;&#26679;&#26412;&#21644;&#36817;&#20284;&#20215;&#20540;&#36845;&#20195;&#38382;&#39064;&#65292;&#20197;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#31639;&#27861;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is concerned with the sample efficiency of reinforcement learning, assuming access to a generative model (or simulator). We first consider $\gamma$-discounted infinite-horizon Markov decision processes (MDPs) with state space $\mathcal{S}$ and action space $\mathcal{A}$. Despite a number of prior works tackling this problem, a complete picture of the trade-offs between sample complexity and statistical accuracy is yet to be determined. In particular, all prior results suffer from a severe sample size barrier, in the sense that their claimed statistical guarantees hold only when the sample size exceeds at least $\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$. The current paper overcomes this barrier by certifying the minimax optimality of two algorithms -- a perturbed model-based algorithm and a conservative model-based algorithm -- as soon as the sample size exceeds the order of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). Moving beyond infinite-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#35268;&#21017;&#30340;&#28145;&#24230;&#22270;&#20687;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#37322;&#21644;&#21487;&#20280;&#32553;&#30340;&#29305;&#24449;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/1905.10575</link><description>&lt;p&gt;
&#24102;&#27169;&#31946;&#35268;&#21017;&#30340;&#28145;&#24230;&#22270;&#20687;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Image Feature Learning with Fuzzy Rules. (arXiv:1905.10575v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1905.10575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#35268;&#21017;&#30340;&#28145;&#24230;&#22270;&#20687;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#26356;&#22909;&#30340;&#35299;&#37322;&#21644;&#21487;&#20280;&#32553;&#30340;&#29305;&#24449;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#30340;&#26041;&#27861;&#26159;&#35768;&#22810;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#30340;&#20851;&#38190;&#12290;&#30446;&#21069;&#65292;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#35757;&#32451;&#33258;&#21160;&#25552;&#21462;&#40065;&#26834;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24403;&#21069;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65306;1&#65289;&#23427;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#65307;2&#65289;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#40657;&#21283;&#23376;&#27169;&#22411;&#65292;&#35299;&#37322;&#33021;&#21147;&#24046;&#12290;&#20026;&#20102;&#24212;&#23545;&#20197;&#19978;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#30340;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27169;&#31946;&#35268;&#21017;&#30340;&#28145;&#24230;&#22270;&#20687;&#29305;&#24449;&#23398;&#20064;&#65288;DIFL-FR&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#31946;&#24314;&#27169;&#25216;&#26415;&#21644;&#28145;&#24230;&#22534;&#21472;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36880;&#23618;&#23398;&#20064;&#22522;&#20110;&#27169;&#31946;&#35268;&#21017;&#30340;&#22270;&#20687;&#29305;&#24449;&#65292;&#22240;&#27492;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#30340;&#35268;&#21017;&#26356;&#22909;&#22320;&#35299;&#37322;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;DIFL-FR&#26041;&#27861;&#30340;&#29305;&#24449;&#23398;&#20064;&#19982;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#20854;&#20182;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The methods of extracting image features are the key to many image processing tasks. At present, the most popular method is the deep neural network which can automatically extract robust features through end-to-end training instead of hand-crafted feature extraction. However, the deep neural network currently faces many challenges: 1) its effectiveness is heavily dependent on large datasets, so the computational complexity is very high; 2) it is usually regarded as a black box model with poor interpretability. To meet the above challenges, a more interpretable and scalable feature learning method, i.e., deep image feature learning with fuzzy rules (DIFL-FR), is proposed in the paper, which combines the rule-based fuzzy modeling technique and the deep stacked learning strategy. The method progressively learns image features through a layer-by-layer manner based on fuzzy rules, so the feature learning process can be better explained by the generated rules. More importantly, the learning 
&lt;/p&gt;</description></item></channel></rss>