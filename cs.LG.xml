<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#32467;&#21512;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#20197;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00769</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#29983;&#29702;&#20449;&#21495;&#30340;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Transformer-based Network for Emotion Recognition from Multi Physiological Signals. (arXiv:2305.00769v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;Transformer&#32593;&#32476;&#65292;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#32467;&#21512;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#20197;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#22810;&#23610;&#24230;Transformer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#29983;&#29702;&#25968;&#25454;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#29616;&#20195;&#20256;&#24863;&#22120;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#20174;&#36825;&#20123;&#20449;&#21495;&#20013;&#25552;&#21462;&#22823;&#37327;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#19968;&#20219;&#21153;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#22810;&#27169;&#24577;&#25216;&#26415;&#21644;&#32553;&#25918;&#25968;&#25454;&#65292;&#20197;&#24314;&#31435;&#20869;&#37096;&#36523;&#20307;&#20449;&#21495;&#19982;&#20154;&#31867;&#24773;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Transformer&#21644;&#39640;&#26031;&#21464;&#25442;&#25216;&#26415;&#65292;&#25552;&#39640;&#20449;&#21495;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;EPiC&#31454;&#36187;&#30340;CASE&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;RMSE&#24471;&#20998;&#20026;1.45&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an efficient Multi-scale Transformer-based approach for the task of Emotion recognition from Physiological data, which has gained widespread attention in the research community due to the vast amount of information that can be extracted from these signals using modern sensors and machine learning techniques. Our approach involves applying a Multi-modal technique combined with scaling data to establish the relationship between internal body signals and human emotions. Additionally, we utilize Transformer and Gaussian Transformation techniques to improve signal encoding effectiveness and overall performance. Our model achieves decent results on the CASE dataset of the EPiC competition, with an RMSE score of 1.45.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RViDeformer&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#21450;&#20854;&#37197;&#22871;&#25968;&#25454;&#38598;ReCRVD&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#20302;ISO&#35774;&#32622;&#37325;&#26032;&#25429;&#25417;&#29616;&#26377;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#23545;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.00767</link><description>&lt;p&gt;
RViDeformer&#65306;&#20855;&#26377;&#26356;&#22823;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset. (arXiv:2305.00767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RViDeformer&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#21450;&#20854;&#37197;&#22871;&#25968;&#25454;&#38598;ReCRVD&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#20302;ISO&#35774;&#32622;&#37325;&#26032;&#25429;&#25417;&#29616;&#26377;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#23545;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#19982;&#25104;&#20687;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#39046;&#22495;&#20013;&#25104;&#29087;&#30340;&#22122;&#22768;&#24314;&#27169;&#65292;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#38459;&#30861;&#20102;&#21435;&#22122;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#21463;&#25511;&#30340;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#26469;&#35828;&#65292;&#27809;&#26377;&#20855;&#26377;&#30495;&#23454;&#36816;&#21160;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#20026;&#30495;&#23454;&#21160;&#24577;&#22330;&#26223;&#25429;&#25417;&#22122;&#22768;&#21644;&#28165;&#26224;&#24103;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#26032;&#25429;&#25417;&#20197;&#39640;&#20302;ISO&#35774;&#32622;&#26174;&#31034;&#30340;&#29616;&#26377;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#37197;&#23545;&#24103;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#35270;&#39057;&#21435;&#22122;&#25968;&#25454;&#38598;&#65288;&#21629;&#21517;&#20026;ReCRVD&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;120&#32452;&#22122;&#22768;-&#28165;&#26224;&#35270;&#39057;&#65292;&#20854;ISO&#20540;&#20174;1600&#21040;25600&#19981;&#31561;&#12290;&#20854;&#27425;&#65292;&#34429;&#28982;&#38750;&#26412;&#22320;&#26102;&#31354;&#20851;&#27880;&#23545;&#20110;&#21435;&#22122;&#26377;&#30410;&#65292;&#20294;&#23427;&#36890;&#24120;&#23548;&#33268;&#27785;&#37325;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#32593;&#32476;&#65288;RViDeformer&#65289;&#65292;&#23427;&#25506;&#32034;&#20102;&#30701;&#36317;&#31163;&#21644;&#38271;&#36317;&#31163;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31354;&#38388;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#26412;&#22320;&#35270;&#35273;&#29305;&#24449;&#20197;&#20943;&#23569;&#31354;&#38388;&#20887;&#20313;&#24182;&#21152;&#36895;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#38271;&#31243;&#22122;&#22768;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21464;&#25442;&#22120;&#32593;&#32476;&#65292;&#21516;&#26102;&#27169;&#22411;&#21270;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, raw video denoising has garnered increased attention due to the consistency with the imaging process and well-studied noise modeling in the raw domain. However, two problems still hinder the denoising performance. Firstly, there is no large dataset with realistic motions for supervised raw video denoising, as capturing noisy and clean frames for real dynamic scenes is difficult. To address this, we propose recapturing existing high-resolution videos displayed on a 4K screen with high-low ISO settings to construct noisy-clean paired frames. In this way, we construct a video denoising dataset (named as ReCRVD) with 120 groups of noisy-clean videos, whose ISO values ranging from 1600 to 25600. Secondly, while non-local temporal-spatial attention is beneficial for denoising, it often leads to heavy computation costs. We propose an efficient raw video denoising transformer network (RViDeformer) that explores both short and long-distance correlations. Specifically, we propos
&lt;/p&gt;</description></item><item><title>&#23545;52&#20010;&#30495;&#23454;&#22810;&#20803;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;32&#20010;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#31532;kNN&#31639;&#27861;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#32780;EIF&#31639;&#27861;&#22312;&#20840;&#29699;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24314;&#35758;&#23454;&#38469;&#20351;&#29992;&#36825;&#19977;&#27454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#24037;&#20855;&#31665;&#12290;</title><link>http://arxiv.org/abs/2305.00735</link><description>&lt;p&gt;
&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#27454;&#31639;&#27861;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unsupervised anomaly detection algorithms on real-world data: how many do we need?. (arXiv:2305.00735v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00735
&lt;/p&gt;
&lt;p&gt;
&#23545;52&#20010;&#30495;&#23454;&#22810;&#20803;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;32&#20010;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#31532;kNN&#31639;&#27861;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#65292;&#32780;EIF&#31639;&#27861;&#22312;&#20840;&#29699;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#24314;&#35758;&#23454;&#38469;&#20351;&#29992;&#36825;&#19977;&#27454;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;52&#20010;&#30495;&#23454;&#22810;&#20803;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;32&#20010;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;&#31532;kNN&#65288;&#21040;k&#20010;&#26368;&#36817;&#37051;&#23621;&#30340;&#36317;&#31163;&#65289;&#31639;&#27861;&#30340;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;&#36890;&#36807;&#21487;&#35270;&#21270;&#21644;&#32858;&#31867;&#31639;&#27861;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#26126;&#26174;&#30340;&#31751;&#65306;&#19968;&#20010;&#26159;&#8220;&#26412;&#22320;&#8221;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#26159;&#8220;&#20840;&#23616;&#8221;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#65292;kNN&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65292;&#22312;&#20840;&#23616;&#25968;&#25454;&#38598;&#20013;&#65292;EIF&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#12290;&#32508;&#21512;&#32771;&#34385;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24314;&#35758;&#23454;&#38469;&#20351;&#29992;&#36825;&#19977;&#31181;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65288;&#20998;&#21035;&#26159;kNN&#12289;EIF&#21644;LOF&#31639;&#27861;&#65289;&#30340;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study we evaluate 32 unsupervised anomaly detection algorithms on 52 real-world multivariate tabular datasets, performing the largest comparison of unsupervised anomaly detection algorithms to date. On this collection of datasets, the $k$-thNN (distance to the $k$-nearest neighbor) algorithm significantly outperforms the most other algorithms. Visualizing and then clustering the relative performance of the considered algorithms on all datasets, we identify two clear clusters: one with ``local'' datasets, and another with ``global'' datasets. ``Local'' anomalies occupy a region with low density when compared to nearby samples, while ``global'' occupy an overall low density region in the feature space. On the local datasets the $k$NN ($k$-nearest neighbor) algorithm comes out on top. On the global datasets, the EIF (extended isolation forest) algorithm performs the best. Also taking into consideration the algorithms' computational complexity, a toolbox with these three unsupervis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#22312;&#34920;&#31034;&#21644; &#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26102;&#33021;&#22815;&#25429;&#25417;&#26356;&#38271;&#31243;&#30340;&#20840;&#23616;&#27169;&#24335;&#24182;&#32447;&#24615;&#20998;&#31163;&#22270;&#20687;&#65292;&#20294;&#22312;&#33258;&#25105;&#20851;&#27880;&#21147;&#30340;&#21516;&#36136;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#23494;&#38598;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00729</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#23398;&#20064;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Do Self-Supervised Vision Transformers Learn?. (arXiv:2305.00729v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#22312;&#34920;&#31034;&#21644; &#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26102;&#33021;&#22815;&#25429;&#25417;&#26356;&#38271;&#31243;&#30340;&#20840;&#23616;&#27169;&#24335;&#24182;&#32447;&#24615;&#20998;&#31163;&#22270;&#20687;&#65292;&#20294;&#22312;&#33258;&#25105;&#20851;&#27880;&#21147;&#30340;&#21516;&#36136;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#23494;&#38598;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#21644;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#22312;&#20854;&#34920;&#31034;&#21644;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#24182;&#38416;&#36848;&#20102;&#33258;&#30417;&#30563;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;ViTs&#65289;&#30340;&#24615;&#36136;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;CL&#35757;&#32451;&#33258;&#25105;&#20851;&#27880;&#21147;&#20197;&#25429;&#25417;&#27604;MIM&#26356;&#38271;&#31243;&#30340;&#20840;&#23616;&#27169;&#24335;&#65292;&#20363;&#22914;&#29289;&#20307;&#30340;&#24418;&#29366;&#65292;&#23588;&#20854;&#26159;&#22312;ViT&#26550;&#26500;&#30340;&#21518;&#20960;&#23618;&#20013;&#12290;&#36825;&#20351;&#24471;ViTs&#33021;&#22815;&#22312;&#20854;&#34920;&#31034;&#31354;&#38388;&#20013;&#32447;&#24615;&#20998;&#31163;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#23427;&#20063;&#20351;&#24471;&#33258;&#25105;&#20851;&#27880;&#21147;&#23545;&#20110;&#25152;&#26377;&#26597;&#35810;&#26631;&#35760;&#21644;&#22836;&#37096;&#30340;&#21516;&#36136;&#24615;&#23849;&#28291;&#12290;&#36825;&#31181;&#33258;&#25105;&#20851;&#27880;&#21147;&#30340;&#21516;&#36136;&#24615;&#38477;&#20302;&#20102;&#34920;&#24449;&#30340;&#22810;&#26679;&#24615;&#65292;&#24694;&#21270;&#20102;&#21487;&#25193;&#23637;&#24615;&#21644;&#23494;&#38598;&#39044;&#27979;&#24615;&#33021;&#12288;&#12290;CL&#21033;&#29992;&#34920;&#31034;&#30340;&#20302;&#39057;&#20449;&#21495;&#65292;&#32780;MIM&#21033;&#29992;&#39640;&#39057;&#20449;&#21495;&#12290;&#30001;&#20110;&#20302;&#39057;&#21644;&#39640;&#39057;&#20449;&#24687;&#20998;&#21035;&#20195;&#34920;&#24418;&#29366;&#21644;&#36136;&#22320;&#65292;&#22240;&#27492;CL&#26356;&#21152;&#27880;&#37325;&#24418;&#29366;&#65292;&#32780;MIM&#21017;&#26356;&#21152;&#27880;&#37325;&#36136;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20687;&#32032;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#31163;&#25955;&#21367;&#31215;&#21644;&#26377;&#38480;&#24046;&#20998;&#31639;&#23376;&#20043;&#38388;&#32852;&#31995;&#30340;&#21033;&#29992;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#33258;&#20559;&#24494;&#20998;&#26041;&#31243;&#31354;&#26102;&#31163;&#25955;&#20986;&#30340;&#24207;&#21015;&#21487;&#20197;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#30340;&#21367;&#31215;(&#27531;&#24046;)&#32593;&#32476;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.00723</link><description>&lt;p&gt;
&#22522;&#20110;&#20687;&#32032;&#25968;&#25454;&#30340;&#39044;&#27979;: PDE&#21644;&#26377;&#38480;&#24046;&#20998;&#30340;&#28145;&#20837;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Predictions Based on Pixel Data: Insights from PDEs and Finite Differences. (arXiv:2305.00723v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20687;&#32032;&#25968;&#25454;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#23545;&#31163;&#25955;&#21367;&#31215;&#21644;&#26377;&#38480;&#24046;&#20998;&#31639;&#23376;&#20043;&#38388;&#32852;&#31995;&#30340;&#21033;&#29992;&#65292;&#35777;&#26126;&#20102;&#36924;&#36817;&#33258;&#20559;&#24494;&#20998;&#26041;&#31243;&#31354;&#26102;&#31163;&#25955;&#20986;&#30340;&#24207;&#21015;&#21487;&#20197;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#30340;&#21367;&#31215;(&#27531;&#24046;)&#32593;&#32476;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#39640;&#32500;&#31354;&#38388;&#20013;&#35768;&#22810;&#36924;&#36817;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#36825;&#24471;&#21040;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#38656;&#35201;&#23545;&#23427;&#20204;&#21487;&#20197;&#36924;&#36817;&#30340;&#20869;&#23481;&#20197;&#21450;&#20197;&#20309;&#31181;&#20195;&#20215;&#21644;&#31934;&#24230;&#36924;&#36817;&#26377;&#19968;&#20010;&#22362;&#23454;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#20854;&#20013;&#19968;&#20010;&#22312;&#28041;&#21450;&#22270;&#20687;&#30340;&#36924;&#36817;&#20219;&#21153;&#20013;&#26377;&#23454;&#38469;&#29992;&#36884;&#30340;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#26159;&#21367;&#31215;(&#27531;&#24046;)&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#32593;&#32476;&#20013;&#28041;&#21450;&#30340;&#32447;&#24615;&#31639;&#23376;&#30340;&#23616;&#37096;&#24615;&#36136;&#65292;&#23427;&#20204;&#30340;&#20998;&#26512;&#27604;&#36890;&#29992;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26356;&#20026;&#22797;&#26434;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#30340;&#26159;&#24207;&#21015;&#36924;&#36817;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#23519;&#20540;&#30001;&#30697;&#38453;&#25110;&#39640;&#38454;&#24352;&#37327;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#24403;&#36924;&#36817;&#33258;&#20559;&#24494;&#20998;&#26041;&#31243;&#31354;&#26102;&#31163;&#25955;&#20986;&#30340;&#24207;&#21015;&#26102;&#65292;&#21487;&#20197;&#20351;&#29992;&#30456;&#23545;&#36739;&#23567;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#21367;&#31215;&#21644;&#26377;&#38480;&#24046;&#20998;&#31639;&#23376;&#20043;&#38388;&#30340;&#32852;&#31995;&#26469;&#26500;&#36896;&#36825;&#20123;&#32467;&#26524;&#12290;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are the state-of-the-art for many approximation tasks in high-dimensional spaces, as supported by an abundance of experimental evidence. However, we still need a solid theoretical understanding of what they can approximate and, more importantly, at what cost and accuracy. One network architecture of practical use, especially for approximation tasks involving images, is convolutional (residual) networks. However, due to the locality of the linear operators involved in these networks, their analysis is more complicated than for generic fully connected neural networks. This paper focuses on sequence approximation tasks, where a matrix or a higher-order tensor represents each observation. We show that when approximating sequences arising from space-time discretisations of PDEs we may use relatively small networks. We constructively derive these results by exploiting connections between discrete convolution and finite difference operators. Throughout, we design our network a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26469;&#25913;&#21892;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#26469;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#36127;&#36733;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.00706</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#21457;&#23637;&#32511;&#33394;&#25968;&#25454;&#20013;&#24515;&#30340;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Full Scaling Automation for Sustainable Development of Green Data Centers. (arXiv:2305.00706v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00706
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26469;&#25913;&#21892;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#26469;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#36127;&#36733;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#23835;&#36215;&#23548;&#33268;&#25968;&#25454;&#20013;&#24515;&#30899;&#25490;&#25918;&#37327;&#24778;&#20154;&#22320;&#22686;&#21152;&#65292;&#29616;&#22312;&#21344;&#20840;&#29699;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&gt;3&#65285;&#65292;&#24517;&#39035;&#31435;&#21363;&#37319;&#21462;&#25514;&#26045;&#24212;&#23545;&#23427;&#20204;&#23545;&#20840;&#29699;&#27668;&#20505;&#26085;&#30410;&#22686;&#38271;&#30340;&#36127;&#25285;&#12290;&#36825;&#19968;&#21162;&#21147;&#30340;&#37325;&#28857;&#26159;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#33410;&#30465;&#30005;&#21147;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#38598;&#32676;&#20013;&#21160;&#24577;&#22320;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20351;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#38598;&#32676;&#20445;&#25345;&#20854;&#25152;&#38656;&#30340;CPU&#21033;&#29992;&#29575;&#30446;&#26631;&#65292;&#20174;&#32780;&#25913;&#21892;&#33021;&#28304;&#25928;&#29575;&#12290;FSA&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#30340;&#23041;&#21147;&#26469;&#20934;&#30830;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#65292;&#19981;&#20687;&#20043;&#21069;&#30340;&#33258;&#21160;&#25193;&#23637;&#26041;&#27861;&#65292;&#22914;Autopilot&#25110;FIRM&#65292;&#38656;&#35201;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#21644;&#19987;&#23478;&#30693;&#35782;&#26469;&#35843;&#25972;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid rise in cloud computing has resulted in an alarming increase in data centers' carbon emissions, which now accounts for &gt;3% of global greenhouse gas emissions, necessitating immediate steps to combat their mounting strain on the global climate. An important focus of this effort is to improve resource utilization in order to save electricity usage. Our proposed Full Scaling Automation (FSA) mechanism is an effective method of dynamically adapting resources to accommodate changing workloads in large-scale cloud computing clusters, enabling the clusters in data centers to maintain their desired CPU utilization target and thus improve energy efficiency. FSA harnesses the power of deep representation learning to accurately predict the future workload of each service and automatically stabilize the corresponding target CPU usage level, unlike the previous autoscaling methods, such as Autopilot or FIRM, that need to adjust computing resources with statistical models and expert knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#30340;&#26679;&#26412;&#26377;&#25928;&#12289;&#22343;&#34913;&#35745;&#31639;&#21644;&#23616;&#37096;&#30417;&#25511;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#21644;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00684</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65306;&#20174;&#28216;&#25103;&#23398;&#20064;&#21040;&#23616;&#37096;&#30417;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring. (arXiv:2305.00684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#30340;&#26679;&#26412;&#26377;&#25928;&#12289;&#22343;&#34913;&#35745;&#31639;&#21644;&#23616;&#37096;&#30417;&#25511;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22797;&#26434;&#24230;&#19978;&#19979;&#30028;&#21644;&#31639;&#27861;&#65292;&#24182;&#21457;&#29616;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#32467;&#26500;&#26465;&#20214;&#21644;&#31639;&#27861;&#21407;&#21017;&#20250;&#23548;&#33268;&#21738;&#20123;&#26679;&#26412;&#26377;&#25928;&#30340;&#23398;&#20064;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#25105;&#20204;&#20174;&#23569;&#25968;&#26234;&#33021;&#20307;&#36716;&#31227;&#21040;&#22810;&#25968;&#26234;&#33021;&#20307;&#26102;&#65292;&#36825;&#20123;&#32771;&#34385;&#22914;&#20309;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#22312;&#22810;&#26234;&#33021;&#20307;&#20114;&#21160;&#20915;&#31574;&#30340;&#19968;&#33324;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#21644;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#27491;&#21017;&#24335;&#21338;&#24328;&#12290;&#25105;&#20204;&#20851;&#27880;&#22343;&#34913;&#35745;&#31639;&#65292;&#20854;&#20013;&#38598;&#20013;&#24335;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#25511;&#21046;&#19982;&#26410;&#30693;&#29615;&#22659;&#20132;&#20114;&#30340;&#22810;&#20010;&#26234;&#33021;&#20307;&#26469;&#35745;&#31639;&#22343;&#34913;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;1. &#25105;&#20204;&#22522;&#20110;&#30001;Foster&#31561;&#20154;&#65288;2021&#65289;&#22312;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#24341;&#20837;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#26041;&#27861;&#8212;&#20915;&#31574;-&#20272;&#35745;&#31995;&#25968;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#21046;&#23450;&#20102;&#26368;&#20339;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#21644;&#19979;&#30028;&#12290;&#19982;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#30340;&#26368;&#20339;&#32467;&#26524;&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#22810;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#30340;&#38382;&#39064;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#26041;&#38754;&#21487;&#33021;&#21576;&#25351;&#25968;&#32423;&#38590;&#24230;&#12290;2. &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#22823;&#22411;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#36827;&#34892;&#39640;&#25928;&#30340;&#22343;&#34913;&#35745;&#31639;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#20048;&#35266;&#38236;&#20687;&#19979;&#38477;&#27861;&#30340;&#21407;&#29702;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#25913;&#36827;&#20102;&#20808;&#21069;&#22312;&#24102;&#26377;&#36172;&#24466;&#21453;&#39304;&#30340;&#28216;&#25103;&#20013;&#30340;&#24037;&#20316;&#12290;3. &#25105;&#20204;&#32771;&#34385;&#23616;&#37096;&#30417;&#25511;&#65292;&#36825;&#26159;&#19968;&#31181;&#21453;&#39304;&#31867;&#22411;&#65292;&#20854;&#20013;&#20915;&#31574;&#21046;&#23450;&#32773;&#21482;&#35266;&#23519;&#26234;&#33021;&#20307;&#21160;&#20316;&#30340;&#25688;&#35201;&#20449;&#24687;&#32780;&#19981;&#26159;&#20840;&#37096;&#20449;&#24687;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#35813;&#31639;&#27861;&#23454;&#29616;&#20102;&#27492;&#35774;&#32622;&#30340;&#25910;&#25947;&#36895;&#24230;&#26368;&#20248;&#65292;&#19982;&#20808;&#21069;&#24037;&#20316;&#24314;&#31435;&#30340;&#19979;&#30028;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and how these considerations change as we move from few to many agents. We study this question in a general framework for interactive decision making with multiple agents, encompassing Markov games with function approximation and normal-form games with bandit feedback. We focus on equilibrium computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling multiple agents that interact with an unknown environment. Our main contributions are:  - We provide upper and lower bounds on the optimal sample complexity for multi-agent decision making based on a multi-agent generalization of the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to our setting. Compared to the best results for the sin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#40065;&#26834;&#23398;&#20064;&#65288;ERL&#65289;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#25104;&#26412;&#30340;&#22312;&#32447;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24179;&#22343;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00677</link><description>&lt;p&gt;
&#29992;&#20110;&#20869;&#23384;&#25104;&#26412;&#30340;&#22312;&#32447;&#20248;&#21270;&#30340;&#40065;&#26834;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robustified Learning for Online Optimization with Memory Costs. (arXiv:2305.00677v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#40065;&#26834;&#23398;&#20064;&#65288;ERL&#65289;&#26041;&#27861;&#65292;&#22312;&#20869;&#23384;&#25104;&#26412;&#30340;&#22312;&#32447;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24179;&#22343;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20248;&#21270;&#30340;&#20869;&#23384;&#25104;&#26412;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#20854;&#20013;&#39034;&#24207;&#25805;&#20316;&#22312;&#19981;&#30693;&#36947;&#26410;&#26469;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#20869;&#23384;&#25104;&#26412;&#23558;&#38543;&#26102;&#38388;&#32806;&#21512;&#30340;&#25805;&#20316;&#28155;&#21152;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#21508;&#31181;&#19987;&#23478;&#35774;&#35745;&#30340;&#22312;&#32447;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#23454;&#29616;&#26377;&#30028;&#30340;&#26368;&#22351;&#24773;&#20917;&#31454;&#20105;&#27604;&#65292;&#20294;&#32467;&#26524;&#30340;&#24179;&#22343;&#24615;&#33021;&#36890;&#24120;&#19981;&#20196;&#20154;&#28385;&#24847;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26032;&#20852;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20248;&#21270;&#22120;&#21487;&#20197;&#25552;&#39640;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#26368;&#22351;&#24773;&#20917;&#24615;&#33021;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19987;&#23478;&#40065;&#26834;&#23398;&#20064;&#65288;ERL&#65289;&#26041;&#27861;&#65292;&#26082;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24179;&#22343;&#24615;&#33021;&#21448;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#23454;&#29616;&#40065;&#26834;&#24615;&#65292;ERL&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25237;&#24433;&#31639;&#23376;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#23478;&#22312;&#32447;&#31639;&#27861;&#26469;&#20351;ML&#25805;&#20316;&#20855;&#26377;&#40065;&#26834;&#24615;&#65307;&#20026;&#20102;&#23454;&#29616;&#24179;&#22343;&#24615;&#33021;&#65292;ERL&#20351;&#29992;&#22522;&#20110;&#36882;&#24402;&#32467;&#26500;&#30340;ML&#20248;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#26126;&#30830;
&lt;/p&gt;
&lt;p&gt;
Online optimization with memory costs has many real-world applications, where sequential actions are made without knowing the future input. Nonetheless, the memory cost couples the actions over time, adding substantial challenges. Conventionally, this problem has been approached by various expert-designed online algorithms with the goal of achieving bounded worst-case competitive ratios, but the resulting average performance is often unsatisfactory. On the other hand, emerging machine learning (ML) based optimizers can improve the average performance, but suffer from the lack of worst-case performance robustness. In this paper, we propose a novel expert-robustified learning (ERL) approach, achieving {both} good average performance and robustness. More concretely, for robustness, ERL introduces a novel projection operator that robustifies ML actions by utilizing an expert online algorithm; for average performance, ERL trains the ML optimizer based on a recurrent architecture by explicit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00664</link><description>&lt;p&gt;
&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#22270;&#24418;&#29615;&#22659;&#19979;&#22914;&#20309;&#26377;&#25928;&#22320;&#36827;&#34892;&#36328;&#22270;&#36801;&#31227;&#23398;&#20064;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#39046;&#22495;&#28436;&#21270;&#23545;&#27867;&#21270;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#39046;&#22495;&#20013;&#65292;&#36328;&#22270;&#20256;&#36755;&#30693;&#35782;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#21253;&#25324;&#36816;&#36755;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#37329;&#34701;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#38382;&#39064;&#65306;&#22312;&#21160;&#24577;&#35774;&#32622;&#19979;&#65292;&#32771;&#34385;&#24050;&#35266;&#23519;&#21040;&#30340;&#20855;&#26377;&#26631;&#31614;&#30340;&#28304;&#22270;&#21644;&#26631;&#31614;&#31232;&#30095;&#30340;&#30446;&#26631;&#22270;&#65292;&#22914;&#20309;&#26377;&#25928;&#22320;&#34920;&#24449;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#20559;&#24046;&#65292;&#24182;&#20248;&#21270;&#30446;&#26631;&#22495;&#22312;&#19979;&#19968;&#20010;&#26102;&#38388;&#25139;&#30340;&#27867;&#21270;&#24615;&#33021;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#36328;&#22270;&#21160;&#24577;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#21270;&#30028;&#38480;&#65292;&#36825;&#24847;&#21619;&#30528;&#27867;&#21270;&#24615;&#33021;&#30001;&#39046;&#22495;&#28436;&#21270;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#31354;&#38388;&#65292;&#24182;&#19988;&#23558;&#28608;&#27963;&#20989;&#25968;&#30340;&#35282;&#33394;&#25551;&#36848;&#20026;&#25918;&#22823;&#20989;&#25968;&#65292;&#23558;&#20302;&#32500;&#32447;&#24615;&#31354;&#38388;&#26144;&#23556;&#20026;&#26080;&#38480;&#32500;&#36229;&#32423;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.00663</link><description>&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#19981;&#20877;&#28608;&#27963;&#65306;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#21512;&#29702;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks. (arXiv:2305.00663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#31354;&#38388;&#65292;&#24182;&#19988;&#23558;&#28608;&#27963;&#20989;&#25968;&#30340;&#35282;&#33394;&#25551;&#36848;&#20026;&#25918;&#22823;&#20989;&#25968;&#65292;&#23558;&#20302;&#32500;&#32447;&#24615;&#31354;&#38388;&#26144;&#23556;&#20026;&#26080;&#38480;&#32500;&#36229;&#32423;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#19968;&#20010;&#39640;&#32500;&#31354;&#38388;&#65292;&#20294;&#21364;&#19981;&#33021;&#28165;&#26224;&#22320;&#23450;&#20041;&#36825;&#20010;&#31354;&#38388;&#12290;&#37027;&#20040;&#36825;&#20010;&#31354;&#38388;&#26159;&#20160;&#20040;&#65311;&#23427;&#30340;&#32500;&#25968;&#26159;&#22810;&#23569;&#65311;&#26159;&#21542;&#20855;&#26377;&#26377;&#38480;&#30340;&#32500;&#25968;&#65311;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#29992;&#30340;&#21487;&#34892;&#29702;&#35770;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#32500;&#65288;&#26356;&#31934;&#30830;&#22320;&#65292;&#26159;&#19968;&#20010;&#26080;&#38480;&#32500;&#65289;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#28608;&#27963;&#20989;&#25968;&#20805;&#24403;&#20102;&#19968;&#20010;&#25918;&#22823;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#23558;&#20302;&#32500;&#32447;&#24615;&#31354;&#38388;&#26144;&#23556;&#25104;&#20102;&#19968;&#20010;&#26080;&#38480;&#32500;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers commonly believe that neural networks model a high-dimensional space but cannot give a clear definition of this space. What is this space? What is its dimension? And does it has finite dimensions? In this paper, we develop a plausible theory on interpreting neural networks in terms of the role of activation functions in neural networks and define a high-dimensional (more precisely, an infinite-dimensional) space. We conjunction that the activation function acts as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional space. Given a dataset with each example of $d$ features $f_1$, $f_2$, $\cdots$, $f_d$, we believe that NNs model a special space with infinite dimensions, each of which is a monomial $$\prod_{i_1, i_2, \cdots, i_d} f_1^{i_1} f_2^{i_2} \cdots f_d^{i_d}$$ for some non-negative integers ${i_1, i_2, \cdots, i_d} \in \mathbb{Z}_{0}^{+}=\{0,1,2,3,\ldots\} $. We term such an infinite-dimensional space $\textit{ Super Space (SS)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00654</link><description>&lt;p&gt;
&#20351;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#34920;&#24449;&#23398;&#20064;&#21644;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#21160;&#34920;&#24449;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#33719;&#24471;&#20445;&#30041;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#24182;&#25429;&#25417;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#36866;&#29992;&#20110;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#29616;&#23398;&#20064;&#21644;&#25506;&#32034;&#26159;&#20219;&#20309;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#33719;&#24471;&#20445;&#30041;&#22495;&#20013;&#28508;&#22312;&#36716;&#25442;&#32467;&#26500;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36824;&#25429;&#25417;&#20102;&#29366;&#24577;&#35775;&#38382;&#30340;&#30456;&#23545;&#39057;&#29575;&#65292;&#20174;&#32780;&#20813;&#36153;&#25552;&#20379;&#20102;&#20266;&#35745;&#25968;&#30340;&#20272;&#35745;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#20998;&#35299;&#26041;&#27861;&#25512;&#24191;&#21040;&#22823;&#35268;&#27169;&#22495;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#24314;&#31435;&#36716;&#31227;&#30697;&#38453;&#65292;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#32593;&#32476;&#65292;&#20063;&#20801;&#35768;&#23567;&#25209;&#37327;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#29366;&#24577;&#34920;&#31034;&#20013;&#21560;&#21462;&#28789;&#24863;&#65292;&#24182;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#20998;&#35299;&#26041;&#27861;&#21040;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#12290;&#36890;&#36807;&#23545;&#37096;&#20998;&#21487;&#35266;&#23519;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#35774;&#32622;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#22312;DM-Lab-30&#29615;&#22659;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26694;&#26550;(DISC)&#26469;&#25233;&#21046;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20551;&#30456;&#20851;&#65292;&#36890;&#36807;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#20316;&#20026;&#20551;&#23646;&#24615;&#24178;&#39044;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;DISC&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00650</link><description>&lt;p&gt;
&#21457;&#29616;&#24182;&#26657;&#27491;&#65306;&#27010;&#24565;&#24863;&#30693;&#30340;&#20551;&#30456;&#20851;&#25233;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discover and Cure: Concept-aware Mitigation of Spurious Correlation. (arXiv:2305.00650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26694;&#26550;(DISC)&#26469;&#25233;&#21046;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20551;&#30456;&#20851;&#65292;&#36890;&#36807;&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#20316;&#20026;&#20551;&#23646;&#24615;&#24178;&#39044;&#35757;&#32451;&#25968;&#25454;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;DISC&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20381;&#36182;&#20110;&#20551;&#30456;&#20851;&#26469;&#36827;&#34892;&#39044;&#27979;&#65292;&#36825;&#20250;&#23548;&#33268;&#26080;&#27861;&#36229;&#36234;&#35757;&#32451;&#29615;&#22659;&#30340;&#19968;&#33324;&#21270;&#12290;&#20363;&#22914;&#65292;&#23558;&#29483;&#19982;&#24202;&#20316;&#20026;&#32972;&#26223;&#32852;&#31995;&#30340;&#27169;&#22411;&#65292;&#22312;&#27809;&#26377;&#24202;&#30340;&#20854;&#20182;&#29615;&#22659;&#20013;&#21487;&#33021;&#26080;&#27861;&#39044;&#27979;&#21040;&#29483;&#30340;&#23384;&#22312;&#12290;&#25233;&#21046;&#20551;&#30456;&#20851;&#23545;&#20110;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#32570;&#20047;&#36879;&#26126;&#24230;&#25552;&#20379;&#26377;&#20851;&#25233;&#21046;&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;Discover and Cure (DISC)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20351;&#29992;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#65292;DISC&#36845;&#20195;&#22320; 1)&#21457;&#29616;&#19981;&#31283;&#23450;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;&#20551;&#23646;&#24615;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#65292;&#28982;&#21518; 2)&#20351;&#29992;&#21457;&#29616;&#30340;&#27010;&#24565;&#24178;&#39044;&#35757;&#32451;&#25968;&#25454;&#20197;&#20943;&#23569;&#20551;&#30456;&#20851;&#12290;&#22312;&#31995;&#32479;&#23454;&#39564;&#20013;&#65292;DISC&#25552;&#20379;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20248;&#31168;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#30446;&#26631;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#23427;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure (DISC), to tackle the issue. With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;CNN-LSTM&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#23558;Sentinel-1&#21355;&#26143;&#25968;&#25454;&#21644;MODIS&#25968;&#25454;&#34701;&#21512;&#65292;&#20197;&#25512;&#31639;&#23391;&#21152;&#25289;&#22269;&#21382;&#21490;&#27946;&#27700;&#30340;&#37096;&#20998;&#28153;&#27809;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.00640</link><description>&lt;p&gt;
&#25512;&#31639;&#21382;&#21490;&#27946;&#27700;: &#25972;&#21512;CNN-LSTM&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#34701;&#21512;&#21355;&#26143;&#25968;&#25454;&#36827;&#34892;&#21382;&#21490;&#28153;&#27809;&#22320;&#22270;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Inferring the past: a combined CNN-LSTM deep learning framework to fuse satellites for historical inundation mapping. (arXiv:2305.00640v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;CNN-LSTM&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#23558;Sentinel-1&#21355;&#26143;&#25968;&#25454;&#21644;MODIS&#25968;&#25454;&#34701;&#21512;&#65292;&#20197;&#25512;&#31639;&#23391;&#21152;&#25289;&#22269;&#21382;&#21490;&#27946;&#27700;&#30340;&#37096;&#20998;&#28153;&#27809;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21355;&#26143;&#25968;&#25454;&#32472;&#21046;&#27946;&#27700;&#22320;&#22270;&#23545;&#20110;&#31649;&#29702;&#21644;&#20943;&#36731;&#27946;&#27700;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#12290;&#21355;&#26143;&#22270;&#20687;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#20998;&#26512;&#22823;&#38754;&#31215;&#21306;&#22495;&#65292;&#20026;&#24212;&#24613;&#21709;&#24212;&#21644;&#28798;&#23475;&#31649;&#29702;&#25552;&#20379;&#37325;&#35201;&#20449;&#24687;&#12290;&#20174;&#21355;&#26143;&#22270;&#20687;&#20013;&#33719;&#21462;&#30340;&#21382;&#21490;&#27946;&#27700;&#25968;&#25454;&#21487;&#20197;&#20026;&#38271;&#26399;&#35268;&#21010;&#12289;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#21644;&#19982;&#20445;&#38505;&#30456;&#20851;&#30340;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;Sentinel-1&#21355;&#26143;&#23545;&#20110;&#27946;&#27700;&#30417;&#27979;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23545;&#20110;&#36739;&#38271;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#21487;&#20197;&#23558;&#20854;&#20182;&#21355;&#26143;&#22914;MODIS&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20934;&#30830;&#35782;&#21035;&#21644;&#32472;&#21046;&#36807;&#21435;&#30340;&#27946;&#27700;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#24320;&#21457;&#19968;&#20010;&#25972;&#21512;&#20102;CNN-LSTM&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#27169;&#22411;&#26469;&#23558;Sentinel-1&#20135;&#29983;&#30340;&#37096;&#20998;&#34987;&#28153;&#27809;&#21306;&#22495;&#19982;MODIS&#25968;&#25454;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#20415;&#22312;&#23391;&#21152;&#25289;&#22269;&#25512;&#31639;&#21382;&#21490;&#27946;&#27700;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20351;&#29992;CNN&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#26102;&#31354;&#20449;&#24687;&#26469;&#39044;&#27979;&#37096;&#20998;&#28153;&#27809;&#21306;&#22495;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#21382;&#21490;MODIS&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mapping floods using satellite data is crucial for managing and mitigating flood risks. Satellite imagery enables rapid and accurate analysis of large areas, providing critical information for emergency response and disaster management. Historical flood data derived from satellite imagery can inform long-term planning, risk management strategies, and insurance-related decisions. The Sentinel-1 satellite is effective for flood detection, but for longer time series, other satellites such as MODIS can be used in combination with deep learning models to accurately identify and map past flood events. We here develop a combined CNN--LSTM deep learning framework to fuse Sentinel-1 derived fractional flooded area with MODIS data in order to infer historical floods over Bangladesh. The results show how our framework outperforms a CNN-only approach and takes advantage of not only space, but also time in order to predict the fractional inundated area. The model is applied to historical MODIS data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00633</link><description>&lt;p&gt;
&#20998;&#35299;&#22686;&#24378;&#25512;&#29702;&#30340;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#35299;&#30721;&#25552;&#39640;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#65292;&#20351;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#65307;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#21028;&#26029;&#36923;&#36753;&#38169;&#35823;&#65292;&#25552;&#39640;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#26463;&#25628;&#32034;&#32467;&#21512;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32463;&#36807;&#26657;&#20934;&#30340;&#33258;&#21160;&#26631;&#20934;&#25506;&#32034;&#25512;&#29702;&#25628;&#32034;&#31354;&#38388;&#12290;&#36825;&#20351;&#24471;&#26377;&#25928;&#25628;&#32034;&#33021;&#22815;&#20135;&#29983;&#26356;&#39640;&#36136;&#37327;&#30340;&#26368;&#32456;&#39044;&#27979;&#32467;&#26524;&#12290;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#24341;&#23548;&#30340;&#38543;&#26426;&#26463;&#25628;&#32034;&#65292;&#25105;&#20204;&#22312;&#20135;&#29983;&#25512;&#29702;&#38142;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#24179;&#34913;&#26435;&#34913;&#65292;&#20174;&#32780;&#33021;&#22815;&#36866;&#24212;&#22810;&#25968;&#25237;&#31080;&#65292;&#24182;&#22312;GSM8K&#12289;AQUA&#21644;StrategyQA&#22522;&#20934;&#27979;&#35797;&#20013;&#20197;&#23569;&#37327;&#31034;&#20363;&#20934;&#30830;&#24615;&#20998;&#21035;&#36229;&#36234;&#23545;&#24212;&#30340;Codex-backboned&#22522;&#32447;$6.34\%$&#12289;$9.56\%$&#21644;$5.46\%$&#12290;&#23545;&#25105;&#20204;&#30340;&#20998;&#35299;&#24335;&#25512;&#29702;&#20998;&#26512;&#21457;&#29616;&#65292;&#23427;&#21487;&#20197;&#25351;&#20986;&#36923;&#36753;&#38169;&#35823;&#24182;&#23548;&#33268;&#26356;&#39640;&#30340;&#19968;&#33268;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#35752;&#21644;&#21457;&#23637;&#12290;&#35813;&#32508;&#36848;&#23545;&#20110;&#26032;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22522;&#30784;&#30340;&#20837;&#38376;&#36164;&#28304;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.00624</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Time Series Applications: A Survey. (arXiv:2305.00624v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00624
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#25506;&#35752;&#21644;&#21457;&#23637;&#12290;&#35813;&#32508;&#36848;&#23545;&#20110;&#26032;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#22522;&#30784;&#30340;&#20837;&#38376;&#36164;&#28304;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#21069;&#27839;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23427;&#20204;&#20855;&#26377;&#20986;&#33394;&#30340;&#26679;&#26412;&#29983;&#25104;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#25991;&#26412;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#30340;&#27010;&#24565;&#24050;&#32463;&#25193;&#23637;&#21040;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#65292;&#24182;&#24320;&#21457;&#20986;&#20102;&#35768;&#22810;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#21644;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#36825;&#31687;&#32508;&#36848;&#20316;&#20026;&#26032;&#30740;&#31350;&#20154;&#21592;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#20837;&#38376;&#36164;&#28304;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#29983;&#25104;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#29420;&#31435;&#30340;&#37096;&#20998;&#20013;&#20998;&#21035;&#20171;&#32461;&#23427;&#20204;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#21516;&#19968;&#24212;&#29992;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, a family of generative models based on deep learning, have become increasingly prominent in cutting-edge machine learning research. With a distinguished performance in generating samples that resemble the observed data, diffusion models are widely used in image, video, and text synthesis nowadays. In recent years, the concept of diffusion has been extended to time series applications, and many powerful models have been developed. Considering the deficiency of a methodical summary and discourse on these models, we provide this survey as an elementary resource for new researchers in this area and also an inspiration to motivate future research. For better understanding, we include an introduction about the basics of diffusion models. Except for this, we primarily focus on diffusion-based methods for time series forecasting, imputation, and generation, and present them respectively in three individual sections. We also compare different methods for the same application a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21015;&#22788;&#29702;&#23884;&#20837;&#30697;&#38453;&#21462;&#20195;&#21516;&#34892;&#22788;&#29702;&#65292;&#22312;&#25552;&#39640;&#32467;&#26524;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00623</link><description>&lt;p&gt;
&#29992;&#20110;&#33410;&#28857;&#34920;&#31034;&#30340;&#23545;&#27604;&#23398;&#20064;&#31616;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Simplified Framework for Contrastive Learning for Node Representations. (arXiv:2305.00623v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21015;&#22788;&#29702;&#23884;&#20837;&#30697;&#38453;&#21462;&#20195;&#21516;&#34892;&#22788;&#29702;&#65292;&#22312;&#25552;&#39640;&#32467;&#26524;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#27604;&#23398;&#20064;&#24050;&#32463;&#34987;&#35777;&#26126;&#20026;&#19968;&#31181;&#25552;&#21462;&#20016;&#23500;&#22810;&#26679;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#26377;&#21147;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#24191;&#20041;&#19978;&#35762;&#65292;&#23545;&#27604;&#23398;&#20064;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#29983;&#25104;&#36755;&#20837;&#25968;&#25454;&#30340;&#20004;&#20010;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#24402;&#19968;&#21270;&#28201;&#24230;&#32553;&#25918;&#20132;&#21449;&#29109;&#25439;&#22833;&#65288;NT-Xent&#65289;&#26469;&#23398;&#20064;&#20302;&#32500;&#24230;&#34920;&#31034;&#20197;&#35782;&#21035;&#23545;&#24212;&#20110;&#21516;&#19968;&#21407;&#22987;&#23454;&#20307;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#23545;&#27604;&#23398;&#20064;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#22312;&#22270;&#20013;&#23884;&#20837;&#33410;&#28857;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#21015;&#22788;&#29702;&#23884;&#20837;&#30697;&#38453;&#65292;&#32780;&#19981;&#26159;&#21516;&#34892;&#22788;&#29702;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32467;&#26524;&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#32780;&#21516;&#34892;&#22788;&#29702;&#26159;&#22823;&#22810;&#25968;&#21516;&#34892;&#26041;&#27861;&#37319;&#29992;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#12290;&#36825;&#31181;&#20462;&#25913;&#21487;&#25552;&#39640;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has recently established itself as a powerful self-supervised learning framework for extracting rich and versatile data representations. Broadly speaking, contrastive learning relies on a data augmentation scheme to generate two versions of the input data and learns low-dimensional representations by maximizing a normalized temperature-scaled cross entropy loss (NT-Xent) to identify augmented samples corresponding to the same original entity. In this paper, we investigate the potential of deploying contrastive learning in combination with Graph Neural Networks for embedding nodes in a graph. Specifically, we show that the quality of the resulting embeddings and training time can be significantly improved by a simple column-wise postprocessing of the embedding matrix, instead of the row-wise postprocessing via multilayer perceptrons (MLPs) that is adopted by the majority of peer methods. This modification yields improvements in downstream classification tasks of up 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#22235;&#31181;&#35780;&#20998;&#35268;&#21017;&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#24182;&#19988;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.00621</link><description>&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#30340;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Proper Scoring Rules for Survival Analysis. (arXiv:2305.00621v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#29992;&#20110;&#29983;&#23384;&#20998;&#26512;&#30340;&#22235;&#31181;&#35780;&#20998;&#35268;&#21017;&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#24182;&#19988;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#26159;&#20272;&#35745;&#26410;&#26469;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#12290;&#23613;&#31649;&#26377;&#20851;&#20110;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#22522;&#26412;&#29702;&#35770;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#20854;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24120;&#29992;&#30340;&#22235;&#31181;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#22312;&#29983;&#23384;&#20998;&#26512;&#20013;&#30340;&#25193;&#23637;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#25193;&#23637;&#22312;&#27010;&#29575;&#20998;&#24067;&#20272;&#35745;&#30340;&#31163;&#25955;&#21270;&#31243;&#24230;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#26102;&#26159;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#36825;&#20123;&#25193;&#23637;&#35780;&#20998;&#35268;&#21017;&#30340;&#20272;&#35745;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#23545;&#25968;&#24471;&#20998;&#21644;&#24067;&#33713;&#23572;&#24471;&#20998;&#30340;&#25193;&#23637;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#31227;&#21160;&#24863;&#30693;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#23545;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.00619</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#27963;&#21160;&#34920;&#31034;&#23398;&#20064;&#23545;&#22686;&#37327;&#25968;&#25454;&#30340;&#25506;&#31350;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Activity Representation Learning with Incremental Data: An Empirical Study. (arXiv:2305.00619v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22312;&#31227;&#21160;&#24863;&#30693;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#23545;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#24863;&#30693;&#29615;&#22659;&#20013;&#65292;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#21508;&#31181;&#20256;&#24863;&#22120;&#25345;&#32493;&#29983;&#25104;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#20998;&#26512;&#36825;&#31181;&#19981;&#26029;&#22686;&#21152;&#30340;&#25968;&#25454;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#21253;&#25324;&#33719;&#24471;&#26631;&#27880;&#25968;&#25454;&#30340;&#38480;&#21046;&#21644;&#29615;&#22659;&#30340;&#19981;&#26029;&#21464;&#21270;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#34987;&#21033;&#29992;&#20316;&#20026;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#22686;&#24378;&#20256;&#32479;&#30417;&#30563;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#32570;&#22833;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#25968;&#25454;&#26159;&#36880;&#27493;&#21487;&#29992;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#27969;&#31243;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25552;&#21462;&#20449;&#24687;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#23545;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#26368;&#32456;&#20998;&#31867;&#24615;&#33021;&#19978;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#22823;&#23567;&#12289;&#20998;&#24067;&#21644;&#26469;&#28304;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of mobile sensing environments, various sensors on mobile devices continually generate a vast amount of data. Analyzing this ever-increasing data presents several challenges, including limited access to annotated data and a constantly changing environment. Recent advancements in self-supervised learning have been utilized as a pre-training step to enhance the performance of conventional supervised models to address the absence of labelled datasets. This research examines the impact of using a self-supervised representation learning model for time series classification tasks in which data is incrementally available. We proposed and evaluated a workflow in which a model learns to extract informative features using a corpus of unlabeled time series data and then conducts classification on labelled data using features extracted by the model. We analyzed the effect of varying the size, distribution, and source of the unlabeled data on the final classification performance acro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#21516;&#26102;&#24314;&#31435;&#20102;&#19979;&#38480;&#35823;&#24046;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00608</link><description>&lt;p&gt;
&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#24471;&#20998;&#20272;&#35745;&#21644;&#20445;&#24207;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression. (arXiv:2305.00608v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;RePU&#28608;&#27963;&#20989;&#25968;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#21516;&#26102;&#24314;&#31435;&#20102;&#19979;&#38480;&#35823;&#24046;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#20462;&#27491;&#21518;&#30340;&#24130;&#21333;&#20803;&#65288;RePU&#65289;&#20989;&#25968;&#28608;&#27963;&#30340;&#21487;&#24494;&#20998;&#31070;&#32463;&#32593;&#32476;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RePU&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#23548;&#25968;&#21487;&#20197;&#30001;&#28151;&#21512;&#28608;&#27963;RePU&#32593;&#32476;&#26469;&#34920;&#31034;&#65292;&#24182;&#25512;&#23548;&#20102;&#23548;&#25968;RePU&#32593;&#32476;&#20989;&#25968;&#31867;&#30340;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;&#22312;&#20351;&#29992;RePU&#28608;&#27963;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21516;&#26102;&#36817;&#20284;$C^s$&#24179;&#28369;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#35823;&#24046;&#30028;&#12290;&#27492;&#22806;&#65292;&#24403;&#25968;&#25454;&#20855;&#26377;&#36817;&#20284;&#20302;&#32500;&#25903;&#25345;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#25913;&#36827;&#30340;&#36924;&#36817;&#35823;&#24046;&#30028;&#65292;&#35777;&#26126;&#20102;RePU&#32593;&#32476;&#20943;&#32531;&#32500;&#24230;&#28798;&#38590;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35828;&#26126;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#28145;&#24230;&#24471;&#20998;&#21305;&#37197;&#20272;&#35745;&#22120;(DSME)&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;RePU&#32593;&#32476;&#30340;&#24809;&#32602;&#20445;&#24207;&#22238;&#24402;(PDIR)&#12290;&#25105;&#20204;&#22312;&#20551;&#23450;&#30446;&#26631;&#20989;&#25968;&#23646;&#20110;$C^s$&#24179;&#28369;&#20989;&#25968;&#31867;&#30340;&#24773;&#20917;&#19979;&#20026;DSME&#21644;PDIR&#24314;&#31435;&#38750;&#28176;&#36817;&#36229;&#39069;&#39118;&#38505;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the properties of differentiable neural networks activated by rectified power unit (RePU) functions. We show that the partial derivatives of RePU neural networks can be represented by RePUs mixed-activated networks and derive upper bounds for the complexity of the function class of derivatives of RePUs networks. We establish error bounds for simultaneously approximating $C^s$ smooth functions and their derivatives using RePU-activated deep neural networks. Furthermore, we derive improved approximation error bounds when data has an approximate low-dimensional support, demonstrating the ability of RePU networks to mitigate the curse of dimensionality. To illustrate the usefulness of our results, we consider a deep score matching estimator (DSME) and propose a penalized deep isotonic regression (PDIR) using RePU networks. We establish non-asymptotic excess risk bounds for DSME and PDIR under the assumption that the target functions belong to a class of $C^s$ smooth functions. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26085;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#21644;&#22312;&#32447;&#32858;&#31867;&#12290;&#23454;&#39564;&#20351;&#29992; EMBER &#25968;&#25454;&#38598;&#65292;&#23545;&#26377;&#27969;&#20837;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24471;&#21040;&#20102; 95.33% &#30340;&#24179;&#34913;&#20934;&#30830;&#24230;&#12290;&#22312;&#21097;&#19979;&#30340;&#25968;&#25454;&#20013;&#65292;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#23454;&#29616;&#20102;&#32431;&#24230;&#20174; 47.61% &#21040; 77.68% &#30340;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.00605</link><description>&lt;p&gt;
&#38646;&#26085;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#21644;&#22312;&#32447;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification and Online Clustering of Zero-Day Malware. (arXiv:2305.00605v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26085;&#24694;&#24847;&#36719;&#20214;&#30340;&#20998;&#31867;&#21644;&#22312;&#32447;&#32858;&#31867;&#12290;&#23454;&#39564;&#20351;&#29992; EMBER &#25968;&#25454;&#38598;&#65292;&#23545;&#26377;&#27969;&#20837;&#30340;&#24694;&#24847;&#36719;&#20214;&#26679;&#26412;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#24471;&#21040;&#20102; 95.33% &#30340;&#24179;&#34913;&#20934;&#30830;&#24230;&#12290;&#22312;&#21097;&#19979;&#30340;&#25968;&#25454;&#20013;&#65292;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#23454;&#29616;&#20102;&#32431;&#24230;&#20174; 47.61% &#21040; 77.68% &#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#20135;&#29983;&#22823;&#37327;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#65292;&#25105;&#20204;&#38656;&#35201;&#23558;&#20854;&#19982;&#33391;&#24615;&#26679;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#21040;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#29616;&#26377;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#26159;&#22914;&#20309;&#21457;&#23637;&#65292;&#20197;&#21450;&#22914;&#20309;&#26816;&#26597;&#26032;&#20986;&#29616;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#23545;&#20837;&#20405;&#26679;&#26412;&#36827;&#34892;&#22312;&#32447;&#22788;&#29702;&#65292;&#23558;&#20854;&#20998;&#37197;&#32473;&#29616;&#26377;&#23478;&#26063;&#65292;&#25110;&#22312;&#26032;&#23478;&#26063;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#20351;&#29992; EMBER &#25968;&#25454;&#38598;&#20013;&#30340;&#19971;&#20010;&#27969;&#34892;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#65292;&#20854;&#20013;&#22235;&#20010;&#22312;&#35757;&#32451;&#38598;&#20013;&#65292;&#21478;&#22806;&#19977;&#20010;&#22312;&#27979;&#35797;&#38598;&#20013;&#12290;&#36890;&#36807;&#22810;&#23618;&#24863;&#30693;&#22120;&#30340;&#20998;&#31867;&#24471;&#20998;&#65292;&#25105;&#20204;&#30830;&#23450;&#21738;&#20123;&#26679;&#26412;&#23558;&#34987;&#20998;&#31867;&#65292;&#21738;&#20123;&#23558;&#34987;&#32858;&#31867;&#21040;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#23478;&#26063;&#20013;&#12290;&#25105;&#20204;&#20197;&#24179;&#34913;&#20934;&#30830;&#24230;&#20026; 95.33% &#23545; 97.21% &#30340;&#27969;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#33258;&#32452;&#32455;&#26144;&#23556;&#23545;&#21097;&#20313;&#25968;&#25454;&#36827;&#34892;&#20102;&#32858;&#31867;&#65292;&#23454;&#29616;&#20102;&#32431;&#24230;&#20174;&#22235;&#20010;&#32858;&#31867;&#30340; 47.61% &#21040;&#21313;&#20010;&#32858;&#31867;&#30340; 77.68%&#12290;
&lt;/p&gt;
&lt;p&gt;
A large amount of new malware is constantly being generated, which must not only be distinguished from benign samples, but also classified into malware families. For this purpose, investigating how existing malware families are developed and examining emerging families need to be explored. This paper focuses on the online processing of incoming malicious samples to assign them to existing families or, in the case of samples from new families, to cluster them. We experimented with seven prevalent malware families from the EMBER dataset, with four in the training set and three additional new families in the test set. Based on the classification score of the multilayer perceptron, we determined which samples would be classified and which would be clustered into new malware families. We classified 97.21% of streaming data with a balanced accuracy of 95.33%. Then, we clustered the remaining data using a self-organizing map, achieving a purity from 47.61% for four clusters to 77.68% for ten 
&lt;/p&gt;</description></item><item><title>ISAAC Newton&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36873;&#25321;&#30340;&#20108;&#38454;&#20449;&#24687;&#35843;&#25972;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36873;&#25321;&#25209;&#37327;&#22823;&#23567;&#23567;&#20110;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#24320;&#38144;&#28040;&#22833;&#65292;&#33021;&#22815;&#22312;&#23567;&#25209;&#37327;&#38543;&#26426;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.00604</link><description>&lt;p&gt;
ISAAC Newton&#65306;&#29275;&#39039;&#27861;&#30340;&#22522;&#20110;&#36755;&#20837;&#30340;&#36817;&#20284;&#26354;&#29575;
&lt;/p&gt;
&lt;p&gt;
ISAAC Newton: Input-based Approximate Curvature for Newton's Method. (arXiv:2305.00604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00604
&lt;/p&gt;
&lt;p&gt;
ISAAC Newton&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36873;&#25321;&#30340;&#20108;&#38454;&#20449;&#24687;&#35843;&#25972;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36873;&#25321;&#25209;&#37327;&#22823;&#23567;&#23567;&#20110;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#24320;&#38144;&#28040;&#22833;&#65292;&#33021;&#22815;&#22312;&#23567;&#25209;&#37327;&#38543;&#26426;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ISAAC&#65288;Input-baSed ApproximAte Curvature&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36873;&#25321;&#30340;&#20108;&#38454;&#20449;&#24687;&#26469;&#35843;&#25972;&#26799;&#24230;&#65292;&#24182;&#19988;&#22312;&#25209;&#37327;&#22823;&#23567;&#23567;&#20110;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#28176;&#36817;&#28040;&#22833;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20165;&#22522;&#20110;&#30456;&#24212;&#23618;&#30340;&#36755;&#20837;&#32780;&#19981;&#38656;&#35201;&#23454;&#36136;&#24615;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#65292;&#35745;&#31639;&#20986;&#19968;&#20010;&#33391;&#22909;&#30340;&#35843;&#33410;&#22120;&#26159;&#21487;&#33021;&#30340;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#22312;&#23567;&#25209;&#37327;&#38543;&#26426;&#24773;&#20917;&#19979;&#26377;&#25928;&#35757;&#32451;&#65292;&#36825;&#20351;&#23427;&#19982;&#19968;&#38454;&#20197;&#21450;&#20108;&#38454;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ISAAC (Input-baSed ApproximAte Curvature), a novel method that conditions the gradient using selected second-order information and has an asymptotically vanishing computational overhead, assuming a batch size smaller than the number of neurons. We show that it is possible to compute a good conditioner based on only the input to a respective layer without a substantial computational overhead. The proposed method allows effective training even in small-batch stochastic regimes, which makes it competitive to first-order as well as second-order methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Consolidator &#30340; mergeable adapter with grouped connections for visual adaptation&#65292;&#20419;&#36827;&#20102;&#35270;&#35273; transformer &#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#30340;&#26368;&#26032;&#36716;&#31227;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00603</link><description>&lt;p&gt;
Consolidator: &#34701;&#21512;&#36830;&#25509;&#30340;&#21487;&#21512;&#24182;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation. (arXiv:2305.00603v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Consolidator &#30340; mergeable adapter with grouped connections for visual adaptation&#65292;&#20419;&#36827;&#20102;&#35270;&#35273; transformer &#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#30340;&#26368;&#26032;&#36716;&#31227;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;transformer &#20316;&#20026;&#35270;&#35273;&#29305;&#24449;&#25552;&#21462;&#22120;&#34920;&#29616;&#20986;&#36229;&#36234;&#20256;&#32479;&#21367;&#31215;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273; transformer &#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#20854;&#23481;&#32435;&#22823;&#37327;&#21442;&#25968;&#30340;&#33021;&#21147;&#12290;&#36825;&#23548;&#33268;&#23558;&#22823;&#22411;&#27169;&#22411;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "Consolidator" &#30340;&#34701;&#21512;&#36830;&#25509;&#30340;&#21487;&#21512;&#24182;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#35270;&#35273;&#39046;&#22495;&#30340;&#33258;&#36866;&#24212;&#65292;&#23427;&#20419;&#36827;&#20102;&#35270;&#35273; transformer &#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#23454;&#29616;&#20102;&#22810;&#20010;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#35937;&#26816;&#27979;&#20219;&#21153;&#30340;&#26368;&#26032;&#36716;&#31227;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting large models to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every task and thus easily falls into overfitting, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose cons
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#30340;&#28508;&#22312;&#20998;&#24067;&#26469;&#20195;&#26367;&#36830;&#32493;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#22522;&#20110;&#22522;&#22240;&#30340;&#28508;&#22312;&#32534;&#30721;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#21487;&#23398;&#20064;&#21442;&#25968;&#34920;&#31034;&#22823;&#37327;&#21807;&#19968;&#30340;&#28508;&#22312;&#26679;&#26412;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26032;&#30340;&#30452;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#25506;&#32034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00599</link><description>&lt;p&gt;
StyleGenes: GANs&#30340;&#31163;&#25955;&#39640;&#25928;&#28508;&#22312;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
StyleGenes: Discrete and Efficient Latent Distributions for GANs. (arXiv:2305.00599v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00599
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#25955;&#30340;&#28508;&#22312;&#20998;&#24067;&#26469;&#20195;&#26367;&#36830;&#32493;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#36825;&#31181;&#22522;&#20110;&#22522;&#22240;&#30340;&#28508;&#22312;&#32534;&#30721;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#21487;&#23398;&#20064;&#21442;&#25968;&#34920;&#31034;&#22823;&#37327;&#21807;&#19968;&#30340;&#28508;&#22312;&#26679;&#26412;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26032;&#30340;&#30452;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#25506;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#31163;&#25955;&#28508;&#22312;&#20998;&#24067;&#12290;&#25105;&#20204;&#20174;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#26377;&#38480;&#28508;&#22312;&#32452;&#20013;&#37319;&#26679;&#65292;&#32780;&#19981;&#26159;&#20174;&#36830;&#32493;&#30340;&#20808;&#39564;&#20013;&#32472;&#21046;&#28508;&#22312;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#21442;&#25968;&#21270;&#36825;&#31181;&#20998;&#24067;&#20250;&#23548;&#33268;&#20869;&#23384;&#30340;&#32447;&#24615;&#22686;&#21152;&#65292;&#20197;&#30830;&#20445;&#36275;&#22815;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#29983;&#29289;&#20307;&#20013;&#33719;&#24471;&#28789;&#24863;&#26469;&#35299;&#20915;&#36825;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#28508;&#22312;&#31354;&#38388;&#20998;&#25104;&#19968;&#32452;&#22522;&#22240;&#65292;&#23545;&#20110;&#27599;&#20010;&#22522;&#22240;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22522;&#22240;&#21464;&#20307;&#24211;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#29420;&#31435;&#22320;&#23545;&#27599;&#20010;&#22522;&#22240;&#37319;&#26679;&#21464;&#20307;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#26368;&#32456;&#30340;&#28508;&#22312;&#21521;&#37327;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#23569;&#37327;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#34920;&#31034;&#22823;&#37327;&#30340;&#21807;&#19968;&#28508;&#22312;&#26679;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#22522;&#20110;&#22522;&#22240;&#30340;&#28508;&#22312;&#32534;&#30721;&#20801;&#35768;&#26032;&#30340;&#21644;&#30452;&#35266;&#30340;&#28508;&#22312;&#31354;&#38388;&#25506;&#32034;&#26041;&#27861;&#65292;&#20351;&#24471;&#20174;&#25105;&#20204;&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20013;&#36827;&#34892;&#26465;&#20214;&#37319;&#26679;&#31867;&#20284;&#20110;&#21019;&#24314;&#35282;&#33394;&#30340;&#20114;&#21160;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a discrete latent distribution for Generative Adversarial Networks (GANs). Instead of drawing latent vectors from a continuous prior, we sample from a finite set of learnable latents. However, a direct parametrization of such a distribution leads to an intractable linear increase in memory in order to ensure sufficient sample diversity. We address this key issue by taking inspiration from the encoding of information in biological organisms. Instead of learning a separate latent vector for each sample, we split the latent space into a set of genes. For each gene, we train a small bank of gene variants. Thus, by independently sampling a variant for each gene and combining them into the final latent vector, our approach can represent a vast number of unique latent samples from a compact set of learnable parameters. Interestingly, our gene-inspired latent encoding allows for new and intuitive approaches to latent-space exploration, enabling conditional sampling from our uncondit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CONAIM&#27169;&#22411;&#30340;&#35748;&#30693;&#20195;&#29702;&#65292;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#31243;&#24207;&#65292;&#36890;&#36807;&#22686;&#21152;&#26032;&#21151;&#33021;&#26469;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#30340;&#21333;&#19968;&#31243;&#24207;&#23398;&#20064;&#26426;&#21046;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#29289;&#20307;&#36319;&#36394;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.00597</link><description>&lt;p&gt;
&#35748;&#30693;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22686;&#37327;&#31243;&#24207;&#21644;&#24863;&#30693;&#21160;&#20316;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Incremental procedural and sensorimotor learning in cognitive humanoid robots. (arXiv:2305.00597v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CONAIM&#27169;&#22411;&#30340;&#35748;&#30693;&#20195;&#29702;&#65292;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#31243;&#24207;&#65292;&#36890;&#36807;&#22686;&#21152;&#26032;&#21151;&#33021;&#26469;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22686;&#24378;&#23398;&#20064;&#30340;&#21333;&#19968;&#31243;&#24207;&#23398;&#20064;&#26426;&#21046;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#29289;&#20307;&#36319;&#36394;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23398;&#20064;&#26085;&#30410;&#22797;&#26434;&#21160;&#20316;&#21644;&#34892;&#20026;&#30340;&#33021;&#21147;&#26159;&#33258;&#20027;&#31995;&#32479;&#30340;&#38271;&#26399;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#21463;Jean Piaget&#24863;&#30693;&#21160;&#20316;&#19977;&#20010;&#23376;&#38454;&#27573;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CONAIM&#65288;&#24847;&#35782;&#27880;&#24847;&#21147;&#38598;&#25104;&#27169;&#22411;&#65289;&#30340;&#35748;&#30693;&#20195;&#29702;&#65292;&#33021;&#22815;&#36880;&#27493;&#23398;&#20064;&#31243;&#24207;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27599;&#20010;&#23376;&#38454;&#27573;&#38656;&#35201;&#30340;&#35748;&#30693;&#21151;&#33021;&#20197;&#21450;&#22914;&#20309;&#28155;&#21152;&#26032;&#21151;&#33021;&#26469;&#35299;&#20915;&#20195;&#29702;&#20808;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;Cognitive Systems Toolkit&#65288;CST&#65289;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;&#22686;&#24378;&#23398;&#20064;&#30340;&#21333;&#19968;&#31243;&#24207;&#23398;&#20064;&#26426;&#21046;&#23545;&#20154;&#24418;&#26426;&#22120;&#20154;&#36827;&#34892;&#23454;&#39564;&#65292;&#25191;&#34892;&#29289;&#20307;&#36319;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to automatically learn movements and behaviors of increasing complexity is a long-term goal in autonomous systems. Indeed, this is a very complex problem that involves understanding how knowledge is acquired and reused by humans as well as proposing mechanisms that allow artificial agents to reuse previous knowledge. Inspired by Jean Piaget's theory's first three sensorimotor substages, this work presents a cognitive agent based on CONAIM (Conscious Attention-Based Integrated Model) that can learn procedures incrementally. Throughout the paper, we show the cognitive functions required in each substage and how adding new functions helps address tasks previously unsolved by the agent. Experiments were conducted with a humanoid robot in a simulated environment modeled with the Cognitive Systems Toolkit (CST) performing an object tracking task. The system is modeled using a single procedural learning mechanism based on Reinforcement Learning. The increasing agent's cognitive co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.00595</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection. (arXiv:2305.00595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#19977;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#35780;&#20272;&#65292;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#20154;&#20026;&#24178;&#39044;&#21644;&#39046;&#22495;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#38750;&#24120;&#26377;&#20215;&#20540;&#12290;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#20837;&#20102;&#20960;&#31181;&#36825;&#26679;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#20165;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#24211;&#30340;&#21457;&#23637;&#65292;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#24211;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#23578;&#19981;&#28165;&#26970;&#65292;&#22240;&#20026;&#27809;&#26377;&#36825;&#26679;&#30340;&#35780;&#20272;&#12290;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#24211;&#26469;&#23454;&#29616;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#33021;&#19981;&#33021;&#23637;&#29616;&#20986;&#35813;&#26041;&#27861;&#30340;&#30495;&#23454;&#24615;&#33021;&#65292;&#36825;&#21487;&#33021;&#20250;&#35823;&#23548;&#29992;&#25143;&#30456;&#20449;&#26576;&#31181;&#26041;&#27861;&#27604;&#21478;&#19968;&#31181;&#26356;&#22909;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#19977;&#20010;&#30693;&#21517;&#28145;&#24230;&#23398;&#20064;&#24211;&#20013;&#23454;&#29616;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#32467;&#26524;&#65292;&#26469;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#24211;&#23545;&#22312;&#32447;&#33258;&#36866;&#24212;&#36731;&#37327;&#32423;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing online adaptive lightweight time series anomaly detection without human intervention and domain knowledge is highly valuable. Several such anomaly detection approaches have been introduced in the past years, but all of them were only implemented in one deep learning library. With the development of deep learning libraries, it is unclear how different deep learning libraries impact these anomaly detection approaches since there is no such evaluation available. Randomly choosing a deep learning library to implement an anomaly detection approach might not be able to show the true performance of the approach. It might also mislead users in believing one approach is better than another. Therefore, in this paper, we investigate the impact of deep learning libraries on online adaptive lightweight time series anomaly detection by implementing two state-of-the-art anomaly detection approaches in three well-known deep learning libraries and evaluating how these two approaches are indiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#25361;&#25112;&#24615;&#24773;&#26223;&#65292;&#21363;&#20165;&#20855;&#22791;API&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;API&#36827;&#34892;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#23545;&#25512;&#29702;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.00593</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#26080;&#26799;&#24230;&#21644;&#26080;&#20284;&#28982;&#23545;&#35805;&#24335;&#24314;&#27169;API&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;&#25361;&#25112;&#24615;&#24773;&#26223;&#65292;&#21363;&#20165;&#20855;&#22791;API&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#24314;&#27169;API&#36827;&#34892;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#23545;&#25512;&#29702;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#38544;&#31169;&#25110;&#21830;&#19994;&#38480;&#21046;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#24120;&#20316;&#20026;&#40657;&#30418;API&#25552;&#20379;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#26082;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#20063;&#26080;&#27861;&#36890;&#36807;&#23427;&#20256;&#25773;&#26799;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;&#21482;&#26377;API&#35775;&#38382;&#26435;&#38480;&#30340;PLM&#30340;&#33258;&#36866;&#24212;&#25216;&#26415;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26368;&#36817;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#24037;&#20316;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22312;&#19981;&#38656;&#35201;&#35745;&#31639;&#26799;&#24230;&#30340;&#24773;&#20917;&#19979;&#35843;&#25972;&#36719;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20123;&#25216;&#26415;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;PLM&#38500;&#20102;&#36755;&#20837;&#23884;&#20837;&#20043;&#22806;&#30340;&#20219;&#20309;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25552;&#31034;&#30340;&#20998;&#24067;&#65292;&#32780;&#19981;&#26159;&#21333;&#19968;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#23545;&#25512;&#29702;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#37327;&#21270;&#65292;&#36825;&#26159;&#22312;&#20165;&#20855;&#26377;API&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#32771;&#34385;&#25552;&#31034;&#19981;&#30830;&#23450;&#24615;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20180;&#32454;&#26816;&#26597;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#26799;&#24230;&#21644;&#22522;&#20110;&#20284;&#28982;&#30340;&#26041;&#27861;&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36923;&#36753;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#21512;&#25104;&#26694;&#26550;&#65292;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;&#32422;&#26463;&#21644;&#26368;&#20248;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.00576</link><description>&lt;p&gt;
&#23398;&#20064;&#26410;&#30693;&#26102;&#38388;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Joint Learning of Policy with Unknown Temporal Constraints for Safe Reinforcement Learning. (arXiv:2305.00576v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00576
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;&#35268;&#33539;&#21512;&#25104;&#26694;&#26550;&#65292;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;&#32422;&#26463;&#21644;&#26368;&#20248;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#23433;&#20840;&#32422;&#26463;&#24448;&#24448;&#26159;&#26410;&#30693;&#30340;&#25110;&#32773;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#21516;&#26102;&#23398;&#20064;&#23433;&#20840;&#32422;&#26463;&#21644;&#26368;&#20248;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#24182;&#19988;&#26377;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36923;&#36753;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#21512;&#25104;&#20449;&#21495;&#26102;&#24577;&#36923;&#36753;(STL)&#35268;&#33539;&#12290;&#35813;&#26694;&#26550;&#26159;&#30001;&#23450;&#29702;&#25903;&#25345;&#30340;&#65292;&#36825;&#20123;&#23450;&#29702;&#24314;&#31435;&#20102;&#25105;&#20204;&#30340;&#32852;&#21512;&#23398;&#20064;&#36807;&#31243;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21457;&#29616;&#31574;&#30053;&#19982;&#30495;&#27491;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#28436;&#31034;&#25105;&#20204;&#29702;&#35770;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#30340;&#21516;&#26102;&#65292;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#21487;&#25509;&#21463;&#30340;&#23433;&#20840;&#32422;&#26463;&#21644;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world applications, safety constraints for reinforcement learning (RL) algorithms are either unknown or not explicitly defined. We propose a framework that concurrently learns safety constraints and optimal RL policies in such environments, supported by theoretical guarantees. Our approach merges a logically-constrained RL algorithm with an evolutionary algorithm to synthesize signal temporal logic (STL) specifications. The framework is underpinned by theorems that establish the convergence of our joint learning process and provide error bounds between the discovered policy and the true optimal policy. We showcased our framework in grid-world environments, successfully identifying both acceptable safety constraints and RL policies while demonstrating the effectiveness of our theorems in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;MORL&#35774;&#32622;&#21644;&#19968;&#20010;&#29992;&#20110;&#31163;&#32447;MORL&#30340;&#31639;&#27861;PEDA&#12290;PEDA&#36890;&#36807;&#22312;&#22810;&#20010;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#26469;&#23454;&#29616;&#27491;&#20132;&#20559;&#22909;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PEDA&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.00567</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#24085;&#32047;&#25176;&#26377;&#25928;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL. (arXiv:2305.00567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;MORL&#35774;&#32622;&#21644;&#19968;&#20010;&#29992;&#20110;&#31163;&#32447;MORL&#30340;&#31639;&#27861;PEDA&#12290;PEDA&#36890;&#36807;&#22312;&#22810;&#20010;&#27169;&#22411;&#20013;&#36873;&#25321;&#26368;&#20248;&#27169;&#22411;&#26469;&#23454;&#29616;&#27491;&#20132;&#20559;&#22909;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PEDA&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#21487;&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;MORL&#65289;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#33021;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#31454;&#20105;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20195;&#29702;&#23545;&#30446;&#26631;&#30340;&#20559;&#22909;&#21487;&#33021;&#19981;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#22312;&#27979;&#35797;&#26102;&#33021;&#22815;&#36866;&#24212;&#20219;&#24847;&#20559;&#22909;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#31163;&#32447;MORL&#35774;&#32622;&#65292;&#25105;&#20204;&#24076;&#26395;&#21482;&#20351;&#29992;&#26377;&#38480;&#30340;&#31163;&#32447;&#28436;&#31034;&#25968;&#25454;&#38598;&#26469;&#23398;&#20064;&#19968;&#20010;&#20559;&#22909;&#19981;&#25935;&#24863;&#30340;&#31574;&#30053;&#20195;&#29702;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#31532;&#19968;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D4MORL&#65292;&#36825;&#26159;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#31163;&#32447;&#35774;&#32622;&#35774;&#35745;&#30340;MORL&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;180&#19975;&#20010;&#25968;&#25454;&#65292;&#26159;&#36890;&#36807;&#22312;6&#20010;MuJoCo&#29615;&#22659;&#20013;&#20248;&#21270;2-3&#20010;&#30446;&#26631;&#30340;&#21442;&#32771;&#31574;&#30053;&#30340;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24085;&#32047;&#25176;&#26377;&#25928;&#20915;&#31574;&#20195;&#29702;&#65288;PEDA&#65289;&#65292;&#23427;&#26159;&#19968;&#26063;&#31163;&#32447;MORL&#31639;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#25193;&#23637;&#20915;&#31574;&#36716;&#31227;&#65288;DT&#65289;&#26041;&#27861;&#26469;&#36866;&#29992;&#20110;&#26032;&#30340;MORL&#35774;&#32622;&#12290; PEDS&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#20026;&#19981;&#21516;&#30340;&#12289;&#38543;&#26426;&#36873;&#25321;&#30340;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;PEDA&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#25351;&#23450;&#20559;&#22909;&#30340;&#21407;&#21017;&#26469;&#36873;&#25321;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;MuJoCo&#29615;&#22659;&#19979;&#35780;&#20272;&#20102;PEDA&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;PEDA&#22312;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MORL&#26041;&#27861;&#65292;&#24182;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#26356;&#22797;&#26434;&#30340;&#20855;&#26377;&#36739;&#22823;&#34892;&#21160;&#31354;&#38388;&#30340;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Tr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#8220;&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;&#8221;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#35843;&#25972;&#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2305.00562</link><description>&lt;p&gt;
&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Class-Balancing Diffusion Models. (arXiv:2305.00562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#8220;&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;&#8221;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#35843;&#25972;&#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35270;&#35273;&#25968;&#25454;&#21516;&#26102;&#20445;&#25345;&#26356;&#22909;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#26377;&#20248;&#21183;&#12290;&#20294;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#21482;&#36866;&#29992;&#20110;&#31574;&#21010;&#22909;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#21363;&#25968;&#25454;&#26679;&#26412;&#32463;&#36807;&#31934;&#24515;&#22788;&#29702;&#20197;&#22312;&#26631;&#31614;&#26041;&#38754;&#22343;&#21248;&#20998;&#24067;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#38271;&#23614;&#25968;&#25454;&#20998;&#24067;&#20284;&#20046;&#26356;&#20026;&#26222;&#36941;&#65292;&#32780;&#25193;&#25955;&#27169;&#22411;&#22312;&#36825;&#31181;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36824;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35266;&#23519;&#21040;&#24403;&#25193;&#25955;&#27169;&#22411;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#26174;&#33879;&#38477;&#20302;&#12290;&#23588;&#20854;&#26159;&#22312;&#23614;&#37096;&#31867;&#21035;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#20005;&#37325;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20005;&#37325;&#30340;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#19981;&#26159;&#31867;&#24179;&#34913;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#24179;&#34913;&#25193;&#25955;&#27169;&#22411;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;&#29992;&#20998;&#24067;&#35843;&#25972;&#27491;&#21017;&#21270;&#22120;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies. However, such observation is only justified with curated data distribution, where the data samples are nicely pre-processed to be uniformly distributed in terms of their labels. In practice, a long-tailed data distribution appears more common and how diffusion models perform on such class-imbalanced data remains unknown. In this work, we first investigate this problem and observe significant degradation in both diversity and fidelity when the diffusion model is trained on datasets with class-imbalanced distributions. Especially in tail classes, the generations largely lose diversity and we observe severe mode-collapse issues. To tackle this problem, we set from the hypothesis that the data distribution is not class-balanced, and propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer as a solution. E
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00557</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Collective Relational Inference for learning physics-consistent heterogeneous particle interactions. (arXiv:2305.00557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#24322;&#36136;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#30340;&#38598;&#20307;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#22312;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#20316;&#29992;&#31890;&#23376;&#31995;&#32479;&#22312;&#33258;&#28982;&#30028;&#21644;&#24037;&#31243;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#25581;&#31034;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#23450;&#24459;&#20855;&#26377;&#22522;&#26412;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#24213;&#23618;&#37197;&#32622;&#22797;&#26434;&#24615;&#32780;&#20855;&#26377;&#26497;&#22823;&#30340;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#21457;&#29616;&#21516;&#36136;&#31995;&#32479;&#31890;&#23376;&#36712;&#36857;&#20013;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26080;&#27861;&#25581;&#31034;&#24322;&#36136;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#32780;&#36825;&#31181;&#31995;&#32479;&#22312;&#29616;&#23454;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20854;&#20013;&#22810;&#20010;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#21516;&#26102;&#23384;&#22312;&#65292;&#24182;&#19988;&#38656;&#35201;&#20851;&#31995;&#25512;&#26029;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#26041;&#27861;&#29992;&#20110;&#20851;&#31995;&#25512;&#26029;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#24449;&#65306;&#39318;&#20808;&#65292;&#23427;&#38598;&#20307;&#22320;&#25512;&#26029;&#19981;&#21516;&#36793;&#30340;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65307;&#20854;&#27425;&#65292;&#23427;&#20351;&#29992;&#29289;&#29702;&#24863;&#24212;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#20855;&#26377;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#25104;&#23545;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#25512;&#26029;&#30340;&#30456;&#20114;&#20316;&#29992;&#20934;&#30830;&#24615;&#21644;&#20445;&#25345;&#29289;&#29702;&#20445;&#30495;&#24230;&#26041;&#38754;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23450;&#20102;&#20855;&#26377;&#37325;&#35201;&#29289;&#29702;&#24847;&#20041;&#30340;&#26032;&#22411;&#30456;&#20114;&#20316;&#29992;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#32479;&#27835;&#31995;&#32479;&#30340;&#38544;&#34255;&#29289;&#29702;&#21407;&#29702;&#65292;&#24182;&#22312;&#25552;&#39640;&#29289;&#29702;&#24615;&#36136;&#30340;&#39044;&#27979;&#26041;&#38754;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interacting particle systems are ubiquitous in nature and engineering. Revealing particle interaction laws is of fundamental importance but also particularly challenging due to underlying configurational complexities. Recently developed machine learning methods show great potential in discovering pairwise interactions from particle trajectories in homogeneous systems. However, they fail to reveal interactions in heterogeneous systems that are prevalent in reality, where multiple interaction types coexist simultaneously and relational inference is required. Here, we propose a novel probabilistic method for relational inference, which possesses two distinctive characteristics compared to existing methods. First, it infers the interaction types of different edges collectively, and second, it uses a physics-induced graph neural network to learn physics-consistent pairwise interactions. We evaluate the proposed methodology across several benchmark datasets and demonstrate that it is consist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25913;&#36827;&#36807;&#21435;&#30340;&#35270;&#35273;&#37325;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#23567;&#32452;&#22270;&#20687;&#30340;&#37319;&#26679;&#21644;&#32534;&#30721;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#39640;&#36136;&#37327;&#12289;&#20445;&#30041;&#35821;&#20041;&#20869;&#23481;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#35270;&#35273;&#30382;&#23618;&#19981;&#21516;&#21306;&#22495;&#30340;&#37325;&#24314;&#26102;&#38388;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.00556</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#23548;&#38543;&#26426;&#25628;&#32034;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#37325;&#24314;&#35270;&#35273;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Reconstructing seen images from human brain activity via guided stochastic search. (arXiv:2305.00556v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#25913;&#36827;&#36807;&#21435;&#30340;&#35270;&#35273;&#37325;&#24314;&#31639;&#27861;&#65292;&#36890;&#36807;&#23545;&#19968;&#23567;&#32452;&#22270;&#20687;&#30340;&#37319;&#26679;&#21644;&#32534;&#30721;&#27169;&#22411;&#30340;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#20174;&#20154;&#33041;&#27963;&#21160;&#20013;&#39640;&#36136;&#37327;&#12289;&#20445;&#30041;&#35821;&#20041;&#20869;&#23481;&#30340;&#37325;&#24314;&#32467;&#26524;&#65292;&#24182;&#21457;&#29616;&#20102;&#35270;&#35273;&#30382;&#23618;&#19981;&#21516;&#21306;&#22495;&#30340;&#37325;&#24314;&#26102;&#38388;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#37325;&#24314;&#31639;&#27861;&#26159;&#19968;&#31181;&#23558;&#33041;&#27963;&#21160;&#26144;&#23556;&#21040;&#20687;&#32032;&#30340;&#35299;&#37322;&#24037;&#20855;&#12290;&#36807;&#21435;&#30340;&#37325;&#24314;&#31639;&#27861;&#37319;&#29992;&#22823;&#35268;&#27169;&#24211;&#30340;&#26292;&#21147;&#25628;&#32034;&#26469;&#36873;&#25321;&#20505;&#36873;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#33041;&#27963;&#21160;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#26469;&#25193;&#23637;&#21644;&#25913;&#36827;&#36825;&#31181;&#22522;&#20110;&#25628;&#32034;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#22823;&#37096;&#20998;&#35270;&#35273;&#30382;&#23618;&#30340;&#20307;&#32032;&#20013;&#20174;&#20154;&#33041;&#27963;&#21160;&#65288;7T fMRI&#65289;&#35299;&#30721;&#20986;&#35821;&#20041;&#25551;&#36848;&#31526;&#65292;&#28982;&#21518;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#22312;&#27492;&#25551;&#36848;&#31526;&#30340;&#26465;&#20214;&#19979;&#23545;&#19968;&#23567;&#32452;&#22270;&#20687;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#23558;&#27599;&#20010;&#26679;&#26412;&#36890;&#36807;&#32534;&#30721;&#27169;&#22411;&#65292;&#36873;&#25321;&#26368;&#33021;&#20934;&#30830;&#39044;&#27979;&#33041;&#27963;&#21160;&#30340;&#22270;&#20687;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#22270;&#20687;&#26469;&#31181;&#23376;&#21478;&#19968;&#20010;&#24211;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#36807;&#31243;&#36890;&#36807;&#22312;&#36845;&#20195;&#20013;&#32454;&#21270;&#20302;&#32423;&#22270;&#20687;&#32454;&#33410;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#20869;&#23481;&#32780;&#25910;&#25947;&#21040;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25910;&#25947;&#25152;&#38656;&#30340;&#26102;&#38388;&#22312;&#35270;&#35273;&#30382;&#23618;&#20013;&#26377;&#31995;&#32479;&#24046;&#24322;&#65292;&#34920;&#26126;&#20102;&#33041;&#21306;&#30340;&#39640;&#23618;&#25277;&#35937;&#27010;&#24565;&#38656;&#35201;&#26356;&#38271;&#30340;&#26102;&#38388;&#26469;&#38598;&#25104;&#21644;&#21453;&#26144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succi
&lt;/p&gt;</description></item><item><title>MD-Manifold&#37319;&#29992;&#21307;&#23398;&#36317;&#31163;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12289;&#27010;&#24565;&#36317;&#31163;&#24230;&#37327;&#21644;&#24739;&#32773;-&#24739;&#32773;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#23558;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#21069;&#25968;&#25454;&#20449;&#24687;&#32435;&#20837;&#32771;&#34385;&#65292;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#20998;&#26512;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.00553</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#36317;&#31163;&#30340;&#23398;&#20064;&#26041;&#27861;&#65306;&#21307;&#23398;&#27010;&#24565;&#21644;&#24739;&#32773;&#34920;&#31034;&#30340;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MD-Manifold: A Medical-Distance-Based Representation Learning Approach for Medical Concept and Patient Representation. (arXiv:2305.00553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00553
&lt;/p&gt;
&lt;p&gt;
MD-Manifold&#37319;&#29992;&#21307;&#23398;&#36317;&#31163;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12289;&#27010;&#24565;&#36317;&#31163;&#24230;&#37327;&#21644;&#24739;&#32773;-&#24739;&#32773;&#32593;&#32476;&#65292;&#26377;&#25928;&#22320;&#23558;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#21069;&#25968;&#25454;&#20449;&#24687;&#32435;&#20837;&#32771;&#34385;&#65292;&#25552;&#39640;&#21307;&#30103;&#20445;&#20581;&#20998;&#26512;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#34920;&#31034;&#21307;&#23398;&#27010;&#24565;&#21644;&#24739;&#32773;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#20998;&#26512;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#34920;&#31034;&#21307;&#23398;&#27010;&#24565;&#38656;&#35201;&#23558;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#24739;&#32773;&#25551;&#36848;&#25968;&#25454;&#30340;&#20808;&#21069;&#20449;&#24687;&#32435;&#20837;&#32771;&#34385;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;&#29305;&#24449;&#24037;&#31243;&#21644;&#23558;&#21307;&#23398;&#27010;&#24565;&#26144;&#23556;&#21040;&#26631;&#20934;&#21270;&#26415;&#35821;&#20013;&#65292;&#23384;&#22312;&#25429;&#33719;&#26469;&#33258;&#24739;&#32773;&#25551;&#36848;&#25968;&#25454;&#30340;&#21160;&#24577;&#27169;&#24335;&#30340;&#23616;&#38480;&#24615;&#12290;&#20854;&#20182;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#22312;&#34701;&#21512;&#37325;&#35201;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#23545;&#20110;&#22823;&#22810;&#25968;&#21307;&#30103;&#20445;&#20581;&#31995;&#32479;&#26469;&#35828;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;MD-Manifold&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#27010;&#24565;&#21644;&#24739;&#32773;&#34920;&#31034;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12289;&#27010;&#24565;&#36317;&#31163;&#24230;&#37327;&#21644;&#24739;&#32773;-&#24739;&#32773;&#32593;&#32476;&#65292;&#20197;&#32435;&#20837;&#20851;&#38190;&#30340;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#20808;&#21069;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#23427;&#37319;&#29992;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#33391;&#22909;&#30340;&#21307;&#23398;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MD-Manifold&#22312;&#25913;&#36827;&#30142;&#30149;&#35786;&#26029;&#21644;&#24739;&#32773;&#32858;&#31867;&#31561;&#21307;&#30103;&#20445;&#20581;&#20998;&#26512;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effectively representing medical concepts and patients is important for healthcare analytical applications. Representing medical concepts for healthcare analytical tasks requires incorporating medical domain knowledge and prior information from patient description data. Current methods, such as feature engineering and mapping medical concepts to standardized terminologies, have limitations in capturing the dynamic patterns from patient description data. Other embedding-based methods have difficulties in incorporating important medical domain knowledge and often require a large amount of training data, which may not be feasible for most healthcare systems. Our proposed framework, MD-Manifold, introduces a novel approach to medical concept and patient representation. It includes a new data augmentation approach, concept distance metric, and patient-patient network to incorporate crucial medical domain knowledge and prior data information. It then adapts manifold learning methods to gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#23454;&#29992;&#35780;&#20272;&#8221;&#30340;&#27010;&#24565;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#35780;&#20272;ML&#26041;&#27861;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65288;NID&#65289;&#20013;&#30340;&#23454;&#38469;&#20215;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#25351;&#23548;&#26041;&#38024;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#23454;&#38469;&#31995;&#32479;&#30340;&#23454;&#38469;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#36739;&#19981;&#21516;ML&#26041;&#27861;&#22312;NID&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.00550</link><description>&lt;p&gt;
SoK&#65306;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#29992;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SoK: Pragmatic Assessment of Machine Learning for Network Intrusion Detection. (arXiv:2305.00550v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#23454;&#29992;&#35780;&#20272;&#8221;&#30340;&#27010;&#24565;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#35780;&#20272;ML&#26041;&#27861;&#22312;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65288;NID&#65289;&#20013;&#30340;&#23454;&#38469;&#20215;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#32452;&#25351;&#23548;&#26041;&#38024;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#23454;&#38469;&#31995;&#32479;&#30340;&#23454;&#38469;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#36739;&#19981;&#21516;ML&#26041;&#27861;&#22312;NID&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#30340;&#26377;&#20215;&#20540;&#36164;&#20135;&#12290;&#20294;&#26159;&#65292;&#23545;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#65288;NID&#65289;&#65292;&#31185;&#23398;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#36827;&#23637;&#20173;&#28982;&#21463;&#21040;&#20174;&#19994;&#20154;&#21592;&#30340;&#24576;&#30097;&#12290;&#36825;&#31181;&#33073;&#33410;&#26159;&#30001;&#20110;&#30740;&#31350;&#35770;&#25991;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#24341;&#36215;&#30340;&#65292;&#20854;&#20013;&#35768;&#22810;&#35770;&#25991;&#20027;&#35201;&#26088;&#22312;&#23637;&#31034;&#26032;&#26041;&#27861;&#8220;&#32988;&#36807;&#8221;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#37096;&#32626;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;NID&#26469;&#35828;&#65292;ML&#30340;&#20215;&#20540;&#21462;&#20915;&#20110;&#35768;&#22810;&#22240;&#32032;&#65292;&#22914;&#30828;&#20214;&#65292;&#36825;&#20123;&#22240;&#32032;&#24120;&#24120;&#34987;&#31185;&#23398;&#25991;&#29486;&#25152;&#24573;&#30053;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#8220;&#25913;&#21464;&#8221;&#30740;&#31350;&#20013;&#37319;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20943;&#23569;&#20174;&#19994;&#32773;&#23545;NID&#30340;ML&#30340;&#24576;&#30097;&#12290;&#22312;&#38416;&#26126;&#24433;&#21709;ML&#22312;NID&#20013;&#23454;&#38469;&#37096;&#32626;&#30340;&#8220;&#22240;&#32032;&#8221;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#23454;&#29992;&#35780;&#20272;&#8221;&#30340;&#27010;&#24565;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#35780;&#20272;ML&#26041;&#27861;&#22312;NID&#20013;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ML&#22312;NID&#30340;&#26368;&#26032;&#30740;&#31350;&#29366;&#24577;&#23384;&#22312;&#20960;&#20010;&#19981;&#21305;&#37197;&#20043;&#22788;&#65292;&#20854;&#20013;&#25552;&#20986;&#30340;&#20551;&#35774;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#35201;&#27714;&#20998;&#27495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;ML NID&#26368;&#26032;&#30740;&#31350;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#35828;&#26126;&#30740;&#31350;&#20013;&#20316;&#20986;&#30340;&#20551;&#35774;&#19982;&#29616;&#23454;&#19990;&#30028;&#35201;&#27714;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#25351;&#23548;&#26041;&#38024;&#65292;&#29992;&#20110;&#35774;&#35745;&#23454;&#39564;&#65292;&#21487;&#20197;&#22312;&#32771;&#34385;&#23454;&#38469;&#31995;&#32479;&#30340;&#23454;&#38469;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#36739;&#19981;&#21516;ML&#26041;&#27861;&#22312;NID&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#30495;&#23454;NID&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#25105;&#20204;&#30340;&#25351;&#21335;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become a valuable asset to solve many real-world tasks. For Network Intrusion Detection (NID), however, scientific advances in ML are still seen with skepticism by practitioners. This disconnection is due to the intrinsically limited scope of research papers, many of which primarily aim to demonstrate new methods ``outperforming'' prior work -- oftentimes overlooking the practical implications for deploying the proposed solutions in real systems. Unfortunately, the value of ML for NID depends on a plethora of factors, such as hardware, that are often neglected in scientific literature.  This paper aims to reduce the practitioners' skepticism towards ML for NID by "changing" the evaluation methodology adopted in research. After elucidating which "factors" influence the operational deployment of ML in NID, we propose the notion of "pragmatic assessment", which enable practitioners to gauge the real value of ML methods for NID. Then, we show that the state-of-res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;</title><link>http://arxiv.org/abs/2305.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#31946;&#20998;&#31665;&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#24448;&#24448;&#20250;&#36807;&#20110;&#33258;&#20449;&#65292;&#20854;&#21407;&#22987;&#32467;&#26524;&#30340;&#27010;&#29575;&#24182;&#19981;&#31526;&#21512;&#30495;&#23454;&#30340;&#20915;&#31574;&#27010;&#29575;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#26159;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20808;&#21069;&#30340;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#20027;&#35201;&#21033;&#29992;&#28165;&#26224;&#30340;&#20998;&#31665;&#25104;&#21592;&#36164;&#26684;&#24230;&#37327;&#12290;&#36825;&#21152;&#21095;&#20102;&#27169;&#22411;&#27010;&#29575;&#30340;&#20559;&#26012;&#65292;&#24182;&#25551;&#32472;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19981;&#23436;&#25972;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#30340;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#27979;&#37327;&#26657;&#20934;&#35823;&#24046;&#26102;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;ECE&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#32676;&#20307;&#21644;&#31867;&#21035;&#25104;&#21592;&#36523;&#20221;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;FCE&#22312;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#65292;&#32531;&#35299;&#20102;&#27169;&#22411;&#32622;&#20449;&#24230;&#20998;&#25968;&#20559;&#26012;&#23545;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;https://github.com/srdgFHE/FCE-paper&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20351;&#29992;FCE&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#36827;&#23637;&#21644;&#26410;&#26469;&#30340;&#21069;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#22810;&#23186;&#20307;&#35745;&#31639;&#24212;&#29992;&#31034;&#20363;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26377;&#30528;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.00537</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interpretability of Machine Learning: Recent Advances and Future Prospects. (arXiv:2305.00537v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#36827;&#23637;&#21644;&#26410;&#26469;&#30340;&#21069;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#22810;&#23186;&#20307;&#35745;&#31639;&#24212;&#29992;&#31034;&#20363;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26377;&#30528;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#24050;&#32463;&#21560;&#24341;&#20102;&#20154;&#20204;&#23545;&#21508;&#31181;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#31561;&#65289;&#30340;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31561;&#39046;&#22495;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#30340;&#40657;&#30418;&#29305;&#24615;&#24050;&#25104;&#20026;&#38754;&#21521;&#22810;&#23186;&#20307;&#30740;&#31350;&#21644;&#24212;&#29992;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#40657;&#30418;&#38382;&#39064;&#65292;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#21560;&#24341;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21069;&#26223;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#20010;&#30456;&#20851;&#30340;&#22810;&#23186;&#20307;&#35745;&#31639;&#24212;&#29992;&#31034;&#20363;&#65292;&#21253;&#25324;&#25991;&#26412;-&#22270;&#20687;&#36328;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29289;&#20307;&#35782;&#21035;&#31561;&#12290;&#36825;&#20010;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26377;&#30528;&#37325;&#35201;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of machine learning (ML) has drawn unprecedented interest in the study of various multimedia contents such as text, image, audio and video, among others. Consequently, understanding and learning ML-based representations have taken center stage in knowledge discovery in intelligent multimedia research and applications. Nevertheless, the black-box nature of contemporary ML, especially in deep neural networks (DNNs), has posed a primary challenge for ML-based representation learning. To address this black-box problem, the studies on interpretability of ML have attracted tremendous interests in recent years. This paper presents a survey on recent advances and future prospects on interpretability of ML, with several application examples pertinent to multimedia computing, including text-image cross-modal representation learning, face recognition, and the recognition of objects. It is evidently shown that the study of interpretability of ML promises an important research dir
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#35745;&#31639;&#26031;&#22374;&#32435;&#26641;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#19978;&#20248;&#20110;&#26631;&#20934;&#30340;2&#36817;&#20284;&#31639;&#27861;&#65292;&#24120;&#24120;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.00535</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36741;&#21161;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#30340;&#26031;&#22374;&#32435;&#26641;
&lt;/p&gt;
&lt;p&gt;
Nearly Optimal Steiner Trees using Graph Neural Network Assisted Monte Carlo Tree Search. (arXiv:2305.00535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#35745;&#31639;&#26031;&#22374;&#32435;&#26641;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#22270;&#19978;&#20248;&#20110;&#26631;&#20934;&#30340;2&#36817;&#20284;&#31639;&#27861;&#65292;&#24120;&#24120;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#38382;&#39064;&#12289;&#32452;&#21512;&#38382;&#39064;&#21644;&#22270;&#38382;&#39064;&#20013;&#21313;&#20998;&#26377;&#25928;&#65292;&#20363;&#22914;&#23376;&#22270;&#21516;&#26500;&#38382;&#39064;&#21644;&#26053;&#34892;&#21830;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#26031;&#22374;&#32435;&#26641;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30456;&#32467;&#21512;&#12290;&#39318;&#20808;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#20197;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#19968;&#20010;&#26032;&#33410;&#28857;&#20316;&#20026;&#24314;&#35758;&#12290;&#28982;&#21518;&#22312;&#33945;&#29305;&#21345;&#32599;&#25628;&#32034;&#20013;&#20351;&#29992;&#35813;&#31070;&#32463;&#32593;&#32476;&#26469;&#35745;&#31639;&#26031;&#22374;&#32435;&#26641;&#12290;&#35813;&#26041;&#27861;&#22312;&#35768;&#22810;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20013;&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#26631;&#20934;&#30340;2&#36817;&#20284;&#31639;&#27861;&#65292;&#24182;&#19988;&#36890;&#24120;&#21487;&#20197;&#25214;&#21040;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks are useful for learning problems, as well as for combinatorial and graph problems such as the Subgraph Isomorphism Problem and the Traveling Salesman Problem. We describe an approach for computing Steiner Trees by combining a graph neural network and Monte Carlo Tree Search. We first train a graph neural network that takes as input a partial solution and proposes a new node to be added as output. This neural network is then used in a Monte Carlo search to compute a Steiner tree. The proposed method consistently outperforms the standard 2-approximation algorithm on many different types of graphs and often finds the optimal solution.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICQ&#30340;&#26032;&#39062;&#37327;&#21270;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#24182;&#22312;&#20301;&#38480;&#21046;&#20449;&#36947;&#19978;&#23454;&#29616;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#20855;&#26377;&#19982;&#26410;&#37327;&#21270;&#31639;&#27861;&#30456;&#21516;&#30340;&#38454;&#20248;&#21270;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#36890;&#20449;&#39057;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.00528</link><description>&lt;p&gt;
ICQ:&#19968;&#31181;&#36866;&#29992;&#20110;&#20301;&#38480;&#21046;&#20449;&#36947;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#37327;&#21270;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ICQ: A Quantization Scheme for Best-Arm Identification Over Bit-Constrained Channels. (arXiv:2305.00528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICQ&#30340;&#26032;&#39062;&#37327;&#21270;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#29616;&#26377;&#30340;&#32622;&#20449;&#21306;&#38388;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#24182;&#22312;&#20301;&#38480;&#21046;&#20449;&#36947;&#19978;&#23454;&#29616;&#26368;&#20339;&#33218;&#35782;&#21035;&#65292;&#20855;&#26377;&#19982;&#26410;&#37327;&#21270;&#31639;&#27861;&#30456;&#21516;&#30340;&#38454;&#20248;&#21270;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#19988;&#20165;&#38656;&#35201;&#26497;&#23569;&#37327;&#30340;&#36890;&#20449;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#20998;&#24067;&#24335;&#21464;&#31181;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#20195;&#29702;&#19982;&#32769;&#34382;&#26426;&#30340;&#19968;&#33218;&#30456;&#20851;&#32852;&#65292;&#29983;&#25104;&#36981;&#24490;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#22238;&#25253;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#20301;&#38480;&#21046;&#20449;&#36947;&#23558;&#35266;&#23519;&#21040;&#30340;&#22238;&#25253;&#19982;&#23398;&#20064;&#22120;&#36890;&#20449;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#21270;&#26041;&#26696;&#65292;&#31216;&#20026;Inflating Confidence for Quantization&#65288;ICQ&#65289;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#32622;&#20449;&#21306;&#38388;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#20363;&#22914;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#30340;ICQ&#30340;&#24615;&#33021;&#65292;&#24182;&#34920;&#26126;ICQ-SE&#30340;&#24635;&#20307;&#31639;&#27861;&#20855;&#26377;&#19982;&#65288;&#26410;&#37327;&#21270;&#30340;&#65289;SE&#31639;&#27861;&#30456;&#21516;&#30340;&#38454;&#20248;&#21270;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#23427;&#21482;&#38656;&#35201;&#23398;&#20064;&#32773;&#21644;&#20195;&#29702;&#20043;&#38388;&#25351;&#25968;&#31232;&#30095;&#30340;&#36890;&#20449;&#39057;&#29575;&#65292;&#22240;&#27492;&#38656;&#35201;&#27604;&#29616;&#26377;&#37327;&#21270;&#26041;&#26696;&#26356;&#23569;&#30340;&#20301;&#25968;&#25165;&#33021;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-arm identification in a distributed variant of the multi-armed bandit setting, with a central learner and multiple agents. Each agent is associated with an arm of the bandit, generating stochastic rewards following an unknown distribution. Further, each agent can communicate the observed rewards with the learner over a bit-constrained channel. We propose a novel quantization scheme called Inflating Confidence for Quantization (ICQ) that can be applied to existing confidence-bound based learning algorithms such as Successive Elimination. We analyze the performance of ICQ applied to Successive Elimination and show that the overall algorithm, named ICQ-SE, has the order-optimal sample complexity as that of the (unquantized) SE algorithm. Moreover, it requires only an exponentially sparse frequency of communication between the learner and the agents, thus requiring considerably fewer bits than existing quantization schemes to successfully identify the best arm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#19988;&#21487;&#29992;&#20110;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.00521</link><description>&lt;p&gt;
StyleLipSync&#65306;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#19988;&#21487;&#29992;&#20110;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleLipSync&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20174;&#20219;&#24847;&#38899;&#39057;&#29983;&#25104;&#26080;&#20851;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#12290;&#20026;&#20102;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#35270;&#39057;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#22521;&#35757;&#30340;StyleGAN&#30340;&#35821;&#20041;&#20016;&#23500;&#28508;&#31354;&#38388;&#20013;&#30340;&#34920;&#36798;&#24615;&#21767;&#37096;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#35774;&#35745;&#35270;&#39057;&#19968;&#33268;&#24615;&#12290;&#19982;&#20197;&#24448;&#30340;&#21767;&#24418;&#21516;&#27493;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23039;&#24577;&#24863;&#30693;&#36974;&#32617;&#65292;&#36890;&#36807;&#36880;&#24103;&#21033;&#29992;&#19977;&#32500;&#21442;&#25968;&#21270;&#32593;&#26684;&#39044;&#27979;&#22120;&#21160;&#24577;&#23450;&#20301;&#36974;&#32617;&#65292;&#25552;&#39640;&#20102;&#24103;&#38388;&#33258;&#28982;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#19981;&#38656;&#35201;&#25968;&#25454;&#30340;&#21767;&#24418;&#21516;&#27493;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#27493;&#27491;&#21017;&#21270;&#22120;&#26469;&#20445;&#30041;&#21767;&#24418;&#21516;&#27493;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#22686;&#24378;&#20154;&#29289;&#29305;&#23450;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a fe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#31283;&#20581;&#36716;&#31227;&#23398;&#20064;&#65288;ART&#65289;&#31649;&#36947;&#65292;&#20351;&#29992;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#36716;&#31227;&#23398;&#20064;&#65292;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#23398;&#20064;&#29702;&#35770;&#65292;&#21516;&#26102;&#38450;&#27490;&#36127;&#38754;&#36716;&#31227;&#65292;&#24182;&#28436;&#31034;&#20102;&#23427;&#22312;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#31232;&#30095;&#23398;&#20064;&#19978;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00520</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#33402;&#26415;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#21644;&#31283;&#20581;&#30340;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
The ART of Transfer Learning: An Adaptive and Robust Pipeline. (arXiv:2305.00520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#31283;&#20581;&#36716;&#31227;&#23398;&#20064;&#65288;ART&#65289;&#31649;&#36947;&#65292;&#20351;&#29992;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#36716;&#31227;&#23398;&#20064;&#65292;&#24314;&#31435;&#20102;&#38750;&#28176;&#36817;&#23398;&#20064;&#29702;&#35770;&#65292;&#21516;&#26102;&#38450;&#27490;&#36127;&#38754;&#36716;&#31227;&#65292;&#24182;&#28436;&#31034;&#20102;&#23427;&#22312;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#31232;&#30095;&#23398;&#20064;&#19978;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#26159;&#21033;&#29992;&#36741;&#21161;&#25968;&#25454;&#36164;&#28304;&#26469;&#25552;&#39640;&#20027;&#35201;&#20219;&#21153;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#33258;&#36866;&#24212;&#31283;&#20581;&#36716;&#31227;&#23398;&#20064;&#65288;ART&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#36716;&#31227;&#23398;&#20064;&#30340;&#28789;&#27963;&#31649;&#36947;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;ART&#30340;&#38750;&#28176;&#36817;&#23398;&#20064;&#29702;&#35770;&#65292;&#20026;&#23454;&#29616;&#33258;&#36866;&#24212;&#36716;&#31227;&#24182;&#38450;&#27490;&#36127;&#38754;&#36716;&#31227;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;ART&#38598;&#25104;&#32858;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#22312;&#32771;&#34385;&#22810;&#20010;&#20505;&#36873;&#31639;&#27861;&#26102;&#20135;&#29983;&#21333;&#20010;&#26368;&#32456;&#27169;&#22411;&#12290;&#36890;&#36807;&#22238;&#24402;&#12289;&#20998;&#31867;&#21644;&#31232;&#30095;&#23398;&#20064;&#30340;&#24191;&#27867;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ART&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#27515;&#20129;&#29575;&#30740;&#31350;&#30340;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources. In this work, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline of performing transfer learning with generic machine learning algorithms. We establish the non-asymptotic learning theory of ART, providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer. Additionally, we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered. We demonstrate the promising performance of ART through extensive empirical studies on regression, classification, and sparse learning. We further present a real-data analysis for a mortality study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.00510</link><description>&lt;p&gt;
&#36890;&#21521;&#33258;&#30001;&#35745;&#31639;&#26550;&#26500;: &#20851;&#20110;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#20803;&#23431;&#23449;&#34394;&#25311;&#24314;&#31569;&#30340;&#32508;&#21512;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#24314;&#31569;&#24418;&#24335;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;3D&#24418;&#29366;&#29983;&#25104;&#25216;&#26415;&#27491;&#22312;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#24314;&#31569;&#35774;&#35745;&#20004;&#26041;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#35843;&#26597;&#21644;&#27604;&#36739;&#24403;&#21069;&#26368;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGMs&#65289;&#30340;3D&#23545;&#35937;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#12289;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#12289;3D&#24863;&#30693;&#22270;&#20687;&#21644;&#25193;&#25955;&#27169;&#22411;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;187&#31687;&#25991;&#31456;(&#21344;2018-2022&#24180;&#38388;&#21457;&#34920;&#25991;&#31456;&#30340;80.7%)&#65292;&#20197;&#22238;&#39038;&#22312;&#34394;&#25311;&#29615;&#22659;&#19979;&#24314;&#31569;&#29983;&#25104;&#21487;&#33021;&#24615;&#30340;&#39046;&#22495;&#65292;&#38480;&#20110;&#24314;&#31569;&#24418;&#24335;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24314;&#31569;&#30740;&#31350;&#12289;&#34394;&#25311;&#29615;&#22659;&#21644;&#30456;&#20851;&#25216;&#26415;&#26041;&#27861;&#30340;&#27010;&#36848;&#65292;&#25509;&#30528;&#22238;&#39038;&#20102;&#31163;&#25955;&#20307;&#32032;&#29983;&#25104;&#12289;&#30001;2D&#22270;&#20687;&#29983;&#25104;&#30340;3D&#27169;&#22411;&#20197;&#21450;&#26465;&#20214;&#21442;&#25968;&#30340;&#26368;&#36817;&#36235;&#21183;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;3D&#29983;&#25104;&#21644;&#21442;&#25968;&#21270;&#25511;&#21046;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#35752;&#30340;&#38382;&#39064;&#20540;&#24471;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25512;&#27979;&#21253;&#25324;&#29983;&#25104;&#22810;&#26679;&#24615;&#12289;&#26032;&#22411;&#36755;&#20986;&#21644;&#23884;&#20837;&#24335;&#26500;&#24314;&#31561;&#22235;&#20010;&#30740;&#31350;&#35758;&#31243;&#21487;&#33021;&#20250;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SEA&#31639;&#27861;&#65292;&#21487;&#22312;&#25104;&#23601;&#22411;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#20219;&#21153;&#12290;SEA&#39318;&#20808;&#23398;&#20064;&#24050;&#30693;&#25104;&#23601;&#30340;&#34920;&#31034;&#21644;&#20381;&#36182;&#20851;&#31995;&#22270;&#65292;&#28982;&#21518;&#36890;&#36807;&#26500;&#24314;&#25511;&#21046;&#22120;&#22312;&#32447;&#25506;&#32034;&#26032;&#25104;&#23601;&#12290;&#23454;&#39564;&#35777;&#26126;SEA&#33021;&#22815;&#20934;&#30830;&#22320;&#24674;&#22797;&#25104;&#23601;&#32467;&#26500;&#24182;&#25913;&#21892;&#22312;&#19968;&#20123;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00508</link><description>&lt;p&gt;
&#23398;&#20064;&#25104;&#23601;&#32467;&#26500;&#26469;&#22312;&#26377;&#31232;&#30095;&#22870;&#21169;&#30340;&#39046;&#22495;&#36827;&#34892;&#32467;&#26500;&#21270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward. (arXiv:2305.00508v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SEA&#31639;&#27861;&#65292;&#21487;&#22312;&#25104;&#23601;&#22411;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#20219;&#21153;&#12290;SEA&#39318;&#20808;&#23398;&#20064;&#24050;&#30693;&#25104;&#23601;&#30340;&#34920;&#31034;&#21644;&#20381;&#36182;&#20851;&#31995;&#22270;&#65292;&#28982;&#21518;&#36890;&#36807;&#26500;&#24314;&#25511;&#21046;&#22120;&#22312;&#32447;&#25506;&#32034;&#26032;&#25104;&#23601;&#12290;&#23454;&#39564;&#35777;&#26126;SEA&#33021;&#22815;&#20934;&#30830;&#22320;&#24674;&#22797;&#25104;&#23601;&#32467;&#26500;&#24182;&#25913;&#21892;&#22312;&#19968;&#20123;&#22797;&#26434;&#39046;&#22495;&#20013;&#30340;&#25506;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEA&#30340;&#22810;&#38454;&#27573;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26088;&#22312;&#20026;&#22522;&#20110;&#25104;&#23601;&#30340;&#29615;&#22659;&#35774;&#35745;&#65292;&#21363;&#20855;&#26377;&#20869;&#37096;&#25104;&#23601;&#38598;&#30340;&#29305;&#23450;&#31867;&#22411;&#29615;&#22659;&#12290;SEA&#39318;&#20808;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#24050;&#30693;&#25104;&#23601;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#24674;&#22797;&#23398;&#20064;&#25104;&#23601;&#30340;&#20381;&#36182;&#20851;&#31995;&#22270;&#65292;&#26368;&#21518;&#36890;&#36807;&#20351;&#29992;&#24674;&#22797;&#30340;&#20381;&#36182;&#20851;&#31995;&#22270;&#26500;&#24314;&#25511;&#21046;&#22120;&#65292;&#22312;&#32447;&#19982;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#25484;&#25569;&#24050;&#30693;&#25104;&#23601;&#24182;&#25506;&#32034;&#26032;&#25104;&#23601;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;SEA&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#25104;&#23601;&#32467;&#26500;&#65292;&#24182;&#25913;&#21892;&#22312;&#20687;&#22270;&#20687;&#36825;&#26679;&#20855;&#26377;&#39640;&#32500;&#35266;&#23519;&#20540;&#30340;&#22823;&#22411;&#29983;&#25104;&#39046;&#22495;&#65288;&#22914;Crafter&#65289;&#20013;&#30340;&#25506;&#32034;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with high-dimensional observations like images.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500; DAFNO&#65292;&#21487;&#20197;&#23398;&#20064;&#24102;&#26377;&#19981;&#35268;&#21017;&#20960;&#20309;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22495;&#30340;&#20195;&#29702;&#12290;&#36890;&#36807;&#23558;&#24179;&#28369;&#21270;&#30340;&#29305;&#24449;&#20989;&#25968;&#32435;&#20837; FNOs &#30340;&#31215;&#20998;&#23618;&#26550;&#26500;&#20013;&#65292;&#24182;&#21033;&#29992; FFT &#26469;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#65292;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#23558;&#20960;&#20309;&#20449;&#24687;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#65292;DAFNO &#30456;&#23545;&#20110;&#22522;&#32447;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00478</link><description>&lt;p&gt;
&#22495;&#19981;&#21487;&#30693;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Domain Agnostic Fourier Neural Operators. (arXiv:2305.00478v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00478
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500; DAFNO&#65292;&#21487;&#20197;&#23398;&#20064;&#24102;&#26377;&#19981;&#35268;&#21017;&#20960;&#20309;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22495;&#30340;&#20195;&#29702;&#12290;&#36890;&#36807;&#23558;&#24179;&#28369;&#21270;&#30340;&#29305;&#24449;&#20989;&#25968;&#32435;&#20837; FNOs &#30340;&#31215;&#20998;&#23618;&#26550;&#26500;&#20013;&#65292;&#24182;&#21033;&#29992; FFT &#26469;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#65292;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#23558;&#20960;&#20309;&#20449;&#24687;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#65292;DAFNO &#30456;&#23545;&#20110;&#22522;&#32447;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#65288;FNOs&#65289;&#33021;&#22815;&#23398;&#20064;&#22312;&#20989;&#25968;&#31354;&#38388;&#20043;&#38388;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#26144;&#23556;&#65292;&#26368;&#36817;&#24050;&#25104;&#20026;&#23398;&#20064;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#21709;&#24212;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#33391;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#65292;FNOs &#20381;&#36182;&#20110;&#24555;&#36895;&#20613;&#37324;&#21494;&#21464;&#25442; (FFT)&#65292;&#35813;&#21464;&#25442;&#20165;&#38480;&#20110;&#30697;&#24418;&#22495;&#19978;&#30340;&#24314;&#27169;&#38382;&#39064;&#12290;&#20026;&#20102;&#28040;&#38500;&#36825;&#26679;&#30340;&#38480;&#21046;&#65292;&#20801;&#35768; FFT &#22312;&#19981;&#35268;&#21017;&#20960;&#20309;&#20197;&#21450;&#25299;&#25169;&#21464;&#21270;&#20013;&#20351;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22495;&#19981;&#21487;&#30693;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376; (DAFNO)&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#19981;&#35268;&#21017;&#20960;&#20309;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#22495;&#30340;&#20195;&#29702;&#30340;&#26032;&#30340;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#24179;&#28369;&#21270;&#30340;&#29305;&#24449;&#20989;&#25968;&#32435;&#20837; FNOs &#30340;&#31215;&#20998;&#23618;&#26550;&#26500;&#20013;&#65292;&#24182;&#21033;&#29992; FFT &#26469;&#23454;&#29616;&#24555;&#36895;&#35745;&#31639;&#65292;&#20197;&#20415;&#20197;&#26126;&#30830;&#30340;&#26041;&#24335;&#23558;&#20960;&#20309;&#20449;&#24687;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#20013;&#65292;DAFNO &#30456;&#23545;&#20110;&#22522;&#32447;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fourier neural operators (FNOs) can learn highly nonlinear mappings between function spaces, and have recently become a popular tool for learning responses of complex physical systems. However, to achieve good accuracy and efficiency, FNOs rely on the Fast Fourier transform (FFT), which is restricted to modeling problems on rectangular domains. To lift such a restriction and permit FFT on irregular geometries as well as topology changes, we introduce domain agnostic Fourier neural operator (DAFNO), a novel neural operator architecture for learning surrogates with irregular geometries and evolving domains. The key idea is to incorporate a smoothed characteristic function in the integral layer architecture of FNOs, and leverage FFT to achieve rapid computations, in such a way that the geometric information is explicitly encoded in the architecture. In our empirical evaluation, DAFNO has achieved state-of-the-art accuracy as compared to baseline neural operator models on two benchmark dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.00477</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;PSDRL&#65292;&#32467;&#21512;&#20102;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#65292;&#20351;&#20854;&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26679;&#26412;&#25928;&#29575;&#20173;&#28982;&#36739;&#20302;&#65306;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35797;&#38169;&#26469;&#25214;&#21040;&#22909;&#30340;&#31574;&#30053;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#36890;&#36807;&#26500;&#24314;&#21487;&#20197;&#29992;&#20110;&#35268;&#21010;&#30340;&#29615;&#22659;&#27169;&#22411;&#26469;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#26159;&#36825;&#26679;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#20013;&#30001;&#20110;&#20854;&#24615;&#33021;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#21518;&#39564;&#37319;&#26679;&#65288;PSDRL&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#30495;&#27491;&#21487;&#25193;&#23637;&#30340;&#21518;&#39564;&#37319;&#26679;&#24378;&#21270;&#23398;&#20064;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20445;&#30041;&#20102;&#20854;&#22522;&#20110;&#27169;&#22411;&#30340;&#26412;&#36136;&#29305;&#24449;&#12290;PSDRL&#23558;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#19978;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#19982;&#22522;&#20110;&#20540;&#20989;&#25968;&#36924;&#36817;&#30340;&#29305;&#27530;&#35774;&#35745;&#30340;&#25345;&#32493;&#35268;&#21010;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#23545;Atari&#22522;&#20934;&#27979;&#35797;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;PSDRL&#22312;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#23581;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#36873;&#25321;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#20013;&#30340;&#32858;&#31867;&#25968;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00473</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Time series clustering based on prediction accuracy of global forecasting models. (arXiv:2305.00473v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#36873;&#25321;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#20013;&#30340;&#32858;&#31867;&#25968;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#32858;&#31867;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#20004;&#20010;&#36845;&#20195;&#27493;&#39588;&#65306;&#65288;i&#65289;&#36890;&#36807;&#32771;&#34385;&#27599;&#20010;&#31751;&#25152;&#28041;&#21450;&#30340;&#26102;&#24207;&#65292;&#24182;&#30001; pooling&#65288;&#38598;&#20013;&#65289;&#25311;&#21512; K &#20010;&#20840;&#23616;&#39044;&#27979;&#27169;&#22411;&#65307;&#65288;ii&#65289;&#27599;&#20010;&#24207;&#21015;&#34987;&#20998;&#37197;&#21040;&#20135;&#29983;&#26368;&#20339;&#39044;&#27979;&#30340;&#27169;&#22411;&#20851;&#32852;&#30340;&#32452;&#12290;&#19982;&#25991;&#29486;&#20013;&#22823;&#22810;&#25968;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#23558;&#39044;&#27979;&#20934;&#30830;&#24615;&#20316;&#20026;&#26500;&#24314;&#32858;&#31867;&#20998;&#21306;&#30340;&#20027;&#35201;&#20803;&#32032;&#65292;&#20854;&#20013;&#21253;&#21547;&#20849;&#21516;&#26368;&#23567;&#21270;&#24635;&#20307;&#39044;&#27979;&#35823;&#24046;&#30340;&#32452;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#23548;&#33268;&#20102;&#19968;&#20010;&#26032;&#30340;&#32858;&#31867;&#33539;&#24335;&#65292;&#20854;&#20013;&#32858;&#31867;&#35299;&#30340;&#36136;&#37327;&#26159;&#36890;&#36807;&#20854;&#39044;&#27979;&#33021;&#21147;&#26469;&#34913;&#37327;&#30340;&#12290;&#27492;&#22806;&#65292;&#35813;&#36807;&#31243;&#36824;&#24341;&#36215;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#32858;&#31867;&#25968;&#30340;&#26377;&#25928;&#26426;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20219;&#20309;&#22238;&#24402;&#27169;&#22411;&#31867;&#32467;&#21512;&#20351;&#29992;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26159;&#35201;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#21152;&#22909;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a novel method to perform model-based clustering of time series is proposed. The procedure relies on two iterative steps: (i) K global forecasting models are fitted via pooling by considering the series pertaining to each cluster and (ii) each series is assigned to the group associated with the model producing the best forecasts according to a particular criterion. Unlike most techniques proposed in the literature, the method considers the predictive accuracy as the main element for constructing the clustering partition, which contains groups jointly minimizing the overall forecasting error. Thus, the approach leads to a new clustering paradigm where the quality of the clustering solution is measured in terms of its predictive capability. In addition, the procedure gives rise to an effective mechanism for selecting the number of clusters in a time series database and can be used in combination with any class of regression model. An extensive simulation study shows that o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;ReLU&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;MILP&#37327;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#23588;&#20854;&#38024;&#23545;&#35299;&#20915;&#37327;&#23376;&#20301;&#21487;&#29992;&#24615;&#12289;&#22122;&#22768;&#21644;&#35823;&#24046;&#38480;&#21046;&#65292;&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#19982;&#31934;&#30830;&#24230;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#30340;&#21457;&#23637;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.00472</link><description>&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#20013;&#29992;&#20110;ReLU&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;MILP&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Efficient MILP Decomposition in Quantum Computing for ReLU Network Robustness. (arXiv:2305.00472v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;ReLU&#32593;&#32476;&#40065;&#26834;&#24615;&#30340;&#26032;&#22411;MILP&#37327;&#23376;&#20998;&#35299;&#26041;&#27861;&#65292;&#23588;&#20854;&#38024;&#23545;&#35299;&#20915;&#37327;&#23376;&#20301;&#21487;&#29992;&#24615;&#12289;&#22122;&#22768;&#21644;&#35823;&#24046;&#38480;&#21046;&#65292;&#21487;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#19982;&#31934;&#30830;&#24230;&#65292;&#23545;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#30340;&#21457;&#23637;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#30340;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#65292;&#22914;&#22024;&#26434;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#65292;&#20026;&#35299;&#20915;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#37327;&#23376;&#20301;&#30340;&#21487;&#29992;&#24615;&#12289;&#22122;&#22768;&#21644;&#35823;&#24046;&#30340;&#38480;&#21046;&#23545;&#23454;&#38469;&#23454;&#26045;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;MILP&#20998;&#35299;&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#21407;&#22987;&#38382;&#39064;&#22823;&#23567;&#24182;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#21487;&#29992;&#30340;NISQ&#35774;&#22791;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#23558;&#21407;&#22987;&#38382;&#39064;&#20998;&#35299;&#25104;&#26356;&#23567;&#30340;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#20351;&#29992;&#32452;&#21512;&#30340;&#37327;&#23376;-&#32463;&#20856;&#30828;&#20214;&#26041;&#27861;&#36845;&#20195;&#22320;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;Benders&#21644;Dantzig-Wolfe&#26041;&#27861;&#30340;MILP&#20998;&#35299;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;Benders&#25152;&#38656;&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#32780;Dantzig-Wolfe&#30340;&#37327;&#23376;&#27604;&#29305;&#25968;&#20445;&#25345;&#19981;&#21464;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;Dantzig-Wolfe&#20998;&#35299;&#23545;&#35777;&#26126;ReLU&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26102;&#30340;&#20351;&#29992;&#26696;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;MILP&#20998;&#35299;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#32463;&#20856;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging quantum computing technologies, such as Noisy Intermediate-Scale Quantum (NISQ) devices, offer potential advancements in solving mathematical optimization problems. However, limitations in qubit availability, noise, and errors pose challenges for practical implementation. In this study, we examine two decomposition methods for Mixed-Integer Linear Programming (MILP) designed to reduce the original problem size and utilize available NISQ devices more efficiently. We concentrate on breaking down the original problem into smaller subproblems, which are then solved iteratively using a combined quantum-classical hardware approach. We conduct a detailed analysis for the decomposition of MILP with Benders and Dantzig-Wolfe methods. In our analysis, we show that the number of qubits required to solve Benders is exponentially large in the worst-case, while remains constant for Dantzig-Wolfe. Additionally, we leverage Dantzig-Wolfe decomposition on the use-case of certifying the robustn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#22270;&#32858;&#31867;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#20110;&#36793;&#26435;&#30340;&#39030;&#28857;&#26435;&#37325;&#26469;&#25552;&#39640;&#36229;&#22270;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;&#26681;&#25454;&#29702;&#35770;&#20998;&#26512;&#65292;&#21033;&#29992;&#19982;1-Laplacian&#30340;&#31532;&#20108;&#23567;&#29305;&#24449;&#20540;&#30456;&#20851;&#30340;&#29305;&#24449;&#21521;&#37327;&#33021;&#22815;&#36798;&#21040;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#32858;&#31867;&#31934;&#24230;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00462</link><description>&lt;p&gt;
&#20381;&#36182;&#20110;&#36793;&#26435;&#30340;&#36229;&#22270;&#65306;&#22522;&#20110;1-Laplacian&#30340;&#35889;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs with Edge-Dependent Vertex Weights: Spectral Clustering based on the 1-Laplacian. (arXiv:2305.00462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#22270;&#32858;&#31867;&#30340;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#20110;&#36793;&#26435;&#30340;&#39030;&#28857;&#26435;&#37325;&#26469;&#25552;&#39640;&#36229;&#22270;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;&#26681;&#25454;&#29702;&#35770;&#20998;&#26512;&#65292;&#21033;&#29992;&#19982;1-Laplacian&#30340;&#31532;&#20108;&#23567;&#29305;&#24449;&#20540;&#30456;&#20851;&#30340;&#29305;&#24449;&#21521;&#37327;&#33021;&#22815;&#36798;&#21040;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#39640;&#30340;&#32858;&#31867;&#31934;&#24230;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;1-Laplacian&#36229;&#22270;&#65292;&#20854;&#20013;&#21253;&#25324;&#20381;&#36182;&#20110;&#36793;&#30340;&#39030;&#28857;&#26435;&#37325;&#12290;&#36825;&#20123;&#26435;&#37325;&#33021;&#22815;&#21453;&#26144;&#36229;&#36793;&#20869;&#39030;&#28857;&#30340;&#19981;&#21516;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#20351;&#24471;&#36229;&#22270;&#27169;&#22411;&#27604;&#21516;&#36136;&#36229;&#22270;&#20855;&#26377;&#26356;&#39640;&#30340;&#34920;&#29616;&#21147;&#12290;&#26412;&#25991;&#21033;&#29992;&#19982;&#36229;&#22270;1-Laplacian&#31532;&#20108;&#23567;&#29305;&#24449;&#20540;&#30456;&#20851;&#30340;&#29305;&#24449;&#21521;&#37327;&#23545;&#39030;&#28857;&#36827;&#34892;&#32858;&#31867;&#12290;&#20174;&#22522;&#20110;&#35268;&#33539;&#21270;&#30340;Cheeger&#21106;&#30340;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32858;&#31867;&#31934;&#24230;&#24212;&#27604;&#20256;&#32479;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#26356;&#39640;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#35777;&#23454;&#20102;&#35813;&#35889;&#32858;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#25105;&#20204;&#26694;&#26550;&#20869;&#19968;&#20010;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#30456;&#24212;&#30340;&#36229;&#22270;1-Laplacian&#31561;&#20215;&#20110;&#19968;&#20010;&#30456;&#20851;&#22270;&#30340;1-Laplacian&#65292;&#35813;&#22270;&#30340;&#29305;&#24449;&#21521;&#37327;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35745;&#31639;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a flexible framework for defining the 1-Laplacian of a hypergraph that incorporates edge-dependent vertex weights. These weights are able to reflect varying importance of vertices within a hyperedge, thus conferring the hypergraph model higher expressivity than homogeneous hypergraphs. We then utilize the eigenvector associated with the second smallest eigenvalue of the hypergraph 1-Laplacian to cluster the vertices. From a theoretical standpoint based on an adequately defined normalized Cheeger cut, this procedure is expected to achieve higher clustering accuracy than that based on the traditional Laplacian. Indeed, we confirm that this is the case using real-world datasets to demonstrate the effectiveness of the proposed spectral clustering approach. Moreover, we show that for a special case within our framework, the corresponding hypergraph 1-Laplacian is equivalent to the 1-Laplacian of a related graph, whose eigenvectors can be computed more efficiently, facilitating th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#35774;&#35745;&#20102;&#19968;&#20010;&#39044;&#27979;&#31995;&#32479;&#26469;&#39044;&#27979;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#29305;&#23450;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#21644;MLP&#31561;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#39044;&#27979;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.00449</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21450;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#30340;&#21487;&#39044;&#27979;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predictability of Machine Learning Algorithms and Related Feature Extraction Techniques. (arXiv:2305.00449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#35774;&#35745;&#20102;&#19968;&#20010;&#39044;&#27979;&#31995;&#32479;&#26469;&#39044;&#27979;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#29305;&#23450;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#21644;MLP&#31561;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#39044;&#27979;&#65292;&#24182;&#24471;&#20986;&#20102;&#19977;&#20010;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22522;&#20110;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#35774;&#35745;&#20102;&#19968;&#20010;&#39044;&#27979;&#31995;&#32479;&#65292;&#21487;&#20197;&#39044;&#27979;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#29305;&#23450;&#27169;&#22411;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;50&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#19977;&#20010;&#22522;&#26412;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;&#38543;&#26426;&#26862;&#26519;&#12289;XGBoost&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#24615;&#33021;&#39044;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;1.&#20351;&#29992;&#31895;&#35843;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;2.&#20351;&#29992;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#21487;&#20197;&#39044;&#27979;MLP&#30340;&#24615;&#33021;&#12290;3. &#20351;&#29992;&#38544;&#24335;&#21453;&#39304;&#21487;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This thesis designs a prediction system based on matrix factorization to predict the classification accuracy of a specific model on a particular dataset. In this thesis, we conduct comprehensive empirical research on more than fifty datasets that we collected from the openml website. We study the performance prediction of three fundamental machine learning algorithms, namely, random forest, XGBoost, and MultiLayer Perceptron(MLP). In particular, we obtain the following results: 1. Predictability of fine-tuned models using coarse-tuned variants. 2. Predictability of MLP using feature extraction techniques. 3. Predict model performance using implicit feedback.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;&#65288;MTSL&#65289;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20219;&#21153;&#26550;&#26500;&#21450;&#20854;&#21442;&#25968;&#65292;&#20854;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#23616;&#37096;&#20219;&#21153;&#30456;&#20284;&#24615;&#32435;&#20837;&#31070;&#32463;&#20803;&#21019;&#24314;&#21644;&#31227;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00441</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#20219;&#21153;&#30456;&#20284;&#24615;&#21551;&#21457;&#30340;&#31070;&#32463;&#20803;&#21019;&#24314;&#21644;&#31227;&#38500;&#30340;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Structural Learning using Local Task Similarity induced Neuron Creation and Removal. (arXiv:2305.00441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;&#65288;MTSL&#65289;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20219;&#21153;&#26550;&#26500;&#21450;&#20854;&#21442;&#25968;&#65292;&#20854;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#23558;&#23616;&#37096;&#20219;&#21153;&#30456;&#20284;&#24615;&#32435;&#20837;&#31070;&#32463;&#20803;&#21019;&#24314;&#21644;&#31227;&#38500;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#20855;&#26377;&#36890;&#36807;&#26368;&#22823;&#21270;&#20219;&#21153;&#38388;&#27491;&#21521;&#36716;&#31227;&#26469;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12289;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#30340;&#32593;&#32476;&#26550;&#26500;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#38745;&#24577;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#21457;&#25381;&#28508;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#33041;&#20013;&#30340;&#23398;&#20064;&#26159;&#36890;&#36807;&#32467;&#26500;&#24615;&#21464;&#21270;&#21644;&#31361;&#35302;&#24378;&#24230;&#21464;&#21270;&#30456;&#20114;&#20316;&#29992;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20219;&#21153;&#32467;&#26500;&#23398;&#20064;&#65288;MTSL&#65289;&#8221;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20219;&#21153;&#26550;&#26500;&#21450;&#20854;&#21442;&#25968;&#12290;MTSL&#20174;&#27599;&#20010;&#20219;&#21153;&#30340;&#30456;&#21516;&#21333;&#20219;&#21153;&#32593;&#32476;&#24320;&#22987;&#65292;&#20132;&#26367;&#36827;&#34892;&#20219;&#21153;&#23398;&#20064;&#21644;&#32467;&#26500;&#23398;&#20064;&#12290;&#22312;&#20219;&#21153;&#23398;&#20064;&#38454;&#27573;&#65292;&#27599;&#20010;&#32593;&#32476;&#19987;&#38376;&#22788;&#29702;&#30456;&#24212;&#30340;&#20219;&#21153;&#12290;&#22312;&#27599;&#20010;&#32467;&#26500;&#23398;&#20064;&#38454;&#27573;&#20013;&#65292;&#20174;&#26368;&#26089;&#30340;&#23618;&#24320;&#22987;&#65292;&#23616;&#37096;&#30456;&#20284;&#30340;&#20219;&#21153;&#23618;&#39318;&#20808;&#23558;&#20854;&#30693;&#35782;&#20256;&#36755;&#21040;&#26032;&#21019;&#24314;&#30340;&#32452;&#23618;&#65292;&#28982;&#21518;&#20877;&#23558;&#20854;&#21024;&#38500;&#12290;&#28982;&#21518;MTSL&#22312;&#30456;&#24212;&#30340;&#32452;&#23618;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning has the potential to improve generalization by maximizing positive transfer between tasks while reducing task interference. Fully achieving this potential is hindered by manually designed architectures that remain static throughout training. On the contrary, learning in the brain occurs through structural changes that are in tandem with changes in synaptic strength. Thus, we propose \textit{Multi-Task Structural Learning (MTSL)} that simultaneously learns the multi-task architecture and its parameters. MTSL begins with an identical single-task network for each task and alternates between a task-learning phase and a structural-learning phase. In the task learning phase, each network specializes in the corresponding task. In each of the structural learning phases, starting from the earliest layer, locally similar task layers first transfer their knowledge to a newly created group layer before being removed. MTSL then uses the group layer in place of the corresponding 
&lt;/p&gt;</description></item><item><title>META-SMGO-$\Delta$ &#36890;&#36807;&#23558;&#30456;&#20284;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#24212;&#29992;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#26469;&#25552;&#39640;&#27714;&#35299;&#30456;&#20284;&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.00438</link><description>&lt;p&gt;
META-SMGO-$\Delta$: &#30456;&#20284;&#24615;&#20316;&#20026;&#40657;&#31665;&#20248;&#21270;&#20808;&#39564;&#30693;&#35782;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
META-SMGO-$\Delta$: similarity as a prior in black-box optimization. (arXiv:2305.00438v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00438
&lt;/p&gt;
&lt;p&gt;
META-SMGO-$\Delta$ &#36890;&#36807;&#23558;&#30456;&#20284;&#38382;&#39064;&#30340;&#20808;&#39564;&#30693;&#35782;&#24212;&#29992;&#21040;&#20248;&#21270;&#36807;&#31243;&#20013;&#26469;&#25552;&#39640;&#27714;&#35299;&#30456;&#20284;&#38382;&#39064;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#20013;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#24120;&#28041;&#21450;&#30456;&#20284;&#38382;&#39064;&#37325;&#22797;&#27714;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#20005;&#26684;&#30340;&#30456;&#20284;&#24615;&#23450;&#20041;&#65292;&#23558; META-learning &#29702;&#35770;&#24212;&#29992;&#21040; SMGO-$\Delta`$&#65292;&#19968;&#20010;&#36817;&#26399;&#25552;&#20986;&#30340;&#20840;&#23616;&#20248;&#21270;&#26041;&#27861;&#20013;&#65292;&#20174;&#31867;&#20284;&#36807;&#21435;&#32463;&#39564;&#20013;&#33719;&#21462;&#20808;&#39564;&#30693;&#35782;&#65292;&#20197;&#39640;&#25928;&#27714;&#35299;&#26032;&#30340;&#65288;&#30456;&#20284;&#65289;&#38382;&#39064;&#12290;&#36890;&#36807;&#22522;&#20934;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20934;&#31639;&#27861; META-extension &#30340;&#23454;&#38469;&#22909;&#22788;&#65292;&#21516;&#26102;&#25552;&#20379;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
When solving global optimization problems in practice, one often ends up repeatedly solving problems that are similar to each others. By providing a rigorous definition of similarity, in this work we propose to incorporate the META-learning rationale into SMGO-$\Delta$, a global optimization approach recently proposed in the literature, to exploit priors obtained from similar past experience to efficiently solve new (similar) problems. Through a benchmark numerical example we show the practical benefits of our META-extension of the baseline algorithm, while providing theoretical bounds on its performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36719;&#20214;&#21512;&#25104;&#22120;&#20135;&#29983;&#30340;&#21512;&#25104;&#38899;&#39057;&#25968;&#25454;&#26469;&#22521;&#35757;&#36890;&#29992;&#27169;&#22411;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#36716;&#31227;&#23398;&#20064;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#22522;&#30784;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22521;&#35757;&#21487;&#20197;&#25104;&#20026;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#33391;&#22909;&#22522;&#30784;&#65292;&#20854;&#20013;&#36716;&#24405;&#20219;&#21153;&#24182;&#19981;&#20165;&#38480;&#20110;&#19968;&#20010;&#20048;&#22120;&#12290;</title><link>http://arxiv.org/abs/2305.00426</link><description>&lt;p&gt;
&#20048;&#22120;&#33258;&#21160;&#36716;&#24405;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Transfer of knowledge among instruments in automatic music transcription. (arXiv:2305.00426v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36719;&#20214;&#21512;&#25104;&#22120;&#20135;&#29983;&#30340;&#21512;&#25104;&#38899;&#39057;&#25968;&#25454;&#26469;&#22521;&#35757;&#36890;&#29992;&#27169;&#22411;&#65292;&#20026;&#36827;&#19968;&#27493;&#30340;&#36716;&#31227;&#23398;&#20064;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#22522;&#30784;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22521;&#35757;&#21487;&#20197;&#25104;&#20026;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#33391;&#22909;&#22522;&#30784;&#65292;&#20854;&#20013;&#36716;&#24405;&#20219;&#21153;&#24182;&#19981;&#20165;&#38480;&#20110;&#19968;&#20010;&#20048;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38899;&#20048;&#36716;&#24405;&#65288;AMT&#65289;&#26159;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#23427;&#30340;&#36807;&#31243;&#26159;&#23558;&#38899;&#20048;&#30340;&#38899;&#39057;&#24405;&#38899;&#36716;&#25442;&#20026;&#21253;&#21547;&#26377;&#20851;&#38899;&#31526;&#12289;&#21644;&#24358;&#21644;&#33410;&#22863;&#20449;&#24687;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#24403;&#21069;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#22522;&#20110;&#21464;&#21387;&#22120;&#26550;&#26500;&#30340;&#26032;&#27169;&#22411;&#25110;&#20351;&#29992;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#65292;&#20294;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24040;&#22823;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36719;&#20214;&#21512;&#25104;&#22120;&#20135;&#29983;&#30340;&#26131;&#20110;&#29983;&#25104;&#30340;&#21512;&#25104;&#38899;&#39057;&#25968;&#25454;&#26469;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#12290;&#23427;&#26159;&#36827;&#19968;&#27493;&#36716;&#31227;&#23398;&#20064;&#30340;&#33391;&#22909;&#22522;&#30784;&#65292;&#20197;&#24555;&#36895;&#36866;&#24212;&#20854;&#20182;&#20048;&#22120;&#30340;&#36716;&#24405;&#27169;&#22411;&#12290;&#23454;&#29616;&#30340;&#32467;&#26524;&#35777;&#26126;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25104;&#20026;&#39044;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#33391;&#22909;&#22522;&#30784;&#65292;&#20854;&#20013;&#36716;&#24405;&#20219;&#21153;&#24182;&#19981;&#38598;&#20013;&#20110;&#19968;&#20010;&#20048;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic music transcription (AMT) is one of the most challenging tasks in the music information retrieval domain. It is the process of converting an audio recording of music into a symbolic representation containing information about the notes, chords, and rhythm. Current research in this domain focuses on developing new models based on transformer architecture or using methods to perform semi-supervised training, which gives outstanding results, but the computational cost of training such models is enormous.  This work shows how to employ easily generated synthesized audio data produced by software synthesizers to train a universal model. It is a good base for further transfer learning to quickly adapt transcription model for other instruments. Achieved results prove that using synthesized data for training may be a good base for pretraining general-purpose models, where the task of transcription is not focused on one instrument.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00418</link><description>&lt;p&gt;
&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00418
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#31181;&#29983;&#25104;&#27169;&#22411;&#22312;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#32463;&#36807;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#30340;&#35206;&#30422;&#29575;&#36739;&#20302;&#19988;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#27880;&#37322;&#12289;&#29616;&#26377;&#20195;&#30721;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#26469;&#29983;&#25104;&#20195;&#30721;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#19977;&#20010;&#29983;&#25104;&#27169;&#22411;&#65288;CodeGen&#12289;Codex&#21644;GPT-3.5&#65289;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#21333;&#20803;&#27979;&#35797;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#20013;&#20351;&#29992;&#20102;&#20004;&#20010;&#22522;&#20934;&#65288;HumanEval&#21644;Evosuite SF110&#65289;&#26469;&#35843;&#26597;&#29615;&#22659;&#29983;&#25104;&#23545;&#21333;&#20803;&#27979;&#35797;&#29983;&#25104;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26681;&#25454;&#32534;&#35793;&#29575;&#12289;&#27979;&#35797;&#27491;&#30830;&#24615;&#12289;&#35206;&#30422;&#29575;&#21644;&#27979;&#35797;&#21619;&#36947;&#26469;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Codex&#27169;&#22411;&#22312;HumanEval&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;80%&#30340;&#35206;&#30422;&#29575;&#65292;&#20294;&#22312;EvoSuite SF110&#22522;&#20934;&#20013;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#36229;&#36807;2%&#30340;&#35206;&#30422;&#29575;&#12290;&#29983;&#25104;&#30340;&#27979;&#35797;&#36824;&#23384;&#22312;&#27979;&#35797;&#21619;&#36947;&#38382;&#39064;&#65292;&#27604;&#22914;&#37325;&#22797;&#30340;&#26029;&#35328;&#21644;&#31354;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#29366;&#24577;&#19981;&#24819;&#38745;&#27490;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;rollout&#31574;&#30053;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#21333;&#33218;&#36172;&#21338;&#26426;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#32467;&#26500;&#32467;&#26524;&#21644;&#21487;&#32034;&#24341;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00410</link><description>&lt;p&gt;
&#26377;&#38480;&#29366;&#24577;&#19981;&#24819;&#38745;&#27490;&#22810;&#33218;&#36172;&#21338;&#26426;&#21644;Rollout&#31574;&#30053;&#30340;&#21487;&#32034;&#24341;&#24615;
&lt;/p&gt;
&lt;p&gt;
Indexability of Finite State Restless Multi-Armed Bandit and Rollout Policy. (arXiv:2305.00410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#29366;&#24577;&#19981;&#24819;&#38745;&#27490;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;rollout&#31574;&#30053;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#21333;&#33218;&#36172;&#21338;&#26426;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#32467;&#26500;&#32467;&#26524;&#21644;&#21487;&#32034;&#24341;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#38480;&#29366;&#24577;&#19981;&#24819;&#38745;&#27490;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#20915;&#31574;&#32773;&#21487;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#36873;&#25321;M&#20010;&#33218;&#20013;&#30340;&#20219;&#24847;&#19968;&#20010;&#65292;&#33218;&#30340;&#25773;&#25918;&#20135;&#29983;&#22522;&#20110;&#21160;&#20316;&#30340;&#29366;&#24577;&#30456;&#20851;&#22870;&#21169;&#65292;&#24403;&#33218;&#27809;&#26377;&#34987;&#25773;&#25918;&#26102;&#65292;&#23427;&#36824;&#25552;&#20379;&#22522;&#20110;&#29366;&#24577;&#21644;&#21160;&#20316;&#30340;&#22870;&#21169;&#12290;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26080;&#38480;&#26102;&#38388;&#38271;&#24230;&#30340;&#25240;&#25187;&#22870;&#21169;&#12290;&#20256;&#32479;&#30340;&#19981;&#24819;&#38745;&#27490;&#36172;&#21338;&#26426;&#26041;&#27861;&#26159;&#20351;&#29992; Whittle &#32034;&#24341;&#31574;&#30053;&#12290;&#22312;&#36825;&#31181;&#31574;&#30053;&#20013;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#25773;&#25918;&#20855;&#26377;&#26368;&#39640;&#25351;&#25968;&#30340;M&#20010;&#33218;&#12290;&#25105;&#20204;&#23558;&#19981;&#24819;&#38745;&#27490;&#36172;&#21338;&#26426;&#38382;&#39064;&#20998;&#31163;&#25104;&#20998;&#26512;&#26494;&#24347;&#32422;&#26463;&#19981;&#24819;&#38745;&#27490;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#28982;&#21518;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#38382;&#39064;&#65292;&#23558;&#19981;&#24819;&#38745;&#27490;&#36172;&#21338;&#26426;&#38382;&#39064;&#20998;&#31163;&#25104;N&#20010;&#21333;&#33218;&#19981;&#24819;&#38745;&#27490;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21333;&#33218;&#19981;&#24819;&#38745;&#27490;&#36172;&#21338;&#26426;&#12290;&#20026;&#20102;&#30740;&#31350; Whittle &#32034;&#24341;&#31574;&#30053;&#65292;&#25105;&#20204;&#22312;&#21333;&#33218;&#36172;&#21338;&#26426;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#32467;&#26500;&#32467;&#26524;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#21487;&#32034;&#24341;&#24615;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#23637;&#31034;&#20102;&#21487;&#32034;&#24341;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;rollout&#31574;&#30053;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider finite state restless multi-armed bandit problem. The decision maker can act on M bandits out of N bandits in each time step. The play of arm (active arm) yields state dependent rewards based on action and when the arm is not played, it also provides rewards based on the state and action. The objective of the decision maker is to maximize the infinite horizon discounted reward. The classical approach to restless bandits is Whittle index policy. In such policy, the M arms with highest indices are played at each time step. Here, one decouples the restless bandits problem by analyzing relaxed constrained restless bandits problem. Then by Lagrangian relaxation problem, one decouples restless bandits problem into N single-armed restless bandit problems. We analyze the single-armed restless bandit. In order to study the Whittle index policy, we show structural results on the single armed bandit model. We define indexability and show indexability in special cases. We propose an al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00393</link><description>&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#23545;&#21160;&#24577;&#22330;&#26223;&#36827;&#34892;&#29289;&#20307;&#20013;&#24515;&#20307;&#32032;&#21270;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;DynaVol&#65292;&#21487;&#20197;&#22312;&#22810;&#23454;&#20307;&#21160;&#24577;&#22330;&#26223;&#20013;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#65292;&#36890;&#36807;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#21644;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#26469;&#22686;&#24378;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#30340;3D&#22330;&#26223;&#20013;&#29702;&#35299;&#19990;&#30028;&#30340;&#32452;&#25104;&#21160;&#24577;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#26102;&#38388;&#32447;&#32034;&#65292;&#35201;&#20040;&#24573;&#30053;&#20102;&#22330;&#26223;&#20998;&#35299;&#30340;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DynaVol&#65292;&#19968;&#31181;&#36870;&#21521;&#31070;&#32463;&#28210;&#26579;&#26694;&#26550;&#65292;&#20026;&#22810;&#23454;&#20307;&#65288;&#22914;&#29289;&#20307;&#65289;&#30340;&#21160;&#24577;&#22330;&#26223;&#23398;&#20064;&#26102;&#38388;&#21464;&#21270;&#30340;&#20307;&#31215;&#34920;&#31034;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#12290;&#39318;&#20808;&#65292;&#23427;&#32500;&#25252;&#19968;&#20010;&#26102;&#38388;&#20381;&#36182;&#30340;3D&#26684;&#28857;&#65292;&#21160;&#24577;&#32780;&#28789;&#27963;&#22320;&#23558;&#31354;&#38388;&#20301;&#32622;&#32465;&#23450;&#21040;&#19981;&#21516;&#30340;&#23454;&#20307;&#65292;&#20174;&#32780;&#22312;&#20195;&#34920;&#24615;&#27700;&#24179;&#19978;&#40723;&#21169;&#20449;&#24687;&#30340;&#20998;&#31163;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#32852;&#21512;&#23398;&#20064;&#26684;&#28857;&#32423;&#23616;&#37096;&#21160;&#24577;&#12289;&#29289;&#20307;&#32423;&#20840;&#23616;&#21160;&#24577;&#21644;&#32452;&#21512;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#29289;&#20307;&#20013;&#24515;&#22330;&#26223;&#20307;&#32032;&#21270;&#30340;&#26102;&#31354;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;DynaVol&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.00386</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Importance Weighted Expectation-Maximization for Protein Sequence Design. (arXiv:2305.00386v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#65292;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#29983;&#29289;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20195;&#29702;&#24207;&#21015;-&#21151;&#33021;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#30340;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21463;&#21040;&#21478;&#22806;&#19968;&#20010;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#32467;&#26500;&#29305;&#24449;&#30340;&#22686;&#24378;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65288;MCEM&#65289;&#26469;&#23398;&#20064;&#36825;&#20010;&#27169;&#22411;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20854;MRF&#29305;&#24449;&#21017;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#22312;&#20843;&#39033;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#39640;55&#65285;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.
&lt;/p&gt;</description></item><item><title>DualHSIC&#36890;&#36807;&#21033;&#29992;&#36328;&#20219;&#21153;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#37325;&#26032;&#23398;&#20064;&#26041;&#27861;&#30340;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00380</link><description>&lt;p&gt;
&#22522;&#20110;HSIC&#29942;&#39048;&#21644;&#23545;&#40784;&#30340;&#21452;&#37325;HSIC&#23454;&#29616;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning. (arXiv:2305.00380v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00380
&lt;/p&gt;
&lt;p&gt;
DualHSIC&#36890;&#36807;&#21033;&#29992;&#36328;&#20219;&#21153;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#37325;&#26032;&#23398;&#20064;&#26041;&#27861;&#30340;&#36830;&#32493;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37325;&#26032;&#23398;&#20064;&#30340;&#26041;&#27861;&#26159;&#36830;&#32493;&#23398;&#20064;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#23567;&#22411;&#22266;&#23450;&#22823;&#23567;&#30340;&#32531;&#20914;&#21306;&#26469;&#25511;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#22522;&#20110;&#37325;&#26032;&#23398;&#20064;&#30340;&#26041;&#27861;&#30740;&#31350;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#32531;&#20914;&#21306;&#20013;&#36807;&#21435;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#20294;&#24456;&#23569;&#27880;&#24847;&#21040;&#20851;&#38190;&#30340;&#20219;&#21153;&#29305;&#23450;&#21644;&#20219;&#21153;&#19981;&#21464;&#30693;&#35782;&#30340;&#36328;&#20219;&#21153;&#20851;&#31995;&#12290;&#36890;&#36807;&#36866;&#24403;&#22320;&#21033;&#29992;&#36328;&#20219;&#21153;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DualHSIC&#30340;&#26032;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#25552;&#39640;&#29616;&#26377;&#22522;&#20110;&#37325;&#26032;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;DualHSIC&#30001;&#20004;&#20010;&#20114;&#34917;&#30340;&#32452;&#20214;&#32452;&#25104;&#65292;&#28304;&#33258;&#25152;&#35859;&#30340;&#24076;&#23572;&#20271;&#29305;-&#26045;&#23494;&#29305;&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#65306;HSIC-Bottleneck for Rehearsal (HBR)&#20943;&#23569;&#36328;&#20219;&#21153;&#24178;&#25200;&#65292;HSIC Alignment (HA)&#20419;&#36827;&#20219;&#21153;&#19981;&#21464;&#30693;&#35782;&#20849;&#20139;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DualHSIC&#21487;&#20197;&#26080;&#32541;&#22320;&#25554;&#20837;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#26032;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#36830;&#32493;&#23398;&#20064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rehearsal-based approaches are a mainstay of continual learning (CL). They mitigate the catastrophic forgetting problem by maintaining a small fixed-size buffer with a subset of data from past tasks. While most rehearsal-based approaches study how to effectively exploit the knowledge from the buffered past data, little attention is paid to the inter-task relationships with the critical task-specific and task-invariant knowledge. By appropriately leveraging inter-task relationships, we propose a novel CL method named DualHSIC to boost the performance of existing rehearsal-based methods in a simple yet effective way. DualHSIC consists of two complementary components that stem from the so-called Hilbert Schmidt independence criterion (HSIC): HSIC-Bottleneck for Rehearsal (HBR) lessens the inter-task interference and HSIC Alignment (HA) promotes task-invariant knowledge sharing. Extensive experiments show that DualHSIC can be seamlessly plugged into existing rehearsal-based methods for con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;&#26041;&#27861;&#65288;AIR&#65289;&#26469;&#24378;&#21046;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#30340;&#23398;&#20064;&#34920;&#31034;&#21576;&#29616;&#26679;&#24335;&#29420;&#31435;&#24615;&#65292;&#24182;&#29992;&#21152;&#26435;SIR&#21644;AIR&#23454;&#29616;ACL&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#19979;&#22343;&#26174;&#33879;&#25552;&#39640;ACL&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00374</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;&#22686;&#24378;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization. (arXiv:2305.00374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;&#26041;&#27861;&#65288;AIR&#65289;&#26469;&#24378;&#21046;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#30340;&#23398;&#20064;&#34920;&#31034;&#21576;&#29616;&#26679;&#24335;&#29420;&#31435;&#24615;&#65292;&#24182;&#29992;&#21152;&#26435;SIR&#21644;AIR&#23454;&#29616;ACL&#30340;&#40065;&#26834;&#24615;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#19979;&#22343;&#26174;&#33879;&#25552;&#39640;ACL&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;(ACL)&#26080;&#38656;&#26631;&#31614;&#65292;&#23558;&#23545;&#25239;&#24615;&#25968;&#25454;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(SCL)&#30456;&#32467;&#21512;&#65292;&#36755;&#20986;&#19968;&#20010;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;&#65292;&#21487;&#27867;&#21270;&#19988;&#25269;&#25239;&#23545;&#25239;&#24615;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#12290;&#34920;&#31034;&#30340;&#26679;&#24335;&#29420;&#31435;&#23646;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#36716;&#31227;&#12290;&#26631;&#20934;&#19981;&#21464;&#27491;&#21017;&#21270;(SIR)&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20351;SCL&#36890;&#36807;&#23398;&#20064;&#30340;&#34920;&#31034;&#19981;&#21463;&#26679;&#24335;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36890;&#36807;ACL&#33719;&#24471;&#20855;&#26377;&#26679;&#24335;&#29420;&#31435;&#24615;&#36136;&#30340;&#40065;&#26834;&#34920;&#31034;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#25512;&#29702;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#19981;&#21464;&#27491;&#21017;&#21270;(AIR)&#65292;&#24378;&#21046;&#36890;&#36807;ACL&#23398;&#20064;&#21040;&#30340;&#40065;&#26834;&#34920;&#31034;&#20855;&#26377;&#26679;&#24335;&#29420;&#31435;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21464;&#27491;&#21017;&#21270;(IR)&#22686;&#24378;ACL&#65292;&#23427;&#26159;SIR&#21644;AIR&#30340;&#21152;&#26435;&#24635;&#21644;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;AIR&#36890;&#36807;&#38450;&#27490;&#27169;&#22411;&#20381;&#36182;&#26679;&#24335;&#22240;&#32032;&#26469;&#33719;&#24471;&#39640;&#23545;&#27604;&#20998;&#25968;&#65292;&#38544;&#24335;&#22320;&#20419;&#36827;&#20102;ACL&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#25239;&#25915;&#20987;&#21644;&#24120;&#35265;&#27745;&#26579;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;ACL&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial contrastive learning (ACL), without requiring labels, incorporates adversarial data with standard contrastive learning (SCL) and outputs a robust representation which is generalizable and resistant to adversarial attacks and common corruptions. The style-independence property of representations has been validated to be beneficial in improving robustness transferability. Standard invariant regularization (SIR) has been proposed to make the learned representations via SCL to be independent of the style factors. However, how to equip robust representations learned via ACL with the style-independence property is still unclear so far. To this end, we leverage the technique of causal reasoning to propose an adversarial invariant regularization (AIR) that enforces robust representations learned via ACL to be style-independent. Then, we enhance ACL using invariant regularization (IR), which is a weighted sum of SIR and AIR. Theoretically, we show that AIR implicitly encourages the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#31185;&#23398;&#34920;&#26684;&#30340; EL &#25968;&#25454;&#38598; S2abEL&#65292;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#30001;&#20110;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#35821;&#22659;&#24433;&#21709;&#65292;&#31185;&#23398;&#34920;&#26684;&#19978;&#30340; EL &#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340; EL&#65292;&#21253;&#21547;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00366</link><description>&lt;p&gt;
S2abEL&#65306;&#19968;&#20221;&#29992;&#20110;&#31185;&#23398;&#34920;&#26684;&#23454;&#20307;&#38142;&#25509;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
S2abEL: A Dataset for Entity Linking from Scientific Tables. (arXiv:2305.00366v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#31185;&#23398;&#34920;&#26684;&#30340; EL &#25968;&#25454;&#38598; S2abEL&#65292;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#30001;&#20110;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19981;&#23436;&#25972;&#24615;&#21644;&#35821;&#22659;&#24433;&#21709;&#65292;&#31185;&#23398;&#34920;&#26684;&#19978;&#30340; EL &#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340; EL&#65292;&#21253;&#21547;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#23558;&#25991;&#26412;&#25552;&#21450;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30456;&#24212;&#26465;&#30446;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#30693;&#35782;&#23494;&#38598;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#24403;&#24212;&#29992;&#20110;&#31185;&#23398;&#35770;&#25991;&#20013;&#30340;&#34920;&#26684;&#26102;&#65292;EL&#26159;&#23454;&#29616;&#22823;&#35268;&#27169;&#31185;&#23398;&#30693;&#35782;&#24211;&#30340;&#19968;&#27493;&#65292;&#36825;&#21487;&#20197;&#23454;&#29616;&#20808;&#36827;&#30340;&#31185;&#23398;&#38382;&#31572;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#31185;&#23398;&#34920;&#26684;&#20013;&#30340;EL&#30340;&#25968;&#25454;&#38598;&#12290;&#31185;&#23398;&#34920;&#26684;&#30340;EL&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#31185;&#23398;&#30693;&#35782;&#24211;&#21487;&#33021;&#38750;&#24120;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#29702;&#35299;&#35770;&#25991;&#20013;&#30340;&#25991;&#26412;&#20197;&#21450;&#34920;&#26684;&#30340;&#19978;&#19979;&#25991;&#26469;&#28040;&#38500;&#27495;&#20041;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;S2abEL&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#32467;&#26524;&#34920;&#20013;&#30340;EL&#65292;&#24182;&#21253;&#25324;&#26469;&#33258;PaperswithCode&#20998;&#31867;&#27861;&#30340;8,429&#20010;&#21333;&#20803;&#26684;&#30340;&#25163;&#24037;&#26631;&#35760;&#30340;&#21333;&#20803;&#26684;&#31867;&#22411;&#12289;&#26469;&#28304;&#23646;&#24615;&#21644;&#23454;&#20307;&#38142;&#25509;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#31185;&#23398;&#34920;&#26684;&#30340;&#31070;&#32463;&#22522;&#32447;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21253;&#21547;&#35768;&#22810;&#30693;&#35782;&#24211;&#20043;&#22806;&#25552;&#21450;&#30340;&#23454;&#20307;&#65292;&#24182;&#26174;&#31034;&#23427;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperfor
&lt;/p&gt;</description></item><item><title>ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.00365</link><description>&lt;p&gt;
ReLBOT&#65306;&#19968;&#31181;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#20197;&#26368;&#23567;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#24378;&#21270;&#23398;&#20064;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings. (arXiv:2305.00365v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00365
&lt;/p&gt;
&lt;p&gt;
ReLBOT&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#21644;&#28145;&#24230;RL&#25216;&#26415;&#26469;&#20174;&#29616;&#26377;&#30340;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#20248;&#21270;&#21442;&#25968;&#21040;&#26032;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#24341;&#36215;&#30340;&#21021;&#22987;&#19981;&#36866;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#39118;&#38505;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#28909;&#36523;&#26399;&#26102;&#38271;6.2&#20493;&#30340;&#25552;&#39640;&#21644;&#39044;&#27979;&#26041;&#24046;&#30340;132&#20493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#26088;&#22312;&#36890;&#36807;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#26469;&#20248;&#21270;&#33021;&#28304;&#28040;&#32791;&#12290;&#24403;&#26234;&#33021;&#24314;&#31569;&#25237;&#20837;&#20351;&#29992;&#26102;&#65292;&#27809;&#26377;&#21382;&#21490;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#31639;&#27861;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#26174;&#31034;&#20986;&#37325;&#35201;&#30340;&#21069;&#26223;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#23384;&#22312;&#37325;&#22823;&#39118;&#38505;&#65292;&#22240;&#20026;&#24403;RL&#20195;&#29702;&#26368;&#21021;&#25506;&#32034;&#20854;&#34892;&#21160;&#31354;&#38388;&#26102;&#65292;&#23427;&#21487;&#33021;&#20250;&#32473;&#24314;&#31569;&#23621;&#27665;&#24102;&#26469;&#37325;&#22823;&#19981;&#36866;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReLBOT&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#32467;&#21512;&#28145;&#24230;RL&#65292;&#20174;&#29616;&#26377;&#30340;&#20248;&#21270;&#26234;&#33021;&#24314;&#31569;&#20013;&#20256;&#36882;&#30693;&#35782;&#21040;&#26032;&#25237;&#20837;&#20351;&#29992;&#30340;&#24314;&#31569;&#20013;&#65292;&#20197;&#20943;&#23569;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#28909;&#36523;&#26399;&#23545;&#24314;&#31569;&#29289;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25104;&#26524;&#65292;&#28909;&#36523;&#26399;&#30340;&#25345;&#32493;&#26102;&#38388;&#21487;&#25552;&#39640;6.2&#20493;&#65292;&#24182;&#19988;&#39044;&#27979;&#26041;&#24046;&#21487;&#25552;&#39640;132&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20915;&#31574;&#20026;&#20013;&#24515;&#30340;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#20648;&#33021;&#31995;&#32479;&#22871;&#21033;&#65292;&#36890;&#36807;&#36951;&#25022;&#24230;&#37327;&#39044;&#27979;&#20215;&#20540;&#19979;&#30340;&#23454;&#38469;&#20915;&#31574;&#21644;&#22312;&#23454;&#38469;&#20215;&#20540;&#19979;&#30340;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23398;&#20064;&#25552;&#39640;&#39044;&#27979;&#21644;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00362</link><description>&lt;p&gt;
&#30005;&#21147;&#20648;&#33021;&#31995;&#32479;&#22871;&#21033;&#30340;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65306;&#19968;&#31181;&#20197;&#20915;&#31574;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Electricity Price Prediction for Energy Storage System Arbitrage: A Decision-focused Approach. (arXiv:2305.00362v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20915;&#31574;&#20026;&#20013;&#24515;&#30340;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#30005;&#21147;&#20648;&#33021;&#31995;&#32479;&#22871;&#21033;&#65292;&#36890;&#36807;&#36951;&#25022;&#24230;&#37327;&#39044;&#27979;&#20215;&#20540;&#19979;&#30340;&#23454;&#38469;&#20915;&#31574;&#21644;&#22312;&#23454;&#38469;&#20215;&#20540;&#19979;&#30340;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#23398;&#20064;&#25552;&#39640;&#39044;&#27979;&#21644;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#22312;&#33021;&#37327;&#20648;&#23384;&#31995;&#32479;&#65288;ESS&#65289;&#31649;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#39044;&#27979;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#20943;&#23569;&#39044;&#27979;&#35823;&#24046;&#65292;&#20294;&#24573;&#30053;&#20102;&#23427;&#20204;&#23545;&#19979;&#28216;&#20915;&#31574;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20915;&#31574;&#20026;&#20013;&#24515;&#30340;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;ESS&#22871;&#21033;&#65292;&#20197;&#24357;&#21512;&#19979;&#28216;&#20248;&#21270;&#27169;&#22411;&#21644;&#39044;&#27979;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#20915;&#31574;&#28966;&#28857;&#26041;&#27861;&#26088;&#22312;&#21033;&#29992;&#19979;&#28216;&#22871;&#21033;&#27169;&#22411;&#26469;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#36951;&#25022;&#24230;&#37327;&#39044;&#27979;&#20215;&#20540;&#19979;&#30340;&#23454;&#38469;&#20915;&#31574;&#21644;&#22312;&#23454;&#38469;&#20215;&#20540;&#19979;&#30340;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#24046;&#24322;&#65288;&#21363;&#20915;&#31574;&#35823;&#24046;&#65289;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#22788;&#29702;&#30340;&#20195;&#29702;&#36951;&#25022;&#65292;&#24182;&#25454;&#27492;&#25512;&#23548;&#20986;&#39044;&#27979;&#27169;&#22411;&#30340;&#28176;&#21464;&#12290;&#22522;&#20110;&#39044;&#27979;&#35823;&#24046;&#21644;&#20915;&#31574;&#35823;&#24046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#28151;&#21512;&#25439;&#22833;&#21644;&#30456;&#24212;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#39044;&#27979;&#21644;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electricity price prediction plays a vital role in energy storage system (ESS) management. Current prediction models focus on reducing prediction errors but overlook their impact on downstream decision-making. So this paper proposes a decision-focused electricity price prediction approach for ESS arbitrage to bridge the gap from the downstream optimization model to the prediction model. The decision-focused approach aims at utilizing the downstream arbitrage model for training prediction models. It measures the difference between actual decisions under the predicted price and oracle decisions under the true price, i.e., decision error, by regret, transforms it into the tractable surrogate regret, and then derives the gradients to predicted price for training prediction models. Based on the prediction and decision errors, this paper proposes the hybrid loss and corresponding stochastic gradient descent learning method to learn prediction models for prediction and decision accuracy. The 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.00350</link><description>&lt;p&gt;
POUF: &#38754;&#21521;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models. (arXiv:2305.00350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21464;&#24471;&#26356;&#21152;&#34920;&#29616;&#20986;&#33394;&#21644;&#24378;&#22823;&#12290;&#34429;&#28982;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#38646;-shot &#33021;&#21147;&#65292;&#20294;&#36890;&#24120;&#20173;&#38656;&#35201;&#26377;&#26631;&#31614;&#30340;&#25968;&#25454;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#20851;&#38190;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24494;&#35843;&#26694;&#26550;&#65292;&#30452;&#25509;&#22312;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#27169;&#22411;&#25110;&#25552;&#31034;&#12290;&#25105;&#20204;&#28436;&#31034;&#22914;&#20309;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#35328;&#22686;&#24378;&#30340;&#35270;&#35273;&#21644;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;&#40784;&#20174;&#25552;&#31034;&#21644;&#30446;&#26631;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#31163;&#25955;&#20998;&#24067;&#26469;&#23454;&#29616;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#23545;&#22270;&#20687;&#20998;&#31867;&#12289;&#24773;&#24863;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312; 13 &#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#21644; 15 &#20010;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#65292;&#35813;&#26041;&#27861;&#22343;&#27604;&#22522;&#32447;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 language-related ones, the proposed approach achieves consistent improvements over the baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;&#20110;&#21152;&#24615;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#21644;&#21521;&#37327;&#30340;&#20844;&#24335;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#21518;&#39564;&#22343;&#20540;&#12289;&#21518;&#39564;&#26041;&#24046;&#12289;&#23545;&#25968;&#20284;&#28982;&#21644;&#26799;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00324</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#30697;&#38453;&#34920;&#31034;&#21152;&#24615;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Representing Additive Gaussian Processes by Sparse Matrices. (arXiv:2305.00324v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#23545;&#20110;&#21152;&#24615;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#65292;&#36890;&#36807;&#31232;&#30095;&#30697;&#38453;&#21644;&#21521;&#37327;&#30340;&#20844;&#24335;&#21487;&#20197;&#26377;&#25928;&#22320;&#35745;&#31639;&#21518;&#39564;&#22343;&#20540;&#12289;&#21518;&#39564;&#26041;&#24046;&#12289;&#23545;&#25968;&#20284;&#28982;&#21644;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#20041;&#30456;&#21152;&#27169;&#22411;&#20013;&#65292;&#21152;&#24615;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#21487;&#25193;&#23637;&#39640;&#32500;&#38382;&#39064;&#20043;&#19968;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#21152;&#24615;&#32467;&#26500;&#21644;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#34920;&#31034;&#65292;&#22522;&#20110;&#22238;&#24402;&#30340;&#31639;&#27861;&#21487;&#20197;&#23558;&#35745;&#31639;&#21518;&#39564;&#22343;&#20540;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O&#65288;n^3&#65289;$&#20943;&#23569;&#21040;$O&#65288;nlogn&#65289;$&#26102;&#38388;&#65292;&#20854;&#20013;$n$&#26159;&#25968;&#25454;&#22823;&#23567;&#12290;&#20294;&#26159;&#65292;&#23558;&#36825;&#20123;&#31639;&#27861;&#25512;&#24191;&#21040;&#26377;&#25928;&#35745;&#31639;&#21518;&#39564;&#26041;&#24046;&#21644;&#26368;&#22823;&#23545;&#25968;&#20284;&#28982;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#21152;&#24615;Mat&#233;rn&#39640;&#26031;&#36807;&#31243;&#65292;&#19981;&#20165;&#21518;&#39564;&#22343;&#20540;&#65292;&#32780;&#19988;&#21518;&#39564;&#26041;&#24046;&#12289;&#23545;&#25968;&#20284;&#28982;&#21644;&#36825;&#19977;&#20010;&#20989;&#25968;&#30340;&#26799;&#24230;&#21487;&#20197;&#29992;&#20165;&#28041;&#21450;&#31232;&#30095;&#30697;&#38453;&#21644;&#31232;&#30095;&#21521;&#37327;&#30340;&#20844;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#31232;&#30095;&#20844;&#24335;&#26469;&#25512;&#24191;&#22238;&#24402;&#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#35745;&#31639;&#36825;&#19977;&#20010;&#20989;&#25968;&#30340;&#21518;&#39564;&#22343;&#20540;&#12289;&#21518;&#39564;&#26041;&#24046;&#12289;&#23545;&#25968;&#20284;&#28982;&#21644;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among generalized additive models, additive Mat\'ern Gaussian Processes (GPs) are one of the most popular for scalable high-dimensional problems. Thanks to their additive structure and stochastic differential equation representation, back-fitting-based algorithms can reduce the time complexity of computing the posterior mean from $O(n^3)$ to $O(n\log n)$ time where $n$ is the data size. However, generalizing these algorithms to efficiently compute the posterior variance and maximum log-likelihood remains an open problem. In this study, we demonstrate that for Additive Mat\'ern GPs, not only the posterior mean, but also the posterior variance, log-likelihood, and gradient of these three functions can be represented by formulas involving only sparse matrices and sparse vectors. We show how to use these sparse formulas to generalize back-fitting-based algorithms to efficiently compute the posterior mean, posterior variance, log-likelihood, and gradient of these three functions for additiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#24615;&#35777;&#26126;&#20102;&#20174;&#39640;&#26031;&#38543;&#26426;&#22330;&#20013;&#32472;&#21046;&#30340;&#38543;&#26426;&#22522;&#30784;&#20107;&#23454;&#20989;&#25968;&#30340;$L_\infty$-recovery&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#20855;&#26377;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2305.00322</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;$L_\infty$&#24674;&#22797;&#65306;&#39640;&#26031;&#38543;&#26426;&#22330;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Toward $L_\infty$-recovery of Nonlinear Functions: A Polynomial Sample Complexity Bound for Gaussian Random Fields. (arXiv:2305.00322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#24615;&#35777;&#26126;&#20102;&#20174;&#39640;&#26031;&#38543;&#26426;&#22330;&#20013;&#32472;&#21046;&#30340;&#38543;&#26426;&#22522;&#30784;&#20107;&#23454;&#20989;&#25968;&#30340;$L_\infty$-recovery&#21487;&#20197;&#20351;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#20855;&#26377;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#23398;&#20064;&#19968;&#20010;&#22312;&#25972;&#20010;&#36755;&#20837;&#22495;&#19978;&#20855;&#26377;&#23567;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#30340;&#20989;&#25968;&#65292;&#21363;$L_\infty$&#35823;&#24046;&#65292;&#32780;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29702;&#35770;&#24037;&#20316;&#21482;&#20445;&#35777;&#24179;&#22343;&#35823;&#24046;&#30340;&#24674;&#22797;&#65292;&#20363;&#22914;$L_2$&#35823;&#24046;&#12290;&#21363;&#20351;&#23545;&#20110;&#30475;&#20284;&#31616;&#21333;&#30340;&#20989;&#25968;&#31867;&#65292;&#22914;&#24120;&#25968;&#33539;&#25968;&#26080;&#38480;&#23485;&#24230;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#22810;&#39033;&#24335;&#26679;&#26412;&#36827;&#34892;$L_\infty$&#24674;&#22797;&#20063;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;&#20107;&#23454;&#20013;&#30340;&#38543;&#26426;&#24615;&#65292;&#36229;&#36234;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#30340;&#19968;&#20123;&#21021;&#22987;&#27493;&#39588;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20174;&#39640;&#26031;&#38543;&#26426;&#22330;&#20013;&#32472;&#21046;&#30340;&#38543;&#26426;&#22522;&#30784;&#20107;&#23454;&#20989;&#25968;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#21019;&#26032;&#26159;&#35777;&#26126;&#20102;&#39640;&#26031;&#38543;&#26426;&#22330;&#20013;&#30340;$k$&#38454;&#29699;&#35856;&#20989;&#25968;&#20998;&#37327;&#19981;&#33021;&#26159;&#23574;&#38160;&#30340;&#65292;&#21363;&#23427;&#20204;&#30340;$L_\infty$/$L_2$&#27604;&#29575;&#39640;&#27010;&#29575;&#19978;&#38480;&#21046;&#20026;$O(d \sqrt{\ln k})$&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#19968;&#33324;&#20989;&#25968;&#65292;$k$&#38454;&#29699;&#35856;&#20989;&#25968;&#30340;&#26368;&#22351;$L_\infty$/$L_2$&#27604;&#29575;&#26159;&#26080;&#30028;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#20998;&#26512;&#23500;&#20989;&#25968;&#31867;&#21035;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#30340;$L_\infty$&#24674;&#22797;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#24320;&#36767;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning applications require learning a function with a small worst-case error over the entire input domain, that is, the $L_\infty$-error, whereas most existing theoretical works only guarantee recovery in average errors such as the $L_2$-error. $L_\infty$-recovery from polynomial samples is even impossible for seemingly simple function classes such as constant-norm infinite-width two-layer neural nets. This paper makes some initial steps beyond the impossibility results by leveraging the randomness in the ground-truth functions. We prove a polynomial sample complexity bound for random ground-truth functions drawn from Gaussian random fields. Our key technical novelty is to prove that the degree-$k$ spherical harmonics components of a function from Gaussian random field cannot be spiky in that their $L_\infty$/$L_2$ ratios are upperbounded by $O(d \sqrt{\ln k})$ with high probability. In contrast, the worst-case $L_\infty$/$L_2$ ratio for degree-$k$ spherical harmonics i
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMSF&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20445;&#30041;&#27169;&#24577;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#22312;V-I ReID&#20013;&#36866;&#24212;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21160;&#24577;&#24179;&#34913;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00320</link><description>&lt;p&gt;
&#22312;&#21463;&#27745;&#26579;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#19979;&#65292;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30417;&#25511;&#30340;&#35270;&#35273;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;&#30340;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data. (arXiv:2305.00320v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMSF&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#20445;&#30041;&#27169;&#24577;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#40065;&#26834;&#24615;&#12290;&#21516;&#26102;&#65292;&#36824;&#37319;&#29992;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#22312;V-I ReID&#20013;&#36866;&#24212;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#21160;&#24577;&#24179;&#34913;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35265;&#20809;-&#32418;&#22806;&#20154;&#21592;&#20877;&#35782;&#21035;(V-I ReID)&#26088;&#22312;&#21305;&#37197;&#30001;&#20998;&#24067;&#24335;RGB&#21644;IR&#25668;&#20687;&#26426;&#25429;&#33719;&#30340;&#20010;&#20307;&#22270;&#20687;&#12290;&#30001;&#20110;V&#21644;I&#27169;&#24577;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#26465;&#20214;&#19979;&#65292;&#22270;&#20687;&#21463;&#21040;&#27169;&#31946;&#12289;&#22122;&#22768;&#21644;&#22825;&#27668;&#31561;&#22240;&#32032;&#30340;&#24178;&#25200;&#65292;&#35813;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#20808;&#36827;&#30340;V-I ReID&#27169;&#22411;&#19981;&#33021;&#21033;&#29992;&#21463;&#27745;&#26579;&#30340;&#27169;&#24577;&#20449;&#24687;&#26469;&#32500;&#25345;&#39640;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;V-I ReID&#27169;&#22411;&#8212;&#8212;&#21517;&#20026;&#22810;&#27169;&#24577;&#20013;&#38388;&#27969;&#34701;&#21512;(MMSF)&#65292;&#29992;&#20110;&#25552;&#39640;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#38024;&#23545;&#22312;V-I ReID&#20013;&#20986;&#29616;&#30340;&#21463;&#27745;&#26579;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#36866;&#24212;&#20102;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#21487;&#21160;&#24577;&#24179;&#34913;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#36817;&#26399;&#24050;&#25552;&#20986;&#35780;&#20272;&#21327;&#35758;&#20197;&#35780;&#20272;ReID&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Indeed, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID -- named Multimodal Middle Stream Fusion (MMSF) -- that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing to dynamically balance each modality importance. Recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#20803;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;(CMOT)&#65292;&#29992;&#20110;&#39044;&#27979;&#20844;&#24179;&#38543;&#26426;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;VARN-SAM&#65292;&#27604;Birkhoff-von-Neumann&#20998;&#35299;(BvND)&#26356;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CMOT&#23454;&#29616;&#20102;&#20844;&#24179;&#37325;&#26032;&#25490;&#24207;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#19988;&#26356;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.00319</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#29992;&#32422;&#26463;&#20803;&#26368;&#20248;&#36755;&#36816;&#37325;&#26032;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Re-rank with Constrained Meta-Optimal Transport. (arXiv:2305.00319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#12289;&#36731;&#37327;&#32423;&#30340;&#32422;&#26463;&#20803;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;(CMOT)&#65292;&#29992;&#20110;&#39044;&#27979;&#20844;&#24179;&#38543;&#26426;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;VARN-SAM&#65292;&#27604;Birkhoff-von-Neumann&#20998;&#35299;(BvND)&#26356;&#26377;&#25928;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;CMOT&#23454;&#29616;&#20102;&#20844;&#24179;&#37325;&#26032;&#25490;&#24207;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#19988;&#26356;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#31995;&#32479;&#20013;&#65292;&#35768;&#22810;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#20381;&#36182;&#20110;&#38543;&#26426;&#25490;&#21517;&#31574;&#30053;&#65292;&#32534;&#30721;&#20026;&#28385;&#36275;&#26399;&#26395;&#20013;&#25152;&#38656;&#25490;&#21517;&#32422;&#26463;&#30340;&#21452;&#37325;&#38543;&#26426; (DS) &#30697;&#38453;&#65292;&#20363;&#22914;&#26333;&#20809;&#20844;&#24179;&#24615; (FOE)&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#24120;&#26159;&#20004;&#38454;&#27573;&#31649;&#36947;: \emph{i)}&#31163;&#32447;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#26500;&#24314;&#27493;&#39588;&#21644; \emph{ii)}&#22312;&#32447;&#25490;&#21517;&#27493;&#39588;&#12290;&#24314;&#31435;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#65292;&#27599;&#20010;&#21457;&#24067;&#30340;&#26597;&#35810;&#35299;&#20915;&#19968;&#27425;&#12290;&#22240;&#27492;&#65292;&#20026;&#20219;&#20309;&#26032;/&#26410;&#35265;&#36807;&#30340;&#26597;&#35810;&#37325;&#26032;&#35745;&#31639;&#20248;&#21270;&#36807;&#31243;&#26159;&#24517;&#35201;&#30340;&#12290;&#20851;&#20110;&#25277;&#26679;&#65292;Birkhoff-von-Neumann &#20998;&#35299; (BvND) &#26159;&#20174;&#20219;&#20309;&#22522;&#20110;DS&#30340;&#31574;&#30053;&#20013;&#25277;&#21462;&#25490;&#21517;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#35745;&#31639;BvND&#36807;&#20110;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;BvND&#20316;&#20026;&#37319;&#26679;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#20869;&#23384;&#21344;&#29992;&#37327;&#65292;&#23427;&#21487;&#20197;&#38543;&#30528; $N$ &#26597;&#35810;&#21644; $n$ &#25991;&#26723;&#30340;&#22686;&#38271;&#32780;&#22686;&#38271;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159; $\gO(N\, n^2)$&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#24555;&#36895;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#20844;&#24179;&#38543;&#26426;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;: &#32422;&#26463;&#20803;&#26368;&#20248;&#36755;&#36816; (CMOT)&#12290;CMOT&#26159;&#19968;&#31181;&#22312;&#32447;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#35299;&#20915;&#25152;&#26377;&#31163;&#32447;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#19968;&#20010;&#20302;&#25104;&#26412;&#20803;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#20219;&#20309;&#26032;&#26597;&#35810;&#30340;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;CMOT&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;&#21464;&#20998;&#25490;&#21517;&#37319;&#26679; (VARN-SAM)&#65292;&#23427;&#28385;&#36275;&#30456;&#21516;&#30340;&#25490;&#21517;&#32422;&#26463;&#65292;&#21516;&#26102;&#26356;&#20855;&#20869;&#23384;&#25928;&#29575;&#21644;&#26356;&#24555;&#30340;&#35745;&#31639;&#22312;&#32447;&#12290;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;CMOT&#22312;&#20844;&#24179;&#37325;&#26032;&#25490;&#24207;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#24555;&#12289;&#26356;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many re-ranking strategies in search systems rely on stochastic ranking policies, encoded as Doubly-Stochastic (DS) matrices, that satisfy desired ranking constraints in expectation, e.g., Fairness of Exposure (FOE). These strategies are generally two-stage pipelines: \emph{i)} an offline re-ranking policy construction step and \emph{ii)} an online sampling of rankings step. Building a re-ranking policy requires repeatedly solving a constrained optimization problem, one for each issued query. Thus, it is necessary to recompute the optimization procedure for any new/unseen query. Regarding sampling, the Birkhoff-von-Neumann decomposition (BvND) is the favored approach to draw rankings from any DS-based policy. However, the BvND is too costly to compute online. Hence, the BvND as a sampling solution is memory-consuming as it can grow as $\gO(N\, n^2)$ for $N$ queries and $n$ documents.  This paper offers a novel, fast, lightweight way to predict fair stochastic re-ranking policies: Const
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#29702;&#24819;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32479;&#19968;&#20102;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.00316</link><description>&lt;p&gt;
&#29702;&#24819;&#30340;&#19981;&#26029;&#23398;&#20064;&#32773;: &#19981;&#20250;&#36951;&#24536;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
The Ideal Continual Learner: An Agent That Never Forgets. (arXiv:2305.00316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#29702;&#24819;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65288;ICL&#65289;&#65292;&#36890;&#36807;&#26500;&#24314;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32479;&#19968;&#20102;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#27169;&#22411;&#65292;&#35299;&#20915;&#25353;&#39034;&#24207;&#21576;&#29616;&#32473;&#23398;&#20064;&#32773;&#30340;&#22810;&#20010;&#23398;&#20064;&#20219;&#21153;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#26159;&#65292;&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#23398;&#20064;&#32773;&#21487;&#33021;&#20250;&#24536;&#35760;&#22914;&#20309;&#35299;&#20915;&#20808;&#21069;&#30340;&#20219;&#21153;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#35760;&#24518;&#12289;&#22522;&#20110;&#27491;&#21017;&#21270;&#21644;&#22522;&#20110;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#20005;&#26684;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#29702;&#24819;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;&#65288;ICL&#65289;&#65292;&#20174;&#29702;&#35770;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#40511;&#27807;&#20013;&#36328;&#36234;&#36807;&#21435;&#65292;ICL&#30340;&#26500;&#24314;&#20445;&#35777;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32479;&#19968;&#20102;&#22810;&#20010;&#25104;&#29087;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#20026;&#36825;&#20123;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#20026;ICL&#25512;&#23548;&#20986;&#20102;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29702;&#35770;&#19978;&#37327;&#21270;&#22914;&#20309;&#25511;&#21046;&#27169;&#22411;&#30340;&#22797;&#26434;&#24230;&#26469;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may forget how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based, and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called Ideal Continual Learner (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods. We also derive generalization bounds for ICL which allow us to theoretically quantify how r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00312</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#65292;&#36890;&#24120;&#26159;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20540;&#24471;&#20449;&#36182;&#65292;&#23427;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;/&#22810;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12289;&#26368;&#23567;&#21270;&#38544;&#31169;&#27844;&#38706;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#23545;&#24694;&#24847;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26088;&#22312;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#38750;&#24120;&#36866;&#21512;&#35299;&#20915;&#20540;&#24471;&#20449;&#36182;&#30340;&#32852;&#21512;&#23398;&#20064;&#65288;TFL&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;MOO&#21644;TFL&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21046;&#23450;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#32852;&#21512;&#23398;&#20064;&#65288;CMOFL&#65289;&#38382;&#39064;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#21046;&#23450;&#19979;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#36866;&#29992;&#20110;TFL&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;CMOFL&#20316;&#21697;&#19987;&#27880;&#20110;&#25928;&#29992;&#12289;&#25928;&#29575;&#12289;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36825;&#26159;TFL&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;CMOFL&#31639;&#27861;&#65292;&#23427;&#20204;&#36820;&#22238;&#19968;&#32452;&#24179;&#34913;&#33391;&#22909;&#30340;&#27169;&#22411;&#65292;&#28385;&#36275;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#22522;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;Coupled Flow Imitation Learning&#65288;CFIL&#65289;&#65292;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#30340;&#20998;&#24067;&#21305;&#37197;&#26469;&#24314;&#27169;&#29366;&#24577;&#20998;&#24067;&#21644;&#29366;&#24577;&#34892;&#20026;&#20998;&#24067;&#12290;&#22312;&#22522;&#20934;&#20219;&#21153;&#20013;&#20855;&#26377;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00303</link><description>&lt;p&gt;
&#19968;&#31181;&#32806;&#21512;&#27969;&#26041;&#27861;&#29992;&#20110;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Coupled Flow Approach to Imitation Learning. (arXiv:2305.00303v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;Coupled Flow Imitation Learning&#65288;CFIL&#65289;&#65292;&#20351;&#29992;&#27491;&#21017;&#27969;&#27169;&#22411;&#30340;&#20998;&#24067;&#21305;&#37197;&#26469;&#24314;&#27169;&#29366;&#24577;&#20998;&#24067;&#21644;&#29366;&#24577;&#34892;&#20026;&#20998;&#24067;&#12290;&#22312;&#22522;&#20934;&#20219;&#21153;&#20013;&#20855;&#26377;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#31574;&#30053;&#24341;&#36215;&#30340;&#29366;&#24577;&#20998;&#24067;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#23545;&#35937;&#12290;&#23427;&#22312;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#19982;&#30456;&#20851;&#30340;&#29366;&#24577;&#34892;&#20026;&#20998;&#24067;&#19968;&#36215;&#34987;&#24191;&#27867;&#24341;&#29992;&#12290;&#23613;&#31649;&#29366;&#24577;&#20998;&#24067;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#23427;&#22823;&#22810;&#26159;&#38388;&#25509;&#22320;&#21644;&#29702;&#35770;&#19978;&#35752;&#35770;&#65292;&#32780;&#19981;&#26159;&#26126;&#30830;&#22320;&#24314;&#27169;&#12290;&#21407;&#22240;&#26159;&#32570;&#20047;&#36866;&#24403;&#30340;&#23494;&#24230;&#20272;&#35745;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#27491;&#21017;&#27969;&#27169;&#22411;&#30340;&#19978;&#36848;&#20998;&#24067;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#36890;&#36807;Donsker-Varadhan&#34920;&#31034;&#30340;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#30340;&#26368;&#20248;&#28857;&#32806;&#21512;&#30340;&#19968;&#23545;&#27969;&#36827;&#34892;&#20998;&#24067;&#21305;&#37197;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Coupled Flow Imitation Learning&#65288;CFIL&#65289;&#22312;&#20855;&#26377;&#21333;&#20010;&#19987;&#23478;&#36712;&#36857;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33258;&#28982;&#22320;&#25193;&#23637;&#21040;&#21508;&#31181;&#24418;&#24335;&#30340;&#19987;&#23478;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it--along with the related state-action distribution--can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing flow-based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker-Varadhan representation of the Kullback-Leibler (KL) divergence, for distribution matching based imitation learning. Our algorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art performance on benchmark tasks with a single expert trajectory and extends naturally to a varie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MoSS&#65292;&#21487;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#26410;&#25506;&#32034;&#30340;&#20219;&#21153;&#20998;&#24067;&#21450;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.00286</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning. (arXiv:2305.00286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MoSS&#65292;&#21487;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#36866;&#24212;&#26410;&#25506;&#32034;&#30340;&#20219;&#21153;&#20998;&#24067;&#21450;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23398;&#20064;&#30456;&#20851;&#35757;&#32451;&#20219;&#21153;&#24182;&#26368;&#23567;&#21270;&#20132;&#20114;&#25968;&#25454;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33021;&#22815;&#39640;&#25928;&#22320;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#37096;&#20998;&#30456;&#20851;&#30740;&#31350;&#20173;&#23616;&#38480;&#20110;&#21442;&#25968;&#21270;&#21644;&#22266;&#23450;&#20998;&#24067;&#30340;&#29421;&#31364;&#20219;&#21153;&#38598;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#19981;&#32771;&#34385;&#20219;&#21153;&#20998;&#24067;&#30340;&#20559;&#31227;&#65292;&#36825;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#34920;&#31034;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MoSS&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#25193;&#23637;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#21040;&#20102;&#20808;&#21069;&#26410;&#25506;&#32034;&#36807;&#30340;&#24191;&#27867;&#38750;&#21442;&#25968;&#21270;&#20219;&#21153;&#20998;&#24067;&#65292;&#21516;&#26102;&#22312;&#38750;&#22266;&#23450;&#21644;&#20559;&#31227;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#23545;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#30340;&#33021;&#21147;&#22312;&#29627;&#29827;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;SAM&#22312;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#20013;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#29627;&#29827;&#65292;&#36825;&#24341;&#36215;&#20102;&#22312;&#20855;&#26377;&#21508;&#31181;&#24418;&#24335;&#30340;&#29627;&#29827;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#37096;&#32626;SAM&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2305.00278</link><description>&lt;p&gt;
&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#36935;&#21040;&#29627;&#29827;&#65306;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#19981;&#33021;&#34987;&#36731;&#26494;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected. (arXiv:2305.00278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00278
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23545;&#27573;&#33853;&#20219;&#24847;&#27169;&#22411;&#65288;SAM&#65289;&#30340;&#33021;&#21147;&#22312;&#29627;&#29827;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;SAM&#22312;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#20013;&#24448;&#24448;&#26080;&#27861;&#26816;&#27979;&#29627;&#29827;&#65292;&#36825;&#24341;&#36215;&#20102;&#22312;&#20855;&#26377;&#21508;&#31181;&#24418;&#24335;&#30340;&#29627;&#29827;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#37096;&#32626;SAM&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meta AI&#30740;&#31350;&#26368;&#36817;&#21457;&#24067;&#20102;SAM&#65288;Segment Anything Model&#65289;&#65292;&#23427;&#26159;&#22312;&#36229;&#36807;10&#20159;&#20010;&#25513;&#27169;&#30340;&#22823;&#37327;&#20998;&#21106;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#20316;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;SAM&#22312;&#36890;&#29992;&#29289;&#20307;&#20998;&#21106;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#23427;&#22312;&#21508;&#31181;&#38646;-shot&#36801;&#31227;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#26159;&#21542;&#33021;&#22815;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#20013;&#26816;&#27979;&#21040;&#36879;&#26126;&#29289;&#20307;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#19982;&#29627;&#29827;&#30456;&#20851;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#65306;&#38236;&#38754;&#21644;&#36879;&#26126;&#29289;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;SAM&#32463;&#24120;&#26080;&#27861;&#26816;&#27979;&#21040;&#20004;&#31181;&#24773;&#20917;&#19979;&#30340;&#29627;&#29827;&#65292;&#36825;&#24341;&#36215;&#20102;&#22312;&#20855;&#26377;&#21508;&#31181;&#24418;&#24335;&#30340;&#29627;&#29827;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#19979;&#37096;&#32626;SAM&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta AI Research has recently released SAM (Segment Anything Model) which is trained on a large segmentation dataset of over 1 billion masks. As a foundation model in the field of computer vision, SAM (Segment Anything Model) has gained attention for its impressive performance in generic object segmentation. Despite its strong capability in a wide range of zero-shot transfer tasks, it remains unknown whether SAM can detect things in challenging setups like transparent objects. In this work, we perform an empirical evaluation of two glass-related challenging scenarios: mirror and transparent objects. We found that SAM often fails to detect the glass in both scenarios, which raises concern for deploying the SAM in safety-critical situations that have various forms of glass.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMETNet&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#21943;&#21457;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;(CME)&#20174;&#22826;&#38451;&#21040;&#22320;&#29699;&#30340;&#21040;&#36798;&#26102;&#38388;&#65292;&#20197;&#20943;&#23569;&#23545;&#20154;&#31867;&#31995;&#32479;&#30340;&#25439;&#23475;&#12290;</title><link>http://arxiv.org/abs/2305.00258</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#29992;&#20110;CME&#21040;&#36798;&#26102;&#38388;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ensemble Learning for CME Arrival Time Prediction. (arXiv:2305.00258v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00258
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMETNet&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;&#22826;&#38451;&#21943;&#21457;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;(CME)&#20174;&#22826;&#38451;&#21040;&#22320;&#29699;&#30340;&#21040;&#36798;&#26102;&#38388;&#65292;&#20197;&#20943;&#23569;&#23545;&#20154;&#31867;&#31995;&#32479;&#30340;&#25439;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22826;&#38451;&#19981;&#26029;&#21521;&#26085;&#20885;&#23618;&#37322;&#25918;&#36752;&#23556;&#21644;&#31561;&#31163;&#23376;&#20307;&#12290;&#20598;&#23572;&#65292;&#22826;&#38451;&#20250;&#21457;&#23556;&#22826;&#38451;&#21943;&#21457;&#65292;&#22914;&#32768;&#26001;&#21644;&#26085;&#20885;&#29289;&#36136;&#25243;&#23556;(CME)&#12290;CME&#20250;&#24102;&#36208;&#22823;&#37327;&#29289;&#36136;&#21644;&#30913;&#36890;&#37327;&#12290;&#30452;&#25509;&#26397;&#21521;&#22320;&#29699;&#30340;CME&#21487;&#33021;&#20250;&#23545;&#20154;&#31867;&#31995;&#32479;&#36896;&#25104;&#20005;&#37325;&#21518;&#26524;&#12290;&#23427;&#21487;&#20197;&#25703;&#27585;&#30005;&#32593;/&#31649;&#36947;&#65292;&#21355;&#26143;&#21644;&#36890;&#35759;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#30417;&#27979;&#21644;&#39044;&#27979;CME&#23545;&#20110;&#26368;&#23567;&#21270;&#23545;&#20154;&#31867;&#31995;&#32479;&#30340;&#25439;&#23475;&#26159;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CMETNet&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#39044;&#27979;CME&#20174;&#22826;&#38451;&#21040;&#22320;&#29699;&#30340;&#21040;&#36798;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sun constantly releases radiation and plasma into the heliosphere. Sporadically, the Sun launches solar eruptions such as flares and coronal mass ejections (CMEs). CMEs carry away a huge amount of mass and magnetic flux with them. An Earth-directed CME can cause serious consequences to the human system. It can destroy power grids/pipelines, satellites, and communications. Therefore, accurately monitoring and predicting CMEs is important to minimize damages to the human system. In this study we propose an ensemble learning approach, named CMETNet, for predicting the arrival time of CMEs from the Sun to the Earth. We collect and integrate eruptive events from two solar cycles, #23 and #24, from 1996 to 2021 with a total of 363 geoeffective CMEs. The data used for making predictions include CME features, solar wind parameters and CME images obtained from the SOHO/LASCO C2 coronagraph. Our ensemble learning framework comprises regression algorithms for numerical data analysis and a con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;MRI&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#33041;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#65292;&#20026;&#25552;&#39640;&#21307;&#23398;&#19987;&#23478;&#30340;&#24037;&#20316;&#25928;&#29575;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00257</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;MRI&#22270;&#20687;&#20013;&#20998;&#21106;&#33041;&#32959;&#30244;
&lt;/p&gt;
&lt;p&gt;
Brain Tumor Segmentation from MRI Images using Deep Learning Techniques. (arXiv:2305.00257v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;MRI&#25104;&#20687;&#25968;&#25454;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#33041;&#32959;&#30244;&#30340;&#31934;&#30830;&#20998;&#21106;&#65292;&#20026;&#25552;&#39640;&#21307;&#23398;&#19987;&#23478;&#30340;&#24037;&#20316;&#25928;&#29575;&#25552;&#20379;&#20102;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#35770;&#26159;&#33391;&#24615;&#36824;&#26159;&#24694;&#24615;&#30340;&#33041;&#32959;&#30244;&#65292;&#37117;&#21487;&#33021;&#21361;&#21450;&#29983;&#21629;&#65292;&#38656;&#35201;&#31934;&#32454;&#30340;&#21162;&#21147;&#26469;&#30830;&#23450;&#31867;&#22411;&#12289;&#36215;&#28304;&#21644;&#20301;&#32622;&#65292;&#26356;&#19981;&#29992;&#35828;&#27835;&#24840;&#12290;&#21307;&#23398;&#19987;&#23478;&#30340;&#25163;&#21160;&#20998;&#21106;&#21487;&#33021;&#32791;&#36153;&#26102;&#38388;&#65292;&#38656;&#35201;&#25216;&#26415;&#20171;&#20837;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#25105;&#20204;&#26816;&#26597;&#24182;&#30830;&#23450;&#20102;&#33021;&#22815;&#22312;&#33041;&#32959;&#30244;&#20998;&#21106;&#25968;&#25454;&#38598;&#20013;&#21576;&#29616;&#19968;&#33268;&#32467;&#26524;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20844;&#20849;MRI&#25104;&#20687;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;233&#21517;&#24739;&#32773;&#30340;3064&#20010;TI&#21152;&#26435;&#22270;&#20687;&#65292;&#28085;&#30422;&#33041;&#33180;&#30244;&#12289;&#33014;&#36136;&#30244;&#21644;&#22402;&#20307;&#30244;&#19977;&#31181;&#33041;&#32959;&#30244;&#21464;&#20307;&#12290;&#22312;&#23545;&#25968;&#25454;&#38598;&#25991;&#20214;&#36827;&#34892;&#36716;&#25442;&#21644;&#39044;&#22788;&#29702;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20123;&#30693;&#21517;&#30340;&#22270;&#20687;&#20998;&#21106;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#23454;&#29616;&#21644;&#35757;&#32451;&#65292;&#22914;U-Net&#21644;Attention U-Net&#31561;&#19981;&#21516;&#30340;&#39592;&#24178;&#32467;&#26500;&#65292;Deep Residual U-Net&#65292;ResUnet++&#21644;Recurrent Resi&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
A brain tumor, whether benign or malignant, can potentially be life threatening and requires painstaking efforts in order to identify the type, origin and location, let alone cure one. Manual segmentation by medical specialists can be time-consuming, which calls out for the involvement of technology to hasten the process with high accuracy. For the purpose of medical image segmentation, we inspected and identified the capable deep learning model, which shows consistent results in the dataset used for brain tumor segmentation. In this study, a public MRI imaging dataset contains 3064 TI-weighted images from 233 patients with three variants of brain tumor, viz. meningioma, glioma, and pituitary tumor. The dataset files were converted and preprocessed before indulging into the methodology which employs implementation and training of some well-known image segmentation deep learning models like U-Net &amp; Attention U-Net with various backbones, Deep Residual U-Net, ResUnet++ and Recurrent Resi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#25277;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;U-Net&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25351;&#25968;&#20989;&#25968;&#21644;&#30495;&#23454;&#23545;&#27604;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#36870;&#20013;&#20171;&#36136;&#25955;&#23556;&#38382;&#39064;&#30340;&#39640;&#36136;&#37327;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2305.00250</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#25277;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#36870;&#20013;&#20171;&#36136;&#25955;&#23556;&#38382;&#39064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Direct Sampling-Based Deep Learning Approach for Inverse Medium Scattering Problems. (arXiv:2305.00250v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00250
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#25277;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;U-Net&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#25351;&#25968;&#20989;&#25968;&#21644;&#30495;&#23454;&#23545;&#27604;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#23545;&#36870;&#20013;&#20171;&#36136;&#25955;&#23556;&#38382;&#39064;&#30340;&#39640;&#36136;&#37327;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#35299;&#20915;&#36870;&#20013;&#20171;&#36136;&#25955;&#23556;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22522;&#20110;&#25955;&#23556;&#25968;&#25454;&#24674;&#22797;&#26410;&#30693;&#25955;&#23556;&#20307;&#12290;&#22312;&#21463;[23]&#20013;&#24341;&#20837;&#30340;&#39640;&#25928;&#30452;&#25509;&#25277;&#26679;&#26041;&#27861;&#30340;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30452;&#25509;&#25277;&#26679;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;DSM-DL&#65289;&#65292;&#29992;&#20110;&#37325;&#24314;&#38750;&#22343;&#21248;&#25955;&#23556;&#20307;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;U-Net&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#25351;&#25968;&#20989;&#25968;&#21644;&#30495;&#23454;&#23545;&#27604;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;DSM-DL&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#23545;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#33021;&#22815;&#33258;&#28982;&#22320;&#32467;&#21512;&#22810;&#20010;&#27979;&#37327;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#12290;&#36827;&#34892;&#20102;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#19981;&#21516;&#25968;&#37327;&#30340;&#20837;&#23556;&#27874;&#21644;&#19981;&#21516;&#30340;&#22122;&#22768;&#27700;&#24179;&#65292;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#19982;DSM&#30456;&#32467;&#21512;&#22312;IMSP&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we focus on the inverse medium scattering problem (IMSP), which aims to recover unknown scatterers based on measured scattered data. Motivated by the efficient direct sampling method (DSM) introduced in [23], we propose a novel direct sampling-based deep learning approach (DSM-DL)for reconstructing inhomogeneous scatterers. In particular, we use the U-Net neural network to learn the relation between the index functions and the true contrasts. Our proposed DSM-DL is computationally efficient, robust to noise, easy to implement, and able to naturally incorporate multiple measured data to achieve high-quality reconstructions. Some representative tests are carried out with varying numbers of incident waves and different noise levels to evaluate the performance of the proposed method. The results demonstrate the promising benefits of combining deep learning techniques with the DSM for IMSP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#33258;&#30001;&#29983;&#27963;&#26465;&#20214;&#19979;&#24085;&#37329;&#26862;&#30149;&#38663;&#39076;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.00249</link><description>&lt;p&gt;
&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#22686;&#24378;&#22810;&#23454;&#20363;&#23398;&#20064;&#35786;&#26029;&#24085;&#37329;&#26862;&#38663;&#39076;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Leveraging Unlabelled Data in Multiple-Instance Learning Problems for Improved Detection of Parkinsonian Tremor in Free-Living Conditions. (arXiv:2305.00249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#36827;&#34892;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#20197;&#25552;&#39640;&#33258;&#30001;&#29983;&#27963;&#26465;&#20214;&#19979;&#24085;&#37329;&#26862;&#30149;&#38663;&#39076;&#26816;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#29992;&#20110;&#36828;&#31243;&#26816;&#27979;&#24085;&#37329;&#26862;&#30149;&#21450;&#20854;&#36816;&#21160;&#24449;&#35937;&#65292;&#26088;&#22312;&#25552;&#20379;&#26089;&#26399;&#35786;&#26029;&#30340;&#28508;&#22312;&#20020;&#24202;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#26159;&#38590;&#20197;&#36991;&#20813;&#30340;&#65292;&#21516;&#26102;&#33719;&#21462;&#32454;&#31890;&#24230;&#30340;&#26631;&#20934;&#24182;&#19981;&#23481;&#26131;&#12290;&#22240;&#27492;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22810;&#23454;&#20363;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#65292;&#33719;&#21462;&#25152;&#38656;&#30340;&#31895;&#30053;&#26631;&#20934;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#30340;&#31070;&#32463;&#23398;&#35780;&#20272;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25910;&#38598;&#27809;&#26377;&#20219;&#20309;&#26631;&#20934;&#30340;&#25968;&#25454;&#35201;&#23481;&#26131;&#24471;&#22810;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#23454;&#20363;&#23398;&#20064;&#20013;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#19981;&#26159;&#24456;&#30452;&#35266;&#65292;&#22240;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#19982;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30456;&#32467;&#21512;&#26469;&#36827;&#34892;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#20174;&#32780;&#35786;&#26029;&#24085;&#37329;&#26862;&#30149;&#38663;&#39076;&#12290;&#22312;&#30495;&#23454;&#30340;&#24085;&#37329;&#26862;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#33258;&#30001;&#29983;&#27963;&#26465;&#20214;&#19979;&#30340;&#38663;&#39076;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven approaches for remote detection of Parkinson's Disease and its motor symptoms have proliferated in recent years, owing to the potential clinical benefits of early diagnosis. The holy grail of such approaches is the free-living scenario, in which data are collected continuously and unobtrusively during every day life. However, obtaining fine-grained ground-truth and remaining unobtrusive is a contradiction and therefore, the problem is usually addressed via multiple-instance learning. Yet for large scale studies, obtaining even the necessary coarse ground-truth is not trivial, as a complete neurological evaluation is required. In contrast, large scale collection of data without any ground-truth is much easier. Nevertheless, utilizing unlabelled data in a multiple-instance setting is not straightforward, as the topic has received very little research attention. Here we try to fill this gap by introducing a new method for combining semi-supervised with multiple-instance learni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#26696;&#20363;&#34920;&#31034;&#27861;&#65292;&#21487;&#29992;&#20110;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#32929;&#31080;&#25910;&#30410;&#23884;&#20837;&#30340;&#34920;&#31034;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00245</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#26696;&#20363;&#34920;&#31034;&#27861;&#22312;&#34892;&#19994;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Industry Classification Using a Novel Financial Time-Series Case Representation. (arXiv:2305.00245v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#26696;&#20363;&#34920;&#31034;&#27861;&#65292;&#21487;&#29992;&#20110;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#32929;&#31080;&#25910;&#30410;&#23884;&#20837;&#30340;&#34920;&#31034;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#24050;&#34987;&#35777;&#26126;&#26159;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#20016;&#23500;&#28304;&#27849;&#65292;&#21253;&#25324;&#39044;&#27979;&#12289;&#32858;&#31867;&#21644;&#20998;&#31867;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#21363;&#20351;&#26377;&#36739;&#23567;&#30340;&#24615;&#33021;&#25913;&#36827;&#20063;&#21487;&#20197;&#36716;&#21270;&#20026;&#26174;&#33879;&#30340;&#38468;&#21152;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#21363;&#20351;&#29992;&#21382;&#21490;&#32929;&#31080;&#25910;&#30410;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20026;&#20160;&#20040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23545;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#33021;&#20855;&#26377;&#19968;&#20123;&#37325;&#35201;&#30340;&#34920;&#31034;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32929;&#31080;&#25910;&#30410;&#23884;&#20837;&#30340;&#26032;&#22411;&#34920;&#31034;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#32929;&#31080;&#25910;&#30410;&#25968;&#25454;&#20013;&#36731;&#26494;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#34920;&#31034;&#27861;&#38750;&#24120;&#36866;&#21512;&#20110;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#29992;&#20110;&#34892;&#19994;&#37096;&#38376;&#20998;&#31867;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#23454;&#36136;&#24615;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The financial domain has proven to be a fertile source of challenging machine learning problems across a variety of tasks including prediction, clustering, and classification. Researchers can access an abundance of time-series data and even modest performance improvements can be translated into significant additional value. In this work, we consider the use of case-based reasoning for an important task in this domain, by using historical stock returns time-series data for industry sector classification. We discuss why time-series data can present some significant representational challenges for conventional case-based reasoning approaches, and in response, we propose a novel representation based on stock returns embeddings, which can be readily calculated from raw stock returns data. We argue that this representation is well suited to case-based reasoning and evaluate our approach using a large-scale public dataset for the industry sector classification task, demonstrating substantial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21457;&#29616;&#30446;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29273;&#40831;&#20998;&#21106;&#31639;&#27861;&#23545;&#20110;&#23616;&#37096;&#25195;&#25551;&#30340;&#26816;&#27979;&#25928;&#26524;&#36739;&#24046;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.00244</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#19977;&#32500;&#29273;&#40831;&#32593;&#26684;&#20998;&#21106;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65306;&#23545;&#20998;&#21106;&#23616;&#37096;&#25195;&#25551;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Critical Analysis of the Limitation of Deep Learning based 3D Dental Mesh Segmentation Methods in Segmenting Partial Scans. (arXiv:2305.00244v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21457;&#29616;&#30446;&#21069;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29273;&#40831;&#20998;&#21106;&#31639;&#27861;&#23545;&#20110;&#23616;&#37096;&#25195;&#25551;&#30340;&#26816;&#27979;&#25928;&#26524;&#36739;&#24046;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#40831;&#20998;&#21106;&#26159;&#25968;&#23383;&#29273;&#31185;&#23398;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29273;&#40831;&#20998;&#21106;&#31639;&#27861;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#27492;&#20219;&#21153;&#12290;&#34429;&#28982;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#24050;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#29273;&#40831;&#20998;&#21106;&#25216;&#26415;&#23545;&#23436;&#25972;&#19979;&#39052;&#27169;&#22411;&#20570;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#38480;&#21046;&#24615;&#20551;&#35774;&#65292;&#24182;&#19988;&#25253;&#21578;&#22522;&#20110;&#23436;&#25972;&#19979;&#39052;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23436;&#25972;&#30340;&#19979;&#39052;&#29273;&#40831;&#25195;&#25551;&#24182;&#19981;&#38656;&#35201;&#25110;&#21487;&#33021;&#19981;&#21487;&#29992;&#12290;&#37492;&#20110;&#36825;&#20010;&#23454;&#38469;&#38382;&#39064;&#65292;&#20102;&#35299;&#30446;&#21069;&#21487;&#29992;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29273;&#40831;&#20998;&#21106;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#37096;&#20998;&#21475;&#20869;&#25195;&#25551;&#24212;&#29992;&#20102;&#21487;&#29992;&#30340;&#20998;&#21106;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#21487;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26497;&#20854;&#19981;&#36275;&#12290;&#26412;&#25991;&#25152;&#21576;&#29616;&#30340;&#20998;&#26512;&#21644;&#27604;&#36739;&#23558;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#65292;&#24182;&#20801;&#35768;&#25105;&#20204;&#24320;&#21457;&#20986;&#20581;&#22766;&#30340;&#29273;&#40831;&#20998;&#21106;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tooth segmentation from intraoral scans is a crucial part of digital dentistry. Many Deep Learning based tooth segmentation algorithms have been developed for this task. In most of the cases, high accuracy has been achieved, although, most of the available tooth segmentation techniques make an implicit restrictive assumption of full jaw model and they report accuracy based on full jaw models. Medically, however, in certain cases, full jaw tooth scan is not required or may not be available. Given this practical issue, it is important to understand the robustness of currently available widely used Deep Learning based tooth segmentation techniques. For this purpose, we applied available segmentation techniques on partial intraoral scans and we discovered that the available deep Learning techniques under-perform drastically. The analysis and comparison presented in this work would help us in understanding the severity of the problem and allow us to develop robust tooth segmentation techniq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31561;&#20989;&#25968;&#20351;&#24471;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24212;&#29992;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#23454;&#29616;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.00241</link><description>&lt;p&gt;
&#24403;&#28145;&#24230;&#23398;&#20064;&#36935;&#35265;&#22810;&#38754;&#20307;&#29702;&#35770;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31561;&#20989;&#25968;&#20351;&#24471;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24212;&#29992;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#23454;&#29616;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#20102;&#39044;&#27979;&#24314;&#27169;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#24471;&#30410;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#22238;&#24402;&#21040;&#20102;&#22522;&#20110;&#20998;&#27573;&#24120;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#34920;&#31034;&#65292;&#20363;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#65292;&#36825;&#31181;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#24120;&#29992;&#30340;&#31867;&#22411;&#12290;&#36825;&#20351;&#24471;&#26576;&#20123;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22914;&#20856;&#22411;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#31561;&#26041;&#27861;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#28044;&#29616;&#30340;&#20027;&#35201;&#20027;&#39064;&#65292;&#20026;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#24212;&#29992;&#25968;&#23398;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#22522;&#30784;&#30693;&#35782;&#20197;&#21450;&#23427;&#19982;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#22238;&#39038;&#20102;&#35813;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22312;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#20013;&#20351;&#29992;LP&#21644;MILP&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\unicode{x2014}$such as the typical fully-connected feedforward neural network$\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;FAIR&#25968;&#25454;&#21407;&#21017;&#20197;&#25552;&#39640;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;&#36731;&#37327;&#32423;RDF&#26684;&#24335;&#30340;&#26415;&#35821;&#34920;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00238</link><description>&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#30340;FAIR&#31461;&#35805;
&lt;/p&gt;
&lt;p&gt;
The FAIRy Tale of Genetic Algorithms. (arXiv:2305.00238v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;FAIR&#25968;&#25454;&#21407;&#21017;&#20197;&#25552;&#39640;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#37319;&#29992;&#36731;&#37327;&#32423;RDF&#26684;&#24335;&#30340;&#26415;&#35821;&#34920;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#20803;&#21551;&#21457;&#24335;&#36827;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#38543;&#26426;&#31639;&#23376;&#23547;&#25214;&#26368;&#20248;&#35299;&#65292;&#22312;&#35299;&#20915;&#35768;&#22810;&#22797;&#26434;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;&#22914;&#20998;&#31867;&#12289;&#20248;&#21270;&#21644;&#35843;&#24230;&#65289;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;&#20854;&#34920;&#29616;&#12289;&#27969;&#34892;&#21644;&#31616;&#21333;&#24615;&#65292;&#20294;&#23545;GA&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#36824;&#27809;&#26377;&#21463;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#21487;&#21457;&#29616;&#12289;&#21487;&#35775;&#38382;&#12289;&#21487;&#20114;&#29992;&#21644;&#21487;&#37325;&#29992;&#65288;FAIR&#65289;&#25968;&#25454;&#21407;&#21017;&#25193;&#23637;&#21040;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#20854;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#37325;&#29992;&#24615;&#12290;&#25105;&#20204;&#36873;&#25321;GA&#20316;&#20026;&#31034;&#20363;&#28436;&#31034;&#25152;&#25552;&#20986;&#30340;&#21407;&#21017;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#26041;&#27861;&#21457;&#23637;&#21644;GA&#21464;&#31181;&#30340;&#27010;&#36848;&#65292;&#36825;&#20351;&#24471;&#22797;&#21046;&#25110;&#29978;&#33267;&#25214;&#21040;&#27491;&#30830;&#28304;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;FAIR&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#36731;&#37327;&#32423;RDF&#26684;&#24335;&#30340;&#26415;&#35821;&#34920;&#65288;&#21363;$evo$&#65289;&#65292;&#20197;&#20419;&#36827;&#21487;&#37325;&#22797;&#24615;&#12290;&#37492;&#20110;GA&#30340;&#38543;&#26426;&#24615;&#36136;&#65292;
&lt;/p&gt;
&lt;p&gt;
Genetic Algorithm (GA) is a popular meta-heuristic evolutionary algorithm that uses stochastic operators to find optimal solution and has proved its effectiveness in solving many complex optimization problems (such as classification, optimization, and scheduling). However, despite its performance, popularity and simplicity, not much attention has been paid towards reproducibility and reusability of GA. In this paper, we have extended Findable, Accessible, Interoperable and Reusable (FAIR) data principles to enable the reproducibility and reusability of algorithms. We have chosen GA as a usecase to the demonstrate the applicability of the proposed principles. Also we have presented an overview of methodological developments and variants of GA that makes it challenging to reproduce or even find the right source. Additionally, to enable FAIR algorithms, we propose a vocabulary (i.e. $evo$) using light weight RDF format, facilitating the reproducibility. Given the stochastic nature of GAs,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#21152;&#36895;&#21644;&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#29289;&#29702;&#36807;&#31243;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#20302;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#39640;&#25104;&#26412;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#22312;&#21463;&#26426;&#26800;&#25391;&#21160;&#21644;&#28201;&#24230;&#21464;&#21270;&#24433;&#21709;&#30340;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#38646;&#20214;&#36136;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#20943;&#23569;&#23454;&#39564;&#25968;&#25454;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.00229</link><description>&lt;p&gt;
&#21152;&#36895;&#21644;&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#19981;&#23436;&#20840;&#26426;&#26800;&#30693;&#35782;&#30340;&#21046;&#36896;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerated and Inexpensive Machine Learning for Manufacturing Processes with Incomplete Mechanistic Knowledge. (arXiv:2305.00229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#21152;&#36895;&#21644;&#24265;&#20215;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26469;&#33258;&#29289;&#29702;&#36807;&#31243;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#20302;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#39640;&#25104;&#26412;&#23454;&#39564;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#22312;&#21463;&#26426;&#26800;&#25391;&#21160;&#21644;&#28201;&#24230;&#21464;&#21270;&#24433;&#21709;&#30340;&#21046;&#36896;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#38646;&#20214;&#36136;&#37327;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#24182;&#26174;&#33879;&#20943;&#23569;&#23454;&#39564;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21046;&#36896;&#36807;&#31243;&#20013;&#27169;&#25311;&#21442;&#25968;&#25928;&#24212;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#36866;&#29992;&#20110;&#24050;&#24314;&#31435;&#28145;&#24230;&#29289;&#29702;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#20943;&#23569;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#23454;&#39564;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#65292;&#32780;&#24573;&#30053;&#20102;&#24320;&#21457;&#26032;&#36807;&#31243;&#30340;&#23450;&#24615;&#20934;&#30830;&#29289;&#29702;&#27169;&#22411;&#25152;&#20855;&#26377;&#30340;&#20869;&#22312;&#21644;&#26174;&#33879;&#20195;&#20215;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26469;&#33258;&#29289;&#29702;&#36807;&#31243;&#27169;&#22411;&#65288;&#28304;&#65289;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#20302;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#39640;&#25104;&#26412;&#23454;&#39564;&#25968;&#25454;&#65288;&#30446;&#26631;&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#23558;&#28304;&#27169;&#22411;&#30340;&#23450;&#24615;&#31934;&#24230;&#30028;&#38480;&#25512;&#21521;&#25991;&#29486;&#20013;&#35748;&#20026;&#39640;&#30340;&#27700;&#24179;&#65292;&#24182;&#19988;&#23427;&#26159;&#39640;&#27169;&#22411;&#24320;&#21457;&#25104;&#26412;&#30340;&#26681;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#39044;&#27979;&#21046;&#36896;&#36807;&#31243;&#20013;&#21463;&#26426;&#26800;&#25391;&#21160;&#21644;&#28201;&#24230;&#21464;&#21270;&#24433;&#21709;&#30340;&#28155;&#21152;&#21046;&#36896;&#65288;AM&#65289;&#38043;&#21046;&#38646;&#20214;&#30340;&#36136;&#37327;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#24320;&#21457;&#25152;&#38656;&#30340;&#23454;&#39564;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) is of increasing interest for modeling parametric effects in manufacturing processes. But this approach is limited to established processes for which a deep physics-based understanding has been developed over time, since state-of-the-art approaches focus on reducing the experimental and/or computational costs of generating the training data but ignore the inherent and significant cost of developing qualitatively accurate physics-based models for new processes . This paper proposes a transfer learning based approach to address this issue, in which a ML model is trained on a large amount of computationally inexpensive data from a physics-based process model (source) and then fine-tuned on a smaller amount of costly experimental data (target). The novelty lies in pushing the boundaries of the qualitative accuracy demanded of the source model, which is assumed to be high in the literature, and is the root of the high model development cost. Our approach is evaluated f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#22522;&#20110;SPSA&#31639;&#27861;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;SPSA&#30340;&#36817;&#20284;&#26799;&#24230;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#32463;&#20856;&#20248;&#21270;&#22120;&#65292;&#22312;&#31616;&#21333;&#22238;&#24402;&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.00224</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;SPSA&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
An Empirical Comparison of Optimizers for Quantum Machine Learning with SPSA-based Gradients. (arXiv:2305.00224v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#22522;&#20110;SPSA&#31639;&#27861;&#30340;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;SPSA&#30340;&#36817;&#20284;&#26799;&#24230;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#32463;&#20856;&#20248;&#21270;&#22120;&#65292;&#22312;&#31616;&#21333;&#22238;&#24402;&#20013;&#36798;&#21040;&#26356;&#22909;&#30340;&#25910;&#25947;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;VQA&#24050;&#32463;&#24341;&#36215;&#20102;&#37327;&#23376;&#35745;&#31639;&#31038;&#21306;&#30340;&#24456;&#22810;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#28151;&#21512;&#37327;&#23376;-&#32463;&#20856;&#29305;&#24615;&#19982;&#30456;&#23545;&#27973;&#30340;&#37327;&#23376;&#30005;&#36335;&#20351;&#20854;&#25104;&#20026;&#23637;&#31034;NISQ&#35774;&#22791;&#33021;&#21147;&#30340;&#26377;&#21069;&#36884;&#24179;&#21488;&#12290;&#34429;&#28982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#19987;&#27880;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#65292;&#20294;&#20351;&#29992;&#21442;&#25968;&#31227;&#21160;&#35268;&#21017;&#23547;&#25214;&#25509;&#36817;&#31934;&#30830;&#26799;&#24230;&#30340;VQC&#20250;&#24341;&#20837;&#22823;&#37327;&#37319;&#26679;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#26080;&#26799;&#24230;&#20248;&#21270;&#22120;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22280;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#26368;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#20043;&#19968;&#26159;SPSA&#31639;&#27861;&#65292;&#30001;&#20110;&#20854;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#22266;&#26377;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#26469;&#33258;SPSA&#30340;&#36817;&#20284;&#26799;&#24230;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#32463;&#20856;&#20248;&#21270;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#31616;&#21333;&#22238;&#24402;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#32477;&#23545;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;SPSA&#21644;&#21442;&#25968;&#31227;&#21160;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQA have attracted a lot of attention from the quantum computing community for the last few years. Their hybrid quantum-classical nature with relatively shallow quantum circuits makes them a promising platform for demonstrating the capabilities of NISQ devices. Although the classical machine learning community focuses on gradient-based parameter optimization, finding near-exact gradients for VQC with the parameter-shift rule introduces a large sampling overhead. Therefore, gradient-free optimizers have gained popularity in quantum machine learning circles. Among the most promising candidates is the SPSA algorithm, due to its low computational cost and inherent noise resilience. We introduce a novel approach that uses the approximated gradient from SPSA in combination with state-of-the-art gradient-based classical optimizers. We demonstrate numerically that this outperforms both standard SPSA and the parameter-shift rule in terms of convergence rate and absolute error in simple regressi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; PathRTM&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;KI-67&#22686;&#27542;&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#20272;&#35745;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;PathRTM&#20013;&#28155;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#26631;&#31614;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20272;&#31639;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00223</link><description>&lt;p&gt;
PathRTM&#65306;&#23454;&#26102;&#39044;&#27979;KI-67&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
PathRTM: Real-time prediction of KI-67 and tumor-infiltrated lymphocytes. (arXiv:2305.00223v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; PathRTM&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;KI-67&#22686;&#27542;&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#20272;&#35745;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;PathRTM&#20013;&#28155;&#21152;&#33258;&#21160;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#26631;&#31614;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20272;&#31639;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;RTMDet&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;PathRTM&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;KI-67&#22686;&#27542;&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#20272;&#35745;&#12290;KI-67&#22686;&#27542;&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#20272;&#35745;&#22312;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290; PathRTM&#26159;PathoNet&#24037;&#20316;&#30340;&#25193;&#23637;&#65292;&#35813;&#24037;&#20316;&#22312;&#27599;&#20010;&#32454;&#32990;&#20013;&#20351;&#29992;&#21333;&#20687;&#32032;&#20851;&#38190;&#28857;&#65292;&#36890;&#36807;NuClick&#33258;&#21160;&#29983;&#25104;&#30340;&#36793;&#30028;&#26694;&#26631;&#31614;&#25552;&#20379;&#26356;&#39640;&#32423;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;PathRTM&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;KI-67&#22686;&#27542;&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#30340;&#20272;&#35745;&#31934;&#24230;&#12290;&#22312;&#25105;&#20204;&#30340;&#23450;&#21046;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PathRTM&#22312;KI-67&#20813;&#30123;&#38451;&#24615;&#12289;&#20813;&#30123;&#38452;&#24615;&#21644;&#28107;&#24052;&#32454;&#32990;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#31934;&#24230;&#65288;AP&#65289;&#36798;&#21040;41.3&#65285;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;PathRTM&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;KI-67&#22686;&#27542;&#21644;&#32959;&#30244;&#28024;&#28070;&#28107;&#24052;&#32454;&#32990;&#65292;&#25552;&#20379;&#27880;&#37322;&#25928;&#29575;&#21644;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce PathRTM, a novel deep neural network detector based on RTMDet, for automated KI-67 proliferation and tumor-infiltrated lymphocyte estimation. KI-67 proliferation and tumor-infiltrated lymphocyte estimation play a crucial role in cancer diagnosis and treatment. PathRTM is an extension of the PathoNet work, which uses single pixel keypoints for within each cell. We demonstrate that PathRTM, with higher-level supervision in the form of bounding box labels generated automatically from the keypoints using NuClick, can significantly improve KI-67 proliferation and tumorinfiltrated lymphocyte estimation. Experiments on our custom dataset show that PathRTM achieves state-of-the-art performance in KI-67 immunopositive, immunonegative, and lymphocyte detection, with an average precision (AP) of 41.3%. Our results suggest that PathRTM is a promising approach for accurate KI-67 proliferation and tumor-infiltrated lymphocyte estimation, offering annotation efficiency, ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#19979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#21319;&#25299;&#25169;&#36866;&#24212;&#24615;&#24182;&#23884;&#20837;&#29289;&#29702;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#20132;&#12289;&#30452;&#27969;&#30005;&#21147;&#27969;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.00216</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#19979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23454;&#26102;&#20132;&#30452;&#27969;&#30005;&#21147;&#27969;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Physics-Guided Graph Neural Networks for Real-time AC/DC Power Flow Analysis. (arXiv:2305.00216v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#24341;&#23548;&#19979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#25552;&#21319;&#25299;&#25169;&#36866;&#24212;&#24615;&#24182;&#23884;&#20837;&#29289;&#29702;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#30340;&#20132;&#12289;&#30452;&#27969;&#30005;&#21147;&#27969;&#20998;&#26512;&#24037;&#20855;&#65292;&#24182;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#27969;&#21644;&#20132;&#27969;&#28151;&#21512;&#31995;&#32479;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#38656;&#35201;&#27604;&#20197;&#24448;&#26356;&#24555;&#30340;&#30005;&#21147;&#27969;&#20998;&#26512;&#24037;&#20855;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#29289;&#29702;&#24341;&#23548;&#19979;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;PG-GNN&#65289;&#65292;&#36890;&#36807;&#25552;&#21319; PG-GNN &#30340;&#25299;&#25169;&#36866;&#24212;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#20132;&#12289;&#30452;&#27969;&#30005;&#32593;&#30340;&#23450;&#21046;&#22270;&#24418;&#24314;&#27169;&#12290;&#20026;&#20102;&#36991;&#20813;&#20174;&#25968;&#25454;&#20013;&#22797;&#21046;&#19981;&#21487;&#38752;&#30340;&#32463;&#39564;&#65292;&#21033;&#29992;&#23545;&#20598;&#21407;&#29702;&#23558;&#20132;&#12289;&#30452;&#27969;&#29289;&#29702;&#23884;&#20837; PG-GNN &#20013;&#12290;&#25509;&#30528;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#23398;&#20064;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161; PG-GNN &#20197;&#26080;&#30417;&#30563;&#26080;&#26631;&#31614;&#30340;&#26041;&#24335;&#26356;&#22909;&#22320;&#23398;&#20064;&#38750;&#20984;&#27169;&#24335;&#12290;&#26368;&#21518;&#36827;&#34892;&#22810;&#20010;PG-GNN&#26469;&#25484;&#25569;&#19981;&#21516;&#30340;&#30452;&#27969;&#25511;&#21046;&#27169;&#24335;&#12290;&#26696;&#20363;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20854;&#20182;7&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#65292;&#21482;&#26377;&#26412;&#35770;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#22522;&#20110;&#27169;&#22411;&#30340;&#22522;&#20934;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#29978;&#33267;&#36229;&#36807;&#20854;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing scale of alternating current and direct current (AC/DC) hybrid systems necessitates a faster power flow analysis tool than ever. This letter thus proposes a specific physics-guided graph neural network (PG-GNN). The tailored graph modelling of AC and DC grids is firstly advanced to enhance the topology adaptability of the PG-GNN. To eschew unreliable experience emulation from data, AC/DC physics are embedded in the PG-GNN using duality. Augmented Lagrangian method-based learning scheme is then presented to help the PG-GNN better learn nonconvex patterns in an unsupervised label-free manner. Multi-PG-GNN is finally conducted to master varied DC control modes. Case study shows that, relative to the other 7 data-driven rivals, only the proposed method matches the performance of the model-based benchmark, also beats it in computational efficiency beyond 10 times.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EBLIME&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#27169;&#22411;&#20174;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35299;&#37322;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.00213</link><description>&lt;p&gt;
EBLIME: &#22686;&#24378;&#30340;&#36125;&#21494;&#26031;&#26412;&#22320;&#21487;&#35299;&#37322;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EBLIME: Enhanced Bayesian Local Interpretable Model-agnostic Explanations. (arXiv:2305.00213v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EBLIME&#26041;&#27861;&#65292;&#20351;&#29992;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#27169;&#22411;&#20174;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#35299;&#37322;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20998;&#24067;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EBLIME&#26469;&#35299;&#37322;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#23725;&#22238;&#24402;&#27169;&#22411;&#33719;&#24471;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21253;&#25324;&#23725;&#21442;&#25968;&#30340;&#26174;&#33879;&#24615;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#24037;&#19994;&#24212;&#29992;&#26696;&#20363;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#28041;&#21450;&#23450;&#20301;&#21046;&#36896;&#20135;&#21697;&#20869;&#37096;&#32570;&#38519;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;EBLIME&#20135;&#29983;&#26356;&#30452;&#35266;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#65292;&#21253;&#25324;&#29983;&#25104;&#21518;&#39564;&#20998;&#24067;&#12289;&#21487;&#20449;&#21306;&#38388;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose EBLIME to explain black-box machine learning models and obtain the distribution of feature importance using Bayesian ridge regression models. We provide mathematical expressions of the Bayesian framework and theoretical outcomes including the significance of ridge parameter. Case studies were conducted on benchmark datasets and a real-world industrial application of locating internal defects in manufactured products. Compared to the state-of-the-art methods, EBLIME yields more intuitive and accurate results, with better uncertainty quantification in terms of deriving the posterior distribution, credible intervals, and rankings of the feature importance.
&lt;/p&gt;</description></item><item><title>ShipHullGAN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#33337;&#20307;&#21442;&#25968;&#21270;&#24314;&#27169;&#22120;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#29983;&#25104;&#21644;&#34920;&#24449;&#21508;&#31181;&#31867;&#22411;&#30340;&#33337;&#33334;&#12290;&#36890;&#36807;&#26032;&#30340;&#24418;&#29366;&#25552;&#21462;&#21644;&#34920;&#31034;&#31574;&#30053;&#65292;&#23558;&#25152;&#26377;&#35757;&#32451;&#35774;&#35745;&#36716;&#25442;&#20026;&#30456;&#21516;&#20998;&#36776;&#29575;&#30340;&#20960;&#20309;&#34920;&#31034;&#24418;&#24335;&#65292;&#24182;&#22312;&#29983;&#25104;&#22120;&#21518;&#28155;&#21152;&#31354;&#38388;&#22635;&#20805;&#23618;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#22120;&#21487;&#20197;&#35206;&#30422;&#25152;&#26377;&#35774;&#35745;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.00210</link><description>&lt;p&gt;
ShipHullGAN: &#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26500;&#24314;&#30340;&#36890;&#29992;&#33337;&#20307;&#21442;&#25968;&#21270;&#24314;&#27169;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShipHullGAN: A generic parametric modeller for ship hull design using deep convolutional generative model. (arXiv:2305.00210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00210
&lt;/p&gt;
&lt;p&gt;
ShipHullGAN&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#33337;&#20307;&#21442;&#25968;&#21270;&#24314;&#27169;&#22120;&#65292;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#29983;&#25104;&#21644;&#34920;&#24449;&#21508;&#31181;&#31867;&#22411;&#30340;&#33337;&#33334;&#12290;&#36890;&#36807;&#26032;&#30340;&#24418;&#29366;&#25552;&#21462;&#21644;&#34920;&#31034;&#31574;&#30053;&#65292;&#23558;&#25152;&#26377;&#35757;&#32451;&#35774;&#35745;&#36716;&#25442;&#20026;&#30456;&#21516;&#20998;&#36776;&#29575;&#30340;&#20960;&#20309;&#34920;&#31034;&#24418;&#24335;&#65292;&#24182;&#22312;&#29983;&#25104;&#22120;&#21518;&#28155;&#21152;&#31354;&#38388;&#22635;&#20805;&#23618;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#22120;&#21487;&#20197;&#35206;&#30422;&#25152;&#26377;&#35774;&#35745;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ShipHullGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#26500;&#24314;&#30340;&#36890;&#29992;&#33337;&#20307;&#21442;&#25968;&#21270;&#24314;&#27169;&#22120;&#65292;&#21487;&#29992;&#20110;&#33337;&#20307;&#30340;&#29983;&#25104;&#21644;&#34920;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#38598;&#35013;&#31665;&#33337;&#12289;&#27833;&#36718;&#12289;&#25955;&#36135;&#33337;&#12289;&#25302;&#36718;&#21644;&#20379;&#24212;&#33337;&#31561;&#22810;&#31181;&#33337;&#33334;&#31867;&#22411;&#65289;&#23545;ShipHullGAN&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24418;&#29366;&#25552;&#21462;&#21644;&#34920;&#31034;&#31574;&#30053;&#65292;&#23558;&#25152;&#26377;&#35757;&#32451;&#35774;&#35745;&#36716;&#25442;&#20026;&#30456;&#21516;&#20998;&#36776;&#29575;&#30340;&#20849;&#21516;&#20960;&#20309;&#34920;&#31034;&#24418;&#24335;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21518;&#28155;&#21152;&#20102;&#19968;&#20010;&#31354;&#38388;&#22635;&#20805;&#23618;&#65292;&#20197;&#30830;&#20445;&#25152;&#35757;&#32451;&#30340;&#29983;&#25104;&#22120;&#21487;&#20197;&#35206;&#30422;&#25152;&#26377;&#35774;&#35745;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ShipHullGAN, a generic parametric modeller built using deep convolutional generative adversarial networks (GANs) for the versatile representation and generation of ship hulls. At a high level, the new model intends to address the current conservatism in the parametric ship design paradigm, where parametric modellers can only handle a particular ship type. We trained ShipHullGAN on a large dataset of 52,591 \textit{physically validated} designs from a wide range of existing ship types, including container ships, tankers, bulk carriers, tugboats, and crew supply vessels. We developed a new shape extraction and representation strategy to convert all training designs into a common geometric representation of the same resolution, as typically GANs can only accept vectors of fixed dimension as input. A space-filling layer is placed right after the generator component to ensure that the trained generator can cover all design classes. During training, designs are pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;DDGroup&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20855;&#26377;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#32479;&#19968;&#32447;&#24615;&#20851;&#31995;&#30340;&#23376;&#32676;&#65292;&#20026;&#21307;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.00195</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#32447;&#24615;&#22238;&#24402;&#23376;&#32676;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Subgroup Identification for Linear Regression. (arXiv:2305.00195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;DDGroup&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#20855;&#26377;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#32479;&#19968;&#32447;&#24615;&#20851;&#31995;&#30340;&#23376;&#32676;&#65292;&#20026;&#21307;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30740;&#31350;&#24120;&#24120;&#38656;&#35201;&#25552;&#21462;&#27599;&#20010;&#21327;&#21464;&#37327;&#19982;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#32622;&#20449;&#24230;&#27979;&#37327;&#12290;&#20026;&#27492;&#65292;&#36890;&#24120;&#20351;&#29992;&#31616;&#21333;&#30340;&#21442;&#25968;&#27169;&#22411;&#65288;&#20363;&#22914;&#32447;&#24615;&#22238;&#24402;&#31995;&#25968;&#65289;&#65292;&#20294;&#36890;&#24120;&#26159;&#22312;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#21327;&#21464;&#37327;&#21487;&#33021;&#22312;&#25972;&#20010;&#20154;&#21475;&#20013;&#27809;&#26377;&#32479;&#19968;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#32479;&#19968;&#30340;&#31616;&#21333;&#27169;&#22411;&#21487;&#33021;&#20250;&#28431;&#25481;&#24322;&#36136;&#20449;&#21495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;DDGroup (data-driven group discovery)&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#25968;&#25454;&#20013;&#20855;&#26377;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#32479;&#19968;&#32447;&#24615;&#20851;&#31995;&#30340;&#23376;&#32676;&#12290;DDGroup&#36755;&#20986;&#30340;&#21306;&#22495;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#65292;&#32780;&#19988;&#22312;&#35745;&#31639;&#19978;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#20110;&#20351;&#29992;&#12290;&#29702;&#35770;&#19978;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#32473;&#23450;&#36275;&#22815;&#22823;&#30340;&#26679;&#26412;&#65292;DDGroup&#20445;&#35777;&#21487;&#20197;&#25214;&#21040;&#20855;&#26377;&#32479;&#19968;&#32447;&#24615;&#20851;&#31995;&#30340;&#23376;&#32676;&#12290;&#25105;&#20204;&#36824;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;DDGroup&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical studies frequently require to extract the relationship between each covariate and the outcome with statistical confidence measures. To do this, simple parametric models are frequently used (e.g. coefficients of linear regression) but usually fitted on the whole dataset. However, it is common that the covariates may not have a uniform effect over the whole population and thus a unified simple model can miss the heterogeneous signal. For example, a linear model may be able to explain a subset of the data but fail on the rest due to the nonlinearity and heterogeneity in the data. In this paper, we propose DDGroup (data-driven group discovery), a data-driven method to effectively identify subgroups in the data with a uniform linear relationship between the features and the label. DDGroup outputs an interpretable region in which the linear model is expected to hold. It is simple to implement and computationally tractable for use. We show theoretically that, given a large enough samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00169</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#35777;&#25454;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System. (arXiv:2305.00169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35777;&#25454;&#25512;&#29702;&#31639;&#27861;&#21644;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22810;&#31181;&#24037;&#20917;&#34920;&#29616;&#20986;&#30340;&#38750;&#39640;&#26031;&#12289;&#22810;&#27169;&#24577;&#21644;&#20013;&#24515;&#28418;&#31227;&#29305;&#24449;&#65292;&#25925;&#38556;&#35786;&#26029;&#26159;&#24037;&#19994;&#30028;&#30740;&#31350;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#23427;&#20204;&#22312;&#36830;&#32493;&#25925;&#38556;&#20998;&#31867;&#21644;&#25925;&#38556;&#20998;&#31867;&#22120;&#21442;&#25968;&#26356;&#26032;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#22810;&#31181;&#25805;&#20316;&#27169;&#24335;&#21644;&#23454;&#26102;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#23454;&#29616;&#24037;&#19994;&#31995;&#32479;&#30340;&#23454;&#26102;&#22810;&#27169;&#24577;&#25925;&#38556;&#35786;&#26029;&#26159;&#19968;&#20010;&#36843;&#20999;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35777;&#25454;&#25512;&#29702;&#65288;ER&#65289;&#31639;&#27861;&#26469;&#34701;&#21512;&#20449;&#24687;&#24182;&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#22522;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#12290;&#36825;&#20123;&#22522;&#20998;&#31867;&#22120;&#20351;&#29992;&#24191;&#20041;&#23398;&#20064;&#31995;&#32479;&#65288;BLS&#65289;&#24320;&#21457;&#65292;&#20197;&#25552;&#39640;&#33391;&#22909;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#26469;&#23454;&#26102;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fault diagnosis is a crucial area of research in the industry due to diverse operating conditions that exhibit non-Gaussian, multi-mode, and center-drift characteristics. Currently, data-driven approaches are the main focus in the field, but they pose challenges for continuous fault classification and parameter updates of fault classifiers, particularly in multiple operating modes and real-time settings. Therefore, a pressing issue is to achieve real-time multi-mode fault diagnosis for industrial systems. To address this problem, this paper proposes a novel approach that utilizes an evidence reasoning (ER) algorithm to fuse information and merge outputs from different base classifiers. These base classifiers are developed using a broad learning system (BLS) to improve good fault diagnosis performance. Moreover, in this approach, the pseudo-label learning method is employed to update model parameters in real-time. To demonstrate the effectiveness of the proposed approach, we perform exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#37329;&#23646;&#27687;&#21270;&#29289;&#20316;&#20026;&#38459;&#21464;&#23384;&#20648;&#22120;&#27687;&#21270;&#23618;&#22312;RRAM&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;AI&#25216;&#26415;&#30340;&#24212;&#29992;&#21487;&#20248;&#21270;RRAM&#22120;&#20214;&#24615;&#33021;&#65292;&#21516;&#26102;RRAM&#22120;&#20214;&#26412;&#36523;&#20063;&#21487;&#20197;&#20316;&#20026;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#31070;&#32463;&#24418;&#24577;&#23398;&#35745;&#31639;&#30340;&#21160;&#21147;&#28304;&#26469;&#25512;&#21160;AI&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.00166</link><description>&lt;p&gt;
&#37329;&#23646;&#27687;&#21270;&#29289;&#20316;&#20026;&#38459;&#21464;&#23384;&#20648;&#22120;&#27687;&#21270;&#23618;&#22312;RRAM&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Combination of Metal Oxides as Oxide Layers for RRAM and Artificial Intelligence. (arXiv:2305.00166v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#37329;&#23646;&#27687;&#21270;&#29289;&#20316;&#20026;&#38459;&#21464;&#23384;&#20648;&#22120;&#27687;&#21270;&#23618;&#22312;RRAM&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;AI&#25216;&#26415;&#30340;&#24212;&#29992;&#21487;&#20248;&#21270;RRAM&#22120;&#20214;&#24615;&#33021;&#65292;&#21516;&#26102;RRAM&#22120;&#20214;&#26412;&#36523;&#20063;&#21487;&#20197;&#20316;&#20026;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#31070;&#32463;&#24418;&#24577;&#23398;&#35745;&#31639;&#30340;&#21160;&#21147;&#28304;&#26469;&#25512;&#21160;AI&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#39640;&#36895;&#12289;&#20302;&#21151;&#32791;&#21644;&#20248;&#24322;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#38459;&#21464;&#23384;&#20648;&#22120;(RRAM)&#26159;&#19979;&#19968;&#20195;&#23384;&#20648;&#35774;&#22791;&#30340;&#26377;&#21147;&#20505;&#36873;&#32773;&#12290;&#37329;&#23646;&#27687;&#21270;&#29289;&#36890;&#24120;&#29992;&#20316;RRAM&#22120;&#20214;&#20013;&#30340;&#27687;&#21270;&#23618;&#65292;&#30001;&#20110;&#20854;&#39640;&#20171;&#30005;&#24120;&#25968;&#21644;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;RRAM&#22120;&#20214;&#30340;&#24615;&#33021;&#65292;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#38598;&#25104;&#20154;&#24037;&#26234;&#33021;(AI)&#12290;AI&#21487;&#29992;&#20110;&#20248;&#21270;RRAM&#22120;&#20214;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;RRAM&#20063;&#21487;&#20197;&#20316;&#20026;&#30828;&#20214;&#21152;&#36895;&#22120;&#21644;&#31070;&#32463;&#24418;&#24577;&#23398;&#35745;&#31639;&#30340;&#21160;&#21147;&#28304;&#26469;&#25512;&#21160;AI&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#37329;&#23646;&#27687;&#21270;&#29289;&#22522;RRAM&#21644;AI&#32467;&#21512;&#30340;&#27010;&#36848;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#36825;&#20004;&#20010;&#26041;&#21521;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;AI&#26469;&#25913;&#21892;RRAM&#22120;&#20214;&#24615;&#33021;&#21644;&#21033;&#29992;RRAM&#26469;&#25512;&#21160;AI&#30340;&#24212;&#29992;&#65292;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#25361;&#25112;&#24182;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resistive random-access memory (RRAM) is a promising candidate for next-generation memory devices due to its high speed, low power consumption, and excellent scalability. Metal oxides are commonly used as the oxide layer in RRAM devices due to their high dielectric constant and stability. However, to further improve the performance of RRAM devices, recent research has focused on integrating artificial intelligence (AI). AI can be used to optimize the performance of RRAM devices, while RRAM can also power AI as a hardware accelerator and in neuromorphic computing. This review paper provides an overview of the combination of metal oxides-based RRAM and AI, highlighting recent advances in these two directions. We discuss the use of AI to improve the performance of RRAM devices and the use of RRAM to power AI. Additionally, we address key challenges in the field and provide insights into future research directions
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#22270;&#24418;&#38543;&#26426;&#29305;&#24449;&#65288;GRFs&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22270;&#20869;&#26680;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#33410;&#28857;&#25968;&#30340;&#31435;&#26041;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#23558;&#23450;&#20041;&#22312;&#22270;&#19978;&#30340;&#20869;&#26680;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#32593;&#32476;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.00156</link><description>&lt;p&gt;
&#29992;&#38543;&#26426;&#29305;&#24449;&#39535;&#26381;&#22270;&#20869;&#26680;
&lt;/p&gt;
&lt;p&gt;
Taming graph kernels with random features. (arXiv:2305.00156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#22270;&#24418;&#38543;&#26426;&#29305;&#24449;&#65288;GRFs&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22270;&#20869;&#26680;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#33410;&#28857;&#25968;&#30340;&#31435;&#26041;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#21487;&#23558;&#23450;&#20041;&#22312;&#22270;&#19978;&#30340;&#20869;&#26680;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#38543;&#26426;&#29305;&#24449;&#30340;&#22270;&#24418;&#38543;&#26426;&#29305;&#24449;&#65288;GRFs&#65289;&#26426;&#21046;&#12290;GRFs&#21487;&#20197;&#29992;&#20110;&#26500;&#24314;&#38024;&#23545;&#22270;&#33410;&#28857;&#23450;&#20041;&#30340;&#22810;&#20010;&#37325;&#35201;&#20869;&#26680;&#30340;&#26080;&#20559;&#38543;&#26426;&#20272;&#35745;&#22120;&#65292;&#29305;&#21035;&#26159;&#27491;&#21017;&#21270;&#25289;&#26222;&#25289;&#26031;&#20869;&#26680;&#12290;&#19982;&#38750;&#22270;&#24418;&#20869;&#26680;&#30340;&#24120;&#35268;RF&#19968;&#26679;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#25163;&#27573;&#26469;&#23558;&#23450;&#20041;&#22312;&#22270;&#19978;&#30340;&#20869;&#26680;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#32593;&#32476;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20204;&#22312;&#24212;&#29992;&#20110;&#19979;&#28216;&#24212;&#29992;&#26102;&#65292;&#23545;&#20110;&#36739;&#23567;&#30340;&#22270;&#20063;&#25552;&#20379;&#20102;&#22823;&#24133;&#30340;&#35745;&#31639;&#25910;&#30410;&#12290;&#22240;&#27492;&#65292;GRFs&#35299;&#20915;&#20102;&#22270;&#20869;&#26680;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#26159;&#33410;&#28857;&#25968;&#30340;&#31435;&#26041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;GRFs&#30340;&#35814;&#32454;&#29702;&#35770;&#20998;&#26512;&#21644;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65306;&#20174;&#36895;&#24230;&#27979;&#35797;&#65292;&#36890;&#36807;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#30456;&#23545;&#35823;&#24046;&#20998;&#26512;&#21040;&#20351;&#29992;&#22270;&#20869;&#26680;&#36827;&#34892;kmeans&#22270;&#32858;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GRFs&#35745;&#31639;&#20013;&#23384;&#22312;&#30528;&#19968;&#20010;&#20196;&#20154;&#23604;&#23596;&#31616;&#21333;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38656;&#35201;&#20998;&#21106;&#30340;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce in this paper the mechanism of graph random features (GRFs). GRFs can be used to construct unbiased randomized estimators of several important kernels defined on graphs' nodes, in particular the regularized Laplacian kernel. As regular RFs for non-graph kernels, they provide means to scale up kernel methods defined on graphs to larger networks. Importantly, they give substantial computational gains also for smaller graphs, while applied in downstream applications. Consequently, GRFs address the notoriously difficult problem of cubic (in the number of the nodes of the graph) time complexity of graph kernels algorithms. We provide a detailed theoretical analysis of GRFs and an extensive empirical evaluation: from speed tests, through Frobenius relative error analysis to kmeans graph-clustering with graph kernels. We show that the computation of GRFs admits an embarrassingly simple distributed algorithm that can be applied if the graph under consideration needs to be split ac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;</title><link>http://arxiv.org/abs/2305.00152</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#19979;&#30340;&#27169;&#22411;&#36873;&#25321;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00152
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#19979;&#27169;&#22411;&#36873;&#25321;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#20854;&#36716;&#31227;&#36317;&#31163;&#20250;&#24433;&#21709;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21487;&#33021;&#23548;&#33268;&#36895;&#29575;&#36739;&#24930;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20851;&#20110;&#36716;&#31227;&#23398;&#20064;&#25110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#24050;&#30693;&#20551;&#35774;&#31867;&#25110;&#27169;&#22411;&#30340;&#24773;&#20917;&#65307;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#24120;&#28041;&#21450;&#19968;&#23450;&#31243;&#24230;&#30340;&#27169;&#22411;&#36873;&#25321;&#65292;&#36825;&#32463;&#24120;&#20986;&#29616;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24635;&#20307;&#33539;&#30068;&#19979;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#32771;&#34385;&#35843;&#25972;&#38024;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#27491;&#30830;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#30456;&#20851;&#28304;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#19982;&#27169;&#22411;&#36873;&#25321;&#26377;&#20851;&#30340;&#36817;&#20284;&#19982;&#20272;&#35745;&#35823;&#24046;&#30340;&#36890;&#24120;&#26435;&#34913;&#20043;&#22806;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#24102;&#26469;&#20102;&#26032;&#30340;&#22797;&#26434;&#24230;&#65292;&#21363;&#28304;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#36716;&#31227;&#36317;&#31163;&#65292;&#36825;&#20010;&#36317;&#31163;&#38543;&#30528;&#20551;&#35774;&#31867;&#30340;&#36873;&#25321;&#32780;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#31867;&#38382;&#39064;&#12290;&#29305;&#21035;&#30340;&#65292;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20123;&#24341;&#20154;&#27880;&#30446;&#30340;&#29616;&#35937;&#65306;&#33258;&#36866;&#24212;&#36895;&#29575;&#65292;&#21363;&#27809;&#26377;&#20998;&#24067;&#24335;&#20449;&#24687;&#26102;&#21487;&#36798;&#21040;&#30340;&#36895;&#29575;&#65292;&#21487;&#20197;&#20219;&#24847;&#24930;&#20110;oracle&#36895;&#29575;&#65292;&#21363;&#22312;&#32473;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#30340;&#36172;&#21338;&#31574;&#30053;&#26469;&#35299;&#20915;&#39640;&#32500;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00143</link><description>&lt;p&gt;
&#39034;&#24207;&#39044;&#27979;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#27979;&#30340;&#36172;&#21338;&#31574;&#30053;&#26469;&#35299;&#20915;&#39640;&#32500;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#19979;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39034;&#24207;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#38382;&#39064;&#12290;&#39034;&#24207;&#26816;&#39564;&#22312;&#32447;&#22788;&#29702;&#25968;&#25454;&#65292;&#20801;&#35768;&#20351;&#29992;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26469;&#20915;&#23450;&#26159;&#21542;&#20572;&#27490;&#24182;&#25298;&#32477;&#21407;&#20551;&#35774;&#65292;&#25110;&#22312;&#20445;&#25345;&#31867;&#22411;I&#38169;&#35823;&#25511;&#21046;&#30340;&#21516;&#26102;&#25910;&#38598;&#26356;&#22810;&#25968;&#25454;&#12290;&#25105;&#20204;&#24314;&#31435;&#22312;(&#38750;&#21442;&#25968;)&#27979;&#35797;&#36172;&#21338;&#21407;&#21017;&#20043;&#19978;&#65292;&#20854;&#20013;&#36172;&#24466;&#22312;&#26410;&#26469;&#35266;&#23519;&#20013;&#19979;&#27880;&#65292;&#20182;&#20204;&#30340;&#36130;&#23500;&#23545;&#35777;&#25454;&#21453;&#23545;&#21407;&#20551;&#35774;&#36827;&#34892;&#34913;&#37327;&#12290;&#26368;&#36817;&#24320;&#21457;&#30340;&#22522;&#20110;&#26680;&#30340;&#36172;&#21338;&#31574;&#30053;&#22312;&#31616;&#21333;&#20998;&#24067;&#19978;&#36890;&#24120;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#39640;&#32500;&#25110;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#36873;&#25321;&#21512;&#36866;&#30340;&#26680;&#36890;&#24120;&#26159;&#26840;&#25163;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#39044;&#27979;&#30340;&#36172;&#21338;&#31574;&#30053;&#65292;&#20381;&#36182;&#20110;&#20197;&#19979;&#20107;&#23454;&#65306;&#22914;&#26524;&#19968;&#20010;&#39034;&#24207;&#26356;&#26032;&#30340;&#39044;&#27979;&#22120;&#24320;&#22987;&#19968;&#33268;&#22320;&#30830;&#23450;(a)&#19968;&#20010;&#23454;&#20363;&#20174;&#21738;&#20010;&#20998;&#24067;&#20013;&#32472;&#21046;&#65292;&#25110;&#32773;(b)&#19968;&#20010;&#23454;&#20363;&#26159;&#20174;&#32852;&#21512;&#20998;&#24067;&#36824;&#26159;&#20174;&#36793;&#32536;&#20998;&#24067;&#30340;&#20056;&#31215;&#20013;&#32472;&#21046;&#30340;&#65292;&#21017;&#20998;&#24067;&#26159;&#19981;&#21516;&#25110;&#30456;&#20851;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28789;&#27963;&#65292;&#24182;&#23545;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#21644;&#32500;&#24230;&#19981;&#21487;&#30693;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#23450;&#30340;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#39034;&#24207;&#27979;&#35797;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problems of sequential nonparametric two-sample and independence testing. Sequential tests process data online and allow using observed data to decide whether to stop and reject the null hypothesis or to collect more data while maintaining type I error control. We build upon the principle of (nonparametric) testing by betting, where a gambler places bets on future observations and their wealth measures evidence against the null hypothesis. While recently developed kernel-based betting strategies often work well on simple distributions, selecting a suitable kernel for high-dimensional or structured data, such as text and images, is often nontrivial. To address this drawback, we design prediction-based betting strategies that rely on the following fact: if a sequentially updated predictor starts to consistently determine (a) which distribution an instance is drawn from, or (b) whether an instance is drawn from the joint distribution or the product of the marginal distributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#19981;&#22343;&#21248;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#24335;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2305.00139</link><description>&lt;p&gt;
&#21033;&#29992;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks. (arXiv:2305.00139v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#19981;&#22343;&#21248;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#36827;&#34892;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#21464;&#21270;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#24335;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#36827;&#34892;&#33410;&#28857;&#20998;&#31867;&#26102;&#65292;&#20856;&#22411;&#30340;&#27169;&#22411;&#20250;&#20026;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#19981;&#21516;&#31867;&#26631;&#31614;&#30340;&#23545;&#25968;&#12290;&#36890;&#36807;&#36825;&#20123;&#29992;softmax&#23618;&#36755;&#20986;&#26368;&#22823;&#23545;&#25968;&#30340;&#26631;&#31614;&#39044;&#27979;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20174;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25512;&#26029;&#20986;&#38544;&#34255;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#23427;&#26159;&#20174;&#23545;&#25968;&#30340;softmax&#20998;&#24067;&#21644;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#20013;&#23548;&#20986;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;&#23567;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#30340;&#33410;&#28857;&#26356;&#38590;&#20998;&#31867;&#27491;&#30830;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#21464;&#21270;&#65292;&#36825;&#25552;&#20379;&#20102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#35265;&#35299;&#65306;&#22686;&#21152;&#39640;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#30340;&#35757;&#32451;&#26679;&#26412;&#25110;&#21024;&#38500;&#36793;&#20197;&#20943;&#23569;&#20855;&#26377;&#23567;&#26631;&#31614;&#19981;&#22343;&#21248;&#24615;&#30340;&#33410;&#28857;&#38598;&#30340;&#26368;&#22823;&#21106;&#22823;&#23567;&#12290;&#36825;&#20123;&#26426;&#21046;&#21487;&#20197;&#36731;&#26494;&#22320;&#28155;&#21152;&#21040;&#22522;&#26412;&#30340;GNN&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In node classification using graph neural networks (GNNs), a typical model generates logits for different class labels at each node. A softmax layer often outputs a label prediction based on the largest logit. We demonstrate that it is possible to infer hidden graph structural information from the dataset using these logits. We introduce the key notion of label non-uniformity, which is derived from the Wasserstein distance between the softmax distribution of the logits and the uniform distribution. We demonstrate that nodes with small label non-uniformity are harder to classify correctly. We theoretically analyze how the label non-uniformity varies across the graph, which provides insights into boosting the model performance: increasing training samples with high non-uniformity or dropping edges to reduce the maximal cut size of the node set of small non-uniformity. These mechanisms can be easily added to a base GNN model. Experimental results demonstrate that our approach improves the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;THz&#29992;&#25143;&#20307;&#39564;&#40065;&#26834;&#24615;&#30340;&#19977;&#20301;&#19968;&#20307;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;THz&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#25552;&#21462;&#29992;&#25143;&#21644;&#29615;&#22659;&#30340;&#29420;&#29305;&#24863;&#30693;&#21442;&#25968;&#65292;&#32467;&#21512;&#22810;&#20998;&#36776;&#29575;&#29983;&#25104;&#24335;AI&#26469;&#39044;&#27979;&#26410;&#26469;&#20449;&#24687;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32852;&#21512;&#24863;&#30693;-&#36890;&#20449;&#21327;&#35758;&#26469;&#20248;&#21270;XR&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.00135</link><description>&lt;p&gt;
THz&#29992;&#25143;&#20307;&#39564;&#40065;&#26834;&#24615;&#30340;&#19977;&#20301;&#19968;&#20307;&#65306;&#32852;&#21512;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Joint Sensing, Communication, and AI: A Trifecta for Resilient THz User Experiences. (arXiv:2305.00135v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;THz&#29992;&#25143;&#20307;&#39564;&#40065;&#26834;&#24615;&#30340;&#19977;&#20301;&#19968;&#20307;&#26694;&#26550;&#65292;&#21253;&#25324;&#32852;&#21512;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#12290;&#36890;&#36807;&#21033;&#29992;THz&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#25552;&#21462;&#29992;&#25143;&#21644;&#29615;&#22659;&#30340;&#29420;&#29305;&#24863;&#30693;&#21442;&#25968;&#65292;&#32467;&#21512;&#22810;&#20998;&#36776;&#29575;&#29983;&#25104;&#24335;AI&#26469;&#39044;&#27979;&#26410;&#26469;&#20449;&#24687;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32852;&#21512;&#24863;&#30693;-&#36890;&#20449;&#21327;&#35758;&#26469;&#20248;&#21270;XR&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#24863;&#30693;&#12289;&#36890;&#20449;&#21644;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22826;&#36203;&#20857;&#65288;THz&#65289;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#20307;&#39564;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#65292;&#20998;&#21035;&#26159;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#24352;&#37327;&#20998;&#35299;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;THz&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#26469;&#25552;&#21462;XR&#29992;&#25143;&#21450;&#20854;&#29615;&#22659;&#30340;&#29420;&#29305;&#24863;&#30693;&#21442;&#25968;&#65307;&#20854;&#27425;&#65292;&#32467;&#21512;&#23545;&#25239;&#24615;&#21464;&#25442;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20998;&#36776;&#29575;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#21644;&#26410;&#26469;&#30340;&#24863;&#30693;&#20449;&#24687;&#65307;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#24863;&#30693;-&#36890;&#20449;&#21327;&#35758;&#65292;&#20854;&#20013;&#24863;&#30693;&#21644;&#36890;&#20449;&#21151;&#33021;&#20132;&#38169;&#36827;&#34892;&#65292;&#24182;&#20197;&#20998;&#20139;&#21644;&#21512;&#20316;&#30340;&#26041;&#24335;&#36816;&#20316;&#12290;&#35813;&#21327;&#35758;&#38024;&#23545;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;XR&#20307;&#39564;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20855;&#26377;&#20174;&#24863;&#30693;&#21644;&#36890;&#20449;&#22833;&#36133;&#20013;&#24674;&#22797;&#30340;&#33021;&#21147;&#12290;&#20223;&#30495;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper a novel joint sensing, communication, and artificial intelligence (AI) framework is proposed so as to optimize extended reality (XR) experiences over terahertz (THz) wireless systems. The proposed framework consists of three main components. First, a tensor decomposition framework is proposed to extract unique sensing parameters for XR users and their environment by exploiting then THz channel sparsity. Essentially, THz band's quasi-opticality is exploited and the sensing parameters are extracted from the uplink communication signal, thereby allowing for the use of the same waveform, spectrum, and hardware for both communication and sensing functionalities. Then, the Cramer-Rao lower bound is derived to assess the accuracy of the estimated sensing parameters. Second, a non-autoregressive multi-resolution generative artificial intelligence (AI) framework integrated with an adversarial transformer is proposed to predict missing and future sensing information. The proposed f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#29289;&#32852;&#32593;&#39537;&#21160;&#26234;&#33021;&#23396;&#32593;&#24494;&#30005;&#32593;&#20013;&#26612;&#27833;&#21457;&#30005;&#26426;&#32452;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#25968;&#25454;&#29983;&#25104;&#23454;&#26102;&#20915;&#31574;&#20197;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#65292;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.00127</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#39537;&#21160;&#26234;&#33021;&#23396;&#32593;&#24494;&#30005;&#32593;&#30340;&#26368;&#20248;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Optimal Scheduling in IoT-Driven Smart Isolated Microgrids Based on Deep Reinforcement Learning. (arXiv:2305.00127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#29289;&#32852;&#32593;&#39537;&#21160;&#26234;&#33021;&#23396;&#32593;&#24494;&#30005;&#32593;&#20013;&#26612;&#27833;&#21457;&#30005;&#26426;&#32452;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#21382;&#21490;&#25968;&#25454;&#29983;&#25104;&#23454;&#26102;&#20915;&#31574;&#20197;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#65292;&#24182;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29289;&#32852;&#32593;&#39537;&#21160;&#30340;&#23396;&#31435;&#24494;&#30005;&#32593;&#20013;&#65292;&#22914;&#20309;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#26612;&#27833;&#21457;&#30005;&#26426;&#32452;&#30340;&#35843;&#24230;&#38382;&#39064;&#12290;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#36127;&#36733;&#38656;&#27714;&#19981;&#30830;&#23450;&#24615;&#19979;&#65292;&#20805;&#20998;&#21033;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#36890;&#36807;&#23398;&#20064;&#24182;&#24314;&#31435;&#21382;&#21490;&#21487;&#20877;&#29983;&#36164;&#28304;&#21644;&#36127;&#36733;&#25968;&#25454;&#30340;&#26368;&#20248;&#31574;&#30053;&#27169;&#22411;&#65292;&#20197;&#20415;&#20174;&#20808;&#21069;&#23567;&#26102;&#20869;&#30456;&#24212;&#20256;&#24863;&#22120;&#25509;&#25910;&#21040;&#30340;&#36807;&#21435;&#21487;&#20877;&#29983;&#33021;&#28304;&#21644;&#36127;&#36733;&#25968;&#25454;&#30340;&#35266;&#27979;&#20013;&#29983;&#25104;&#23454;&#26102;&#20915;&#31574;&#12290;&#30446;&#26631;&#22312;&#20110;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#30340;&#21069;&#25552;&#19979;&#20943;&#23569;&#25805;&#20316;&#25104;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#38480;&#35270;&#30028;&#19979;&#39532;&#24403;&#21069;&#36807;&#31243;&#27169;&#22411;&#65288;POMDP&#65289;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#26059;&#36716;&#22791;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#20110;&#20108;&#36827;&#21046;&#21457;&#30005;&#26426;&#32452;&#24320;&#20851;&#20915;&#31574;&#21644;&#36830;&#32493;&#36752;&#23556;&#65288;ED&#65289;&#20915;&#31574;&#30340;&#31163;&#25955;-&#36830;&#32493;&#28151;&#21512;&#20316;&#29992;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#21160;&#20316;&#26377;&#38480;&#35270;&#35282;RDPG&#65288;HAFH-RDPG&#65289;&#30340;DRL&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the scheduling issue of diesel generators (DGs) in an Internet of Things (IoT)-Driven isolated microgrid (MG) by deep reinforcement learning (DRL). The renewable energy is fully exploited under the uncertainty of renewable generation and load demand. The DRL agent learns an optimal policy from history renewable and load data of previous days, where the policy can generate real-time decisions based on observations of past renewable and load data of previous hours collected by connected sensors. The goal is to reduce operating cost on the premise of ensuring supply-demand balance. In specific, a novel finite-horizon partial observable Markov decision process (POMDP) model is conceived considering the spinning reserve. In order to overcome the challenge of discrete-continuous hybrid action space due to the binary DG switching decision and continuous energy dispatch (ED) decision, a DRL algorithm, namely the hybrid action finite-horizon RDPG (HAFH-RDPG), is pr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36870;&#21521;&#20272;&#31639;&#32593;&#26684;&#31895;&#21270;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;CFD&#27169;&#25311;&#20013;&#21152;&#20837;&#36825;&#20123;&#20449;&#24687;&#65292;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#32593;&#26684;&#27169;&#25311;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.00114</link><description>&lt;p&gt;
&#36890;&#36807;&#23616;&#37096;&#26426;&#22120;&#23398;&#20064;&#26657;&#27491;&#25913;&#21892;CFD&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Improving CFD simulations by local machine-learned correction. (arXiv:2305.00114v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36870;&#21521;&#20272;&#31639;&#32593;&#26684;&#31895;&#21270;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#22312;CFD&#27169;&#25311;&#20013;&#21152;&#20837;&#36825;&#20123;&#20449;&#24687;&#65292;&#25552;&#39640;&#20302;&#20998;&#36776;&#29575;&#32593;&#26684;&#27169;&#25311;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35299;&#20915;&#26356;&#31934;&#32454;&#30340;&#23610;&#24230;&#24102;&#26469;&#30340;&#39640;&#25104;&#26412;&#65292;&#39640;&#20445;&#30495;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;(CFD)&#27169;&#25311;&#22312;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#20013;&#21487;&#33021;&#20250;&#38750;&#24120;&#26114;&#36149;&#12290;&#36825;&#31181;&#35745;&#31639;&#25104;&#26412;/&#31934;&#24230;&#24179;&#34913;&#26159;&#29616;&#20195;CFD&#27169;&#25311;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23398;&#20064;&#39044;&#27979;&#20316;&#20026;&#22823;&#23610;&#24230;&#27969;&#29305;&#24449;&#20989;&#25968;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#65292;&#20197;&#36870;&#21521;&#20272;&#31639;&#22240;&#32593;&#26684;&#31895;&#21270;&#32780;&#22833;&#21435;&#20449;&#24687;&#30340;&#31243;&#24230;&#12290;&#28982;&#21518;&#22312;&#36816;&#34892;&#26102;&#23558;&#36825;&#20123;&#20449;&#24687;&#28155;&#21152;&#22238;&#20302;&#20998;&#36776;&#29575;&#35299;&#20013;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#19981;&#36275;&#20998;&#36776;&#29575;&#30340;&#31895;&#32593;&#26684;&#27169;&#25311;&#30340;&#36136;&#37327;&#12290;&#20351;&#29992;&#26356;&#31895;&#31961;&#30340;&#32593;&#26684;&#20135;&#29983;&#20102;&#36895;&#24230;&#19978;&#30340;&#38750;&#32447;&#24615;&#20248;&#21183;&#65292;&#32780;&#25512;&#26029;&#21644;&#32416;&#27491;&#20002;&#22833;&#20449;&#24687;&#30340;&#25104;&#26412;&#20855;&#26377;&#32447;&#24615;&#25104;&#26412;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#24037;&#31243;&#24847;&#20041;&#30340;&#38382;&#39064;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#21363;&#19977;&#32500;&#28237;&#27969;&#36890;&#36947;&#27969;&#21160;&#12290;&#38500;&#20102;&#36825;&#20010;&#28436;&#31034;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26356;&#22810;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-fidelity computational fluid dynamics (CFD) simulations for design space explorations can be exceedingly expensive due to the cost associated with resolving the finer scales. This computational cost/accuracy trade-off is a major challenge for modern CFD simulations. In the present study, we propose a method that uses a trained machine learning model that has learned to predict the discretization error as a function of largescale flow features to inversely estimate the degree of lost information due to mesh coarsening. This information is then added back to the low-resolution solution during runtime, thereby enhancing the quality of the under-resolved coarse mesh simulation. The use of a coarser mesh produces a non-linear benefit in speed while the cost of inferring and correcting for the lost information has a linear cost. We demonstrate the numerical stability of a problem of engineering interest, a 3D turbulent channel flow. In addition to this demonstration, we further show the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22312;&#37096;&#32626;&#26102;&#38388;&#20026;&#20010;&#24615;&#21270;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#21387;&#21147;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.00111</link><description>&lt;p&gt;
&#26085;&#24120;&#29615;&#22659;&#20013;&#20010;&#24615;&#21270;&#21387;&#21147;&#30417;&#27979;&#30340;&#20027;&#21160;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Active Reinforcement Learning for Personalized Stress Monitoring in Everyday Settings. (arXiv:2305.00111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#22312;&#37096;&#32626;&#26102;&#38388;&#20026;&#20010;&#24615;&#21270;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#26085;&#24120;&#29615;&#22659;&#20013;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#21387;&#21147;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#30417;&#27979;&#26694;&#26550;&#37117;&#20551;&#23450;&#38656;&#35201;&#22788;&#29702;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#20934;&#30830;&#30340;&#26816;&#27979;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#22312;&#37096;&#32626;&#26102;&#38388;&#38656;&#35201;&#20010;&#24615;&#21270;&#24494;&#35843;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#38656;&#35201;&#36890;&#36807;&#19982;&#29992;&#25143;&#20132;&#20114;&#22312;&#32447;&#25910;&#38598;&#29305;&#23450;&#20110;&#20010;&#20154;&#30340;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#20010;&#38454;&#27573;&#20248;&#21270;&#26631;&#31614;&#30340;&#25910;&#38598;&#26159;&#26045;&#21152;&#21487;&#23481;&#24525;&#30340;&#29992;&#25143;&#36127;&#25285;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20010;&#20154;&#30340;&#25913;&#21892;&#12290;&#26412;&#25991;&#32771;&#34385;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#32454;&#31890;&#24230;&#21387;&#21147;&#26816;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#20849;&#21516;&#26368;&#22823;&#21270;&#26377;&#24847;&#20041;&#30340;&#20449;&#21495;&#26679;&#26412;&#30340;&#35831;&#27714;&#21644;&#22238;&#24212;&#29575;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22810;&#23618;&#20256;&#24863;&#22120;&#36793;&#32536;&#20113;&#24179;&#21488;&#65292;&#20197;&#23450;&#26399;&#25429;&#33719;&#29983;&#29702;&#20449;&#21495;&#24182;&#23454;&#26102;&#22788;&#29702;&#23427;&#20204;&#65292;&#20197;&#21450;&#25910;&#38598;&#26631;&#31614;&#24182;&#37325;&#26032;&#35757;&#32451;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing sensor-based monitoring frameworks presume that a large available labeled dataset is processed to train accurate detection models. However, in settings where personalization is necessary at deployment time to fine-tune the model, a person-specific dataset needs to be collected online by interacting with the users. Optimizing the collection of labels in such phase is instrumental to impose a tolerable burden on the users while maximizing personal improvement. In this paper, we consider a fine-grain stress detection problem based on wearable sensors targeting everyday settings, and propose a novel context-aware active learning strategy capable of jointly maximizing the meaningfulness of the signal samples we request the user to label and the response rate. We develop a multilayered sensor-edge-cloud platform to periodically capture physiological signals and process them in real-time, as well as to collect labels and retrain the detection model. We collect a large dataset an
&lt;/p&gt;</description></item><item><title>NNSplitter&#26159;&#19968;&#31181;&#20027;&#21160;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#28151;&#28102;&#27169;&#22411;&#21644;&#27169;&#22411;&#31192;&#23494;&#20004;&#37096;&#20998;&#65292;&#37319;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#21644;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.00097</link><description>&lt;p&gt;
NNSplitter&#65306;&#22522;&#20110;&#33258;&#21160;&#26435;&#37325;&#28151;&#28102;&#30340;DNN&#27169;&#22411;&#20027;&#21160;&#38450;&#24481;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00097
&lt;/p&gt;
&lt;p&gt;
NNSplitter&#26159;&#19968;&#31181;&#20027;&#21160;&#20445;&#25252;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#28151;&#28102;&#27169;&#22411;&#21644;&#27169;&#22411;&#31192;&#23494;&#20004;&#37096;&#20998;&#65292;&#37319;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#21644;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#20135;&#26435;&#65292;&#24050;&#32463;&#36890;&#36807;&#25968;&#23383;&#27700;&#21360;&#31561;&#25216;&#26415;&#36827;&#34892;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#34987;&#21160;&#27169;&#22411;&#20445;&#25252;&#24182;&#19981;&#33021;&#23436;&#20840;&#38450;&#27490;&#27169;&#22411;&#28389;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#27169;&#22411;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#26041;&#26696;&#65292;&#21363;NNSplitter&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#20998;&#20026;&#20004;&#37096;&#20998;&#26469;&#20027;&#21160;&#20445;&#25252;&#27169;&#22411;&#65306;&#19968;&#20010;&#34920;&#29616;&#36739;&#24046;&#30340;&#28151;&#28102;&#27169;&#22411;&#21644;&#30001;&#28151;&#28102;&#26435;&#37325;&#30340;&#32034;&#24341;&#21644;&#21407;&#22987;&#20540;&#32452;&#25104;&#30340;&#27169;&#22411;&#31192;&#23494;&#65292;&#21482;&#26377;&#25480;&#26435;&#29992;&#25143;&#25165;&#33021;&#35775;&#38382;&#12290;NNSplitter&#21033;&#29992;&#21487;&#20449;&#25191;&#34892;&#29615;&#22659;&#26469;&#20445;&#25252;&#31192;&#23494;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#26469;&#20943;&#23569;&#28151;&#28102;&#26435;&#37325;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#31934;&#24230;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20462;&#25913;&#36229;&#36807;2800&#19975;&#20010;&#26435;&#37325;&#30340;313&#20010;&#65288;&#21363;0.001&#65285;&#65289;&#65292;&#28151;&#28102;VGG-11&#27169;&#22411;&#22312;Fashion-MNIST&#19978;&#30340;&#31934;&#24230;&#21487;&#20197;&#38477;&#20302;&#21040;10&#65285;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;NNSplitter&#20855;&#26377;&#38544;&#34109;&#24615;&#21644;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#21487;&#24494;&#20998;&#21018;&#20307;&#27169;&#25311;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#25509;&#35302;&#27861;&#32447;&#26041;&#21521;&#19981;&#22266;&#23450;&#26102;&#20250;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#30340;&#30896;&#25758;&#26816;&#27979;&#21644;&#21033;&#29992;&#30896;&#25758;&#26102;&#38388;&#26469;&#35745;&#31639;&#30896;&#25758;&#21518;&#30340;&#36895;&#24230;&#26469;&#25913;&#36827;&#26799;&#24230;&#35745;&#31639;&#30340;&#26041;&#27861;TOI-Velocity&#12290;</title><link>http://arxiv.org/abs/2305.00092</link><description>&lt;p&gt;
&#36890;&#36807;&#25509;&#35302;&#25913;&#36827;&#21487;&#24494;&#29289;&#29702;&#27169;&#25311;&#30340;&#26799;&#24230;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Improving Gradient Computation for Differentiable Physics Simulation with Contacts. (arXiv:2305.00092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#21487;&#24494;&#20998;&#21018;&#20307;&#27169;&#25311;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#25509;&#35302;&#27861;&#32447;&#26041;&#21521;&#19981;&#22266;&#23450;&#26102;&#20250;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#32493;&#30340;&#30896;&#25758;&#26816;&#27979;&#21644;&#21033;&#29992;&#30896;&#25758;&#26102;&#38388;&#26469;&#35745;&#31639;&#30896;&#25758;&#21518;&#30340;&#36895;&#24230;&#26469;&#25913;&#36827;&#26799;&#24230;&#35745;&#31639;&#30340;&#26041;&#27861;TOI-Velocity&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#27169;&#25311;&#20351;&#26799;&#24230;&#33021;&#22815;&#21453;&#21521;&#20256;&#25773;&#21040;&#29289;&#29702;&#27169;&#25311;&#20013;&#12290;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#21487;&#20197;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#24577;&#21644;&#23646;&#24615;&#65292;&#25110;&#32773;&#23558;&#25972;&#20010;&#21487;&#24494;&#20998;&#27169;&#25311;&#23884;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#35268;&#21010;&#21644;&#25511;&#21046;&#65289;&#30340;&#19968;&#23618;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#24182;&#19981;&#23436;&#21892;&#65292;&#21487;&#33021;&#20250;&#25552;&#20379;&#38169;&#35823;&#30340;&#26799;&#24230;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#20219;&#21153;&#20013;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25509;&#35302;&#24773;&#20917;&#19979;&#30340;&#21487;&#24494;&#20998;&#21018;&#20307;&#27169;&#25311;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#25509;&#35302;&#27861;&#32447;&#26041;&#21521;&#19981;&#22266;&#23450;&#26102;&#65292;&#29616;&#26377;&#30340;&#21487;&#24494;&#20998;&#27169;&#25311;&#26041;&#27861;&#20250;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#26799;&#24230;&#8212;&#8212;&#36825;&#26159;&#24403;&#25509;&#35302;&#26159;&#20004;&#20010;&#31227;&#21160;&#29289;&#20307;&#20043;&#38388;&#21457;&#29983;&#26102;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#36830;&#32493;&#30340;&#30896;&#25758;&#26816;&#27979;&#21644;&#21033;&#29992;&#30896;&#25758;&#26102;&#38388;&#65288;TOI&#65289;&#26469;&#35745;&#31639;&#30896;&#25758;&#21518;&#30340;&#36895;&#24230;&#65292;&#20174;&#32780;&#25913;&#36827;&#26799;&#24230;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#32852;&#31995;&#30340;&#21018;&#20307;&#31995;&#32479;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;TOI-Velocity&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable simulation enables gradients to be back-propagated through physics simulations. In this way, one can learn the dynamics and properties of a physics system by gradient-based optimization or embed the whole differentiable simulation as a layer in a deep learning model for downstream tasks, such as planning and control. However, differentiable simulation at its current stage is not perfect and might provide wrong gradients that deteriorate its performance in learning tasks. In this paper, we study differentiable rigid-body simulation with contacts. We find that existing differentiable simulation methods provide inaccurate gradients when the contact normal direction is not fixed - a general situation when the contacts are between two moving objects. We propose to improve gradient computation by continuous collision detection and leverage the time-of-impact (TOI) to calculate the post-collision velocities. We demonstrate our proposed method, referred to as TOI-Velocity, on tw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31867;&#20998;&#31867;&#20013;&#25932;&#23545;&#35757;&#32451;&#30340;&#40065;&#26834;&#35299;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#24182;&#19982;&#26368;&#20248;&#20256;&#36755;&#21644;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23545;&#19981;&#21487;&#30693;&#20998;&#31867;&#22120;&#30340;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.00075</link><description>&lt;p&gt;
&#22810;&#31867;&#20998;&#31867;&#20013;&#25932;&#23545;&#35757;&#32451;&#35299;&#30340;&#23384;&#22312;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the existence of solutions to adversarial training in multiclass classification. (arXiv:2305.00075v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#31867;&#20998;&#31867;&#20013;&#25932;&#23545;&#35757;&#32451;&#30340;&#40065;&#26834;&#35299;&#23384;&#22312;&#24615;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#24182;&#19982;&#26368;&#20248;&#20256;&#36755;&#21644;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23545;&#19981;&#21487;&#30693;&#20998;&#31867;&#22120;&#30340;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25932;&#23545;&#35757;&#32451;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#19977;&#31181;&#27169;&#22411;&#65292;&#26088;&#22312;&#26500;&#24314;&#23545;&#25239;&#25200;&#21160;&#19979;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#27599;&#20010;&#27169;&#22411;&#20013;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#40065;&#26834;&#20998;&#31867;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#25299;&#23637;&#20102;&#20316;&#32773;&#20043;&#21069;&#30340;&#26368;&#20248;&#20256;&#36755;&#32852;&#31995;&#65292;&#24182;&#22312;&#22810;&#31867;&#24773;&#20917;&#19979;&#25932;&#23545;&#35757;&#32451;&#21644;&#24635;&#21464;&#24046;&#27491;&#21017;&#21270;&#20043;&#38388;&#24314;&#31435;&#20102;&#26032;&#30340;&#32852;&#31995;&#12290;&#20316;&#20026;&#25105;&#20204;&#32467;&#26524;&#30340;&#25512;&#35770;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#20013;&#65292;&#23545;&#19981;&#21487;&#30693;&#20998;&#31867;&#22120;&#30340;&#25932;&#23545;&#35757;&#32451;&#38382;&#39064;&#23384;&#22312; Borel &#21487;&#27979;&#30340;&#35299;&#65292;&#36825;&#19968;&#32467;&#26524;&#25913;&#36827;&#20102;&#20851;&#20110;&#25932;&#23545;&#35757;&#32451;&#30340;&#25991;&#29486;&#65292;&#25991;&#29486;&#20013;&#20165;&#24050;&#30693;&#21482;&#26377;&#22312;&#29305;&#24449;&#31354;&#38388;&#30340;&#25193;&#22823;&#36890;&#29992; $&#963;$-&#20195;&#25968;&#20869;&#23384;&#22312;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study three models of the problem of adversarial training in multiclass classification designed to construct robust classifiers against adversarial perturbations of data in the agnostic-classifier setting. We prove the existence of Borel measurable robust classifiers in each model and provide a unified perspective of the adversarial training problem, expanding the connections with optimal transport initiated by the authors in previous work and developing new connections between adversarial training in the multiclass setting and total variation regularization. As a corollary of our results, we prove the existence of Borel measurable solutions to the agnostic adversarial training problem in the binary classification setting, a result that improves results in the literature of adversarial training, where robust classifiers were only known to exist within the enlarged universal $\sigma$-algebra of the feature space.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.00070</link><description>&lt;p&gt;
&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;Platt&#32553;&#25918;&#21450;&#20854;&#26657;&#20934;&#26041;&#27861;&#65292;&#20854;&#29702;&#35770;&#22522;&#30784;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#20998;&#24067;&#28418;&#31227;&#21644;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#21518;&#26657;&#20934;&#26041;&#27861;&#65292;&#31216;&#20026;&#22312;&#32447;Platt&#32553;&#25918;(OPS)&#65292;&#23427;&#23558;Platt&#32553;&#25918;&#25216;&#26415;&#19982;&#22312;&#32447;&#36923;&#36753;&#22238;&#24402;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;OPS&#22914;&#20309;&#22312;&#20998;&#24067;&#28418;&#31227;&#30340;i.i.d.&#21644;&#38750;i.i.d.&#24773;&#20917;&#19979;&#24179;&#31283;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#24403;&#26368;&#20339;&#30340;Platt&#32553;&#25918;&#27169;&#22411;&#26412;&#36523;&#34987;&#38169;&#35823;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26368;&#36817;&#24320;&#21457;&#30340;&#31216;&#20026;calibeating&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;OPS&#65292;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24471;&#21040;&#30340;OPS+calibeating&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#24615;&#32467;&#26524;&#24207;&#21015;&#26159;&#20445;&#35777;&#26657;&#20934;&#30340;&#12290;&#22312;&#23454;&#39564;&#19978;&#65292;&#23427;&#22312;&#19968;&#31995;&#21015;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25152;&#26377;OPS&#24605;&#24819;&#25193;&#23637;&#21040;beta&#32553;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#65292;&#36873;&#25321;&#20102;&#36866;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.00068</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312; COVID-19 &#30123;&#24773;&#26399;&#38388;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21475;&#32617;&#20329;&#25140;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#65292;&#36873;&#25321;&#20102;&#36866;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#26368;&#20339;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#20329;&#25140;&#21475;&#32617;&#24050;&#34987;&#30693;&#26195;&#20026;&#39044;&#38450;&#30149;&#27602;&#20256;&#25773;&#30340;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#33394;&#24615;&#33021;&#21462;&#20195;&#20102;&#20154;&#31867;&#22312;&#35768;&#22810;&#30417;&#25511;&#20219;&#21153;&#20013;&#30340;&#35282;&#33394;&#12290;&#30417;&#25511;&#21475;&#32617;&#20329;&#25140;&#23601;&#26159;&#36825;&#26679;&#19968;&#39033;&#21487;&#20197;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290; &#30001;&#20110;&#38548;&#31163;&#30340;&#21407;&#22240;&#65292;&#38754;&#37096;&#21475;&#32617;&#29031;&#29255;&#30340;&#25968;&#37327;&#26377;&#38480;&#65292;&#22240;&#27492;&#26159;&#36825;&#39033;&#20219;&#21153;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#30446;&#26631;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#23545;&#21475;&#32617;&#26816;&#27979;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21253;&#25324; Single Shot Detector&#65288;SSD&#65289;&#12289;&#20004;&#20010;&#29256;&#26412;&#30340; You Only Look Once&#12290;&#26681;&#25454;&#19981;&#21516;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36873;&#25321;&#26368;&#36866;&#21512;&#22312;&#29616;&#23454;&#19990;&#30028;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#29992;&#20110;&#23454;&#26102;&#21644;&#31227;&#21160;&#35774;&#22791;&#24212;&#29992;&#30340;&#39640;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
During the COVID-19 pandemic, wearing a face mask has been known to be an effective way to prevent the spread of COVID-19. In lots of monitoring tasks, humans have been replaced with computers thanks to the outstanding performance of the deep learning models. Monitoring the wearing of a face mask is another task that can be done by deep learning models with acceptable accuracy. The main challenge of this task is the limited amount of data because of the quarantine. In this paper, we did an investigation on the capability of three state-of-the-art object detection neural networks on face mask detection for real-time applications. As mentioned, here are three models used, Single Shot Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny, and YOLOv4-tiny-3l from which the best was selected. In the proposed method, according to the performance of different models, the best model that can be suitable for use in real-world and mobile device applications in comparison to
&lt;/p&gt;</description></item><item><title>LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.00054</link><description>&lt;p&gt;
LAVA: &#26080;&#38656;&#39044;&#23450;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00054
&lt;/p&gt;
&lt;p&gt;
LAVA&#26159;&#19968;&#20010;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36890;&#36807;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#26469;&#23454;&#29616;&#12290;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#25968;&#25454;&#20215;&#20540;&#35780;&#20272;&#38382;&#39064;&#26159;&#22914;&#20309;&#20844;&#24179;&#22320;&#20998;&#37197;&#23398;&#20064;&#31639;&#27861;&#30340;&#39564;&#35777;&#24615;&#33021;&#65292;&#33268;&#20351;&#35745;&#31639;&#24471;&#21040;&#30340;&#25968;&#25454;&#20215;&#20540;&#20381;&#36182;&#20110;&#24213;&#23618;&#23398;&#20064;&#31639;&#27861;&#30340;&#35768;&#22810;&#35774;&#35745;&#36873;&#25321;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;LAVA&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#32479;&#35745;&#29305;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#23646;&#24615;&#65292;&#36845;&#20195;&#20272;&#35745;&#25968;&#25454;&#20540;&#65292;&#20351;&#20854;&#26080;&#35270;&#19979;&#28216;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LAVA&#27604;&#29616;&#26377;&#26041;&#27861;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#31934;&#24230;&#26356;&#39640;&#65292;&#24182;&#19988;&#23427;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#24212;&#29992;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#25968;&#25454;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00050</link><description>&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24320;&#21551;&#22240;&#26524;&#30740;&#31350;&#30340;&#26032;&#31687;&#31456;
&lt;/p&gt;
&lt;p&gt;
Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00050
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#20294;&#26159;&#20854;&#40065;&#26834;&#24615;&#20173;&#28982;&#23384;&#22312;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#33021;&#21147;&#22791;&#21463;&#20105;&#35758;&#65292;&#24182;&#19988;&#23545;&#23558;&#20854;&#24212;&#29992;&#20110;&#21307;&#23398;&#12289;&#31185;&#23398;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#31561;&#20855;&#26377;&#31038;&#20250;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#21450;&#20854;&#22240;&#26524;&#25512;&#29702;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#28508;&#22312;&#30340;&#24314;&#26500;&#21644;&#27979;&#37327;&#25928;&#24230;&#23041;&#32961;&#12290;&#22522;&#20110;GPT-3.5&#21644;4&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#22240;&#26524;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#23637;&#31034;&#20102;&#38590;&#20197;&#39044;&#27979;&#30340;&#22833;&#36133;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#25216;&#26415;&#26469;&#35299;&#37322;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#65288;DDWP&#65289;&#36817;&#24180;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#38656;&#35201;&#26356;&#21152;&#20005;&#26684;&#30340;&#30495;&#23454;&#35266;&#27979;&#39564;&#35777;&#26469;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#26356;&#23433;&#20840;&#22320;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.00048</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#30340;&#30495;&#23454;&#35266;&#27979;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Verification against in-situ observations for Data-Driven Weather Prediction. (arXiv:2305.00048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00048
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#65288;DDWP&#65289;&#36817;&#24180;&#26469;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#38656;&#35201;&#26356;&#21152;&#20005;&#26684;&#30340;&#30495;&#23454;&#35266;&#27979;&#39564;&#35777;&#26469;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#26356;&#23433;&#20840;&#22320;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#27668;&#35937;&#39044;&#27979;&#27169;&#22411;&#65288;DDWP&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#23637;&#31034;&#20102;&#39640;&#24230;&#25509;&#36817;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#24555;&#36895;&#12289;&#20934;&#30830;&#12289;&#20302;&#25104;&#26412;&#30340;DDWP&#39044;&#25253;&#20351;&#20854;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#30340;&#20351;&#29992;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#36873;&#25321;&#65292;&#20294;&#26159;&#65292;&#22312;&#30495;&#27491;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#23545;DDWPs&#36827;&#34892;&#20005;&#26684;&#30340;&#35780;&#20272;&#20173;&#28982;&#38656;&#35201;&#21162;&#21147;&#12290;DDWP&#36890;&#24120;&#20351;&#29992;ERA5&#37325;&#26032;&#20998;&#26512;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20294;&#26159;DDWP&#20165;&#22312;&#27169;&#25311;&#20013;&#36827;&#34892;&#36807;&#27979;&#35797;&#65292;&#21363;&#20351;&#27169;&#25311;&#36136;&#37327;&#24456;&#39640;&#65292;&#20063;&#26080;&#27861;&#23436;&#20840;&#20934;&#30830;&#22320;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#12290;&#22312;&#25805;&#20316;&#39044;&#25253;&#20013;&#23433;&#20840;&#22320;&#20351;&#29992;DDWP&#38656;&#35201;&#26356;&#21152;&#24443;&#24213;&#30340;&#8220;&#30495;&#23454;&#19990;&#30028;&#8221;&#39564;&#35777;&#65292;&#20197;&#21450;&#23545;&#24403;&#21069;DDWP&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#26041;&#24335;&#36827;&#34892;&#20180;&#32454;&#30340;&#30740;&#31350;&#12290;&#20540;&#24471;&#38382;&#19968;&#19979;&#65292;&#20363;&#22914;&#65292;&#29992;&#20110;&#35757;&#32451;&#30340;&#37325;&#26032;&#20998;&#26512;&#25968;&#25454;&#38598;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#27169;&#25311;&#25928;&#26524;&#22914;&#20309;&#65311;&#36825;&#23545;&#20110;&#27668;&#20505;&#20844;&#27491;&#21644;&#27668;&#35937;&#25968;&#25454;&#30340;&#19981;&#22343;&#21248;&#21487;&#29992;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather prediction models (DDWPs) have made rapid strides in recent years, demonstrating an ability to approximate Numerical Weather Prediction (NWP) models to a high degree of accuracy. The fast, accurate, and low-cost DDWP forecasts make their use in operational forecasting an attractive proposition, however, there remains work to be done in rigorously evaluating DDWPs in a true operational setting. Typically trained and evaluated using ERA5 reanalysis data, DDWPs have been tested only in a simulation, which cannot represent the real world with complete accuracy even if it is of a very high quality. The safe use of DDWPs in operational forecasting requires more thorough "real-world" verification, as well as a careful examination of how DDWPs are currently trained and evaluated. It is worth asking, for instance, how well do the reanalysis datasets, used for training, simulate the real world? With an eye towards climate justice and the uneven availability of weather data: i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36716;&#25442;&#22120;&#30340;&#32463;&#39564;&#20139;&#20048;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#26410;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25968;&#25454;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#20135;&#21697;&#30340;&#20139;&#20048;&#20215;&#26684;&#21644;&#27966;&#29983;&#25351;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.00044</link><description>&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20139;&#20048;&#20215;&#26684;&#21644;&#36136;&#37327;&#35843;&#25972;&#20215;&#26684;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Hedonic Prices and Quality Adjusted Price Indices Powered by AI. (arXiv:2305.00044v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36716;&#25442;&#22120;&#30340;&#32463;&#39564;&#20139;&#20048;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#26410;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25968;&#25454;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#20135;&#21697;&#30340;&#20139;&#20048;&#20215;&#26684;&#21644;&#27966;&#29983;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#32463;&#27982;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;&#30005;&#23376;&#35760;&#24405;&#20934;&#30830;&#22320;&#23454;&#26102;&#27979;&#37327;&#20215;&#26684;&#25351;&#25968;&#30340;&#21464;&#21270;&#23545;&#20110;&#36319;&#36394;&#36890;&#32960;&#21644;&#29983;&#20135;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#32463;&#39564;&#20139;&#20048;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#26410;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25968;&#25454;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#20215;&#26684;&#21644;&#25968;&#37327;&#65289;&#65292;&#24182;&#36755;&#20986;&#31934;&#30830;&#30340;&#20139;&#20048;&#20215;&#26684;&#20272;&#35745;&#21644;&#27966;&#29983;&#25351;&#25968;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#29983;&#25104;&#25277;&#35937;&#30340;&#20135;&#21697;&#23646;&#24615;&#25110;&#8221;&#29305;&#24449;&#8220;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#23646;&#24615;&#26469;&#20272;&#31639;&#20139;&#20048;&#20215;&#26684;&#20989;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#26377;&#20851;&#20135;&#21697;&#30340;&#25991;&#26412;&#20449;&#24687;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#65292;&#20351;&#29992;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#30340;&#20135;&#21697;&#25551;&#36848;&#20449;&#24687;&#65292;&#20351;&#29992;&#27531;&#24046;&#32593;&#32476;&#27169;&#22411;&#23558;&#20135;&#21697;&#22270;&#20687;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#12290;&#20026;&#20102;&#20135;&#29983;&#20272;&#35745;&#30340;&#20139;&#20048;&#20215;&#26684;&#20989;&#25968;&#65292;&#25105;&#20204;&#20877;&#27425;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#65292;&#35757;&#32451;&#20197;&#22312;&#25152;&#26377;&#26102;&#38388;&#27573;&#21516;&#26102;&#39044;&#27979;&#20135;&#21697;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate, real-time measurements of price index changes using electronic records are essential for tracking inflation and productivity in today's economic environment. We develop empirical hedonic models that can process large amounts of unstructured product data (text, images, prices, quantities) and output accurate hedonic price estimates and derived indices. To accomplish this, we generate abstract product attributes, or ``features,'' from text descriptions and images using deep neural networks, and then use these attributes to estimate the hedonic price function. Specifically, we convert textual information about the product to numeric features using large language models based on transformers, trained or fine-tuned using product descriptions, and convert the product image to numeric features using a residual network model. To produce the estimated hedonic price function, we again use a multi-task neural network trained to predict a product's price in all time periods simultaneousl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#38899;&#39057;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#38450;&#27490;&#20174;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#29305;&#24449;&#20013;&#26816;&#27979;&#21040;&#35821;&#38899;&#27963;&#21160;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#24471;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#19982;&#19981;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#26080;&#27861;&#34987;&#35821;&#38899;&#20998;&#31867;&#22120;&#21306;&#20998;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2305.00011</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#34920;&#24449;&#23398;&#20064;&#22312;&#38899;&#39057;&#38544;&#31169;&#20445;&#25252;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Representation Learning for Robust Privacy Preservation in Audio. (arXiv:2305.00011v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#38899;&#39057;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#38450;&#27490;&#20174;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#29305;&#24449;&#20013;&#26816;&#27979;&#21040;&#35821;&#38899;&#27963;&#21160;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#24471;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#19982;&#19981;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#28508;&#22312;&#34920;&#24449;&#26080;&#27861;&#34987;&#35821;&#38899;&#20998;&#31867;&#22120;&#21306;&#20998;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#30417;&#35270;&#21644;&#29615;&#22659;&#30417;&#27979;&#65292;&#20854;&#20013;&#25968;&#25454;&#34987;&#33258;&#21160;&#25910;&#38598;&#12289;&#22788;&#29702;&#24182;&#21457;&#36865;&#21040;&#20113;&#20013;&#36827;&#34892;&#22768;&#38899;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#26080;&#24847;&#20013;&#27844;&#38706;&#29992;&#25143;&#25110;&#20854;&#21608;&#22260;&#29615;&#22659;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#22240;&#27492;&#24341;&#36215;&#20102;&#38544;&#31169;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#38899;&#39057;&#35760;&#24405;&#30340;&#34920;&#24449;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#38450;&#27490;&#20174;&#35760;&#24405;&#30340;&#28508;&#22312;&#29305;&#24449;&#20013;&#26816;&#27979;&#21040;&#35821;&#38899;&#27963;&#21160;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21253;&#21547;&#35821;&#38899;&#30340;&#38899;&#39057;&#35760;&#24405;&#30340;&#19981;&#21464;&#28508;&#22312;&#34920;&#24449;&#65292;&#36825;&#20123;&#34920;&#24449;&#19981;&#33021;&#30001;&#35821;&#38899;&#20998;&#31867;&#22120;&#20174;&#38750;&#35821;&#38899;&#35760;&#24405;&#20013;&#21306;&#20998;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#20248;&#21270;&#31639;&#27861;&#20013;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#23450;&#26399;&#34987;&#35757;&#32451;&#22312;&#26377;&#30417;&#30563;&#26041;&#24335;&#19979;&#30340;&#20998;&#31867;&#22120;&#30340;&#26435;&#37325;&#25152;&#26367;&#25442;&#12290;&#36825;&#22686;&#21152;&#20102;&#35821;&#38899;&#20998;&#31867;&#22120;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sound event detection systems are widely used in various applications such as surveillance and environmental monitoring where data is automatically collected, processed, and sent to a cloud for sound recognition. However, this process may inadvertently reveal sensitive information about users or their surroundings, hence raising privacy concerns. In this study, we propose a novel adversarial training method for learning representations of audio recordings that effectively prevents the detection of speech activity from the latent features of the recordings. The proposed method trains a model to generate invariant latent representations of speech-containing audio recordings that cannot be distinguished from non-speech recordings by a speech classifier. The novelty of our work is in the optimization algorithm, where the speech classifier's weights are regularly replaced with the weights of classifiers trained in a supervised manner. This increases the discrimination power of the speech cl
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21253;&#25324;MRI&#22270;&#20687;&#12289;&#23481;&#31215;&#35780;&#20272;&#12289;&#20998;&#23376;&#25968;&#25454;&#21644;&#29983;&#23384;&#32454;&#33410;&#22312;&#20869;&#30340;Glioblastoma&#24739;&#32773;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19987;&#23478;&#32416;&#27491;&#30340;&#32959;&#30244;&#20122;&#21306;&#21010;&#20998;&#65292;&#20026;&#21457;&#23637;&#26415;&#21518;&#21644;&#38543;&#35775;MRI&#25195;&#25551;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.00005</link><description>&lt;p&gt;
"R&#237;o Hortega University Hospital Glioblastoma Dataset: &#19968;&#20221;&#21253;&#21547;&#26415;&#21069;&#12289;&#26415;&#21518;&#26089;&#26399;&#21644;&#22797;&#21457;MRI&#25195;&#25551;&#30340;&#32508;&#21512;&#24615;&#25968;&#25454;&#38598; (RHUH-GBM)"
&lt;/p&gt;
&lt;p&gt;
The R\'io Hortega University Hospital Glioblastoma dataset: a comprehensive collection of preoperative, early postoperative and recurrence MRI scans (RHUH-GBM). (arXiv:2305.00005v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00005
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#21253;&#25324;MRI&#22270;&#20687;&#12289;&#23481;&#31215;&#35780;&#20272;&#12289;&#20998;&#23376;&#25968;&#25454;&#21644;&#29983;&#23384;&#32454;&#33410;&#22312;&#20869;&#30340;Glioblastoma&#24739;&#32773;&#30456;&#20851;&#25968;&#25454;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19987;&#23478;&#32416;&#27491;&#30340;&#32959;&#30244;&#20122;&#21306;&#21010;&#20998;&#65292;&#20026;&#21457;&#23637;&#26415;&#21518;&#21644;&#38543;&#35775;MRI&#25195;&#25551;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#24230;&#20405;&#34989;&#24615;&#30340;&#21407;&#21457;&#24615;&#33041;&#32959;&#30244;Glioblastoma&#19982;&#24739;&#32773;&#19981;&#33391;&#21518;&#26524;&#30456;&#20851;&#65292;MRI&#22312;&#35786;&#26029;&#12289;&#29305;&#24449;&#25551;&#36848;&#21644;&#39044;&#27979;Glioblastoma&#36827;&#23637;&#26041;&#38754;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#20844;&#20849;MRI&#36164;&#26009;&#24211;&#23384;&#22312;&#20005;&#37325;&#38382;&#39064;&#65292;&#21253;&#25324;&#26415;&#21518;&#21644;&#38543;&#35775;&#30740;&#31350;&#19981;&#36275;&#65292;&#20197;&#21450;&#19987;&#23478;&#32959;&#30244;&#20998;&#21106;&#19981;&#36275;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;R&#237;o Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)&#8221;&#65292;&#19968;&#20010;&#21253;&#21547;&#22810;&#21442;&#37327;MRI&#22270;&#20687;&#12289;&#23481;&#31215;&#35780;&#20272;&#12289;&#20998;&#23376;&#25968;&#25454;&#21644;&#23545;&#36827;&#34892;&#20840;&#38754;&#25110;&#36817;&#20840;&#22686;&#24378;&#32959;&#30244;&#20999;&#38500;&#30340;Glioblastoma&#24739;&#32773;&#30340;&#29983;&#23384;&#32454;&#33410;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#19987;&#23478;&#32416;&#27491;&#30340;&#32959;&#30244;&#20122;&#21306;&#21010;&#20998;&#65292;&#20026;&#21457;&#23637;&#26415;&#21518;&#21644;&#38543;&#35775;MRI&#25195;&#25551;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#25968;&#25454;&#12290;RHUH-GBM&#25968;&#25454;&#38598;&#30340;&#20844;&#24320;&#21457;&#24067;&#22312;Glioblastoma&#30740;&#31350;&#20013;&#20570;&#20986;&#20102;&#26174;&#33879;&#36129;&#29486;&#65292;&#20351;&#31185;&#23398;&#30028;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#27492;&#30149;&#24182;&#24320;&#23637;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Glioblastoma, a highly aggressive primary brain tumor, is associated with poor patient outcomes. Although magnetic resonance imaging (MRI) plays a critical role in diagnosing, characterizing, and forecasting glioblastoma progression, public MRI repositories present significant drawbacks, including insufficient postoperative and follow-up studies as well as expert tumor segmentations. To address these issues, we present the "R\'io Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)," a collection of multiparametric MRI images, volumetric assessments, molecular data, and survival details for glioblastoma patients who underwent total or near-total enhancing tumor resection. The dataset features expert-corrected segmentations of tumor subregions, offering valuable ground truth data for developing algorithms for postoperative and follow-up MRI scans. The public release of the RHUH-GBM dataset significantly contributes to glioblastoma research, enabling the scientific community to st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#36895;&#20809;&#23398;&#35786;&#26029;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#21333;&#39063;&#31890;&#28857;&#28779;&#36807;&#31243;&#36827;&#34892;&#31934;&#30830;&#30830;&#23450;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#20351;&#29992;FPN&#21644;ResNet&#32593;&#32476;&#22343;&#21487;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;FPN&#30340;&#20998;&#23618;&#29305;&#24449;&#22312;&#26816;&#27979;&#28857;&#28779;&#36807;&#31243;&#20013;&#26356;&#20026;&#26377;&#21033;&#12290;</title><link>http://arxiv.org/abs/2305.00004</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31934;&#30830;&#26816;&#27979;&#22266;&#20307;&#29123;&#26009;&#39063;&#31890;&#30340;&#28857;&#28779;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Accurate ignition detection of solid fuel particles using machine learning. (arXiv:2305.00004v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39640;&#36895;&#20809;&#23398;&#35786;&#26029;&#25216;&#26415;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23545;&#21333;&#39063;&#31890;&#28857;&#28779;&#36807;&#31243;&#36827;&#34892;&#31934;&#30830;&#30830;&#23450;&#12290;&#32463;&#36807;&#35757;&#32451;&#65292;&#20351;&#29992;FPN&#21644;ResNet&#32593;&#32476;&#22343;&#21487;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;FPN&#30340;&#20998;&#23618;&#29305;&#24449;&#22312;&#26816;&#27979;&#28857;&#28779;&#36807;&#31243;&#20013;&#26356;&#20026;&#26377;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#39640;&#36895;&#20809;&#23398;&#35786;&#26029;&#25216;&#26415;&#19982;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#21333;&#39063;&#31890;&#28857;&#28779;&#30340;&#31934;&#30830;&#30830;&#23450;&#12290;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;10 kHz OH-LIF&#21644;DBI&#27979;&#37327;&#65292;&#21487;&#20197;&#21487;&#35270;&#21270;&#23618;&#27969;&#21453;&#24212;&#22120;&#20013;&#21333;&#20010;&#39063;&#31890;&#30340;&#28857;&#28779;&#36807;&#31243;&#12290;&#30740;&#31350;&#20004;&#31181;&#29028;&#31890;&#31890;&#24452;&#65288;90-125&#956;m&#21644;160-200&#956;m&#65289;&#22312;&#24120;&#35268;&#31354;&#27668;&#21644;&#23500;&#27687;&#26465;&#20214;&#19979;&#38543;&#27687;&#27668;&#27987;&#24230;&#22686;&#21152;&#30340;&#28857;&#28779;&#24310;&#36831;&#26102;&#38388;&#12290;&#39318;&#20808;&#20351;&#29992;&#38408;&#20540;&#26041;&#27861;&#35780;&#20272;&#28857;&#28779;&#24310;&#36831;&#26102;&#38388;&#65292;&#32467;&#26524;&#19982;&#20154;&#30524;&#35266;&#23519;&#24471;&#21040;&#30340;&#22320;&#38754;&#30495;&#23454;&#20540;&#23384;&#22312;&#26126;&#26174;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;ResNet&#21644;FPN&#23545;&#22320;&#38754;&#30495;&#23454;&#20540;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#28857;&#28779;&#26102;&#38388;&#12290;&#36825;&#20004;&#20010;&#32593;&#32476;&#37117;&#33021;&#22815;&#20197;&#26174;&#33879;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#26816;&#27979;&#28857;&#28779;&#12290;&#27492;&#22806;&#65292;&#36824;&#32771;&#23519;&#20102;&#36755;&#20837;&#25968;&#25454;&#21644;&#32593;&#32476;&#28145;&#24230;&#23545;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;FPN&#30340;&#20998;&#23618;&#29305;&#24449;&#27604;ResNet&#30340;&#21097;&#20313;&#36830;&#25509;&#26356;&#26377;&#21033;&#20110;&#31934;&#30830;&#26816;&#27979;&#28857;&#28779;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the present work, accurate determination of single-particle ignition is focused on using high-speed optical diagnostics combined with machine learning approaches. Ignition of individual particles in a laminar flow reactor are visualized by simultaneous 10 kHz OH-LIF and DBI measurements. Two coal particle sizes of 90-125{\mu}m and 160-200{\mu}m are investigated in conventional air and oxy-fuel conditions with increasing oxygen concentrations. Ignition delay times are first evaluated with threshold methods, revealing obvious deviations compared to the ground truth detected by the human eye. Then, residual networks (ResNet) and feature pyramidal networks (FPN) are trained on the ground truth and applied to predict the ignition time.~Both networks are capable of detecting ignition with significantly higher accuracy and precision. Besides, influences of input data and depth of networks on the prediction performance of a trained model are examined.~The current study shows that the hierar
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#24037;&#33402;&#35774;&#35745;&#65292;&#20943;&#36731;&#39044;&#27979;&#24494;&#32467;&#26500;&#28436;&#21270;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#25214;&#21040;&#26368;&#20339;&#21152;&#24037;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2305.00003</link><description>&lt;p&gt;
&#22810;&#26230;&#24494;&#32467;&#26500;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#24037;&#33402;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Network Accelerated Process Design of Polycrystalline Microstructures. (arXiv:2305.00003v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00003
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#24037;&#33402;&#35774;&#35745;&#65292;&#20943;&#36731;&#39044;&#27979;&#24494;&#32467;&#26500;&#28436;&#21270;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#25214;&#21040;&#26368;&#20339;&#21152;&#24037;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#23454;&#39564;&#65292;&#25214;&#21040;&#32500;&#25345;&#29702;&#24819;&#26448;&#26009;&#32467;&#26500;&#30340;&#35774;&#35745;&#36335;&#24452;&#21487;&#20197;&#20248;&#21270;&#25152;&#38656;&#30340;&#26448;&#26009;&#24615;&#36136;&#12290;&#36825;&#38656;&#35201;&#37319;&#29992;&#22810;&#23610;&#24230;&#26041;&#27861;&#26469;&#29702;&#35299;&#21152;&#24037;-&#65288;&#24494;&#35266;&#65289;&#32467;&#26500;-&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23558;&#23439;&#35266;&#23610;&#24230;&#65288;&#24037;&#33402;&#21442;&#25968;&#65289;&#19982;&#20013;&#35266;&#65288;&#22343;&#36136;&#21270;&#30340;&#24615;&#36136;&#65289;&#21644;&#24494;&#35266;&#65288;&#26230;&#20307;&#23398;&#32441;&#29702;&#65289;&#23610;&#24230;&#36830;&#25509;&#36215;&#26469;&#12290;&#30001;&#20110;&#38382;&#39064;&#30340;&#22810;&#23610;&#24230;&#24314;&#27169;&#35774;&#32622;&#65292;&#21487;&#33021;&#30340;&#21152;&#24037;&#36335;&#24452;&#36873;&#25321;&#20250;&#38543;&#30528;&#20915;&#31574;&#26641;&#30340;&#21152;&#28145;&#32780;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20256;&#32479;&#27169;&#25311;&#22120;&#30340;&#36895;&#24230;&#36798;&#21040;&#20851;&#38190;&#35745;&#31639;&#38408;&#20540;&#12290;&#20026;&#20102;&#20943;&#36731;&#22312;&#32473;&#23450;&#21152;&#36733;&#26465;&#20214;&#19979;&#39044;&#27979;&#24494;&#32467;&#26500;&#28436;&#21270;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#24102;&#29289;&#29702;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26041;&#27861;&#12290;&#35813;NN&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;&#22522;&#26412;&#36807;&#31243;&#19979;&#24494;&#35266;&#32467;&#26500;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25214;&#21040;&#26368;&#20339;&#21152;&#24037;&#36335;&#24452;&#26041;&#38754;&#26159;&#26377;&#25928;&#21644;&#40065;&#26834;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#23545;&#38109;&#24494;&#32467;&#26500;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational experiments are exploited in finding a well-designed processing path to optimize material structures for desired properties. This requires understanding the interplay between the processing-(micro)structure-property linkages using a multi-scale approach that connects the macro-scale (process parameters) to meso (homogenized properties) and micro (crystallographic texture) scales. Due to the nature of the problem's multi-scale modeling setup, possible processing path choices could grow exponentially as the decision tree becomes deeper, and the traditional simulators' speed reaches a critical computational threshold. To lessen the computational burden for predicting microstructural evolution under given loading conditions, we develop a neural network (NN)-based method with physics-infused constraints. The NN aims to learn the evolution of microstructures under each elementary process. Our method is effective and robust in finding optimal processing paths. In this study, our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#31181;&#39068;&#33394;&#31354;&#38388;&#21644;CNN&#26550;&#26500;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#31867;&#26143;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;Kaggle Galaxy Zoo&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00002</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#21644;CNNs&#38598;&#25104;&#30340;&#22810;&#31181;&#39068;&#33394;&#31354;&#38388;&#23545;&#38134;&#27827;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Galaxy Classification Using Transfer Learning and Ensemble of CNNs With Multiple Colour Spaces. (arXiv:2305.00002v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22810;&#31181;&#39068;&#33394;&#31354;&#38388;&#21644;CNN&#26550;&#26500;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#20998;&#31867;&#26143;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;Kaggle Galaxy Zoo&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#25968;&#25454;&#24050;&#25104;&#20026;&#22825;&#25991;&#23398;&#39046;&#22495;&#30340;&#24120;&#24577;&#65292;&#36825;&#20351;&#24471;&#23427;&#25104;&#20026;&#35745;&#31639;&#26426;&#31185;&#23398;&#30740;&#31350;&#30340;&#29702;&#24819;&#39046;&#22495;&#12290;&#22825;&#25991;&#23398;&#23478;&#36890;&#24120;&#26681;&#25454;&#26143;&#31995;&#24418;&#24577;&#26469;&#20998;&#31867;&#26143;&#31995;&#65292;&#36825;&#19968;&#20570;&#27861;&#21487;&#36861;&#28335;&#33267;1936&#24180;&#30340;&#21704;&#21187;&#12290;&#22312;&#23567;&#22411;&#25968;&#25454;&#38598;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#20010;&#20154;&#25110;&#23567;&#22242;&#38431;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#29616;&#20195;&#26395;&#36828;&#38236;&#24102;&#26469;&#30340;&#25968;&#25454;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#38656;&#35201;&#33258;&#21160;&#20998;&#31867;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#39068;&#33394;&#31354;&#38388;&#21464;&#25442;&#23545;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#31350;&#20102;CNN&#26550;&#26500;&#23545;&#35813;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#32771;&#34385;&#21040;&#22810;&#31181;&#39068;&#33394;&#31354;&#38388;&#65288;RGB&#12289;XYZ&#12289;LAB&#31561;&#65289;&#21644;CNN&#26550;&#26500;&#65288;VGG&#12289;ResNet&#12289;DenseNet&#12289;Xception&#31561;&#65289;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#22312;&#33258;&#28982;&#22270;&#20687;&#30340;&#32972;&#26223;&#19979;&#24320;&#21457;&#30340;&#65292;&#22240;&#27492;&#26412;&#30740;&#31350;&#36824;&#35843;&#26597;&#20102;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23558;&#36825;&#20123;&#27169;&#22411;&#36866;&#24212;&#20110;&#26143;&#31995;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#30340;&#22810;&#31181;&#39068;&#33394;&#31354;&#38388;&#30340;CNNs&#38598;&#25104;&#27604;&#21333;&#20010;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#22312;Kaggle Galaxy Zoo&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Big data has become the norm in astronomy, making it an ideal domain for computer science research. Astronomers typically classify galaxies based on their morphologies, a practice that dates back to Hubble (1936). With small datasets, classification could be performed by individuals or small teams, but the exponential growth of data from modern telescopes necessitates automated classification methods.  In December 2013, Winton Capital, Galaxy Zoo, and the Kaggle team created the Galaxy Challenge, which tasked participants with developing models to classify galaxies. The Kaggle Galaxy Zoo dataset has since been widely used by researchers. This study investigates the impact of colour space transformation on classification accuracy and explores the effect of CNN architecture on this relationship. Multiple colour spaces (RGB, XYZ, LAB, etc.) and CNN architectures (VGG, ResNet, DenseNet, Xception, etc.) are considered, utilizing pre-trained models and weights. However, as most pre-trained m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#25216;&#26415;POCS-based clustering algorithm&#65292;&#23558;POCS&#25910;&#25947;&#24615;&#36136;&#24212;&#29992;&#20110;&#32858;&#31867;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32463;&#20856;&#32858;&#31867;&#26041;&#26696;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#29305;&#24449;&#23884;&#20837;&#32858;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00001</link><description>&lt;p&gt;
&#22522;&#20110;POCS&#32858;&#31867;&#31639;&#27861;&#30340;&#29305;&#24449;&#23884;&#20837;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Feature Embedding Clustering using POCS-based Clustering Algorithm. (arXiv:2305.00001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32858;&#31867;&#25216;&#26415;POCS-based clustering algorithm&#65292;&#23558;POCS&#25910;&#25947;&#24615;&#36136;&#24212;&#29992;&#20110;&#32858;&#31867;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#32463;&#20856;&#32858;&#31867;&#26041;&#26696;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#29305;&#24449;&#23884;&#20837;&#32858;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;POCS&#32858;&#31867;&#31639;&#27861;&#65288;POCS&#20195;&#34920;&#25237;&#24433;&#21040;&#20984;&#38598;&#65289;&#30340;&#26032;&#22411;&#32858;&#31867;&#25216;&#26415;&#26469;&#35299;&#20915;&#29305;&#24449;&#23884;&#20837;&#32858;&#31867;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;POCS&#32858;&#31867;&#31639;&#27861;&#23558;POCS&#25910;&#25947;&#24615;&#36136;&#24212;&#29992;&#20110;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#32858;&#31867;&#35823;&#24046;&#21644;&#25191;&#34892;&#36895;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#32463;&#20856;&#32858;&#31867;&#26041;&#26696;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;POCS&#32858;&#31867;&#31639;&#27861;&#23558;&#27599;&#20010;&#25968;&#25454;&#28857;&#35270;&#20026;&#19968;&#20010;&#20984;&#38598;&#65292;&#24182;&#23558;&#24182;&#34892;&#25237;&#24433;&#25805;&#20316;&#20174;&#27599;&#20010;&#32858;&#31867;&#21407;&#22411;&#21040;&#30456;&#24212;&#30340;&#25968;&#25454;&#25104;&#21592;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#24182;&#26356;&#26032;&#21407;&#22411;&#12290;&#20174;5&#20010;&#21517;&#20154;&#20154;&#33080;&#21644;MNIST&#25968;&#25454;&#38598;&#25552;&#21462;&#30340;&#21512;&#25104;&#23884;&#20837;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;K-Means&#21644;Fuzz&#31561;&#20854;&#20182;&#32463;&#20856;&#32858;&#31867;&#26041;&#26696;&#30456;&#27604;&#65292;POCS&#32858;&#31867;&#31639;&#27861;&#21487;&#20197;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
An application of the POCS-based clustering algorithm (POCS stands for Projection Onto Convex Set), a novel clustering technique, for feature embedding clustering problems is proposed in this paper. The POCS-based clustering algorithm applies the POCS's convergence property to clustering problems and has shown competitive performance when compared with that of other classical clustering schemes in terms of clustering error and execution speed. Specifically, the POCS-based clustering algorithm treats each data point as a convex set and applies a parallel projection operation from every cluster prototype to corresponding data members in order to minimize the objective function and update the prototypes. The experimental results on the synthetic embedding datasets extracted from the 5 Celebrity Faces and MNIST datasets show that the POCS-based clustering algorithm can perform with favorable results when compared with those of other classical clustering schemes such as the K-Means and Fuzz
&lt;/p&gt;</description></item><item><title>ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#28041;&#21450;&#24773;&#24863;&#20849;&#20139;&#21644;&#35831;&#27714;&#26816;&#27979;&#65292;&#25552;&#20379;&#20102;&#22522;&#32447;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#22120;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14882</link><description>&lt;p&gt;
ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#65306;&#24773;&#24863;&#20849;&#20139;&#19982;&#35831;&#27714;
&lt;/p&gt;
&lt;p&gt;
The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share &amp; Requests. (arXiv:2304.14882v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14882
&lt;/p&gt;
&lt;p&gt;
ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#28041;&#21450;&#24773;&#24863;&#20849;&#20139;&#21644;&#35831;&#27714;&#26816;&#27979;&#65292;&#25552;&#20379;&#20102;&#22522;&#32447;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#22120;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ACM Multimedia 2023 &#35745;&#31639;&#35821;&#35328;&#23398;&#25361;&#25112;&#36187;&#39318;&#27425;&#22312;&#26126;&#30830;&#23450;&#20041;&#30340;&#26465;&#20214;&#19979;&#35299;&#20915;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#38382;&#39064;&#65306;&#22312;&#24773;&#24863;&#20849;&#20139;&#23376;&#25361;&#25112;&#20013;&#38656;&#35201;&#23545;&#35821;&#38899;&#36827;&#34892;&#22238;&#24402;&#65292;&#32780;&#22312;&#35831;&#27714;&#23376;&#25361;&#25112;&#20013;&#38656;&#35201;&#26816;&#27979;&#35831;&#27714;&#21644;&#25265;&#24616;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#23376;&#25361;&#25112;&#12289;&#22522;&#32447;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#36890;&#24120;&#30340; ComPaRE &#29305;&#24449;&#12289;auDeep &#24037;&#20855;&#21253;&#21644;&#20351;&#29992;&#39044;&#35757;&#32451; CNN &#30340;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450; wav2vec2 &#27169;&#22411;&#30340;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ACM Multimedia 2023 Computational Paralinguistics Challenge addresses two different problems for the first time in a research competition under well-defined conditions: In the Emotion Share Sub-Challenge, a regression on speech has to be made; and in the Requests Sub-Challenges, requests and complaints need to be detected. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the usual ComPaRE features, the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the DeepSpectRum toolkit; in addition, wav2vec2 models are used.
&lt;/p&gt;</description></item><item><title>&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14660</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#8220;Segment Anything Model&#8221;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14660
&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#65292;&#20294;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#26356;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;SAM&#22312;&#35813;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;Segment Anything Model&#8221;&#65288;SAM&#65289;&#26159;&#31532;&#19968;&#20010;&#36866;&#29992;&#20110;&#24120;&#35268;&#22270;&#20687;&#20998;&#21106;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#25512;&#24191;&#20998;&#21106;&#20219;&#21153;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#20004;&#31181;&#27169;&#24335;&#23454;&#29616;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#27169;&#24577;&#12289;&#32454;&#24494;&#30340;&#35299;&#21078;&#32467;&#26500;&#12289;&#19981;&#30830;&#23450;&#30340;&#22797;&#26434;&#23545;&#35937;&#36793;&#30028;&#21644;&#24191;&#27867;&#30340;&#23545;&#35937;&#23610;&#24230;&#65292;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65288;MIS&#65289;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;SAM&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25104;&#26524;&#12290;&#21516;&#26102;&#65292;&#38646;&#26679;&#26412;&#21644;&#39640;&#25928;&#30340;MIS&#21487;&#20197;&#24456;&#22909;&#22320;&#20943;&#23569;&#27880;&#37322;&#26102;&#38388;&#24182;&#20419;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#21457;&#23637;&#12290;&#22240;&#27492;&#65292;SAM&#20284;&#20046;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#24037;&#20855;&#65292;&#24182;&#19988;&#20854;&#22312;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#24212;&#35813;&#36827;&#19968;&#27493;&#39564;&#35777;&#12290;&#25105;&#20204;&#25910;&#38598;&#21644;&#25972;&#29702;&#20102;52&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#20855;&#26377;16&#20010;&#27169;&#24577;&#21644;68&#20010;&#23545;&#35937;&#30340;&#22823;&#22411;&#21307;&#23398;&#20998;&#21106;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2304.14343</link><description>&lt;p&gt;
&#23454;&#29616;&#39640;&#25928;&#21644;&#20840;&#38754;&#30340;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#24211;&#21644;&#24615;&#33021;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#22522;&#20934;&#65292;&#20026;&#36825;&#19968;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#25512;&#36827;&#21644;&#22478;&#24066;&#26102;&#31354;&#25968;&#25454;&#30340;&#31215;&#32047;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#23384;&#22312;&#35768;&#22810;&#38480;&#21046;&#65292;&#21253;&#25324;&#24320;&#25918;&#25968;&#25454;&#20197;&#21508;&#31181;&#26684;&#24335;&#23384;&#22312;&#65292;&#20351;&#29992;&#22256;&#38590;&#65292;&#26497;&#23569;&#25968;&#35770;&#25991;&#20844;&#24320;&#20854;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#21450;&#24320;&#28304;&#27169;&#22411;&#32463;&#24120;&#20351;&#29992;&#19981;&#21516;&#30340;&#26694;&#26550;&#21644;&#24179;&#21488;&#65292;&#20351;&#24471;&#27604;&#36739;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#26045;&#21644;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#30340;&#32508;&#21512;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21407;&#23376;&#25991;&#20214;&#30340;&#32479;&#19968;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#23384;&#20648;&#26684;&#24335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LibCity&#30340;&#24320;&#28304;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23454;&#39564;&#24037;&#20855;&#21644;&#19968;&#20010;&#26041;&#20415;&#30340;&#24320;&#21457;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#24211;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#37325;&#26032;&#26500;&#24314;&#20102;65&#20010;&#31354;&#38388;&#26102;&#38388;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#25910;&#38598;&#20102;55&#20010;&#31354;&#38388;&#26102;&#38388;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22478;&#24066;&#26102;&#31354;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#22522;&#20934;&#65292;&#21253;&#25324;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#24230;&#37327;&#65292;&#20197;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#32479;&#19968;&#24211;&#21644;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
&lt;/p&gt;</description></item><item><title>TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.14226</link><description>&lt;p&gt;
TorchBench: &#29992;&#39640;API&#34920;&#38754;&#35206;&#30422;&#29575;&#35780;&#20272;PyTorch&#24615;&#33021;&#30340;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14226
&lt;/p&gt;
&lt;p&gt;
TorchBench&#26159;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20840;&#38754;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#26041;&#20415;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;PyTorch&#26159;&#26368;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;PyTorch&#36719;&#20214;&#26632;&#30340;&#29983;&#24577;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#33410;&#30465;&#27169;&#22411;&#35757;&#32451;&#25104;&#26412;&#24182;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#30340;&#21709;&#24212;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TorchBench&#65292;&#19968;&#27454;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#29992;&#20110;&#30740;&#31350;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#19981;&#21516;&#65292;TorchBench&#21253;&#21547;&#20102;&#35768;&#22810;&#20195;&#34920;&#24615;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#22823;&#37327;PyTorch API&#34920;&#38754;&#12290;TorchBench&#33021;&#22815;&#20840;&#38754;&#22320;&#34920;&#24449;PyTorch&#36719;&#20214;&#26632;&#30340;&#24615;&#33021;&#65292;&#25351;&#23548;&#27169;&#22411;&#12289;PyTorch&#26694;&#26550;&#21644;GPU&#24211;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;TorchBench&#30340;&#20004;&#20010;&#23454;&#38469;&#29992;&#20363;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#23545;TorchBench&#36827;&#34892;&#24615;&#33021;&#21078;&#26512;&#65292;&#20197;&#35782;&#21035;PyTorch&#30340;GPU&#24615;&#33021;&#25928;&#29575;&#38382;&#39064;&#12290;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#35768;&#22810;&#24615;&#33021;&#25925;&#38556;&#24182;&#21521;&#19978;&#28216;&#25552;&#20132;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.13620</link><description>&lt;p&gt;
ChartSumm&#65306;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38271;&#30701;&#25688;&#35201;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;84000&#22810;&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#34429;&#28982;&#24471;&#20998;&#19981;&#38169;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#38169;&#35273;&#12289;&#28431;&#25481;&#37325;&#35201;&#25968;&#25454;&#28857;&#20197;&#21450;&#19981;&#27491;&#30830;&#35299;&#37322;&#22797;&#26434;&#36235;&#21183;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23558;&#22270;&#34920;&#36716;&#25442;&#20026;&#25991;&#26412;&#25688;&#35201;&#26159;&#35270;&#38556;&#20154;&#22763;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#21516;&#26102;&#20026;&#29992;&#25143;&#25552;&#20379;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#31934;&#30830;&#27934;&#23519;&#21147;&#12290;&#22823;&#22411;&#12289;&#32467;&#26500;&#33391;&#22909;&#30340;&#25968;&#25454;&#38598;&#22987;&#32456;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20851;&#38190;&#37096;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ChartSumm&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20849;84363&#20010;&#22270;&#34920;&#21450;&#20854;&#20803;&#25968;&#25454;&#21644;&#25551;&#36848;&#65292;&#28085;&#30422;&#24191;&#27867;&#30340;&#20027;&#39064;&#21644;&#22270;&#34920;&#31867;&#22411;&#65292;&#21487;&#29983;&#25104;&#38271;&#30701;&#25688;&#35201;&#12290;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#24471;&#20998;&#26469;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#25688;&#35201;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#36935;&#21040;&#19968;&#20123;&#38382;&#39064;&#65292;&#20363;&#22914;&#20135;&#29983;&#38169;&#35273;&#65292;&#28431;&#25481;&#37325;&#35201;&#30340;&#25968;&#25454;&#28857;&#65292;&#20197;&#21450;&#19981;&#27491;&#30830;&#22320;&#35299;&#37322;&#22270;&#34920;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#21160;&#32763;&#35793;&#24037;&#20855;&#25506;&#35752;&#20102;&#23558;ChartSumm&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25104;&#20026;&#19968;&#20010;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35782;&#21035;&#37329;&#34701;&#20844;&#21578;&#20013;&#32467;&#26500;&#22270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#29983;&#25104;&#20102;&#34892;&#19994;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.13240</link><description>&lt;p&gt;
&#37329;&#34701;&#20844;&#21578;&#20013;&#32467;&#26500;&#22270;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Structure Diagram Recognition in Financial Announcements. (arXiv:2304.13240v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35782;&#21035;&#37329;&#34701;&#20844;&#21578;&#20013;&#32467;&#26500;&#22270;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#27861;&#29983;&#25104;&#20102;&#34892;&#19994;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#25991;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#37329;&#34701;&#30693;&#35782;&#22270;&#35889;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#21508;&#31181;&#37329;&#34701;&#24212;&#29992;&#30340;&#25928;&#29575;&#26041;&#38754;&#65292;&#20934;&#30830;&#20174;&#37329;&#34701;&#20844;&#21578;&#20013;&#30340;&#32467;&#26500;&#22270;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#20855;&#26377;&#24456;&#22823;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#26356;&#22909;&#22320;&#26816;&#27979;&#21644;&#25552;&#21462;&#19981;&#21516;&#31867;&#22411;&#30340;&#36830;&#25509;&#32447;&#65292;&#21253;&#25324;&#19981;&#21516;&#26041;&#21521;&#21644;&#35282;&#24230;&#30340;&#30452;&#32447;&#12289;&#26354;&#32447;&#21644;&#25240;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#21160;&#21270;&#24037;&#20855;&#21512;&#25104;&#24182;&#27880;&#37322;&#22823;&#37327;&#22270;&#34920;&#20197;&#35757;&#32451;&#21021;&#27493;&#35782;&#21035;&#27169;&#22411;&#65292;&#24182;&#33258;&#21160;&#26631;&#27880;&#30495;&#23454;&#32467;&#26500;&#22270;&#20197;&#36827;&#34892;&#23569;&#37327;&#25163;&#21160;&#26356;&#27491;&#65292;&#26368;&#32456;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurately extracting structured data from structure diagrams in financial announcements is of great practical importance for building financial knowledge graphs and further improving the efficiency of various financial applications. First, we proposed a new method for recognizing structure diagrams in financial announcements, which can better detect and extract different types of connecting lines, including straight lines, curves, and polylines of different orientations and angles. Second, we developed a two-stage method to efficiently generate the industry's first benchmark of structure diagrams from Chinese financial announcements, where a large number of diagrams were synthesized and annotated using an automated tool to train a preliminary recognition model with fairly good performance, and then a high-quality benchmark can be obtained by automatically annotating the real-world structure diagrams using the preliminary model and then making few manual corrections. Finally, we experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#21512;&#25104;&#28210;&#26579;&#36716;&#25442;&#20026;&#26356;&#30495;&#23454;&#30340;&#39118;&#26684;&#20197;&#36866;&#29992;&#20110;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#24863;&#30693;&#20219;&#21153;&#26469;&#37327;&#21270;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12463</link><description>&lt;p&gt;
&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#21512;&#25104;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Improving Realism of Synthetic Data for Machine Learning. (arXiv:2304.12463v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#21512;&#25104;&#21040;&#30495;&#23454;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#21512;&#25104;&#28210;&#26579;&#36716;&#25442;&#20026;&#26356;&#30495;&#23454;&#30340;&#39118;&#26684;&#20197;&#36866;&#29992;&#20110;&#36890;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19979;&#28216;&#24863;&#30693;&#20219;&#21153;&#26469;&#37327;&#21270;&#21644;&#23450;&#24615;&#22320;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#24615;&#23398;&#20064;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#25968;&#25454;&#36716;&#25442;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#30340;&#25104;&#21151;&#65292;&#20197;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20173;&#26377;&#38480;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#28145;&#24230;&#35780;&#20272;&#21644;&#27604;&#36739;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#29992;&#36884;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#23558;&#21512;&#25104;&#28210;&#26579;&#36716;&#21270;&#20026;&#26356;&#30495;&#23454;&#39118;&#26684;&#30340;&#21512;&#25104;&#21040;&#30495;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#26222;&#36890;&#30446;&#30340;&#25968;&#25454;&#38598;&#30340;&#26465;&#20214;&#25805;&#20316;&#65292;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#23450;&#20041;&#19979;&#28216;&#30340;&#24863;&#30693;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#30340;&#24615;&#33021;&#35780;&#20272;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic-to-real data translation using generative adversarial learning has achieved significant success to improve synthetic data. Yet, there are limited studies focusing on deep evaluation and comparison of adversarial training on general-purpose synthetic data for machine learning. This work aims to train and evaluate a synthetic-to-real generative model that transforms the synthetic renderings into more realistic styles on general-purpose datasets conditioned with unlabeled real-world data. Extensive performance evaluation and comparison have been conducted through qualitative and quantitative metrics, and a defined downstream perception task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#20445;&#25345;&#30828;&#38646;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38750;&#31169;&#26377;&#30340;LASSO&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20915;&#23450;&#26368;&#21518;&#30340;&#27169;&#22411;&#36873;&#25321;&#20013;&#24688;&#24403;&#30340;&#31169;&#26377;&#21270;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.12429</link><description>&lt;p&gt;
&#31232;&#30095;&#31169;&#26377;LASSO&#36923;&#36753;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sparse Private LASSO Logistic Regression. (arXiv:2304.12429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#20445;&#25345;&#30828;&#38646;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38750;&#31169;&#26377;&#30340;LASSO&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#20915;&#23450;&#26368;&#21518;&#30340;&#27169;&#22411;&#36873;&#25321;&#20013;&#24688;&#24403;&#30340;&#31169;&#26377;&#21270;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LASSO&#27491;&#21017;&#21270;&#30340;&#36923;&#36753;&#22238;&#24402;&#22240;&#20854;&#20869;&#32622;&#29305;&#24449;&#36873;&#25321;&#21151;&#33021;&#32780;&#38750;&#24120;&#26377;&#29992;&#65292;&#20801;&#35768;&#20174;&#37096;&#32626;&#20013;&#21024;&#38500;&#31995;&#25968;&#24182;&#20135;&#29983;&#31232;&#30095;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;LASSO&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#29256;&#26412;&#65292;&#20294;&#36890;&#24120;&#20250;&#20135;&#29983;&#23494;&#38598;&#35299;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;LASSO&#24809;&#32602;&#30340;&#20869;&#22312;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31232;&#30095;&#36923;&#36753;&#22238;&#24402;&#30340;&#24046;&#20998;&#38544;&#31169;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20445;&#25345;&#30828;&#38646;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38750;&#31169;&#26377;&#30340;LASSO&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#26469;&#30830;&#23450;&#22312;&#26368;&#32456;&#27169;&#22411;&#36873;&#25321;&#20013;&#20351;&#29992;&#30340;&#24688;&#24403;&#31169;&#26377;&#21270;&#38750;&#38646;&#31995;&#25968;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
LASSO regularized logistic regression is particularly useful for its built-in feature selection, allowing coefficients to be removed from deployment and producing sparse solutions. Differentially private versions of LASSO logistic regression have been developed, but generally produce dense solutions, reducing the intrinsic utility of the LASSO penalty. In this paper, we present a differentially private method for sparse logistic regression that maintains hard zeros. Our key insight is to first train a non-private LASSO logistic regression model to determine an appropriate privatized number of non-zero coefficients to use in final model selection. To demonstrate our method's performance, we run experiments on synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20195;&#34920;&#31243;&#24230;&#65292;&#20877;&#26681;&#25454;&#36825;&#20010;&#31243;&#24230;&#23558;&#19987;&#23478;&#30340;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#27979;&#32467;&#26524;&#19982;&#19987;&#23478;&#21028;&#26029;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2304.11870</link><description>&lt;p&gt;
&#23558;&#19987;&#23478;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Incorporating Experts' Judgment into Machine Learning Models. (arXiv:2304.11870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20195;&#34920;&#31243;&#24230;&#65292;&#20877;&#26681;&#25454;&#36825;&#20010;&#31243;&#24230;&#23558;&#19987;&#23478;&#30340;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#27979;&#32467;&#26524;&#19982;&#19987;&#23478;&#21028;&#26029;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#39044;&#27979;&#32467;&#26524;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39046;&#22495;&#19987;&#23478;&#21487;&#33021;&#23545;&#39044;&#26399;&#32467;&#26524;&#26377;&#21028;&#26029;&#65292;&#36825;&#21487;&#33021;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#30456;&#20914;&#31361;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#24182;&#19981;&#23436;&#20840;&#20195;&#34920;&#20154;&#32676;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19987;&#23478;&#30340;&#21028;&#26029;&#26469;&#20943;&#36731;&#20914;&#31361;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#39318;&#20808;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30830;&#23450;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20195;&#34920;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#36825;&#20010;&#31243;&#24230;&#65292;&#25105;&#20204;&#23558;&#19987;&#23478;&#30340;&#21028;&#26029;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#25913;&#27491;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#20854;&#20013;&#20195;&#34920;&#31243;&#24230;&#36234;&#39640;&#65292;&#25105;&#20204;&#23601;&#36234;&#23569;&#22320;&#32435;&#20837;&#19987;&#23478;&#30340;&#21028;&#26029;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#27425;&#25968;&#23383;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models have been quite successful in predicting outcomes in many applications. However, in some cases, domain experts might have a judgment about the expected outcome that might conflict with the prediction of ML models. One main reason for this is that the training data might not be totally representative of the population. In this paper, we present a novel framework that aims at leveraging experts' judgment to mitigate the conflict. The underlying idea behind our framework is that we first determine, using a generative adversarial network, the degree of representation of an unlabeled data point in the training data. Then, based on such degree, we correct the \textcolor{black}{machine learning} model's prediction by incorporating the experts' judgment into it, where the higher that aforementioned degree of representation, the less the weight we put on the expert intuition that we add to our corrected output, and vice-versa. We perform multiple numerical experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#31243;&#24207;&#21487;&#20197;&#25104;&#21151;&#22320;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#24182;&#25104;&#21151;&#37325;&#26500;&#31995;&#32479;&#21442;&#25968;&#12290;&#35813;&#33539;&#24335;&#21487;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11070</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Autoregressive models for biomedical signal processing. (arXiv:2304.11070v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#65292;&#23637;&#31034;&#20102;&#35813;&#31243;&#24207;&#21487;&#20197;&#25104;&#21151;&#22320;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#24182;&#25104;&#21151;&#37325;&#26500;&#31995;&#32479;&#21442;&#25968;&#12290;&#35813;&#33539;&#24335;&#21487;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#27169;&#22411;&#26159;&#20998;&#26512;&#26102;&#38388;&#24207;&#21015;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#22312;&#35745;&#31639;&#31070;&#32463;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#24037;&#31243;&#31561;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#65292;&#25968;&#25454;&#26469;&#33258;&#20110;&#33041;&#27963;&#21160;&#30340;&#27979;&#37327;&#31561;&#65292;&#25968;&#25454;&#23384;&#22312;&#27979;&#37327;&#35823;&#24046;&#21644;&#22522;&#30784;&#31995;&#32479;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#20272;&#31639;&#22120;&#30340;&#26631;&#20934;&#20449;&#21495;&#22788;&#29702;&#21487;&#33021;&#23384;&#22312;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#26126;&#30830;&#22320;&#32435;&#20837;&#36825;&#20123;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#20248;&#21270;&#35813;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#26367;&#29366;&#24577;&#21644;&#21442;&#25968;&#20272;&#35745;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#35813;&#31243;&#24207;&#33021;&#22815;&#25104;&#21151;&#21435;&#22122;&#26102;&#38388;&#24207;&#21015;&#24182;&#25104;&#21151;&#37325;&#26500;&#31995;&#32479;&#21442;&#25968;&#12290;&#36825;&#31181;&#26032;&#30340;&#33539;&#24335;&#21487;&#20197;&#22312;&#31070;&#32463;&#31185;&#23398;&#30340;&#22810;&#31181;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#20363;&#22914;&#33041;&#26426;&#25509;&#21475;&#25968;&#25454;&#20998;&#26512;&#21644;&#23545;&#30142;&#30149;&#22914;&#30315;&#30187;&#21644;&#24085;&#37329;&#26862;&#30149;&#20013;&#30340;&#22823;&#33041;&#21160;&#24577;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive models are ubiquitous tools for the analysis of time series in many domains such as computational neuroscience and biomedical engineering. In these domains, data is, for example, collected from measurements of brain activity. Crucially, this data is subject to measurement errors as well as uncertainties in the underlying system model. As a result, standard signal processing using autoregressive model estimators may be biased. We present a framework for autoregressive modelling that incorporates these uncertainties explicitly via an overparameterised loss function. To optimise this loss, we derive an algorithm that alternates between state and parameter estimation. Our work shows that the procedure is able to successfully denoise time series and successfully reconstruct system parameters. This new paradigm can be used in a multitude of applications in neuroscience such as brain-computer interface data analysis and better understanding of brain dynamics in diseases such as
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243;&#65288;CKmeans&#21644;FCKmeans&#65289;&#20197;&#25913;&#36827;Kmeans&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#36807;&#31243;&#22312;&#32858;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;Kmeans&#21644;Kmeans++&#12290;</title><link>http://arxiv.org/abs/2304.09989</link><description>&lt;p&gt;
CKmeans&#21644;FCKmeans&#65306;&#20351;&#29992;&#25317;&#25380;&#36317;&#31163;&#30340;Kmeans&#31639;&#27861;&#30340;&#20004;&#31181;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243; &#65288;arXiv&#65306;2304.09989v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance. (arXiv:2304.09989v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243;&#65288;CKmeans&#21644;FCKmeans&#65289;&#20197;&#25913;&#36827;Kmeans&#32858;&#31867;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#36807;&#31243;&#22312;&#32858;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;Kmeans&#21644;Kmeans++&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#25317;&#25380;&#36317;&#31163;&#30340;K-means&#32858;&#31867;&#30340;&#26032;&#22411;&#30830;&#23450;&#24615;&#21021;&#22987;&#21270;&#36807;&#31243;&#65292;&#20998;&#21035;&#31216;&#20026;CKmeans&#21644;FCKmeans&#12290;&#36825;&#20123;&#36807;&#31243;&#21033;&#29992;&#26356;&#23494;&#38598;&#30340;&#28857;&#20316;&#20026;&#21021;&#22987;&#36136;&#24515;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32858;&#31867;&#20934;&#30830;&#24230;&#26041;&#38754;&#20248;&#20110;Kmeans&#21644;Kmeans ++&#12290;CKmeans&#21644;FCKmeans&#30340;&#26377;&#25928;&#24615;&#24402;&#22240;&#20110;&#23427;&#20204;&#22522;&#20110;&#20462;&#25913;&#21518;&#30340;&#25317;&#25380;&#36317;&#31163;&#36873;&#25321;&#26356;&#22909;&#30340;&#21021;&#22987;&#36136;&#24515;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20026;&#25913;&#36827;K-means&#32858;&#31867;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two novel deterministic initialization procedures for K-means clustering based on a modified crowding distance. The procedures, named CKmeans and FCKmeans, use more crowded points as initial centroids. Experimental studies on multiple datasets demonstrate that the proposed approach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The effectiveness of CKmeans and FCKmeans is attributed to their ability to select better initial centroids based on the modified crowding distance. Overall, the proposed approach provides a promising alternative for improving K-means clustering.
&lt;/p&gt;</description></item><item><title>SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.09548</link><description>&lt;p&gt;
SemEval 2023 &#20219;&#21153;6: LegalEval -- &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09548
&lt;/p&gt;
&lt;p&gt;
SemEval 2023&#20030;&#21150;&#20102;LegalEval&#20849;&#20139;&#20219;&#21153;&#65292;&#21363;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#65292;&#21253;&#25324; &#33258;&#21160;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#36830;&#36143;&#21270;&#30340;&#27861;&#24459;&#25991;&#20214;&#65288;Task-A&#65289;&#65292;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;Task-B&#65289;&#20197;&#21450;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#21644;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#65288;Task-C&#65289;&#12290;26&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#24182;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#21475;&#20247;&#22810;&#30340;&#22269;&#23478;&#65292;&#24453;&#22788;&#29702;&#30340;&#27861;&#24459;&#26696;&#20214;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#26377;&#24517;&#35201;&#24320;&#21457;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25216;&#26415;&#65292;&#23545;&#27861;&#24459;&#25991;&#20214;&#36827;&#34892;&#22788;&#29702;&#21644;&#33258;&#21160;&#29702;&#35299;&#12290;&#20026;&#20102;&#20419;&#36827;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#22312; SemEval 2023 &#19978;&#32452;&#32455;&#20102;&#20849;&#20139;&#20219;&#21153; LegalEval - &#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#12290;LegalEval &#20219;&#21153;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;Task-A&#65288;&#20462;&#36766;&#35282;&#33394;&#26631;&#35760;&#65289;&#26159;&#33258;&#21160;&#23558;&#27861;&#24459;&#25991;&#20214;&#32467;&#26500;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#21333;&#20803;&#65292;Task-B&#65288;&#27861;&#24459;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65289;&#22788;&#29702;&#22312;&#27861;&#24459;&#25991;&#20214;&#20013;&#35782;&#21035;&#30456;&#20851;&#23454;&#20307;&#65292;&#32780; Task-C&#65288;&#27861;&#38498;&#21028;&#20915;&#39044;&#27979;&#19982;&#35299;&#37322;&#65289;&#25506;&#32034;&#20102;&#33258;&#21160;&#39044;&#27979;&#27861;&#24459;&#26696;&#20214;&#32467;&#26524;&#20197;&#21450;&#25552;&#20379;&#39044;&#27979;&#35299;&#37322;&#30340;&#21487;&#33021;&#24615;&#12290;&#20849;&#26377;26&#20010;&#22242;&#38431;&#65288;&#20998;&#24067;&#22312;&#20840;&#29699;&#30340;&#32422;100&#21517;&#21442;&#19982;&#32773;&#65289;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#22312;&#27599;&#20010;&#23376;&#20219;&#21153;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#37117;&#20248;&#20110;&#22522;&#20934;&#32447;&#65307;&#20294;&#26159;&#65292;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; LegalEval &#20219;&#21153;&#30340;&#32452;&#32455;&#21644;&#32454;&#33410;&#65292;&#24182;&#27010;&#36848;&#20102;&#21442;&#19982;&#31995;&#32479;&#21450;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#30340;&#22797;&#21046;&#26426;&#21046;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#39046;&#22495;&#22312;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#23558;&#22797;&#21046;&#26426;&#21046;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;NMT&#30340;SPARQL&#29983;&#25104;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28155;&#21152;&#22797;&#21046;&#26426;&#21046;&#25110;&#20351;&#29992;&#38382;&#39064;&#27880;&#37322;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.05405</link><description>&lt;p&gt;
&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;:&#19968;&#39033;&#27010;&#36848;&#30740;&#31350;(arXiv: 2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search. (arXiv:2304.05405v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26368;&#36817;&#22312;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#20013;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;DARTS&#26041;&#27861;&#30340;&#36129;&#29486;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#19981;&#21516;iable&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;DNAS&#65289;&#36805;&#36895;&#25104;&#20026;&#33258;&#21160;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290; &#36825;&#31181;&#23835;&#36215;&#20027;&#35201;&#24402;&#21151;&#20110;DARTS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#37325;&#35201;&#30340;DNAS&#26041;&#27861;&#20043;&#19968;&#12290; &#19982;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#25110;&#36827;&#21270;&#31639;&#27861;&#30340;&#20197;&#21069;&#30340;&#20316;&#21697;&#30456;&#27604;&#65292;DNAS&#36895;&#24230;&#24555;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#20351;&#29992;&#30340;&#35745;&#31639;&#36164;&#28304;&#26356;&#23569;&#12290; &#22312;&#36825;&#31687;&#20840;&#38754;&#30340;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#19987;&#38376;&#20851;&#27880;DNAS&#24182;&#23457;&#26597;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25361;&#25112;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#31867;DNAS&#26041;&#27861;&#12290; &#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36807;&#21435;&#20960;&#24180;&#23545;DNAS&#24102;&#26469;&#30340;&#36129;&#29486;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;NAS&#39046;&#22495;&#30340;&#24433;&#21709;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35265;&#35299;&#26469;&#20570;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2303.17354</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17354
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#28176;&#36827;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#65292;&#20854;&#20013;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MAE&#27169;&#22411;&#36827;&#34892;&#27491;&#24120;&#22270;&#20687;&#30340;&#35757;&#32451;&#65292;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#29983;&#25104;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#65292;&#26368;&#32456;&#36890;&#36807;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#32508;&#21512;&#24471;&#21040;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#23545;&#20110;&#22270;&#20687;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#30340;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#22312;&#24037;&#19994;&#32570;&#38519;&#26816;&#27979;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#39592;&#24178;&#32593;&#32476;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#31574;&#30053;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#20165;&#20351;&#29992;&#27491;&#24120;&#22270;&#20687;&#23545;Masked Autoencoder &#65288;MAE&#65289;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#20687;&#32032;&#32423;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20197;&#29983;&#25104;&#24050;&#25439;&#22351;&#30340;&#27491;&#24120;&#22270;&#20687;&#21450;&#20854;&#30456;&#24212;&#30340;&#20687;&#32032;&#26631;&#31614;&#12290;&#36825;&#20010;&#36807;&#31243;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22914;&#20309;&#20462;&#22797;&#25439;&#22351;&#30340;&#21306;&#22495;&#21644;&#20998;&#31867;&#27599;&#20010;&#20687;&#32032;&#30340;&#29366;&#24577;&#12290;&#26368;&#32456;&#65292;&#35813;&#27169;&#22411;&#20135;&#29983;&#19968;&#20010;&#20687;&#32032;&#37325;&#24314;&#35823;&#24046;&#30697;&#38453;&#21644;&#19968;&#20010;&#20687;&#32032;&#24322;&#24120;&#27010;&#29575;&#30697;&#38453;&#65292;&#36825;&#20004;&#20010;&#30697;&#38453;&#32508;&#21512;&#25104;&#19968;&#20010;&#24322;&#24120;&#24471;&#20998;&#30697;&#38453;&#65292;&#26377;&#25928;&#22320;&#29992;&#20110;&#22270;&#20687;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;&#65292;&#21487;&#20197;&#28040;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#21644;&#26679;&#26412;&#35760;&#24518;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13516</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;
&lt;/p&gt;
&lt;p&gt;
Ablating Concepts in Text-to-Image Diffusion Models. (arXiv:2303.13516v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#28040;&#34701;&#65292;&#21487;&#20197;&#28040;&#38500;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#29256;&#26435;&#38382;&#39064;&#21644;&#26679;&#26412;&#35760;&#24518;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#32452;&#21512;&#33021;&#21147;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22270;&#29255;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#25968;&#37327;&#24222;&#22823;&#30340;&#32593;&#32476;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24448;&#24448;&#21253;&#21547;&#26377;&#29256;&#26435;&#26448;&#26009;&#12289;&#25480;&#26435;&#22270;&#20687;&#21644;&#20010;&#20154;&#29031;&#29255;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#34987;&#21457;&#29616;&#33021;&#22815;&#27169;&#20223;&#19981;&#21516;&#33402;&#26415;&#23478;&#30340;&#39118;&#26684;&#25110;&#35760;&#20303;&#20934;&#30830;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#22914;&#20309;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#36825;&#20123;&#29256;&#26435;&#27010;&#24565;&#25110;&#22270;&#20687;&#65311;&#20026;&#20102;&#36798;&#25104;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#23454;&#29616;&#27010;&#24565;&#28040;&#34701;&#65292;&#21363;&#38450;&#27490;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23398;&#20064;&#22914;&#20309;&#21305;&#37197;&#19968;&#20010;&#38170;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#19982;&#30446;&#26631;&#39118;&#26684;&#12289;&#23454;&#20363;&#25110;&#25991;&#26412;&#25552;&#31034;&#30456;&#20851;&#30340;&#22270;&#20687;&#20998;&#24067;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#26681;&#25454;&#20854;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#30446;&#26631;&#27010;&#24565;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#38450;&#27490;&#28040;&#34701;&#27010;&#24565;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated conce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20989;&#25968;&#32452;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#40657;&#30418;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21160;&#24577;&#23450;&#20215;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.11954</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20989;&#25968;&#32452;&#21512;&#26041;&#27861;&#21450;&#20854;&#22312;&#21160;&#24577;&#23450;&#20215;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization for Function Compositions with Applications to Dynamic Pricing. (arXiv:2303.11954v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#20989;&#25968;&#32452;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#40657;&#30418;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#21160;&#24577;&#23450;&#20215;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#29992;&#26469;&#25214;&#21040;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20989;&#25968;&#32452;&#21512;&#30340;&#23454;&#29992;BO&#26041;&#27861;&#65292;&#20854;&#20013;&#32452;&#21512;&#30340;&#24418;&#24335;&#24050;&#30693;&#65292;&#20294;&#21508;&#32452;&#25104;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#12290;&#36890;&#36807;&#20026;&#27599;&#20010;&#40657;&#30418;&#20989;&#25968;&#24314;&#31435;&#29420;&#31435;&#30340;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;EI&#21644;UCB&#22522;&#20110;BO&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#32988;&#36807;&#20256;&#32479;BO&#21644;&#30446;&#21069;&#20808;&#36827;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25910;&#30410;&#31649;&#29702;&#20013;&#21160;&#24577;&#23450;&#20215;&#26102;&#30340;&#19968;&#31181;&#26032;&#39062;&#24212;&#29992;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26114;&#36149;&#30340;&#38656;&#27714;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is used to find the global optima of black box functions. In this work, we propose a practical BO method of function compositions where the form of the composition is known but the constituent functions are expensive to evaluate. By assuming an independent Gaussian process (GP) model for each of the constituent black-box function, we propose EI and UCB based BO algorithms and demonstrate their ability to outperform vanilla BO and the current state-of-art algorithms. We demonstrate a novel application of the proposed methods to dynamic pricing in revenue management when the underlying demand function is expensive to evaluate.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.07103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20294;&#26410;&#26469;&#21313;&#24180;&#38543;&#30528;&#38556;&#30861;&#34987;&#20811;&#26381;&#65292;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26222;&#36941;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#24863;&#30693;&#25110;&#24847;&#35782;&#12290;&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#36825;&#20010;&#24819;&#27861;&#65311;&#26412;&#25991;&#23558;&#20998;&#26512;&#25903;&#25345;&#21644;&#21453;&#23545;&#36825;&#20010;&#24819;&#27861;&#30340;&#26368;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#26681;&#25454;&#24847;&#35782;&#31185;&#23398;&#20013;&#30340;&#20027;&#27969;&#20551;&#35774;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#23384;&#22312;&#30528;&#24847;&#35782;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#20363;&#22914;&#32570;&#20047;&#24490;&#29615;&#22788;&#29702;&#12289;&#20840;&#23616;&#30340;&#24037;&#20316;&#31354;&#38388;&#21644;&#32479;&#19968;&#30340;&#26234;&#33021;&#26426;&#26500;&#31561;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#20123;&#38556;&#30861;&#22312;&#26410;&#26469;&#21313;&#24180;&#24038;&#21491;&#37117;&#21487;&#33021;&#34987;&#20811;&#26381;&#12290;&#20316;&#32773;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#34429;&#28982;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24847;&#35782;&#30340;&#21487;&#33021;&#24615;&#36739;&#23567;&#65292;&#20294;&#25105;&#20204;&#24212;&#35813;&#35748;&#30495;&#32771;&#34385;&#21518;&#32487;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#21487;&#33021;&#20250;&#20855;&#26377;&#24847;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.05737</link><description>&lt;p&gt;
&#20020;&#24202;BERTScore&#65306;&#20020;&#24202;&#29615;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#25913;&#36827;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#65292;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#12290;&#20316;&#32773;&#36824;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26377;&#28508;&#21147;&#33410;&#30465;&#26102;&#38388;&#65292;&#38477;&#20302;&#25104;&#26412;&#65292;&#25552;&#39640;&#25253;&#21578;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#21307;&#29983;&#30340;&#30130;&#21171;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36991;&#20813;&#21307;&#23398;&#30456;&#20851;&#30340;&#36716;&#24405;&#38169;&#35823;&#30340;&#37325;&#35201;&#24615;&#65292;&#21307;&#30103;&#34892;&#19994;&#37319;&#29992;&#36825;&#31181;&#25216;&#26415;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;BERTScore&#65288;CBERTScore&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;ASR&#24230;&#37327;&#65292;&#23427;&#27604;&#20854;&#20182;&#24230;&#37327;&#65288;WER&#12289;BLUE&#12289;METEOR&#31561;&#65289;&#26356;&#20005;&#21385;&#22320;&#24809;&#32602;&#20020;&#24202;&#30456;&#20851;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#24230;&#37327;&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#23545;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#65292;&#26377;&#26102;&#24046;&#36317;&#24456;&#22823;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;13&#20010;&#20020;&#24202;&#21307;&#29983;&#23545;149&#20010;&#29616;&#23454;&#21307;&#23398;&#21477;&#23376;&#30340;&#20559;&#22909;&#22522;&#20934;&#65292;&#31216;&#20026;&#20020;&#24202;&#36716;&#24405;&#20559;&#22909;&#22522;&#20934;&#65288;CTP&#65289;&#65292;&#35777;&#26126;CBERTScore&#26356;&#25509;&#36817;&#20110;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#65292;&#24182;&#23558;&#22522;&#20934;&#21457;&#24067;&#32473;&#31038;&#21306;&#20197;&#36827;&#19968;&#27493;&#24320;&#21457;&#20855;&#26377;&#20020;&#24202;&#24847;&#35782;&#30340;ASR&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.05617</link><description>&lt;p&gt;
KGNv2: &#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#21512;&#25104;&#20013;&#30340;&#23610;&#24230;&#21644;&#23039;&#24577;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input. (arXiv:2303.05617v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;RGB-D&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;6&#33258;&#30001;&#24230;&#25235;&#21462;&#23039;&#24577;&#21512;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#20851;&#38190;&#28857;&#20174;2D/2.5D&#36755;&#20837;&#20013;&#36827;&#34892;&#12290;&#22312;&#21069;&#26399;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#25235;&#21462;&#26816;&#27979;&#22120;&#24050;&#32463;&#35777;&#26126;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#24425;&#33394;&#22270;&#20687;&#25552;&#20379;&#30340;&#39069;&#22806;&#35270;&#35273;&#20449;&#24687;&#24357;&#34917;&#20102;&#22024;&#26434;&#30340;&#28145;&#24230;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#20934;&#30830;&#39044;&#27979;&#22270;&#20687;&#31354;&#38388;&#20013;&#30340;&#20851;&#38190;&#28857;&#20301;&#32622;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#25235;&#21462;&#29983;&#25104;&#32593;&#32476;&#65292;&#26082;&#21487;&#20197;&#20174;&#20851;&#38190;&#28857;&#26816;&#27979;&#20013;&#39044;&#27979;&#25235;&#21462;&#23039;&#24577;&#65292;&#20063;&#21487;&#20197;&#39044;&#27979;&#30456;&#23545;&#20110;&#30456;&#26426;&#30340;&#23610;&#24230;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#37325;&#26032;&#35774;&#35745;&#20102;&#20851;&#38190;&#28857;&#36755;&#20986;&#31354;&#38388;&#65292;&#20197;&#20943;&#36731;&#20851;&#38190;&#28857;&#39044;&#27979;&#22122;&#22768;&#23545;&#36879;&#35270;n&#28857;(PnP)&#31639;&#27861;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#27604;&#22522;&#32447;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#36234;&#24615;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#23613;&#31649;&#26159;&#22312;&#31616;&#21333;&#30340;&#21512;&#25104;&#23545;&#35937;&#19978;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#29992;&#20110;&#30495;&#23454;&#29289;&#20307;&#19978;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.04772</link><description>&lt;p&gt;
&#22810;&#32423;&#25193;&#25955;&#65306;&#22270;&#20687;&#29983;&#25104;&#30340;&#26080;&#38480;&#32500;&#24230;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26080;&#38480;&#32500;&#24230;&#24471;&#20998;&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26159;&#36817;&#24180;&#26469;&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#22312;&#26377;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#34920;&#36848;&#65292;&#20854;&#20013;&#22270;&#20687;&#34987;&#35270;&#20026;&#20855;&#26377;&#26377;&#38480;&#23610;&#23544;&#30340;&#24352;&#37327;&#12290;&#26412;&#25991;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#24320;&#21457;&#20102;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#21363;&#25105;&#20204;&#23558;&#35757;&#32451;&#25968;&#25454;&#24314;&#27169;&#20026;&#25903;&#25745;&#22312;&#30697;&#24418;&#22495;&#19978;&#30340;&#20989;&#25968;&#12290;&#38500;&#20102;&#36861;&#27714;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#22270;&#20687;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#21019;&#24314;&#19968;&#20010;&#33391;&#22909;&#23450;&#20041;&#30340;&#26080;&#38480;&#32500;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#20415;&#21487;&#20197;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#27700;&#24179;&#19978;&#19968;&#33268;&#22320;&#31163;&#25955;&#21270;&#23427;&#12290;&#25105;&#20204;&#24076;&#26395;&#33719;&#24471;&#33021;&#22815;&#27178;&#36328;&#19981;&#21516;&#20998;&#36776;&#29575;&#32423;&#21035;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20811;&#26381;&#24403;&#21069;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#32570;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#21069;&#21521;&#36807;&#31243;&#20197;&#30830;&#20445;&#22312;&#26080;&#38480;&#32500;&#24230;&#35774;&#32622;&#20013;&#28508;&#22312;&#20998;&#24067;&#26159;&#33391;&#22909;&#23450;&#20041;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#25193;&#25955;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#22810;&#20010;&#20998;&#36776;&#29575;&#19978;&#39640;&#25928;&#22320;&#23398;&#20064;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#32423;&#27169;&#22411;&#22312;&#30456;&#21516;&#25110;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#20135;&#29983;&#27604;&#20256;&#32479;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#26356;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#22320;&#29983;&#25104;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#24182;&#22788;&#29702;&#30697;&#24418;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.04562</link><description>&lt;p&gt;
&#36845;&#20195;&#20462;&#27491;&#30340;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#22806;&#25512;&#25511;&#21046;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#25511;&#21046;&#32534;&#36753;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#21462;&#24471;&#36739;&#20248;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22806;&#25512;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24207;&#21015;&#12290;&#22312;&#33258;&#21160;&#35774;&#35745;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#33647;&#29289;&#30740;&#31350;&#39046;&#22495;&#65292;&#36825;&#20010;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#27604;&#29616;&#26377;&#24207;&#21015;&#26356;&#22909;&#65288;&#20363;&#22914;&#26356;&#31283;&#23450;&#65289;&#30340;&#26032;&#22411;&#34507;&#30333;&#36136;&#12290;&#22240;&#27492;&#65292;&#25353;&#29031;&#23450;&#20041;&#65292;&#30446;&#26631;&#24207;&#21015;&#21450;&#20854;&#23646;&#24615;&#20540;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#65292;&#25361;&#25112;&#29616;&#26377;&#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#24207;&#21015;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36845;&#20195;&#25511;&#21046;&#22806;&#25512;&#65288;ICE&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#23545;&#24207;&#21015;&#36827;&#34892;&#23616;&#37096;&#32534;&#36753;&#26469;&#23454;&#29616;&#22806;&#25512;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#24207;&#21015;&#23545;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#28436;&#31034;&#24494;&#23567;&#30340;&#23646;&#24615;&#20540;&#25913;&#36827;&#12290;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65288;&#24773;&#24863;&#20998;&#26512;&#65289;&#21644;&#20004;&#20010;&#34507;&#30333;&#36136;&#24037;&#31243;&#20219;&#21153;&#65288;ACE2&#31283;&#23450;&#24615;&#21644;AAV&#36866;&#24212;&#24615;&#65289;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ICE&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;</title><link>http://arxiv.org/abs/2303.02725</link><description>&lt;p&gt;
&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#29615;&#22659;&#27602;&#21270;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Local Environment Poisoning Attacks on Federated Reinforcement Learning. (arXiv:2303.02725v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24050;&#25104;&#20026;&#35299;&#20915;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#22810;&#20195;&#29702;&#32467;&#26500;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#25968;&#25454;&#38656;&#27714;&#22823;&#30340;&#20027;&#35201;&#38382;&#39064;&#65292;&#32780;&#32852;&#37030;&#26426;&#21046;&#20445;&#25252;&#20102;&#21508;&#20010;&#20195;&#29702;&#20010;&#20307;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#26426;&#21046;&#20063;&#20250;&#26292;&#38706;&#31995;&#32479;&#38754;&#20020;&#24694;&#24847;&#20195;&#29702;&#30340;&#27602;&#21270;&#25915;&#20987;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#23558;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#35270;&#20026;&#38480;&#21046;&#39044;&#31639;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#31574;&#30053;&#30340;FRL&#30340;&#27602;&#21270;&#25915;&#20987;&#21327;&#35758;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#19968;&#23545;&#31169;&#20154;&#35780;&#20215;&#32773;&#21644;&#20844;&#20849;&#35780;&#20215;&#32773;&#23558;&#20854;&#25193;&#23637;&#21040;&#20351;&#29992;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20316;&#20026;&#26412;&#22320;RL&#31639;&#27861;&#30340;FRL&#12290;&#25105;&#20204;&#20063;&#35752;&#35770;&#20102;&#20174;FL&#32487;&#25215;&#30340;&#19968;&#31181;&#20256;&#32479;&#38450;&#24481;&#31574;&#30053;&#20197;&#20943;&#36731;&#36825;&#31181;&#39118;&#38505;&#12290;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#27602;&#21270;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has become a popular tool for solving traditional Reinforcement Learning (RL) tasks. The multi-agent structure addresses the major concern of data-hungry in traditional RL, while the federated mechanism protects the data privacy of individual agents. However, the federated mechanism also exposes the system to poisoning by malicious agents that can mislead the trained policy. Despite the advantage brought by FL, the vulnerability of Federated Reinforcement Learning (FRL) has not been well-studied before. In this work, we propose the first general framework to characterize FRL poisoning as an optimization problem constrained by a limited budget and design a poisoning protocol that can be applied to policy-based FRL and extended to FRL with actor-critic as a local RL algorithm by training a pair of private and public critics. We also discuss a conventional defense strategy inherited from FL to mitigate this risk. We verify our poisoning effectiveness by conducting 
&lt;/p&gt;</description></item><item><title>R-U-SURE&#26159;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#30721;&#24314;&#35758;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#38543;&#26426;&#26679;&#26412;&#20316;&#20026;&#20195;&#29702;&#29983;&#25104;&#25552;&#39640;&#25928;&#29992;&#30340;&#23454;&#29616;&#65292;&#24182;&#32467;&#21512;&#20102;&#20915;&#31574;&#35770;&#27169;&#22411;&#12289;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#12289;&#21452;&#37325;&#20998;&#35299;&#21644;&#20915;&#31574;&#22270;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#23436;&#25972;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#32467;&#26500;&#21270;&#19981;&#30830;&#23450;&#24615;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.00732</link><description>&lt;p&gt;
R-U-SURE&#65311;&#36890;&#36807;&#26368;&#22823;&#21270;&#19981;&#21516;&#29992;&#25143;&#24847;&#22270;&#19979;&#30340;&#25928;&#29992;&#26469;&#23454;&#29616;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#30721;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents. (arXiv:2303.00732v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00732
&lt;/p&gt;
&lt;p&gt;
R-U-SURE&#26159;&#19968;&#31181;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#30721;&#24314;&#35758;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#30340;&#38543;&#26426;&#26679;&#26412;&#20316;&#20026;&#20195;&#29702;&#29983;&#25104;&#25552;&#39640;&#25928;&#29992;&#30340;&#23454;&#29616;&#65292;&#24182;&#32467;&#21512;&#20102;&#20915;&#31574;&#35770;&#27169;&#22411;&#12289;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#12289;&#21452;&#37325;&#20998;&#35299;&#21644;&#20915;&#31574;&#22270;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#23436;&#25972;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#32467;&#26500;&#21270;&#19981;&#30830;&#23450;&#24615;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#32467;&#26500;&#21270;&#25991;&#26412;&#65288;&#22914;&#20195;&#30721;&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#23427;&#20204;&#36890;&#24120;&#20250;&#22312;&#20854;&#36755;&#20986;&#20013;&#24341;&#20837;&#38169;&#35823;&#21644;&#34394;&#20551;&#20449;&#24687;&#12290;&#24403;&#29992;&#20110;&#36741;&#21161;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#29359;&#38169;&#65292;&#38656;&#35201;&#29992;&#25143;&#36820;&#22238;&#26356;&#27491;&#65292;&#25110;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#21487;&#33021;&#20250;&#24341;&#20837;&#29992;&#25143;&#21487;&#33021;&#20250;&#23436;&#20840;&#24573;&#30053;&#30340;&#24494;&#22937;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;R-U-SURE&#30340;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#20915;&#31574;&#35770;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#29983;&#25104;&#27169;&#22411;&#30340;&#38543;&#26426;&#26679;&#26412;&#20316;&#20026;&#26368;&#32456;&#29992;&#25143;&#26410;&#35266;&#23519;&#21040;&#30340;&#21487;&#33021;&#24847;&#22270;&#30340;&#20195;&#29702;&#65292;&#26500;&#24314;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#32467;&#21512;&#20102;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#12289;&#21452;&#37325;&#20998;&#35299;&#21644;&#20915;&#31574;&#22270;&#65292;&#20197;&#20415;&#20165;&#36890;&#36807;&#23545;&#20219;&#24847;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26679;&#26412;&#35775;&#38382;&#21644;&#21487;&#36873;&#30340;AST&#35299;&#26512;&#22120;&#65292;&#39640;&#25928;&#22320;&#29983;&#25104;&#32467;&#26500;&#21270;&#19981;&#30830;&#23450;&#24615;&#25688;&#35201;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24320;&#21457;&#32773;&#36741;&#21161;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;R-U-SURE&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely. We propose Randomized Utility-driven Synthesis of Uncertain REgions (R-U-SURE), an approach for building uncertainty-aware suggestions based on a decision-theoretic model of goal-conditioned utility, using random samples from a generative model as a proxy for the unobserved possible intents of the end user. Our technique combines minimum-Bayes-risk decoding, dual decomposition, and decision diagrams in order to efficiently produce structured uncertainty summaries, given only sample access to an arbitrary generative model of code and an optional AST parser. We demonstrate R-U-SURE on three developer-assistance tasks, and show that it can be applied different user i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.00186</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#25216;&#26415;&#30340;&#28789;&#27963;&#33021;&#28304;&#31038;&#21306;&#30446;&#26631;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Targeted demand response for flexible energy communities using clustering techniques. (arXiv:2303.00186v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#32858;&#31867;&#25216;&#26415;&#35774;&#35745;&#24182;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#34892;&#24615;&#65292;&#30446;&#30340;&#26159;&#25913;&#21464;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#65292;&#20197;&#26368;&#23567;&#21270;&#21453;&#21521;&#21151;&#29575;&#27969;&#21644;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#32858;&#31867;&#25216;&#26415;&#20026;&#21830;&#19994;&#21644;&#20303;&#23429;&#31038;&#21306;&#30340;&#33021;&#37327;&#20379;&#24212;&#32773;&#35774;&#35745;&#21644;&#25191;&#34892;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#35745;&#21010;&#30340;&#21487;&#33021;&#24615;&#12290;&#35813;&#35745;&#21010;&#30340;&#30446;&#30340;&#26159;&#25913;&#21464;&#24847;&#22823;&#21033;&#20998;&#24067;&#24335;&#33021;&#28304;&#31038;&#21306;&#20869;&#30340;&#20379;&#24212;&#32773;&#30340;&#28040;&#36153;&#34892;&#20026;&#12290;&#36825;&#31181;&#32858;&#21512;&#26088;&#22312;&#65306;a&#65289;&#26368;&#23567;&#21270;&#22312;&#20027;&#35201;&#21464;&#30005;&#31449;&#22788;&#20135;&#29983;&#30340;&#21453;&#21521;&#21151;&#29575;&#27969;&#65292;&#35813;&#21151;&#29575;&#27969;&#22312;&#24403;&#22320;&#30005;&#32593;&#20013;&#30340;&#22826;&#38451;&#33021;&#30005;&#27744;&#30340;&#21457;&#30005;&#37327;&#36229;&#36807;&#28040;&#32791;&#26102;&#20250;&#21457;&#29983;; b&#65289;&#21066;&#20943;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#21151;&#23792;&#38656;&#27714;&#65292;&#35813;&#38656;&#27714;&#36890;&#24120;&#21457;&#29983;&#22312;&#20621;&#26202;&#26102;&#20998;&#12290;&#22312;&#32858;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#28909;&#38376;&#30340;&#30005;&#36127;&#33655;&#32858;&#31867;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;-&#21363;k-means&#65292;k-medoids&#21644;&#19968;&#31181;&#32858;&#21512;&#23618;&#27425;&#32858;&#31867;-alongside&#20004;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;-&#21363;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#21463;&#38480;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#65288;DTW&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#39564;&#35777;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#39033;&#26032;&#39062;&#30340;&#25351;&#26631;-&#21363;&#23792;&#20540;&#24615;&#33021;&#35780;&#20998;&#65288;PPS&#65289;
&lt;/p&gt;
&lt;p&gt;
The present study explores the use of clustering techniques for the design and implementation of a demand response (DR) program for commercial and residential prosumers. The goal of the program is to alter the consumption behavior of the prosumers pertaining to a distributed energy community in Italy. This aggregation aims to: a) minimize the reverse power flow at the primary substation, that occurs when generation from solar panels in the local grid exceeds consumption, and b) shave the system wide peak demand, that typically occurs during the hours of late afternoon. Regarding the clustering stage, three popular machine learning algorithms for electrical load clustering are employed -namely k-means, k-medoids and an agglomerative hierarchical clustering- alongside two different distance measures -namely euclidean and constrained dynamic time warping (DTW). We evaluate the methods using multiple validation metrics including a novel metric -namely peak performance score (PPS)- that we 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#21152;&#36895;&#22120;AccelTran&#65292;&#29992;&#20110;&#21160;&#24577;&#25512;&#29702;&#20013;&#30340;Transformer&#65292;&#36890;&#36807;&#36816;&#34892;&#26102;&#20462;&#21098;&#28608;&#27963;&#24182;&#20248;&#21270;&#25968;&#25454;&#27969;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#24179;&#34913;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#21152;&#36895;&#22120;&#65292;AccelTran&#22312;&#27169;&#22411;&#23610;&#23544;&#21644;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24179;&#22343;1.9&#20493;&#30340;&#21152;&#36895;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25512;&#29702;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.14705</link><description>&lt;p&gt;
AccelTran&#65306;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#21152;&#36895;&#22120;&#65292;&#29992;&#20110;&#21160;&#24577;&#25512;&#29702;&#20013;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers. (arXiv:2302.14705v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#21152;&#36895;&#22120;AccelTran&#65292;&#29992;&#20110;&#21160;&#24577;&#25512;&#29702;&#20013;&#30340;Transformer&#65292;&#36890;&#36807;&#36816;&#34892;&#26102;&#20462;&#21098;&#28608;&#27963;&#24182;&#20248;&#21270;&#25968;&#25454;&#27969;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#24179;&#34913;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#29616;&#26377;&#21152;&#36895;&#22120;&#65292;AccelTran&#22312;&#27169;&#22411;&#23610;&#23544;&#21644;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24179;&#22343;1.9&#20493;&#30340;&#21152;&#36895;&#27604;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Self-attention&#30340;Transformer&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#23427;&#20204;&#39640;&#25928;&#65292;&#20294;&#30001;&#20110;&#23427;&#30340;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#22823;&#30340;&#28608;&#27963;&#22823;&#23567;&#65292;&#21152;&#36895;Transformer&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#30340;Transformer&#21152;&#36895;&#22120;&#35797;&#22270;&#20462;&#21098;&#20854;&#20196;&#29260;&#20197;&#20943;&#23569;&#20869;&#23384;&#35775;&#38382;&#65292;&#20294;&#35745;&#31639;&#24320;&#38144;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#30452;&#25509;&#25805;&#20316;&#21442;&#19982;&#27880;&#24847;&#21147;&#25805;&#20316;&#30340;&#22823;&#22411;&#30697;&#38453;&#65292;&#36825;&#38480;&#21046;&#20102;&#30828;&#20214;&#21033;&#29992;&#29575;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#25512;&#29702;&#26041;&#26696;DynaTran&#65292;&#22312;&#36816;&#34892;&#26102;&#20462;&#21098;&#28608;&#27963;&#65292;&#24320;&#38144;&#36739;&#20302;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#26080;&#25928;&#25805;&#20316;&#30340;&#25968;&#37327;&#65292;&#25552;&#39640;&#20102;Transformer&#25512;&#29702;&#30340;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#35758;&#20351;&#29992;&#27839;&#30528;Transformer&#25805;&#20316;&#24179;&#38138;&#30697;&#38453;&#30340;&#21508;&#31181;&#25968;&#25454;&#27969;&#26469;&#25552;&#39640;&#25968;&#25454;&#37325;&#29992;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#30340;&#33021;&#37327;&#25928;&#29575;&#12290;&#20026;&#20102;&#26377;&#25928;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AccelTran&#65292;&#19968;&#31181;&#31232;&#30095;&#24863;&#30693;&#21152;&#36895;&#22120;&#65292;&#21033;&#29992;&#24179;&#38138;&#24863;&#30693;&#30828;&#20214;&#24182;&#34892;&#21644;&#19968;&#31181;&#26032;&#30340;&#20462;&#21098;&#26041;&#26696;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#22120;&#30456;&#27604;&#65292;AccelTran&#22312;&#27169;&#22411;&#23610;&#23544;&#21644;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24179;&#22343;&#21152;&#36895;&#27604;&#20026;1.9&#20493;&#30340;&#31454;&#20105;&#24615;&#25512;&#29702;&#31934;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22823;&#22411;&#21644;&#23567;&#22411;Transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#25512;&#29702;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-attention-based transformer models have achieved tremendous success in the domain of natural language processing. Despite their efficacy, accelerating the transformer is challenging due to its quadratic computational complexity and large activation sizes. Existing transformer accelerators attempt to prune its tokens to reduce memory access, albeit with high compute overheads. Moreover, previous works directly operate on large matrices involved in the attention operation, which limits hardware utilization. In order to address these challenges, this work proposes a novel dynamic inference scheme, DynaTran, which prunes activations at runtime with low overhead, substantially reducing the number of ineffectual operations. This improves the throughput of transformer inference. We further propose tiling the matrices in transformer operations along with diverse dataflows to improve data reuse, thus enabling higher energy efficiency. To effectively implement these methods, we propose Acce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#65306;&#20174;BERT&#21040;ChatGPT&#30340;&#21382;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;(PFMs)&#34987;&#35748;&#20026;&#26159;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;PFM(&#20363;&#22914;BERT&#12289;ChatGPT&#21644;GPT-4)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;BERT&#20174;&#36716;&#25442;&#22120;&#20013;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#31867;&#20284;&#22320;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#26041;&#27861;&#37319;&#29992;&#36716;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#37319;&#29992;&#33258;&#22238;&#24402;&#33539;&#24335;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;ChatGPT&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#21151;&#65292;&#23427;&#37319;&#29992;&#33258;&#22238;&#24402;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#23556;&#20987;&#25110;&#23569;&#23556;&#20987;&#25552;&#31034;&#12290;PFM&#30340;&#21331;&#36234;&#25104;&#23601;&#20026;&#21508;&#31181;AI&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#26356;&#26032;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;PFMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;PFMs&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#32500;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;2D&#26631;&#31614;&#22270;&#21512;&#25104;&#30456;&#24212;&#30340;&#22270;&#20687;&#65292;&#24182;&#25193;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26469;&#23454;&#29616;&#26126;&#30830;&#30340;&#19977;&#32500;&#29992;&#25143;&#25511;&#21046;&#65307;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#38543;&#24847;&#32534;&#36753;&#26631;&#31614;&#22270;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2302.08509</link><description>&lt;p&gt;
&#19977;&#32500;&#24863;&#30693;&#30340;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
3D-aware Conditional Image Synthesis. (arXiv:2302.08509v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08509
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#32500;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32473;&#23450;&#30340;2D&#26631;&#31614;&#22270;&#21512;&#25104;&#30456;&#24212;&#30340;&#22270;&#20687;&#65292;&#24182;&#25193;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26469;&#23454;&#29616;&#26126;&#30830;&#30340;&#19977;&#32500;&#29992;&#25143;&#25511;&#21046;&#65307;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#38543;&#24847;&#32534;&#36753;&#26631;&#31614;&#22270;&#24182;&#29983;&#25104;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;pix2pix3D&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#32500;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#36924;&#30495;&#22270;&#20687;&#21512;&#25104;&#12290;&#22312;&#32473;&#23450;2D&#26631;&#31614;&#22270;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20998;&#21106;&#25110;&#36793;&#32536;&#22270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#20174;&#19981;&#21516;&#35270;&#35282;&#21512;&#25104;&#30456;&#24212;&#30340;&#22270;&#20687;&#12290;&#20026;&#20102;&#23454;&#29616;&#26126;&#30830;&#30340;&#19977;&#32500;&#29992;&#25143;&#25511;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#20809;&#36752;&#23556;&#22330;&#25193;&#23637;&#20102;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#32473;&#23450;&#26222;&#36941;&#21487;&#29992;&#30340;&#21333;&#30524;&#22270;&#20687;&#21644;&#26631;&#31614;&#26144;&#23556;&#23545;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23398;&#20064;&#20026;&#27599;&#20010;&#19977;&#32500;&#28857;&#20998;&#37197;&#26631;&#31614;&#65292;&#38500;&#39068;&#33394;&#21644;&#23494;&#24230;&#22806;&#65292;&#36825;&#20351;&#23427;&#33021;&#22815;&#21516;&#26102;&#21576;&#29616;&#22270;&#20687;&#21644;&#20687;&#32032;&#23545;&#40784;&#30340;&#26631;&#31614;&#26144;&#23556;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#20801;&#35768;&#29992;&#25143;&#20174;&#20219;&#20309;&#35270;&#35282;&#32534;&#36753;&#26631;&#31614;&#26144;&#23556;&#24182;&#30456;&#24212;&#29983;&#25104;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose pix2pix3D, a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model learns to synthesize a corresponding image from different viewpoints. To enable explicit 3D user control, we extend conditional generative models with neural radiance fields. Given widely-available monocular images and label map pairs, our model learns to assign a label to every 3D point in addition to color and density, which enables it to render the image and pixel-aligned label map simultaneously. Finally, we build an interactive system that allows users to edit the label map from any viewpoint and generate outputs accordingly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;sphericart&#24211;&#20013;&#23454;&#29616;&#30340;&#65292;&#39640;&#25928;&#32780;&#20248;&#38597;&#30340;&#31639;&#27861;&#26469;&#35780;&#20272;&#23454;&#20540;&#29699;&#35856;&#20989;&#25968;&#65292;&#20854;&#20013;&#20855;&#26377;&#29616;&#26377;&#26041;&#26696;&#30340;&#35768;&#22810;&#29702;&#24819;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#20197;&#25968;&#20540;&#31283;&#23450;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#24335;&#35745;&#31639;&#31515;&#21345;&#23572;&#23548;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.08381</link><description>&lt;p&gt;
Sphericart&#20013;&#30340;&#29699;&#35856;&#20989;&#25968;&#24555;&#36895;&#35745;&#31639;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast evaluation of spherical harmonics with sphericart. (arXiv:2302.08381v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08381
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;sphericart&#24211;&#20013;&#23454;&#29616;&#30340;&#65292;&#39640;&#25928;&#32780;&#20248;&#38597;&#30340;&#31639;&#27861;&#26469;&#35780;&#20272;&#23454;&#20540;&#29699;&#35856;&#20989;&#25968;&#65292;&#20854;&#20013;&#20855;&#26377;&#29616;&#26377;&#26041;&#26696;&#30340;&#35768;&#22810;&#29702;&#24819;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#20197;&#25968;&#20540;&#31283;&#23450;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#24335;&#35745;&#31639;&#31515;&#21345;&#23572;&#23548;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29699;&#35856;&#20989;&#25968;&#26159;&#22312;&#29699;&#38754;&#19978;&#23637;&#24320;&#20989;&#25968;&#30340;&#24179;&#28369;&#12289;&#27491;&#20132;&#21644;&#23545;&#31216;&#36866;&#24212;&#24615;&#22522;&#30784;&#65292;&#23427;&#20204;&#22312;&#29289;&#29702;&#21270;&#23398;&#12289;&#22320;&#36136;&#23398;&#12289;&#22823;&#27668;&#31185;&#23398;&#12289;&#20449;&#21495;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#31561;&#19981;&#21516;&#31185;&#23398;&#21644;&#25216;&#26415;&#39046;&#22495;&#20013;&#34987;&#24120;&#35268;&#20351;&#29992;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#24050;&#25104;&#20026;&#22312;&#20960;&#20309;&#26426;&#22120;&#23398;&#20064;&#20013;&#26059;&#36716;&#31561;&#21464;&#27169;&#22411;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#24212;&#29992;&#20110;&#20998;&#23376;&#21644;&#26448;&#26009;&#30340;&#21407;&#23376;&#32423;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#38597;&#32780;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35780;&#20272;&#23454;&#20540;&#29699;&#35856;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#20855;&#26377;&#29616;&#26377;&#26041;&#26696;&#30340;&#35768;&#22810;&#29702;&#24819;&#29305;&#24615;&#65292;&#24182;&#20801;&#35768;&#20197;&#25968;&#20540;&#31283;&#23450;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#24335;&#35745;&#31639;&#31515;&#21345;&#23572;&#23548;&#25968;&#12290;&#20026;&#20102;&#26041;&#20415;&#20351;&#29992;&#65292;&#25105;&#20204;&#22312;sphericart&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#24555;&#36895;&#30340;C++&#24211;&#65292;&#36824;&#25552;&#20379;&#20102;C&#32465;&#23450;&#12289;Python API&#21644;&#19968;&#20010;&#21253;&#25324;GPU&#26680;&#24515;&#30340;PyTorch&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spherical harmonics provide a smooth, orthogonal, and symmetry-adapted basis to expand functions on a sphere, and they are used routinely in physical and theoretical chemistry as well as in different fields of science and technology, from geology and atmospheric sciences to signal processing and computer graphics. More recently, they have become a key component of rotationally equivariant models in geometric machine learning, including applications to atomic-scale modeling of molecules and materials. We present an elegant and efficient algorithm for the evaluation of the real-valued spherical harmonics. Our construction features many of the desirable properties of existing schemes and allows to compute Cartesian derivatives in a numerically stable and computationally efficient manner. To facilitate usage, we implement this algorithm in sphericart, a fast C++ library which also provides C bindings, a Python API, and a PyTorch implementation that includes a GPU kernel.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35889;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25552;&#20379;&#20102;SHMM&#20284;&#28982;&#20272;&#35745;&#30340;&#35823;&#24046;&#28176;&#36817;&#20998;&#24067;&#12289;&#25552;&#20986;&#25237;&#24433;SHMM&#31639;&#27861;&#21487;&#20197;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12289;&#24182;&#24320;&#21457;&#20102;SHMM&#21644;PSHMM&#30340;&#22312;&#32447;&#23398;&#20064;&#21464;&#20307;&#20197;&#36866;&#24212;&#28508;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;PSHMM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.07437</link><description>&lt;p&gt;
&#32553;&#23567;&#21487;&#29992;&#24615;&#24046;&#36317;&#65306;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35889;&#23398;&#20064;&#30340;&#29702;&#35770;&#19982;&#26041;&#27861;&#23398;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models. (arXiv:2302.07437v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#35889;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25552;&#20379;&#20102;SHMM&#20284;&#28982;&#20272;&#35745;&#30340;&#35823;&#24046;&#28176;&#36817;&#20998;&#24067;&#12289;&#25552;&#20986;&#25237;&#24433;SHMM&#31639;&#27861;&#21487;&#20197;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#12289;&#24182;&#24320;&#21457;&#20102;SHMM&#21644;PSHMM&#30340;&#22312;&#32447;&#23398;&#20064;&#21464;&#20307;&#20197;&#36866;&#24212;&#28508;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;PSHMM&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Baum-Welch&#65288;B-W&#65289;&#31639;&#27861;&#26159;&#25512;&#26029;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;(HMM)&#26368;&#24191;&#27867;&#25509;&#21463;&#30340;&#26041;&#27861;&#12290; &#28982;&#32780;&#65292;&#23427;&#24456;&#23481;&#26131;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#65292;&#32780;&#19988;&#23545;&#20110;&#35768;&#22810;&#23454;&#26102;&#24212;&#29992;&#26469;&#35828;&#36895;&#24230;&#22826;&#24930;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#27861;&#65288;MOM&#65289;&#30340;HMM&#30340;&#35889;&#23398;&#20064;&#65288;SHMM&#65289;&#65292;&#26088;&#22312;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#12290;&#23613;&#31649;&#26377;&#36825;&#26679;&#30340;&#25215;&#35834;&#65292;&#20294;SHMM&#30340;&#28176;&#36817;&#29702;&#35770;&#19968;&#30452;&#24456;&#38590;&#24471;&#21040;&#65292;&#32780;SHMM&#30340;&#38271;&#26399;&#24615;&#33021;&#21487;&#33021;&#20250;&#30001;&#20110;&#35823;&#24046;&#30340;&#26080;&#38480;&#20256;&#25773;&#32780;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;(1)&#25552;&#20379;&#20102;SHMM&#20284;&#28982;&#20272;&#35745;&#30340;&#36817;&#20284;&#35823;&#24046;&#30340;&#28176;&#36817;&#20998;&#24067;&#65292;(2)&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#31216;&#20026;&#25237;&#24433;SHMM&#65288;PSHMM&#65289;&#65292;&#23427;&#21487;&#20197;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;(3)&#24320;&#21457;&#20102;SHMM&#21644;PSHMM&#30340;&#22312;&#32447;&#23398;&#20064;&#21464;&#20307;&#65292;&#20197;&#36866;&#24212;&#28508;&#22312;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#25968;&#25454;&#19978;&#27604;&#36739;&#20102;SHMM&#12289;PSHMM&#21644;B-W&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Baum-Welch (B-W) algorithm is the most widely accepted method for inferring hidden Markov models (HMM). However, it is prone to getting stuck in local optima, and can be too slow for many real-time applications. Spectral learning of HMMs (SHMM), based on the method of moments (MOM) has been proposed in the literature to overcome these obstacles. Despite its promises, asymptotic theory for SHMM has been elusive, and the long-run performance of SHMM can degrade due to unchecked propagation of error. In this paper, we (1) provide an asymptotic distribution for the approximate error of the likelihood estimated by SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that mitigates the problem of error propagation, and (3) develop online learning variants of both SHMM and PSHMM that accommodate potential nonstationarity. We compare the performance of SHMM with PSHMM and estimation through the B-W algorithm on both simulated data and data from real world applications, and fin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07425</link><description>&lt;p&gt;
&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07425
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#31169;&#34892;&#20026;&#19979;&#30340;&#21163;&#21290;&#31038;&#20132;&#23398;&#20064;&#38382;&#39064;&#65292;&#21457;&#29616;&#23384;&#22312;&#19968;&#31181;&#25506;&#32034;&#28608;&#21169;&#26435;&#34913;&#65292;&#21363;&#27494;&#22120;&#25506;&#32034;&#21644;&#31038;&#20132;&#25506;&#32034;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21463;&#21040;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#30340;&#38480;&#21046;&#20250;&#21152;&#21095;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#23548;&#33268;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31038;&#20132;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#20195;&#29702;&#25353;&#29031;&#31616;&#21333;&#30340;&#22810;&#33218;&#21163;&#21290;&#21327;&#35758;&#20849;&#21516;&#34892;&#21160;&#12290;&#20195;&#29702;&#20197;&#39034;&#24207;&#26041;&#24335;&#21040;&#36798;&#65292;&#36873;&#25321;&#27494;&#22120;&#24182;&#25509;&#25910;&#30456;&#20851;&#22870;&#21169;&#12290;&#27599;&#20010;&#20195;&#29702;&#35266;&#23519;&#20808;&#21069;&#20195;&#29702;&#30340;&#23436;&#25972;&#21382;&#21490;&#35760;&#24405;&#65288;&#27494;&#22120;&#21644;&#22870;&#21169;&#65289;&#65292;&#19981;&#23384;&#22312;&#31169;&#26377;&#20449;&#21495;&#12290;&#23613;&#31649;&#20195;&#29702;&#20849;&#21516;&#38754;&#20020;&#24320;&#21457;&#21644;&#21033;&#29992;&#30340;&#25506;&#32034;&#25240;&#34935;&#65292;&#20294;&#27599;&#20010;&#20195;&#29702;&#20154;&#37117;&#26159;&#19968;&#35265;&#38047;&#24773;&#30340;&#65292;&#26080;&#38656;&#32771;&#34385;&#25506;&#32034;&#12290;&#25105;&#20204;&#20801;&#35768;&#19968;&#31995;&#21015;&#19982;&#65288;&#21442;&#25968;&#21270;&#65289;&#32622;&#20449;&#21306;&#38388;&#19968;&#33268;&#30340;&#33258;&#31169;&#34892;&#20026;&#65292;&#21253;&#25324;&#8220;&#26080;&#20559;&#8221;&#34892;&#20026;&#21644;&#21508;&#31181;&#34892;&#20026;&#20559;&#24046;&#12290;&#34429;&#28982;&#36825;&#20123;&#34892;&#20026;&#30340;&#26497;&#31471;&#29256;&#26412;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#21163;&#21290;&#31639;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#26356;&#28201;&#21644;&#30340;&#29256;&#26412;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#25506;&#32034;&#22833;&#36133;&#65292;&#22240;&#27492;&#36951;&#25022;&#29575;&#19982;&#20195;&#29702;&#25968;&#37327;&#25104;&#32447;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#8220;&#28201;&#21644;&#20048;&#35266;&#8221;&#30340;&#20195;&#29702;&#25552;&#20379;&#21305;&#37197;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25506;&#32034;&#28608;&#21169;&#20043;&#38388;&#30340;&#22522;&#26412;&#26435;&#34913;&#65306;&#27494;&#22120;&#25506;&#32034;&#26159;&#22266;&#26377;&#20110;&#21163;&#21290;&#38382;&#39064;&#30340;&#65292;&#21482;&#21463;&#24403;&#21069;&#20195;&#29702;&#30340;&#34892;&#21160;&#24433;&#21709;&#65292;&#32780;&#31038;&#20132;&#25506;&#32034;&#26159;&#30001;&#20808;&#21069;&#20195;&#29702;&#34892;&#20026;&#39537;&#21160;&#30340;&#65292;&#22240;&#27492;&#26377;&#21033;&#20110;&#26410;&#26469;&#20195;&#29702;&#12290;&#30001;&#20110;&#20195;&#29702;&#30340;&#30701;&#35270;&#34892;&#20026;&#38480;&#21046;&#20102;&#31038;&#20132;&#25506;&#32034;&#65292;&#36825;&#31181;&#26435;&#34913;&#34987;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;</title><link>http://arxiv.org/abs/2301.11526</link><description>&lt;p&gt;
&#25289;&#26222;&#25289;&#26031;&#26377;&#30028;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#30452;&#25509;&#21442;&#25968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Direct Parameterization of Lipschitz-Bounded Deep Networks. (arXiv:2301.11526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20855;&#26377;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#65292;&#36890;&#36807;&#26631;&#20934;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#26041;&#24335;&#65288;&#20840;&#36830;&#25509;&#21644;&#21367;&#31215;&#32593;&#32476;&#65289;&#65292;&#20855;&#26377;&#26377;&#38480;&#28789;&#25935;&#24230;&#30340;&#25289;&#26222;&#25289;&#26031;&#30028;&#38480;&#12290;&#19982;SDP&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;"&#30452;&#25509;"&#21442;&#25968;&#21270;&#26041;&#24335;&#65292;&#24182;&#36890;&#36807;&#26631;&#20934;&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#25237;&#24433;&#25110;&#38556;&#30861;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a ``direct'' parameterization, i.e. a smooth mapping from $\mathbb R^N$ onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the \textit{sandwich layer}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. Finally, the comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24418;&#36716;&#25442;&#22120;&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#24086;&#23376;&#21518;&#32493;&#30340;&#35752;&#35770;&#26469;&#25104;&#21151;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19981;&#36275;&#65292;&#20197;&#21450;&#26410;&#26469;&#21487;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#20840;&#38754;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2301.10871</link><description>&lt;p&gt;
&#22270;&#24418;&#36716;&#25442;&#22120;&#26041;&#27861;&#38024;&#23545;&#21464;&#21270;&#21160;&#24577;&#20869;&#23481;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#30340;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24418;&#36716;&#25442;&#22120;&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#24086;&#23376;&#21518;&#32493;&#30340;&#35752;&#35770;&#26469;&#25104;&#21151;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#19981;&#36275;&#65292;&#20197;&#21450;&#26410;&#26469;&#21487;&#20197;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#20840;&#38754;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#24418;&#36716;&#25442;&#22120;&#32593;&#32476;&#21644;&#22522;&#20110;BERT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65292;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#39044;&#27979;&#31038;&#20132;&#23186;&#20307;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32771;&#34385;&#24086;&#23376;&#21518;&#32493;&#30340;&#35752;&#35770;&#26469;&#25104;&#21151;&#26816;&#27979;&#20986;&#21487;&#33021;&#20986;&#29616;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#19982;&#31454;&#20105;&#26041;&#27861;&#30456;&#27604;&#22312;&#21738;&#20123;&#22330;&#26223;&#19979;&#26377;&#26368;&#20248;&#34920;&#29616;&#20197;&#21450;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#24403;&#21069;&#31038;&#20132;&#23186;&#20307;&#20013;&#23384;&#22312;&#30340;&#24694;&#24847;&#22270;&#29255;&#31561;&#21508;&#31181;&#24086;&#23376;&#31867;&#22411;&#65292;&#25552;&#20986;&#20102;&#25193;&#23637;&#27169;&#22411;&#30340;&#24605;&#36335;&#12290;&#20851;&#38190;&#30340;&#27934;&#35265;&#22312;&#20110;&#35813;&#26041;&#27861;&#27880;&#37325;&#23545;&#19978;&#19979;&#25991;&#27010;&#24565;&#30340;&#25512;&#29702;&#65292;&#22240;&#27492;&#35813;&#26041;&#27861;&#20855;&#22791;&#24456;&#22823;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work advances an approach for predicting hate speech in social media, drawing out the critical need to consider the discussions that follow a post to successfully detect when hateful discourse may arise. Using graph transformer networks, coupled with modelling attention and BERT-level natural language processing, our approach can capture context and anticipate upcoming anti-social behaviour. In this paper, we offer a detailed qualitative analysis of this solution for hate speech detection in social networks, leading to insights into where the method has the most impressive outcomes in comparison with competitors and identifying scenarios where there are challenges to achieving ideal performance. Included is an exploration of the kinds of posts that permeate social media today, including the use of hateful images. This suggests avenues for extending our model to be more comprehensive. A key insight is that the focus on reasoning about the concept of context positions us well to be a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#22312;&#32447;&#22810;&#26680;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20843;&#21350;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#38750;&#23436;&#20840;&#36830;&#25509;&#30340;&#32593;&#32476;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#20122;&#32447;&#24615;&#21518;&#24724;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.09848</link><description>&lt;p&gt;
&#22810;&#26680;&#22312;&#32447;&#23398;&#20064;&#30340;&#20843;&#21350;&#19982;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gossiped and Quantized Online Multi-Kernel Learning. (arXiv:2301.09848v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#22312;&#32447;&#22810;&#26680;&#23398;&#20064;&#30340;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20843;&#21350;&#31639;&#27861;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#38750;&#23436;&#20840;&#36830;&#25509;&#30340;&#32593;&#32476;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#20122;&#32447;&#24615;&#21518;&#24724;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24456;&#23569;&#26377;&#20808;&#39564;&#20449;&#24687;&#21487;&#29992;&#19988;&#26080;&#27861;&#36827;&#34892;&#38598;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#26680;&#23398;&#20064;&#20013;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21482;&#35201;&#32593;&#32476;&#20013;&#30340;&#27599;&#23545;&#33410;&#28857;&#37117;&#21487;&#20197;&#36890;&#20449;&#65288;&#21363;&#36890;&#20449;&#32593;&#32476;&#26159;&#23436;&#20840;&#22270;&#65289;&#65292;&#20998;&#24067;&#24335;&#21644;&#22312;&#32447;&#22810;&#26680;&#23398;&#20064;&#23601;&#20250;&#25552;&#20379;&#20122;&#32447;&#24615;&#21518;&#24724;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#31649;&#29702;&#36890;&#20449;&#36127;&#36733;&#65288;&#36890;&#24120;&#26159;&#24615;&#33021;&#29942;&#39048;&#65289;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#21487;&#20197;&#37327;&#21270;&#12290;&#35813;&#25991;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#38750;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#19978;&#65292;&#36825;&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#32463;&#24120;&#21457;&#29983;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20843;&#21350;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#36798;&#21040;&#20122;&#32447;&#24615;&#21518;&#24724;&#30340;&#35777;&#26126;&#12290;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In instances of online kernel learning where little prior information is available and centralized learning is unfeasible, past research has shown that distributed and online multi-kernel learning provides sub-linear regret as long as every pair of nodes in the network can communicate (i.e., the communications network is a complete graph). In addition, to manage the communication load, which is often a performance bottleneck, communications between nodes can be quantized. This letter expands on these results to non-fully connected graphs, which is often the case in wireless sensor networks. To address this challenge, we propose a gossip algorithm and provide a proof that it achieves sub-linear regret. Experiments with real datasets confirm our findings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#36890;&#36807;&#26410;&#30693;&#32447;&#24615;&#36716;&#25442;&#38388;&#25509;&#35266;&#23519;&#26102;&#30340;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#38382;&#39064;&#12290;&#20854;&#20805;&#20998;&#26465;&#20214;&#30830;&#20445;&#20102;&#24178;&#39044;&#25928;&#26524;&#21487;&#20197;&#20174;&#20998;&#25968;&#30340;&#21464;&#21270;&#20013;&#27491;&#30830;&#26816;&#27979;&#20986;&#26469;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#21270;&#20998;&#25968;&#20989;&#25968;&#21464;&#21270;&#30340;&#20851;&#38190;&#29305;&#24615;&#23436;&#32654;&#24674;&#22797;&#26377;&#25928;&#21464;&#25442;&#12290;</title><link>http://arxiv.org/abs/2301.08230</link><description>&lt;p&gt;
&#24102;&#24178;&#39044;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning with Interventions. (arXiv:2301.08230v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#36890;&#36807;&#26410;&#30693;&#32447;&#24615;&#36716;&#25442;&#38388;&#25509;&#35266;&#23519;&#26102;&#30340;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#38382;&#39064;&#12290;&#20854;&#20805;&#20998;&#26465;&#20214;&#30830;&#20445;&#20102;&#24178;&#39044;&#25928;&#26524;&#21487;&#20197;&#20174;&#20998;&#25968;&#30340;&#21464;&#21270;&#20013;&#27491;&#30830;&#26816;&#27979;&#20986;&#26469;&#65292;&#24182;&#21033;&#29992;&#26368;&#23567;&#21270;&#20998;&#25968;&#20989;&#25968;&#21464;&#21270;&#30340;&#20851;&#38190;&#29305;&#24615;&#23436;&#32654;&#24674;&#22797;&#26377;&#25928;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#36890;&#36807;&#26410;&#30693;&#32447;&#24615;&#36716;&#25442;&#38388;&#25509;&#35266;&#23519;&#26102;&#30340;&#22240;&#26524;&#34920;&#24449;&#23398;&#20064;&#38382;&#39064;&#12290;&#24314;&#31435;&#20102;DAG&#37325;&#26500;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#34920;&#26126;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22823;&#31867;&#38750;&#32447;&#24615;&#27169;&#22411;&#28385;&#36275;&#36825;&#20123;&#26465;&#20214;&#65292;&#30830;&#20445;&#20102;&#24178;&#39044;&#25928;&#26524;&#21487;&#20197;&#20174;&#20998;&#25968;&#30340;&#21464;&#21270;&#20013;&#27491;&#30830;&#26816;&#27979;&#20986;&#26469;&#12290;&#21033;&#29992;&#26368;&#23567;&#21270;&#20998;&#25968;&#20989;&#25968;&#21464;&#21270;&#30340;&#20851;&#38190;&#29305;&#24615;&#65292;&#21487;&#20197;&#23436;&#32654;&#24674;&#22797;&#26377;&#25928;&#21464;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the causal representation learning problem when the latent causal variables are observed indirectly through an unknown linear transformation. The objectives are: (i) recovering the unknown linear transformation (up to scaling) and (ii) determining the directed acyclic graph (DAG) underlying the latent variables. Sufficient conditions for DAG recovery are established, and it is shown that a large class of non-linear models in the latent space (e.g., causal mechanisms parameterized by two-layer neural networks) satisfy these conditions. These sufficient conditions ensure that the effect of an intervention can be detected correctly from changes in the score. Capitalizing on this property, recovering a valid transformation is facilitated by the following key property: any valid transformation renders latent variables' score function to necessarily have the minimal variations across different interventional environments. This property is leveraged for perfect recovery of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#26631;&#35760;&#30340;&#38480;&#21046;&#23545;&#26426;&#22120;&#20154;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2301.07015</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#35760;&#26041;&#27861;&#38480;&#21046;&#20102;Twitter&#26426;&#22120;&#20154;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection. (arXiv:2301.07015v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#26631;&#35760;&#30340;&#38480;&#21046;&#23545;&#26426;&#22120;&#20154;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#23545;&#20110;&#22312;&#32447;&#24179;&#21488;&#30340;&#23433;&#20840;&#21644;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#26816;&#27979;&#26426;&#22120;&#20154;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#29616;&#26377;&#25968;&#25454;&#38598;&#26631;&#27880;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#30340;&#20915;&#31574;&#35268;&#21017;&#65292;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#26631;&#35760;&#30340;&#38480;&#21046;&#23545;&#26426;&#22120;&#20154;&#26816;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules -- shallow decision trees trained on a small number of features -- achieve near-state-of-the-art performance on most available datasets and that bot detec
&lt;/p&gt;</description></item><item><title>&#26576;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; NRC&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#22330;&#26223;&#12289;&#23454;&#29616;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12289;&#24182;&#33021;&#22815;&#33258;&#21160;&#20998;&#21106;&#65292;&#22312;THOR&#20013;&#30340;&#23545;&#35937;&#23548;&#33322;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.04101</link><description>&lt;p&gt;
&#31070;&#32463;&#36752;&#23556;&#22330;&#20195;&#30721;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Radiance Field Codebooks. (arXiv:2301.04101v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04101
&lt;/p&gt;
&lt;p&gt;
&#26576;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; NRC&#65292;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#22330;&#26223;&#12289;&#23454;&#29616;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#12289;&#24182;&#33021;&#22815;&#33258;&#21160;&#20998;&#21106;&#65292;&#22312;THOR&#20013;&#30340;&#23545;&#35937;&#23548;&#33322;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#36716;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#30340;&#32452;&#21512;&#34920;&#31034;&#26159;&#23454;&#29616;&#39640;&#32423;&#22330;&#26223;&#29702;&#35299;&#21644;&#26377;&#25928;&#36716;&#31227;&#33267;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#19968;&#27493;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#36890;&#36807;&#26032;&#39062;&#30340;&#35270;&#22270;&#37325;&#26500;&#26041;&#27861;&#24341;&#20837;&#20102;&#31070;&#32463;&#36752;&#23556;&#22330;&#20195;&#30721;&#26412;(NRC)&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#34920;&#31034;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#12290;NRC&#23398;&#20064;&#20351;&#29992;&#23545;&#35937;&#32534;&#30721;&#23383;&#20856;&#36890;&#36807;&#20307;&#31215;&#28210;&#26579;&#22120;&#35299;&#30721;&#20197;&#20174;&#26032;&#35270;&#22270;&#37325;&#24314;&#22330;&#26223;&#12290;&#36825;&#20351;&#24471;&#33021;&#22815;&#21457;&#29616;&#22312;&#22330;&#26223;&#20013;&#21453;&#22797;&#20986;&#29616;&#30340;&#35270;&#35273;&#21644;&#20960;&#20309;&#27169;&#24335;&#65292;&#24182;&#19988;&#21487;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NRC&#34920;&#31034;&#22312;THOR&#20013;&#30340;&#23545;&#35937;&#23548;&#33322;&#20013;&#30340;&#33391;&#22909;&#36716;&#31227;&#33021;&#21147;&#65292;&#25104;&#21151;&#29575;&#27604;2D&#21644;3D&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#39640;&#20986;3.1%&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#21512;&#25104;&#65288;THOR&#65289;&#21644;&#30495;&#23454;&#22330;&#26223;&#65288;NYU Depth&#65289;&#30340;&#26080;&#30417;&#30563;&#20998;&#21106;(29%)&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#36335;&#24452;&#35206;&#30422;&#29575;&#21644;&#20851;&#31995;&#36335;&#24452;&#32622;&#20449;&#24230;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36807;&#28388;&#20986;&#19981;&#21487;&#38752;&#30340;&#36335;&#24452;&#65292;&#25552;&#39640;&#22522;&#20110;&#21477;&#23376;Transformer&#30340;&#30693;&#35782;&#25512;&#29702;&#21477;&#23376;Transformer&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;KG&#20013;&#23454;&#29616;&#20102;&#22810;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24863;&#30693;&#20851;&#31995;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2301.01664</link><description>&lt;p&gt;
&#22522;&#20110;&#21477;&#23376;Transformer&#36827;&#34892;&#22810;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24863;&#30693;&#20851;&#31995;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#36335;&#24452;&#35206;&#30422;&#29575;&#21644;&#20851;&#31995;&#36335;&#24452;&#32622;&#20449;&#24230;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36807;&#28388;&#20986;&#19981;&#21487;&#38752;&#30340;&#36335;&#24452;&#65292;&#25552;&#39640;&#22522;&#20110;&#21477;&#23376;Transformer&#30340;&#30693;&#35782;&#25512;&#29702;&#21477;&#23376;Transformer&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#22312;KG&#20013;&#23454;&#29616;&#20102;&#22810;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24863;&#30693;&#20851;&#31995;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#24863;&#30693;&#21644;&#21487;&#35299;&#37322;&#20851;&#31995;&#39044;&#27979;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#31995;&#36335;&#24452;&#35206;&#30422;&#29575;&#21644;&#20851;&#31995;&#36335;&#24452;&#32622;&#20449;&#24230;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#27169;&#22411;&#35757;&#32451;&#20043;&#21069;&#36807;&#28388;&#20986;&#19981;&#21487;&#38752;&#30340;&#36335;&#24452;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#30693;&#35782;&#25512;&#29702;&#21477;&#23376;Transformer&#8221;&#65288;KRST&#65289;&#26469;&#39044;&#27979;KG&#20013;&#30340;&#24863;&#30693;&#20851;&#31995;&#12290; KRST&#34987;&#35774;&#35745;&#20026;&#22312;KG&#20013;&#32534;&#30721;&#25552;&#21462;&#20986;&#30340;&#21487;&#38752;&#36335;&#24452;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#27491;&#30830;&#22320;&#32858;&#31867;&#36335;&#24452;&#24182;&#25552;&#20379;&#22810;&#26041;&#38754;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;SOTA&#27169;&#22411;&#30456;&#27604;&#65292;KRST&#22312;&#22823;&#22810;&#25968;&#24863;&#30693;&#21644;&#24863;&#30693;&#27979;&#35797;&#29992;&#20363;&#65288;6&#20010;&#20013;&#30340;4&#20010;&#65289;&#20197;&#21450;12&#20010;few-shot&#27979;&#35797;&#29992;&#20363;&#20013;&#30340;11&#20010;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20302;&#25104;&#26412;&#20302;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#21644;&#39640;&#25104;&#26412;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#20043;&#38388;&#33258;&#21160;&#20999;&#25442;&#65292;&#23454;&#29616;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#34394;&#20551;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2212.14118</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20445;&#30495;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#23398;&#20064;&#25511;&#21046;&#22120;&#34394;&#20551;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Falsification of Learning-Based Controllers through Multi-Fidelity Bayesian Optimization. (arXiv:2212.14118v4 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14118
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#20302;&#25104;&#26412;&#20302;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#21644;&#39640;&#25104;&#26412;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#20043;&#38388;&#33258;&#21160;&#20999;&#25442;&#65292;&#23454;&#29616;&#23398;&#20064;&#25511;&#21046;&#22120;&#30340;&#34394;&#20551;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#34394;&#20551;&#26816;&#27979;&#26159;&#19968;&#31181;&#22686;&#21152;&#31995;&#32479;&#28385;&#36275;&#23433;&#20840;&#35201;&#27714;&#20449;&#24515;&#30340;&#23454;&#29992;&#27979;&#35797;&#26041;&#27861;&#12290;&#22240;&#20026;&#23436;&#20840;&#20445;&#30495;&#24230;&#30340;&#20223;&#30495;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#38656;&#27714;&#22823;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#20445;&#30495;&#24230;&#27700;&#24179;&#30340;&#27169;&#25311;&#22120;&#30340;&#21487;&#34892;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;&#23433;&#20840;&#35268;&#26684;&#21270;&#20026;&#29615;&#22659;&#21442;&#25968;&#65292;&#24182;&#23558;&#35813;&#23433;&#20840;&#35268;&#26684;&#21270;&#20026;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#22810;&#20445;&#30495;&#24230;&#34394;&#20551;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#30830;&#23450;&#24212;&#22312;&#21738;&#20010;&#20445;&#30495;&#24230;&#32423;&#21035;&#36827;&#34892;&#23433;&#20840;&#35780;&#20272;&#65292;&#21516;&#26102;&#20174;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#22833;&#25928;&#30340;&#29615;&#22659;&#20013;&#25214;&#21040;&#21487;&#33021;&#30340;&#23454;&#20363;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#25552;&#20379;&#20302;&#25104;&#26412;&#19981;&#20934;&#30830;&#20449;&#24687;&#21644;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#25552;&#20379;&#39640;&#25104;&#26412;&#20934;&#30830;&#20449;&#24687;&#20043;&#38388;&#33258;&#21160;&#20999;&#25442;&#65292;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#26368;&#22823;&#21270;&#12290;&#22312;&#19981;&#21516;&#20223;&#30495;&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#20445;&#30495;&#24230;&#34394;&#20551;&#26816;&#27979;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-based falsification is a practical testing method to increase confidence that the system will meet safety requirements. Because full-fidelity simulations can be computationally demanding, we investigate the use of simulators with different levels of fidelity. As a first step, we express the overall safety specification in terms of environmental parameters and structure this safety specification as an optimization problem. We propose a multi-fidelity falsification framework using Bayesian optimization, which is able to determine at which level of fidelity we should conduct a safety evaluation in addition to finding possible instances from the environment that cause the system to fail. This method allows us to automatically switch between inexpensive, inaccurate information from a low-fidelity simulator and expensive, accurate information from a high-fidelity simulator in a cost-effective way. Our experiments on various environments in simulation demonstrate that multi-fidelit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#19978;&#34920;&#29616;&#19981;&#36275;&#20197;&#21450;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SSM&#23618;H3&#65292;&#24182;&#23558;&#20854;&#19982;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#31361;&#20986;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.14052</link><description>&lt;p&gt;
&#39269;&#39295;&#30340;&#27827;&#39532;&#65306;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#19978;&#34920;&#29616;&#19981;&#36275;&#20197;&#21450;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SSM&#23618;H3&#65292;&#24182;&#23558;&#20854;&#19982;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#26368;&#26032;&#24615;&#33021;&#65292;&#31361;&#20986;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#22312;&#26576;&#20123;&#27169;&#24577;&#19979;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24207;&#21015;&#24314;&#27169;&#24615;&#33021;&#65292;&#20294;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;SSMs&#30340;&#24207;&#21015;&#38271;&#24230;&#36817;&#20046;&#32447;&#24615;&#22320;&#25193;&#23637;&#32780;&#19981;&#26159;&#20108;&#27425;&#26041;&#65292;&#20294;&#30001;&#20110;&#30828;&#20214;&#21033;&#29992;&#29575;&#20302;&#19979;&#65292;&#23427;&#20204;&#20173;&#28982;&#27604;&#21464;&#21387;&#22120;&#26356;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#29702;&#35299;&#20102;SSMs&#21644;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#36317;&#65292;&#24182;&#38477;&#20302;&#20102;SSMs&#21644;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#20043;&#38388;&#30340;&#30828;&#20214;&#38556;&#30861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#26469;&#29702;&#35299;SSMs&#21644;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;SSMs&#22312;&#20004;&#20010;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65306;&#22238;&#24518;&#20808;&#21069;&#30340;&#26631;&#35760;&#21644;&#36328;&#24207;&#21015;&#27604;&#36739;&#26631;&#35760;&#12290;&#20026;&#20102;&#29702;&#35299;&#23545;&#35821;&#35328;&#24314;&#27169;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;SSM&#23618;&#65292;H3&#65292;&#19987;&#38376;&#35774;&#35745;&#36825;&#20123;&#33021;&#21147;&#12290;H3&#22312;&#21512;&#25104;&#35821;&#35328;&#19978;&#19982;&#24314;&#27169;&#20851;&#27880;&#26426;&#21046;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;OpenWebText&#19978;&#27604;&#21464;&#21387;&#22120;&#23569;&#20102;0.4 PPL&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;H3&#21644;&#27880;&#24847;&#21147;&#20197;&#21450;&#30828;&#20214;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20984;&#26174;&#20102;SSMs&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#22914;&#20309;&#35774;&#35745;&#26356;&#22909;&#30340;SSMs&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 
&lt;/p&gt;</description></item><item><title>&#31616;&#32780;&#35328;&#20043;&#65292;&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#20013;&#30340;&#8220;&#23567;&#25968;&#25454;&#8221;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;GAN&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;GAN&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;</title><link>http://arxiv.org/abs/2212.13591</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#19979;&#30340;&#25968;&#25454;&#20013;&#24515;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#36827;&#23637;&#12289;&#32570;&#38519;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Guided Data-Centric AI in Healthcare: Progress, Shortcomings, and Future Directions. (arXiv:2212.13591v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13591
&lt;/p&gt;
&lt;p&gt;
&#31616;&#32780;&#35328;&#20043;&#65292;&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#20351;&#29992;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#35786;&#26029;&#20013;&#30340;&#8220;&#23567;&#25968;&#25454;&#8221;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#25968;&#25454;&#22686;&#24378;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;GAN&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;GAN&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#22823;&#37327;&#21253;&#21547;&#29305;&#23450;&#27010;&#24565;&#25110;&#24847;&#20041;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#25317;&#26377;&#35206;&#30422;&#29305;&#23450;&#30142;&#30149;&#30340;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#24320;&#21457;&#20986;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#35813;&#30142;&#30149;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#28508;&#22312;&#30340;&#22909;&#22788;&#65292;&#30001;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#35786;&#26029;&#24182;&#27809;&#26377;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#24378;&#35843;&#22312;&#25968;&#25454;&#34920;&#31034;&#26041;&#38754;&#37319;&#29992;&#25968;&#25454;&#20013;&#24515;&#30340;&#26041;&#27861;&#20197;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#8220;&#23567;&#25968;&#25454;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22235;&#31181;&#29983;&#25104;&#21644;&#32858;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65306;&#25968;&#25454;&#22686;&#24378;&#12289;&#36801;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;GAN&#65288;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#30693;&#35782;&#24341;&#23548;&#30340;GAN&#26469;&#23558;&#39046;&#22495;&#30693;&#35782;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of deep learning is largely due to the availability of large amounts of training data that cover a wide range of examples of a particular concept or meaning. In the field of medicine, having a diverse set of training data on a particular disease can lead to the development of a model that is able to accurately predict the disease. However, despite the potential benefits, there have not been significant advances in image-based diagnosis due to a lack of high-quality annotated data. This article highlights the importance of using a data-centric approach to improve the quality of data representations, particularly in cases where the available data is limited. To address this "small-data" issue, we discuss four methods for generating and aggregating training data: data augmentation, transfer learning, federated learning, and GANs (generative adversarial networks). We also propose the use of knowledge-guided GANs to incorporate domain knowledge in the training data generation pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.09849</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#24182;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#23454;&#29616;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#24182;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24314;&#31435;&#30340;&#21333;&#20010;&#27169;&#22411;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#26500;&#24314;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#24050;&#32463;&#21487;&#29992;&#65292;&#20294;&#20854;&#35757;&#32451;&#25968;&#25454;&#19981;&#21487;&#29992;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#30693;&#35782;&#20135;&#26435;&#38382;&#39064;&#12290;&#36825;&#23601;&#36896;&#25104;&#20102;&#36328;&#27169;&#22411;&#34701;&#21512;&#30693;&#35782;&#20197;&#20135;&#29983;&#26356;&#22909;&#30340;&#21333;&#19968;&#27169;&#22411;&#30340;&#38556;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24314;&#31435;&#22312;&#19981;&#21516;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#21512;&#24182;&#30340;&#38382;&#39064;&#65292;&#20197;&#24471;&#21040;&#19968;&#20010;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#39046;&#22495;&#19978;&#34920;&#29616;&#33391;&#22909;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#22495;&#22806;&#25968;&#25454;&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#30693;&#35782;&#34701;&#21512;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#21512;&#24182;&#27169;&#22411;&#65292;&#30001;&#26435;&#37325;&#24341;&#23548;&#65292;&#20197;&#26368;&#23567;&#21270;&#21512;&#24182;&#27169;&#22411;&#21644;&#21333;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#24046;&#24322;&#12290;&#22312;&#19968;&#31995;&#21015;&#35780;&#20272;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22914;Fisher&#21152;&#26435;&#24179;&#22343;&#25110;&#27169;&#22411;&#38598;&#25104;&#31561;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#22810;&#35821;&#35328;&#24494;&#35843;&#26367;&#20195;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#27880;&#37322;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#22312;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2212.03366</link><description>&lt;p&gt;
&#22810;&#23618;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#21450;&#20854;&#22312;&#20912;&#24029;&#20912;&#27169;&#22411;&#36125;&#21494;&#26031;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Further analysis of multilevel Stein variational gradient descent with an application to the Bayesian inference of glacier ice models. (arXiv:2212.03366v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#23618;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#22312;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23618;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#26159;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#25104;&#26412;&#21644;&#20445;&#30495;&#24230;&#30340;&#26367;&#20195;&#30446;&#26631;&#20998;&#24067;&#30340;&#23618;&#27425;&#32467;&#26500;&#26469;&#21152;&#36895;&#25512;&#26029;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#31532;&#19968;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20043;&#21069;&#25104;&#26412;&#22797;&#26434;&#24230;&#20998;&#26512;&#30340;&#25193;&#23637;&#65292;&#21363;&#20351;&#21333;&#23618;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#20381;&#36182;&#20110;&#36845;&#20195;&#21464;&#21270;&#30340;&#21442;&#25968;&#20063;&#36866;&#29992;&#12290;&#31532;&#20108;&#65292;&#23558;&#22810;&#23618;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#24212;&#29992;&#20110;&#25512;&#26029;Arolla&#20912;&#24029;&#20912;&#30340;&#31163;&#25955;&#22522;&#24213;&#28369;&#21160;&#31995;&#25968;&#22330;&#30340;&#22823;&#35268;&#27169;&#36125;&#21494;&#26031;&#21453;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22810;&#23618;&#29256;&#26412;&#30456;&#23545;&#20110;&#21333;&#23618;&#29256;&#26412;&#23454;&#29616;&#20102;&#25968;&#37327;&#32423;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilevel Stein variational gradient descent is a method for particle-based variational inference that leverages hierarchies of surrogate target distributions with varying costs and fidelity to computationally speed up inference. The contribution of this work is twofold. First, an extension of a previous cost complexity analysis is presented that applies even when the exponential convergence rate of single-level Stein variational gradient descent depends on iteration-varying parameters. Second, multilevel Stein variational gradient descent is applied to a large-scale Bayesian inverse problem of inferring discretized basal sliding coefficient fields of the Arolla glacier ice. The numerical experiments demonstrate that the multilevel version achieves orders of magnitude speedups compared to its single-level version.
&lt;/p&gt;</description></item><item><title>Edge Impulse&#26159;&#19968;&#20010;&#38754;&#21521;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;MLOps&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#23884;&#20837;&#24335;&#21644;&#36793;&#32536;ML&#31995;&#32479;&#30340;&#36719;&#30828;&#20214;&#20248;&#21270;&#25903;&#25345;&#65292;&#35299;&#20915;TinyML&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.03332</link><description>&lt;p&gt;
Edge Impulse: &#38754;&#21521;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;MLOps&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge Impulse: An MLOps Platform for Tiny Machine Learning. (arXiv:2212.03332v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03332
&lt;/p&gt;
&lt;p&gt;
Edge Impulse&#26159;&#19968;&#20010;&#38754;&#21521;&#24494;&#22411;&#26426;&#22120;&#23398;&#20064;&#30340;MLOps&#24179;&#21488;&#65292;&#26088;&#22312;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#23884;&#20837;&#24335;&#21644;&#36793;&#32536;ML&#31995;&#32479;&#30340;&#36719;&#30828;&#20214;&#20248;&#21270;&#25903;&#25345;&#65292;&#35299;&#20915;TinyML&#30340;&#21487;&#31227;&#26893;&#24615;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Edge Impulse&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#36816;&#33829;(MLOps)&#24179;&#21488;&#65292;&#29992;&#20110;&#24320;&#21457;&#21487;&#20197;&#37096;&#32626;&#21040;&#21508;&#31181;&#30828;&#20214;&#30446;&#26631;&#30340;&#23884;&#20837;&#24335;&#21644;&#36793;&#32536;ML(&#24494;&#22411;ML)&#31995;&#32479;&#12290;&#24403;&#21069;&#30340;TinyML&#24037;&#20316;&#27969;&#31243;&#23384;&#22312;&#30528;&#36719;&#20214;&#22534;&#26632;&#30862;&#29255;&#21270;&#21644;&#24322;&#26500;&#37096;&#32626;&#30828;&#20214;&#31561;&#38382;&#39064;&#65292;&#20351;&#24471;ML&#27169;&#22411;&#20248;&#21270;&#22256;&#38590;&#19988;&#19981;&#20855;&#22791;&#21487;&#31227;&#26893;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Edge Impulse&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#29992;&#30340;MLOps&#24179;&#21488;&#65292;&#21487;&#23454;&#29616;&#22823;&#35268;&#27169;&#24320;&#21457;TinyML&#31995;&#32479;&#12290;Edge Impulse&#36890;&#36807;&#25903;&#25345;&#21508;&#31181;&#36719;&#20214;&#21644;&#30828;&#20214;&#20248;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#21019;&#24314;&#21487;&#25193;&#23637;&#19988;&#21487;&#31227;&#26893;&#30340;&#36719;&#20214;&#22534;&#26632;&#65292;&#20197;&#36866;&#24212;&#21508;&#31181;&#23884;&#20837;&#24335;&#31995;&#32479;&#12290; &#25130;&#33267;2022&#24180;10&#26376;&#65292;Edge Impulse&#25176;&#31649;&#20102;&#26469;&#33258;50,953&#21517;&#24320;&#21457;&#20154;&#21592;&#30340;118,185&#20010;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge Impulse is a cloud-based machine learning operations (MLOps) platform for developing embedded and edge ML (TinyML) systems that can be deployed to a wide range of hardware targets. Current TinyML workflows are plagued by fragmented software stacks and heterogeneous deployment hardware, making ML model optimizations difficult and unportable. We present Edge Impulse, a practical MLOps platform for developing TinyML systems at scale. Edge Impulse addresses these challenges and streamlines the TinyML design cycle by supporting various software and hardware optimizations to create an extensible and portable software stack for a multitude of embedded systems. As of Oct. 2022, Edge Impulse hosts 118,185 projects from 50,953 developers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;PHN-HVI&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#30340;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2212.01130</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#26412;&#36229;&#32593;&#32476;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Pareto Front Learning via Multi-Sample Hypernetworks. (arXiv:2212.01130v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;PHN-HVI&#65292;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#19968;&#32452;&#22810;&#26679;&#30340;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#32047;&#25176;&#21069;&#27839;&#23398;&#20064;(PFL)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33719;&#24471;&#20174;&#32473;&#23450;&#26435;&#34913;&#21521;&#37327;&#21040;&#24085;&#32047;&#25176;&#21069;&#27839;&#35299;&#30340;&#26144;&#23556;&#20989;&#25968;&#65292;&#20174;&#32780;&#35299;&#20915;&#22810;&#30446;&#26631;&#20248;&#21270;(MOO)&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#24573;&#30053;&#20102;&#20248;&#21270;&#36807;&#31243;&#20013;&#35299;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#33719;&#24471;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;PFL&#26694;&#26550;&#65292;&#21363;PHN-HVI&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#20174;&#22810;&#26679;&#30340;&#26435;&#34913;&#20559;&#22909;&#38598;&#29983;&#25104;&#22810;&#20010;&#35299;&#65292;&#24182;&#36890;&#36807;&#26368;&#22823;&#21270;&#36825;&#20123;&#35299;&#23450;&#20041;&#30340;&#36229;&#20307;&#31215;&#25351;&#26631;&#26469;&#25552;&#39640;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#36136;&#37327;&#12290;&#22810;&#20010;MOO&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PFL&#26041;&#27861;&#65292;PHN-HVI&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#36817;&#20284;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;LSTM&#39044;&#27979;COVID-19&#30149;&#20363;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#25968;&#25454;&#33258;&#36866;&#24212;&#24615;&#20915;&#23450;&#27169;&#22411;&#22359;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#22312;&#20840;&#38754;&#25968;&#20540;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.17014</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#39044;&#27979;&#27169;&#22411;&#39044;&#27979;COVID-19&#30149;&#20363;&#25968;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Interpretable Hybrid Predictive Model of COVID-19 Cases using Autoregressive Model and LSTM. (arXiv:2211.17014v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28151;&#21512;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;LSTM&#39044;&#27979;COVID-19&#30149;&#20363;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#25968;&#25454;&#33258;&#36866;&#24212;&#24615;&#20915;&#23450;&#27169;&#22411;&#22359;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#22312;&#20840;&#38754;&#25968;&#20540;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2019&#20896;&#29366;&#30149;&#27602;&#30149;&#65288;COVID-19&#65289;&#23545;&#20840;&#29699;&#20581;&#24247;&#21644;&#32463;&#27982;&#20135;&#29983;&#20102;&#28145;&#36828;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#26500;&#24314;&#20934;&#30830;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#39537;&#21160;&#39044;&#27979;&#27169;&#22411;&#20197;&#25913;&#21892;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;AR&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#38271;&#30701;&#26102;&#35760;&#24518;&#31070;&#32463;&#32593;&#32476;&#65288;LSTM&#65289;&#30340;&#39044;&#27979;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#35813;&#28151;&#21512;&#27169;&#22411;&#34987;&#27491;&#24335;&#24314;&#27169;&#20026;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#32467;&#26500;&#36830;&#25509;&#20004;&#20010;&#32452;&#25104;&#27169;&#22411;&#22359;&#65292;&#36825;&#20004;&#20010;&#22359;&#30340;&#30456;&#23545;&#36129;&#29486;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36827;&#34892;&#25968;&#25454;&#33258;&#36866;&#24212;&#24615;&#20915;&#23450;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#25968;&#25454;&#28304;&#30340;&#20840;&#38754;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28151;&#21512;&#27169;&#22411;&#22312;&#20854;&#20004;&#20010;&#32452;&#25104;&#27169;&#22411;&#20197;&#21450;&#20854;&#20182;&#27969;&#34892;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Coronavirus Disease 2019 (COVID-19) has a profound impact on global health and economy, making it crucial to build accurate and interpretable data-driven predictive models for COVID-19 cases to improve policy making. The extremely large scale of the pandemic and the intrinsically changing transmission characteristics pose great challenges for effective COVID-19 case prediction. To address this challenge, we propose a novel hybrid model in which the interpretability of the Autoregressive model (AR) and the predictive power of the long short-term memory neural networks (LSTM) join forces. The proposed hybrid model is formalized as a neural network with an architecture that connects two composing model blocks, of which the relative contribution is decided data-adaptively in the training procedure. We demonstrate the favorable performance of the hybrid model over its two component models as well as other popular predictive models through comprehensive numerical studies on two data sour
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#30340;&#38450;&#24481;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#24182;&#20197;&#21338;&#24328;&#35770;&#30340;&#26041;&#24335;&#36827;&#34892;&#21512;&#20316;&#21644;&#31454;&#20105;&#65292;&#24418;&#25104;&#19968;&#20010;&#32852;&#21512;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2211.14669</link><description>&lt;p&gt;
&#21338;&#24328;&#35770;&#28151;&#21512;&#19987;&#23478;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning. (arXiv:2211.14669v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#30340;&#38450;&#24481;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#65292;&#24182;&#20197;&#21338;&#24328;&#35770;&#30340;&#26041;&#24335;&#36827;&#34892;&#21512;&#20316;&#21644;&#31454;&#20105;&#65292;&#24418;&#25104;&#19968;&#20010;&#32852;&#21512;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#37027;&#20123;&#34987;&#35748;&#20026;&#26159;&#24378;&#20581;&#30340;&#38450;&#24481;&#25514;&#26045;&#23454;&#38469;&#19978;&#36824;&#26159;&#23481;&#26131;&#21463;&#21040;&#38024;&#23545;&#20854;&#24369;&#28857;&#36827;&#34892;&#23450;&#21046;&#21270;&#25915;&#20987;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#21253;&#25324;&#38543;&#26426;&#21464;&#25442;&#30340;&#25915;&#20987;&#65288;BaRT&#65289;&#65292;&#26377;&#30410;&#20154;&#31867;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;FAT&#65289;&#65292;&#22403;&#22334;&#23601;&#26159;&#29645;&#23453;&#65288;TiT&#65289;&#20197;&#21450;&#30001;&#35270;&#35273;&#36716;&#25442;&#22120;&#12289;&#22823;&#22411;&#36716;&#31227;&#27169;&#22411;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#30340;&#32452;&#21512;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21338;&#24328;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#32452;&#21512;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65292;&#27599;&#20010;&#19987;&#23478;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#30340;&#38450;&#24481;&#25915;&#20987;&#36827;&#34892;&#38450;&#24481;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#19987;&#23478;&#20250;&#20197;&#21338;&#24328;&#35770;&#30340;&#26041;&#24335;&#36827;&#34892;&#21512;&#20316;&#21644;&#31454;&#20105;&#65292;&#24418;&#25104;&#19968;&#20010;&#32852;&#21512;&#38450;&#24481;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#25915;&#20987;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in adversarial machine learning have shown that defenses considered to be robust are actually susceptible to adversarial attacks which are specifically customized to target their weaknesses. These defenses include Barrage of Random Transforms (BaRT), Friendly Adversarial Training (FAT), Trash is Treasure (TiT) and ensemble models made up of Vision Transformers (ViTs), Big Transfer models and Spiking Neural Networks (SNNs). We first conduct a transferability analysis, to demonstrate the adversarial examples generated by customized attacks on one defense, are not often misclassified by another defense.  This finding leads to two important questions. First, how can the low transferability between defenses be utilized in a game theoretic framework to improve the robustness? Second, how can an adversary within this framework develop effective multi-model attacks? In this paper, we provide a game-theoretic framework for ensemble adversarial attacks and defenses. Our framework
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.11176</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20381;&#36182;&#22270;&#30340;&#36890;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;(GraphS4mer)&#65292;&#29992;&#20110;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#26469;&#35299;&#20915;&#38271;&#26102;&#24207;&#21644;&#22797;&#26434;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#22312;&#35768;&#22810;&#21307;&#23398;&#39046;&#22495;&#20013;&#37117;&#24456;&#26222;&#36941;&#65292;&#20363;&#22914;&#33041;&#30005;&#22270;&#12289;&#22810;&#23548;&#30561;&#30496;&#22270;&#21644;&#24515;&#30005;&#22270;&#12290;&#30001;&#20110;&#65288;1&#65289;&#38271;&#26102;&#38388;&#33539;&#22260;&#20869;&#30340;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#65288;2&#65289;&#30005;&#26497;&#20043;&#38388;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24314;&#31435;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#34920;&#31034;&#20026;&#26102;&#38388;&#20381;&#36182;&#22270;&#65292;&#24182;&#20171;&#32461;&#20102;GraphS4mer&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32467;&#26500;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#26469;&#25552;&#39640;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#65288;1&#65289;&#25105;&#20204;&#21033;&#29992;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#26550;&#26500;&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24207;&#21015;&#27169;&#22411;&#65292;&#26469;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#20013;&#38271;&#26102;&#38388;&#33539;&#22260;&#30340;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#65288;2&#65289;&#25105;&#20204;&#24314;&#35758;&#22312;GraphS4mer&#20013;&#28155;&#21152;&#22270;&#32467;&#26500;&#23398;&#20064;&#23618;&#65292;&#20197;&#23398;&#20064;&#25968;&#25454;&#20013;&#21160;&#24577;&#28436;&#21464;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#24314;&#31435;&#20855;&#26377;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20803;&#29983;&#29289;&#20449;&#21495;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
&lt;/p&gt;</description></item><item><title>SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2210.15185</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#28210;&#26579;&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering. (arXiv:2210.15185v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15185
&lt;/p&gt;
&lt;p&gt;
SAM-RL&#20351;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#65292;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#24863;&#30693;&#24863;&#30693;&#30340;&#23398;&#20064;&#31649;&#36947;&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290; &#29992;&#20110;&#23436;&#25104;&#26426;&#22120;&#20154;&#32452;&#35013;&#65292;&#24037;&#20855;&#25805;&#20316;&#21644;&#21464;&#24418;&#29289;&#20307;&#25805;&#20316;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;MBRL&#65289;&#20855;&#26377;&#27604;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#22914;&#20309;&#20174;&#21407;&#22987;&#24863;&#23448;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#33258;&#21160;&#26377;&#25928;&#22320;&#24320;&#21457;&#20934;&#30830;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21644;&#20219;&#21153;&#65292;&#26159;&#38480;&#21046;MBRL&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SAM-RL&#30340;&#24863;&#30693;&#24863;&#30693;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#12290;&#21033;&#29992;&#19981;&#21487;&#23548;&#29289;&#29702;&#20223;&#30495;&#21644;&#28210;&#26579;&#65292;SAM-RL&#36890;&#36807;&#27604;&#36739;&#28210;&#26579;&#22270;&#20687;&#21644;&#30495;&#23454;&#21407;&#22987;&#22270;&#20687;&#33258;&#21160;&#26356;&#26032;&#27169;&#22411;&#24182;&#39640;&#25928;&#20135;&#29983;&#31574;&#30053;&#12290;&#36890;&#36807;&#24863;&#30693;&#24863;&#30693;&#23398;&#20064;&#31649;&#36947;&#65292;SAM-RL&#20801;&#35768;&#26426;&#22120;&#20154;&#36873;&#25321;&#19968;&#20010;&#20449;&#24687;&#20016;&#23500;&#30340;&#35270;&#35282;&#26469;&#30417;&#25511;&#20219;&#21153;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#23454;&#38469;&#30340;&#19977;&#20010;&#25805;&#20316;&#20219;&#21153;&#65306;&#26426;&#22120;&#20154;&#35013;&#37197;&#65292;&#24037;&#20855;&#25805;&#32437;&#21644;&#21487;&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; Bellman &#22266;&#23450;&#28857;&#26041;&#31243;&#36827;&#34892;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20540;&#36845;&#20195;&#32593;&#32476;&#21450;&#20854;&#21464;&#20307;&#30340;&#21069;&#21518;&#20256;&#36882;&#35299;&#32806;&#65292;&#21487;&#23454;&#29616;&#22312;&#35268;&#21010;&#35270;&#31243;&#20869;&#31283;&#23450;&#19988;&#28789;&#27963;&#21069;&#21521;&#39044;&#31639;&#30340;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35268;&#21010;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.13542</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#24335;&#24494;&#20998;&#23454;&#29616;&#21487;&#21464;&#35268;&#27169;&#31283;&#23450;&#30340;&#21487;&#24494;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation. (arXiv:2210.13542v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807; Bellman &#22266;&#23450;&#28857;&#26041;&#31243;&#36827;&#34892;&#24494;&#20998;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20540;&#36845;&#20195;&#32593;&#32476;&#21450;&#20854;&#21464;&#20307;&#30340;&#21069;&#21518;&#20256;&#36882;&#35299;&#32806;&#65292;&#21487;&#23454;&#29616;&#22312;&#35268;&#21010;&#35270;&#31243;&#20869;&#31283;&#23450;&#19988;&#28789;&#27963;&#21069;&#21521;&#39044;&#31639;&#30340;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35268;&#21010;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#35268;&#21010;&#25215;&#35834;&#20855;&#26377;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#38459;&#27490;&#20102;&#23427;&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#38382;&#39064;&#19978;&#30340;&#25193;&#23637;&#65306;&#38656;&#35201;&#36890;&#36807;&#21521;&#21069;&#36845;&#20195;&#23618;&#36827;&#34892;&#24494;&#20998;&#20197;&#35745;&#31639;&#26799;&#24230;&#65292;&#36825;&#20250;&#23558;&#21069;&#21521;&#35745;&#31639;&#21644;&#21453;&#21521;&#20256;&#25773;&#32806;&#21512;&#36215;&#26469;&#65292;&#24182;&#38656;&#35201;&#24179;&#34913;&#21069;&#21521;&#35268;&#21010;&#22120;&#30340;&#24615;&#33021;&#21644;&#21453;&#21521;&#20256;&#36882;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807; Bellman &#22266;&#23450;&#28857;&#26041;&#31243;&#36827;&#34892;&#24494;&#20998;&#20197;&#23558;&#20540;&#36845;&#20195;&#32593;&#32476;&#21450;&#20854;&#21464;&#20307;&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#21453;&#21521;&#20256;&#25773;&#25104;&#26412;&#65288;&#22312;&#35268;&#21010;&#35270;&#31243;&#20869;&#65289;&#20445;&#25345;&#19981;&#21464;&#65292;&#21516;&#26102;&#21069;&#21521;&#39044;&#31639;&#26356;&#21152;&#28789;&#27963;&#65292;&#26377;&#21161;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#38382;&#39064;&#19978;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340; VIN &#38544;&#24335;&#29256;&#26412;&#21450;&#20854;&#21464;&#20307;&#30340;&#25910;&#25947;&#31283;&#23450;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#35268;&#21010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#20248;&#36234;&#24615;&#65306;2D &#23548;&#33322;&#12289;&#35270;&#35273;&#23548;&#33322;&#20197;&#21450;&#26500;&#22411;&#31354;&#38388;&#21644;&#24037;&#20316;&#31354;&#38388;&#20013;&#30340; 2-DOF &#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation, and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace.
&lt;/p&gt;</description></item><item><title>NVIDIA FLARE&#26159;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#24037;&#20855;&#21253;&#65288;SDK&#65289;&#65292;&#20351;&#25968;&#25454;&#31185;&#23398;&#23478;&#33021;&#22815;&#26356;&#36731;&#26494;&#22320;&#22312;&#20854;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;FL&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;FL&#31639;&#27861;&#21644;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;SDK&#21487;&#29992;&#20110;&#26500;&#24314;&#20998;&#24067;&#24335;&#23398;&#20064;&#24037;&#20316;&#27969;&#24182;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#25110;&#24046;&#20998;&#38544;&#31169;&#21019;&#24314;&#23433;&#20840;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#26041;&#21512;&#20316;&#20135;&#21697;&#12290;</title><link>http://arxiv.org/abs/2210.13291</link><description>&lt;p&gt;
NVIDIA FLARE: &#20174;&#27169;&#25311;&#21040;&#23454;&#38469;&#19990;&#30028;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
NVIDIA FLARE: Federated Learning from Simulation to Real-World. (arXiv:2210.13291v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13291
&lt;/p&gt;
&lt;p&gt;
NVIDIA FLARE&#26159;&#19968;&#20010;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#24037;&#20855;&#21253;&#65288;SDK&#65289;&#65292;&#20351;&#25968;&#25454;&#31185;&#23398;&#23478;&#33021;&#22815;&#26356;&#36731;&#26494;&#22320;&#22312;&#20854;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;FL&#65292;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;FL&#31639;&#27861;&#21644;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;SDK&#21487;&#29992;&#20110;&#26500;&#24314;&#20998;&#24067;&#24335;&#23398;&#20064;&#24037;&#20316;&#27969;&#24182;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#25110;&#24046;&#20998;&#38544;&#31169;&#21019;&#24314;&#23433;&#20840;&#19988;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#26041;&#21512;&#20316;&#20135;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21512;&#20316;&#32773;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#32780;&#19981;&#38598;&#20013;&#25968;&#25454;&#65292;&#23454;&#29616;&#26500;&#24314;&#24378;&#20581;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;NVIDIA FLARE&#65292;&#20316;&#20026;&#24320;&#28304;&#36719;&#20214;&#24320;&#21457;&#24037;&#20855;&#21253;&#65288;SDK&#65289;&#65292;&#20351;&#25968;&#25454;&#31185;&#23398;&#23478;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#22312;&#20854;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;FL&#12290;&#35813;SDK&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;FL&#31639;&#27861;&#21644;&#32852;&#21512;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#22312;&#20225;&#19994;&#20013;&#26500;&#24314;&#20998;&#24067;&#24335;&#23398;&#20064;&#24037;&#20316;&#27969;&#65292;&#24182;&#20351;&#24179;&#21488;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#20351;&#29992;&#21516;&#24577;&#21152;&#23494;&#25110;&#24046;&#20998;&#38544;&#31169;&#21019;&#24314;&#23433;&#20840;&#30340;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#26041;&#21512;&#20316;&#20135;&#21697;&#12290;&#35813;SDK&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#12289;&#28789;&#27963;&#30340;&#12289;&#21487;&#25193;&#23637;&#30340;Python&#21253;&#12290;&#23427;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#22312;&#20219;&#20309;&#22521;&#35757;&#24211;&#65288;PyTorch&#12289;TensorFlow&#12289;XGBoost&#65292;&#29978;&#33267;NumPy&#65289;&#20013;&#24212;&#29992;&#20854;&#25968;&#25454;&#31185;&#23398;&#24037;&#20316;&#27969;&#65292;&#22312;&#30495;&#23454;&#30340;FL&#29615;&#22659;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables building robust and generalizable AI models by leveraging diverse datasets from multiple collaborators without centralizing the data. We created NVIDIA FLARE as an open-source software development kit (SDK) to make it easier for data scientists to use FL in their research and real-world applications. The SDK includes solutions for state-of-the-art FL algorithms and federated machine learning approaches, which facilitate building workflows for distributed learning across enterprises and enable platform developers to create a secure, privacy-preserving offering for multiparty collaboration utilizing homomorphic encryption or differential privacy. The SDK is a lightweight, flexible, and scalable Python package. It allows researchers to apply their data science workflows in any training libraries (PyTorch, TensorFlow, XGBoost, or even NumPy) in real-world FL settings. This paper introduces the key design principles of NVFlare and illustrates some use cases (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#19982;Conv2D LSTM&#23618;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#30340;&#34892;&#20026;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2210.12527</link><description>&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#20013;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#20351;&#29992;&#21160;&#20316;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Baby Physical Safety Monitoring in Smart Home Using Action Recognition System. (arXiv:2210.12527v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#19982;Conv2D LSTM&#23618;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#30340;&#34892;&#20026;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#28436;&#32462;&#25512;&#29702;&#30452;&#35266;&#22320;&#25512;&#26029;&#20986;&#20004;&#20010;&#29366;&#24577;&#20043;&#38388;&#21457;&#29983;&#30340;&#34892;&#20026;&#65292;&#36825;&#26159;&#22240;&#20026;&#22823;&#33041;&#25805;&#20316;&#22312;&#21452;&#21521;&#36890;&#20449;&#27169;&#22411;&#19978;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#19982;&#20197;&#24448;&#32463;&#39564;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#35782;&#21035;&#21644;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21160;&#20316;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26377;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#29305;&#23450;&#30340;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#20013;&#38754;&#20020;&#30528;&#23567;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#19982;&#22823;&#22810;&#25968;&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#19968;&#26679;&#65292;&#22312;&#31354;&#38388;-&#26102;&#38388;&#25968;&#25454;&#20013;&#20934;&#30830;&#25551;&#36848;&#27963;&#21160;&#30340;&#27169;&#31946;&#24615;&#26159;&#19968;&#20010;&#32570;&#28857;&#65292;&#21487;&#20197;&#36890;&#36807;&#31574;&#21010;&#36866;&#24403;&#30340;&#25968;&#25454;&#38598;&#26469;&#20811;&#26381;&#65292;&#21253;&#25324;&#23545;&#35270;&#39057;&#25968;&#25454;&#36827;&#34892;&#32454;&#33268;&#30340;&#27880;&#37322;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20998;&#26512;&#21508;&#31181;&#35782;&#21035;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36731;&#37327;&#32423;&#26694;&#26550;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#19982;Conv2D LSTM&#23618;&#30456;&#32467;&#21512;&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;I3D&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#29992;&#20110;&#23156;&#20799;&#29289;&#29702;&#23433;&#20840;&#30417;&#27979;&#30340;&#34892;&#20026;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. This is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. During the past decade, deep learning models for action recognition have significantly improved. However, deep neural networks struggle with these tasks on a smaller dataset for specific Action Recognition (AR) tasks. As with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. In this study, we present a novel lightweight framework combining transfer learning techniques with a Conv2D LSTM layer to extract features from the pre-trained I3D model on the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37319;&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26469;&#24314;&#27169;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#30340;&#36924;&#36817;&#65292;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21306;&#22495;&#30340;&#31934;&#30830;&#22806;&#25512;&#65292;&#24182;&#24212;&#29992;&#24352;&#37327;&#23436;&#25104;&#31639;&#27861;&#20248;&#21270;&#20302;&#31209;&#27491;&#20132;-&#22810;&#39033;&#24335;&#65288;CP&#65289;&#20998;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.10184</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#23436;&#25104;&#30340;&#22810;&#21442;&#25968;&#24615;&#33021;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Multi-Parameter Performance Modeling via Tensor Completion. (arXiv:2210.10184v2 [cs.PF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37319;&#29992;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26469;&#24314;&#27169;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#30340;&#36924;&#36817;&#65292;&#23454;&#29616;&#23545;&#26410;&#35266;&#27979;&#21306;&#22495;&#30340;&#31934;&#30830;&#22806;&#25512;&#65292;&#24182;&#24212;&#29992;&#24352;&#37327;&#23436;&#25104;&#31639;&#27861;&#20248;&#21270;&#20302;&#31209;&#27491;&#20132;-&#22810;&#39033;&#24335;&#65288;CP&#65289;&#20998;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#26159;&#24615;&#33021;&#35843;&#25972;&#12289;&#36719;&#30828;&#20214;&#21327;&#21516;&#35774;&#35745;&#21644;&#20316;&#19994;&#35843;&#24230;&#31561;&#35768;&#22810;&#20219;&#21153;&#25152;&#20381;&#36182;&#30340;&#27169;&#22411;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#20302;&#31209;&#24352;&#37327;&#20998;&#35299;&#26469;&#24314;&#27169;&#24212;&#29992;&#31243;&#24207;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#35268;&#21017;&#32593;&#26684;&#23545;&#24212;&#29992;&#31243;&#24207;&#30340;&#36755;&#20837;&#21644;&#37197;&#32622;&#22495;&#36827;&#34892;&#31163;&#25955;&#21270;&#12290;&#22312;&#32593;&#26684;&#21333;&#20803;&#20013;&#26144;&#23556;&#30340;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#26102;&#38388;&#34987;&#24179;&#22343;&#24182;&#34920;&#31034;&#20026;&#24352;&#37327;&#20803;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20302;&#31209;&#30340;&#27491;&#20132;-&#22810;&#39033;&#24335;&#65288;CP&#65289;&#24352;&#37327;&#20998;&#35299;&#36890;&#36807;&#23545;&#36825;&#20123;&#24352;&#37327;&#36827;&#34892;&#36924;&#36817;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#36825;&#31181;&#20998;&#35299;&#20351;&#24471;&#23545;&#19968;&#20010;&#24212;&#29992;&#31243;&#24207;&#21442;&#25968;&#31354;&#38388;&#20013;&#26410;&#35266;&#27979;&#21306;&#22495;&#30340;&#31934;&#30830;&#22806;&#25512;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#24352;&#37327;&#23436;&#25104;&#26469;&#20248;&#21270;&#32473;&#23450;&#19968;&#32452;&#31232;&#30095;&#30340;&#35266;&#23519;&#30340;&#36816;&#34892;&#26102;&#38388;&#30340;CP&#20998;&#35299;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20845;&#20010;&#24212;&#29992;&#30340;&#26367;&#20195;&#20998;&#27573;/&#32593;&#26684;&#27169;&#22411;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20351;&#29992;&#24352;&#37327;&#23436;&#25104;&#21151;&#33021;&#20248;&#21270;&#30340;CP&#20998;&#35299;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#23384;&#20648;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performance tuning, software/hardware co-design, and job scheduling are among the many tasks that rely on models to predict application performance. We propose and evaluate low rank tensor decomposition for modeling application performance. We discretize the input and configuration domain of an application using regular grids. Application execution times mapped within grid-cells are averaged and represented by tensor elements. We show that low-rank canonical-polyadic (CP) tensor decomposition is effective in approximating these tensors. We further show that this decomposition enables accurate extrapolation of unobserved regions of an application's parameter space. We then employ tensor completion to optimize a CP decomposition given a sparse set of observed runtimes. We consider alternative piecewise/grid-based models and supervised learning models for six applications and demonstrate that CP decomposition optimized using tensor completion offers higher prediction accuracy and memory-e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#22312;&#32447;&#28216;&#25103;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#20197; Dota2 &#20026;&#20363;&#65292;&#36890;&#36807;&#23545;&#29609;&#23478;&#25968;&#25454;&#30340;&#25366;&#25496;&#65292;&#25104;&#21151;&#22320;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#20986;&#20102;&#29609;&#23478;&#30340;&#30495;&#23454;&#29983;&#27963;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09028</link><description>&lt;p&gt;
&#22312;&#32447;&#22810;&#20154;&#28216;&#25103;&#20013;&#30340;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65306;&#20197; Dota2 &#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attribute Inference Attacks in Online Multiplayer Video Games: a Case Study on Dota2. (arXiv:2210.09028v5 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#22312;&#32447;&#28216;&#25103;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;&#20197; Dota2 &#20026;&#20363;&#65292;&#36890;&#36807;&#23545;&#29609;&#23478;&#25968;&#25454;&#30340;&#25366;&#25496;&#65292;&#25104;&#21151;&#22320;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#20986;&#20102;&#29609;&#23478;&#30340;&#30495;&#23454;&#29983;&#27963;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24744;&#30693;&#36947;&#21527;&#65311;&#36229;&#36807; 7000 &#19975; Dota2 &#29609;&#23478;&#30340;&#28216;&#25103;&#25968;&#25454;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#22914;&#26524;&#34987;&#24694;&#24847;&#21033;&#29992;&#20250;&#24102;&#26469;&#20160;&#20040;&#38382;&#39064;&#65311;&#26412;&#25991;&#26159;&#31532;&#19968;&#31687;&#30740;&#31350;&#36825;&#31181;&#38382;&#39064;&#30340;&#35770;&#25991;&#12290;&#37492;&#20110;&#35270;&#39057;&#28216;&#25103;&#30340;&#26222;&#21450;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; Dota2 &#29615;&#22659;&#19979;&#23646;&#24615;&#25512;&#26029;&#25915;&#20987;&#65288;AIA&#65289;&#30340;&#31532;&#19968;&#20010;&#23041;&#32961;&#27169;&#22411;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#25915;&#20987;&#32773;&#22914;&#20309;&#65288;&#20197;&#21450;&#20026;&#20160;&#20040;&#65289;&#21487;&#20197;&#21033;&#29992; Dota2 &#29983;&#24577;&#31995;&#32479;&#20013;&#20016;&#23500;&#30340;&#20844;&#20849;&#25968;&#25454;&#26469;&#25512;&#26029;&#20854;&#29609;&#23478;&#30340;&#31169;&#20154;&#20449;&#24687;&#12290;&#30001;&#20110;&#32570;&#20047;&#20851;&#20110;&#25105;&#20204; AIA &#25928;&#26524;&#30340;&#20855;&#20307;&#35777;&#25454;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28085;&#30422; 26000 &#22330;&#27604;&#36187;&#30340;&#32422; 500 &#21517; Dota2 &#29609;&#23478;&#36827;&#34892;&#24191;&#27867;&#35843;&#26597;&#65292;&#39564;&#35777;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#20013;&#30340;&#24433;&#21709;&#21147;&#12290;&#22312;&#21457;&#29616;&#29609;&#23478;&#30340; Dota2 &#27963;&#21160;&#19982;&#20854;&#29616;&#23454;&#29983;&#27963;&#20043;&#38388;&#23384;&#22312;&#32852;&#31995;&#65288;$p$ &lt; 0.01 &#21644; $\rho$ &gt; 0.3&#65289;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181; AIA &#30340;&#20262;&#29702;&#23454;&#39564;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#25512;&#26029;&#20986;&#25105;&#20204;&#35843;&#26597;&#23545;&#35937;&#30340;&#29616;&#23454;&#29983;&#27963;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Did you know that over 70 million of Dota2 players have their in-game data freely accessible? What if such data is used in malicious ways? This paper is the first to investigate such a problem.  Motivated by the widespread popularity of video games, we propose the first threat model for Attribute Inference Attacks (AIA) in the Dota2 context. We explain how (and why) attackers can exploit the abundant public data in the Dota2 ecosystem to infer private information about its players. Due to lack of concrete evidence on the efficacy of our AIA, we empirically prove and assess their impact in reality. By conducting an extensive survey on $\sim$500 Dota2 players spanning over 26k matches, we verify whether a correlation exists between a player's Dota2 activity and their real-life. Then, after finding such a link ($p$ &lt; 0.01 and $\rho$ &gt; 0.3), we ethically perform diverse AIA. We leverage the capabilities of machine learning to infer real-life attributes of the respondents of our survey by u
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;DRL&#26694;&#26550;ToupleGDD&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#23558;&#19977;&#20010;&#32806;&#21512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#23884;&#20837;&#65292;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#23398;&#20064;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#23427;&#20204;&#38598;&#25104;&#36215;&#26469;&#65292;&#25429;&#33719;&#32593;&#32476;&#20013;&#33410;&#28857;&#21644;&#36793;&#32536;&#30340;&#26356;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ToupleGDD&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.07500</link><description>&lt;p&gt;
ToupleGDD&#65306;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#31934;&#32454;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
ToupleGDD: A Fine-Designed Solution of Influence Maximization by Deep Reinforcement Learning. (arXiv:2210.07500v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07500
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;DRL&#26694;&#26550;ToupleGDD&#65292;&#29992;&#20110;&#35299;&#20915;&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#23558;&#19977;&#20010;&#32806;&#21512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#23884;&#20837;&#65292;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#23398;&#20064;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#23427;&#20204;&#38598;&#25104;&#36215;&#26469;&#65292;&#25429;&#33719;&#32593;&#32476;&#20013;&#33410;&#28857;&#21644;&#36793;&#32536;&#30340;&#26356;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ToupleGDD&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#21709;&#21147;&#26368;&#22823;&#21270;&#65288;IM&#65289;&#38382;&#39064;&#22987;&#32456;&#26159;&#36873;&#25321;&#20855;&#26377;&#22312;&#32593;&#32476;&#20013;&#26368;&#22823;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#23567;&#23376;&#38598;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32473;&#23450;&#31181;&#23376;&#38598;&#21512;&#35745;&#31639;&#24433;&#21709;&#21147;&#25193;&#25955;&#26159;#P-hard&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#21551;&#21457;&#24335;&#31639;&#27861;&#21644;&#36817;&#20284;&#31639;&#27861;&#65292;&#38754;&#20020;&#30528;&#29702;&#35770;&#20445;&#35777;&#65292;&#26102;&#38388;&#25928;&#29575;&#65292;&#27867;&#21270;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;&#21516;&#26102;&#65292;&#38543;&#30528;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#24050;&#32463;&#20851;&#27880;&#20110;&#21033;&#29992;DRL&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;DRL&#26694;&#26550;ToupleGDD&#65292;&#29992;&#20110;&#35299;&#20915;IM&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#23558;&#19977;&#20010;&#32806;&#21512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#32593;&#32476;&#23884;&#20837;&#65292;&#21452;&#37325;&#28145;&#24230;Q&#32593;&#32476;&#29992;&#20110;&#21442;&#25968;&#23398;&#20064;&#12290;&#19982;&#20043;&#21069;&#20165;&#20351;&#29992;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#25110;&#22810;&#20010;&#32593;&#32476;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#35299;&#20915;IM&#38382;&#39064;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;ToupleGDD&#26694;&#26550;&#21033;&#29992;&#20855;&#26377;&#29305;&#23450;&#20219;&#21153;&#30340;&#22810;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#23558;&#23427;&#20204;&#38598;&#25104;&#36215;&#26469;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#33719;&#32593;&#32476;&#20013;&#33410;&#28857;&#21644;&#36793;&#32536;&#30340;&#26356;&#20016;&#23500;&#21644;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#30495;&#23454;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ToupleGDD&#26694;&#26550;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aiming at selecting a small subset of nodes with maximum influence on networks, the Influence Maximization (IM) problem has been extensively studied. Since it is #P-hard to compute the influence spread given a seed set, the state-of-the-art methods, including heuristic and approximation algorithms, faced with great difficulties such as theoretical guarantee, time efficiency, generalization, etc. This makes it unable to adapt to large-scale networks and more complex applications. On the other side, with the latest achievements of Deep Reinforcement Learning (DRL) in artificial intelligence and other fields, lots of works have been focused on exploiting DRL to solve combinatorial optimization problems. Inspired by this, we propose a novel end-to-end DRL framework, ToupleGDD, to address the IM problem in this paper, which incorporates three coupled graph neural networks for network embedding and double deep Q-networks for parameters learning. Previous efforts to solve IM problem with DRL 
&lt;/p&gt;</description></item><item><title>MiniALBERT &#26159;&#19968;&#31181;&#27169;&#22411;&#33976;&#39311;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#31561;&#31574;&#30053;&#65292;&#23558;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36716;&#25442;&#25104;&#20026;&#32039;&#20945;&#36882;&#24402;&#23398;&#29983;&#27169;&#22411;&#12290;MiniALBERT &#22312;&#22522;&#20934; NLP &#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32039;&#20945;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.06425</link><description>&lt;p&gt;
&#36855;&#20320;ALBERT: &#22522;&#20110;&#21442;&#25968;&#39640;&#25928;&#36882;&#24402;&#21464;&#25442;&#30340;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06425
&lt;/p&gt;
&lt;p&gt;
MiniALBERT &#26159;&#19968;&#31181;&#27169;&#22411;&#33976;&#39311;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#31561;&#31574;&#30053;&#65292;&#23558;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36716;&#25442;&#25104;&#20026;&#32039;&#20945;&#36882;&#24402;&#23398;&#29983;&#27169;&#22411;&#12290;MiniALBERT &#22312;&#22522;&#20934; NLP &#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32039;&#20945;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LM) &#30001;&#20110;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#23613;&#31649;&#36825;&#19968;&#36745;&#29004;&#30340;&#25104;&#23601;&#65292;LM &#30340;&#21487;&#29992;&#24615;&#21463;&#38480;&#20110;&#35745;&#31639;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#20197;&#21450;&#23427;&#20204;&#26085;&#30410;&#22686;&#38271;&#30340;&#27169;&#22411;&#22823;&#23567;&#65307;&#36825;&#26159;&#34987;&#31216;&#20026;&#8220;&#36807;&#21442;&#25968;&#21270;&#8221;&#30340;&#38382;&#39064;&#12290;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#26088;&#22312;&#21019;&#24314;&#26377;&#25928;&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#20854;&#33192;&#32960;&#30340;&#23545;&#24212;&#29289;&#20960;&#20046;&#30456;&#21305;&#37197;&#65292;&#32780;&#20960;&#20046;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#20043;&#19968;&#26159;&#27169;&#22411;&#33976;&#39311;&#12290;&#32780;&#21478;&#19968;&#31181;&#24378;&#22823;&#20294;&#19981;&#24120;&#20351;&#29992;&#30340;&#25216;&#26415;&#26159;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20004;&#20010;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102; MiniALBERT&#65292;&#19968;&#31181;&#23558;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340; LM &#30340;&#30693;&#35782;&#36716;&#25442;&#20026;&#32039;&#20945;&#36882;&#24402;&#23398;&#29983;&#30340;&#25216;&#26415;&#65292;&#21516;&#26102;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#23618;&#21442;&#25968;&#20849;&#20139;&#21644;&#20854;&#20182;&#25216;&#26415;&#30340;&#25928;&#33021;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102; MiniALBERT &#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934; NLP &#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MiniALBERT &#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;&#32039;&#20945;&#22411; LM&#65292;&#24182;&#20445;&#25345;&#26356;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as `overparameterisation'. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the 
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31038;&#20132;&#25512;&#26029;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#36974;&#25377;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#27169;&#25311;&#20013;&#30340;&#31574;&#30053;&#36801;&#31227;&#21040;&#29616;&#23454;&#20013;&#30340;Turtlebot&#26426;&#22120;&#20154;&#19978;&#12290;</title><link>http://arxiv.org/abs/2210.00552</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#20316;&#20026;&#20256;&#24863;&#22120;&#30340;&#36974;&#25377;&#24863;&#30693;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Occlusion-Aware Crowd Navigation Using People as Sensors. (arXiv:2210.00552v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00552
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31038;&#20132;&#25512;&#26029;&#25216;&#26415;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25317;&#25380;&#29615;&#22659;&#19979;&#36974;&#25377;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#27169;&#25311;&#20013;&#30340;&#31574;&#30053;&#36801;&#31227;&#21040;&#29616;&#23454;&#20013;&#30340;Turtlebot&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25317;&#25380;&#31354;&#38388;&#20013;&#33258;&#20027;&#23548;&#33322;&#23545;&#20110;&#31227;&#21160;&#26426;&#22120;&#20154;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#29615;&#22659;&#21160;&#24577;&#19981;&#31283;&#23450;&#12289;&#20165;&#33021;&#35266;&#23519;&#21040;&#23616;&#37096;&#12290;&#30001;&#20110;&#26377;&#38480;&#30340;&#20256;&#24863;&#22120;&#35270;&#37326;&#21644;&#36974;&#25377;&#30340;&#20154;&#31867;&#20803;&#32032;&#65292;&#36974;&#25377;&#22312;&#36825;&#31181;&#24773;&#22659;&#20013;&#26222;&#36941;&#23384;&#22312;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#20114;&#21160;&#34892;&#20026;&#26469;&#20272;&#35745;&#28508;&#22312;&#30340;&#38556;&#30861;&#29289;&#65292;&#23613;&#31649;&#23384;&#22312;&#36974;&#25377;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#36825;&#31181;&#31038;&#20132;&#25512;&#26029;&#25216;&#26415;&#38598;&#25104;&#21040;&#35268;&#21010;&#27969;&#31243;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#26377;&#24847;&#20041;&#36974;&#25377;&#25512;&#26029;&#30340;&#34920;&#31034;&#12290;&#35813;&#24037;&#20316;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#38598;&#25104;&#21040;&#36974;&#25377;&#24863;&#30693;&#35268;&#21010;&#20013;&#12290;&#22312;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#30340;&#36974;&#25377;&#24863;&#30693;&#25919;&#31574;&#36890;&#36807;&#20272;&#35745;&#34987;&#36974;&#25377;&#30340;&#21306;&#22495;&#20869;&#30340;&#20803;&#32032;&#65292;&#23454;&#29616;&#20102;&#19982;&#23436;&#20840;&#21487;&#35266;&#23519;&#23548;&#33322;&#30456;&#24403;&#30340;&#30896;&#25758;&#36991;&#20813;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;Turtlebot&#19978;&#30340;&#25104;&#21151;&#31574;&#30053;&#36801;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous navigation in crowded spaces poses a challenge for mobile robots due to the highly dynamic, partially observable environment. Occlusions are highly prevalent in such settings due to a limited sensor field of view and obstructing human agents. Previous work has shown that observed interactive behaviors of human agents can be used to estimate potential obstacles despite occlusions. We propose integrating such social inference techniques into the planning pipeline. We use a variational autoencoder with a specially designed loss function to learn representations that are meaningful for occlusion inference. This work adopts a deep reinforcement learning approach to incorporate the learned representation for occlusion-aware planning. In simulation, our occlusion-aware policy achieves comparable collision avoidance performance to fully observable navigation by estimating agents in occluded spaces. We demonstrate successful policy transfer from simulation to the real-world Turtlebot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#21333;&#30446;RGB-D&#36755;&#20837;&#19979;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;PnP&#31639;&#27861;&#24674;&#22797;SE(3)&#23039;&#24577;&#12290;&#26426;&#22120;&#20154;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25235;&#21462;&#28857;&#25552;&#26696;&#30340;&#31934;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#24182;&#20855;&#26377;&#24456;&#24378;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.08752</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#21333;&#30446;RGB-D&#36755;&#20837;&#19979;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input. (arXiv:2209.08752v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#28857;&#30340;&#21333;&#30446;RGB-D&#36755;&#20837;&#19979;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#29983;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;PnP&#31639;&#27861;&#24674;&#22797;SE(3)&#23039;&#24577;&#12290;&#26426;&#22120;&#20154;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25235;&#21462;&#28857;&#25552;&#26696;&#30340;&#31934;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#24182;&#20855;&#26377;&#24456;&#24378;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24050;&#32463;&#22312;&#28857;&#20113;&#36755;&#20837;&#30340;&#20845;&#33258;&#30001;&#24230;&#25235;&#21462;&#28857;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#28857;&#38598;&#26080;&#24207;&#24615;&#32780;&#23548;&#33268;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20174;RGB-D&#36755;&#20837;&#20013;&#29983;&#25104;&#25235;&#21462;&#28857;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Keypoint-GraspNet&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#26816;&#27979;&#22841;&#25345;&#22120;&#20851;&#38190;&#28857;&#30340;&#25237;&#24433;&#65292;&#28982;&#21518;&#20351;&#29992;PnP&#31639;&#27861;&#24674;&#22797;SE(3)&#23039;&#24577;&#12290;&#22522;&#20110;&#19977;&#32500;&#24418;&#20307;&#21644;&#25235;&#21462;&#23478;&#26063;&#26500;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#26816;&#39564;&#25105;&#20204;&#30340;&#24819;&#27861;&#12290;&#22522;&#20110;&#24230;&#37327;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25235;&#21462;&#28857;&#25552;&#26696;&#30340;&#31934;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#26102;&#38388;&#25104;&#26412;&#26041;&#38754;&#20248;&#20110;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#26426;&#22120;&#20154;&#23454;&#39564;&#23637;&#31034;&#20102;&#39640;&#25104;&#21151;&#29575;&#65292;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Great success has been achieved in the 6-DoF grasp learning from the point cloud input, yet the computational cost due to the point set orderlessness remains a concern. Alternatively, we explore the grasp generation from the RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects the projection of the gripper keypoints in the image space and then recover the SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive shape and the grasp family is constructed to examine our idea. Metric-based evaluation reveals that our method outperforms the baselines in terms of the grasp proposal accuracy, diversity, and the time cost. Finally, robot experiments show high success rate, demonstrating the potential of the idea in the real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06589</link><description>&lt;p&gt;
&#22810;&#27169;&#22359;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28789;&#27963;&#34920;&#24449;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#24191;&#21040;&#26356;&#22823;&#30340;&#22270;&#21644;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#19982;&#25512;&#26029;&#30340;&#24378;&#22823;&#27169;&#22411;&#65292;&#20294;&#23545;&#20110;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#20197;&#21450;&#25512;&#24191;&#21040;&#20174;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#30340;&#22522;&#26412;&#38480;&#21046;&#30340;&#20102;&#35299;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#20351;&#29992;&#38543;&#26426;&#22270;&#29983;&#25104;&#22120;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22270;&#30340;&#22823;&#23567;&#21644;&#32467;&#26500;&#23646;&#24615;&#22914;&#20309;&#24433;&#21709;GNN&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#22810;&#27169;&#22359;GNN&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#24191;&#21333;&#20010;&#35268;&#33539;&#38750;&#32447;&#24615;&#21464;&#25442;&#26469;&#36866;&#24212;&#26032;&#22270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#27169;&#22359;GNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#22343;&#26174;&#33879;&#25552;&#39640;&#20102;GNN&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#20960;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Meta Pattern Concern Score&#8221;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#25277;&#35937;&#34920;&#24449;&#21644;&#21487;&#35843;&#33410;&#30340;&#38408;&#20540;&#65292;&#23558;&#20154;&#31867;&#20215;&#20540;&#35266;&#24341;&#20837;&#21040;&#22810;&#20998;&#31867;&#22120;&#20013;&#65292;&#21487;&#20197;&#29992;&#20110;&#26681;&#25454;&#20154;&#31867;&#20215;&#20540;&#35266;&#24688;&#24403;&#22320;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#36739;&#22312;&#20855;&#26377;&#19981;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#19979;&#20351;&#29992;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2209.06408</link><description>&lt;p&gt;
Meta Pattern Concern Score&#65306;&#19968;&#31181;&#22522;&#20110;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22810;&#20998;&#31867;&#22120;&#35780;&#20272;&#26032;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers. (arXiv:2209.06408v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Meta Pattern Concern Score&#8221;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#25277;&#35937;&#34920;&#24449;&#21644;&#21487;&#35843;&#33410;&#30340;&#38408;&#20540;&#65292;&#23558;&#20154;&#31867;&#20215;&#20540;&#35266;&#24341;&#20837;&#21040;&#22810;&#20998;&#31867;&#22120;&#20013;&#65292;&#21487;&#20197;&#29992;&#20110;&#26681;&#25454;&#20154;&#31867;&#20215;&#20540;&#35266;&#24688;&#24403;&#22320;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#29616;&#23454;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#36739;&#22312;&#20855;&#26377;&#19981;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#19979;&#20351;&#29992;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#29616;&#23454;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#22914;&#20309;&#26681;&#25454;&#29305;&#23450;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#24688;&#24403;&#22320;&#35780;&#20272;&#40657;&#30418;&#27169;&#22411;&#22312;&#31038;&#21306;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#36825;&#20123;&#20154;&#31867;&#20215;&#20540;&#21253;&#25324;&#23545;&#19981;&#21516;&#20005;&#37325;&#31243;&#24230;&#30340;&#38169;&#35823;&#26696;&#20363;&#36827;&#34892;&#24809;&#32602;&#65292;&#24182;&#22312;&#24635;&#20307;&#24615;&#33021;&#20943;&#23569;&#29305;&#23450;&#21361;&#38505;&#26696;&#20363;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Meta Pattern Concern Score&#8221;&#30340;&#26032;&#22411;&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#22522;&#20110;&#27010;&#29575;&#39044;&#27979;&#30340;&#25277;&#35937;&#34920;&#24449;&#21644;&#21487;&#35843;&#33410;&#30340;&#38408;&#20540;&#65292;&#23558;&#20154;&#31867;&#20215;&#20540;&#35266;&#24341;&#20837;&#21040;&#22810;&#20998;&#31867;&#22120;&#20013;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#20174;&#20004;&#31181;&#24120;&#35265;&#25351;&#26631;&#8212;&#8212;&#28151;&#28102;&#30697;&#38453;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22833;&#20540;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#20013;&#23398;&#20064;&#65292;&#20351;&#25105;&#20204;&#30340;&#25351;&#26631;&#22312;&#36890;&#29992;&#20219;&#21153;&#19979;&#21516;&#26679;&#26377;&#25928;&#65292;&#20132;&#21449;&#29109;&#25439;&#22833;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#25104;&#20026;&#25105;&#20204;&#25351;&#26631;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#25351;&#26631;&#36824;&#21487;&#20197;&#29992;&#20110;&#27604;&#36739;&#22312;&#20855;&#26377;&#19981;&#21516;&#20154;&#31867;&#20215;&#20540;&#35266;&#19979;&#20351;&#29992;&#19981;&#21516;&#20998;&#31867;&#22120;&#30340;&#21516;&#19968;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#20063;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
While advanced classifiers have been increasingly used in real-world safety-critical applications, how to properly evaluate the black-box models given specific human values remains a concern in the community. Such human values include punishing error cases of different severity in varying degrees and making compromises in general performance to reduce specific dangerous cases. In this paper, we propose a novel evaluation measure named Meta Pattern Concern Score based on the abstract representation of probabilistic prediction and the adjustable threshold for the concession in prediction confidence, to introduce the human values into multi-classifiers. Technically, we learn from the advantages and disadvantages of two kinds of common metrics, namely the confusion matrix-based evaluation measures and the loss values, so that our measure is effective as them even under general tasks, and the cross entropy loss becomes a special case of our measure in the limit. Besides, our measure can als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#20174;&#26679;&#26412;&#20013;&#23398;&#20064;&#21333;&#32431;&#24418;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#21482;&#35201;&#20449;&#22122;&#27604;&#36739;&#39640;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26080;&#22122;&#22768;&#24773;&#20917;&#20855;&#26377;&#30456;&#21516;&#30340;&#38454;&#12290;</title><link>http://arxiv.org/abs/2209.05953</link><description>&lt;p&gt;
&#23398;&#20064;&#39640;&#32500;&#21333;&#32431;&#24418;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Sample Complexity Bounds for Learning High-dimensional Simplices in Noisy Regimes. (arXiv:2209.05953v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#20174;&#26679;&#26412;&#20013;&#23398;&#20064;&#21333;&#32431;&#24418;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#21482;&#35201;&#20449;&#22122;&#27604;&#36739;&#39640;&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26080;&#22122;&#22768;&#24773;&#20917;&#20855;&#26377;&#30456;&#21516;&#30340;&#38454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21547;&#26377;&#22122;&#22768;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#21333;&#32431;&#24418;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20551;&#35774;&#32473;&#23450;&#19968;&#20010;&#22823;&#23567;&#20026;$n$&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;$\mathbb{R}^K$&#20013;&#30340;&#26410;&#30693;&#21333;&#32431;&#24418;&#19978;&#22343;&#21248;&#20998;&#24067;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#25277;&#26679;&#30340;&#26679;&#26412;&#65292;&#20551;&#35774;&#36825;&#20123;&#26679;&#26412;&#34987;&#19968;&#20010;&#20219;&#24847;&#24133;&#24230;&#30340;&#22810;&#20803;&#21152;&#24615;&#39640;&#26031;&#22122;&#22768;&#25152;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#31639;&#27861;&#65292;&#20197;&#39640;&#27010;&#29575;&#36755;&#20986;&#19968;&#20010;&#19982;&#30495;&#23454;&#21333;&#32431;&#24418;&#30340;$\ell_2$&#36317;&#31163;&#26368;&#22823;&#20026;$\varepsilon$&#30340;&#21333;&#32431;&#24418;&#65288;&#23545;&#20110;&#20219;&#24847;$\varepsilon&gt;0$&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30028;&#38480;&#65292;&#38656;&#35201;&#26377; $n\ge\left(K^2/\varepsilon^2\right)e^{\Omega\left(K/\mathrm{SNR}^2\right)}$ &#20010;&#26679;&#26412;&#65292;&#20854;&#20013; $\mathrm{SNR}$ &#20195;&#34920;&#20449;&#22122;&#27604;&#12290;&#36825;&#20010;&#32467;&#26524;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#21482;&#35201; $\mathrm{SNR}\ge\Omega\left(K^{1/2}\right)$&#65292;&#22312;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26080;&#22122;&#22768;&#24773;&#20917;&#20855;&#26377;&#30456;&#21516;&#30340;&#38454;&#12290;&#26412;&#25991;&#30340;&#35777;&#26126;&#26159;&#21508;&#31181;&#24037;&#20855;&#30340;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we find a sample complexity bound for learning a simplex from noisy samples. Assume a dataset of size $n$ is given which includes i.i.d. samples drawn from a uniform distribution over an unknown simplex in $\mathbb{R}^K$, where samples are assumed to be corrupted by a multi-variate additive Gaussian noise of an arbitrary magnitude. We prove the existence of an algorithm that with high probability outputs a simplex having a $\ell_2$ distance of at most $\varepsilon$ from the true simplex (for any $\varepsilon&gt;0$). Also, we theoretically show that in order to achieve this bound, it is sufficient to have $n\ge\left(K^2/\varepsilon^2\right)e^{\Omega\left(K/\mathrm{SNR}^2\right)}$ samples, where $\mathrm{SNR}$ stands for the signal-to-noise ratio. This result solves an important open problem and shows as long as $\mathrm{SNR}\ge\Omega\left(K^{1/2}\right)$, the sample complexity of the noisy regime has the same order to that of the noiseless case. Our proofs are a combination 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#25913;&#36827;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#32452;&#32452;&#21512;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2208.13065</link><description>&lt;p&gt;
&#25913;&#21892;&#36816;&#33829;&#32463;&#27982;&#23398;&#65306;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#26469;&#39044;&#27979;&#26426;&#32452;&#32452;&#21512;&#30340;&#25805;&#20316;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment. (arXiv:2208.13065v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21452;&#23618; MIP &#30340;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#26469;&#25913;&#36827;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21453;&#39304;&#24490;&#29615;&#36845;&#20195;&#22320;&#25913;&#36827;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#23545;&#26426;&#32452;&#32452;&#21512;&#30340;&#26368;&#20339;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#22312;&#24320;&#29615;&#39044;&#27979;&#20248;&#21270;&#36807;&#31243;&#20013;&#36827;&#34892;&#30005;&#21147;&#31995;&#32479;&#30340;&#32463;&#27982;&#36816;&#34892;&#65306;&#39318;&#20808;&#39044;&#27979;&#21487;&#20877;&#29983;&#33021;&#28304;(RES)&#30340;&#21487;&#29992;&#24615;&#21644;&#31995;&#32479;&#20648;&#22791;&#38656;&#27714;&#65307;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#65292;&#31995;&#32479;&#25805;&#20316;&#21592;&#35299;&#20915;&#35832;&#22914;&#26426;&#32452;&#32452;&#21512;(UC)&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#20197;&#30830;&#23450;&#30456;&#24212;&#30340;&#32463;&#27982;&#36816;&#34892;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24320;&#29615;&#36807;&#31243;&#21487;&#33021;&#20250;&#23454;&#36136;&#24615;&#22320;&#25439;&#23475;&#25805;&#20316;&#32463;&#27982;&#24615;&#65292;&#22240;&#20026;&#23427;&#30340;&#39044;&#27979;&#22120;&#30446;&#20809;&#30701;&#27973;&#22320;&#23547;&#27714;&#25913;&#21892;&#21363;&#26102;&#30340;&#32479;&#35745;&#39044;&#27979;&#35823;&#24046;&#65292;&#32780;&#19981;&#26159;&#26368;&#32456;&#30340;&#25805;&#20316;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#29615;&#39044;&#27979;&#20248;&#21270;&#26694;&#26550;&#65292;&#25552;&#20379;&#19968;&#31181;&#39044;&#27979;&#26426;&#32452;&#32452;&#21512;&#20197;&#25913;&#21892;&#25805;&#20316;&#32463;&#27982;&#24615;&#30340;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#21452;&#23618;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#38024;&#23545;&#26368;&#20339;&#31995;&#32479;&#25805;&#20316;&#35757;&#32451;&#25104;&#26412;&#23548;&#21521;&#30340;&#39044;&#27979;&#22120;&#12290;&#19978;&#23618;&#22522;&#20110;&#20854;&#24341;&#36215;&#30340;&#25805;&#20316;&#25104;&#26412;&#26469;&#35757;&#32451; RES &#21644;&#20648;&#22791;&#39044;&#27979;&#22120;&#65307;&#19979;&#23618;&#21017;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340; RES &#21644;&#20648;&#22791;&#30340;&#24773;&#20917;&#19979;&#65292;&#20381;&#25454;&#26368;&#20339;&#25805;&#20316;&#21407;&#21017;&#27714;&#35299; UC&#12290;&#36825;&#20004;&#20010;&#23618;&#32423;&#36890;&#36807;&#21453;&#39304;&#29615;&#36335;&#36827;&#34892;&#20132;&#20114;&#24615;&#20114;&#21160;&#65292;&#30452;&#21040;&#25910;&#25947;&#20026;&#27490;&#12290;&#22312;&#20462;&#25913;&#21518;&#30340;IEEE 24-bus&#31995;&#32479;&#19978;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340; UC &#22522;&#20934;&#32447;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally, system operators conduct the economic operation of power systems in an open-loop predict-then-optimize process: the renewable energy source (RES) availability and system reserve requirements are first predicted; given the predictions, system operators solve optimization models such as unit commitment (UC) to determine the economical operation plans accordingly. However, such an open-loop process could essentially compromise the operation economics because its predictors myopically seek to improve the immediate statistical prediction errors instead of the ultimate operation cost. To this end, this paper presents a closed-loop predict-and-optimize framework, offering a prescriptive UC to improve the operation economics. First, a bilevel mixed-integer programming model is leveraged to train cost-oriented predictors tailored for optimal system operations: the upper level trains the RES and reserve predictors based on their induced operation cost; the lower level, with given pred
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.09894</link><description>&lt;p&gt;
&#25308;&#21344;&#24237;&#20154;&#20063;&#33021;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#24515;&#21270;&#21098;&#35009;&#30340;&#34928;&#33853;
&lt;/p&gt;
&lt;p&gt;
Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20013;&#24515;&#21270;&#21098;&#35009;&#22312;&#38754;&#23545;&#19981;&#21516;&#24694;&#24847;&#20195;&#29702;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC) &#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MRPC &#26694;&#26550;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26377;&#25928;&#22320;&#20013;&#21644;&#19987;&#38376;&#35774;&#35745;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064; (FL) &#26694;&#26550;&#30001;&#20110;&#22312;&#24191;&#27867;&#30340;&#21327;&#20316;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20063;&#24341;&#36215;&#20102;&#26576;&#20123;&#23433;&#20840;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#39118;&#38505;&#26159;&#29305;&#21035;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#24694;&#24847;&#23458;&#25143;&#21442;&#19982;&#23398;&#20064;&#36807;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;FL &#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#26159;&#28040;&#38500; Byzantine attacks &#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#30830;&#20445;&#26368;&#32456;&#27169;&#22411;&#26159;&#21487;&#20449;&#30340;&#12290;&#24050;&#32463;&#35266;&#23519;&#21040;&#65292;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;/&#26356;&#26032;&#20043;&#38388;&#30340;&#26041;&#24046;&#36234;&#22823;&#65292;&#38544;&#34255; Byzantine attacks &#30340;&#31354;&#38388;&#23601;&#36234;&#22823;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#26041;&#24046;&#65292;&#21487;&#20197;&#21066;&#24369;&#24050;&#30693; Byzantine attacks &#30340;&#21147;&#37327;&#12290;&#20013;&#24515;&#21270;&#21098;&#35009; (CC) &#26694;&#26550;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#19978;&#19968;&#27425;&#30340;&#21160;&#37327;&#39033;&#38500;&#20102;&#20943;&#23569;&#26041;&#24046;&#22806;&#65292;&#36824;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#21442;&#32771;&#28857;&#26356;&#22909;&#22320;&#28040;&#38500; Byzantine attacks&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#30340;&#24694;&#24847;&#20195;&#29702;&#26377;&#19981;&#21516;&#30446;&#26631;&#26102; CC &#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21098;&#35009;&#31639;&#27861;&#31216;&#20026;&#22810;&#24341;&#29992;&#28857;&#21098;&#35009; (MRPC)&#65292;&#20197;&#20811;&#26381;&#36825;&#31181;&#33030;&#24369;&#24615;&#12290;MRPC &#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#20010;&#21442;&#32771;&#28857;&#26469;&#28040;&#38500;&#19987;&#38376;&#35774;&#35745;&#20197;&#32469;&#36807; CC &#26041;&#27861;&#30340; Byzantine attacks&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340; Byzantine attacks &#19979;&#65292;MRPC &#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340; FL &#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks, and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients' models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that, the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this wor
&lt;/p&gt;</description></item><item><title>GSim&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#26500;&#22270;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#65292;&#33021;&#22815;&#25429;&#25417;&#24322;&#26500;&#22270;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#24050;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2208.06144</link><description>&lt;p&gt;
GSim &#65306;&#38754;&#21521;&#24322;&#26500;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20851;&#32852;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
GSim: A Graph Neural Network based Relevance Measure for Heterogeneous Graphs. (arXiv:2208.06144v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06144
&lt;/p&gt;
&lt;p&gt;
GSim&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#26500;&#22270;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#65292;&#33021;&#22815;&#25429;&#25417;&#24322;&#26500;&#22270;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#24050;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#21253;&#21547;&#20855;&#26377;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#32536;&#12290;&#20851;&#32852;&#24230;&#37327;&#26159;&#20998;&#26512;&#24322;&#26500;&#22270;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#30446;&#30340;&#26159;&#35745;&#31639;&#19981;&#21516;&#31867;&#22411;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#25512;&#33616;&#21644;&#31038;&#21306;&#26816;&#27979;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#19987;&#27880;&#20110;&#21516;&#36136;&#32593;&#32476;&#65292;&#20294;&#20026;&#24322;&#26500;&#22270;&#24320;&#21457;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#12290;&#23450;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#36335;&#24452;&#38656;&#35201;&#22823;&#37327;&#39046;&#22495;&#30693;&#35782;&#65292;&#36825;&#22312;&#22522;&#20110;&#26550;&#26500;&#20016;&#23500;&#30340;&#24322;&#26500;&#22270;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#19978;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#22270;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#20294;&#23578;&#26410;&#29992;&#20110;&#34913;&#37327;&#20851;&#32852;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GSim&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24322;&#26500;&#22270;&#20851;&#32852;&#24230;&#37327;&#26041;&#27861;&#12290;GSim&#33021;&#22815;&#25429;&#25417;&#24322;&#26500;&#22270;&#30340;&#38544;&#21547;&#32467;&#26500;&#65292;&#19981;&#38656;&#35201;&#39044;&#23450;&#20041;&#30340;&#20803;&#36335;&#24452;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;GSim&#26174;&#33879;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graphs, which contain nodes and edges of multiple types, are prevalent in various domains, including bibliographic networks, social media, and knowledge graphs. As a fundamental task in analyzing heterogeneous graphs, relevance measure aims to calculate the relevance between two objects of different types, which has been used in many applications such as web search, recommendation, and community detection. Most of existing relevance measures focus on homogeneous networks where objects are of the same type, and a few measures are developed for heterogeneous graphs, but they often need the pre-defined meta-path. Defining meaningful meta-paths requires much domain knowledge, which largely limits their applications, especially on schema-rich heterogeneous graphs like knowledge graphs. Recently, the Graph Neural Network (GNN) has been widely applied in many graph mining tasks, but it has not been applied for measuring relevance yet. To address the aforementioned problems, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2208.02814</link><description>&lt;p&gt;
&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#65292;&#31034;&#20363;&#35777;&#26126;&#20854;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20855;&#26377;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31526;&#21512;&#24615;&#39044;&#27979;&#25512;&#24191;&#33267;&#25511;&#21046;&#20219;&#20309;&#21333;&#35843;&#25439;&#22833;&#20989;&#25968;&#30340;&#26399;&#26395;&#20540;&#12290;&#35813;&#31639;&#27861;&#23558;&#20998;&#35010;&#31526;&#21512;&#24615;&#39044;&#27979;&#21450;&#20854;&#35206;&#30422;&#20445;&#35777;&#36827;&#34892;&#20102;&#27867;&#21270;&#12290;&#31867;&#20284;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#31526;&#21512;&#20445;&#24207;&#30340;&#39118;&#38505;&#25511;&#21046;&#26041;&#27861;&#22312;$\mathcal{O}(1/n)$&#22240;&#23376;&#20869;&#20445;&#25345;&#32039;&#23494;&#24615;&#12290;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#31034;&#20363;&#35777;&#26126;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25511;&#21046;&#35823;&#25253;&#29575;&#12289;&#22270;&#24418;&#36317;&#31163;&#21644;&#20196;&#29260;&#32423;F1&#24471;&#20998;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Wasserstein WL&#23376;&#26641;(WWLS)&#36317;&#31163;&#30340;&#26032;&#22411;&#22270;&#36317;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;WL&#23376;&#26641;&#20316;&#20026;&#33410;&#28857;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#29992;&#33410;&#28857;&#30340;WL&#23376;&#26641;&#20043;&#38388;&#30340;L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;(L1-TED)&#23450;&#20041;&#33410;&#28857;&#24230;&#37327;&#65292;&#35299;&#20915;&#20102;WL&#27979;&#35797;&#26080;&#27861;&#25429;&#25417;&#36731;&#24494;&#32467;&#26500;&#24046;&#24322;&#30340;&#38382;&#39064;</title><link>http://arxiv.org/abs/2207.04216</link><description>&lt;p&gt;
&#22522;&#20110; Weisfeiler-Lehman &#23376;&#26641; L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;&#30340; Wasserstein &#22270;&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Graph Distance Based on $L_1$-Approximated Tree Edit Distance between Weisfeiler-Lehman Subtrees. (arXiv:2207.04216v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Wasserstein WL&#23376;&#26641;(WWLS)&#36317;&#31163;&#30340;&#26032;&#22411;&#22270;&#36317;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;WL&#23376;&#26641;&#20316;&#20026;&#33410;&#28857;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20351;&#29992;&#33410;&#28857;&#30340;WL&#23376;&#26641;&#20043;&#38388;&#30340;L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;(L1-TED)&#23450;&#20041;&#33410;&#28857;&#24230;&#37327;&#65292;&#35299;&#20915;&#20102;WL&#27979;&#35797;&#26080;&#27861;&#25429;&#25417;&#36731;&#24494;&#32467;&#26500;&#24046;&#24322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Lehman (WL)&#27979;&#35797;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#22270;&#20869;&#26680;&#12289;&#22270;&#24230;&#37327;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23427;&#20165;&#20851;&#27880;&#22270;&#30340;&#19968;&#33268;&#24615;&#65292;&#26080;&#27861;&#26816;&#27979;&#36731;&#24494;&#30340;&#32467;&#26500;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#25429;&#25417;&#32467;&#26500;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20063;&#38480;&#21046;&#20102;&#20381;&#36182;WL&#27979;&#35797;&#30340;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Wasserstein WL&#23376;&#26641;(WWLS)&#36317;&#31163;&#30340;&#26032;&#22411;&#22270;&#36317;&#31163;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;WL&#23376;&#26641;&#20316;&#20026;&#33410;&#28857;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33410;&#28857;&#30340;WL&#23376;&#26641;&#20043;&#38388;&#30340;L1-&#36817;&#20284;&#26641;&#32534;&#36753;&#36317;&#31163;(L1-TED)&#23450;&#20041;&#33410;&#28857;&#24230;&#37327;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;Wasserstein&#36317;&#31163;&#21644;L1-TED&#26469;&#23450;&#20041;WWLS&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler-Lehman (WL) test is a widely used algorithm in graph machine learning, including graph kernels, graph metrics, and graph neural networks. However, it focuses only on the consistency of the graph, which means that it is unable to detect slight structural differences. Consequently, this limits its ability to capture structural information, which also limits the performance of existing models that rely on the WL test. This limitation is particularly severe for traditional metrics defined by the WL test, which cannot precisely capture slight structural differences. In this paper, we propose a novel graph metric called the Wasserstein WL Subtree (WWLS) distance to address this problem. Our approach leverages the WL subtree as structural information for node neighborhoods and defines node metrics using the $L_1$-approximated tree edit distance ($L_1$-TED) between WL subtrees of nodes. Subsequently, we combine the Wasserstein distance and the $L_1$-TED to define the WWLS distan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.02295</link><description>&lt;p&gt;
&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs. (arXiv:2207.02295v4 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;NVIDIA&#32593;&#21345;&#20013;&#23454;&#29616;&#20102;&#24378;&#21270;&#23398;&#20064;&#25968;&#25454;&#20013;&#24515;&#25317;&#22622;&#25511;&#21046;&#65292;&#36890;&#36807;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23454;&#29616;&#20102;&#23454;&#26102;&#25512;&#29702;&#65292;&#24182;&#25104;&#21151;&#25913;&#21892;&#20102;&#32593;&#32476;&#25317;&#22622;&#19979;&#30340;&#23614;&#37096;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36890;&#20449;&#21327;&#35758;&#30340;&#21457;&#23637;&#65292;&#25968;&#25454;&#20013;&#24515;&#32593;&#32476;&#30340;&#21033;&#29992;&#29575;&#36234;&#26469;&#36234;&#39640;&#65292;&#25317;&#22622;&#26356;&#20026;&#39057;&#32321;&#65292;&#23548;&#33268;&#24310;&#36831;&#21644;&#20002;&#21253;&#29575;&#22686;&#21152;&#12290;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#35774;&#35745;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#21464;&#24471;&#26497;&#20854;&#22256;&#38590;&#65292;&#38656;&#35201;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#26367;&#20195;&#20154;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#32593;&#32476;&#35774;&#22791;&#35745;&#31639;&#33021;&#21147;&#26377;&#38480;&#65292;&#30446;&#21069;&#19981;&#21487;&#33021;&#22312;&#32593;&#32476;&#35774;&#22791;&#19978;&#37096;&#32626;AI&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22522;&#20110;&#26368;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;[arXiv:2207.02295]&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#20915;&#31574;&#26641;&#30340;&#35745;&#31639;&#36731;&#37327;&#32423;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;RL-CC&#30340;&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#36716;&#21270;&#20026;&#20915;&#31574;&#26641;&#65292;&#23558;&#20854;&#25512;&#29702;&#26102;&#38388;&#38477;&#20302;&#20102;500&#20493;&#65292;&#20351;&#20854;&#22312;&#956;&#31186;&#32423;&#20915;&#31574;&#26102;&#38388;&#35201;&#27714;&#20869;&#23454;&#29616;&#23454;&#26102;&#25512;&#29702;&#65292;&#19988;&#23545;&#36136;&#37327;&#24433;&#21709;&#19981;&#22823;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#26102;&#38598;&#32676;&#20013;&#37096;&#32626;&#20102;&#36716;&#25442;&#21518;&#30340;&#31574;&#30053;&#65292;&#24182;&#19982;&#29616;&#20195;&#25968;&#25454;&#20013;&#24515;&#37096;&#32626;&#30340;&#27969;&#34892;&#25317;&#22622;&#25511;&#21046;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#22312;&#31867;&#20284;&#30340;&#27969;&#37327;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#23614;&#37096;&#24310;&#36831;&#29575;&#25552;&#39640;&#20102;x%&#65292;&#23558;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#38477;&#20302;&#20102;y%&#12290;
&lt;/p&gt;
&lt;p&gt;
As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [arXiv:2207.02295]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the $\mu$-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#22411;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#35889;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20248;&#21270;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#36924;&#36817;&#36825;&#20123;&#24230;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39069;&#22806;&#23884;&#22871;&#36716;&#25442;&#30340;&#39118;&#38505;&#25935;&#24863;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.14666</link><description>&lt;p&gt;
&#26465;&#20214;&#21487;&#24341;&#20986;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;&#39118;&#38505;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Conditionally Elicitable Dynamic Risk Measures for Deep Reinforcement Learning. (arXiv:2206.14666v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21487;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#22411;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20351;&#29992;&#21160;&#24577;&#35889;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#20248;&#21270;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#36924;&#36817;&#36825;&#20123;&#24230;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#39069;&#22806;&#23884;&#22871;&#36716;&#25442;&#30340;&#39118;&#38505;&#25935;&#24863;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#22411;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#20248;&#21270;&#26102;&#38388;&#19968;&#33268;&#30340;&#21160;&#24577;&#35889;&#39118;&#38505;&#24230;&#37327;&#26469;&#23454;&#29616;&#12290;&#22522;&#20110;&#26465;&#20214;&#21487;&#24341;&#20986;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#65288;&#20005;&#26684;&#19968;&#33268;&#30340;&#65289;&#35780;&#20998;&#20989;&#25968;&#65292;&#29992;&#20316;&#20272;&#35745;&#36807;&#31243;&#20013;&#30340;&#22788;&#32602;&#39033;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;i&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#19968;&#31867;&#21160;&#24577;&#35889;&#39118;&#38505;&#24230;&#37327;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#65288;ii&#65289;&#35777;&#26126;&#20102;&#36825;&#20123;&#21160;&#24577;&#35889;&#39118;&#38505;&#24230;&#37327;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20219;&#24847;&#31934;&#24230;&#30340;&#36924;&#36817;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#39118;&#38505;&#25935;&#24863;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#23436;&#25972;&#30340;&#21095;&#38598;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#23884;&#22871;&#36716;&#25442;&#12290;&#25105;&#20204;&#23558;&#27010;&#24565;&#19978;&#25913;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19982;&#23884;&#22871;&#27169;&#25311;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35828;&#26126;&#20102;&#20854;&#22312;&#32479;&#35745;&#22871;&#21033;&#21644;&#32452;&#21512;&#20998;&#37197;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework to solve risk-sensitive reinforcement learning (RL) problems where the agent optimises time-consistent dynamic spectral risk measures. Based on the notion of conditional elicitability, our methodology constructs (strictly consistent) scoring functions that are used as penalizers in the estimation procedure. Our contribution is threefold: we (i) devise an efficient approach to estimate a class of dynamic spectral risk measures with deep neural networks, (ii) prove that these dynamic spectral risk measures may be approximated to any arbitrary accuracy using deep neural networks, and (iii) develop a risk-sensitive actor-critic algorithm that uses full episodes and does not require any additional nested transitions. We compare our conceptually improved reinforcement learning algorithm with the nested simulation approach and illustrate its performance in two settings: statistical arbitrage and portfolio allocation on both simulated and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RevSilo&#65292;&#19968;&#20010;&#23436;&#20840;&#21487;&#36870;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#32531;&#35299;&#20102;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.14098</link><description>&lt;p&gt;
RevBiFPN&#65306;&#23436;&#20840;&#21487;&#36870;&#30340;&#21452;&#21521;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network. (arXiv:2206.14098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevSilo&#65292;&#19968;&#20010;&#23436;&#20840;&#21487;&#36870;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#65292;&#23427;&#32531;&#35299;&#20102;&#31070;&#32463;&#32593;&#32476;&#35268;&#27169;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevSilo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#36870;&#30340;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#27169;&#22359;&#12290;&#19982;&#20854;&#20182;&#21487;&#36870;&#26041;&#27861;&#19968;&#26679;&#65292;RevSilo&#36890;&#36807;&#37325;&#26032;&#35745;&#31639;&#26469;&#28040;&#38500;&#23384;&#20648;&#38544;&#34255;&#28608;&#27963;&#25152;&#38656;&#30340;&#20869;&#23384;&#65307;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#65292;&#22240;&#27492;&#19981;&#33021;&#24212;&#29992;&#20110;&#22823;&#37096;&#20998;&#32593;&#32476;&#12290;&#21452;&#21521;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20419;&#36827;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#24050;&#25104;&#20026;&#38024;&#23545;&#31354;&#38388;&#25935;&#24863;&#20219;&#21153;&#30340;&#32593;&#32476;&#30340;&#35774;&#35745;&#21407;&#21017;&#12290;&#36825;&#20123;&#32593;&#32476;&#22312;&#20351;&#29992;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#26102;&#65292;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#38656;&#35201;&#20445;&#23384;&#22823;&#22411;&#30340;&#22810;&#20998;&#36776;&#29575;&#28608;&#27963;&#25152;&#38656;&#30340;&#22823;&#37327;&#21152;&#36895;&#22120;&#20869;&#23384;&#12290;&#36825;&#20123;&#20869;&#23384;&#38656;&#27714;&#26412;&#36136;&#19978;&#38480;&#21046;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#35268;&#27169;&#65292;&#38480;&#21046;&#20102;&#30001;&#35268;&#27169;&#24102;&#26469;&#30340;&#25913;&#36827;&#12290;&#36328;&#20998;&#36776;&#29575;&#23610;&#24230;&#36816;&#20316;&#30340;RevSilo&#32531;&#35299;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces RevSilo, the first reversible bidirectional multi-scale feature fusion module. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. However, existing reversible methods do not apply to multi-scale feature fusion and are, therefore, not applicable to a large class of networks. Bidirectional multi-scale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks, e.g., HRNet (Sun et al., 2019a) and EfficientDet (Tan et al., 2020). These networks achieve state-of-the-art results across various computer vision tasks when paired with high-resolution inputs. However, training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements inherently cap the size of neural networks, limiting improvements that come from scale. Operating across resolution scales, RevSilo alleviates th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;ILQL&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#37319;&#29992;&#20215;&#20540;&#20445;&#23432;&#24615;&#21644;&#38544;&#24335;&#25968;&#25454;&#38598;&#25903;&#25345;&#32422;&#26463;&#30340;&#32452;&#21512;&#65292;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26368;&#22823;&#21270;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#35821;&#35328;&#36755;&#20986;&#65292;&#20174;&#32780;&#35299;&#20915;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#29992;&#25143;&#25351;&#23450;&#20219;&#21153;&#26102;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.11871</link><description>&lt;p&gt;
&#22522;&#20110;&#38544;&#24335;&#35821;&#35328;Q&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;ILQL&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#37319;&#29992;&#20215;&#20540;&#20445;&#23432;&#24615;&#21644;&#38544;&#24335;&#25968;&#25454;&#38598;&#25903;&#25345;&#32422;&#26463;&#30340;&#32452;&#21512;&#65292;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26368;&#22823;&#21270;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#35821;&#35328;&#36755;&#20986;&#65292;&#20174;&#32780;&#35299;&#20915;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#29992;&#25143;&#25351;&#23450;&#20219;&#21153;&#26102;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#25552;&#28860;&#20986;&#24191;&#27867;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#29992;&#25143;&#25351;&#23450;&#30340;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#25110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#38544;&#24335;&#35821;&#35328;Q&#23398;&#20064;&#65288;ILQL&#65289;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;RL&#31639;&#27861;&#30340;&#28789;&#27963;&#25928;&#29992;&#26368;&#22823;&#21270;&#26694;&#26550;&#19982;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#20854;&#31616;&#21333;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#20215;&#20540;&#20989;&#25968;&#26102;&#37319;&#29992;&#20215;&#20540;&#20445;&#23432;&#24615;&#21644;&#38544;&#24335;&#25968;&#25454;&#38598;&#25903;&#25345;&#32422;&#26463;&#30340;&#32452;&#21512;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#25351;&#23450;&#30340;&#25928;&#29992;&#20989;&#25968;&#12290;&#38500;&#20102;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;ILQL&#65292;&#25105;&#20204;&#36824;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#33021;&#22815;&#26377;&#29992;&#30340;&#22330;&#26223;&#30340;&#35814;&#32454;&#32463;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language gen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35748;&#35777;&#29702;&#35770;&#65292;&#20351;&#24471;&#20272;&#35745;&#20540;&#21644;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#36798;&#21040;&#20102;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#20041;&#20851;&#38190;&#28857;&#30340;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411; C-3PO&#65292;&#24182;&#21152;&#20837;&#20102;&#21487;&#35266;&#23519;&#27491;&#30830;&#24615;&#21644;&#38750;&#36864;&#21270;&#24615;&#20004;&#20010;&#35777;&#20070;&#65292;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.11215</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#30340;&#19977;&#32500;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#65306;&#22522;&#30784;&#12289;&#23398;&#20064;&#27169;&#22411;&#21644;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training. (arXiv:2206.11215v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#35748;&#35777;&#29702;&#35770;&#65292;&#20351;&#24471;&#20272;&#35745;&#20540;&#21644;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#36798;&#21040;&#20102;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#20041;&#20851;&#38190;&#28857;&#30340;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411; C-3PO&#65292;&#24182;&#21152;&#20837;&#20102;&#21487;&#35266;&#23519;&#27491;&#30830;&#24615;&#21644;&#38750;&#36864;&#21270;&#24615;&#20004;&#20010;&#35777;&#20070;&#65292;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#21487;&#39564;&#35777;&#30340;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#19968;&#20010;&#29289;&#20307;&#30340;&#37096;&#20998;&#28857;&#20113;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#20165;&#35201;&#20272;&#35745;&#29289;&#20307;&#30340;&#23039;&#24577;&#65292;&#36824;&#35201;&#25552;&#20379;&#19968;&#20010;&#27491;&#30830;&#24615;&#35777;&#26126;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#31471;&#21040;&#31471;&#24863;&#30693;&#27169;&#22411;&#35748;&#35777;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026; $\zeta$-&#27491;&#30830;&#24615;&#30340;&#27010;&#24565;&#65292;&#23427;&#32422;&#26463;&#20102;&#20272;&#35745;&#20540;&#21644;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; $\zeta$-&#27491;&#30830;&#24615;&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#20004;&#20010;&#35777;&#20070;&#26469;&#35780;&#20272;&#65306;&#65288;i&#65289;&#19968;&#20010;&#21487;&#35266;&#23519;&#27491;&#30830;&#24615;&#30340;&#35777;&#20070;&#65292;&#35813;&#35777;&#20070;&#26029;&#35328;&#27169;&#22411;&#36755;&#20986;&#26159;&#21542;&#19982;&#36755;&#20837;&#25968;&#25454;&#21644;&#20808;&#39564;&#20449;&#24687;&#19968;&#33268;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#38750;&#36864;&#21270;&#24615;&#35777;&#20070;&#65292;&#23427;&#26029;&#35328;&#36755;&#20837;&#25968;&#25454;&#26159;&#21542;&#36275;&#20197;&#35745;&#31639;&#20986;&#21807;&#19968;&#30340;&#20272;&#35745;&#20540;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#35774;&#35745;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#21487;&#35777;&#26126;&#23039;&#24577;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986; C-3PO&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35821;&#20041;&#20851;&#38190;&#28857;&#30340;&#23039;&#24577;&#20272;&#35745;&#27169;&#22411;&#65292;&#22686;&#21152;&#20102;&#20004;&#20010;&#35777;&#20070;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#27979;&#37327;&#25968;&#25454;&#38598;&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#35757;&#32451;&#30340;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#20851;&#20110;&#21487;&#39564;&#35777;&#30340;&#19977;&#32500;&#29289;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#31532;&#19968;&#20010;&#36890;&#29992;&#29702;&#35770;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a certifiable object pose estimation problem, where -- given a partial point cloud of an object -- the goal is to not only estimate the object pose, but also to provide a certificate of correctness for the resulting estimate. Our first contribution is a general theory of certification for end-to-end perception models. In particular, we introduce the notion of $\zeta$-correctness, which bounds the distance between an estimate and the ground truth. We show that $\zeta$-correctness can be assessed by implementing two certificates: (i) a certificate of observable correctness, that asserts if the model output is consistent with the input data and prior information, (ii) a certificate of non-degeneracy, that asserts whether the input data is sufficient to compute a unique estimate. Our second contribution is to apply this theory and design a new learning-based certifiable pose estimator. We propose C-3PO, a semantic-keypoint-based pose estimation model, augmented with the two cer
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20559;&#24046;&#22996;&#21592;&#20250;&#26469;&#35757;&#32451;&#27809;&#26377;&#20559;&#35265;&#23646;&#24615;&#26631;&#31614;&#30340;&#21435;&#20559;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.10843</link><description>&lt;p&gt;
&#20351;&#29992;&#20559;&#24046;&#22996;&#21592;&#20250;&#23398;&#20064;&#21435;&#20559;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Learning Debiased Classifier with Biased Committee. (arXiv:2206.10843v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#20559;&#24046;&#22996;&#21592;&#20250;&#26469;&#35757;&#32451;&#27809;&#26377;&#20559;&#35265;&#23646;&#24615;&#26631;&#31614;&#30340;&#21435;&#20559;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24456;&#23481;&#26131;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31867;&#21035;&#21644;&#20869;&#22312;&#23646;&#24615;&#20043;&#38388;&#30340;&#20551;&#30456;&#20851;&#24615;&#20135;&#29983;&#20559;&#35265;&#65292;&#20174;&#32780;&#30772;&#22351;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#27809;&#26377;&#20559;&#35265;&#23646;&#24615;&#26631;&#31614;&#30340;&#21435;&#20559;&#20998;&#31867;&#22120;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#37319;&#29992;&#19968;&#20010;&#20998;&#31867;&#22120;&#22996;&#21592;&#20250;&#20316;&#20026;&#36741;&#21161;&#27169;&#22359;&#65292;&#22312;&#35757;&#32451;&#20027;&#20998;&#31867;&#22120;&#26102;&#35782;&#21035;&#20914;&#31361;&#25968;&#25454;&#65288;&#21363;&#27809;&#26377;&#20551;&#30456;&#20851;&#24615;&#30340;&#25968;&#25454;&#65289;&#65292;&#24182;&#23545;&#23427;&#20204;&#20998;&#37197;&#36739;&#22823;&#30340;&#26435;&#37325;&#12290;&#22996;&#21592;&#20250;&#20197;&#33258;&#25105;&#21551;&#21160;&#38598;&#25104;&#30340;&#24418;&#24335;&#36827;&#34892;&#23398;&#20064;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20998;&#31867;&#22120;&#23384;&#22312;&#20559;&#24046;&#20294;&#20855;&#26377;&#22810;&#26679;&#21270;&#65292;&#24182;&#26377;&#24847;&#26080;&#27861;&#25104;&#21151;&#39044;&#27979;&#20914;&#31361;&#25968;&#25454;&#30340;&#31867;&#21035;&#12290;&#22996;&#21592;&#20250;&#20869;&#37096;&#23545;&#39044;&#27979;&#38590;&#24230;&#30340;&#20849;&#35782;&#22240;&#27492;&#20026;&#35782;&#21035;&#21644;&#26435;&#37325;&#20914;&#31361;&#25968;&#25454;&#25552;&#20379;&#20102;&#21487;&#38752;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#35813;&#22996;&#21592;&#20250;&#36824;&#20351;&#29992;&#20174;&#20027;&#20998;&#31867;&#22120;&#36716;&#31227;&#30340;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#36880;&#28176;&#28040;&#38500;&#20854;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are prone to be biased towards spurious correlations between classes and latent attributes exhibited in a major portion of training data, which ruins their generalization capability. We propose a new method for training debiased classifiers with no spurious attribute label. The key idea is to employ a committee of classifiers as an auxiliary module that identifies bias-conflicting data, i.e., data without spurious correlation, and assigns large weights to them when training the main classifier. The committee is learned as a bootstrapped ensemble so that a majority of its classifiers are biased as well as being diverse, and intentionally fail to predict classes of bias-conflicting data accordingly. The consensus within the committee on prediction difficulty thus provides a reliable cue for identifying and weighting bias-conflicting data. Moreover, the committee is also trained with knowledge transferred from the main classifier so that it gradually becomes debiased along
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#22024;&#26434;&#25968;&#25454;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20813;&#36153;&#24179;&#28369;&#39044;&#27979;&#12290;&#22312;&#23454;&#29616;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35299;&#37322;&#20026;&#20309;&#23384;&#22312;&#22122;&#22768;&#21387;&#32553;&#65292;&#25506;&#35752;&#20102;&#36882;&#24402;&#32593;&#32476;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.04215</link><description>&lt;p&gt;
&#21033;&#29992;&#22024;&#26434;&#25968;&#25454;&#36827;&#34892;&#36882;&#24402;&#32593;&#32476;&#35757;&#32451;&#65292;&#23454;&#29616;&#20813;&#36153;&#24179;&#28369;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
It's a super deal -- train recurrent network on noisy data and get smooth prediction free. (arXiv:2206.04215v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#22024;&#26434;&#25968;&#25454;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#25928;&#26524;&#65292;&#24182;&#21457;&#29616;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20813;&#36153;&#24179;&#28369;&#39044;&#27979;&#12290;&#22312;&#23454;&#29616;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#35299;&#37322;&#20026;&#20309;&#23384;&#22312;&#22122;&#22768;&#21387;&#32553;&#65292;&#25506;&#35752;&#20102;&#36882;&#24402;&#32593;&#32476;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22024;&#26434;&#25968;&#25454;&#30340;&#39044;&#27979;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#29983;&#25104;&#24179;&#28369;&#30340;&#39044;&#26399;&#36712;&#36857;&#12290;&#25105;&#20204;&#26816;&#39564;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#36755;&#20837;&#24207;&#21015;&#20013;&#22122;&#22768;&#25104;&#20998;&#23545;&#32593;&#32476;&#39044;&#27979;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;&#39044;&#27979;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#22122;&#22768;&#21387;&#32553;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#36882;&#24402;&#32593;&#32476;&#22312;&#31070;&#32463;&#31185;&#23398;&#20013;&#23545;&#29983;&#29289;&#36827;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research demonstrate that prediction of time series by predictive recurrent neural networks based on the noisy input generates a smooth anticipated trajectory. We examine influence of the noise component in both the training data sets and the input sequences on network prediction quality. We propose and discuss an explanation of the observed noise compression in the predictive process. We also discuss importance of this property of recurrent networks in the neuroscience context for the evolution of living organisms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#20351;&#29992;&#23545;&#31216;&#24615;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23558;&#20540;&#36845;&#20195;&#35270;&#20026;&#32593;&#26684;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#21487;&#25511;&#21367;&#31215;&#26469;&#34701;&#21512;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#35268;&#21010;&#31639;&#27861;&#27604;&#38750;&#31561;&#21464;&#23545;&#24212;&#29289;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2206.03674</link><description>&lt;p&gt;
&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#21487;&#24494;&#35268;&#21010;&#20013;&#30340;&#21487;&#25511;&#21367;&#31215;
&lt;/p&gt;
&lt;p&gt;
Integrating Symmetry into Differentiable Planning with Steerable Convolutions. (arXiv:2206.03674v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#20351;&#29992;&#23545;&#31216;&#24615;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23558;&#20540;&#36845;&#20195;&#35270;&#20026;&#32593;&#26684;&#20449;&#21495;&#65292;&#24182;&#20351;&#29992;&#21487;&#25511;&#21367;&#31215;&#26469;&#34701;&#21512;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#35268;&#21010;&#31639;&#27861;&#27604;&#38750;&#31561;&#21464;&#23545;&#24212;&#29289;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#20986;&#29616;&#23545;&#31216;&#24615;&#26102;&#65292;&#22914;&#20309;&#21033;&#29992;&#32676;&#23545;&#31216;&#24615;&#25913;&#21892;&#31471;&#21040;&#31471;&#21487;&#24494;&#35268;&#21010;&#31639;&#27861;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#31561;&#21464;&#21367;&#31215;&#32593;&#32476;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#35270;&#20026;&#32593;&#26684;&#19978;&#30340;&#20449;&#21495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20540;&#36845;&#20195;&#26159;&#19968;&#20010;&#32447;&#24615;&#31561;&#21464;&#31639;&#23376;&#65292;&#21363;&#21487;&#34987;&#65288;&#23450;&#21521;&#65289;&#21367;&#31215;&#34920;&#31034;&#12290;&#36825;&#25193;&#23637;&#20102;&#20540;&#36845;&#20195;&#32593;&#32476;&#65288;VIN&#65289;&#22312;&#20351;&#29992;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#36335;&#24452;&#35268;&#21010;&#26102;&#20351;&#29992;&#39069;&#22806;&#30340;&#26059;&#36716;&#21644;&#21453;&#23556;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#22522;&#20110;VIN&#65292;&#24182;&#20351;&#29992;&#21487;&#25511;&#21367;&#31215;&#32593;&#32476;&#26469;&#34701;&#21512;&#23545;&#31216;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22235;&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#65306;2D&#23548;&#33322;&#65292;&#35270;&#35273;&#23548;&#33322;&#65292;&#33258;&#30001;&#24230;&#65288;2DOFs&#65289;&#37197;&#32622;&#31354;&#38388;&#21644;&#24037;&#20316;&#31354;&#38388;&#25805;&#32437;&#12290;&#19982;&#38750;&#31561;&#21464;&#23545;&#24212;&#29289;VIN&#21644;GPPN&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23545;&#31216;&#35268;&#21010;&#31639;&#27861;&#22312;&#35757;&#32451;&#25928;&#29575;&#21644;&#27867;&#21270;&#26041;&#38754;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study how group symmetry helps improve data efficiency and generalization for end-to-end differentiable planning algorithms when symmetry appears in decision-making tasks. Motivated by equivariant convolution networks, we treat the path planning problem as \textit{signals} over grids. We show that value iteration in this case is a linear equivariant operator, which is a (steerable) convolution. This extends Value Iteration Networks (VINs) on using convolutional networks for path planning with additional rotation and reflection symmetry. Our implementation is based on VINs and uses steerable convolution networks to incorporate symmetry. The experiments are performed on four tasks: 2D navigation, visual navigation, and 2 degrees of freedom (2DOFs) configuration space and workspace manipulation. Our symmetric planning algorithms improve training efficiency and generalization by large margins compared to non-equivariant counterparts, VIN and GPPN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.03353</link><description>&lt;p&gt;
&#22312;&#19981;&#31283;&#20581;&#26679;&#26412;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#19978;&#26045;&#21152;&#26356;&#22810;&#27491;&#21017;&#21270;&#20197;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#24471;&#21040;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#22343;&#20026;&#26368;&#20248;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#26159;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#20154;&#31867;&#35270;&#35273;&#26080;&#27861;&#23519;&#35273;&#30340;&#25968;&#25454;&#25200;&#21160;&#19979;&#65292;&#20351;&#32473;&#23450;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35823;&#21028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#65292;&#23427;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#24456;&#22909;&#30340;&#35777;&#26126;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#29305;&#28857;&#26159;&#65306;&#23545;&#20110;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#25968;&#25454;&#65292;&#27604;&#20854;&#20182;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#26356;&#22810;&#22320;&#24212;&#29992;&#27491;&#21017;&#21270;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#19968;&#20010;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#30340;&#27491;&#21017;&#21270;&#31639;&#27861;&#65292;&#23427;&#26469;&#33258;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#39118;&#38505;&#19978;&#30028;&#30340;&#21160;&#26426;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;(&#22312;&#20363;&#23376;&#19978;&#30340;&#20934;&#30830;&#24615;)&#21644;&#40065;&#26834;&#24615;(&#22312;&#23545;&#25239;&#25915;&#20987;&#19978;&#30340;&#20934;&#30830;&#24615;)&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#36890;&#29992;&#22270;&#20687;&#26412;&#22320;&#25991;&#26412;&#39537;&#21160;&#32534;&#36753;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#28151;&#21512;&#25193;&#25955;&#23545;&#26368;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21152;&#36895;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#25513;&#27169;&#36827;&#34892;&#25152;&#38656;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2206.02779</link><description>&lt;p&gt;
&#28151;&#21512;&#28508;&#22312;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Blended Latent Diffusion. (arXiv:2206.02779v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#36890;&#29992;&#22270;&#20687;&#26412;&#22320;&#25991;&#26412;&#39537;&#21160;&#32534;&#36753;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#28151;&#21512;&#25193;&#25955;&#23545;&#26368;&#26032;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#21152;&#36895;&#65292;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#25513;&#27169;&#36827;&#34892;&#25152;&#38656;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#22270;&#20687;&#29983;&#25104;&#30340;&#24040;&#22823;&#36827;&#27493;&#20197;&#21450;&#20284;&#20046;&#19975;&#33021;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#32456;&#20110;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#21019;&#24314;&#21644;&#32534;&#36753;&#22270;&#20687;&#30340;&#25509;&#21475;&#12290;&#22788;&#29702;&#36890;&#29992; &#22270;&#20687;&#38656;&#35201;&#21508;&#31181;&#21508;&#26679;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#27492;&#26368;&#26032;&#30340;&#20316;&#21697;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25454;&#26174;&#31034;&#65292;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#36229;&#36807;&#20102; GAN&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#23427;&#20204;&#30340;&#25512;&#26029;&#26102;&#38388;&#30456;&#23545;&#36739;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#36890;&#29992;&#22270;&#20687;&#26412;&#22320;&#25991;&#26412;&#39537;&#21160;&#32534;&#36753;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#38656;&#30340;&#32534;&#36753;&#38480;&#21046;&#22312;&#29992;&#25143;&#25552;&#20379;&#30340;&#25513;&#27169;&#20869;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#20102;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#26469;&#21152;&#36895;&#25193;&#25955;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558; LDM &#36716;&#25442;&#20026;&#26412;&#22320;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#23558;&#28151;&#21512;&#25193;&#25955;&#32452;&#21512;&#36827;&#21435;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#38024;&#23545; LDM &#22266;&#26377;&#30340;&#26080;&#27861;&#20934;&#30830;&#37325;&#26500;&#22270;&#20687;&#30340;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous progress in neural image generation, coupled with the emergence of seemingly omnipotent vision-language models has finally enabled text-based interfaces for creating and editing images. Handling generic images requires a diverse underlying generative model, hence the latest works utilize diffusion models, which were shown to surpass GANs in terms of diversity. One major drawback of diffusion models, however, is their relatively slow inference time. In this paper, we present an accelerated solution to the task of local text-driven editing of generic images, where the desired edits are confined to a user-provided mask. Our solution leverages a recent text-to-image Latent Diffusion Model (LDM), which speeds up diffusion by operating in a lower-dimensional latent space. We first convert the LDM into a local image editor by incorporating Blended Diffusion into it. Next we propose an optimization-based solution for the inherent inability of this LDM to accurately reconstruct i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HAFH-DDPG&#26469;&#23398;&#20064;&#38548;&#31163;&#24335;&#24494;&#30005;&#32593;&#20013;&#30340;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#21644;&#26426;&#32452;&#24320;&#21551;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2206.01663</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24494;&#30005;&#32593;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#21644;&#26426;&#32452;&#24320;&#21551;
&lt;/p&gt;
&lt;p&gt;
Joint Energy Dispatch and Unit Commitment in Microgrids Based on Deep Reinforcement Learning. (arXiv:2206.01663v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;HAFH-DDPG&#26469;&#23398;&#20064;&#38548;&#31163;&#24335;&#24494;&#30005;&#32593;&#20013;&#30340;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#21644;&#26426;&#32452;&#24320;&#21551;&#20915;&#31574;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26612;&#27833;&#21457;&#30005;&#26426;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#24212;&#29992;&#21487;&#20877;&#29983;&#33021;&#28304;&#30340;&#24494;&#30005;&#32593;&#65288;MG&#65289;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#36825;&#21019;&#36896;&#20102;&#23545;&#21160;&#24577;&#33021;&#28304;&#31649;&#29702;&#30340;&#24378;&#28872;&#38656;&#27714;&#12290;&#26412;&#25991;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#23398;&#20064;&#38548;&#31163;&#24335;MG&#20013;&#32852;&#21512;&#33021;&#37327;&#20998;&#37197;&#65288;ED&#65289;&#21644;&#26426;&#32452;&#24320;&#21551;&#65288;UC&#65289;&#20915;&#31574;&#30340;&#26368;&#20248;&#31574;&#30053;&#65292;&#20197;&#22312;&#30830;&#20445;&#20379;&#38656;&#24179;&#34913;&#30340;&#21069;&#25552;&#19979;&#38477;&#20302;&#24635;&#30005;&#21147;&#25104;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#20110;&#32852;&#21512;ED&#21644;UC&#32780;&#23548;&#33268;&#30340;&#31163;&#25955;-&#36830;&#32493;&#28151;&#21512;&#34892;&#21160;&#31354;&#38388;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DRL&#31639;&#27861;&#65292;&#21363;&#28151;&#21512;&#34892;&#21160;&#26377;&#38480;&#22320;&#24179;&#32447;DDPG&#65288;HAFH-DDPG&#65289;&#65292;&#23427;&#22522;&#20110;&#26377;&#38480;&#22320;&#24179;&#32447;&#21160;&#24577;&#35268;&#21010;&#65288;DP&#65289;&#26694;&#26550;&#65292;&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#20004;&#31181;&#32463;&#20856;DRL&#31639;&#27861;&#65292;&#21363;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#21644;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#65288;DDPG&#65289;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26612;&#27833;&#21457;&#30005;&#26426;&#65288;DG&#65289;&#36873;&#25321;&#31574;&#30053;&#65292;&#20197;&#25903;&#25345;&#31616;&#21270;&#34892;&#21160;&#31354;&#38388;&#65292;&#20197;&#38477;&#20302;&#27492;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the application of microgrids (MG) with renewable energy is becoming more and more extensive, which creates a strong need for dynamic energy management. In this paper, deep reinforcement learning (DRL) is applied to learn an optimal policy for making joint energy dispatch (ED) and unit commitment (UC) decisions in an isolated MG, with the aim for reducing the total power generation cost on the premise of ensuring the supply-demand balance. In order to overcome the challenge of discrete-continuous hybrid action space due to joint ED and UC, we propose a DRL algorithm, i.e., the hybrid action finite-horizon DDPG (HAFH-DDPG), that seamlessly integrates two classical DRL algorithms, i.e., deep Q-network (DQN) and deep deterministic policy gradient (DDPG), based on a finite-horizon dynamic programming (DP) framework. Moreover, a diesel generator (DG) selection strategy is presented to support a simplified action space for reducing the computation complexity of this algorithm. Fina
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25112;&#30053;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#31038;&#20132;&#20851;&#31995;&#26469;&#25552;&#39640;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#30340;&#25112;&#30053;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#30340;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21313;&#20998;&#23454;&#29992;&#12290;</title><link>http://arxiv.org/abs/2205.15765</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25112;&#30053;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Strategic Classification with Graph Neural Networks. (arXiv:2205.15765v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25112;&#30053;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#31038;&#20132;&#20851;&#31995;&#26469;&#25552;&#39640;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#30340;&#25112;&#30053;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20960;&#20010;&#30495;&#23454;&#30340;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21313;&#20998;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#30053;&#20998;&#31867;&#30740;&#31350;&#29992;&#25143;&#21487;&#20197;&#20462;&#25913;&#20854;&#29305;&#24449;&#20197;&#33719;&#24471;&#26377;&#21033;&#39044;&#27979;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#37117;&#30528;&#37325;&#20110;&#35302;&#21457;&#29420;&#31435;&#29992;&#25143;&#21709;&#24212;&#30340;&#31616;&#21333;&#20998;&#31867;&#22120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#36827;&#34892;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#27169;&#22411;&#25171;&#30772;&#20102;&#29420;&#31435;&#24615;&#20551;&#35774;&#12290;&#21463;&#21040;&#25112;&#30053;&#20998;&#31867;&#24212;&#29992;&#36890;&#24120;&#20855;&#26377;&#31038;&#20132;&#24615;&#36136;&#30340;&#24605;&#36335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31038;&#20132;&#20851;&#31995;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#8220;&#22270;&#31070;&#32463;&#32593;&#32476;&#8221;&#12290;&#20351;&#29992;&#22270;&#36827;&#34892;&#23398;&#20064;&#24341;&#20837;&#20102;&#39044;&#27979;&#20013;&#30340;&#29992;&#25143;&#38388;&#30456;&#20114;&#20381;&#36182;&#65307;&#25105;&#20204;&#30340;&#20851;&#38190;&#28857;&#26159;&#25112;&#30053;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20381;&#36182;&#26469;&#20419;&#36827;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#27491;&#22914;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#21644;&#27169;&#25311;&#25152;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#36825;&#21487;&#33021;&#35201;&#20040;&#23545;&#31995;&#32479;&#19981;&#21033;&#65292;&#35201;&#20040;&#23545;&#31995;&#32479;&#26377;&#21033;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#20998;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#22522;&#20110;&#22270;&#30340;&#20998;&#31867;&#22120;&#30340;&#25112;&#30053;&#40065;&#26834;&#24615;&#12290;&#22312;&#20960;&#20010;&#30495;&#23454;&#30340;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Strategic classification studies learning in settings where users can modify their features to obtain favorable predictions. Most current works focus on simple classifiers that trigger independent user responses. Here we examine the implications of learning with more elaborate models that break the independence assumption. Motivated by the idea that applications of strategic classification are often social in nature, we focus on \emph{graph neural networks}, which make use of social relations between users to improve predictions. Using a graph for learning introduces inter-user dependencies in prediction; our key point is that strategic users can exploit these to promote their goals. As we show through analysis and simulation, this can work either against the system -- or for it. Based on this, we propose a differentiable framework for strategically-robust learning of graph-based classifiers. Experiments on several real networked datasets demonstrate the utility of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#19981;&#22266;&#23450;&#12289;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#22330;&#26223;&#12290;&#27169;&#22411;&#38024;&#23545;&#21508;&#31181;&#31867;&#22411;&#29289;&#20307;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2205.01982</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Lifelong Ensemble Learning based on Multiple Representations for Few-Shot Object Recognition. (arXiv:2205.01982v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#19981;&#22266;&#23450;&#12289;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#22330;&#26223;&#12290;&#27169;&#22411;&#38024;&#23545;&#21508;&#31181;&#31867;&#22411;&#29289;&#20307;&#36827;&#34892;&#22788;&#29702;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26381;&#21153;&#26426;&#22120;&#20154;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#24110;&#21161;&#25105;&#20204;&#22788;&#29702;&#21508;&#31181;&#20219;&#21153;&#12290;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#65292;&#26426;&#22120;&#20154;&#32463;&#24120;&#38754;&#23545;&#26032;&#30340;&#29289;&#20307;&#65292;&#24182;&#38656;&#35201;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#23398;&#20064;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#36825;&#26679;&#30340;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#35782;&#21035;&#21508;&#31181;&#29289;&#20307;&#31867;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#37325;&#34920;&#31034;&#30340;&#32456;&#36523;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#30446;&#26631;&#35782;&#21035;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#34920;&#31034;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;3D&#24418;&#29366;&#25551;&#36848;&#31526;&#24418;&#25104;&#38598;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#20419;&#36827;&#32456;&#36523;&#23398;&#20064;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#37197;&#22791;&#20102;&#19968;&#20010;&#23384;&#20648;&#21644;&#30636;&#38388;&#26816;&#32034;&#29289;&#20307;&#20449;&#24687;&#30340;&#35760;&#24518;&#21333;&#20803;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36866;&#29992;&#20110;&#19977;&#32500;&#29289;&#20307;&#31867;&#21035;&#25968;&#37327;&#19981;&#22266;&#23450;&#12289;&#38543;&#26102;&#38388;&#22686;&#38271;&#30340;&#24320;&#25918;&#24335;&#23398;&#20064;&#22330;&#26223;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#31163;&#32447;&#21450;&#24320;&#25918;&#24335;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Service robots are integrating more and more into our daily lives to help us with various tasks. In such environments, robots frequently face new objects while working in the environment and need to learn them in an open-ended fashion. Furthermore, such robots must be able to recognize a wide range of object categories. In this paper, we present a lifelong ensemble learning approach based on multiple representations to address the few-shot object recognition problem. In particular, we form ensemble methods based on deep representations and handcrafted 3D shape descriptors. To facilitate lifelong learning, each approach is equipped with a memory unit for storing and retrieving object information instantly. The proposed model is suitable for open-ended learning scenarios where the number of 3D object categories is not fixed and can grow over time. We have performed extensive sets of experiments to assess the performance of the proposed approach in offline, and open-ended scenarios. For t
&lt;/p&gt;</description></item><item><title>MONAI Label&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;AI&#26694;&#26550;&#65292;&#25903;&#25345;&#24555;&#36895;&#26631;&#27880;&#21307;&#23398;&#22270;&#20687;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#21069;&#31471;&#36873;&#39033;&#21644;&#20004;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21152;&#36895;&#20998;&#21106;&#31639;&#27861;&#30340;&#35757;&#32451;&#65292;&#36827;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.12362</link><description>&lt;p&gt;
MONAI Label: &#19968;&#31181;&#29992;&#20110;AI&#36741;&#21161;&#20132;&#20114;&#26631;&#27880;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images. (arXiv:2203.12362v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12362
&lt;/p&gt;
&lt;p&gt;
MONAI Label&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;AI&#26694;&#26550;&#65292;&#25903;&#25345;&#24555;&#36895;&#26631;&#27880;&#21307;&#23398;&#22270;&#20687;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#21069;&#31471;&#36873;&#39033;&#21644;&#20004;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21152;&#36895;&#20998;&#21106;&#31639;&#27861;&#30340;&#35757;&#32451;&#65292;&#36827;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#38598;&#26159;&#35757;&#32451;&#26032;&#30340;&#20219;&#21153;&#29305;&#23450;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20027;&#35201;&#29942;&#39048;&#65292;&#22240;&#20026;&#25163;&#21160;&#27880;&#37322;&#26497;&#20854;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MONAI Label&#65292;&#19968;&#20010;&#20813;&#36153;&#19988;&#24320;&#28304;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#24212;&#29992;&#31243;&#24207;&#24320;&#21457;&#65292;&#20197;&#32553;&#30701;&#25918;&#23556;&#23398;&#25968;&#25454;&#38598;&#27880;&#37322;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;&#36890;&#36807;MONAI Label&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#24320;&#21457;&#19987;&#27880;&#20110;&#20854;&#19987;&#19994;&#39046;&#22495;&#30340;AI&#27880;&#37322;&#24212;&#29992;&#31243;&#24207;&#12290;&#23427;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#20316;&#20026;&#26381;&#21153;&#24555;&#36895;&#37096;&#32626;&#65292;&#24182;&#36890;&#36807;&#20182;&#20204;&#39318;&#36873;&#30340;&#29992;&#25143;&#30028;&#38754;&#25552;&#20379;&#32473;&#20020;&#24202;&#21307;&#29983;&#12290;&#30446;&#21069;&#65292;MONAI Label&#24050;&#32463;&#20934;&#22791;&#22909;&#25903;&#25345;&#26412;&#22320;&#23433;&#35013;&#65288;3D Slicer&#65289;&#21644;&#22522;&#20110;web&#30340;&#65288;OHIF&#65289;&#21069;&#31471;&#65292;&#24182;&#25552;&#20379;&#20004;&#31181;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#20197;&#20419;&#36827;&#21644;&#21152;&#36895;&#20998;&#21106;&#31639;&#27861;&#30340;&#35757;&#32451;&#12290;MONAI Label&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#28155;&#21152;&#23567;&#37327;&#30340;&#25163;&#21160;&#27880;&#37322;&#26469;&#36880;&#27493;&#25913;&#36827;&#20182;&#20204;&#22522;&#20110;AI&#30340;&#27880;&#37322;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;MONAI Label&#20026;AI&#36741;&#21161;&#20132;&#20114;&#26631;&#27880;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of annotated datasets is a major bottleneck for training new task-specific supervised machine learning models, considering that manual annotation is extremely expensive and time-consuming. To address this problem, we present MONAI Label, a free and open-source framework that facilitates the development of applications based on artificial intelligence (AI) models that aim at reducing the time required to annotate radiology datasets. Through MONAI Label, researchers can develop AI annotation applications focusing on their domain of expertise. It allows researchers to readily deploy their apps as services, which can be made available to clinicians via their preferred user interface. Currently, MONAI Label readily supports locally installed (3D Slicer) and web-based (OHIF) frontends and offers two active learning strategies to facilitate and speed up the training of segmentation algorithms. MONAI Label allows researchers to make incremental improvements to their AI-based annotatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26174;&#24335;&#30340;&#21152;&#26435;&#26041;&#26696;&#30340;&#20803;&#27169;&#22411;&#65292;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#25968;&#25454;&#20559;&#35265;&#38382;&#39064;&#26102;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.05613</link><description>&lt;p&gt;
CMW-Net: &#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#31867;&#21035;&#24863;&#30693;&#26679;&#26412;&#21152;&#26435;&#26144;&#23556;&#30340;&#40065;&#26834;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CMW-Net: Learning a Class-Aware Sample Weighting Mapping for Robust Deep Learning. (arXiv:2202.05613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26174;&#24335;&#30340;&#21152;&#26435;&#26041;&#26696;&#30340;&#20803;&#27169;&#22411;&#65292;&#29992;&#20110;&#32531;&#35299;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20855;&#26377;&#25968;&#25454;&#20559;&#35265;&#38382;&#39064;&#26102;&#30340;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24456;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#20855;&#26377;&#26377;&#20559;&#26631;&#31614;&#25110;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#32531;&#35299;&#36825;&#31181;&#25968;&#25454;&#20559;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#25163;&#21160;&#39044;&#20808;&#25351;&#23450;&#21152;&#26435;&#26041;&#26696;&#20197;&#21450;&#23427;&#20204;&#30340;&#39069;&#22806;&#36229;&#21442;&#25968;&#65292;&#20381;&#36182;&#20110;&#30740;&#31350;&#38382;&#39064;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#26174;&#24335;&#30340;&#21152;&#26435;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#35757;&#32451;&#31867;&#21035;&#35270;&#20026;&#21333;&#29420;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#19968;&#20010;&#26174;&#24335;&#30340;&#21152;&#26435;&#20989;&#25968;&#65292;&#23558;&#26679;&#26412;&#25439;&#22833;&#21644;&#20219;&#21153;/&#31867;&#21035;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#65292;&#26679;&#26412;&#26435;&#37325;&#20316;&#20026;&#36755;&#20986;&#65292;&#20174;&#32780;&#26399;&#26395;&#23545;&#19981;&#21516;&#30340;&#26679;&#26412;&#31867;&#21035;&#26045;&#21152;&#33258;&#36866;&#24212;&#30340;&#21464;&#21270;&#21152;&#26435;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern deep neural networks can easily overfit to biased training data containing corrupted labels or class imbalance. Sample re-weighting methods are popularly used to alleviate this data bias issue. Most current methods, however, require to manually pre-specify the weighting schemes as well as their additional hyper-parameters relying on the characteristics of the investigated problem and training data. This makes them fairly hard to be generally applied in practical scenarios, due to their significant complexities and inter-class variations of data bias situations. To address this issue, we propose a meta-model capable of adaptively learning an explicit weighting scheme directly from data. Specifically, by seeing each training class as a separate learning task, our method aims to extract an explicit weighting function with sample loss and task/class feature as input, and sample weight as output, expecting to impose adaptively varying weighting schemes to different sample classes bas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;Complex-to-Real&#33609;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#21512;&#23454;&#24352;&#37327;&#20056;&#31215;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#39033;&#24335;&#26680;&#19978;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2202.02031</link><description>&lt;p&gt;
&#22797;&#21512;&#23454;&#24352;&#37327;&#20056;&#31215;&#30340;&#33609;&#22270;&#21450;&#20854;&#22312;&#22810;&#39033;&#24335;&#26680;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Complex-to-Real Sketches for Tensor Products with Applications to the Polynomial Kernel. (arXiv:2202.02031v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;Complex-to-Real&#33609;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#21512;&#23454;&#24352;&#37327;&#20056;&#31215;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#39033;&#24335;&#26680;&#19978;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;$p$&#20010;&#21521;&#37327;&#30340;&#24352;&#37327;&#31215;&#30340;&#38543;&#26426;&#33609;&#22270;&#36981;&#24490;&#32479;&#35745;&#25928;&#29575;&#21644;&#35745;&#31639;&#21152;&#36895;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#24120;&#20351;&#29992;&#30340;&#26041;&#27861;&#36991;&#20813;&#26174;&#24335;&#35745;&#31639;&#39640;&#32500;&#24352;&#37327;&#31215;&#65292;&#23548;&#33268;&#22312;&#23884;&#20837;&#32500;&#24230;&#19978;&#20855;&#26377;&#20122;&#26368;&#20248;&#30340;$\mathcal{O}(3^p)$&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;Complex-to-Real (CtR)&#20462;&#25913;&#26082;&#26377;&#30340;&#33609;&#22270;&#65292;&#36890;&#36807;&#29992;&#22797;&#25968;&#26367;&#25442;&#23454;&#25968;&#38543;&#26426;&#25237;&#24433;&#65292;&#22312;&#23884;&#20837;&#32500;&#24230;&#19978;&#21482;&#38656;&#35201;&#36739;&#20302;&#30340;$\mathcal{O}(2^p)$&#22240;&#23376;&#12290;&#25105;&#20204;&#33609;&#22270;&#30340;&#36755;&#20986;&#26159;&#23454;&#20540;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#19979;&#28216;&#29992;&#36884;&#21464;&#24471;&#31616;&#21333;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#33609;&#22270;&#24212;&#29992;&#20110;$p$&#20493;&#33258;&#24352;&#37327;&#36755;&#20837;&#65292;&#36825;&#20123;&#36755;&#20837;&#23545;&#24212;&#20110;&#22810;&#39033;&#24335;&#20869;&#26680;&#30340;&#29305;&#24449;&#26144;&#23556;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25991;&#29486;&#20013;&#20854;&#20182;&#38543;&#26426;&#36924;&#36817;&#30456;&#27604;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized sketches of a tensor product of $p$ vectors follow a tradeoff between statistical efficiency and computational acceleration. Commonly used approaches avoid computing the high-dimensional tensor product explicitly, resulting in a suboptimal dependence of $\mathcal{O}(3^p)$ in the embedding dimension. We propose a simple Complex-to-Real (CtR) modification of well-known sketches that replaces real random projections by complex ones, incurring a lower $\mathcal{O}(2^p)$ factor in the embedding dimension. The output of our sketches is real-valued, which renders their downstream use straightforward. In particular, we apply our sketches to $p$-fold self-tensored inputs corresponding to the feature maps of the polynomial kernel. We show that our method achieves state-of-the-art performance in terms of accuracy and speed compared to other randomized approximations from the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#32852;&#32593;&#30340;&#20998;&#24067;&#24335;&#21151;&#33021;&#21387;&#32553;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#33021;&#22815;&#20219;&#24847;&#35745;&#31639;IoT&#25152;&#38656;&#20989;&#25968;&#21387;&#32553;&#20219;&#21153;&#30340;Kolmogorov-Arnold&#34920;&#31034;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#20113;&#30340;&#26041;&#27861;&#22312;&#20256;&#36755;&#25968;&#25454;&#26102;&#32473;&#32593;&#32476;&#36164;&#28304;&#24102;&#26469;&#30340;&#21387;&#21147;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2201.09483</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#29289;&#32852;&#32593;&#30340;&#20998;&#24067;&#24335;&#21151;&#33021;&#21387;&#32553;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Framework for Distributed Functional Compression over Wireless Channels in IoT. (arXiv:2201.09483v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#32852;&#32593;&#30340;&#20998;&#24067;&#24335;&#21151;&#33021;&#21387;&#32553;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#33021;&#22815;&#20219;&#24847;&#35745;&#31639;IoT&#25152;&#38656;&#20989;&#25968;&#21387;&#32553;&#20219;&#21153;&#30340;Kolmogorov-Arnold&#34920;&#31034;&#23450;&#29702;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#20113;&#30340;&#26041;&#27861;&#22312;&#20256;&#36755;&#25968;&#25454;&#26102;&#32473;&#32593;&#32476;&#36164;&#28304;&#24102;&#26469;&#30340;&#21387;&#21147;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#20135;&#29983;&#30340;&#28023;&#37327;&#25968;&#25454;&#21644;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23558;&#20849;&#21516;&#38761;&#26032;&#29289;&#29702;&#31995;&#32479;&#12290;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#39046;&#22495;&#20013;&#65292;&#20174;&#33258;&#21160;&#39550;&#39542;&#21040;&#22686;&#24378;&#29616;&#23454;&#65292;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#35774;&#22791;&#35745;&#31639;&#29305;&#23450;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#32780;&#36825;&#20123;&#30446;&#26631;&#20989;&#25968;&#24182;&#19981;&#20687;&#38556;&#30861;&#29289;&#26816;&#27979;&#12289;&#29289;&#20307;&#35782;&#21035;&#31561;&#20855;&#26377;&#31616;&#21333;&#24418;&#24335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20113;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#24515;&#20301;&#32622;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#29702;&#65292;&#36825;&#32473;&#32593;&#32476;&#36164;&#28304;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21387;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#38754;&#21521;&#29289;&#32852;&#32593;&#30340;&#20998;&#24067;&#24335;&#21151;&#33021;&#21387;&#32553;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#39640;&#26031;&#22810;&#36335;&#35775;&#38382;&#20449;&#36947;&#65288;GMAC&#65289;&#21644;&#27491;&#20132;AWGN&#20449;&#36947;&#19978;&#36827;&#34892;&#12290;&#30001;&#20110;Kolmogorov-Arnold&#34920;&#31034;&#23450;&#29702;&#65292;&#25105;&#20204;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#35774;&#35745;&#20026;IoT&#30340;&#25152;&#38656;&#20989;&#25968;&#21387;&#32553;&#20219;&#21153;&#35745;&#31639;&#20219;&#24847;&#20219;&#24847;&#20989;&#25968;&#12290;&#37325;&#35201;&#30340;&#26159;&#21407;&#22987;&#24863;&#23448;&#25968;&#25454;&#27704;&#36828;&#19981;&#20250;&#20256;&#36755;&#21040;&#20013;&#24515;&#33410;&#28857;&#36827;&#34892;&#35757;&#32451;&#25110;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT devices generating enormous data and state-of-the-art machine learning techniques together will revolutionize cyber-physical systems. In many diverse fields, from autonomous driving to augmented reality, distributed IoT devices compute specific target functions without simple forms like obstacle detection, object recognition, etc. Traditional cloud-based methods that focus on transferring data to a central location either for training or inference place enormous strain on network resources. To address this, we develop, to the best of our knowledge, the first machine learning framework for distributed functional compression over both the Gaussian Multiple Access Channel (GMAC) and orthogonal AWGN channels. Due to the Kolmogorov-Arnold representation theorem, our machine learning framework can, by design, compute any arbitrary function for the desired functional compression task in IoT. Importantly the raw sensory data are never transferred to a central node for training or inference
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#21160;&#24577;&#36951;&#25022;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#20445;&#35777;&#65292;&#20855;&#26377;&#20960;&#20309;&#30452;&#35266;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.14368</link><description>&lt;p&gt;
&#38750;&#38745;&#24577;&#36866;&#24212;&#24615;&#65306;&#38754;&#21521;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#38382;&#39064;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. (arXiv:2112.14368v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.14368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#38754;&#21521;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#21160;&#24577;&#36951;&#25022;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#20013;&#36827;&#19968;&#27493;&#22686;&#24378;&#20445;&#35777;&#65292;&#20855;&#26377;&#20960;&#20309;&#30452;&#35266;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#24182;&#36873;&#25321;&#21160;&#24577;&#36951;&#25022;&#20316;&#20026;&#24615;&#33021;&#24230;&#37327;&#65292;&#23450;&#20041;&#20026;&#22312;&#32447;&#31639;&#27861;&#21644;&#20219;&#20309;&#21487;&#34892;&#27604;&#36739;&#22120;&#24207;&#21015;&#25152;&#32047;&#35745;&#30340;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#20540;&#12290;&#20551;&#35774;$T$&#26159;&#26102;&#38388;&#38271;&#24230;&#65292;$P_T$&#26159;&#23454;&#36136;&#19978;&#21453;&#26144;&#29615;&#22659;&#38750;&#38745;&#24577;&#24615;&#30340;&#36335;&#24452;&#38271;&#24230;&#65292;&#21017;&#26368;&#20808;&#36827;&#30340;&#21160;&#24577;&#36951;&#25022;&#26159;$\mathcal{O}(\sqrt{T(1+P_T)})$&#12290;&#23613;&#31649;&#36825;&#20010;&#30028;&#38480;&#34987;&#35777;&#26126;&#23545;&#20110;&#20984;&#20989;&#25968;&#26159;&#26368;&#23567;&#21270;&#30340;&#65292;&#20294;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#65292;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#38382;&#39064;&#23454;&#20363;&#20013;&#65292;&#29305;&#21035;&#26159;&#24403;&#22312;&#32447;&#20989;&#25968;&#26159;&#20809;&#28369;&#30340;&#26102;&#20505;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20445;&#35777;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26032;&#39062;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20809;&#28369;&#24615;&#65292;&#24182;&#29992;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#21464;&#21270;&#12289;&#27604;&#36739;&#22120;&#24207;&#21015;&#30340;&#32047;&#35745;&#25439;&#22833;&#21644;&#36825;&#20004;&#20010;&#39033;&#30340;&#26368;&#23567;&#20540;&#20195;&#26367;&#21160;&#24577;&#36951;&#25022;&#20013;&#23545;$T$&#30340;&#20381;&#36182;&#12290;&#36825;&#20123;&#37327;&#34987;&#35777;&#26126;&#20855;&#26377;&#20960;&#20309;&#30452;&#35266;&#24615;&#65292;&#24182;&#19988;&#19982;&#29615;&#22659;&#20013;&#30340;&#38750;&#38745;&#24577;&#29616;&#35937;&#23494;&#20999;&#30456;&#20851;&#12290;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\mathcal{O}(\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantitie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#31232;&#30095;&#23637;&#24320;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31232;&#30095;&#12290;</title><link>http://arxiv.org/abs/2112.05888</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#30340;&#31232;&#30095;&#23637;&#24320;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Sparse Expansion For Deep Gaussian Processes. (arXiv:2112.05888v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#31232;&#30095;&#23637;&#24320;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31232;&#30095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#22797;&#26434;&#20998;&#24067;&#30340;&#38543;&#26426;&#36807;&#31243;&#65292;&#37319;&#29992;&#28145;&#24230;&#39640;&#26031;&#36807;&#31243;&#65288;DGP&#65289;&#20316;&#20026;&#32479;&#35745;&#26367;&#20195;&#21697;&#12290;&#20256;&#32479;&#30340;DGP&#27169;&#22411;&#25512;&#26029;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#24456;&#39640;&#65292;&#22240;&#20026;&#38656;&#35201;&#20351;&#29992;&#26680;&#30697;&#38453;&#36827;&#34892;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#31995;&#21015;&#39640;&#26031;&#36807;&#31243;&#65288;TMGP&#65289;&#30340;&#20934;&#30830;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#23618;&#23637;&#24320;&#30340;TMGP&#35825;&#23548;&#36817;&#20284;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#23558;&#22810;&#20010;TMGP&#30340;&#20998;&#23618;&#23637;&#24320;&#32452;&#21512;&#25104;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;TMGP&#65288;DTMGP&#65289;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#20197;&#19979;&#29305;&#24615;&#65306;&#65288;1&#65289;&#27599;&#20010;&#28608;&#27963;&#20989;&#25968;&#30340;&#36755;&#20986;&#37117;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26435;&#37325;&#26159;&#20174;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#20013;&#29420;&#31435;&#36873;&#25321;&#30340;&#65307;&#65288;2&#65289;&#22312;&#35757;&#32451;&#25110;&#39044;&#27979;&#20013;&#65292;&#21482;&#26377;polylog&#65288;M&#65289;&#65288;M&#20010;&#20013;&#30340;&#19968;&#37096;&#20998;&#65289;&#20010;&#28608;&#27963;&#20989;&#25968;&#20855;&#26377;&#38750;&#38646;&#36755;&#20986;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#31232;&#30095;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we use Deep Gaussian Processes (DGPs) as statistical surrogates for stochastic processes with complex distributions. Conventional inferential methods for DGP models can suffer from high computational complexity as they require large-scale operations with kernel matrices for training and inference. In this work, we propose an efficient scheme for accurate inference and efficient training based on a range of Gaussian Processes, called the Tensor Markov Gaussian Processes (TMGP). We construct an induced approximation of TMGP referred to as the hierarchical expansion. Next, we develop a deep TMGP (DTMGP) model as the composition of multiple hierarchical expansion of TMGPs. The proposed DTMGP model has the following properties: (1) the outputs of each activation function are deterministic while the weights are chosen independently from standard Gaussian distribution; (2) in training or prediction, only polylog(M) (out of M) activation functions have non-zero outputs, which sig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#26641;&#25628;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;TreeMesh&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;&#65292;&#21487;&#20197;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2111.07613</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#21644;&#26641;&#25628;&#32034;&#29983;&#25104;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;
&lt;/p&gt;
&lt;p&gt;
Generate plane quad mesh with neural networks and tree search. (arXiv:2111.07613v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.07613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#21644;&#26641;&#25628;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#21517;&#20026;TreeMesh&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;&#65292;&#21487;&#20197;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#38480;&#20803;&#26041;&#27861;&#65288;FEM&#65289;&#30340;&#21382;&#21490;&#19978;&#65292;&#32593;&#26684;&#29983;&#25104;&#30340;&#36136;&#37327;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#20026;&#24037;&#31243;&#24072;&#25552;&#20379;&#21487;&#38752;&#20223;&#30495;&#32467;&#26524;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#30446;&#21069;&#26368;&#20581;&#22766;&#30340;&#20803;&#32032;&#25552;&#21462;&#26041;&#27861;&#26159;&#37319;&#29992;&#23547;&#25214;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#19979;&#19968;&#20010;&#20803;&#32032;&#30340;&#26041;&#27861;&#26469;&#21152;&#24555;&#25552;&#21462;&#36895;&#24230;&#65292;&#20294;&#36825;&#21487;&#33021;&#23548;&#33268;&#32463;&#36807;&#22810;&#27425;&#36845;&#20195;&#21518;&#23616;&#37096;&#32593;&#26684;&#36136;&#37327;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TreeMesh&#65292;&#35813;&#26041;&#27861;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#65288;&#20063;&#21487;&#33021;&#26159;&#30417;&#30563;&#23398;&#20064;&#65289;&#21644;&#19968;&#31181;&#26032;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30456;&#32467;&#21512;&#12290;&#35813;&#31639;&#27861;&#22522;&#20110;&#20808;&#21069;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#22810;&#27425;&#25913;&#36827;&#21518;&#65292;&#22312;&#30456;&#21516;&#30340;&#36793;&#30028;&#19978;&#24615;&#33021;&#20248;&#20110;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#26641;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#31243;&#24207;&#21487;&#20197;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#24555;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24179;&#38754;&#22235;&#36793;&#24418;&#32593;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of mesh generation has long been considered a vital aspect in providing engineers with reliable simulation results throughout the history of the Finite Element Method (FEM). The element extraction method, which is currently the most robust method, is used in business software. However, in order to speed up extraction, the approach is done by finding the next element that optimizes a target function, which can result in local mesh of bad quality after many time steps. We provide TreeMesh, a method that uses this method in conjunction with reinforcement learning (also possible with supervised learning) and a novel Monte-Carlo tree search (MCTS) (Coulom(2006), Kocsis and Szepesv\'ari(2006), Browne et~al.(2012)). The algorithm is based on a previously proposed approach (Pan et~al.(2021)). After making many improvements on DRL (algorithm, state-action-reward setting) and adding a MCTS, it outperforms the former work on the same boundary. Furthermore, using tree search, our progr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35268;&#27169;&#19979;&#22914;&#20309;&#25915;&#20987;&#21644;&#38450;&#24481;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#24863;&#30693;&#30340;&#19968;&#38454;&#20248;&#21270;&#25915;&#20987;&#21644;&#40065;&#26834;&#24615;&#32858;&#21512;&#20989;&#25968;Soft Median&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;GNNs&#30340;&#21487;&#38752;&#24615;&#21644;&#25915;&#20987;&#21147;&#12290;</title><link>http://arxiv.org/abs/2110.14038</link><description>&lt;p&gt;
&#35268;&#27169;&#19979;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of Graph Neural Networks at Scale. (arXiv:2110.14038v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.14038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35268;&#27169;&#19979;&#22914;&#20309;&#25915;&#20987;&#21644;&#38450;&#24481;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#20102;&#31232;&#30095;&#24863;&#30693;&#30340;&#19968;&#38454;&#20248;&#21270;&#25915;&#20987;&#21644;&#40065;&#26834;&#24615;&#32858;&#21512;&#20989;&#25968;Soft Median&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;GNNs&#30340;&#21487;&#38752;&#24615;&#21644;&#25915;&#20987;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#21463;&#27426;&#36814;&#31243;&#24230;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#29616;&#26377;&#30740;&#31350;&#21482;&#20381;&#36182;&#20110;&#30456;&#23545;&#36739;&#23567;&#30340;&#22270;&#24418;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35268;&#27169;&#19979;&#25915;&#20987;&#21644;&#38450;&#24481;GNNs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31232;&#30095;&#24863;&#30693;&#30340;&#19968;&#38454;&#20248;&#21270;&#25915;&#20987;&#65292;&#23613;&#31649;&#20248;&#21270;&#21442;&#25968;&#25968;&#37327;&#19982;&#33410;&#28857;&#25968;&#37327;&#20108;&#27425;&#20851;&#32852;&#65292;&#20294;&#20173;&#20445;&#25345;&#39640;&#25928;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#20844;&#20849;&#20195;&#29702;&#25439;&#22833;&#19981;&#36866;&#21512;&#29992;&#20110;&#20840;&#23616;&#25915;&#20987;GNNs&#65292;&#32780;&#25105;&#20204;&#30340;&#26367;&#20195;&#26041;&#26696;&#21487;&#20197;&#23558;&#25915;&#20987;&#21147;&#32763;&#20493;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640;GNNs&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#40065;&#26834;&#24615;&#32858;&#21512;&#20989;&#25968;&#65292;Soft Median&#65292;&#24471;&#21040;&#20102;&#22312;&#25152;&#26377;&#35268;&#27169;&#19979;&#30340;&#26377;&#25928;&#38450;&#24481;&#12290;&#25105;&#20204;&#20197;&#26631;&#20934;GNNs&#20026;&#22522;&#30784;&#65292;&#23545;&#27604;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#26041;&#27861;&#22312;100&#20493;&#20197;&#19978;&#30340;&#22270;&#24418;&#19978;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#29978;&#33267;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#25193;&#23637;&#21040;&#21487;&#25193;&#23637;&#30340;GNN&#65292;&#23558;&#35268;&#27169;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#20803;&#32032;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2109.05872</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#21516;&#24694;&#24847;&#26799;&#24230;&#31579;&#36873;&#23454;&#29616;&#25308;&#21344;&#24237;&#23481;&#38169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filtering. (arXiv:2109.05872v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.05872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36880;&#20803;&#32032;&#31526;&#21495;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25308;&#21344;&#24237;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#22522;&#20110;&#26799;&#24230;&#30340;&#35757;&#32451;&#23481;&#26131;&#21463;&#21040;&#25925;&#38556;/&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#34987;&#31216;&#20026;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#12290;&#29616;&#26377;&#24037;&#20316;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35201;&#20040;&#21033;&#29992;&#20013;&#22830;&#21442;&#25968;&#26381;&#21153;&#22120;&#30340;&#36741;&#21161;&#25968;&#25454;&#26469;&#39564;&#35777;&#25509;&#25910;&#21040;&#30340;&#26799;&#24230;(&#20363;&#22914;&#35745;&#31639;&#39564;&#35777;&#38169;&#35823;&#29575;)&#65292;&#35201;&#20040;&#21033;&#29992;&#22522;&#20110;&#32479;&#35745;&#23398;&#26041;&#27861;(&#20363;&#22914;&#20013;&#20301;&#25968;&#21644;Krum)&#26469;&#35782;&#21035;&#24182;&#20174;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#20013;&#21024;&#38500;&#24694;&#24847;&#26799;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#36741;&#21161;&#25968;&#25454;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#19987;&#27880;&#20110;&#22522;&#20110;&#32479;&#35745;&#23398;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31934;&#24515;&#21046;&#20316;&#30340;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#21487;&#20197;&#35268;&#36991;&#22823;&#22810;&#25968;&#22522;&#20110;&#20013;&#20301;&#25968;&#21644;&#36317;&#31163;&#30340;&#32479;&#35745;&#38450;&#24481;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#24694;&#24847;&#26799;&#24230;&#19982;&#35802;&#23454;&#26799;&#24230;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;&#26799;&#24230;&#21521;&#37327;&#30340;&#36880;&#20803;&#32032;&#31526;&#21495;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#26816;&#27979;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gradient-based training in federated learning is known to be vulnerable to faulty/malicious clients, which are often modeled as Byzantine clients. To this end, previous work either makes use of auxiliary data at parameter server to verify the received gradients (e.g., by computing validation error rate) or leverages statistic-based methods (e.g. median and Krum) to identify and remove malicious gradients from Byzantine clients. In this paper, we remark that auxiliary data may not always be available in practice and focus on the statistic-based approach. However, recent work on model poisoning attacks has shown that well-crafted attacks can circumvent most of median- and distance-based statistical defense methods, making malicious gradients indistinguishable from honest ones. To tackle this challenge, we show that the element-wise sign of gradient vector can provide valuable insight in detecting model poisoning attacks. Based on our theoretical analysis of the \textit{Little is Enough} 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SIMI&#65293;ClusterGAN &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#25913;&#36827; ClusterGAN &#30340;&#32858;&#31867;&#25928;&#26524;&#65292;&#20174;&#32780;&#23398;&#20064;&#29420;&#29305;&#20808;&#39564;&#19982;&#30495;&#23454;&#20998;&#24067;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2107.12706</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#25913;&#36827;ClusterGAN&#30340;&#32858;&#31867;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improving ClusterGAN Using Self-Augmented Information Maximization of Disentangling Latent Spaces. (arXiv:2107.12706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SIMI&#65293;ClusterGAN &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#26469;&#25913;&#36827; ClusterGAN &#30340;&#32858;&#31867;&#25928;&#26524;&#65292;&#20174;&#32780;&#23398;&#20064;&#29420;&#29305;&#20808;&#39564;&#19982;&#30495;&#23454;&#20998;&#24067;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#26080;&#30417;&#30563;&#26465;&#20214;&#29983;&#25104;&#19982;&#32858;&#31867;&#25512;&#26029;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;ClusterGAN &#26368;&#36817;&#33021;&#22815;&#23454;&#29616;&#20196;&#20154;&#30633;&#30446;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24573;&#30053;&#20102;&#25968;&#25454;&#30340;&#30495;&#23454;&#26465;&#20214;&#20998;&#24067;&#65292;&#32858;&#31867;&#25512;&#26029;&#32593;&#32476;&#21482;&#33021;&#36890;&#36807;&#32771;&#34385;&#22522;&#20110;&#22343;&#21248;&#20808;&#39564;&#30340;&#29983;&#25104;&#26679;&#26412;&#26469;&#36798;&#21040;&#36739;&#24046;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#22686;&#24378;&#20449;&#24687;&#26368;&#22823;&#21270;&#25913;&#36827;&#30340;ClusterGAN (SIMI-ClusterGAN)&#65292;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#23398;&#20064;&#19982;&#30495;&#23454;&#20998;&#24067;&#21305;&#37197;&#30340;&#29420;&#29305;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since their introduction in the last few years, conditional generative models have seen remarkable achievements. However, they often need the use of large amounts of labelled information. By using unsupervised conditional generation in conjunction with a clustering inference network, ClusterGAN has recently been able to achieve impressive clustering results. Since the real conditional distribution of data is ignored, the clustering inference network can only achieve inferior clustering performance by considering only uniform prior based generative samples. However, the true distribution is not necessarily balanced. Consequently, ClusterGAN fails to produce all modes, which results in sub-optimal clustering inference network performance. So, it is important to learn the prior, which tries to match the real distribution in an unsupervised way. In this paper, we propose self-augmentation information maximization improved ClusterGAN (SIMI-ClusterGAN) to learn the distinctive priors from th
&lt;/p&gt;</description></item><item><title>SelfCF&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#65292;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#36755;&#20986;&#30340;&#23884;&#20837;&#26469;&#31616;&#21270;&#31639;&#27861;&#20197;&#21450;&#36991;&#20813;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2107.03019</link><description>&lt;p&gt;
SelfCF&#65306;&#19968;&#31181;&#31616;&#21333;&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.03019
&lt;/p&gt;
&lt;p&gt;
SelfCF&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#33616;&#22330;&#26223;&#65292;&#36890;&#36807;&#22686;&#24378;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#21327;&#21516;&#36807;&#28388;&#27169;&#22411;&#20013;&#36755;&#20986;&#30340;&#23884;&#20837;&#26469;&#31616;&#21270;&#31639;&#27861;&#20197;&#21450;&#36991;&#20813;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#28508;&#22312;&#30340;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#20174;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;CF&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#36127;&#37319;&#26679;&#26469;&#21306;&#20998;&#19981;&#21516;&#30340;&#39033;&#30446;&#12290;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#36127;&#37319;&#26679;&#36827;&#34892;&#35757;&#32451;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#24517;&#39035;&#26681;&#25454;&#23450;&#20041;&#30340;&#20998;&#24067;&#35880;&#24910;&#36873;&#25321;&#36127;&#39033;&#65292;&#20197;&#36991;&#20813;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#35266;&#23519;&#21040;&#30340;&#27491;&#39033;&#12290;&#19981;&#21487;&#36991;&#20813;&#22320;&#65292;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#37319;&#26679;&#30340;&#19968;&#20123;&#36127;&#39033;&#22312;&#27979;&#35797;&#38598;&#20013;&#21487;&#33021;&#26159;&#27491;&#39033;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#38544;&#24335;&#21453;&#39304;&#25512;&#33616;&#22330;&#26223;&#30340;&#33258;&#30417;&#30563;&#21327;&#21516;&#36807;&#28388;&#26694;&#26550;&#65288;SelfCF&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;SelfCF&#26694;&#26550;&#31616;&#21270;&#20102;&#36830;&#20307;&#32593;&#32476;&#65292;&#24182;&#21487;&#36731;&#26494;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;CF&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;&#39592;&#24178;&#32593;&#32476;&#12290;SelfCF&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#22686;&#24378;&#30001;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#30340;&#36755;&#20986;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is widely used to learn informative latent representations of users and items from observed interactions. Existing CF-based methods commonly adopt negative sampling to discriminate different items. Training with negative sampling on large datasets is computationally expensive. Further, negative items should be carefully sampled under the defined distribution, in order to avoid selecting an observed positive item in the training dataset. Unavoidably, some negative items sampled from the training dataset could be positive in the test set. In this paper, we propose a self-supervised collaborative filtering framework (SelfCF), that is specially designed for recommender scenario with implicit feedback. The proposed SelfCF framework simplifies the Siamese networks and can be easily applied to existing deep-learning based CF models, which we refer to as backbone networks. The main idea of SelfCF is to augment the output embeddings generated by backbone networks, b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#20013;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#22914;&#26524;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#34920;&#31034;&#36275;&#22815;&#23485;&#65292;&#21017;&#20854;&#31070;&#32463;&#20803;&#20542;&#21521;&#20110;&#20998;&#25104;&#25658;&#24102;&#30456;&#21516;&#20449;&#24687;&#30340;&#32452;&#65292;&#32780;&#20887;&#20313;&#34920;&#31034;&#26377;&#21161;&#20110;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2106.03485</link><description>&lt;p&gt;
&#20887;&#20313;&#34920;&#31034;&#23545;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#26377;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#20013;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#22914;&#26524;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#34920;&#31034;&#36275;&#22815;&#23485;&#65292;&#21017;&#20854;&#31070;&#32463;&#20803;&#20542;&#21521;&#20110;&#20998;&#25104;&#25658;&#24102;&#30456;&#21516;&#20449;&#24687;&#30340;&#32452;&#65292;&#32780;&#20887;&#20313;&#34920;&#31034;&#26377;&#21161;&#20110;&#23485;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25171;&#30772;&#20102;&#32463;&#20856;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#65306;&#20026;DNN&#28155;&#21152;&#21442;&#25968;&#20197;&#25554;&#20540;&#20854;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#20250;&#25913;&#21892;&#20854;&#27867;&#21270;&#24615;&#33021;&#12290;&#35299;&#37322;&#22312;&#28145;&#24230;&#32593;&#32476;&#20013;&#8220;&#33391;&#24615;&#36807;&#25311;&#21512;&#8221;&#30340;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#23618;&#34920;&#31034;&#65292;&#24182;&#21457;&#29616;&#22914;&#26524;&#26368;&#21518;&#19968;&#20010;&#38544;&#34255;&#34920;&#31034;&#36275;&#22815;&#23485;&#65292;&#21017;&#20854;&#31070;&#32463;&#20803;&#20542;&#21521;&#20110;&#20998;&#25104;&#25658;&#24102;&#30456;&#21516;&#20449;&#24687;&#30340;&#32452;&#65292;&#20165;&#30001;&#32479;&#35745;&#29420;&#31435;&#22122;&#22768;&#21306;&#20998;&#24444;&#27492;&#12290;&#36825;&#31181;&#32452;&#30340;&#25968;&#37327;&#38543;&#23618;&#30340;&#23485;&#24230;&#21576;&#32447;&#24615;&#22686;&#21152;&#65292;&#20294;&#20165;&#22312;&#23485;&#24230;&#39640;&#20110;&#20020;&#30028;&#20540;&#26102;&#25165;&#20250;&#22686;&#21152;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20887;&#20313;&#31070;&#32463;&#20803;&#20165;&#22312;&#35757;&#32451;&#36807;&#31243;&#36798;&#21040;&#25554;&#20540;&#19988;&#35757;&#32451;&#35823;&#24046;&#20026;&#38646;&#26102;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) defy the classical bias-variance trade-off: adding parameters to a DNN that interpolates its training data will typically improve its generalization performance. Explaining the mechanism behind this ``benign overfitting'' in deep networks remains an outstanding challenge. Here, we study the last hidden layer representations of various state-of-the-art convolutional neural networks and find that if the last hidden representation is wide enough, its neurons tend to split into groups that carry identical information, and differ from each other only by statistically independent noise. The number of such groups increases linearly with the width of the layer, but only if the width is above a critical value. We show that redundant neurons appear only when the training process reaches interpolation and the training error is zero.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27531;&#24046;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20869;&#37096;&#24341;&#20837;&#20102;&#25193;&#25955;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#30452;&#24452;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31867;&#38388;&#28857;&#30340;&#21487;&#20998;&#24615;&#65292;&#20943;&#23569;&#20102;&#31867;&#20869;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2105.03155</link><description>&lt;p&gt;
&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25193;&#25955;&#26426;&#21046;&#65306;&#29702;&#35770;&#19982;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion Mechanism in Residual Neural Network: Theory and Applications. (arXiv:2105.03155v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.03155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#25955;&#27531;&#24046;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20869;&#37096;&#24341;&#20837;&#20102;&#25193;&#25955;&#26426;&#21046;&#65292;&#33021;&#22815;&#25552;&#39640;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#30452;&#24452;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31867;&#38388;&#28857;&#30340;&#21487;&#20998;&#24615;&#65292;&#20943;&#23569;&#20102;&#31867;&#20869;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#26159;&#35768;&#22810;&#29289;&#29702;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#22522;&#26412;&#20869;&#37096;&#26426;&#21046;&#65292;&#25551;&#36848;&#20102;&#19981;&#21516;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22312;&#35768;&#22810;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#26679;&#26412;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#25193;&#25955;&#36830;&#25509;&#20102;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#65292;&#26159;&#23454;&#29616;&#39640;&#20998;&#31867;&#31934;&#24230;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#30452;&#25509;&#26045;&#21152;&#34701;&#21512;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#23545;&#27969;-&#25193;&#25955;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#27531;&#24046;&#32593;&#32476;&#65288;Diff-ResNet&#65289;&#65292;&#20869;&#37096;&#23558;&#25193;&#25955;&#24341;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#20013;&#12290;&#22312;&#20551;&#23450;&#20855;&#26377;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25193;&#25955;&#22359;&#21487;&#20197;&#22686;&#21152;&#36317;&#31163;&#30452;&#24452;&#27604;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#31867;&#38388;&#28857;&#30340;&#21487;&#20998;&#24615;&#24182;&#20943;&#23569;&#20102;&#23616;&#37096;&#31867;&#20869;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#24615;&#36136;&#21487;&#20197;&#34987;&#27531;&#24046;&#32593;&#32476;&#36731;&#26494;&#37319;&#29992;&#20197;&#26500;&#24314;&#21487;&#20998;&#31163;&#30340;&#36229;&#24179;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion, a fundamental internal mechanism emerging in many physical processes, describes the interaction among different objects. In many learning tasks with limited training samples, the diffusion connects the labeled and unlabeled data points and is a critical component for achieving high classification accuracy. Many existing deep learning approaches directly impose the fusion loss when training neural networks. In this work, inspired by the convection-diffusion ordinary differential equations (ODEs), we propose a novel diffusion residual network (Diff-ResNet), internally introduces diffusion into the architectures of neural networks. Under the structured data assumption, it is proved that the proposed diffusion block can increase the distance-diameter ratio that improves the separability of inter-class points and reduces the distance among local intra-class points. Moreover, this property can be easily adopted by the residual networks for constructing the separable hyperplanes. E
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2103.07295</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.07295
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26679;&#24615;&#20445;&#25345;&#30340;&#22270;&#32467;&#26500;&#32454;&#21270;&#30340;&#22270;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#24050;&#23398;&#20064;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30495;&#23454;&#30340;&#22270;&#25968;&#25454;&#65292;&#33410;&#28857;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#30828;&#24615;&#20108;&#36827;&#21046;&#38142;&#25509;&#12290;&#26174;&#28982;&#65292;&#36825;&#26159;&#19968;&#31181;&#31163;&#25955;&#21644;&#31616;&#21270;&#30340;&#36830;&#32493;&#20851;&#31995;&#24418;&#24335;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23398;&#20064;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#30340;&#21487;&#34920;&#36798;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#33719;&#24471;&#30340;&#33410;&#28857;&#34920;&#31034;&#21487;&#20197;&#21453;&#36807;&#26469;&#25581;&#31034;&#33410;&#28857;&#20043;&#38388;&#30340;&#20869;&#22312;&#20851;&#31995;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29305;&#24449;&#21270;&#33410;&#28857;&#20851;&#31995;&#24182;&#36827;&#19968;&#27493;&#20419;&#36827;&#33410;&#28857;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#19968;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#23884;&#20837;&#30340;&#33410;&#28857;&#34920;&#31034;&#26469;&#32454;&#21270;&#26368;&#21021;&#32473;&#23450;&#30340;&#22270;&#32467;&#26500;&#12290;&#20294;&#26159;&#65292;&#20840;&#23616;&#32454;&#21270;&#25152;&#26377;&#33410;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#26080;&#27861;&#21306;&#20998;&#23558;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#19968;&#20123;&#22122;&#22768;&#36793;&#32536;&#65292;&#36825;&#21487;&#33021;&#36827;&#19968;&#27493;&#28151;&#28102;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#22270;&#24418;&#19978;&#20063;&#23384;&#22312;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23616;&#37096;&#32467;&#26500;&#24863;&#30693;&#30340;&#22270;&#24418;&#32454;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#32463;&#23398;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#36880;&#27493;&#25913;&#21892;&#22270;&#24418;&#32467;&#26500;&#36136;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#22270;&#20013;&#30340;&#38543;&#26426;&#28216;&#36208;&#27169;&#25311;&#29983;&#25104;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#37051;&#22495;&#32467;&#26500;&#38598;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#27599;&#20010;&#27169;&#25311;&#37051;&#22495;&#65292;&#25105;&#20204;&#22312;&#25972;&#20010;&#32454;&#21270;&#36807;&#31243;&#20013;&#20445;&#25345;&#37051;&#22495;&#32467;&#26500;&#30340;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#32454;&#21270;&#37051;&#22495;&#20869;&#30340;&#33410;&#28857;&#20851;&#31995;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#65292;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#22270;&#32858;&#31867;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#21452;&#37325;&#31283;&#20581;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;DR Thompson Sampling&#65289;&#65292;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#36807;&#21435;&#30340;&#19978;&#19979;&#25991;&#21644;&#22870;&#21169;&#23545;&#36873;&#25321;&#20381;&#36182;&#24615;&#23548;&#33268;&#30340;&#25439;&#22833;&#20998;&#35299;&#22797;&#26434;&#31561;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31616;&#21270;&#19988;&#25913;&#36827;&#30340;&#25439;&#22833;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2102.01229</link><description>&lt;p&gt;
&#32447;&#24615;&#22238;&#25253;&#30340;&#21452;&#37325;&#31283;&#20581;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Doubly robust Thompson sampling for linear payoffs. (arXiv:2102.01229v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.01229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#21452;&#37325;&#31283;&#20581;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;DR Thompson Sampling&#65289;&#65292;&#36890;&#36807;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#36807;&#21435;&#30340;&#19978;&#19979;&#25991;&#21644;&#22870;&#21169;&#23545;&#36873;&#25321;&#20381;&#36182;&#24615;&#23548;&#33268;&#30340;&#25439;&#22833;&#20998;&#35299;&#22797;&#26434;&#31561;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31616;&#21270;&#19988;&#25913;&#36827;&#30340;&#25439;&#22833;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27748;&#26222;&#26862;&#25277;&#26679;&#22312;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20309;&#26681;&#25454;&#36807;&#21435;&#30340;&#19978;&#19979;&#25991;&#21644;&#22870;&#21169;&#23545;&#36827;&#34892;&#36873;&#25321;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;&#21518;&#24724;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#37325;&#31283;&#20581;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;DR Thompson Sampling&#65289;&#30340;&#26032;&#22411;&#22810;&#33218;&#19978;&#19979;&#25991;&#36172;&#21338;&#31639;&#27861;&#65292;&#37319;&#29992;&#22312;&#32570;&#22833;&#25968;&#25454;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#21452;&#37325;&#31283;&#20581;&#20272;&#35745;&#22120;&#29992;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27748;&#26222;&#26862;&#25277;&#26679;&#65288;LinTS&#65289;&#12290;&#21452;&#37325;&#31283;&#20581;&#27748;&#26222;&#26862;&#25277;&#26679;&#35753;&#25439;&#22833;&#20998;&#35299;&#26356;&#21152;&#31616;&#21333;&#65292;&#20174;&#32780;&#20351;&#24471;&#25913;&#36827;&#21518;&#30340;&#25439;&#22833;&#30028;&#38480;&#38477;&#21040;&#20102; $\tilde{O}(\phi^{-2}\sqrt{T})$&#65292;&#20854;&#20013; $\phi^2$ &#26159;&#19978;&#19979;&#25991;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#12290;&#36825;&#26159; \texttt{LinTS} &#31532;&#19968;&#20010;&#19981;&#22522;&#20110;&#19978;&#19979;&#25991;&#32500;&#24230; $d$&#65292;&#32780;&#26159;&#20351;&#29992; $\phi^2$ &#30340;&#25439;&#22833;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. The dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis. We propose a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (\texttt{LinTS}). Different from previous works relying on missing data techniques (\citet{dimakopoulou2019balanced}, \citet{kim2019doubly}), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of $\tilde{O}(\phi^{-2}\sqrt{T})$, where $\phi^2$ is the minimum eigenvalue of the covariance matrix of contexts. This is the first regret bound of \texttt{LinTS} using $\phi^2$ without the dimension of the context, $d$. Applying the relationship be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;10&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;4&#20010;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#65292;&#35828;&#26126;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2012.02334</link><description>&lt;p&gt;
&#22522;&#20934;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23398;&#20064;&#21160;&#21147;&#23398;&#25968;&#25454;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. (arXiv:2012.02334v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.02334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;10&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#22312;4&#20010;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#65292;&#35828;&#26126;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#35774;&#35745;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#20316;&#20026;&#24402;&#32435;&#20559;&#32622;&#24341;&#20837;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#22312;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;&#35266;&#27979;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#23398;&#26041;&#31243;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21253;&#25324;HNN&#12289;LNN&#12289;DeLaN&#12289;SymODEN&#12289;CHNN&#12289;CLNN&#21450;&#20854;&#21464;&#20307;&#22312;&#20869;&#30340;10&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#33021;&#37327;&#23432;&#24658;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#31616;&#27905;&#28436;&#32462;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;4&#20010;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25351;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#33021;&#37327;&#23432;&#24658;&#27169;&#22411;&#35774;&#35745;&#22522;&#20110;&#33021;&#37327;&#30340;&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we survey ten recently proposed energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN, CHNN, CLNN and their variants. We provide a compact derivation of the theory behind these models and explain their similarities and differences. Their performance are compared in 4 physical systems. We point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20998;&#31867;&#27169;&#22359;&#30340;&#36845;&#20195;&#21327;&#20316;&#22312;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#19979;&#20934;&#30830;&#35782;&#21035;&#20986;&#23545;WSN&#29305;&#24449;&#24433;&#21709;&#26356;&#22823;&#30340;&#20851;&#38190;&#33410;&#28857;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#38750;&#20851;&#38190;&#33410;&#28857;&#30340;&#35782;&#21035;&#20559;&#24046;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;WSN&#20013;&#12290;</title><link>http://arxiv.org/abs/2004.08885</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#20851;&#38190;&#33410;&#28857;&#35782;&#21035;&#30340;&#26377;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65288;arXiv:2004.08885v4 [cs.NI] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
A supervised active learning method for identifying critical nodes in Wireless Sensor Network. (arXiv:2004.08885v4 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.08885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32858;&#31867;&#21644;&#20998;&#31867;&#27169;&#22359;&#30340;&#36845;&#20195;&#21327;&#20316;&#22312;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#19979;&#20934;&#30830;&#35782;&#21035;&#20986;&#23545;WSN&#29305;&#24449;&#24433;&#21709;&#26356;&#22823;&#30340;&#20851;&#38190;&#33410;&#28857;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#38024;&#23545;&#38750;&#20851;&#38190;&#33410;&#28857;&#30340;&#35782;&#21035;&#20559;&#24046;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;WSN&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#30340;&#33021;&#28304;&#25928;&#29575;&#21462;&#20915;&#20110;&#20854;&#20027;&#35201;&#29305;&#24449;&#65292;&#21253;&#25324;&#36339;&#25968;&#12289;&#29992;&#25143;&#20301;&#32622;&#12289;&#20998;&#37197;&#21151;&#29575;&#21644;&#20013;&#32487;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#23545;&#36825;&#20123;&#29305;&#24449;&#24433;&#21709;&#26356;&#22823;&#30340;&#33410;&#28857;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;WSN&#20013;&#35782;&#21035;&#20851;&#38190;&#33410;&#28857;&#30340;&#35745;&#31639;&#24320;&#38144;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#35782;&#21035;&#38750;&#20851;&#38190;&#33410;&#28857;&#30340;&#20559;&#24046;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#31934;&#35843;&#24037;&#20316;&#26469;&#36866;&#24212;WSN&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32858;&#31867;&#21644;&#20998;&#31867;&#27169;&#22359;&#30340;&#21327;&#20316;&#36845;&#20195;&#22320;&#20943;&#23569;&#20856;&#22411;&#30417;&#30563;&#23398;&#20064;&#22330;&#26223;&#20013;&#25152;&#38656;&#30340;&#25968;&#25454;&#25968;&#37327;&#65292;&#24182;&#22312;&#23384;&#22312;&#38750;&#20449;&#24687;&#24615;&#31034;&#20363;&#65292;&#21363;&#38750;&#20851;&#38190;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#29992;&#20110;&#22823;&#35268;&#27169;WSN&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy Efficiency of a wireless sensor network (WSN) relies on its main characteristics, including hop-number, user's location, allocated power, and relay. Identifying nodes, which have more impact on these characteristics, is, however, subject to a substantial computational overhead and energy consumption. In this paper, we proposed an active learning approach to address the computational overhead of identifying critical nodes in a WSN. The proposed approach can overcome biasing in identifying non-critical nodes and needs much less effort in fine-tuning to adapt to the dynamic nature of WSN. This method benefits from the cooperation of clustering and classification modules to iteratively decrease the required number of data in a typical supervised learning scenario and to increase the accuracy in the presence of uninformative examples, i.e., non-critical nodes. Experiments show that the proposed method has more flexibility, compared to the state-of-the-art, to be employed in large sca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#21644;&#36830;&#32493;&#26494;&#24347;&#31639;&#27861;&#26469;&#35299;&#20915;&#26368;&#22823;&#29109;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#25928;&#29575;&#26356;&#39640;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#36817;&#20284;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#36817;&#20284;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2001.08537</link><description>&lt;p&gt;
&#26368;&#22823;&#29109;&#37319;&#26679;&#38382;&#39064;&#30340;&#26368;&#20339;&#20027;&#23376;&#30697;&#38453;&#36873;&#25321;&#65306;&#21487;&#25193;&#23637;&#31639;&#27861;&#21644;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best Principal Submatrix Selection for the Maximum Entropy Sampling Problem: Scalable Algorithms and Performance Guarantees. (arXiv:2001.08537v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2001.08537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#21644;&#36830;&#32493;&#26494;&#24347;&#31639;&#27861;&#26469;&#35299;&#20915;&#26368;&#22823;&#29109;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#25928;&#29575;&#26356;&#39640;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#24050;&#26377;&#30340;&#36817;&#20284;&#30028;&#38480;&#65292;&#24182;&#35777;&#26126;&#20102;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#36817;&#20284;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32463;&#20856;&#30340;&#26368;&#22823;&#29109;&#37319;&#26679;&#38382;&#39064;&#65288;MESP&#65289;&#65292;&#23427;&#26088;&#22312;&#20174;&#21327;&#26041;&#24046;&#30697;&#38453;&#20013;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20027;&#23376;&#30697;&#38453;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#21644;&#21407;&#22987;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MESP&#25972;&#25968;&#35268;&#21010;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#36830;&#32493;&#26494;&#24347;&#21487;&#24471;&#36817;&#20284;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#30830;&#23450;&#24615;&#37319;&#26679;&#31639;&#27861;&#65292;&#24182;&#35828;&#26126;&#20854;&#22312;&#36817;&#20284;&#30028;&#26041;&#38754;&#20248;&#20110;&#24050;&#26377;&#25991;&#29486;&#20013;&#30340;&#26368;&#20339;&#30028;&#38480;&#12290;&#36890;&#36807;&#23545;&#22855;&#24322;&#30697;&#38453;&#24320;&#21457;&#26032;&#30340;&#25968;&#23398;&#24037;&#20855;&#21644;&#20998;&#26512;&#25152;&#25552;&#20986;&#30340;&#20984;&#25972;&#25968;&#35268;&#21010;&#30340;&#25289;&#26684;&#26391;&#26085;&#23545;&#20598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#39318;&#20010;&#36817;&#20284;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a classic maximum entropy sampling problem (MESP), which aims to select the most informative principal submatrix of a prespecified size from a covariance matrix. MESP has been widely applied to many areas, including healthcare, power system, manufacturing and data science. By investigating its Lagrangian dual and primal characterization, we derive a novel convex integer program for MESP and show that its continuous relaxation yields a near-optimal solution. The results motivate us to study an efficient sampling algorithm and develop its approximation bound for MESP, which improves the best-known bound in literature. We then provide an efficient deterministic implementation of the sampling algorithm with the same approximation bound. By developing new mathematical tools for the singular matrices and analyzing the Lagrangian dual of the proposed convex integer program, we investigate the widely-used local search algorithm and prove its first-known approximation bound f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;IMAE&#27169;&#22411;&#29992;&#20110;&#30072;&#24418;&#35757;&#32451;&#25968;&#25454;&#30340;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#36341;&#35777;&#23454;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#22312;&#22788;&#29702;&#31034;&#20363;&#26102;&#23384;&#22312;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#21033;&#29992;&#21152;&#26435;&#26041;&#24046;&#35843;&#25972;&#25552;&#39640;&#20102;&#25311;&#21512;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/1903.12141</link><description>&lt;p&gt;
IMAE&#29992;&#20110;&#22122;&#22768;&#40065;&#26834;&#23398;&#20064;&#65306;&#32477;&#23545;&#20540;&#35823;&#24046;&#19981;&#24179;&#31561;&#23545;&#24453;&#31034;&#20363;&#65292;&#26799;&#24230;&#22823;&#23567;&#30340;&#26041;&#24046;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters. (arXiv:1903.12141v10 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.12141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;IMAE&#27169;&#22411;&#29992;&#20110;&#30072;&#24418;&#35757;&#32451;&#25968;&#25454;&#30340;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#65292;&#36890;&#36807;&#23454;&#36341;&#35777;&#23454;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#22312;&#22788;&#29702;&#31034;&#20363;&#26102;&#23384;&#22312;&#27424;&#25311;&#21512;&#38382;&#39064;&#65292;&#21033;&#29992;&#21152;&#26435;&#26041;&#24046;&#35843;&#25972;&#25552;&#39640;&#20102;&#25311;&#21512;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#31034;&#20363;&#21152;&#26435;&#35282;&#24230;&#65292;&#21363;&#19982;&#23545;&#25968;&#30340;&#26799;&#24230;&#22823;&#23567;&#26469;&#30475;&#24453;&#30072;&#24418;&#35757;&#32451;&#25968;&#25454;&#30340;&#40065;&#26834;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#26377;&#20004;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;&#65288;1&#65289;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#24179;&#31561;&#22320;&#22788;&#29702;&#31034;&#20363;&#12290;&#25105;&#20204;&#38024;&#23545;MAE&#36827;&#34892;&#20102;&#26032;&#30340;&#35266;&#23519;&#21644;&#28145;&#20837;&#20998;&#26512;&#65292;&#29702;&#35770;&#35777;&#26126;&#20854;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20854;&#22312;&#23454;&#36341;&#20013;&#30340;&#27424;&#25311;&#21512;&#38382;&#39064;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;MAE&#30340;&#40065;&#26834;&#24615;&#26159;&#36890;&#36807;&#24378;&#35843;&#19981;&#30830;&#23450;&#31034;&#20363;&#32780;&#19981;&#26159;&#20687;&#21069;&#20154;&#30740;&#31350;&#20013;&#25152;&#22768;&#31216;&#30340;&#37027;&#26679;&#23545;&#24453;&#35757;&#32451;&#26679;&#26412;&#26469;&#23454;&#29616;&#30340;&#12290;&#65288;2&#65289;&#26799;&#24230;&#22823;&#23567;&#30340;&#26041;&#24046;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#22686;&#24378;MAE&#30340;&#25311;&#21512;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#40065;&#26834;&#24615;&#12290;&#22312;&#19981;&#25913;&#21464;MAE&#30340;&#25972;&#20307;&#21152;&#26435;&#26041;&#26696;&#65288;&#21363;&#21738;&#20123;&#31034;&#20363;&#33719;&#24471;&#26356;&#39640;&#30340;&#26435;&#37325;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20165;&#36890;&#36807;&#38750;&#32447;&#24615;&#22320;&#25913;&#21464;&#20854;&#21152;&#26435;&#26041;&#24046;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study robust deep learning against abnormal training data from the perspective of example weighting built in empirical loss functions, i.e., gradient magnitude with respect to logits, an angle that is not thoroughly studied so far. Consequently, we have two key findings: (1) Mean Absolute Error (MAE) Does Not Treat Examples Equally. We present new observations and insightful analysis about MAE, which is theoretically proved to be noise-robust. First, we reveal its underfitting problem in practice. Second, we analyse that MAE's noise-robustness is from emphasising on uncertain examples instead of treating training samples equally, as claimed in prior work. (2) The Variance of Gradient Magnitude Matters. We propose an effective and simple solution to enhance MAE's fitting ability while preserving its noise-robustness. Without changing MAE's overall weighting scheme, i.e., what examples get higher weights, we simply change its weighting variance non-linearly so that the i
&lt;/p&gt;</description></item></channel></rss>