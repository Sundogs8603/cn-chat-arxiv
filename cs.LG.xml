<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05666</link><description>&lt;p&gt;
&#23384;&#22312;&#23545;&#31216;&#24615;&#21644;&#29366;&#24577;&#25277;&#35937;&#30340;&#25919;&#31574;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Gradient Methods in the Presence of Symmetries and State Abstractions. (arXiv:2305.05666v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#65292;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#36827;&#34892;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#23398;&#20064;&#65292;&#26368;&#21518;&#23637;&#31034;&#20102;&#31639;&#27861;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#29615;&#22659;&#21644;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#39640;&#32500;&#24230;&#21644;&#22797;&#26434;&#38382;&#39064;&#65292;&#24378;&#21270;&#23398;&#20064;&#20381;&#38752;&#25277;&#35937;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#25277;&#35937;&#65292;&#24182;&#23558;MDP&#21516;&#24577;&#30340;&#23450;&#20041;&#25193;&#23637;&#21040;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#38024;&#23545;&#25277;&#35937;MDP&#30340;&#38543;&#26426;&#21644;&#30830;&#23450;&#24615;&#31574;&#30053;&#23548;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26799;&#24230;&#32467;&#26524;&#20801;&#35768;&#21033;&#29992;&#29615;&#22659;&#30340;&#36817;&#20284;&#23545;&#31216;&#24615;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#22522;&#20110;&#36825;&#20123;&#23450;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#33021;&#22815;&#21516;&#26102;&#23398;&#20064;&#31574;&#30053;&#21644;MDP&#21516;&#24577;&#26144;&#23556;&#65292;&#20351;&#29992;&#26494;&#25955;&#21452;&#20223;&#23556;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#20855;&#26377;&#36830;&#32493;&#23545;&#31216;&#24615;&#30340;&#29615;&#22659;&#65292;&#20197;&#36827;&#19968;&#27493;&#23637;&#31034;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#23384;&#22312;&#36825;&#20123;&#23545;&#31216;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21160;&#20316;&#25277;&#35937;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#29615;&#22659;&#20197;&#21450;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning on high-dimensional and complex problems relies on abstraction for improved efficiency and generalization. In this paper, we study abstraction in the continuous-control setting, and extend the definition of MDP homomorphisms to the setting of continuous state and action spaces. We derive a policy gradient theorem on the abstract MDP for both stochastic and deterministic policies. Our policy gradient results allow for leveraging approximate symmetries of the environment for policy optimization. Based on these theorems, we propose a family of actor-critic algorithms that are able to learn the policy and the MDP homomorphism map simultaneously, using the lax bisimulation metric. Finally, we introduce a series of environments with continuous symmetries to further demonstrate the ability of our algorithm for action abstraction in the presence of such symmetries. We demonstrate the effectiveness of our method on our environments, as well as on challenging visual contro
&lt;/p&gt;</description></item><item><title>ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05665</link><description>&lt;p&gt;
ImageBind:&#19968;&#20010;&#20849;&#21516;&#23884;&#20837;&#31354;&#38388;&#32465;&#23450;&#25152;&#26377;&#27169;&#24577;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ImageBind: One Embedding Space To Bind Them All. (arXiv:2305.05665v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05665
&lt;/p&gt;
&lt;p&gt;
ImageBind&#26159;&#19968;&#31181;&#26032;&#30340;&#36328;&#27169;&#24577;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#20351;&#29992;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#21487;&#20197;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#32465;&#23450;&#22312;&#19968;&#36215;&#65292;&#24182;&#23454;&#29616;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#32452;&#21512;&#21644;&#29983;&#25104;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ImageBind&#65292;&#36825;&#26159;&#19968;&#31181;&#36328;&#36234;&#22270;&#20687;&#12289;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#28145;&#24230;&#12289;&#28909;&#20256;&#24863;&#21644;IMU&#25968;&#25454;&#30340;&#20845;&#31181;&#19981;&#21516;&#27169;&#24577;&#30340;&#32852;&#21512;&#23884;&#20837;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#25152;&#26377;&#37197;&#23545;&#25968;&#25454;&#65292;&#21482;&#38656;&#35201;&#22270;&#20687;&#37197;&#23545;&#25968;&#25454;&#23601;&#36275;&#20197;&#23558;&#36825;&#20123;&#27169;&#24577;&#32465;&#23450;&#22312;&#19968;&#36215;&#12290;ImageBind&#21487;&#20197;&#21033;&#29992;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#23427;&#20204;&#19982;&#22270;&#20687;&#30340;&#33258;&#28982;&#37197;&#23545;&#65292;&#23558;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#25193;&#23637;&#21040;&#26032;&#30340;&#27169;&#24577;&#12290;&#23427;&#21487;&#20197;&#23454;&#29616;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;&#26032;&#22411;&#24212;&#29992;&#31243;&#24207;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#26816;&#32034;&#12289;&#29992;&#31639;&#26415;&#32452;&#21512;&#27169;&#24577;&#12289;&#36328;&#27169;&#24577;&#26816;&#27979;&#21644;&#29983;&#25104;&#12290;&#26032;&#22411;&#24212;&#29992;&#38543;&#30528;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#24378;&#24230;&#32780;&#19981;&#26029;&#25913;&#36827;&#65292;&#25105;&#20204;&#22312;&#36328;&#27169;&#24577;&#30340;&#38646;&#26679;&#26412;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20248;&#25104;&#32489;&#65292;&#36229;&#36807;&#20102;&#19987;&#23478;&#30417;&#30563;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24378;&#30340;&#20960;&#20309;&#35782;&#21035;&#32467;&#26524;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;ImageBind&#25104;&#20026;&#20102;&#35780;&#20272;&#35270;&#35273;&#27169;&#24577;&#32852;&#21512;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ImageBind, an approach to learn a joint embedding across six different modalities - images, text, audio, depth, thermal, and IMU data. We show that all combinations of paired data are not necessary to train such a joint embedding, and only image-paired data is sufficient to bind the modalities together. ImageBind can leverage recent large scale vision-language models, and extends their zero-shot capabilities to new modalities just by using their natural pairing with images. It enables novel emergent applications 'out-of-the-box' including cross-modal retrieval, composing modalities with arithmetic, cross-modal detection and generation. The emergent capabilities improve with the strength of the image encoder and we set a new state-of-the-art on emergent zero-shot recognition tasks across modalities, outperforming specialist supervised models. Finally, we show strong few-shot recognition results outperforming prior work, and that ImageBind serves as a new way to evaluate visio
&lt;/p&gt;</description></item><item><title>ShapeCoder&#26159;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22522;&#20803;&#20013;&#21457;&#29616;&#35270;&#35273;&#31243;&#24207;&#30340;&#25277;&#35937;&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#37325;&#20889;&#31243;&#24207;&#20197;&#20351;&#20854;&#26356;&#32039;&#20945;&#65292;&#26292;&#38706;&#33258;&#30001;&#24230;&#26356;&#23569;&#12290;&#22312;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;ShapeCoder&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31243;&#24207;&#22823;&#23567;&#20943;&#23569;&#20102;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.05661</link><description>&lt;p&gt;
ShapeCoder&#65306;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22522;&#20803;&#20013;&#21457;&#29616;&#35270;&#35273;&#31243;&#24207;&#30340;&#25277;&#35937;
&lt;/p&gt;
&lt;p&gt;
ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives. (arXiv:2305.05661v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05661
&lt;/p&gt;
&lt;p&gt;
ShapeCoder&#26159;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#30340;&#22522;&#20803;&#20013;&#21457;&#29616;&#35270;&#35273;&#31243;&#24207;&#30340;&#25277;&#35937;&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#37325;&#20889;&#31243;&#24207;&#20197;&#20351;&#20854;&#26356;&#32039;&#20945;&#65292;&#26292;&#38706;&#33258;&#30001;&#24230;&#26356;&#23569;&#12290;&#22312;&#23545;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;ShapeCoder&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31243;&#24207;&#22823;&#23567;&#20943;&#23569;&#20102;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31243;&#24207;&#26159;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#21487;&#35270;&#21270;&#25968;&#25454;&#34920;&#31034;&#26041;&#24335;&#65292;&#21487;&#20197;&#25581;&#31034;&#32039;&#20945;&#12289;&#21487;&#35299;&#37322;&#30340;&#32467;&#26500;&#65292;&#25903;&#25345;&#25805;&#20316;&#12290;&#35270;&#35273;&#31243;&#24207;&#36890;&#24120;&#20197;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;(DSLs)&#32534;&#20889;&#12290;&#25214;&#21040;&#8220;&#22909;&#8221;&#30340;&#31243;&#24207;&#65292;&#21363;&#20165;&#26292;&#38706;&#26377;&#24847;&#20041;&#30340;&#33258;&#30001;&#24230;&#65292;&#38656;&#35201;&#35775;&#38382;&#20855;&#26377;&#8220;&#22909;&#8221;&#20989;&#25968;&#24211;&#30340;DSLs&#65292;&#36825;&#20123;&#20989;&#25968;&#24211;&#36890;&#24120;&#30001;&#39046;&#22495;&#19987;&#23478;&#32534;&#20889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ShapeCoder&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25509;&#21463;&#24418;&#29366;&#25968;&#25454;&#38598;&#30340;&#31995;&#32479;&#65292;&#35813;&#25968;&#25454;&#38598;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#22522;&#20803;&#34920;&#31034;&#65292;&#24182;&#20849;&#21516;&#21457;&#29616;(i)&#26377;&#29992;&#30340;&#25277;&#35937;&#20989;&#25968;&#21644;(ii)&#20351;&#29992;&#36825;&#20123;&#25277;&#35937;&#26469;&#35299;&#37322;&#36755;&#20837;&#24418;&#29366;&#30340;&#31243;&#24207;&#12290;&#21457;&#29616;&#30340;&#25277;&#35937;&#25429;&#33719;&#25968;&#25454;&#38598;&#20013;&#30340;&#24120;&#35265;&#27169;&#24335;(&#32467;&#26500;&#21644;&#21442;&#25968;)&#65292;&#22240;&#27492;&#65292;&#20351;&#29992;&#36825;&#20123;&#25277;&#35937;&#37325;&#20889;&#30340;&#31243;&#24207;&#26356;&#32039;&#20945;&#65292;&#26292;&#38706;&#30340;&#33258;&#30001;&#24230;&#26356;&#23569;&#12290;ShapeCoder&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#25277;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#22312;&#26356;&#22797;&#26434;&#30340;&#36755;&#20837;&#19979;&#65292;&#21457;&#29616;&#26356;&#22909;&#30340;&#25277;&#35937;&#65292;&#22312;&#36739;&#23569;&#30340;&#20808;&#39564;&#30693;&#35782;&#19979;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ShapeCoder&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#65292;&#31243;&#24207;&#22823;&#23567;&#20943;&#23569;&#20102;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Programs are an increasingly popular representation for visual data, exposing compact, interpretable structure that supports manipulation. Visual programs are usually written in domain-specific languages (DSLs). Finding "good" programs, that only expose meaningful degrees of freedom, requires access to a DSL with a "good" library of functions, both of which are typically authored by domain experts. We present ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across the dataset, so that programs rewritten with these abstractions are more compact, and expose fewer degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05658</link><description>&lt;p&gt;
TidyBot: &#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#29289;&#29702;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#29289;&#29702;&#36741;&#21161;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#29992;&#25143;&#30340;&#20010;&#20154;&#21916;&#22909;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25441;&#36215;&#29289;&#21697;&#24182;&#23558;&#20854;&#25918;&#22238;&#21407;&#22788;&#26469;&#25972;&#29702;&#25151;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#29289;&#21697;&#30340;&#27491;&#30830;&#20301;&#32622;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#21916;&#22909;&#21487;&#20197;&#22240;&#20010;&#20154;&#21697;&#21619;&#25110;&#25991;&#21270;&#32972;&#26223;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#25277;&#23625;&#37324;&#65292;&#32780;&#21478;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#26550;&#23376;&#19978;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#29305;&#23450;&#20154;&#30340;&#20808;&#21069;&#20132;&#20114;&#23398;&#20064;&#36825;&#26679;&#30340;&#21916;&#22909;&#65292;&#32780;&#21482;&#38656;&#35201;&#20960;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#22522;&#20110;&#35821;&#35328;&#30340;&#35268;&#21010;&#21644;&#24863;&#30693;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#24191;&#27867;&#36866;&#29992;&#20110;&#26410;&#26469;&#20132;&#20114;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;91.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#65292;&#21487;&#22312;&#20302;&#25104;&#26412;&#19979;&#23545;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#36827;&#34892;&#22823;&#35268;&#27169;&#31579;&#26597;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#39118;&#38505;&#35780;&#20998;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.05648</link><description>&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning. (arXiv:2305.05648v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05648
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;&#21644;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#65292;&#21487;&#22312;&#20302;&#25104;&#26412;&#19979;&#23545;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#36827;&#34892;&#22823;&#35268;&#27169;&#31579;&#26597;&#65292;&#24182;&#36229;&#36234;&#20102;&#20256;&#32479;&#39118;&#38505;&#35780;&#20998;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;(CVDs)&#26159;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#26089;&#26399;&#27515;&#20129;&#29575;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#36825;&#20123;&#20154;&#32676;&#20013;&#65292;&#26089;&#26399; CVD &#26816;&#27979;&#21644;&#24178;&#39044;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#35768;&#22810;&#29616;&#26377;&#30340; CVD &#39118;&#38505;&#35780;&#20998;&#38656;&#35201;&#36523;&#20307;&#26816;&#26597;&#25110;&#23454;&#39564;&#23460;&#26816;&#27979;&#65292;&#36825;&#22312;&#36825;&#26679;&#30340;&#20581;&#24247;&#31995;&#32479;&#20013;&#20250;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#21463;&#38480;&#30340;&#21487;&#21450;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#26415;(PPG)&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#22823;&#22810;&#25968;&#26234;&#33021;&#25163;&#26426;&#19978;&#37117;&#21487;&#29992;&#30340;&#20256;&#24863;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#22823;&#35268;&#27169;&#31579;&#26597;&#65292;&#29992;&#20110; CVD &#39118;&#38505;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340; PPG-CVD &#39118;&#38505;&#35780;&#20998;(DLS)&#65292;&#20165;&#20351;&#29992;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#21560;&#28895;&#29366;&#24577;&#21644; PPG &#20316;&#20026;&#39044;&#27979;&#22240;&#23376;&#65292;&#39044;&#27979;&#26410;&#26469; 10 &#24180;&#21457;&#29983;&#37325;&#35201;&#30340;&#19981;&#33391;&#24515;&#34880;&#31649;&#20107;&#20214;(MACE&#65306;&#38750;&#33268;&#21629;&#24615;&#24515;&#32908;&#26775;&#22622;&#65292;&#20013;&#39118;&#21644;&#24515;&#34880;&#31649;&#27515;&#20129;) &#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#19982;&#21150;&#20844;&#23460;&#21046;&#23450;&#30340; refit-WHO &#35780;&#20998;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35813;&#35780;&#20998;&#37319;&#29992;&#20102; WHO &#21644; Globorisk &#35780;&#20998;&#30340;&#20849;&#20139;&#39044;&#27979;&#22240;&#23376;&#65288;&#24180;&#40836;&#65292;&#24615;&#21035;&#65292;&#21560;&#28895;&#29366;&#24577;&#65292;&#25910;&#32553;&#21387;&#21644;&#24635;&#32966;&#22266;&#37255;&#65289;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#21069;&#30651;&#24615;&#38431;&#21015;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20004;&#20010;&#38431;&#21015;&#20013;&#65292;DLS &#22312;&#39044;&#27979; MACE &#26041;&#38754;&#34920;&#29616;&#20248;&#20110; refit-WHO &#35780;&#20998;&#65292;&#20998;&#21035;&#20026;0.76&#21644;0.80&#65292;&#32780;&#21150;&#20844;&#23460;&#21046;&#23450;&#30340; refit-WHO &#35780;&#20998;&#20026;0.66&#21644;0.70&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25552;&#31034;&#65292;&#22522;&#20110; PPG &#30340; DLS &#20855;&#26377;&#28508;&#21147;&#24110;&#21161;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#26089;&#26399;&#20302;&#25104;&#26412;&#35782;&#21035; CVD &#39640;&#39118;&#38505;&#20010;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiovascular diseases (CVDs) are responsible for a large proportion of premature deaths in low- and middle-income countries. Early CVD detection and intervention is critical in these populations, yet many existing CVD risk scores require a physical examination or lab measurements, which can be challenging in such health systems due to limited accessibility. Here we investigated the potential to use photoplethysmography (PPG), a sensing technology available on most smartphones that can potentially enable large-scale screening at low cost, for CVD risk prediction. We developed a deep learning PPG-based CVD risk score (DLS) to predict the probability of having major adverse cardiovascular events (MACE: non-fatal myocardial infarction, stroke, and cardiovascular death) within ten years, given only age, sex, smoking status and PPG as predictors. We compared the DLS with the office-based refit-WHO score, which adopts the shared predictors from WHO and Globorisk scores (age, sex, smoking st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#20998;&#26512;&#30340;&#23545;&#20598;&#24615;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#19981;&#20250;&#21463;&#21040;&#32500;&#25968;&#28798;&#38590;&#30340;&#24433;&#21709;&#65292;&#20351; RFMs &#21487;&#20197;&#22312;&#26680;&#33539;&#22260;&#20043;&#22806;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05642</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#20998;&#26512;&#30340;&#23545;&#20598;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A duality framework for generalization analysis of random feature models and two-layer neural networks. (arXiv:2305.05642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#20998;&#26512;&#30340;&#23545;&#20598;&#24615;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#23398;&#20064;&#19981;&#20250;&#21463;&#21040;&#32500;&#25968;&#28798;&#38590;&#30340;&#24433;&#21709;&#65292;&#20351; RFMs &#21487;&#20197;&#22312;&#26680;&#33539;&#22260;&#20043;&#22806;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#39640;&#32500;&#20998;&#26512;&#20013;&#20986;&#29616;&#30340;&#33258;&#28982;&#20989;&#25968;&#31354;&#38388; $\mathcal{F}_{p,\pi}$ &#21644; Barron &#31354;&#38388;&#20013;&#23398;&#20064;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#20598;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#31354;&#38388;&#30340;&#36924;&#36817;&#21644;&#20272;&#35745;&#21487;&#20197;&#22312;&#26576;&#31181;&#24847;&#20041;&#19979;&#34987;&#35270;&#20026;&#31561;&#20215;&#30340;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#30740;&#31350;&#36825;&#20004;&#31181;&#27169;&#22411;&#30340;&#27867;&#21270;&#26102;&#26356;&#19987;&#27880;&#20110;&#26356;&#23481;&#26131;&#30340;&#36924;&#36817;&#21644;&#20272;&#35745;&#38382;&#39064;&#12290;&#36890;&#36807;&#23450;&#20041;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#30340;&#22797;&#26434;&#24230;&#26469;&#26377;&#25928;&#22320;&#25511;&#21046;&#20272;&#35745;&#35823;&#24046;&#65292;&#24314;&#31435;&#20102;&#23545;&#20598;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#20004;&#20010;&#20855;&#20307;&#24212;&#29992;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23545;&#20598;&#24615;&#26694;&#26550;&#30340;&#28789;&#27963;&#24615;&#12290;&#31532;&#19968;&#20010;&#24212;&#29992;&#26159;&#30740;&#31350;&#20351;&#29992; RFMs &#23398;&#20064; $\mathcal{F}_{p,\pi}$ &#20013;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#21482;&#35201; $p&gt;1$&#65292;&#23398;&#20064;&#19981;&#20250;&#21463;&#21040;&#32500;&#25968;&#28798;&#38590;&#30340;&#24433;&#21709;&#65292;&#36825;&#24847;&#21619;&#30528; RFMs &#21487;&#20197;&#22312;&#26680;&#33539;&#22260;&#20043;&#22806;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning functions in the $\mathcal{F}_{p,\pi}$ and Barron spaces, which are natural function spaces that arise in the high-dimensional analysis of random feature models (RFMs) and two-layer neural networks. Through a duality analysis, we reveal that the approximation and estimation of these spaces can be considered equivalent in a certain sense. This enables us to focus on the easier problem of approximation and estimation when studying the generalization of both models. The dual equivalence is established by defining an information-based complexity that can effectively control estimation errors. Additionally, we demonstrate the flexibility of our duality framework through comprehensive analyses of two concrete applications.  The first application is to study learning functions in $\mathcal{F}_{p,\pi}$ with RFMs. We prove that the learning does not suffer from the curse of dimensionality as long as $p&gt;1$, implying RFMs can work beyond the kernel regime. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#30340;&#26032;&#26041;&#27861;&#30452;&#25509;&#20174;&#38647;&#36798;&#20449;&#21495;&#30340; range-Doppler &#22320;&#22270;&#20013;&#20272;&#35745;&#31227;&#21160;&#30446;&#26631;&#30340;&#36317;&#31163;&#21644;&#36895;&#24230;&#12290;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604; (SNR) &#21306;&#38388;&#20869;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#20272;&#35745;&#31934;&#24230;&#21644;&#36739;&#30701;&#30340;&#39044;&#27979;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.05621</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#30446;&#26631;&#38647;&#36798;&#26816;&#27979;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-based Estimation for Multitarget Radar Detection. (arXiv:2305.05621v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#30340;&#26032;&#26041;&#27861;&#30452;&#25509;&#20174;&#38647;&#36798;&#20449;&#21495;&#30340; range-Doppler &#22320;&#22270;&#20013;&#20272;&#35745;&#31227;&#21160;&#30446;&#26631;&#30340;&#36317;&#31163;&#21644;&#36895;&#24230;&#12290;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604; (SNR) &#21306;&#38388;&#20869;&#65292;&#35813;&#26041;&#27861;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#20272;&#35745;&#31934;&#24230;&#21644;&#36739;&#30701;&#30340;&#39044;&#27979;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#21644;&#35782;&#21035;&#26159;&#26080;&#32447;&#29615;&#22659;&#20013;&#19968;&#20010;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#23384;&#22312;&#22823;&#37327;&#29289;&#20307;&#65292;&#26080;&#35770;&#26159;&#26377;&#25928;&#22320;&#30830;&#23450;&#23427;&#20204;&#30340;&#20301;&#32622;&#65292;&#36824;&#26159;&#35782;&#21035;&#23427;&#20204;&#24182;&#39044;&#27979;&#23427;&#20204;&#30340;&#31227;&#21160;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (CNN) &#30340;&#26032;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#26816;&#27979;&#20449;&#21495;&#30340; range-Doppler &#22320;&#22270;&#20013;&#20272;&#35745;&#31227;&#21160;&#30446;&#26631;&#30340;&#36317;&#31163;&#21644;&#36895;&#24230;&#12290;&#25105;&#20204;&#23558;&#25152;&#24471;&#32467;&#26524;&#19982;&#20108;&#32500; (2D) &#21608;&#26399;&#22270;&#65292;&#20197;&#21450;&#31867;&#20284;&#30340; 2DResFreq &#21644; VGG-19 &#32593;&#32476;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#25191;&#34892;&#30340;&#20272;&#35745;&#36807;&#31243;&#22312;&#19981;&#21516;&#20449;&#22122;&#27604; (SNR) &#21306;&#38388;&#20869;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#36317;&#31163;&#21644;&#36895;&#24230;&#25351;&#25968;&#20272;&#35745;&#31934;&#24230;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#39044;&#27979;&#26102;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23792;&#20540;&#20449;&#22122;&#27604; (PSNR) &#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#35813;&#25351;&#26631;&#26159;&#20998;&#26512;&#20174;&#21387;&#32553;&#25110;&#38477;&#22122;&#24471;&#21040;&#30340;&#36755;&#20986;&#22270;&#20687;&#36136;&#37327;&#30340;&#30456;&#20851;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Target detection and recognition is a very challenging task in a wireless environment where a multitude of objects are located, whether to effectively determine their positions or to identify them and predict their moves. In this work, we propose a new method based on a convolutional neural network (CNN) to estimate the range and velocity of moving targets directly from the range-Doppler map of the detected signals. We compare the obtained results to the two dimensional (2D) periodogram, and to the similar state of the art methods, 2DResFreq and VGG-19 network and show that the estimation process performed with our model provides better estimation accuracy of range and velocity index in different signal to noise ratio (SNR) regimes along with a reduced prediction time. Afterwards, we assess the performance of our proposed algorithm using the peak signal to noise ratio (PSNR) which is a relevant metric to analyse the quality of an output image obtained from compression or noise reductio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;&#8212;&#8212;&#22823;&#23567;&#65292;&#26469;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#30740;&#31350;&#20854;&#20869;&#37096;&#34920;&#31034;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#30830;&#23450;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26694;&#26550;&#21487;&#20316;&#20026;&#27867;&#21270;&#38169;&#35823;&#30340;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05611</link><description>&lt;p&gt;
&#24230;&#37327;&#31354;&#38388;&#22823;&#23567;&#21644;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Metric Space Magnitude and Generalisation in Neural Networks. (arXiv:2305.05611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#19981;&#21464;&#37327;&#8212;&#8212;&#22823;&#23567;&#65292;&#26469;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#30740;&#31350;&#20854;&#20869;&#37096;&#34920;&#31034;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#30830;&#23450;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#27492;&#26694;&#26550;&#21487;&#20316;&#20026;&#27867;&#21270;&#38169;&#35823;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#30340;&#20869;&#37096;&#24037;&#20316;&#36807;&#31243;&#20173;&#28982;&#26159;&#38590;&#20197;&#25417;&#25720;&#30340;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;&#8220;&#22823;&#23567;&#8221;&#30340;&#26032;&#25299;&#25169;&#19981;&#21464;&#37327;&#30340;&#35270;&#35282;&#26469;&#37327;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#22823;&#23567;&#26159;&#19968;&#31181;&#31561;&#36317;&#19981;&#21464;&#37327;&#65307;&#23427;&#30340;&#23646;&#24615;&#26159;&#30740;&#31350;&#30340;&#19968;&#20010;&#27963;&#36291;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#32534;&#30721;&#20102;&#24230;&#37327;&#31354;&#38388;&#20013;&#35768;&#22810;&#24050;&#30693;&#30340;&#19981;&#21464;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#26469;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#20869;&#37096;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#23427;&#20204;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;&#22823;&#23567;&#32500;&#24230;&#21644;&#27867;&#21270;&#38169;&#35823;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#23454;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25104;&#20026;&#27867;&#21270;&#38169;&#35823;&#30340;&#19968;&#20010;&#33391;&#22909;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have seen significant successes in numerous applications, but their inner workings remain elusive. The purpose of this work is to quantify the learning process of deep neural networks through the lens of a novel topological invariant called magnitude. Magnitude is an isometry invariant; its properties are an active area of research as it encodes many known invariants of a metric space. We use magnitude to study the internal representations of neural networks and propose a new method for determining their generalisation capabilities. Moreover, we theoretically connect magnitude dimension and the generalisation error, and demonstrate experimentally that the proposed framework can be a good indicator of the latter.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;&#25216;&#26415;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#30340;&#29942;&#39048;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05610</link><description>&lt;p&gt;
&#28857;&#20113;&#32593;&#32476;&#33021;&#23398;&#20064;&#35299;&#21078;&#32467;&#26500;&#30340;&#32479;&#35745;&#24418;&#24577;&#27169;&#22411;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can point cloud networks learn statistical shape models of anatomies?. (arXiv:2305.05610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28857;&#20113;&#30340;&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;&#25216;&#26415;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;&#26041;&#27861;&#30340;&#29942;&#39048;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#24418;&#24577;&#24314;&#27169;(SSM)&#26159;&#30740;&#31350;&#21644;&#37327;&#21270;&#20154;&#32676;&#20869;&#35299;&#21078;&#21464;&#21270;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#23545;&#24212;&#20851;&#31995;&#30340;SSM&#29983;&#25104;&#26041;&#27861;&#27599;&#28155;&#21152;&#19968;&#20010;&#26032;&#23545;&#35937;&#23601;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#30340;&#37325;&#26032;&#20248;&#21270;&#36807;&#31243;&#65292;&#20351;&#24471;&#25512;&#29702;&#36807;&#31243;&#22312;&#20020;&#24202;&#30740;&#31350;&#20013;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;SSM&#38656;&#35201;&#36755;&#20837;&#23436;&#25972;&#30340;&#20960;&#20309;&#20195;&#29702;(&#20363;&#22914;&#65292;&#39640;&#20998;&#36776;&#29575;&#20108;&#36827;&#21046;&#20307;&#31215;&#25110;&#34920;&#38754;&#32593;&#26684;)&#20316;&#20026;&#36755;&#20837;&#24418;&#29366;&#12290;&#32780;&#26080;&#24207;&#30340;3D&#28857;&#20113;&#34920;&#31034;&#26356;&#23481;&#26131;&#20174;&#21508;&#31181;&#21307;&#23398;&#25104;&#20687;&#23454;&#36341;&#20013;&#33719;&#21462;(&#20363;&#22914;&#65292;&#38408;&#20540;&#22270;&#20687;&#21644;&#34920;&#38754;&#25195;&#25551;)&#12290;&#26368;&#36817;&#65292;&#28857;&#20113;&#28145;&#24230;&#32593;&#32476;&#22312;&#23398;&#20064;&#19981;&#21516;&#28857;&#20113;&#20219;&#21153;&#30340;&#25490;&#21015;&#19981;&#21464;&#29305;&#24449;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;(&#20363;&#22914;&#65292;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#21106;&#12289;&#20998;&#31867;)&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20174;&#28857;&#20113;&#20013;&#23398;&#20064;SSM&#26041;&#38754;&#30340;&#24212;&#29992;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Statistical Shape Modeling (SSM) is a valuable tool for investigating and quantifying anatomical variations within populations of anatomies. However, traditional correspondence-based SSM generation methods require a time-consuming re-optimization process each time a new subject is added to the cohort, making the inference process prohibitive for clinical research. Additionally, they require complete geometric proxies (e.g., high-resolution binary volumes or surface meshes) as input shapes to construct the SSM. Unordered 3D point cloud representations of shapes are more easily acquired from various medical imaging practices (e.g., thresholded images and surface scanning). Point cloud deep networks have recently achieved remarkable success in learning permutation-invariant features for different point cloud tasks (e.g., completion, semantic segmentation, classification). However, their application to learning SSM from point clouds is to-date unexplored. In this work, we demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#31038;&#20250;&#23398;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#21644;&#24037;&#20855;&#65292;&#30528;&#30524;&#20110;&#30456;&#20851;&#24615;&#22312;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#24212;&#29992;&#21644;&#20316;&#29992;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#20851;&#24615;&#35780;&#20998;&#24212;&#28385;&#36275;&#30340;&#19968;&#32452;&#26399;&#26395;&#26631;&#20934;&#20197;&#23454;&#29616;&#26377;&#24847;&#20041;&#22320;&#25351;&#23548;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2305.05608</link><description>&lt;p&gt;
&#30456;&#20851;&#24615;&#22312;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Role of Relevance in Fair Ranking. (arXiv:2305.05608v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#31038;&#20250;&#23398;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#21644;&#24037;&#20855;&#65292;&#30528;&#30524;&#20110;&#30456;&#20851;&#24615;&#22312;&#20844;&#24179;&#25490;&#24207;&#20013;&#30340;&#24212;&#29992;&#21644;&#20316;&#29992;&#65292;&#24182;&#25512;&#23548;&#20986;&#30456;&#20851;&#24615;&#35780;&#20998;&#24212;&#28385;&#36275;&#30340;&#19968;&#32452;&#26399;&#26395;&#26631;&#20934;&#20197;&#23454;&#29616;&#26377;&#24847;&#20041;&#22320;&#25351;&#23548;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#22312;&#26426;&#20250;&#33719;&#21462;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65306;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#25490;&#21517;&#36890;&#36807;&#22312;&#25307;&#32856;&#24179;&#21488;&#30340;&#24037;&#20316;&#32844;&#20301;&#12289;&#27714;&#32844;&#32773;&#25110;&#22312;&#32447;&#24066;&#22330;&#30340;&#21334;&#23478;&#20013;&#20998;&#37197;&#26333;&#20809;&#26426;&#20250;&#26469;&#21019;&#24314;&#21644;&#38480;&#21046;&#36873;&#39033;&#12290;&#20026;&#20102;&#36127;&#36131;&#20219;&#22320;&#36825;&#26679;&#20570;&#65292;&#36825;&#20123;&#31038;&#20250;&#30456;&#20851;&#31995;&#32479;&#37319;&#29992;&#21508;&#31181;&#20844;&#24179;&#25514;&#26045;&#21644;&#24178;&#39044;&#25514;&#26045;&#65292;&#20854;&#20013;&#35768;&#22810;&#25514;&#26045;&#35797;&#22270;&#26681;&#25454;&#20215;&#20540;&#20998;&#37197;&#26333;&#20809;&#26426;&#20250;&#12290;&#20294;&#26159;&#65292;&#22240;&#20026;&#36825;&#20123;&#26500;&#36896;&#36890;&#24120;&#19981;&#26159;&#30452;&#25509;&#21487;&#35266;&#23519;&#30340;&#65292;&#25152;&#20197;&#24179;&#21488;&#24517;&#39035;&#20351;&#29992;&#20195;&#29702;&#35780;&#20998;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#24182;&#20174;&#25628;&#32034;&#32773;&#30340;&#34892;&#20026;&#20449;&#21495;&#20013;&#25512;&#26029;&#20986;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#30456;&#20851;&#24615;&#22312;&#39640;&#39118;&#38505;&#30340;&#20844;&#24179;&#25490;&#24207;&#20013;&#26159;&#21542;&#23653;&#34892;&#20854;&#20316;&#20026;&#20215;&#20540;&#35780;&#20998;&#36825;&#26679;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#32467;&#21512;&#31038;&#20250;&#23398;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#30340;&#35282;&#24230;&#21644;&#24037;&#20855;&#65292;&#25512;&#23548;&#20986;&#30456;&#20851;&#24615;&#35780;&#20998;&#24212;&#28385;&#36275;&#30340;&#19968;&#32452;&#26399;&#26395;&#26631;&#20934;&#65292;&#20197;&#20415;&#26377;&#24847;&#20041;&#22320;&#25351;&#23548;&#20844;&#24179;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online platforms mediate access to opportunity: relevance-based rankings create and constrain options by allocating exposure to job openings and job candidates in hiring platforms, or sellers in a marketplace. In order to do so responsibly, these socially consequential systems employ various fairness measures and interventions, many of which seek to allocate exposure based on worthiness. Because these constructs are typically not directly observable, platforms must instead resort to using proxy scores such as relevance and infer them from behavioral signals such as searcher clicks. Yet, it remains an open question whether relevance fulfills its role as such a worthiness score in high-stakes fair rankings.  In this paper, we combine perspectives and tools from the social sciences, information retrieval, and fairness in machine learning to derive a set of desired criteria that relevance scores should satisfy in order to meaningfully guide fairness interventions. We then empirically show 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65306;&#24471;&#20998;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35299;&#37322;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#20027;&#35201;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.05601</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65306;&#25968;&#23398;&#23478;&#21644;&#29289;&#29702;&#23398;&#23478;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and Geometric Deep Learning: an introduction for mathematicians and physicists. (arXiv:2305.05601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#28145;&#24230;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#65306;&#24471;&#20998;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35299;&#37322;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#20027;&#35201;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#20102;&#35753;&#35835;&#32773;&#36827;&#19968;&#27493;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#21644;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#20851;&#38190;&#35201;&#32032;&#65306;&#24471;&#20998;&#21644;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#35299;&#37322;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#20027;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#19981;&#25171;&#31639;&#25552;&#20379;&#23436;&#25972;&#21644;&#35814;&#23613;&#30340;&#35770;&#36848;&#65292;&#20294;&#25105;&#20204;&#20250;&#24341;&#20837;&#20960;&#20010;&#27010;&#24565;&#65292;&#20197;&#20415;&#24555;&#36895;&#20171;&#32461;&#35813;&#20027;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#19968;&#20123;&#38468;&#24405;&#65292;&#35752;&#35770;&#20102;KL&#25955;&#24230;&#65292;&#22238;&#24402;&#65292;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this expository paper we want to give a brief introduction, with few key references for further reading, to the inner functioning of the new and successfull algorithms of Deep Learning and Geometric Deep Learning with a focus on Graph Neural Networks. We go over the key ingredients for these algorithms: the score and loss function and we explain the main steps for the training of a model. We do not aim to give a complete and exhaustive treatment, but we isolate few concepts to give a fast introduction to the subject. We provide some appendices to complement our treatment discussing Kullback-Leibler divergence, regression, Multi-layer Perceptrons and the Universal Approximation Theorem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.05588</link><description>&lt;p&gt;
StrAE&#65306;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#30340;&#33258;&#32534;&#30721;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;StrAE&#36825;&#19968;&#33258;&#32534;&#30721;&#26694;&#26550;&#65292;&#25506;&#31350;&#20102;&#22312;NLP&#20013;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#22312;&#20869;&#30340;&#19981;&#21516;&#31867;&#22411;&#21477;&#23376;&#32467;&#26500;&#21644;&#30446;&#26631;&#19979;&#20351;&#29992;StrAE&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#20869;&#22312;&#21644;&#22806;&#22312;&#20219;&#21153;&#19978;&#35780;&#20272;&#25152;&#23398;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;StrAE&#21033;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#24448;&#30340;&#20570;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;&#20316;&#20026;StrAE&#23454;&#29992;&#24615;&#30340;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;p
&lt;/p&gt;
&lt;p&gt;
This work explores the utility of explicit structure for representation learning in NLP by developing StrAE -- an autoencoding framework that faithfully leverages sentence structure to learn multi-level node embeddings in an unsupervised fashion. We use StrAE to train models across different types of sentential structure and objectives, including a novel contrastive loss over structure, and evaluate the learnt embeddings on a series of both intrinsic and extrinsic tasks. Our experiments indicate that leveraging explicit structure through StrAE leads to improved embeddings over prior work, and that our novel contrastive objective over structure outperforms the standard cross-entropy objective. Moreover, in contrast to findings from prior work that weakly leverages structure, we find that being completely faithful to structure does enable disambiguation between types of structure based on the corresponding model's performance. As further evidence of StrAE's utility, we develop a simple p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#20266;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#21512;&#25104;&#22495;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#30495;&#23454;&#19990;&#30028;&#22495;&#65292;&#29992;&#20110;&#26381;&#35013;&#35270;&#35273;&#27169;&#24335;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.05580</link><description>&lt;p&gt;
Fashion CUT&#65306;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#20266;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#29992;&#20110;&#26381;&#35013;&#35270;&#35273;&#27169;&#24335;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels. (arXiv:2305.05580v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05580
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#20266;&#26631;&#31614;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#22312;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23558;&#21512;&#25104;&#22495;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#30495;&#23454;&#19990;&#30028;&#22495;&#65292;&#29992;&#20110;&#26381;&#35013;&#35270;&#35273;&#27169;&#24335;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#20135;&#21697;&#20449;&#24687;&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#21830;&#24215;&#38750;&#24120;&#37325;&#35201;&#65292;&#20197;&#20801;&#35768;&#23458;&#25143;&#27983;&#35272;&#12289;&#31579;&#36873;&#21644;&#25628;&#32034;&#20135;&#21697;&#12290;&#30001;&#20110;&#32570;&#22833;&#25110;&#19981;&#27491;&#30830;&#30340;&#20449;&#24687;&#20250;&#23548;&#33268;&#23458;&#25143;&#20307;&#39564;&#19981;&#20339;&#65292;&#22240;&#27492;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#32416;&#27491;&#19981;&#20934;&#30830;&#25110;&#32570;&#22833;&#30340;&#20449;&#24687;&#26159;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22312;&#26381;&#35013;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#24615;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#20294;&#30001;&#20110;&#26631;&#27880;&#25104;&#26412;&#32780;&#36153;&#29992;&#26114;&#36149;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#29983;&#25104;&#19981;&#38656;&#35201;&#25163;&#21160;&#26631;&#35760;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#20351;&#29992;&#20165;&#30001;&#21512;&#25104;&#22270;&#20687;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#25512;&#29702;&#26102;&#24615;&#33021;&#19979;&#38477;&#65292;&#22240;&#20026;&#23384;&#22312;&#39046;&#22495;&#36716;&#31227;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#23558;&#21512;&#25104;&#22495;&#30340;&#22270;&#20687;&#36716;&#25442;&#20026;&#30495;&#23454;&#19990;&#30028;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#31867;&#22120;&#30340;&#32852;&#21512;&#35757;&#32451;&#65292;&#20197;&#20135;&#29983;&#36924;&#30495;&#30340;&#22270;&#20687;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate product information is critical for e-commerce stores to allow customers to browse, filter, and search for products. Product data quality is affected by missing or incorrect information resulting in poor customer experience. While machine learning can be used to correct inaccurate or missing information, achieving high performance on fashion image classification tasks requires large amounts of annotated data, but it is expensive to generate due to labeling costs. One solution can be to generate synthetic data which requires no manual labeling. However, training a model with a dataset of solely synthetic images can lead to poor generalization when performing inference on real-world data because of the domain shift. We introduce a new unsupervised domain adaptation technique that converts images from the synthetic domain into the real-world domain. Our approach combines a generative neural network and a classifier that are jointly trained to produce realistic images while preser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; FAENet &#65292;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#34920;&#29616;&#21147;&#24378;&#30340; GNN&#65292;&#29992;&#20110;&#26448;&#26009;&#24314;&#27169;&#65292;&#20854;&#36890;&#36807;&#38543;&#26426;&#24103;&#24179;&#22343;&#65288;SFA&#65289;&#20351;&#20219;&#20309;&#27169;&#22411; E(3) &#31561;&#21464;&#25110;&#19981;&#21464;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05577</link><description>&lt;p&gt;
FAENet&#65306;&#29992;&#20110;&#26448;&#26009;&#24314;&#27169;&#30340;&#24103;&#24179;&#22343;&#31561;&#21464;GNN
&lt;/p&gt;
&lt;p&gt;
FAENet: Frame Averaging Equivariant GNN for Materials Modeling. (arXiv:2305.05577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; FAENet &#65292;&#19968;&#31181;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#34920;&#29616;&#21147;&#24378;&#30340; GNN&#65292;&#29992;&#20110;&#26448;&#26009;&#24314;&#27169;&#65292;&#20854;&#36890;&#36807;&#38543;&#26426;&#24103;&#24179;&#22343;&#65288;SFA&#65289;&#20351;&#20219;&#20309;&#27169;&#22411; E(3) &#31561;&#21464;&#25110;&#19981;&#21464;&#12290;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#26448;&#26009;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#21040;&#24050;&#30693;&#23545;&#29305;&#23450;&#23545;&#31216;&#24615;&#20855;&#26377;&#31561;&#21464;&#25110;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#25104;&#21151;&#30340;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;&#27169;&#22411;&#26550;&#26500;&#26469;&#24378;&#21046;&#23454;&#29616;&#23545;&#31216;&#24615;&#65292;&#36825;&#36890;&#24120;&#20250;&#38477;&#20302;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#38543;&#26426;&#24103;&#24179;&#22343;&#65288;SFA&#65289;&#20174;&#32780;&#36890;&#36807;&#25968;&#25454;&#21464;&#25442;&#20351;&#20219;&#20309;&#27169;&#22411; E(3) &#31561;&#21464;&#25110;&#19981;&#21464;. &#21516;&#26102;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;FAENet&#65292;&#23427;&#26159;&#19968;&#20010;&#31616;&#21333;&#12289;&#24555;&#36895;&#19988;&#34920;&#29616;&#21147;&#24378;&#30340; GNN&#65292;&#36890;&#36807;&#20248;&#21270; SFA&#65292;&#20197;&#27809;&#26377;&#20219;&#20309;&#23545;&#31216;&#24615;&#32422;&#26463;&#30340;&#26041;&#24335;&#22788;&#29702;&#20960;&#20309;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;OC20&#25968;&#25454;&#38598;&#65288;S2EF&#65292;IS2RE&#65289;&#20197;&#21450;&#24120;&#35265;&#30340;&#20998;&#23376;&#24314;&#27169;&#20219;&#21153;&#65288;QM9&#65292;QM7-X&#65289;&#19978;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12289;&#36229;&#24378;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20855;&#26377;&#31034;&#20363;&#23454;&#29616;&#30340;&#36719;&#20214;&#21253;&#21487;&#22312; https://faenet.readthedocs.io&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applications of machine learning techniques for materials modeling typically involve functions known to be equivariant or invariant to specific symmetries. While graph neural networks (GNNs) have proven successful in such tasks, they enforce symmetries via the model architecture, which often reduces their expressivity, scalability and comprehensibility. In this paper, we introduce (1) a flexible framework relying on stochastic frame-averaging (SFA) to make any model E(3)-equivariant or invariant through data transformations. (2) FAENet: a simple, fast and expressive GNN, optimized for SFA, that processes geometric information without any symmetrypreserving design constraints. We prove the validity of our method theoretically and empirically demonstrate its superior accuracy and computational scalability in materials modeling on the OC20 dataset (S2EF, IS2RE) as well as common molecular modeling tasks (QM9, QM7-X). A package implementation is available at https://faenet.readthedocs.io.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24863;&#30693;&#30340;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#38750;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#26041;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36798;&#25104;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.05573</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#25239;&#24863;&#30693;&#30340;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Algorithm For Adversary Aware Decentralized Networked MARL. (arXiv:2305.05573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24863;&#30693;&#30340;&#21435;&#20013;&#24515;&#21270;&#32593;&#32476;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20801;&#35768;&#38750;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#22312;&#23545;&#25239;&#26041;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#20013;&#24515;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#22312;&#25991;&#29486;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#24322;&#26500;&#20307;&#25317;&#26377;&#33258;&#24049;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#30456;&#23545;&#20110;&#20551;&#23450;&#25152;&#26377;&#26234;&#33021;&#20307;&#25317;&#26377;&#20849;&#21516;&#22870;&#21169;&#20989;&#25968;&#30340;&#32463;&#20856;&#22810;&#26234;&#33021;&#20307;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36981;&#24490;&#29616;&#26377;&#21512;&#20316;MARL&#30340;&#24037;&#20316;&#65292;&#22312;&#19968;&#20010;&#36830;&#25509;&#30340;&#26102;&#21464;&#32593;&#32476;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#30456;&#20114;&#20132;&#25442;&#20449;&#24687;&#20197;&#36798;&#25104;&#20849;&#35782;&#12290;&#25105;&#20204;&#22312;&#20849;&#35782;&#26356;&#26032;&#20013;&#24341;&#20837;&#28431;&#27934;&#65292;&#22312;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#20013;&#65292;&#26234;&#33021;&#20307;&#21487;&#20197;&#20559;&#31163;&#20854;&#24120;&#35268;&#30340;&#20849;&#35782;&#26356;&#26032;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20351;&#38750;&#23545;&#25239;&#24615;&#26234;&#33021;&#20307;&#21487;&#20197;&#22312;&#21463;&#21040;&#32422;&#26463;&#26465;&#20214;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#23545;&#25239;&#24615;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized multi-agent reinforcement learning (MARL) algorithms have become popular in the literature since it allows heterogeneous agents to have their own reward functions as opposed to canonical multi-agent Markov Decision Process (MDP) settings which assume common reward functions over all agents. In this work, we follow the existing work on collaborative MARL where agents in a connected time varying network can exchange information among each other in order to reach a consensus. We introduce vulnerabilities in the consensus updates of existing MARL algorithms where agents can deviate from their usual consensus update, who we term as adversarial agents. We then proceed to provide an algorithm that allows non-adversarial agents to reach a consensus in the presence of adversaries under a constrained setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SMAClite&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#35299;&#32806;&#20102;&#21407;&#26377;&#30340;&#38381;&#28304;&#28216;&#25103;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#26694;&#26550;&#65292;SMAClite&#22312;&#36816;&#34892;&#26102;&#36895;&#24230;&#21644;&#20869;&#23384;&#26041;&#38754;&#36229;&#36234;&#20102;SMAC&#12290;</title><link>http://arxiv.org/abs/2305.05566</link><description>&lt;p&gt;
SMAClite: &#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#36731;&#37327;&#32423;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning. (arXiv:2305.05566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SMAClite&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#35299;&#32806;&#20102;&#21407;&#26377;&#30340;&#38381;&#28304;&#28216;&#25103;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#26694;&#26550;&#65292;SMAClite&#22312;&#36816;&#34892;&#26102;&#36895;&#24230;&#21644;&#20869;&#23384;&#26041;&#38754;&#36229;&#36234;&#20102;SMAC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#32570;&#23569;&#36866;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26631;&#20934;&#22522;&#20934;&#12290;Starcraft&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#65288;SMAC&#65289;&#24050;&#32463;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#26500;&#24314;&#22312;&#37325;&#22411;&#30340;&#38381;&#28304;&#35745;&#31639;&#26426;&#28216;&#25103;StarCraft II&#20043;&#19978;&#12290;&#22240;&#27492;&#65292;SMAC&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#20855;&#22791;&#20851;&#20110;&#28216;&#25103;&#30340;&#29305;&#27530;&#30693;&#35782;&#21644;&#20351;&#29992;&#19987;&#26377;&#24037;&#20855;&#25165;&#33021;&#23545;&#29615;&#22659;&#36827;&#34892;&#20219;&#20309;&#26377;&#24847;&#20041;&#30340;&#20462;&#25913;&#25110;&#36129;&#29486;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SMAClite - &#19968;&#20010;&#22522;&#20110;SMAC&#30340;&#25361;&#25112;&#65292;&#19981;&#20165;&#19982;Starcraft II&#35299;&#32806;&#65292;&#32780;&#19988;&#26159;&#24320;&#28304;&#30340;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#26032;&#30340;SMAClite&#20869;&#23481;&#32780;&#26080;&#38656;&#20219;&#20309;&#29305;&#27530;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#22312;SMAClite&#19978;&#35757;&#32451;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#22797;&#29616;SMAC&#30340;&#32467;&#26524;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#65292;SMAClite&#22312;&#36816;&#34892;&#26102;&#36895;&#24230;&#21644;&#20869;&#23384;&#26041;&#38754;&#36229;&#36234;SMAC&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning (MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely used in MARL research, but is built on top of a heavy, closed-source computer game, StarCraft II. Thus, SMAC is computationally expensive and requires knowledge and the use of proprietary tools specific to the game for any meaningful alteration or contribution to the environment. We introduce SMAClite -- a challenge based on SMAC that is both decoupled from Starcraft II and open-source, along with a framework which makes it possible to create new content for SMAClite without any special knowledge. We conduct experiments to show that SMAClite is equivalent to SMAC, by training MARL algorithms on SMAClite and reproducing SMAC results. We then show that SMAClite outperforms SMAC in both runtime speed and memory.
&lt;/p&gt;</description></item><item><title>SkelEx&#21644;BoundEx&#26159;&#31532;&#19968;&#25209;&#33258;&#28982;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#28857;&#21644;&#20915;&#31574;&#36793;&#30028;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#20026;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ReLU NN&#24341;&#20837;&#20102;&#38750;&#24120;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.05562</link><description>&lt;p&gt;
SkelEx&#21644;BoundEx&#65306;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks. (arXiv:2305.05562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05562
&lt;/p&gt;
&lt;p&gt;
SkelEx&#21644;BoundEx&#26159;&#31532;&#19968;&#25209;&#33258;&#28982;&#21487;&#35270;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#20851;&#38190;&#28857;&#21644;&#20915;&#31574;&#36793;&#30028;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#20026;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ReLU NN&#24341;&#20837;&#20102;&#38750;&#24120;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26435;&#37325;&#21644;&#20559;&#24046;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#38480;&#65292;&#20294;&#20173;&#28982;&#26159;&#32534;&#30721;ReLU&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20989;&#25968;&#26368;&#27969;&#34892;&#30340;&#26041;&#24335;&#12290;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#25105;&#20204;&#24341;&#20837;SkelEx&#31639;&#27861;&#65292;&#20197;&#25552;&#21462;ReLU NN&#23398;&#20064;&#30340;&#25104;&#21592;&#20989;&#25968;&#30340;&#39592;&#26550;&#65292;&#20351;&#36825;&#20123;&#20989;&#25968;&#26356;&#26131;&#20110;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20174;&#20851;&#38190;&#28857;&#30340;&#35282;&#24230;&#32771;&#34385;&#32447;&#24615;&#21306;&#22495;&#30340;&#24037;&#20316;&#12290;&#20316;&#20026;&#33258;&#28982;&#30340;&#21518;&#32493;&#24037;&#20316;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BoundEx&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#31532;&#19968;&#20010;&#20174;ReLU NN&#30340;&#23454;&#29616;&#20013;&#25552;&#21462;&#20915;&#31574;&#36793;&#30028;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#20026;&#22312;&#20302;&#32500;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;ReLU NN&#24341;&#20837;&#20102;&#38750;&#24120;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their limited interpretability, weights and biases are still the most popular encoding of the functions learned by ReLU Neural Networks (ReLU NNs). That is why we introduce SkelEx, an algorithm to extract a skeleton of the membership functions learned by ReLU NNs, making those functions easier to interpret and analyze. To the best of our knowledge, this is the first work that considers linear regions from the perspective of critical points. As a natural follow-up, we also introduce BoundEx, which is the first analytical method known to us to extract the decision boundary from the realization of a ReLU NN. Both of those methods introduce very natural visualization tool for ReLU NNs trained on low-dimensional data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN&#20132;&#20114;&#24335;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;EEG&#24773;&#32490;&#35782;&#21035;&#65292;&#38598;&#25104;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#36890;&#36807;CNN&#20132;&#20114;&#24335;Transformer&#27169;&#22359;&#20419;&#36827;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05548</link><description>&lt;p&gt;
CIT-EmotionNet&#65306;&#29992;&#20110;EEG&#24773;&#32490;&#35782;&#21035;&#30340;CNN&#20132;&#20114;&#24335;Transformer&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CIT-EmotionNet: CNN Interactive Transformer Network for EEG Emotion Recognition. (arXiv:2305.05548v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN&#20132;&#20114;&#24335;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;EEG&#24773;&#32490;&#35782;&#21035;&#65292;&#38598;&#25104;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#65292;&#36890;&#36807;CNN&#20132;&#20114;&#24335;Transformer&#27169;&#22359;&#20419;&#36827;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33041;&#30005;&#20449;&#21495;&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#26159;&#24773;&#24863;&#35745;&#31639;&#21644;&#26234;&#33021;&#20132;&#20114;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#30740;&#31350;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#33041;&#30005;&#20449;&#21495;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#24773;&#32490;&#35782;&#21035;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#39033;&#22256;&#38590;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CNN&#20132;&#20114;&#24335;Transformer&#32593;&#32476;&#65292;&#29992;&#20110;EEG&#24773;&#32490;&#35782;&#21035;&#65292;&#31216;&#20026;CIT-EmotionNet&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#38598;&#25104;EEG&#20449;&#21495;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#29305;&#24449;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#21407;&#22987;EEG&#20449;&#21495;&#36716;&#25442;&#20026;&#31354;&#38388;&#39057;&#29575;&#34920;&#31034;&#65292;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#22312;&#21333;&#20010;&#26694;&#26550;&#20869;&#24182;&#34892;&#22320;&#38598;&#25104;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;Transformer&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;CNN&#20132;&#20114;&#24335;Transformer&#27169;&#22359;&#65292;&#20419;&#36827;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#30340;&#20132;&#20114;&#21644;&#34701;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#27169;&#22411;&#20174;EEG&#31354;&#38388;&#39057;&#29575;&#34920;&#31034;&#20013;&#25552;&#21462;&#20004;&#31181;&#31867;&#22411;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#30340;CIT-EmotionNet&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;EEG&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition using Electroencephalogram (EEG) signals has emerged as a significant research challenge in affective computing and intelligent interaction. However, effectively combining global and local features of EEG signals to improve performance in emotion recognition is still a difficult task. In this study, we propose a novel CNN Interactive Transformer Network for EEG Emotion Recognition, known as CIT-EmotionNet, which efficiently integrates global and local features of EEG signals. Initially, we convert raw EEG signals into spatial-frequency representations, which serve as inputs. Then, we integrate Convolutional Neural Network (CNN) and Transformer within a single framework in a parallel manner. Finally, we design a CNN interactive Transformer module, which facilitates the interaction and fusion of local and global features, thereby enhancing the model's ability to extract both types of features from EEG spatial-frequency representations. The proposed CIT-EmotionNet outp
&lt;/p&gt;</description></item><item><title>Walk4Me&#26159;&#19968;&#20010;&#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#31995;&#32479;&#65292;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27493;&#24577;&#29305;&#24449;&#26816;&#27979;&#26469;&#35782;&#21035;&#30142;&#30149;&#24739;&#32773;&#30340;&#27493;&#24577;&#38556;&#30861;&#24182;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.05543</link><description>&lt;p&gt;
Walk4Me: &#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#8212;&#8212;&#19968;&#20010;&#26089;&#26399;&#35786;&#26029;&#21644;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression. (arXiv:2305.05543v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05543
&lt;/p&gt;
&lt;p&gt;
Walk4Me&#26159;&#19968;&#20010;&#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#31995;&#32479;&#65292;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#27493;&#24577;&#29305;&#24449;&#26816;&#27979;&#26469;&#35782;&#21035;&#30142;&#30149;&#24739;&#32773;&#30340;&#27493;&#24577;&#38556;&#30861;&#24182;&#36861;&#36394;&#30142;&#30149;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Walk4Me&#65292;&#36825;&#26159;&#19968;&#20010;&#36828;&#31243;&#24247;&#22797;&#31038;&#21306;&#34892;&#21160;&#33021;&#21147;&#35780;&#20272;&#31995;&#32479;&#65292;&#26088;&#22312;&#20419;&#36827;&#26089;&#26399;&#35786;&#26029;&#12289;&#20005;&#37325;&#24615;&#21644;&#36827;&#23637;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#19977;&#20010;&#26041;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#65306;1&#65289;&#20419;&#36827;&#26089;&#26399;&#35786;&#26029;&#65307;2&#65289;&#35782;&#21035;&#20020;&#24202;&#20005;&#37325;&#24615;&#30340;&#26089;&#26399;&#25351;&#26631;&#65307;3&#65289;&#37327;&#21270;&#21644;&#36319;&#36394;&#30142;&#30149;&#22312;&#27493;&#34892;&#26399;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#27493;&#24577;&#29305;&#24449;&#26816;&#27979;&#26469;&#26816;&#27979;&#24739;&#32773;&#21644;&#36890;&#24120;&#21457;&#32946;&#30340;&#21516;&#40836;&#20154;&#30340;&#34892;&#36208;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#36890;&#36807;&#25105;&#20204;&#30340;&#26032;&#22411;Walk4Me API&#20174;&#35774;&#22791;&#20256;&#24863;&#22120;&#65288;&#20363;&#22914;&#31227;&#21160;&#35774;&#22791;&#30340;&#21152;&#36895;&#24230;&#31561;&#65289;&#36828;&#31243;&#23454;&#26102;&#25910;&#38598;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#25552;&#21462;&#26102;&#38388;/&#31354;&#38388;&#27493;&#24577;&#29305;&#24449;&#21644;&#21407;&#22987;&#25968;&#25454;&#20449;&#21495;&#29305;&#24449;&#65292;&#28982;&#21518;&#37319;&#29992;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#35782;&#21035;&#27169;&#24335;&#65292;&#20197;&#20415;&#23454;&#29616;&#65306;1&#65289;&#35782;&#21035;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#27493;&#24577;&#38556;&#30861;&#30340;&#24739;&#32773;&#65307;2&#65289;&#25551;&#36848;&#27963;&#21160;&#33021;&#21147;&#38480;&#21046;&#30340;&#31243;&#24230;&#65307;3&#65289;&#30830;&#23450;&#30142;&#30149;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Walk4Me, a telehealth community mobility assessment system designed to facilitate early diagnosis, severity, and progression identification. Our system achieves this by 1) enabling early diagnosis, 2) identifying early indicators of clinical severity, and 3) quantifying and tracking the progression of the disease across the ambulatory phase of the disease. To accomplish this, we employ an Artificial Intelligence (AI)-based detection of gait characteristics in patients and typically developing peers. Our system remotely and in real-time collects data from device sensors (e.g., acceleration from a mobile device, etc.) using our novel Walk4Me API. Our web application extracts temporal/spatial gait characteristics and raw data signal characteristics and then employs traditional machine learning and deep learning techniques to identify patterns that can 1) identify patients with gait disturbances associated with disease, 2) describe the degree of mobility limitation, and 3) ide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LUENN&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21457;&#23556;&#22120;&#22270;&#20687;&#37325;&#21472;&#26174;&#33879;&#26102;&#65292;&#21033;&#29992;&#29420;&#29305;&#30340;&#26550;&#26500;&#24179;&#28369;&#22320;&#23481;&#32435;&#20174;&#23436;&#20840;&#23396;&#31435;&#21040;&#20849;&#32858;&#30340;&#21457;&#23556;&#22120;&#65292;&#20174;&#32780;&#23558;&#21322;&#24452;&#20026;1&#24494;&#31859;&#24179;&#26041;&#20869;&#21487;&#29992;&#30340;&#21457;&#23556;&#22120;&#23494;&#24230;&#30340;&#33539;&#22260;&#25193;&#22823;&#20102;6&#20493;&#20197;&#19978;&#65292;&#38477;&#20302;&#20102;&#23450;&#20301;&#31934;&#24230;&#30340;&#24809;&#32602;&#65292;&#24182;&#25552;&#39640;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.05542</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36229;&#23494;&#24230;&#21457;&#23556;&#26426;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Localization of Ultra-dense Emitters with Neural Networks. (arXiv:2305.05542v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LUENN&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21457;&#23556;&#22120;&#22270;&#20687;&#37325;&#21472;&#26174;&#33879;&#26102;&#65292;&#21033;&#29992;&#29420;&#29305;&#30340;&#26550;&#26500;&#24179;&#28369;&#22320;&#23481;&#32435;&#20174;&#23436;&#20840;&#23396;&#31435;&#21040;&#20849;&#32858;&#30340;&#21457;&#23556;&#22120;&#65292;&#20174;&#32780;&#23558;&#21322;&#24452;&#20026;1&#24494;&#31859;&#24179;&#26041;&#20869;&#21487;&#29992;&#30340;&#21457;&#23556;&#22120;&#23494;&#24230;&#30340;&#33539;&#22260;&#25193;&#22823;&#20102;6&#20493;&#20197;&#19978;&#65292;&#38477;&#20302;&#20102;&#23450;&#20301;&#31934;&#24230;&#30340;&#24809;&#32602;&#65292;&#24182;&#25552;&#39640;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#20998;&#23376;&#23450;&#20301;&#26174;&#24494;&#26415;&#65288;SMLM&#65289;&#25193;&#23637;&#20102;&#25105;&#20204;&#35266;&#23519;&#20122;&#32454;&#32990;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#26102;&#38388;&#20998;&#36776;&#29575;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290; &#22686;&#21152;&#21457;&#23556;&#22120;&#23494;&#24230;&#23558;&#25913;&#21892;&#26102;&#38388;&#20998;&#36776;&#29575;&#65292;&#20294;&#24403;&#21069;&#30340;&#20998;&#26512;&#31639;&#27861;&#22312;&#21457;&#23556;&#22120;&#22270;&#20687;&#37325;&#21472;&#26174;&#33879;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LUENN&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#26550;&#26500;&#65292;&#25298;&#32477;&#20102;&#23396;&#31435;&#21457;&#23556;&#22120;&#30340;&#20551;&#35774;&#65307; &#23427;&#21487;&#20197;&#24179;&#28369;&#22320;&#23481;&#32435;&#20174;&#23436;&#20840;&#23396;&#31435;&#21040;&#20849;&#32858;&#30340;&#21457;&#23556;&#22120;&#12290; &#38500;&#20102;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22806;&#65292;&#36825;&#31181;&#26550;&#26500;&#36824;&#25552;&#39640;&#20102;&#23454;&#39564;&#23460;&#30340;&#21487;&#29992;&#24615;&#65292;&#32553;&#30701;&#20102;&#25104;&#20687;&#26102;&#38388;&#24182;&#25918;&#23485;&#20102;&#23454;&#39564;&#30340;&#35201;&#27714;&#12290;&#23427;&#23558;&#21322;&#24452;&#20026;1&#24494;&#31859;&#24179;&#26041;&#20869;&#21487;&#29992;&#30340;&#21457;&#23556;&#22120;&#23494;&#24230;&#30340;&#33539;&#22260;&#25193;&#22823;&#20102;6&#20493;&#20197;&#19978;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23450;&#20301;&#31934;&#24230;&#30340;&#24809;&#32602;&#65292;&#24182;&#25552;&#39640;&#20102;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-Molecule Localization Microscopy (SMLM) has expanded our ability to visualize subcellular structures but is limited in its temporal resolution. Increasing emitter density will improve temporal resolution, but current analysis algorithms struggle as emitter images significantly overlap. Here we present a deep convolutional neural network called LUENN which utilizes a unique architecture that rejects the isolated emitter assumption; it can smoothly accommodate emitters that range from completely isolated to co-located. This architecture, alongside an accurate estimator of location uncertainty, extends the range of usable emitter densities by a factor of 6 to over 31 emitters per micrometer-squared with reduced penalty to localization precision and improved temporal resolution. Apart from providing uncertainty estimation, the algorithm improves usability in laboratories by reducing imaging times and easing requirements for successful experiments.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#36824;&#32771;&#34385;&#20102;&#23454;&#20307;&#38388;&#30340;&#36890;&#20449;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05538</link><description>&lt;p&gt;
&#22810;&#20803;&#35774;&#22791;&#32593;&#32476;&#20013;&#39640;&#25928;&#22522;&#20110;&#27169;&#24335;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient pattern-based anomaly detection in a network of multivariate devices. (arXiv:2305.05538v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05538
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#24335;&#25366;&#25496;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#36824;&#32771;&#34385;&#20102;&#23454;&#20307;&#38388;&#30340;&#36890;&#20449;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32452;&#32455;&#31649;&#29702;&#26381;&#21153;&#36136;&#37327;&#24182;&#30417;&#35270;&#22823;&#37327;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20307;&#37117;&#19982;&#36965;&#27979;&#25110;&#29289;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#24207;&#21015;&#30456;&#20851;&#32852;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#34892;&#20026;&#24322;&#24120;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#20851;&#27880;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#24573;&#30053;&#23454;&#20307;&#38388;&#30340;&#36890;&#20449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#25903;&#25345;&#26368;&#32456;&#29992;&#25143;&#19981;&#20165;&#22312;&#23450;&#20301;&#22312;&#26576;&#20010;&#26102;&#26399;&#23548;&#33268;&#24322;&#24120;&#30340;&#23454;&#20307;&#21644;&#20256;&#24863;&#22120;&#26041;&#38754;&#65292;&#32780;&#19988;&#35299;&#37322;&#36825;&#20010;&#20915;&#23450;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#20351;&#29992;&#20004;&#27493;&#26041;&#27861;&#26816;&#27979;&#24322;&#24120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24674;&#22797;&#32593;&#32476;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#22240;&#20026;&#20851;&#31995;&#36890;&#24120;&#26159;&#21160;&#24577;&#30340;&#65292;&#30001;&#26410;&#30693;&#30340;&#22522;&#30784;&#36807;&#31243;&#24341;&#36215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22522;&#20110;&#39034;&#24207;&#27169;&#24335;&#30340;&#23884;&#20837;&#25253;&#21578;&#24322;&#24120;&#12290;&#27169;&#24335;&#25366;&#25496;&#26159;&#39640;&#25928;&#30340;&#24182;&#25903;&#25345;&#35299;&#37322;&#65292;&#21363;&#27169;&#24335;&#20195;&#34920;&#26102;&#38388;&#24207;&#21015;&#20013;&#39057;&#32321;&#21457;&#29983;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#27169;&#24335;&#25366;&#25496;&#20197;&#22522;&#20110;&#39057;&#29575;&#36807;&#28388;&#39034;&#24207;&#27169;&#24335;&#65292;&#24182;&#24341;&#20837;&#27169;&#24335;&#27169;&#26495;&#20197;&#23558;&#27169;&#24335;&#25253;&#21578;&#20026;&#25991;&#26412;&#65292;&#20174;&#32780;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#29575;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many organisations manage service quality and monitor a large set devices and servers where each entity is associated with telemetry or physical sensor data series. Recently, various methods have been proposed to detect behavioural anomalies, however existing approaches focus on multivariate time series and ignore communication between entities. Moreover, we aim to support end-users in not only in locating entities and sensors causing an anomaly at a certain period, but also explain this decision. We propose a scalable approach to detect anomalies using a two-step approach. First, we recover relations between entities in the network, since relations are often dynamic in nature and caused by an unknown underlying process. Next, we report anomalies based on an embedding of sequential patterns. Pattern mining is efficient and supports interpretation, i.e. patterns represent frequent occurring behaviour in time series. We extend pattern mining to filter sequential patterns based on frequen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#32467;&#21512;&#35270;&#39057;&#21644;&#38899;&#39057;&#20449;&#24687;&#26469;&#20272;&#35745;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#65292;&#20351;&#29992;&#22238;&#24402;&#26631;&#35760;&#35299;&#20915;&#20102;&#21487;&#21464;&#35270;&#39057;&#38271;&#24230;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.05534</link><description>&lt;p&gt;
&#21512;&#24182;&#25972;&#20307;&#19982;&#23616;&#37096;&#20449;&#24687;&#20272;&#35745;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Integrating Holistic and Local Information to Estimate Emotional Reaction Intensity. (arXiv:2305.05534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05534
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26550;&#26500;&#65292;&#32467;&#21512;&#35270;&#39057;&#21644;&#38899;&#39057;&#20449;&#24687;&#26469;&#20272;&#35745;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#65292;&#20351;&#29992;&#22238;&#24402;&#26631;&#35760;&#35299;&#20915;&#20102;&#21487;&#21464;&#35270;&#39057;&#38271;&#24230;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#39057;&#30340;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#65288;ERI&#65289;&#20272;&#35745;&#21487;&#20197;&#20174;&#34987;&#35797;&#35266;&#30475;&#21050;&#28608;&#30340;&#35270;&#39057;&#20013;&#27979;&#37327;&#20986;&#20182;&#20204;&#23545;&#20960;&#31181;&#24773;&#24863;&#32500;&#24230;&#30340;&#21453;&#24212;&#24378;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#35270;&#39057;ERI&#26550;&#26500;&#65292;&#32467;&#21512;&#20102;&#35270;&#39057;&#21644;&#38899;&#39057;&#20449;&#24687;&#12290;&#35270;&#39057;&#36755;&#20837;&#39318;&#20808;&#25353;&#24103;&#36827;&#34892;&#31354;&#38388;&#32534;&#30721;&#65292;&#32467;&#21512;&#32534;&#30721;&#20027;&#20307;&#38754;&#37096;&#34920;&#24773;&#30340;&#25972;&#20307;&#29305;&#24449;&#21644;&#32534;&#30721;&#20854;&#34920;&#24773;&#23616;&#37096;&#29305;&#24449;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#36890;&#36807;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRUs&#65289;&#20174;&#24103;&#21040;&#24103;&#36827;&#34892;&#26102;&#38388;&#19978;&#30340;&#32452;&#21512;&#65292;&#20877;&#36890;&#36807;&#21464;&#21387;&#22120;&#22312;&#20840;&#23616;&#33539;&#22260;&#20869;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#22238;&#24402;&#26631;&#35760;&#35299;&#20915;&#21487;&#21464;&#35270;&#39057;&#38271;&#24230;&#30340;&#38382;&#39064;&#65292;&#35813;&#26631;&#35760;&#20174;&#25152;&#26377;&#24103;&#20013;&#32047;&#31215;&#20449;&#24687;&#20197;&#20135;&#29983;&#29420;&#31435;&#20110;&#35270;&#39057;&#38271;&#24230;&#30340;&#22266;&#23450;&#32500;&#24230;&#21521;&#37327;&#12290;&#38899;&#39057;&#20449;&#24687;&#30340;&#22788;&#29702;&#31867;&#20284;&#65306;&#22312;&#27599;&#20010;&#24103;&#20869;&#25552;&#21462;&#30340;&#39057;&#35889;&#20449;&#24687;&#36890;&#36807;&#19968;&#31995;&#21015;GRUs&#21644;&#20855;&#26377;&#22238;&#24402;&#20196;&#29260;&#30340;&#21464;&#21387;&#22120;&#30340;&#26102;&#38388;&#25972;&#21512;&#12290;&#35270;&#39057;&#21644;&#38899;&#39057;&#22238;&#24402;&#20998;&#21035;&#19982;&#24773;&#24863;&#21453;&#24212;&#24378;&#24230;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-based Emotional Reaction Intensity (ERI) estimation measures the intensity of subjects' reactions to stimuli along several emotional dimensions from videos of the subject as they view the stimuli. We propose a multi-modal architecture for video-based ERI combining video and audio information. Video input is encoded spatially first, frame-by-frame, combining features encoding holistic aspects of the subjects' facial expressions and features encoding spatially localized aspects of their expressions. Input is then combined across time: from frame-to-frame using gated recurrent units (GRUs), then globally by a transformer. We handle variable video length with a regression token that accumulates information from all frames into a fixed-dimensional vector independent of video length. Audio information is handled similarly: spectral information extracted within each frame is integrated across time by a cascade of GRUs and a transformer with regression token. The video and audio regressi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#20102;&#19977;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25391;&#21160;&#20449;&#21495;&#30340;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;98.8\%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.05532</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#38598;&#21512;&#29992;&#20110;&#25391;&#21160;&#20449;&#21495;&#30340;&#25925;&#38556;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An ensemble of convolution-based methods for fault detection using vibration signals. (arXiv:2305.05532v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#20102;&#19977;&#31181;&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#25391;&#21160;&#20449;&#21495;&#30340;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#36798;&#21040;&#20102;&#36229;&#36807;98.8\%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#27979;&#35797;&#24179;&#21488;&#19978;&#34892;&#26143;&#40831;&#36718;&#31665;&#25391;&#21160;&#20449;&#21495;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#35299;&#20915;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#12290;&#23545;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#38382;&#39064;&#65292;&#24120;&#35265;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;&#36317;&#31163;&#12289;&#22522;&#20110;&#21151;&#33021;&#25968;&#25454;&#12289;&#22522;&#20110;&#29305;&#24449;&#21644;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;ROCKET&#12289;ResNet&#21644;FCN&#31561;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#26041;&#27861;&#23545;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#31867;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#26041;&#27861;&#30340;&#38598;&#21512;&#65292;&#24182;&#36890;&#36807;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#24182;&#23454;&#29616;&#36229;&#36807;98.8\%&#20934;&#30830;&#29575;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#35299;&#20915;&#25925;&#38556;&#26816;&#27979;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on solving a fault detection problem using multivariate time series of vibration signals collected from planetary gearboxes in a test rig. Various traditional machine learning and deep learning methods have been proposed for multivariate time-series classification, including distance-based, functional data-oriented, feature-driven, and convolution kernel-based methods. Recent studies have shown using convolution kernel-based methods like ROCKET, and 1D convolutional neural networks with ResNet and FCN, have robust performance for multivariate time-series data classification. We propose an ensemble of three convolution kernel-based methods and show its efficacy on this fault detection problem by outperforming other approaches and achieving an accuracy of more than 98.8\%.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#26412;&#25991;&#30528;&#30524;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#24182;&#21457;&#38169;&#35823;&#30340;&#21487;&#34892;&#24615;&#65292;&#25361;&#25112;&#26159;&#35774;&#35745;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#35299;&#20915;&#22024;&#26434;&#25968;&#25454;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.05531</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#25311;&#24182;&#21457;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Modelling Concurrency Bugs Using Machine Learning. (arXiv:2305.05531v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05531
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#26412;&#25991;&#30528;&#30524;&#20110;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#24182;&#21457;&#38169;&#35823;&#30340;&#21487;&#34892;&#24615;&#65292;&#25361;&#25112;&#26159;&#35774;&#35745;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#35299;&#20915;&#22024;&#26434;&#25968;&#25454;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20063;&#36234;&#26469;&#36234;&#24191;&#27867;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#22312;&#24182;&#34892;&#31243;&#24207;&#30340;&#23433;&#20840;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#30340;&#19968;&#20010;&#20855;&#20307;&#20363;&#23376;&#12290;&#26816;&#27979;&#24182;&#21457;&#38169;&#35823;&#24050;&#32463;&#21560;&#24341;&#31243;&#24207;&#21592;&#24456;&#38271;&#26102;&#38388;&#65292;&#22240;&#20026;&#22686;&#21152;&#20102;&#22797;&#26434;&#24230;&#30340;&#24182;&#21457;&#31243;&#24207;&#26356;&#23481;&#26131;&#20986;&#29616;&#22833;&#36133;&#12290;&#36890;&#36807;&#24320;&#21457;&#36825;&#26679;&#30340;&#33258;&#21160;&#26816;&#27979;&#24037;&#20855;&#65292;&#21487;&#20197;&#26497;&#22823;&#22320;&#33410;&#30465;&#35843;&#35797;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#24847;&#22806;&#38169;&#35823;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#36229;&#36807;&#24403;&#21069;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#20174;&#25972;&#20307;&#19978;&#25552;&#39640;&#24037;&#20855;&#30340;&#20934;&#30830;&#24615;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#28789;&#27963;&#24615;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#29305;&#26377;&#30340;&#35832;&#22810;&#25361;&#25112;&#65288;&#20363;&#22914;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#22024;&#26434;&#25968;&#25454;&#65289;&#65292;&#35774;&#35745;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#24182;&#21457;&#38169;&#35823;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence has gained a lot of traction in the recent years, with machine learning notably starting to see more applications across a varied range of fields. One specific machine learning application that is of interest to us is that of software safety and security, especially in the context of parallel programs. The issue of being able to detect concurrency bugs automatically has intrigued programmers for a long time, as the added layer of complexity makes concurrent programs more prone to failure. The development of such automatic detection tools provides considerable benefits to programmers in terms of saving time while debugging, as well as reducing the number of unexpected bugs. We believe machine learning may help achieve this goal by providing additional advantages over current approaches, in terms of both overall tool accuracy as well as programming language flexibility. However, due to the presence of numerous challenges specific to the machine learning approach (
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#25506;&#32034;&#26032;&#27169;&#24335;&#21644;&#20256;&#36882;&#26377;&#29992;&#20449;&#24687;&#30340;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;Birth-Death&#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#25351;&#25968;&#28176;&#36817;&#25910;&#25947;&#31561;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.05529</link><description>&lt;p&gt;
&#21033;&#29992;Birth-Death &#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#21152;&#36895;Langevin&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Accelerate Langevin Sampling with Birth-Death process and Exploration Component. (arXiv:2305.05529v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05529
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#22312;&#25506;&#32034;&#26032;&#27169;&#24335;&#21644;&#20256;&#36882;&#26377;&#29992;&#20449;&#24687;&#30340;&#36807;&#31243;&#20013;&#21033;&#29992;&#20102;Birth-Death&#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#25351;&#25968;&#28176;&#36817;&#25910;&#25947;&#31561;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#37319;&#26679;&#24050;&#30693;&#27010;&#29575;&#20998;&#24067;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#38024;&#23545;&#22810;&#23792;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;Birth-Death&#36807;&#31243;&#21644;&#25506;&#32034;&#32452;&#20214;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#8220;&#19977;&#24605;&#32780;&#21518;&#34892;&#8221;&#12290;&#25105;&#20204;&#20445;&#30041;&#20004;&#32452;&#37319;&#26679;&#22120;&#65292;&#19968;&#32452;&#22312;&#36739;&#39640;&#28201;&#24230;&#19979;&#65292;&#19968;&#32452;&#22312;&#21407;&#22987;&#28201;&#24230;&#19979;&#12290;&#21069;&#32773;&#20316;&#20026;&#25506;&#32034;&#26032;&#27169;&#24335;&#21644;&#23558;&#26377;&#29992;&#20449;&#24687;&#20256;&#36882;&#32473;&#21518;&#32773;&#30340;&#20808;&#39537;&#65292;&#21518;&#32773;&#22312;&#25509;&#25910;&#20449;&#24687;&#21518;&#23545;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22343;&#22330;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#25506;&#32034;&#36807;&#31243;&#22914;&#20309;&#20915;&#23450;&#37319;&#26679;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25351;&#25968;&#28176;&#36817;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#20197;&#21069;&#25991;&#29486;&#20013;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both birth-death process and exploration component. The main idea of this method is \textit{look before you leap}. We keep two sets of samplers, one at warmer temperature and one at original temperature. The former one serves as pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration process determines sampling efficiency. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test on experiments from previous literature and compared our methodology to previous ones.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#24103;&#65292;&#22312;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05525</link><description>&lt;p&gt;
&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25506;&#32034;&#65306;&#20197;&#35780;&#20272;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises. (arXiv:2305.05525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#24103;&#65292;&#22312;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;AI&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#28982;&#32780;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23588;&#20854;&#26159;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#30340;&#25506;&#32034;&#21364;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#21487;&#35299;&#37322;AI&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#20013;&#39118;&#24247;&#22797;&#38203;&#28860;&#20013;&#28041;&#21450;&#34917;&#20607;&#21160;&#20316;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#26174;&#33879;&#24103;&#65292;&#24182;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable artificial intelligence (AI) techniques are increasingly being explored to provide insights into why AI and machine learning (ML) models provide a certain outcome in various applications. However, there has been limited exploration of explainable AI techniques on time-series data, especially in the healthcare context. In this paper, we describe a threshold-based method that utilizes a weakly supervised model and a gradient-based explainable AI technique (i.e. saliency map) and explore its feasibility to identify salient frames of time-series data. Using the dataset from 15 post-stroke survivors performing three upper-limb exercises and labels on whether a compensatory motion is observed or not, we implemented a feed-forward neural network model and utilized gradients of each input on model outcomes to identify salient frames that involve compensatory motions. According to the evaluation using frame-level annotations, our approach achieved a recall of 0.96 and an F2-score of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#23398;&#20064;&#26426;&#26041;&#27861;&#26469;&#36866;&#24212;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12289;&#30830;&#23450;&#24615;&#20197;&#21450;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.05518</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#26368;&#23567;&#23398;&#20064;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimal Learning Machine for Multi-Label Learning. (arXiv:2305.05518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#23398;&#20064;&#26426;&#26041;&#27861;&#26469;&#36866;&#24212;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12289;&#30830;&#23450;&#24615;&#20197;&#21450;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#30417;&#30563;&#26041;&#27861;&#8212;&#8212;&#26368;&#23567;&#23398;&#20064;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#36317;&#31163;&#30697;&#38453;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#25216;&#26415;&#21450;&#20854;&#26680;&#24515;&#32452;&#20214;&#8212;&#8212;&#36317;&#31163;&#26144;&#23556;&#22914;&#20309;&#36866;&#24212;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#23558;&#36317;&#31163;&#26144;&#23556;&#19982;&#36870;&#36317;&#31163;&#21152;&#26435;&#30456;&#32467;&#21512;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26159;&#22810;&#26631;&#31614;&#23398;&#20064;&#25991;&#29486;&#20013;&#26368;&#31616;&#21333;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#23427;&#22312;&#23567;&#21040;&#20013;&#31561;&#35268;&#27169;&#30340;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#23427;&#30340;&#31616;&#21333;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#25490;&#21517;&#25439;&#22833;&#32479;&#35745;&#37327;&#30340;&#26041;&#27861;&#36873;&#25321;&#20854;&#36229;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#65292;&#22240;&#27492;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#36317;&#31163;&#26144;&#23556;&#26500;&#36896;&#26041;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35780;&#20272;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distance-based supervised method, the minimal learning machine, constructs a predictive model from data by learning a mapping between input and output distance matrices. In this paper, we propose methods and evaluate how this technique and its core component, the distance mapping, can be adapted to multi-label learning. The proposed approach is based on combining the distance mapping with an inverse distance weighting. Although the proposal is one of the simplest methods in the multi-label learning literature, it achieves state-of-the-art performance for small to moderate-sized multi-label learning problems. Besides its simplicity, the proposed method is fully deterministic and its hyper-parameter can be selected via ranking loss-based statistic which has a closed form, thus avoiding conventional cross-validation-based hyper-parameter tuning. In addition, due to its simple linear distance mapping-based construction, we demonstrate that the proposed method can assess predictions' uncert
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedGT&#26694;&#26550;&#65292;&#36890;&#36807;&#32676;&#20307;&#27979;&#35797;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24182;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05506</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24179;&#34913;&#38544;&#31169;&#19982;&#23433;&#20840;&#65306;FedGT&#30340;&#32676;&#20307;&#27979;&#35797;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Balancing Privacy and Security in Federated Learning with FedGT: A Group Testing Framework. (arXiv:2305.05506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05506
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;FedGT&#26694;&#26550;&#65292;&#36890;&#36807;&#32676;&#20307;&#27979;&#35797;&#30340;&#26041;&#27861;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24182;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#65292;&#20174;&#32780;&#24179;&#34913;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;FedGT&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#24182;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#12290;&#21463;&#21040;&#32676;&#20307;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#37325;&#21472;&#30340;&#23458;&#25143;&#32452;&#26469;&#26816;&#27979;&#24694;&#24847;&#23458;&#25143;&#30340;&#23384;&#22312;&#65292;&#24182;&#36890;&#36807;&#35793;&#30721;&#25805;&#20316;&#35782;&#21035;&#23427;&#20204;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#34987;&#35782;&#21035;&#30340;&#23458;&#25143;&#20174;&#27169;&#22411;&#30340;&#35757;&#32451;&#20013;&#21024;&#38500;&#65292;&#24182;&#22312;&#20854;&#20313;&#23458;&#25143;&#20043;&#38388;&#25191;&#34892;&#35757;&#32451;&#12290;FedGT&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20801;&#35768;&#25913;&#36827;&#35782;&#21035;&#33021;&#21147;&#21516;&#26102;&#20173;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26381;&#21153;&#22120;&#23398;&#20064;&#27599;&#20010;&#32452;&#20013;&#23458;&#25143;&#30340;&#32858;&#21512;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MNIST&#21644;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedGT&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#30340;&#33021;&#21147;&#65292;&#20855;&#26377;&#20302;&#35823;&#26816;&#21644;&#34394;&#35686;&#27010;&#29575;&#65292;&#20135;&#29983;&#39640;&#27169;&#22411;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose FedGT, a novel framework for identifying malicious clients in federated learning with secure aggregation. Inspired by group testing, the framework leverages overlapping groups of clients to detect the presence of malicious clients in the groups and to identify them via a decoding operation. The identified clients are then removed from the training of the model, which is performed over the remaining clients. FedGT strikes a balance between privacy and security, allowing for improved identification capabilities while still preserving data privacy. Specifically, the server learns the aggregated model of the clients in each group. The effectiveness of FedGT is demonstrated through extensive experiments on the MNIST and CIFAR-10 datasets, showing its ability to identify malicious clients with low misdetection and false alarm probabilities, resulting in high model utility.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36882;&#24402;&#30340;&#26694;&#26550;&#26469;&#25552;&#39640;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#36882;&#24402;&#25968;&#37327;&#65292;&#20197;&#20943;&#23569;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#21333;&#20803;&#26469;&#35843;&#33410;&#27169;&#22411;&#29305;&#24449;&#65292;&#21487;&#20197;&#20351;&#32593;&#32476;&#21066;&#20943;&#39640;&#36798;75%&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05505</link><description>&lt;p&gt;
&#36882;&#24402;&#26159;&#20320;&#38656;&#35201;&#30340;&#20840;&#37096;&#65306;&#26397;&#30528;&#39640;&#25928;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Recursions Are All You Need: Towards Efficient Deep Unfolding Networks. (arXiv:2305.05505v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36882;&#24402;&#30340;&#26694;&#26550;&#26469;&#25552;&#39640;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#36882;&#24402;&#25968;&#37327;&#65292;&#20197;&#20943;&#23569;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#21333;&#20803;&#26469;&#35843;&#33410;&#27169;&#22411;&#29305;&#24449;&#65292;&#21487;&#20197;&#20351;&#32593;&#32476;&#21066;&#20943;&#39640;&#36798;75%&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#22312;&#21387;&#32553;&#24863;&#30693;&#26041;&#38754;&#30340;&#24212;&#29992;&#38750;&#24120;&#25104;&#21151;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#26159;&#36845;&#20195;&#30340;&#65292;&#36825;&#20250;&#22312;&#32593;&#32476;&#20013;&#20135;&#29983;&#36739;&#22823;&#30340;&#20887;&#20313;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#36882;&#24402;&#30340;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#28145;&#24230;&#23637;&#24320;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#36882;&#24402;&#26377;&#25928;&#22320;&#28040;&#38500;&#28145;&#24230;&#23637;&#24320;&#32593;&#32476;&#20013;&#30340;&#20887;&#20313;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#36882;&#24402;&#30340;&#25968;&#37327;&#65292;&#20197;&#20943;&#23569;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;&#36882;&#24402;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#21333;&#20803;&#65292;&#26681;&#25454;&#36845;&#20195;&#24635;&#25968;&#21644;&#24403;&#21069;&#36845;&#20195;&#32034;&#24341;&#35843;&#33410;&#27169;&#22411;&#30340;&#29305;&#24449;&#12290;&#20026;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;ISTA-Net+&#21644;COAST&#12290;&#24191;&#27867;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;&#20801;&#35768;&#32593;&#32476;&#21066;&#20943;&#39640;&#36798;75%&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#21516;&#26102;&#22823;&#22810;&#25968;&#32500;&#25345;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of deep unfolding networks in compressive sensing (CS) has seen wide success as they provide both simplicity and interpretability. However, since most deep unfolding networks are iterative, this incurs significant redundancies in the network. In this work, we propose a novel recursion-based framework to enhance the efficiency of deep unfolding models. First, recursions are used to effectively eliminate the redundancies in deep unfolding networks. Secondly, we randomize the number of recursions during training to decrease the overall training time. Finally, to effectively utilize the power of recursions, we introduce a learnable unit to modulate the features of the model based on both the total number of iterations and the current iteration index. To evaluate the proposed framework, we apply it to both ISTA-Net+ and COAST. Extensive testing shows that our proposed framework allows the network to cut down as much as 75% of its learnable parameters while mostly maintaining its per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25913;&#21464;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#23545;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#26292;&#38706;&#20110;&#19981;&#22826;&#21487;&#33021;&#30340;&#26465;&#20214;&#19979;&#20462;&#25913;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#26102;&#65292;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.05499</link><description>&lt;p&gt;
&#23454;&#38469;&#20132;&#36890;&#26631;&#24535;&#25913;&#21464;&#23545;YOLOv7-&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object Recognition Model. (arXiv:2305.05499v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25913;&#21464;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#23545;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#26292;&#38706;&#20110;&#19981;&#22826;&#21487;&#33021;&#30340;&#26465;&#20214;&#19979;&#20462;&#25913;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#26102;&#65292;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#29289;&#20307;&#35782;&#21035;(OR)&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#26426;&#22330;&#23433;&#20840;&#21644;&#37038;&#20214;&#20998;&#25315;&#12290;&#36825;&#20123;&#27169;&#22411;&#24050;&#25104;&#20026;AI&#33021;&#21147;&#30340;&#26631;&#24535;&#65292;&#24182;&#25903;&#25345;&#30528;&#22269;&#23478;&#37038;&#25919;&#36816;&#33829;&#31561;&#37325;&#35201;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;OR&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#29616;&#23454;&#22330;&#26223;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#20132;&#36890;&#26631;&#24535;&#25913;&#21464;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#25913;&#21464;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#23545;&#30446;&#26631;&#35782;&#21035;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#20844;&#24320;&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#20132;&#36890;&#26631;&#24535;&#20462;&#25913;&#65292;&#21253;&#25324;&#22823;&#23567;&#12289;&#24418;&#29366;&#12289;&#39068;&#33394;&#12289;&#21487;&#35265;&#24615;&#21644;&#35282;&#24230;&#30340;&#25913;&#21464;&#65292;&#24182;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#23545;YOLOv7 (You Only Look Once)&#27169;&#22411;&#30340;&#26816;&#27979;&#21644;&#20998;&#31867;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26292;&#38706;&#20110;&#19981;&#22826;&#21487;&#33021;&#30340;&#26465;&#20214;&#19979;&#20462;&#25913;&#21518;&#30340;&#20132;&#36890;&#26631;&#24535;&#26102;&#65292;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Image Processing has led to the widespread use of Object Recognition (OR) models in various applications, such as airport security and mail sorting. These models have become essential in signifying the capabilities of AI and supporting vital services like national postal operations. However, the performance of OR models can be impeded by real-life scenarios, such as traffic sign alteration. Therefore, this research investigates the effects of altered traffic signs on the accuracy and performance of object recognition models. To this end, a publicly available dataset was used to create different types of traffic sign alterations, including changes to size, shape, color, visibility, and angles. The impact of these alterations on the YOLOv7 (You Only Look Once) model's detection and classification abilities were analyzed. It reveals that the accuracy of object detection models decreases significantly when exposed to modified traffic signs under unlikely conditions. This
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#24322;&#24120;&#20256;&#24863;&#22120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#20855;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;DBSCAN&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#36127;&#37319;&#26679;&#20013;&#20351;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#21306;&#20998;&#33391;&#22909;&#21644;&#24322;&#24120;&#30340;&#20256;&#24863;&#22120;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05495</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#24322;&#24120;&#22303;&#22756;&#28287;&#24230;&#20256;&#24863;&#22120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Anomaly Detection of Rogue Soil Moisture Sensors. (arXiv:2305.05495v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#24322;&#24120;&#20256;&#24863;&#22120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#20855;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;DBSCAN&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#36127;&#37319;&#26679;&#20013;&#20351;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#26469;&#21306;&#20998;&#33391;&#22909;&#21644;&#24322;&#24120;&#30340;&#20256;&#24863;&#22120;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#25968;&#25454;&#26159;&#25104;&#21151;&#30340;&#20892;&#19994;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#26680;&#24515;&#35201;&#32032;&#65292;&#20294;&#29289;&#32852;&#32593;&#25968;&#25454;&#20063;&#24102;&#26469;&#20102;&#24456;&#22810;&#25361;&#25112;&#65292;&#27604;&#22914;&#25968;&#25454;&#27745;&#26579;&#39118;&#38505;&#12290;&#24403;&#20256;&#24863;&#22120;&#25552;&#20379;&#25345;&#32493;&#19981;&#27491;&#30830;&#30340;&#27979;&#37327;&#20540;&#26102;&#65292;&#31216;&#20026;&#24322;&#24120;&#25968;&#25454;&#12290;&#20026;&#30830;&#20445;&#27491;&#30830;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#19982;IoT&#25968;&#25454;&#19968;&#36215;&#24037;&#20316;&#26102;&#24517;&#38656;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26159;&#26816;&#27979;&#36825;&#20123;&#24322;&#24120;&#20256;&#24863;&#22120;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20551;&#23450;&#24050;&#30693;&#33391;&#22909;&#30340;&#20256;&#24863;&#22120;&#25110;&#32773;&#22823;&#22810;&#25968;&#20256;&#24863;&#22120;&#37117;&#26159;&#33391;&#22909;&#30340;&#65292;&#20294;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#24448;&#24448;&#26159;&#23436;&#20840;&#27809;&#26377;&#26631;&#35760;&#30340;&#24182;&#19988;&#25968;&#37327;&#24222;&#22823;&#65292;&#38656;&#35201;&#33021;&#22815;&#22312;&#27809;&#26377;&#20808;&#21069;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21040;&#24322;&#24120;&#20256;&#24863;&#22120;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20855;&#26377;&#23545;&#27604;&#25439;&#22833;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;DBSCAN&#30340;&#33258;&#25105;&#30417;&#30563;&#22411;&#24322;&#24120;&#20256;&#24863;&#22120;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#30340;&#19968;&#20010;&#26680;&#24515;&#36129;&#29486;&#26159;&#22312;&#19977;&#20803;&#32452;&#25439;&#22833;&#30340;&#36127;&#37319;&#26679;&#20013;&#20351;&#29992;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#12290;&#25105;&#20204;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#22303;&#22756;&#28287;&#24230;&#25968;&#25454;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20934;&#30830;&#22320;&#21306;&#20998;&#33391;&#22909;&#21644;&#24322;&#24120;&#30340;&#20256;&#24863;&#22120;&#65292;&#29978;&#33267;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoT data is a central element in the successful digital transformation of agriculture. However, IoT data comes with its own set of challenges. E.g., the risk of data contamination due to rogue sensors. A sensor is considered rogue when it provides incorrect measurements over time. To ensure correct analytical results, an essential preprocessing step when working with IoT data is the detection of such rogue sensors. Existing methods assume that well-behaving sensors are known or that a large majority of the sensors is well-behaving. However, real-world data is often completely unlabeled and voluminous, calling for self-supervised methods that can detect rogue sensors without prior information. We present a self-supervised anomalous sensor detector based on a neural network with a contrastive loss, followed by DBSCAN. A core contribution of our paper is the use of Dynamic Time Warping in the negative sampling for the triplet loss. This novelty makes the use of triplet networks feasible f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#23454;&#26102;&#24847;&#22270;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#32454;&#21270;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;33pp&#12290;</title><link>http://arxiv.org/abs/2305.05474</link><description>&lt;p&gt;
&#36229;&#36234;&#30740;&#31350;&#25968;&#25454;&#38598;&#65306;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#26032;&#22411;&#24847;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Going beyond research datasets: Novel intent discovery in the industry setting. (arXiv:2305.05474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#23454;&#26102;&#24847;&#22270;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#32454;&#21270;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;33pp&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#24847;&#22270;&#21457;&#29616;&#33258;&#21160;&#21270;&#20102;&#23558;&#30456;&#20284;&#30340;&#20449;&#24687;&#65288;&#38382;&#39064;&#65289;&#20998;&#32452;&#20197;&#35782;&#21035;&#20197;&#21069;&#26410;&#30693;&#30340;&#24847;&#22270;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20165;&#20855;&#26377;&#38382;&#39064;&#23383;&#27573;&#24182;&#19988;&#19982;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#38598;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#22312;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#37096;&#32626;&#30340;&#24847;&#22270;&#21457;&#29616;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#30410;&#65306;&#26082;&#26377;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#65292;&#20063;&#26377;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26368;&#20339;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31934;&#35843;&#32858;&#31867;&#20219;&#21153;&#26399;&#38388;&#21033;&#29992;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#38598;&#30340;&#23545;&#35805;&#32467;&#26500;&#65288;&#21363;&#38382;&#39064;&#21644;&#31572;&#26696;&#65289;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;Conv&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25152;&#26377;&#26041;&#27861;&#32508;&#21512;&#21033;&#29992;&#20102;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#38598;&#65292;&#20026;&#21482;&#38024;&#23545;&#38382;&#39064;&#30340;Constrained Deep Adaptive Clustering (CDAC)&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#36798;33pp&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#38024;&#23545;&#38382;&#39064;&#25968;&#25454;&#30340;CDAC&#27169;&#22411;&#21482;&#27604;&#22522;&#20934;&#32447;&#39640;&#36798;13pp&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel intent discovery automates the process of grouping similar messages (questions) to identify previously unknown intents. However, current research focuses on publicly available datasets which have only the question field and significantly differ from real-life datasets. This paper proposes methods to improve the intent discovery pipeline deployed in a large e-commerce platform. We show the benefit of pre-training language models on in-domain data: both self-supervised and with weak supervision. We also devise the best method to utilize the conversational structure (i.e., question and answer) of real-life datasets during fine-tuning for clustering tasks, which we call Conv. All our methods combined to fully utilize real-life datasets give up to 33pp performance boost over state-of-the-art Constrained Deep Adaptive Clustering (CDAC) model for question only. By comparison CDAC model for the question data only gives only up to 13pp performance boost over the naive baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39134;&#32764;&#35774;&#35745;&#65292;&#21033;&#29992;&#24050;&#30693;&#26550;&#26500;&#30340;&#25913;&#36827;&#35299;&#20915;&#20102;&#27714;&#35299;&#20108;&#32500;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#30340;&#36817;&#20284;&#35299;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20851;&#27880;&#20102;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#21644;&#26032;&#24418;&#29366;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.05469</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39134;&#32764;&#35774;&#35745;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Airfoil Design. (arXiv:2305.05469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39134;&#32764;&#35774;&#35745;&#65292;&#21033;&#29992;&#24050;&#30693;&#26550;&#26500;&#30340;&#25913;&#36827;&#35299;&#20915;&#20102;&#27714;&#35299;&#20108;&#32500;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#30340;&#36817;&#20284;&#35299;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20851;&#27880;&#20102;&#27169;&#22411;&#24615;&#33021;&#34920;&#29616;&#21644;&#26032;&#24418;&#29366;&#30340;&#33258;&#21160;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#30740;&#31350;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;Graph&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#35299;&#20915;PDE&#25968;&#20540;&#35745;&#31639;&#26041;&#38754;&#20351;&#29992;&#24191;&#27867;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#20294;&#26159;&#65292;&#22797;&#26434;&#30340;PDE&#65292;&#22914;Navier-Stokes&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#32780;&#30446;&#21069;&#22823;&#37096;&#20998;&#30456;&#20851;&#24037;&#20316;&#37117;&#26159;&#38598;&#20013;&#20110;&#31616;&#21333;&#20960;&#20309;&#32467;&#26500;&#30340;&#39118;&#27969;&#27169;&#25311;&#25110;&#36866;&#29992;&#20110;&#35774;&#35745;&#30446;&#30340;&#30340;&#22806;&#35266;&#23450;&#24615;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#38024;&#23545;PDE&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;GNN&#30340;&#24037;&#20316;&#65292;&#25552;&#20986;&#19968;&#31181;&#24050;&#30693;&#26550;&#26500;&#30340;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#21516;&#39134;&#32764;&#20960;&#20309;&#24418;&#29366;&#19979;&#27714;&#35299;&#20108;&#32500;&#23450;&#24120;&#19981;&#21487;&#21387;&#32553;Navier-Stokes&#26041;&#31243;&#30340;&#36817;&#20284;&#35299;&#38382;&#39064;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#27979;&#35797;&#19981;&#20165;&#35201;&#20851;&#27880;&#39044;&#27979;&#31934;&#24230;&#65292;&#36824;&#35201;&#33021;&#22815;&#33258; &#20027;&#29983;&#25104;&#26032;&#30340;&#21512;&#29702;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of partial differential equations (PDE) through the framework of deep learning emerged a few years ago leading to the impressive approximations of simple dynamics. Graph neural networks (GNN) turned out to be very useful in those tasks by allowing the treatment of unstructured data often encountered in the field of numerical resolutions of PDE. However, the resolutions of harder PDE such as Navier-Stokes equations are still a challenging task and most of the work done on the latter concentrate either on simulating the flow around simple geometries or on qualitative results that looks physical for design purpose. In this study, we try to leverage the work done on deep learning for PDE and GNN by proposing an adaptation of a known architecture in order to tackle the task of approximating the solution of the two-dimensional steady-state incompressible Navier-Stokes equations over different airfoil geometries. In addition to that, we test our model not only on its performance ove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;</title><link>http://arxiv.org/abs/2305.05465</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21160;&#24577;&#20013;&#30340;&#32858;&#31867;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;Transformer&#35270;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24403;&#26435;&#37325;&#19981;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#34920;token&#30340;&#31890;&#23376;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#32780;&#36235;&#21521;&#20110;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#12290;&#20986;&#29616;&#30340;&#26497;&#38480;&#23545;&#35937;&#31867;&#22411;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30697;&#38453;&#25910;&#25947;&#20110;&#20302;&#31209;&#24067;&#23572;&#30697;&#38453;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#32452;&#21512;&#22312;&#25968;&#23398;&#19978;&#35777;&#23454;&#20102;Vaswani&#31561;&#20154;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#20250;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#35745;&#31639;&#26694;&#26550;&#26469;&#37327;&#21270;&#39640;&#26657;&#25945;&#32946;&#36136;&#37327;&#65292;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65288;AQI&#65289;&#26469;&#32508;&#21512;&#34913;&#37327;&#23398;&#32773;&#30340;&#23398;&#26415;&#21697;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.05460</link><description>&lt;p&gt;
&#22522;&#20110;&#20248;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#26657;&#25945;&#32946;&#36136;&#37327;&#37327;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#36879;&#26126;&#25307;&#32856;&#65306;&#31532;&#19968;&#37096;&#20998;&#27169;&#22411;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
Optimization- and AI-based approaches to academic quality quantification for transparent academic recruitment: part 1-model development. (arXiv:2305.05460v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05460
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#20004;&#31181;&#35745;&#31639;&#26694;&#26550;&#26469;&#37327;&#21270;&#39640;&#26657;&#25945;&#32946;&#36136;&#37327;&#65292;&#36890;&#36807;&#19968;&#20010;&#21333;&#19968;&#25351;&#26631;&#65288;AQI&#65289;&#26469;&#32508;&#21512;&#34913;&#37327;&#23398;&#32773;&#30340;&#23398;&#26415;&#21697;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20844;&#27491;&#22320;&#25307;&#32856;&#39640;&#26657;&#21644;&#30740;&#31350;&#26426;&#26500;&#30340;&#25945;&#32946;&#20154;&#21592;&#65292;&#26681;&#25454;&#20840;&#29699;&#20844;&#35748;&#30340;&#23398;&#26415;&#21697;&#36136;&#29305;&#24449;&#30830;&#23450;&#27491;&#30830;&#30340;&#34913;&#37327;&#26631;&#20934;&#26159;&#19968;&#20010;&#21313;&#20998;&#24494;&#22937;&#12289;&#23500;&#26377;&#25361;&#25112;&#24615;&#20294;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#36830;&#20018;&#30340;&#20004;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#31532;&#19968;&#31687;&#35770;&#25991;&#20013;&#30340;&#23398;&#26415;&#21697;&#36136;&#37327;&#21270;&#24314;&#27169;&#37096;&#20998;&#65292;&#32780;&#22312;&#31532;&#20108;&#31687;&#35770;&#25991;&#20013;&#32771;&#34385;&#20102;&#26696;&#20363;&#30740;&#31350;&#37096;&#20998;&#12290;&#38024;&#23545;&#23398;&#26415;&#21697;&#36136;&#37327;&#21270;&#24314;&#27169;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26500;&#24314;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#30340;&#35745;&#31639;&#26694;&#26550;&#65306;(i) &#22522;&#20110;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#20197;&#21450; (ii) &#22522;&#20110;&#23402;&#29983;&#32593;&#32476;&#30340;&#26694;&#26550;&#65288;&#19968;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65289;&#12290;&#20004;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#37117;&#26159;&#19968;&#20010;&#31216;&#20026;&#23398;&#26415;&#21697;&#36136;&#37327;&#25351;&#25968;(Academic Quality Index, AQI)&#30340;&#21333;&#19968;&#25351;&#26631;&#65292;&#23427;&#26159;&#24635;&#20307;&#23398;&#26415;&#21697;&#36136;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#20351;&#29992;&#20840;&#29699;&#31532;&#19968;&#21644;&#24179;&#22343;&#38750;&#31532;&#19968;&#31867;&#22823;&#23398;&#30340;&#23398;&#32773;&#25968;&#25454;&#65292;&#26681;&#25454;&#12298;&#27888;&#26212;&#22763;&#39640;&#31561;&#25945;&#32946;&#19990;&#30028;&#22823;&#23398;&#25490;&#21517;&#12299;&#21644; QS &#19990;&#30028;&#22823;&#23398;&#25490;&#21517;&#36827;&#34892;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
For fair academic recruitment at universities and research institutions, determination of the right measure based on globally accepted academic quality features is a highly delicate, challenging, but quite important problem to be addressed. In a series of two papers, we consider the modeling part for academic quality quantification in the first paper, in this paper, and the case studies part in the second paper. For academic quality quantification modeling, we develop two computational frameworks which can be used to construct a decision-support tool: (i) an optimization-based framework and (ii) a Siamese network (a type of artificial neural network)-based framework. The output of both models is a single index called Academic Quality Index (AQI) which is a measure of the overall academic quality. The data of academics from first-class and average-class world universities, based on Times Higher Education World University Rankings and QS World University Rankings, are assumed as the refe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#20445;&#25252;&#38750;&#25112;&#26007;&#20154;&#21592;&#30340;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#35774;&#35745;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.05459</link><description>&lt;p&gt;
&#19968;&#31181;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#65306;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#21307;&#30103;&#21333;&#20301;&#21644;&#21463;&#20260;&#22763;&#20853;&#30340;&#20445;&#25252;&#36873;&#39033;
&lt;/p&gt;
&lt;p&gt;
A Cross-Frequency Protective Emblem: Protective Options for Medical Units and Wounded Soldiers in the Context of (fully) Autonomous Warfare. (arXiv:2305.05459v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#24212;&#23545;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#20445;&#25252;&#38750;&#25112;&#26007;&#20154;&#21592;&#30340;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#35774;&#35745;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;(&#23436;&#20840;)&#33258;&#20027;&#25112;&#20105;&#20013;&#20445;&#25252;&#38750;&#25112;&#26007;&#20154;&#21592;&#24341;&#21457;&#20102;&#22269;&#38469;&#20445;&#25252;&#26631;&#24535;&#30340;&#26102;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#35774;&#35745;&#19968;&#31181;&#36328;&#39057;&#20445;&#25252;&#24509;&#31456;&#65292;&#20197;&#20415;&#27494;&#22120;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#21040;&#24182;&#30456;&#24212;&#20316;&#20986;&#25514;&#26045;&#12290;&#22312;&#25216;&#26415;&#37096;&#32626;&#26041;&#38754;&#65292;&#32771;&#34385;&#37319;&#29992;&#38647;&#36798;&#20449;&#26631;&#31561;&#24418;&#24335;&#65292;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#36827;&#34892;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The protection of non-combatants in times of (fully) autonomous warfare raises the question of the timeliness of the international protective emblem. Incidents in the recent past indicate that it is becoming necessary to transfer the protective emblem to other dimensions of transmission and representation. (Fully) Autonomous weapon systems are often launched from a great distance to the aiming point and there may be no possibility for the operators to notice protective emblems at the point of impact. In this case, the weapon system would have to detect such protective emblems and, if necessary, disintegrate autonomously or request an abort via human-in-the-loop. In our paper, we suggest ways in which a cross-frequency protective emblem can be designed. On the one hand, the technical deployment, e.g. in the form of RADAR beacons, is considered, as well as the interpretation by methods of machine learning. With regard to the technical deployment, possibilities are considered to address d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;</title><link>http://arxiv.org/abs/2305.05448</link><description>&lt;p&gt;
&#20511;&#21161;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#40065;&#26834;&#24615;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Robust Implicit Regularization via Weight Normalization. (arXiv:2305.05448v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#20316;&#20026;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#40065;&#26834;&#38544;&#24335;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#21442;&#25968;&#30340;&#38544;&#24335;&#20559;&#22909;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#35299;&#20915;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#38548;&#38402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#21487;&#33021;&#26377;&#35768;&#22810;&#25554;&#20540;&#35299;; &#38544;&#24335;&#27491;&#21017;&#21270;&#26159;&#25351;&#29305;&#23450;&#20248;&#21270;&#26041;&#27861;&#23545;&#20247;&#22810;&#25554;&#20540;&#35299;&#20043;&#19968;&#30340;&#38544;&#21547;&#21916;&#22909;&#12290;&#24050;&#32463;&#24314;&#31435;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#65288;&#38543;&#26426;&#65289;&#26799;&#24230;&#19979;&#38477;&#22312;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#26102;&#20542;&#21521;&#20110;&#20855;&#26377;&#20302;&#31209;&#21644;/&#25110;&#31232;&#30095;&#35299;&#30340;&#38544;&#24335;&#20559;&#24046;&#65292;&#20174;&#26576;&#31181;&#31243;&#24230;&#19978;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24179;&#26041;&#25439;&#22833;&#30446;&#26631;&#29702;&#35770;&#36890;&#24120;&#38656;&#35201;&#21487;&#35757;&#32451;&#26435;&#37325;&#30340;&#38750;&#24120;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#36825;&#19982;&#23454;&#36341;&#20013;&#20026;&#20102;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#32780;&#21021;&#22987;&#21270;&#30340;&#26356;&#22823;&#35268;&#27169;&#30340;&#26435;&#37325;&#30683;&#30462;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#32435;&#20837;&#24182;&#20998;&#26512;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20854;&#20013;&#26435;&#37325;&#21521;&#37327;&#20197;&#26497;&#22352;&#26631;&#21442;&#25968;&#21270;&#65292;&#23548;&#33268;&#33258;&#28982;&#30340;&#26435;&#37325;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#30340;&#35774;&#32622;&#20013;&#65292;&#37319;&#29992;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#23545;&#27431;&#20960;&#37324;&#24503;&#33539;&#25968;&#36739;&#20302;&#30340;&#26435;&#37325;&#21521;&#37327;&#20855;&#26377;&#38544;&#24335;&#27491;&#21017;&#21270;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#23558;&#26435;&#37325;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;&#20559;&#24046;&#19982;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#32463;&#39564;&#33539;&#25968;&#27491;&#21017;&#21270;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#32447;&#24615;&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterized models may have many interpolating solutions; implicit regularization refers to the hidden preference of a particular optimization method towards a certain interpolating solution among the many. A by now established line of work has shown that (stochastic) gradient descent tends to have an implicit bias towards low rank and/or sparse solutions when used to train deep linear networks, explaining to some extent why overparameterized neural network models trained by gradient descent tend to have good generalization performance in practice. However, existing theory for square-loss objectives often requires very small initialization of the trainable weights, which is at odds with the larger scale at which weights are initialized in practice for faster convergence and better generalization performance. In this paper, we aim to close this gap by incorporating and analyzing gradient descent with weight normalization, where the weight vector is reparamterized in terms of polar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#21487;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05433</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#29992;&#20110;&#37327;&#23376;&#24577;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Transformer Networks for Quantum State Tomography. (arXiv:2305.05433v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#21487;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#31070;&#32463;&#32593;&#32476;&#19968;&#30452;&#34987;&#29992;&#20110;&#37327;&#23376;&#24577;&#37325;&#26500;&#65288;QST&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#37325;&#26500;&#37327;&#23376;&#24577;&#30340;&#25928;&#29575;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#24314;&#27169;&#19982;&#37327;&#23376;&#24577;&#37325;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20174;&#27979;&#37327;&#32479;&#35745;&#25968;&#25454;&#20013;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#36741;&#21161;&#20351;&#29992;&#32508;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#24110;&#21161;&#26368;&#23567;&#21270;&#23454;&#38469;&#24577;&#19982;&#26816;&#32034;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#36319;&#36394;&#20102;&#28041;&#21450;&#21508;&#31181;&#21442;&#25968;&#35843;&#25972;&#30340;&#24120;&#35265;&#35757;&#32451;&#31574;&#30053;&#23545;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; QST &#26041;&#27861;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#32467;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#26500;&#32431;&#24577;&#21644;&#28151;&#21512;&#24577;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been actively explored for quantum state tomography (QST) due to their favorable expressibility. To further enhance the efficiency of reconstructing quantum states, we explore the similarity between language modeling and quantum state tomography and propose an attention-based QST method that utilizes the Transformer network to capture the correlations between measured results from different measurements. Our method directly retrieves the density matrices of quantum states from measured statistics, with the assistance of an integrated loss function that helps minimize the difference between the actual states and the retrieved states. Then, we systematically trace different impacts within a bag of common training strategies involving various parameter adjustments on the attention-based QST method. Combining these techniques, we establish a robust baseline that can efficiently reconstruct pure and mixed quantum states. Furthermore, by comparing the performance of thre
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21512;&#25104;&#36229;&#22768;&#22270;&#20687;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#22312;&#24038;&#24515;&#23460;&#21644;&#24038;&#25151;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;9.09&#65285;&#12289;3.7&#65285;&#21644;15.0&#65285;&#30340;Dice&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05424</link><description>&lt;p&gt;
&#26469;&#33258;&#22122;&#22768;&#30340;&#22238;&#38899;&#65306;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21512;&#25104;&#36229;&#22768;&#22270;&#20687;&#29983;&#25104;&#29992;&#20110;&#23454;&#38469;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Echo from noise: synthetic ultrasound image generation using diffusion models for real image segmentation. (arXiv:2305.05424v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#30340;&#26041;&#27861;&#26469;&#29983;&#25104;&#21512;&#25104;&#36229;&#22768;&#22270;&#20687;&#65292;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#22312;&#24038;&#24515;&#23460;&#21644;&#24038;&#25151;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20165;&#21512;&#25104;&#22270;&#20687;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;9.09&#65285;&#12289;3.7&#65285;&#21644;15.0&#65285;&#30340;Dice&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#21644;&#24515;&#33039;&#36229;&#22768;&#35821;&#20041;&#26631;&#31614;&#22320;&#22270;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#36825;&#20123;&#21512;&#25104;&#22270;&#20687;&#21487;&#20197;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#65292;&#22914;&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#21512;&#25104;&#30340;2D&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#23545;&#24038;&#24515;&#23460;&#21644;&#24038;&#25151;&#36827;&#34892;&#20998;&#21106;&#12290;&#22312;&#20165;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#26410;&#35265;&#36807;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23545;&#24038;&#24515;&#23460;&#20869;&#33180;&#12289;&#24515;&#22806;&#33180;&#21644;&#24038;&#24515;&#25151;&#30340;&#20998;&#21106;&#20998;&#21035;&#20135;&#29983;&#20102;88.5&#177;6.0&#65285;&#12289;92.3&#177;3.9&#65285;&#21644;86.3&#177;10.7&#65285;&#30340;&#24179;&#22343;Dice&#20998;&#25968;&#12290;&#36825;&#27604;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#25552;&#39640;&#20102;9.09&#65285;&#12289;3.7&#65285;&#21644;15.0&#65285;&#12290;&#35813;&#25552;&#35758;&#30340;&#26041;&#27861;&#22312;&#24212;&#29992;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel pipeline for the generation of synthetic images via Denoising Diffusion Probabilistic Models (DDPMs) guided by cardiac ultrasound semantic label maps. We show that these synthetic images can serve as a viable substitute for real data in the training of deep-learning models for medical image analysis tasks such as image segmentation. To demonstrate the effectiveness of this approach, we generated synthetic 2D echocardiography images and trained a neural network for segmentation of the left ventricle and left atrium. The performance of the network trained on exclusively synthetic images was evaluated on an unseen dataset of real images and yielded mean Dice scores of 88.5 $\pm 6.0$ , 92.3 $\pm 3.9$, 86.3 $\pm 10.7$ \% for left ventricular endocardial, epicardial and left atrial segmentation respectively. This represents an increase of $9.09$, $3.7$ and $15.0$ \% in Dice scores compared to the previous state-of-the-art. The proposed pipeline has the potential for applic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05402</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#12289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#19968;&#23478;&#20027;&#35201;&#32593;&#32476;&#20844;&#21496;&#24050;&#32463;&#22312;&#20351;&#29992;&#30340;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#22312;&#35813;&#27169;&#22411;&#26680;&#24515;&#20013;&#65292;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25509;&#21463;&#20135;&#21697;&#26631;&#39064;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20174;&#25968;&#21315;&#20010;&#21487;&#29992;&#20505;&#36873;&#39033;&#20013;&#36755;&#20986;&#26368;&#21512;&#36866;&#30340;&#31867;&#21035;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31867;&#20284;&#29289;&#21697;&#26631;&#31614;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#26631;&#39064;&#20013;&#20851;&#20110;&#39068;&#33394;&#25110;&#23610;&#23544;&#30340;&#23567;&#21464;&#21270;&#65292;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#30340;&#25512;&#33616;&#25110;&#25628;&#32034;&#24212;&#29992;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05400</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#25506;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#33104;&#36133;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Corruption Robustness of Image Classifiers with Random Lp-norm Corruptions. (arXiv:2305.05400v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#23545;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#65292;&#24182;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#65292;&#21457;&#29616;&#31283;&#20581;&#24615;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#20581;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#38752;&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23545;&#25239;&#31283;&#20581;&#24615;&#21644;&#24418;&#24335;&#31283;&#20581;&#24615;&#39564;&#35777;&#39046;&#22495;&#20013;&#65292;&#31283;&#20581;&#24615;&#36890;&#24120;&#34987;&#23450;&#20041;&#20026;&#22312;Lp&#33539;&#25968;&#36317;&#31163;&#20869;&#23545;&#25152;&#26377;&#36755;&#20837;&#21464;&#21270;&#30340;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#36890;&#24120;&#36890;&#36807;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#35266;&#23519;&#21040;&#30340;&#21464;&#21270;&#26469;&#25913;&#36827;&#21644;&#35780;&#20272;&#65292;&#32780;&#24456;&#23569;&#32771;&#34385;&#25968;&#23398;&#23450;&#20041;&#30340;Lp&#33539;&#25968;&#22833;&#30495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#38543;&#26426;Lp&#33539;&#25968;&#22833;&#30495;&#26469;&#22686;&#24378;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#23545;&#25239;&#31283;&#20581;&#24615;&#39046;&#22495;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#19981;&#21487;&#24863;&#30693;&#38543;&#26426;&#22833;&#30495;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23454;&#35777;&#21644;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;Lp&#33539;&#25968;&#20043;&#38388;&#31283;&#20581;&#24615;&#26159;&#21542;&#21487;&#36716;&#31227;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#21738;&#20123;Lp&#33539;&#25968;&#30340;&#22833;&#30495;&#24212;&#35813;&#29992;&#26469;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#25552;&#39640;&#27169;&#22411;&#22312;&#38543;&#26426;&#22833;&#30495;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#20294;&#20063;&#21487;&#33021;&#20250;&#25439;&#23475;L&#8734;&#33539;&#25968;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness is a fundamental property of machine learning classifiers to achieve safety and reliability. In the fields of adversarial robustness and formal robustness verification of image classification models, robustness is commonly defined as the stability to all input variations within an Lp-norm distance. However, robustness to random corruptions is usually improved and evaluated using variations observed in the real-world, while mathematically defined Lp-norm corruptions are rarely considered. This study investigates the use of random Lp-norm corruptions to augment the training and test data of image classifiers. We adapt an approach from the field of adversarial robustness to assess the model robustness to imperceptible random corruptions. We empirically and theoretically investigate whether robustness is transferable across different Lp-norms and derive conclusions on which Lp-norm corruptions a model should be trained and evaluated on. We find that training data augmentation wi
&lt;/p&gt;</description></item><item><title>SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05392</link><description>&lt;p&gt;
&#20851;&#20110;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05392
&lt;/p&gt;
&lt;p&gt;
SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;SAM&#21333;&#29420;&#20351;&#29992;&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#23545;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;&#65288;SAM&#65289;&#30340;&#26032;&#29702;&#35299;&#12290;&#26412;&#25991;&#25351;&#20986;&#65292;SAM&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65288;AT&#65289;&#37117;&#21487;&#20197;&#35270;&#20026;&#29305;&#23450;&#30340;&#29305;&#24449;&#25200;&#21160;&#65292;&#20854;&#25913;&#21892;&#20102;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;SAM&#21644;AT&#22312;&#25200;&#21160;&#24378;&#24230;&#26041;&#38754;&#26159;&#19981;&#21516;&#30340;&#65292;&#20174;&#32780;&#24102;&#26469;&#20102;&#19981;&#21516;&#30340;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#31616;&#21270;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#22768;&#26126;&#30340;&#29702;&#35770;&#35777;&#25454;&#21644;&#20005;&#26684;&#30340;&#25968;&#23398;&#35777;&#26126;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#20165;&#21033;&#29992;SAM&#21487;&#20197;&#23454;&#29616;&#27604;&#26631;&#20934;&#35757;&#32451;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36825;&#26159;&#24847;&#22806;&#30340;&#22909;&#22788;&#12290;&#30001;&#20110;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#23548;&#33268;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#38477;&#20302;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;SAM&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#28165;&#26224;&#24230;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/weizeming/SAM_AT&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#22270;&#30340;&#21551;&#21457;&#30340;&#30697;&#38453;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#22312;&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#20013;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05389</link><description>&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20013;&#30340;&#20004;&#21040;&#20116;&#20010;&#30495;&#30456;
&lt;/p&gt;
&lt;p&gt;
Two to Five Truths in Non-Negative Matrix Factorization. (arXiv:2305.05389v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#22270;&#30340;&#21551;&#21457;&#30340;&#30697;&#38453;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#22312;&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#20013;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26500;&#24314;&#20027;&#39064;&#27169;&#22411;&#26102;&#65292;&#30697;&#38453;&#32553;&#25918;&#22312;&#35745;&#25968;&#30697;&#38453;&#19978;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#22270;&#65288;NL&#65289; &#30340;&#21551;&#21457;&#30340;&#30697;&#38453;&#32553;&#25918;&#26041;&#27861;&#65292;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#36136;&#37327;&#12290;&#22312;&#25991;&#26412;&#20998;&#26512;&#20013;&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299; (NMF) &#36890;&#24120;&#29992;&#20110;&#35745;&#25968;&#30697;&#38453;&#30340;&#20849;&#29616;&#8220;&#19978;&#19979;&#25991;&#8221;&#21644;&#8220;&#26415;&#35821;&#8221;&#12290;&#21463; LSE &#30340;&#21551;&#21457;&#65292;&#30697;&#38453;&#32553;&#25918;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#20013;&#30340;&#25991;&#26412;&#20027;&#39064;&#27169;&#22411;&#37117;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#30697;&#38453;&#32553;&#25918;&#22312; NMF &#20013;&#30340;&#24040;&#22823;&#24433;&#21709;&#65292;&#21487;&#20197;&#22823;&#22823;&#25913;&#21892;&#20027;&#39064;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the role of matrix scaling on a matrix of counts when building a topic model using non-negative matrix factorization. We present a scaling inspired by the normalized Laplacian (NL) for graphs that can greatly improve the quality of a non-negative matrix factorization. The results parallel those in the spectral graph clustering work of \cite{Priebe:2019}, where the authors proved adjacency spectral embedding (ASE) spectral clustering was more likely to discover core-periphery partitions and Laplacian Spectral Embedding (LSE) was more likely to discover affinity partitions. In text analysis non-negative matrix factorization (NMF) is typically used on a matrix of co-occurrence ``contexts'' and ``terms" counts. The matrix scaling inspired by LSE gives significant improvement for text topic models in a variety of datasets. We illustrate the dramatic difference a matrix scalings in NMF can greatly improve the quality of a topic model on three datasets where human an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05379</link><description>&lt;p&gt;
TASTY&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26102;&#31354;&#22797;&#26434;&#24230;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TASTY: A Transformer based Approach to Space and Time complexitY. (arXiv:2305.05379v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#38750;&#24120;&#26377;&#21069;&#36884;&#30340;&#32467;&#26524;&#65292;&#22914;&#20195;&#30721;&#30340;&#23436;&#21892;&#12289;&#20195;&#30721;&#30340;&#34917;&#20840;&#21644;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#20174;&#20195;&#30721;&#20013;&#20998;&#31867;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#20219;&#21153;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#20165;&#38480;&#20110;Java&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#20195;&#30721;&#29255;&#27573;&#26631;&#35760;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65288;&#30446;&#21069;&#26159;Python&#21644;C ++&#25968;&#25454;&#38598;&#65292;&#19981;&#20037;&#23558;&#21457;&#24067;C&#65292;C&#65283;&#21644;JavaScript&#25968;&#25454;&#38598;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35745;&#31639;&#24211;&#21644;&#24037;&#20855;&#20165;&#36866;&#29992;&#20110;&#23569;&#25968;&#29992;&#20363;&#12290;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20419;&#20351;&#36816;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#20195;&#30721;&#30340;&#22810;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27515;&#20195;&#30721;&#28040;&#38500;&#21644;&#22686;&#21152;LM&#30340;&#26368;&#22823;&#24207;&#21015;&#38271;&#24230;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#26102;&#38388;&#22797;&#26434;&#24615;&#22806;&#65292;&#25105;&#20204;&#36824;&#24314;&#35758;&#20351;&#29992;LM&#26469;&#23547;&#25214;&#31354;&#38388;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code based Language Models (LMs) have shown very promising results in the field of software engineering with applications such as code refinement, code completion and generation. However, the task of time and space complexity classification from code has not been extensively explored due to a lack of datasets, with prior endeavors being limited to Java. In this project, we aim to address these gaps by creating a labelled dataset of code snippets spanning multiple languages (Python and C++ datasets currently, with C, C#, and JavaScript datasets being released shortly). We find that existing time complexity calculation libraries and tools only apply to a limited number of use-cases. The lack of a well-defined rule based system motivates the application of several recently proposed code-based LMs. We demonstrate the effectiveness of dead code elimination and increasing the maximum sequence length of LMs. In addition to time complexity, we propose to use LMs to find space complexities from
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32844;&#19994;&#20934;&#22791;&#25216;&#33021;&#65292;&#27604;&#36739;&#20102;GPT-3&#21644;Turbo-GPT3.5&#22312;1149&#20010;&#19987;&#19994;&#35748;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;Turbo-GPT3.5&#30340;&#36890;&#36807;&#29575;&#36798;&#21040;&#20102;100%&#12290;&#27169;&#22411;&#35777;&#26126;&#20102;&#22312;&#35745;&#31639;&#26426;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#25216;&#33021;&#21644;&#28508;&#21147;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05377</link><description>&lt;p&gt;
&#19987;&#19994;&#35748;&#35777;&#22522;&#20934;&#25968;&#25454;&#38598;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;500&#20010;&#32844;&#20301;
&lt;/p&gt;
&lt;p&gt;
Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models. (arXiv:2305.05377v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#27979;&#35797;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32844;&#19994;&#20934;&#22791;&#25216;&#33021;&#65292;&#27604;&#36739;&#20102;GPT-3&#21644;Turbo-GPT3.5&#22312;1149&#20010;&#19987;&#19994;&#35748;&#35777;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;Turbo-GPT3.5&#30340;&#36890;&#36807;&#29575;&#36798;&#21040;&#20102;100%&#12290;&#27169;&#22411;&#35777;&#26126;&#20102;&#22312;&#35745;&#31639;&#26426;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#30340;&#25216;&#33021;&#21644;&#28508;&#21147;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#19994;&#35748;&#35777;&#35843;&#26597;&#65292;&#20197;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#35780;&#20272;&#20854;&#23601;&#19994;&#25216;&#33021;&#12290;&#23427;&#27604;&#36739;&#20102;&#20004;&#20010;AI&#27169;&#22411;GPT-3&#21644;Turbo-GPT3.5&#22312;&#19968;&#20010;&#21253;&#25324;1149&#20010;&#19987;&#19994;&#35748;&#35777;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#30340;&#26159;&#32844;&#19994;&#20934;&#22791;&#32780;&#19981;&#26159;&#23398;&#26415;&#34920;&#29616;&#12290;GPT-3&#22312;39%&#30340;&#19987;&#19994;&#35748;&#35777;&#20013;&#21462;&#24471;&#20102;&#36890;&#36807;&#20998;&#25968;&#65288;&gt;70%&#27491;&#30830;&#29575;&#65289;&#65292;&#32780;&#27809;&#26377;&#36827;&#34892;&#24494;&#35843;&#25110;&#32771;&#35797;&#20934;&#22791;&#12290;&#27169;&#22411;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#30340;&#36164;&#26684;&#65292;&#22914;&#20113;&#21644;&#34394;&#25311;&#21270;&#12289;&#21830;&#19994;&#20998;&#26512;&#12289;&#32593;&#32476;&#35774;&#32622;&#21644;&#32500;&#20462;&#20197;&#21450;&#25968;&#25454;&#20998;&#26512;&#31561;&#12290;Turbo-GPT3.5&#22312;&#39047;&#20855;&#20215;&#20540;&#30340;Offensive Security Certified Professional&#65288;OSCP&#65289;&#32771;&#35797;&#20013;&#24471;&#20998;100%&#12290;&#27169;&#22411;&#36824;&#23637;&#29616;&#20102;&#22312;&#20854;&#20182;&#32844;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#25252;&#29702;&#12289;&#25345;&#29260;&#21672;&#35810;&#12289;&#33647;&#21058;&#23398;&#21644;&#25945;&#23398;&#20013;&#30340;&#33021;&#21147;&#12290;Turbo-GPT3.5&#22312;&#27809;&#26377;&#20934;&#22791;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20102;&#37329;&#34701;&#19994;&#30417;&#31649;&#23616;&#65288;FINRA&#65289;&#31995;&#21015;6&#32771;&#35797;&#30340;70%&#30340;&#25104;&#32489;&#12290;&#24863;&#20852;&#36259;&#30340;&#35835;&#32773;&#21487;&#20197;&#20351;&#29992;&#36825;&#20010;500&#20010;&#24037;&#20316;&#32844;&#20301;&#30340;&#25968;&#25454;&#38598;&#26469;&#36827;&#19968;&#27493;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#38469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (&gt;70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.05374</link><description>&lt;p&gt;
HybridNet: &#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
HybridNet: Dual-Branch Fusion of Geometrical and Topological Views for VLSI Congestion Prediction. (arXiv:2305.05374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HybridNet&#65292;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#19982;&#25299;&#25169;&#35270;&#35282;&#30340;VLSI&#38459;&#22622;&#39044;&#27979;&#30340;&#21452;&#20998;&#25903;&#34701;&#21512;&#32593;&#32476;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#38459;&#22622;&#39044;&#27979;&#26159;&#24110;&#21161;&#35774;&#35745;&#24072;&#22312;VLSI&#35774;&#35745;&#21608;&#26399;&#20869;&#26356;&#24555;&#36845;&#20195;&#30340;&#37325;&#35201;&#29615;&#33410;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#32593;&#32476;&#32467;&#26500;&#20013;&#20570;&#20986;&#20960;&#20010;&#20851;&#38190;&#35774;&#35745;&#65292;&#20805;&#20998;&#32508;&#21512;&#30005;&#36335;&#30340;&#25299;&#25169;&#19982;&#20960;&#20309;&#29305;&#24449;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#29420;&#31435;&#30340;&#22270;&#65288;&#20960;&#20309;&#22270;&#12289;&#25299;&#25169;&#22270;&#65289;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#21807;&#19968;&#23646;&#24615;&#37319;&#29992;&#19981;&#21516;&#30340;&#36793;&#32536;&#26500;&#24314;&#26041;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#20998;&#25903;&#32593;&#32476;&#65292;&#27599;&#20010;&#36335;&#24452;&#20013;&#37117;&#26377;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#24182;&#36890;&#36807;&#31934;&#32454;&#30340;&#34701;&#21512;&#31574;&#30053;&#36827;&#34892;&#32858;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32593;&#32476;&#21517;&#20026;HybridNet&#65292;&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#21333;&#20803;&#20043;&#38388;&#30340;&#20960;&#20309;&#20132;&#20114;&#65292;&#32780;&#19988;&#36824;&#20445;&#30041;&#20102;&#21407;&#22987;&#30005;&#36335;&#25299;&#25169;&#20851;&#31995;&#12290;&#22312;ISPD2015&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20197;&#24448;&#26041;&#27861;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;10.9&#65285;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate early congestion prediction can prevent unpleasant surprises at the routing stage, playing a crucial character in assisting designers to iterate faster in VLSI design cycles. In this paper, we introduce a novel strategy to fully incorporate topological and geometrical features of circuits by making several key designs in our network architecture. To be more specific, we construct two individual graphs (geometry-graph, topology-graph) with distinct edge construction schemes according to their unique properties. We then propose a dual-branch network with different encoder layers in each pathway and aggregate representations with a sophisticated fusion strategy. Our network, named HybridNet, not only provides a simple yet effective way to capture the geometric interactions of cells, but also preserves the original topological relationships in the netlist. Experimental results on the ISPD2015 benchmarks show that we achieve an improvement of 10.9% compared to previous methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05368</link><description>&lt;p&gt;
GNNs: &#21487;&#20197;&#26356;&#24378;&#12289;&#26356;&#26032;&#12289;&#26356;&#24555;
&lt;/p&gt;
&lt;p&gt;
GNNs,You can be Stronger,Deeper and Faster. (arXiv:2305.05368v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#19968;&#31867;&#21487;&#20197;&#20174;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#23398;&#20064;&#24182;&#36890;&#36807;&#38598;&#25104;&#37051;&#23621;&#33410;&#28857;&#30340;&#34920;&#31034;&#23398;&#20064;&#26469;&#34920;&#29616;&#20986;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;GNN&#30340;&#24615;&#33021;&#20250;&#38543;&#30528;&#23618;&#25968;&#22686;&#21152;&#32780;&#36880;&#28176;&#38477;&#20302;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27010;&#24565;&#8212;&#8212;k&#36339;&#23376;&#22270;&#32858;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35299;GNN&#34920;&#29616;&#33021;&#21147;&#30340;&#35270;&#35282;&#65292;&#25581;&#31034;&#20102;&#20256;&#32479;&#28145;&#23618;GNN&#34920;&#29616;&#36880;&#28176;&#36864;&#21270;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#21253;&#25324;&#32858;&#21512;&#23376;&#22270;&#30340;&#37325;&#21472;&#20197;&#21450;&#22522;&#20110;&#27531;&#24046;&#30340;GNN&#23454;&#38469;&#19978;&#21033;&#29992;&#20102;1&#21040;k&#36339;&#23376;&#22270;&#32858;&#21512;&#32467;&#26524;&#26469;&#25552;&#39640;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#33410;&#28857;&#32423;&#27531;&#24046;&#27169;&#22359;SDF&#65292;&#36890;&#36807;&#29702;&#35770;&#25512;&#23548;&#35777;&#26126;&#20854;&#27604;&#20043;&#21069;&#30340;&#27531;&#24046;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#33021;&#21147;&#65292;&#21487;&#20197;&#21033;&#29992;1&#21040;k&#36339;&#36291;&#23376;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), a type of neural network that can learn from graph-structured data and learn the representation of nodes by aggregating their neighbors, have shown excellent performance in downstream tasks.However, it is known that the performance of graph neural networks (GNNs) degrades gradually as the number of layers increases. Based on k-hop subgraph aggregation, which is a new concept, we propose a new perspective to understand the expressive power of GNN.From this perspective, we reveal the potential causes of the performance degradation of the deep traditional GNN - aggregated subgraph overlap, and the fact that the residual-based graph neural networks in fact exploit the aggregation results of 1 to k hop subgraphs to improve the effectiveness.Further, we propose a new sampling-based node-level residual module named SDF, which is shown by theoretical derivation to obtain a superior expressive power compared to previous residual methods by using information from 1 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#32463;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#33719;&#24471;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.05364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#32463;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#33719;&#24471;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#31034;&#20363;&#26469;&#25191;&#34892;&#25351;&#20196;&#24182;&#25191;&#34892;&#26032;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21442;&#25968;&#21270;LLMs&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#20197;&#27604;&#24494;&#35843;&#20302;&#24471;&#22810;&#30340;&#25104;&#26412;&#25299;&#23637;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#19968;&#25512;&#29702;&#32447;&#36335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;LLM&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#31572;&#30340;&#35828;&#26126;&#24615;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#32780;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#65292;&#22312;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#36335;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;6.4%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#35282;&#24230;&#31361;&#20986;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#25351;&#20986;&#20102;&#35813;&#37197;&#32622;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#27492;&#28431;&#27934;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.05355</link><description>&lt;p&gt;
&#23558;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
Turning Privacy-preserving Mechanisms against Federated Learning. (arXiv:2305.05355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#25351;&#20986;&#20102;&#35813;&#37197;&#32622;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#27492;&#28431;&#27934;&#30340;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26500;&#24314;&#20102;&#22686;&#24378;&#22411;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20174;&#30456;&#20851;&#23454;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#20013;&#23398;&#20064;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#32852;&#21512;&#23398;&#20064;&#20316;&#20026;&#24314;&#31435;&#20840;&#23616;GNN&#27169;&#22411;&#30340;&#26412;&#22320;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26080;&#38656;&#23558;&#25935;&#24863;&#25968;&#25454;&#25910;&#38598;&#21040;&#21333;&#20010;&#35745;&#31639;&#21333;&#20803;&#20013;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#21487;&#33021;&#20250;&#20986;&#29616;&#65292;&#22240;&#20026;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#29983;&#25104;&#30340;&#23616;&#37096;&#27169;&#22411;&#26356;&#26032;&#30340;&#20998;&#26512;&#21487;&#33021;&#20250;&#36820;&#22238;&#19982;&#25935;&#24863;&#26412;&#22320;&#25968;&#25454;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#19987;&#23478;&#20204;&#25552;&#20986;&#20102;&#23558;&#32852;&#21512;&#23398;&#20064;&#19982;&#24046;&#20998;&#38544;&#31169;&#31574;&#30053;&#21644;&#31038;&#21306;&#39537;&#21160;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#28041;&#21450;&#23558;&#26469;&#33258;&#37051;&#23621;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#32452;&#21512;&#36215;&#26469;&#65292;&#20351;&#20010;&#20307;&#23616;&#37096;&#26356;&#26032;&#19981;&#37027;&#20040;&#20381;&#36182;&#20110;&#23616;&#37096;&#25935;&#24863;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#31181;&#37197;&#32622;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#25915;&#20987;&#65292;&#33021;&#22815;&#27450;&#39575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, researchers have successfully employed Graph Neural Networks (GNNs) to build enhanced recommender systems due to their capability to learn patterns from the interaction between involved entities. In addition, previous studies have investigated federated learning as the main solution to enable a native privacy-preserving mechanism for the construction of global GNN models without collecting sensitive data into a single computation unit. Still, privacy issues may arise as the analysis of local model updates produced by the federated clients can return information related to sensitive local data. For this reason, experts proposed solutions that combine federated learning with Differential Privacy strategies and community-driven approaches, which involve combining data from neighbor clients to make the individual local updates less dependent on local sensitive data. In this paper, we identify a crucial security flaw in such a configuration, and we design an attack capable of dece
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#27861;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#21457;&#29616;&#20854;&#32534;&#30721;&#30340;&#34920;&#31034;&#21487;&#33021;&#19982;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#24182;&#19981;&#20005;&#26684;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.05349</link><description>&lt;p&gt;
&#26088;&#22312;&#34920;&#24449;&#22522;&#20110;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards the Characterization of Representations Learned via Capsule-based Network Architectures. (arXiv:2305.05349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#27861;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#21457;&#29616;&#20854;&#32534;&#30721;&#30340;&#34920;&#31034;&#21487;&#33021;&#19982;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#24182;&#19981;&#20005;&#26684;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;&#20316;&#20026;&#26631;&#20934;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26356;&#20026;&#32039;&#20945;&#21644;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#26041;&#27861;&#32780;&#37325;&#26032;&#24341;&#20837;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#21387;&#32553;&#33021;&#21147;&#65292;&#20294;&#33267;&#20170;&#23578;&#26410;&#23436;&#20840;&#35780;&#20272;&#20854;&#21487;&#35299;&#37322;&#24615;&#36136;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#32780;&#21407;&#21017;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#36825;&#31181;&#31867;&#22411;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29305;&#21035;&#27880;&#24847;&#20998;&#26512;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#20013;&#26159;&#21542;&#30830;&#23454;&#32534;&#30721;&#20102;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#30340;&#27700;&#24179;&#12290;&#22312;MNIST&#12289;SVHN&#12289;PASCAL-part&#21644;CelebA&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;CapsNets&#20013;&#32534;&#30721;&#30340;&#34920;&#31034;&#21487;&#33021;&#26082;&#19981;&#20687;&#25991;&#29486;&#20013;&#36890;&#24120;&#25152;&#36848;&#30340;&#37027;&#26679;&#20998;&#31163;&#65292;&#20063;&#19981;&#26159;&#20005;&#26684;&#19982;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capsule Networks (CapsNets) have been re-introduced as a more compact and interpretable alternative to standard deep neural networks. While recent efforts have proved their compression capabilities, to date, their interpretability properties have not been fully assessed. Here, we conduct a systematic and principled study towards assessing the interpretability of these types of networks. Moreover, we pay special attention towards analyzing the level to which part-whole relationships are indeed encoded within the learned representation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA datasets suggest that the representations encoded in CapsNets might not be as disentangled nor strictly related to parts-whole relationships as is commonly stated in the literature.
&lt;/p&gt;</description></item><item><title>Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#29575;&#21487;&#36798;99.95&#65285;&#65292;&#26377;&#28508;&#21147;&#29992;&#20110;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#30340;&#36890;&#20449;&#35774;&#22791;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30456;&#20284;&#25163;&#35821;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.05296</link><description>&lt;p&gt;
Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mediapipe and CNNs for Real-Time ASL Gesture Recognition. (arXiv:2305.05296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05296
&lt;/p&gt;
&lt;p&gt;
Mediapipe&#21644;CNN&#29992;&#20110;&#23454;&#26102;&#32654;&#22269;&#25163;&#35821;&#25163;&#21183;&#35782;&#21035;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#20934;&#30830;&#29575;&#21487;&#36798;99.95&#65285;&#65292;&#26377;&#28508;&#21147;&#29992;&#20110;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#30340;&#36890;&#20449;&#35774;&#22791;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#30456;&#20284;&#25163;&#35821;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#20351;&#29992;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#35782;&#21035;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#36816;&#21160;&#30340;&#23454;&#26102;&#31995;&#32479;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Mediapipe&#24211;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#36827;&#34892;ASL&#25163;&#21183;&#20998;&#31867;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#20197;99.95&#65285;&#30340;&#20934;&#30830;&#29575;&#26816;&#27979;&#25152;&#26377;ASL&#23383;&#27597;&#65292;&#34920;&#26126;&#23427;&#22312;&#20026;&#21548;&#21147;&#38556;&#30861;&#20154;&#22763;&#35774;&#35745;&#30340;&#36890;&#20449;&#35774;&#22791;&#20013;&#26377;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#20855;&#26377;&#30456;&#20284;&#25163;&#37096;&#36816;&#21160;&#30340;&#25163;&#35821;&#65292;&#20174;&#32780;&#21487;&#33021;&#25552;&#39640;&#21548;&#21147;&#20007;&#22833;&#20154;&#22763;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;Mediapipe&#21644;CNN&#36827;&#34892;&#23454;&#26102;&#25163;&#35821;&#35782;&#21035;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research paper describes a realtime system for identifying American Sign Language (ASL) movements that employs modern computer vision and machine learning approaches. The suggested method makes use of the Mediapipe library for feature extraction and a Convolutional Neural Network (CNN) for ASL gesture classification. The testing results show that the suggested system can detect all ASL alphabets with an accuracy of 99.95%, indicating its potential for use in communication devices for people with hearing impairments. The proposed approach can also be applied to additional sign languages with similar hand motions, potentially increasing the quality of life for people with hearing loss. Overall, the study demonstrates the effectiveness of using Mediapipe and CNN for real-time sign language recognition, making a significant contribution to the field of computer vision and machine learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#30423;&#31363;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#22312;&#23454;&#38469;&#30423;&#31363;&#36807;&#31243;&#20013;&#30456;&#20114;&#19981;&#30830;&#23450;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20316;&#32773;&#23581;&#35797;&#20351;&#29992;&#22810;&#20010;&#21487;&#33021;&#30340;&#32593;&#32476;&#24182;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#32452;&#21512;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#20294;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#24494;&#24369;&#30340;&#25913;&#21892;&#12290;&#20316;&#32773;&#21457;&#29616;&#32593;&#32476;&#22810;&#26679;&#24615;&#19981;&#36275;&#26159;&#23548;&#33268;&#36825;&#19968;&#32467;&#26524;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.05293</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#30423;&#31363;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
On the Limitations of Model Stealing with Uncertainty Quantification Models. (arXiv:2305.05293v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#30423;&#31363;&#30340;&#23616;&#38480;&#24615;&#65292;&#21457;&#29616;&#22312;&#23454;&#38469;&#30423;&#31363;&#36807;&#31243;&#20013;&#30456;&#20114;&#19981;&#30830;&#23450;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20316;&#32773;&#23581;&#35797;&#20351;&#29992;&#22810;&#20010;&#21487;&#33021;&#30340;&#32593;&#32476;&#24182;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#32452;&#21512;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#20294;&#32467;&#26524;&#34920;&#26126;&#21482;&#26377;&#24494;&#24369;&#30340;&#25913;&#21892;&#12290;&#20316;&#32773;&#21457;&#29616;&#32593;&#32476;&#22810;&#26679;&#24615;&#19981;&#36275;&#26159;&#23548;&#33268;&#36825;&#19968;&#32467;&#26524;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#30423;&#31363;&#26088;&#22312;&#20197;&#21407;&#22987;&#35757;&#32451;&#25104;&#26412;&#30340;&#19968;&#23567;&#37096;&#20998;&#25512;&#26029;&#21463;&#23475;&#32773;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#23610;&#23544;&#21644;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#26080;&#27861;&#20934;&#30830;&#30830;&#23450;&#65292;&#23548;&#33268;&#22312;&#30423;&#31363;&#36807;&#31243;&#20013;&#30456;&#20114;&#19981;&#30830;&#23450;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#21487;&#33021;&#30340;&#32593;&#32476;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#32452;&#21512;&#36215;&#26469;&#26469;&#26174;&#24335;&#22320;&#22788;&#29702;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#30423;&#31363;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#27169;&#22411;&#22312;&#27169;&#22411;&#30423;&#31363;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#32771;&#34385;&#30340;&#27169;&#22411;&#22312;&#26631;&#31614;&#19968;&#33268;&#24615;&#65288;&#21363;&#20445;&#30495;&#24230;&#65289;&#26041;&#38754;&#21482;&#33021;&#24102;&#26469;&#24494;&#23567;&#30340;&#25913;&#36827;&#12290;&#20026;&#20102;&#25214;&#21040;&#21407;&#22240;&#65292;&#25105;&#20204;&#36890;&#36807;&#26597;&#30475;&#39044;&#27979;&#26041;&#24046;&#20316;&#20026;&#35757;&#32451;&#36845;&#20195;&#20989;&#25968;&#30340;&#26041;&#24335;&#26469;&#26816;&#26597;&#27169;&#22411;&#39044;&#27979;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#24448;&#24448;&#20855;&#26377;&#30456;&#20284;&#30340;&#39044;&#27979;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#24819;&#35201;&#21033;&#29992;&#30340;&#32593;&#32476;&#22810;&#26679;&#24615;&#19981;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model stealing aims at inferring a victim model's functionality at a fraction of the original training cost. While the goal is clear, in practice the model's architecture, weight dimension, and original training data can not be determined exactly, leading to mutual uncertainty during stealing. In this work, we explicitly tackle this uncertainty by generating multiple possible networks and combining their predictions to improve the quality of the stolen model. For this, we compare five popular uncertainty quantification models in a model stealing task. Surprisingly, our results indicate that the considered models only lead to marginal improvements in terms of label agreement (i.e., fidelity) to the stolen model. To find the cause of this, we inspect the diversity of the model's prediction by looking at the prediction variance as a function of training iterations. We realize that during training, the models tend to have similar predictions, indicating that the network diversity we wanted
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#23545;&#35805;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24067;&#26391;&#26725;&#36807;&#31243;&#24314;&#27169;&#23545;&#35805;&#36335;&#24452;&#30340;&#26102;&#38388;&#21160;&#24577;&#20197;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05290</link><description>&lt;p&gt;
&#36890;&#36807;&#24067;&#26391;&#26725;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#20027;&#21160;&#23545;&#35805;&#30340;&#23545;&#35805;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue. (arXiv:2305.05290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#23545;&#35805;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24067;&#26391;&#26725;&#36807;&#31243;&#24314;&#27169;&#23545;&#35805;&#36335;&#24452;&#30340;&#26102;&#38388;&#21160;&#24577;&#20197;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#20027;&#21160;&#22320;&#36798;&#21040;&#39044;&#20808;&#30830;&#23450;&#30340;&#30446;&#26631;&#12290;&#23454;&#29616;&#27492;&#20219;&#21153;&#30340;&#20851;&#38190;&#22312;&#20110;&#35268;&#21010;&#23545;&#35805;&#36335;&#24452;&#65292;&#20351;&#20854;&#24179;&#31283;&#24182;&#36830;&#36143;&#22320;&#25351;&#21521;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#26410;&#34987;&#28145;&#20837;&#25506;&#31350;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#36143;&#23545;&#35805;&#35268;&#21010;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#26469;&#24314;&#27169;&#23545;&#35805;&#36335;&#24452;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#36890;&#36807;&#24067;&#26391;&#26725;&#36807;&#31243;&#25429;&#25417;&#20102;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#36830;&#36143;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#28789;&#27963;&#22320;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#23545;&#35805;&#35268;&#21010;&#12290;&#22522;&#20110;&#23548;&#20986;&#30340;&#28508;&#22312;&#36712;&#36857;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26174;&#24335;&#22320;&#29983;&#25104;&#23545;&#35805;&#36335;&#24452;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36335;&#24452;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24341;&#23548;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#36830;&#36143;&#30340;&#35805;&#35821;&#65292;&#24182;&#20197;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#23454;&#29616;&#20102;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#26410;&#35266;&#23519;&#21464;&#37327;&#32780;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20986;&#27491;&#21017;&#26465;&#20214;&#25511;&#21046;&#31163;&#25955;&#21270;&#35823;&#24046;&#26469;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.05281</link><description>&lt;p&gt;
&#24102;&#26377;&#26410;&#35266;&#23519;&#21464;&#37327;&#30340;&#22240;&#26524;&#20851;&#31995;&#21457;&#29616;&#65306;&#19968;&#31181;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Unobserved Variables: A Proxy Variable Approach. (arXiv:2305.05281v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#21464;&#37327;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22240;&#26410;&#35266;&#23519;&#21464;&#37327;&#32780;&#22312;&#35266;&#27979;&#25968;&#25454;&#20013;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#36866;&#29992;&#20110;&#36830;&#32493;&#21464;&#37327;&#31995;&#32479;&#65292;&#36890;&#36807;&#25552;&#20986;&#27491;&#21017;&#26465;&#20214;&#25511;&#21046;&#31163;&#25955;&#21270;&#35823;&#24046;&#26469;&#35782;&#21035;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26410;&#35266;&#23519;&#21464;&#37327;&#65288;&#20363;&#22914;&#28508;&#22312;&#28151;&#26434;&#25110;&#20013;&#20171;&#65289;&#30340;&#23384;&#22312;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#22240;&#26524;&#35782;&#21035;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#31471;&#22240;&#26524;&#25506;&#32034;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#26410;&#35266;&#23519;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#35843;&#25972;&#20559;&#24046;&#12290;&#29305;&#21035;&#22320;&#65292;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#30340;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;&#24341;&#21457;&#30340;&#32447;&#24615;&#36829;&#35268;&#26469;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#26377;&#20005;&#26684;&#32423;&#21035;&#32422;&#26463;&#30340;&#31163;&#25955;&#25968;&#25454;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25193;&#23637;&#36817;&#31471;&#20551;&#35774;&#26816;&#39564;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#36866;&#29992;&#20110;&#30001;&#36830;&#32493;&#21464;&#37327;&#32452;&#25104;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#26159;&#25552;&#20986;&#32473;&#23450;&#38544;&#34255;&#22240;&#23376;&#30340;&#35266;&#27979;&#21464;&#37327;&#30340;&#26465;&#20214;&#20998;&#24067;&#30340;&#27491;&#21017;&#26465;&#20214;&#65292;&#20351;&#24471;&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#35266;&#23519;&#20195;&#29702;&#20197;&#36275;&#22815;&#30340;&#26377;&#38480;&#32454;&#26684;&#31163;&#25955;&#21270;&#65292;&#21017;&#28041;&#21450;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#21487;&#20197;&#26377;&#25928;&#22320;&#21463;&#21040;&#25511;&#21046;&#12290;&#22522;&#20110;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#20855;&#26377;&#19968;&#33324;&#20808;&#39564;&#38480;&#21046;&#30340;&#26032;&#36817;&#31471;&#22240;&#26524;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering causal relations from observational data is important. The existence of unobserved variables (e.g. latent confounding or mediation) can mislead the causal identification. To overcome this problem, proximal causal discovery methods attempted to adjust for the bias via the proxy of the unobserved variable. Particularly, hypothesis test-based methods proposed to identify the causal edge by testing the induced violation of linearity. However, these methods only apply to discrete data with strict level constraints, which limits their practice in the real world. In this paper, we fix this problem by extending the proximal hypothesis test to cases where the system consists of continuous variables. Our strategy is to present regularity conditions on the conditional distributions of the observed variables given the hidden factor, such that if we discretize its observed proxy with sufficiently fine, finite bins, the involved discretization error can be effectively controlled. Based o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05276</link><description>&lt;p&gt;
&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Subsampled Time Series with Proxy Variables. (arXiv:2305.05276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#30340;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#20013;&#26080;&#38656;&#21442;&#25968;&#32422;&#26463;&#22320;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26159;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#30340;&#26680;&#24515;&#20852;&#36259;&#12290;&#37319;&#26679;&#39057;&#29575;&#36828;&#20302;&#20110;&#22240;&#26524;&#24433;&#21709;&#39057;&#29575;&#26159;&#27492;&#31867;&#25512;&#26029;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#35201;&#20040;&#23616;&#38480;&#20110;&#32447;&#24615;&#24773;&#20917;&#65292;&#35201;&#20040;&#26080;&#27861;&#24314;&#31435;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#21442;&#25968;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#20174;&#23376;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#35782;&#21035;&#25972;&#20010;&#22240;&#26524;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#30340;&#24605;&#24819;&#26159;&#65292;&#23376;&#37319;&#26679;&#30340;&#25361;&#25112;&#20027;&#35201;&#26469;&#33258;&#20110;&#8220;&#26410;&#35266;&#23519;&#21040;&#8221;&#30340;&#26102;&#38388;&#27493;&#65292;&#22240;&#27492;&#24212;&#20351;&#29992;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#35774;&#35745;&#30340;&#24037;&#20855;&#22788;&#29702;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#24037;&#20855;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20195;&#29702;&#21464;&#37327;&#26041;&#27861;&#29305;&#21035;&#36866;&#21512;&#65292;&#22240;&#20026;&#26410;&#35266;&#23519;&#21040;&#21464;&#37327;&#30340;&#20195;&#29702;&#21464;&#37327;&#33258;&#28982;&#26159;&#22312;&#35266;&#23519;&#21040;&#30340;&#26102;&#38388;&#27493;&#19978;&#26412;&#36523;&#12290;&#26681;&#25454;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#32467;&#26500;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring causal structures from time series data is the central interest of many scientific inquiries. A major barrier to such inference is the problem of subsampling, i.e., the frequency of measurements is much lower than that of causal influence. To overcome this problem, numerous model-based and model-free methods have been proposed, yet either limited to the linear case or failed to establish identifiability. In this work, we propose a model-free algorithm that can identify the entire causal structure from subsampled time series, without any parametric constraint. The idea is that the challenge of subsampling arises mainly from \emph{unobserved} time steps and therefore should be handled with tools designed for unobserved variables. Among these tools, we find the proxy variable approach particularly fits, in the sense that the proxy of an unobserved variable is naturally itself at the observed time step. Following this intuition, we establish comprehensive structural identifiabili
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26597;&#34920;&#27861;&#20195;&#26367;CNN&#20013;&#20056;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20445;&#30041;&#20102;&#20027;&#35201;CNN&#25805;&#20316;&#30340;&#35821;&#20041;&#65292;&#19988;&#21487;&#22312;FPGA&#23454;&#29616;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#33021;&#37327;&#38477;&#20302;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.05274</link><description>&lt;p&gt;
DietCNN:&#29992;&#20110;&#37327;&#21270;CNN&#30340;&#26080;&#20056;&#31215;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
DietCNN: Multiplication-free Inference for Quantized CNNs. (arXiv:2305.05274v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26597;&#34920;&#27861;&#20195;&#26367;CNN&#20013;&#20056;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#20445;&#30041;&#20102;&#20027;&#35201;CNN&#25805;&#20316;&#30340;&#35821;&#20041;&#65292;&#19988;&#21487;&#22312;FPGA&#23454;&#29616;&#20013;&#23454;&#29616;&#26174;&#33879;&#30340;&#33021;&#37327;&#38477;&#20302;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23884;&#20837;&#24335;&#31995;&#32479;&#19982;&#26426;&#22120;&#26234;&#33021;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#38656;&#27714;&#24050;&#32463;&#25104;&#20026;&#20419;&#36827;&#30740;&#31350;&#30028;&#22312;&#23884;&#20837;&#24335;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#25512;&#26029;&#30340;&#20652;&#21270;&#21058;&#12290;&#21024;&#38500;&#26114;&#36149;&#30340;&#20056;&#27861;&#25805;&#20316;&#26469;&#37325;&#26032;&#35774;&#35745;CNN&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20943;&#23569;&#25512;&#26029;&#33021;&#37327;&#20351;&#29992;&#26041;&#38754;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#25928;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#26597;&#34920;&#27861;&#20195;&#26367;CNN&#20013;&#20056;&#27861;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#23436;&#20840;&#20462;&#25913;CNN&#25805;&#20316;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;&#20027;&#35201;CNN&#25805;&#20316;&#30340;&#35821;&#20041;&#12290;&#31526;&#21512;CNN&#23618;&#25805;&#20316;&#30340;&#29616;&#26377;&#26426;&#21046;&#30830;&#20445;&#20102;&#26631;&#20934;CNN&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22522;&#20110;&#21333;&#20010;&#28608;&#27963;&#30721;&#26412;&#30340;&#26080;&#20056;&#31215;CNN&#22312;MNIST-LeNet-5&#12289;CIFAR10-VGG-11&#21644;Tiny ImageNet-ResNet&#30340;FPGA&#23454;&#29616;&#20013;&#65292;&#27599;&#27425;&#25512;&#29702;&#33021;&#22815;&#23454;&#29616;4.7&#20493;&#12289;5.6&#20493;&#21644;3.5&#20493;&#30340;&#33021;&#37327;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rising demand for networked embedded systems with machine intelligence has been a catalyst for sustained attempts by the research community to implement Convolutional Neural Networks (CNN) based inferencing on embedded resource-limited devices. Redesigning a CNN by removing costly multiplication operations has already shown promising results in terms of reducing inference energy usage. This paper proposes a new method for replacing multiplications in a CNN by table look-ups. Unlike existing methods that completely modify the CNN operations, the proposed methodology preserves the semantics of the major CNN operations. Conforming to the existing mechanism of the CNN layer operations ensures that the reliability of a standard CNN is preserved. It is shown that the proposed multiplication-free CNN, based on a single activation codebook, can achieve 4.7x, 5.6x, and 3.5x reduction in energy per inference in an FPGA implementation of MNIST-LeNet-5, CIFAR10-VGG-11, and Tiny ImageNet-ResNet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#23383;&#31526;&#34920;&#31034;&#32534;&#30721;&#32454;&#31890;&#24230;&#21457;&#38899;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22768;&#23398;&#20559;&#32622;&#65292;&#20197;&#25552;&#39640;&#38024;&#23545;&#22768;&#38899;&#30456;&#20284;&#24615;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#65292;&#22312;&#31070;&#32463;&#21464;&#25442;&#22120;&#31561;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24615;&#33021;&#20013;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.05271</link><description>&lt;p&gt;
&#31070;&#32463;&#21464;&#25442;&#22120;&#20013;&#30340;&#40065;&#26834;&#24615;&#35821;&#38899;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition. (arXiv:2305.05271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#23383;&#31526;&#34920;&#31034;&#32534;&#30721;&#32454;&#31890;&#24230;&#21457;&#38899;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22768;&#23398;&#20559;&#32622;&#65292;&#20197;&#25552;&#39640;&#38024;&#23545;&#22768;&#38899;&#30456;&#20284;&#24615;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#65292;&#22312;&#31070;&#32463;&#21464;&#25442;&#22120;&#31561;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24615;&#33021;&#20013;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#24050;&#32463;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;E2E ASR&#65289;&#31995;&#32479;&#20013;&#65292;&#22914;&#31070;&#32463;&#21464;&#25442;&#22120;&#20013;&#65292;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#20247;&#30340;&#25110;&#20010;&#24615;&#21270;&#30340;&#32597;&#35265;&#35789;&#30340;&#35782;&#21035;&#26356;&#26159;&#22914;&#27492;&#12290;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#23558;&#27169;&#22411;&#20559;&#32622;&#20110;&#27880;&#20837;&#20026;&#20559;&#32622;&#30701;&#35821;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#23454;&#20307;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23376;&#21333;&#35789;&#32534;&#30721;&#22120;&#26469;&#32534;&#30721;&#20559;&#32622;&#30701;&#35821;&#12290;&#28982;&#32780;&#65292;&#23376;&#21333;&#35789;&#26631;&#35760;&#31895;&#31961;&#65292;&#26080;&#27861;&#25429;&#25417;&#20851;&#38190;&#30340;&#21457;&#38899;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#22522;&#20110;&#22768;&#38899;&#30456;&#20284;&#24615;&#30340;&#20559;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#36731;&#37327;&#32423;&#23383;&#31526;&#34920;&#31034;&#26469;&#32534;&#30721;&#32454;&#31890;&#24230;&#30340;&#21457;&#38899;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21463;&#22768;&#38899;&#30456;&#20284;&#24615;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#65288;&#31216;&#20026;&#22768;&#23398;&#20559;&#32622;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(NLM)&#30340;&#32534;&#30721;&#22120;&#65292;&#23558;&#35805;&#35821;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#19982;&#19978;&#19979;&#25991;&#23454;&#20307;&#19968;&#36215;&#32534;&#30721;&#20197;&#25552;&#39640;&#19979;&#25991;&#20559;&#32622;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based contextual biasing approaches have shown significant improvements in the recognition of generic and/or personal rare-words in End-to-End Automatic Speech Recognition (E2E ASR) systems like neural transducers. These approaches employ cross-attention to bias the model towards specific contextual entities injected as bias-phrases to the model. Prior approaches typically relied on subword encoders for encoding the bias phrases. However, subword tokenizations are coarse and fail to capture granular pronunciation information which is crucial for biasing based on acoustic similarity. In this work, we propose to use lightweight character representations to encode fine-grained pronunciation features to improve contextual biasing guided by acoustic similarity between the audio and the contextual entities (termed acoustic biasing). We further integrate pretrained neural language model (NLM) based encoders to encode the utterance's semantic context along with contextual entities to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102; multi-label &#23398;&#20064;&#20013;&#24120;&#29992;&#30340; Macro-AUC &#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#19981;&#24179;&#34913;&#23545;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26410;&#32463;&#21464;&#37327;&#22788;&#29702;&#30340;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#21487;&#33021;&#30001;&#20110;&#23545;&#26631;&#31614;&#30340;&#19981;&#24179;&#34913;&#26356;&#25935;&#24863;&#32780;&#34920;&#29616;&#36739;&#24046;&#65292;&#36825;&#19968;&#32467;&#35770;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.05248</link><description>&lt;p&gt;
&#20851;&#20110;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;Macro-AUC&#30340;&#27867;&#21270;&#29702;&#35299;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Generalization of Macro-AUC in Multi-label Learning. (arXiv:2305.05248v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102; multi-label &#23398;&#20064;&#20013;&#24120;&#29992;&#30340; Macro-AUC &#30340;&#27867;&#21270;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#26631;&#31614;&#19981;&#24179;&#34913;&#23545;&#27867;&#21270;&#30028;&#38480;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26410;&#32463;&#21464;&#37327;&#22788;&#29702;&#30340;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#21487;&#33021;&#30001;&#20110;&#23545;&#26631;&#31614;&#30340;&#19981;&#24179;&#34913;&#26356;&#25935;&#24863;&#32780;&#34920;&#29616;&#36739;&#24046;&#65292;&#36825;&#19968;&#32467;&#35770;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#65292;Macro-AUC&#26159;&#31867;&#20869;AUC&#31639;&#26415;&#24179;&#22343;&#20540;&#65292;&#36890;&#24120;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#29702;&#35770;&#29702;&#35299;&#36828;&#36828;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#23545;&#24212;&#30340;&#20195;&#29702;&#25439;&#22833;&#20989;&#25968;&#34920;&#24449;&#21508;&#31181;&#23398;&#20064;&#31639;&#27861;&#30340;&#23439;AUC&#30340;&#27867;&#21270;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30830;&#23450;&#20102;&#24433;&#21709;&#27867;&#21270;&#30028;&#38480;&#30340;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#26631;&#31614;&#31867;&#21035;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#23545;&#19981;&#24179;&#34913;&#24863;&#30693;&#35823;&#24046;&#30028;&#38480;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#26410;&#32463;&#21464;&#37327;&#22788;&#29702;&#30340;&#22522;&#20110;&#25439;&#22833;&#20989;&#25968;&#30340;&#31639;&#27861;&#27604;&#25552;&#20986;&#30340;&#22522;&#20110;&#25104;&#23545;&#21644;&#37325;&#26032;&#21152;&#26435;&#30340;&#31639;&#27861;&#26356;&#25935;&#24863;&#20110;&#26631;&#31614;&#31867;&#21035;&#30340;&#19981;&#24179;&#34913;&#65292;&#36825;&#21487;&#33021;&#24847;&#21619;&#30528;&#23427;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#32463;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#23601;&#25216;&#26415;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#65288;&#26356;&#36890;&#29992;&#30340;&#65289;McDiarmid&#22411;&#38598;&#20013;&#19981;&#31561;&#24335;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Macro-AUC is the arithmetic mean of the class-wise AUCs in multi-label learning and is commonly used in practice. However, its theoretical understanding is far lacking. Toward solving it, we characterize the generalization properties of various learning algorithms based on the corresponding surrogate losses w.r.t. Macro-AUC. We theoretically identify a critical factor of the dataset affecting the generalization bounds: \emph{the label-wise class imbalance}. Our results on the imbalance-aware error bounds show that the widely-used univariate loss-based algorithm is more sensitive to the label-wise class imbalance than the proposed pairwise and reweighted loss-based ones, which probably implies its worse performance. Moreover, empirical results on various datasets corroborate our theory findings. To establish it, technically, we propose a new (and more general) McDiarmid-type concentration inequality, which may be of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#21512;&#25104;&#21311;&#21517;&#21270;&#30149;&#20154;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21644;&#22521;&#35757;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#25968;&#25454;&#35775;&#38382;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.05247</link><description>&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65306;&#24179;&#34913;&#30740;&#31350;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy. (arXiv:2305.05247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#21512;&#25104;&#21311;&#21517;&#21270;&#30149;&#20154;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21644;&#22521;&#35757;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24179;&#34913;&#25968;&#25454;&#35775;&#38382;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#26410;&#26469;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#21644;&#25968;&#23383;&#21270;&#21307;&#30103;&#25968;&#25454;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20419;&#20351;&#20102;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#27934;&#35265;&#20197;&#22686;&#24378;&#30149;&#24739;&#32467;&#26524;&#65292;&#24314;&#31435;&#35786;&#26029;&#21644;&#27835;&#30103;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#30495;&#23454;&#30340;&#30149;&#20154;&#25968;&#25454;&#20250;&#23548;&#33268;&#38544;&#31169;&#21644;&#30417;&#31649;&#25361;&#25112;&#65292;&#21253;&#25324;HIPAA&#21644;GDPR&#30340;&#21512;&#35268;&#35201;&#27714;&#12290;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#65292;&#22914;GAN&#21644;VAE&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#24179;&#34913;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#35775;&#38382;&#21644;&#30149;&#20154;&#38544;&#31169;&#20445;&#25252;&#12290;&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#21019;&#24314;&#36924;&#30495;&#30340;&#21311;&#21517;&#21270;&#30149;&#20154;&#25968;&#25454;&#36827;&#34892;&#30740;&#31350;&#21644;&#22521;&#35757;&#65292;&#25506;&#32034;&#21512;&#25104;&#25968;&#25454;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#35752;&#35770;&#20854;&#30410;&#22788;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#21512;&#25104;&#25968;&#25454;&#26377;&#28508;&#21147;&#36890;&#36807;&#25552;&#20379;&#21311;&#21517;&#30149;&#20154;&#25968;&#25454;&#26469;&#38761;&#21629;&#21270;&#21307;&#30103;&#20445;&#20581;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#24182;&#23454;&#29616;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA and GDPR. Synthetic data generation, using generative AI models like GANs and VAEs offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training, explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Learnable Behavioral Control (LBC)&#26694;&#26550;&#65292;&#20351;&#24471;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#24471;&#21040;&#25193;&#22823;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#22312;Atari&#28216;&#25103;&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#36798;&#21040;10&#20010;&#28216;&#25103;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.05239</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#25511;&#21046;&#65306;&#36890;&#36807;&#39640;&#25928;&#34892;&#20026;&#36873;&#25321;&#25171;&#30772;Atari&#20154;&#31867;&#19990;&#30028;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection. (arXiv:2305.05239v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;Learnable Behavioral Control (LBC)&#26694;&#26550;&#65292;&#20351;&#24471;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#24471;&#21040;&#25193;&#22823;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#22312;Atari&#28216;&#25103;&#19978;&#65292;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#36798;&#21040;10&#20010;&#28216;&#25103;&#30340;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#25506;&#32034;&#38382;&#39064;&#26159;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#24037;&#20316;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;&#32676;&#20307;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#19981;&#21516;&#25506;&#32034;&#31574;&#30053;&#30340;&#20154;&#32676;&#20013;&#25910;&#38598;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#26679;&#26412;&#12290;&#33258;&#36866;&#24212;&#31574;&#30053;&#36873;&#25321;&#24050;&#34987;&#29992;&#20110;&#34892;&#20026;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#39044;&#23450;&#20041;&#31574;&#30053;&#31181;&#32676;&#30340;&#38480;&#21046;&#65292;&#36825;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#34892;&#20026;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#31216;&#20026;&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#25511;&#21046;&#65288;LBC&#65289;&#26469;&#35299;&#20915;&#36825;&#31181;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;a)&#36890;&#36807;&#20174;&#25152;&#26377;&#31574;&#30053;&#20013;&#21046;&#23450;&#28151;&#21512;&#34892;&#20026;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#25193;&#22823;&#30340;&#34892;&#20026;&#36873;&#25321;&#31354;&#38388;&#65307;b)&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21487;&#23398;&#20064;&#30340;&#34892;&#20026;&#36873;&#25321;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;LBC&#24341;&#20837;&#20998;&#24067;&#24335;&#31163;&#32447;&#28436;&#21592;-&#35780;&#35770;&#23478;&#26041;&#27861;&#20013;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#36172;&#21338;&#26426;&#30340;&#20803;&#25511;&#21046;&#22120;&#20248;&#21270;&#34892;&#20026;&#26144;&#23556;&#30340;&#36873;&#25321;&#26469;&#23454;&#29616;&#34892;&#20026;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#24050;&#32463;&#22312;10&#20010;Atari&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#65292;&#24182;&#22312;7&#20010;&#28216;&#25103;&#20013;&#36798;&#21040;&#20102;&#30446;&#21069;&#30340;&#26368;&#39640;&#20998;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LBC&#26694;&#26550;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;-&#20113;&#36830;&#32493;&#20307;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25506;&#32034;&#21033;&#29992;&#31227;&#21160;QPUs&#30340;&#31995;&#32479;&#21644;&#20998;&#24067;&#24335;&#24322;&#26500;&#36164;&#28304;&#20197;&#20351;&#28151;&#21512;&#24212;&#29992;&#31243;&#24207;&#21463;&#30410;&#12290;&#35813;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#25972;&#21512;QPUs&#21644;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#24341;&#25806;&#12290;</title><link>http://arxiv.org/abs/2305.05238</link><description>&lt;p&gt;
&#36793;&#32536;-&#20113;&#36830;&#32493;&#20307;&#20013;&#37327;&#23376;&#35745;&#31639;&#30340;&#26550;&#26500;&#24895;&#26223;
&lt;/p&gt;
&lt;p&gt;
Architectural Vision for Quantum Computing in the Edge-Cloud Continuum. (arXiv:2305.05238v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;-&#20113;&#36830;&#32493;&#20307;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#25506;&#32034;&#21033;&#29992;&#31227;&#21160;QPUs&#30340;&#31995;&#32479;&#21644;&#20998;&#24067;&#24335;&#24322;&#26500;&#36164;&#28304;&#20197;&#20351;&#28151;&#21512;&#24212;&#29992;&#31243;&#24207;&#21463;&#30410;&#12290;&#35813;&#25991;&#35752;&#35770;&#20102;&#22914;&#20309;&#25972;&#21512;QPUs&#21644;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#24341;&#25806;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#37327;&#23376;&#22788;&#29702;&#22120;&#65288;QPUs&#65289;&#20165;&#30001;&#20113;&#20379;&#24212;&#21830;&#25552;&#20379;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#21040;&#22788;&#37117;&#21487;&#20197;&#24456;&#24555;&#22320;&#25176;&#31649;QPUs&#12290;&#29616;&#26377;&#24037;&#20316;&#23578;&#26410;&#20511;&#37492;&#36793;&#32536;&#35745;&#31639;&#30740;&#31350;&#65292;&#25506;&#32034;&#21033;&#29992;&#31227;&#21160;QPUs&#30340;&#31995;&#32479;&#65292;&#25110;&#32773;&#20998;&#24067;&#24335;&#24322;&#26500;&#36164;&#28304;&#22914;&#20309;&#20351;&#28151;&#21512;&#24212;&#29992;&#31243;&#24207;&#21463;&#30410;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36793;&#32536;-&#20113;&#36830;&#32493;&#20307;&#20013;&#37327;&#23376;&#35745;&#31639;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#24310;&#20280;&#29616;&#26377;&#32463;&#20856;&#36793;&#32536;&#35745;&#31639;&#24037;&#20316;&#20197;&#25972;&#21512;QPUs&#30340;&#24517;&#35201;&#24615;&#12289;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#28909;&#21551;&#21160;&#26469;&#23450;&#20041;&#21033;&#29992;&#20998;&#24067;&#22312;&#36830;&#32493;&#20307;&#21508;&#22788;&#30340;&#20998;&#23618;&#36164;&#28304;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24102;&#26377;&#28151;&#21512;&#32463;&#20856;-&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#20998;&#24067;&#24335;&#25512;&#29702;&#24341;&#25806;&#65292;&#20197;&#24110;&#21161;&#31995;&#32479;&#35774;&#35745;&#20154;&#21592;&#36866;&#24212;&#20855;&#26377;&#22797;&#26434;&#35201;&#27714;&#19988;&#20135;&#29983;&#26368;&#39640;&#24322;&#36136;&#24230;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#30340;&#32463;&#20856;&#23618;&#20998;&#21306;&#21644;QPU&#20998;&#37197;&#65292;&#32771;&#34385;&#20102;&#31995;&#32479;&#30340;&#29289;&#29702;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum processing units (QPUs) are currently exclusively available from cloud vendors. However, with recent advancements, hosting QPUs is soon possible everywhere. Existing work has yet to draw from research in edge computing to explore systems exploiting mobile QPUs, or how hybrid applications can benefit from distributed heterogeneous resources. Hence, this work presents an architecture for Quantum Computing in the edge-cloud continuum. We discuss the necessity, challenges, and solution approaches for extending existing work on classical edge computing to integrate QPUs. We describe how warm-starting allows defining workflows that exploit the hierarchical resources spread across the continuum. Then, we introduce a distributed inference engine with hybrid classical-quantum neural networks (QNNs) to aid system designers in accommodating applications with complex requirements that incur the highest degree of heterogeneity. We propose solutions focusing on classical layer partitioning a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.05237</link><description>&lt;p&gt;
&#21033;&#29992;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#36827;&#34892;&#26410;&#35265;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#36947;&#36335;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SCPT&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#31354;&#38388;&#39044;&#35757;&#32451;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#26410;&#35265;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#26032;&#36947;&#36335;&#30340;&#20132;&#36890;&#39044;&#27979;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#20250;&#19981;&#26029;&#24314;&#35774;&#26032;&#30340;&#36947;&#36335;&#65292;&#20294;&#26159;&#20043;&#21069;&#30340;&#28145;&#24230;&#39044;&#27979;&#27169;&#22411;&#23545;&#20110;&#26032;&#36947;&#36335;&#65288;&#26410;&#35265;&#25968;&#25454;&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#34987;&#31216;&#20026;&#26102;&#31354;&#65288;ST&#65289;&#20998;&#21106;&#30340;&#26032;&#35774;&#32622;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#19968;&#37096;&#20998;&#30340;&#36947;&#36335;&#25968;&#25454;&#65292;&#20294;&#27979;&#35797;&#26102;&#20351;&#29992;&#26410;&#35265;&#25968;&#25454;&#30340;&#36947;&#36335;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#31354;&#38388;&#23545;&#27604;&#39044;&#35757;&#32451;&#65288;SCPT&#65289;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22359;&#26469;&#25552;&#21462;&#25512;&#29702;&#26102;&#26410;&#35265;&#36947;&#36335;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#36825;&#20010;&#31354;&#38388;&#32534;&#30721;&#22120;&#26159;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#31354;&#38388;&#32534;&#30721;&#22120;&#20165;&#38656;&#35201;&#26032;&#36947;&#36335;&#30340;&#20004;&#22825;&#20132;&#36890;&#25968;&#25454;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#31354;&#38388;&#32534;&#30721;&#22120;&#30340;&#36755;&#20986;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#25512;&#26029;&#26410;&#35265;&#36947;&#36335;&#19978;&#30340;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
New roads are being constructed all the time. However, the capabilities of previous deep forecasting models to generalize to new roads not seen in the training data (unseen roads) are rarely explored. In this paper, we introduce a novel setup called a spatio-temporal (ST) split to evaluate the models' capabilities to generalize to unseen roads. In this setup, the models are trained on data from a sample of roads, but tested on roads not seen in the training data. Moreover, we also present a novel framework called Spatial Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to extract latent features from unseen roads during inference time. This spatial encoder is pre-trained using contrastive learning. During inference, the spatial encoder only requires two days of traffic data on the new roads and does not require any re-training. We also show that the output from the spatial encoder can be used effectively to infer latent node embeddings on unseen roads during 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05230</link><description>&lt;p&gt;
FedNoRo: &#38024;&#23545;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#22312; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;(FNLL)&#27491;&#22312;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#22810;&#28304;&#20998;&#25955;&#23398;&#20064;&#24037;&#20855;&#12290;&#29616;&#26377;&#30740;&#31350;&#22522;&#20110;&#20840;&#23616;&#25968;&#25454;&#31867;&#21035;&#24179;&#34913;&#30340;&#20551;&#35774;&#65292;&#21487;&#33021;&#26080;&#27861;&#24314;&#27169;&#22797;&#26434;&#30340;&#26631;&#31614;&#22122;&#22768;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#30495;&#23454;&#30340;&#32852;&#37030;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#20854;&#20013;&#20840;&#23616;&#25968;&#25454;&#26159;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#65292;&#24182;&#19988;&#26631;&#31614;&#22122;&#22768;&#26159;&#24322;&#36136;&#30340;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; FedNoRo &#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312; FedNoRo &#30340;&#31532;&#19968;&#38454;&#27573;&#65292;&#37319;&#29992;&#27599;&#31867;&#25439;&#22833;&#25351;&#26631;&#20043;&#21518;&#36319;&#38543;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22024;&#26434;&#23458;&#25143;&#31471;&#35782;&#21035;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21516;&#26102;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#36317;&#31163;&#24863;&#30693;&#32858;&#21512;&#20989;&#25968;&#36827;&#34892;&#22122;&#22768;-&#40065;&#26834;&#32852;&#37030;&#27169;&#22411;&#26356;&#26032;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340; ICH &#21644; ISIC2019 &#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FedNoRo &#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340; FNLL &#26041;&#27861;&#22312;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#24322;&#36136;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
&lt;/p&gt;</description></item><item><title>BARA&#26159;&#19968;&#31181;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#29992;&#20110;&#28608;&#21169;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#20026;&#27169;&#22411;&#35757;&#32451;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#34987;&#24573;&#30053;&#30340;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05221</link><description>&lt;p&gt;
BARA: &#39640;&#25928;&#30340;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
BARA: Efficient Incentive Mechanism with Online Reward Budget Allocation in Cross-Silo Federated Learning. (arXiv:2305.05221v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05221
&lt;/p&gt;
&lt;p&gt;
BARA&#26159;&#19968;&#31181;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#29992;&#20110;&#28608;&#21169;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#25152;&#26377;&#32773;&#20026;&#27169;&#22411;&#35757;&#32451;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#34987;&#24573;&#30053;&#30340;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#36328;&#36793;&#32536;&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#20132;&#25442;&#22810;&#20010;&#36890;&#20449;&#24490;&#29615;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20351;&#19981;&#21516;&#32452;&#32455;&#30340;&#23396;&#31435;&#25968;&#25454;&#23707;&#21327;&#20316;&#19968;&#20010;&#21442;&#25968;&#26381;&#21153;&#22120;&#65288;PS&#65289;&#26469;&#23436;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#36328;&#36793;&#32536;FL&#20013;&#65292;&#28608;&#21169;&#26426;&#21046;&#23545;&#20110;&#28608;&#21169;&#25968;&#25454;&#25152;&#26377;&#32773;&#20026;FL&#35757;&#32451;&#20570;&#20986;&#36129;&#29486;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#22312;&#19981;&#21516;&#30340;&#24490;&#29615;&#20043;&#38388;&#20998;&#37197;&#22870;&#21169;&#39044;&#31639;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#34987;&#29616;&#26377;&#30740;&#31350;&#22823;&#37327;&#24573;&#30053;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25361;&#25112;&#22312;&#20110;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#21644;FL&#27169;&#22411;&#25928;&#29992;&#25913;&#36827;&#20043;&#38388;&#30340;&#19981;&#36879;&#26126;&#21453;&#39304;&#65292;&#20351;&#24471;&#26368;&#20248;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#21464;&#24471;&#22797;&#26434;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#22312;&#32447;&#22870;&#21169;&#39044;&#31639;&#20998;&#37197;&#31639;&#27861;&#65292;&#21517;&#20026;BARA&#65288;\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a prospective distributed machine learning framework that can preserve data privacy. In particular, cross-silo FL can complete model training by making isolated data islands of different organizations collaborate with a parameter server (PS) via exchanging model parameters for multiple communication rounds. In cross-silo FL, an incentive mechanism is indispensable for motivating data owners to contribute their models to FL training. However, how to allocate the reward budget among different rounds is an essential but complicated problem largely overlooked by existing works. The challenge of this problem lies in the opaque feedback between reward budget allocation and model utility improvement of FL, making the optimal reward budget allocation complicated. To address this problem, we design an online reward budget allocation algorithm using Bayesian optimization named BARA (\underline{B}udget \underline{A}llocation for \underline{R}everse \underline{A}uction).
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#26367;&#20195;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05218</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network-based surrogate model for granular flows. (arXiv:2305.05218v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05218
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#26367;&#20195;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#24182;&#19988;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#27169;&#25311;&#39063;&#31890;&#27969;&#21160;&#21147;&#23398;&#23545;&#20110;&#35780;&#20272;&#21508;&#31181;&#23721;&#22303;&#24037;&#31243;&#39118;&#38505;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#23665;&#20307;&#28369;&#22369;&#21644;&#27877;&#30707;&#27969;&#12290;&#39063;&#31890;&#27969;&#28041;&#21450;&#39063;&#31890;&#21160;&#24577;&#37325;&#32452;&#65292;&#34920;&#29616;&#20986;&#20174;&#22266;&#20307;&#26679;&#26412;&#21040;&#28082;&#20307;&#26679;&#26412;&#30340;&#22797;&#26434;&#36716;&#21464;&#12290;&#20256;&#32479;&#30340;&#36830;&#32493;&#20307;&#21644;&#31163;&#25955;&#25968;&#20540;&#26041;&#27861;&#22312;&#27169;&#25311;&#22823;&#35268;&#27169;&#31995;&#32479;&#26102;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#22522;&#20110;&#32479;&#35745;&#25110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#20197;&#26377;&#38480;&#30340;&#19968;&#32452;&#21442;&#25968;&#20026;&#22522;&#30784;&#30340;&#32463;&#39564;&#24615;&#27169;&#22411;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#23454;&#29616;&#27867;&#21270;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#23398;&#20064;&#20250;&#20381;&#36182;&#20110;&#25490;&#21015;&#30340;&#39034;&#24207;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#8212;&#8212;&#19968;&#31181;&#23398;&#20064;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#12290;&#22270;&#34920;&#31034;&#20102;&#21160;&#24577;&#21464;&#21270;&#30340;&#39063;&#31890;&#27969;&#30340;&#29366;&#24577;&#21644;&#30456;&#20114;&#20316;&#29992;&#23450;&#24459;&#65292;&#20363;&#22914;&#39063;&#31890;&#20043;&#38388;&#30340;&#33021;&#37327;&#21644;&#21160;&#37327;&#20132;&#25442;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39063;&#31890;&#27969;&#27169;&#25311;&#22120;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#39640;&#24230;&#21160;&#24577;&#30340;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate simulation of granular flow dynamics is crucial for assessing various geotechnical risks, including landslides and debris flows. Granular flows involve a dynamic rearrangement of particles exhibiting complex transitions from solid-like to fluid-like responses. Traditional continuum and discrete numerical methods are limited by their computational cost in simulating large-scale systems. Statistical or machine learning-based models offer an alternative. Still, they are largely empirical, based on a limited set of parameters. Due to their permutation-dependent learning, traditional machine learning-based models require huge training data to generalize. To resolve these problems, we use a graph neural network, a state-of-the-art machine learning architecture that learns local interactions. Graphs represent the state of dynamically changing granular flows and the interaction laws, such as energy and momentum exchange between grains. We develop a graph neural network-based simulator
&lt;/p&gt;</description></item><item><title>&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;10,000&#20010;U&#24418;&#24367;&#31649;&#20869;&#30340;&#27969;&#20307;&#27969;&#21160;&#21644;&#28909;&#20256;&#36882;&#27169;&#25311;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#21644;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#29305;&#24449;&#26159;&#19977;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#35774;&#35745;&#21442;&#25968;&#21644;&#30446;&#26631;&#32452;&#21512;&#12289;&#20960;&#20309;&#32467;&#26500;&#30340;&#20108;&#32500;&#22270;&#20687;&#20197;&#21450;&#25968;&#20540;&#27169;&#25311;&#30340;&#35299;&#21464;&#37327;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05216</link><description>&lt;p&gt;
&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#21442;&#25968;&#21270;U&#24367;&#27969;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dataset of a parameterized U-bend flow for Deep Learning Applications. (arXiv:2305.05216v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05216
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;10,000&#20010;U&#24418;&#24367;&#31649;&#20869;&#30340;&#27969;&#20307;&#27969;&#21160;&#21644;&#28909;&#20256;&#36882;&#27169;&#25311;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#21644;&#26041;&#27861;&#30340;&#30740;&#31350;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#29305;&#24449;&#26159;&#19977;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#21253;&#25324;&#35774;&#35745;&#21442;&#25968;&#21644;&#30446;&#26631;&#32452;&#21512;&#12289;&#20960;&#20309;&#32467;&#26500;&#30340;&#20108;&#32500;&#22270;&#20687;&#20197;&#21450;&#25968;&#20540;&#27169;&#25311;&#30340;&#35299;&#21464;&#37327;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;10,000&#20010;U&#24418;&#24367;&#31649;&#20869;&#30340;&#27969;&#20307;&#27969;&#21160;&#21644;&#28909;&#20256;&#36882;&#27169;&#25311;&#12290;&#27599;&#20010;&#27169;&#25311;&#37117;&#30001;28&#20010;&#35774;&#35745;&#21442;&#25968;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#30740;&#31350;&#21508;&#31181;&#35774;&#35745;&#20248;&#21270;&#38382;&#39064;&#21644;&#26041;&#27861;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#22522;&#20934;&#12290;&#20026;&#36825;&#20123;&#30740;&#31350;&#65292;&#21487;&#20197;&#37319;&#29992;&#30417;&#30563;&#12289;&#21322;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#24449;&#26159;&#65292;&#27599;&#20010;&#24418;&#29366;&#21487;&#20197;&#36890;&#36807;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#34920;&#31034;&#65292;&#21253;&#25324;&#35774;&#35745;&#21442;&#25968;&#21644;&#30446;&#26631;&#32452;&#21512;&#12289;&#20960;&#20309;&#32467;&#26500;&#30340;&#20116;&#20010;&#19981;&#21516;&#20998;&#36776;&#29575;&#30340;&#20108;&#32500;&#22270;&#20687;&#20197;&#21450;&#25968;&#20540;&#27169;&#25311;&#30340;&#35299;&#21464;&#37327;&#30340;&#34920;&#31034;&#12290;&#31532;&#19977;&#20010;&#34920;&#31034;&#26041;&#27861;&#21487;&#20197;&#32771;&#34385;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25968;&#20540;&#27169;&#25311;&#30340;&#29305;&#23450;&#25968;&#25454;&#32467;&#26500;&#12290;&#29983;&#25104;&#25968;&#25454;&#30340;&#28304;&#20195;&#30721;&#21644;&#23481;&#22120;&#24050;&#21457;&#24067;&#22312;GitHub&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This dataset contains 10,000 fluid flow and heat transfer simulations in U-bend shapes. Each of them is described by 28 design parameters, which are processed with the help of Computational Fluid Dynamics methods. The dataset provides a comprehensive benchmark for investigating various problems and methods from the field of design optimization. For these investigations supervised, semi-supervised and unsupervised deep learning approaches can be employed. One unique feature of this dataset is that each shape can be represented by three distinct data types including design parameter and objective combinations, five different resolutions of 2D images from the geometry and the solution variables of the numerical simulation, as well as a representation using the cell values of the numerical mesh. This third representation enables considering the specific data structure of numerical simulations for deep learning approaches. The source code and the container used to generate the data are publ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#25552;&#31034;&#36866;&#24212;&#65292;LLM&#36817;&#20284;&#21644;LLM&#32423;&#32852;&#65292;&#24182;&#19988;&#36890;&#36807; FrugalGPT &#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#25110;&#26159;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05176</link><description>&lt;p&gt;
FrugalGPT: &#22914;&#20309;&#22312;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#25552;&#31034;&#36866;&#24212;&#65292;LLM&#36817;&#20284;&#21644;LLM&#32423;&#32852;&#65292;&#24182;&#19988;&#36890;&#36807; FrugalGPT &#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#25110;&#26159;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#20184;&#36153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26597;&#35810;&#27969;&#34892;&#30340;LLM API&#65288;&#20363;&#22914;GPT-4&#65292;ChatGPT&#65292;J1-Jumbo&#65289;&#28041;&#21450;&#30340;&#25104;&#26412;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24322;&#26500;&#30340;&#20215;&#26684;&#32467;&#26500;&#65292;&#36153;&#29992;&#21487;&#33021;&#30456;&#24046;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#29305;&#21035;&#26159;&#22312;&#22823;&#37327;&#26597;&#35810;&#21644;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;LLM&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#31574;&#30053;&#26469;&#20943;&#23569;&#20351;&#29992;LLM&#30340;&#27719;&#32534;&#25104;&#26412;&#65306;1&#65289;&#25552;&#31034;&#36866;&#24212;&#65292;2&#65289;LLM&#36817;&#20284;&#21644;3&#65289;LLM&#32423;&#32852;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FrugalGPT&#65292;&#23427;&#26159;LLM&#32423;&#32852;&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#23398;&#20064;&#20351;&#29992;&#21738;&#20123;LLM&#32452;&#21512;&#26469;&#22788;&#29702;&#19981;&#21516;&#26597;&#35810;&#65292;&#20197;&#38477;&#20302;&#25104;&#26412;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FrugalGPT&#21487;&#20197;&#22312;&#20165;&#20351;&#29992;&#36153;&#29992;&#30340;98&#65285;&#25110;&#19982;GPT-4&#30456;&#21516;&#30340;&#25104;&#26412;&#19979;&#65292;&#36798;&#21040;&#26368;&#20339;&#21333;&#20010;LLM&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;GPT-4&#65289;&#65292;&#25110;&#32773;&#20197;4&#65285;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;GPT-4&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#32508;&#21512;&#12289;&#35821;&#20041;&#21644;&#35745;&#31639;&#29702;&#35770;&#65292;&#20197;&#25506;&#35752;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#20010;&#32500;&#24230;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2305.05172</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
Logic for Explainable AI. (arXiv:2305.05172v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#32508;&#21512;&#12289;&#35821;&#20041;&#21644;&#35745;&#31639;&#29702;&#35770;&#65292;&#20197;&#25506;&#35752;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#19977;&#20010;&#32500;&#24230;&#65292;&#20197;&#28145;&#20837;&#29702;&#35299;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;&#29702;&#35299;&#65288;&#23398;&#20064;&#65289;&#20998;&#31867;&#22120;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#31181;&#29702;&#35299;&#26377;&#19977;&#20010;&#26041;&#38754;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#31532;&#19968;&#32500;&#19982;&#20026;&#21028;&#26029;&#20915;&#31574;&#25152;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#23454;&#20363;&#26465;&#20214;&#26377;&#20851;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#30340;&#23454;&#20363;&#25277;&#35937;&#65292;&#21487;&#35270;&#20026;&#8220;&#20915;&#31574;&#32972;&#21518;&#30340;&#21407;&#22240;&#8221;&#12290;&#19979;&#19968;&#32500;&#19982;&#25551;&#36848;&#36275;&#20197;&#20316;&#20986;&#20915;&#31574;&#30340;&#26368;&#23567;&#26465;&#20214;&#26377;&#20851;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#20915;&#31574;&#26080;&#20851;&#30340;&#23454;&#20363;&#26368;&#22823;&#26041;&#38754;&#12290;&#26368;&#21518;&#19968;&#20010;&#32500;&#24230;&#23558;&#20854;&#31227;&#21160;&#21040;&#20102;&#20915;&#31574;&#65292;&#21363;&#26631;&#35782;&#23545;&#23454;&#20363;&#36827;&#34892;&#26368;&#23567;&#25200;&#21160;&#20197;&#20135;&#29983;&#26367;&#20195;&#20915;&#31574;&#25152;&#24517;&#38656;&#30340;&#26368;&#23567;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#26412;&#25945;&#31243;&#20013;&#35752;&#35770;&#20102;&#27839;&#36825;&#20123;&#26041;&#38754;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#20840;&#38754;&#12289;&#35821;&#20041;&#21644;&#35745;&#31639;&#29702;&#35770;&#65292;&#36825;&#26159;&#22522;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#19968;&#20123;&#26368;&#26032;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central quest in explainable AI relates to understanding the decisions made by (learned) classifiers. There are three dimensions of this understanding that have been receiving significant attention in recent years. The first dimension relates to characterizing conditions on instances that are necessary and sufficient for decisions, therefore providing abstractions of instances that can be viewed as the "reasons behind decisions." The next dimension relates to characterizing minimal conditions that are sufficient for a decision, therefore identifying maximal aspects of the instance that are irrelevant to the decision. The last dimension relates to characterizing minimal conditions that are necessary for a decision, therefore identifying minimal perturbations to the instance that yield alternate decisions. We discuss in this tutorial a comprehensive, semantical and computational theory of explainability along these dimensions which is based on some recent developments in symbolic logic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21512;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#30142;&#30149;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20379;&#24212;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#30123;&#24773;&#30340;&#24635;&#20307;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2305.05163</link><description>&lt;p&gt;
&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21512;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;
&lt;/p&gt;
&lt;p&gt;
Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization. (arXiv:2305.05163v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21512;&#20316;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#30142;&#30149;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20379;&#24212;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#38477;&#20302;&#30123;&#24773;&#30340;&#24635;&#20307;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30123;&#33495;&#20248;&#20808;&#32771;&#34385;&#31574;&#30053;&#65292;&#20197;&#22312;&#20379;&#24212;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#30123;&#24773;&#30340;&#24635;&#20307;&#36127;&#25285;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20551;&#35774;&#20122;&#32452;&#20154;&#21475;&#20869;&#30340;&#21516;&#36136;&#24615;&#34892;&#20026;&#21644;&#32570;&#20047;&#31227;&#21160;&#24615;&#21160;&#24577;&#38598;&#25104;&#65292;&#36827;&#34892;&#23439;&#35266;&#25110;&#31616;&#21270;&#30340;&#24494;&#35266;&#30123;&#33495;&#20998;&#37197;&#12290;&#30452;&#25509;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#24494;&#35266;&#30123;&#33495;&#20998;&#37197;&#20250;&#23548;&#33268;&#27425;&#20248;&#35299;&#65292;&#22240;&#20026;&#32570;&#20047;&#19982;&#34892;&#20026;&#30456;&#20851;&#30340;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#30142;&#30149;&#21160;&#21147;&#23398;&#20013;&#30340;&#31227;&#21160;&#24615;&#24322;&#36136;&#24615;&#34701;&#20837;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;Trans-vaccine-SEIR&#27169;&#22411;&#27169;&#25311;&#30142;&#30149;&#30340;&#28436;&#21464;&#36807;&#31243;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#23547;&#27714;&#39640;&#24230;&#31354;&#38388;-&#26102;&#38388;&#30142;&#30149;&#28436;&#21270;&#31995;&#32479;&#30340;&#26368;&#20248;&#30123;&#33495;&#20998;&#37197;&#31574;&#30053;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#34987;&#29992;&#26469;&#26377;&#25928;&#25429;&#25417;&#31227;&#21160;&#25509;&#35302;&#32593;&#32476;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#25552;&#21462;&#21160;&#24577;&#30142;&#30149;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;r
&lt;/p&gt;
&lt;p&gt;
This study explores the vaccine prioritization strategy to reduce the overall burden of the pandemic when the supply is limited. Existing methods conduct macro-level or simplified micro-level vaccine distribution by assuming the homogeneous behavior within subgroup populations and lacking mobility dynamics integration. Directly applying these models for micro-level vaccine allocation leads to sub-optimal solutions due to the lack of behavioral-related details. To address the issue, we first incorporate the mobility heterogeneity in disease dynamics modeling and mimic the disease evolution process using a Trans-vaccine-SEIR model. Then we develop a novel deep reinforcement learning to seek the optimal vaccine allocation strategy for the high-degree spatial-temporal disease evolution system. The graph neural network is used to effectively capture the structural properties of the mobility contact network and extract the dynamic disease features. In our evaluation, the proposed framework r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#21307;&#30103;&#20195;&#30721;&#65292;&#24182;&#22312;&#26631;&#31614;&#31354;&#38388;&#19982;&#20020;&#24202;&#25991;&#26412;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#25216;&#26415;&#27700;&#24179;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#20869;&#37096;&#23545;&#40784;&#23454;&#29616;&#26377;&#25928;&#30340;&#21307;&#30103;&#20195;&#30721;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Medical Code Prediction via Label Internal Alignment. (arXiv:2305.05162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#21307;&#30103;&#20195;&#30721;&#65292;&#24182;&#22312;&#26631;&#31614;&#31354;&#38388;&#19982;&#20020;&#24202;&#25991;&#26412;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#25216;&#26415;&#27700;&#24179;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35760;&#24405;&#36890;&#24120;&#26159;&#30001;&#21307;&#29983;&#36755;&#20837;&#31995;&#32479;&#30340;&#12290;&#36825;&#20123;&#35760;&#24405;&#38656;&#35201;&#26631;&#35760;&#26631;&#20934;&#30340;&#21307;&#30103;&#20195;&#30721;&#65292;&#32780;&#27599;&#20010;&#20195;&#30721;&#20195;&#34920;&#19968;&#31181;&#35786;&#26029;&#25110;&#21307;&#30103;&#27835;&#30103;&#31243;&#24207;&#12290;&#23545;&#36825;&#20123;&#35760;&#24405;&#36827;&#34892;&#27880;&#37322;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#21307;&#30103;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19977;&#20010;&#20449;&#24687;&#26041;&#38754;&#65306;&#20020;&#24202;&#25991;&#26412;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#65292;&#26631;&#31614;&#65288;&#21307;&#30103;&#20195;&#30721;&#65289;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#27599;&#20010;&#20020;&#24202;&#25991;&#26412;&#21644;&#21307;&#30103;&#20195;&#30721;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical notes are usually typed into the system by physicians. They are typically required to be marked by standard medical codes, and each code represents a diagnosis or medical treatment procedure. Annotating these notes is time consuming and prone to error. In this paper, we proposed a multi-view attention based Neural network to predict medical codes from clinical texts. Our method incorporates three aspects of information, the semantic context of the clinical text, the relationship among the label (medical codes) space, and the alignment between each pair of a clinical text and medical code. Our method is verified to be effective on the open source dataset. The experimental result shows that our method achieves better performance against the prior state-of-art on multiple metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#30340;&#28508;&#22312;&#20132;&#20114;&#24335;A2C&#26041;&#27861;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#22312;&#31454;&#20105;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#20174;&#20854;&#20182;&#26234;&#33021;&#20307;&#20013;&#33719;&#24471;&#21508;&#31181;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05159</link><description>&lt;p&gt;
&#28508;&#22312;&#20132;&#20114;&#24335;A2C&#65306;&#22312;&#24320;&#25918;&#24335;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Latent Interactive A2C for Improved RL in Open Many-Agent Systems. (arXiv:2305.05159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#30340;&#28508;&#22312;&#20132;&#20114;&#24335;A2C&#26041;&#27861;&#65292;&#20197;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#30340;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#65292;&#35299;&#20915;&#20102;&#22312;&#31454;&#20105;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#20174;&#20854;&#20182;&#26234;&#33021;&#20307;&#20013;&#33719;&#24471;&#21508;&#31181;&#20449;&#24687;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#30340;&#26041;&#27861;&#26159;&#38598;&#20013;&#24335;&#35757;&#32451;&#65292;&#20294;&#35813;&#26041;&#27861;&#38656;&#35201;&#20174;&#20854;&#20182;&#26234;&#33021;&#20307;&#20013;&#33719;&#24471;&#21508;&#31181;&#20449;&#24687;&#65292;&#36825;&#22312;&#31454;&#20105;&#25110;&#23545;&#25239;&#29615;&#22659;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#65292;&#20132;&#20114;&#24335;&#20248;&#21183;&#28436;&#21592;-&#35780;&#35770;&#23478;(IA2C)&#26041;&#27861;&#37319;&#29992;&#20102;&#20998;&#25955;&#30340;&#35757;&#32451;&#21644;&#25191;&#34892;&#65292;&#26088;&#22312;&#20174;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#30340;&#35266;&#23519;&#20013;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#23398;&#20064;&#38544;&#34255;&#29366;&#24577;&#21644;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#21160;&#30340;&#28508;&#22312;IA2C&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#30001;&#20247;&#22810;&#26234;&#33021;&#20307;&#32452;&#25104;&#30340;&#39046;&#22495;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#28508;&#22312;IA2C&#36890;&#36807;&#38477;&#20302;&#26041;&#24046;&#21644;&#26356;&#24555;&#25910;&#25947;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#36825;&#20123;&#39046;&#22495;&#30340;&#24320;&#25918;&#29256;&#26412;&#65292;&#26234;&#33021;&#20307;&#31181;&#32676;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#24182;&#23545;&#36825;&#20123;&#23454;&#20363;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepTree&#30340;&#26032;&#22411;&#24314;&#26641;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#20110;&#24773;&#22659;&#28508;&#21464;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26641;&#26408;&#20998;&#25903;&#32467;&#26500;&#30340;&#21457;&#23637;&#35268;&#24459;&#32780;&#38750;&#25163;&#21160;&#23450;&#20041;&#26469;&#24314;&#27169;&#26641;&#26408;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#24418;&#29366;&#30340;&#26641;&#26408;&#65292;&#32780;&#19981;&#38656;&#35201;&#23450;&#20041;&#22797;&#26434;&#30340;&#20998;&#25903;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.05153</link><description>&lt;p&gt;
&#28145;&#24230;&#26641;&#65306;&#21033;&#29992;&#20301;&#20110;&#24773;&#22659;&#28508;&#21464;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26641;&#24418;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
DeepTree: Modeling Trees with Situated Latents. (arXiv:2305.05153v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DeepTree&#30340;&#26032;&#22411;&#24314;&#26641;&#26041;&#27861;&#65292;&#21033;&#29992;&#20301;&#20110;&#24773;&#22659;&#28508;&#21464;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#26641;&#26408;&#20998;&#25903;&#32467;&#26500;&#30340;&#21457;&#23637;&#35268;&#24459;&#32780;&#38750;&#25163;&#21160;&#23450;&#20041;&#26469;&#24314;&#27169;&#26641;&#26408;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#24418;&#29366;&#30340;&#26641;&#26408;&#65292;&#32780;&#19981;&#38656;&#35201;&#23450;&#20041;&#22797;&#26434;&#30340;&#20998;&#25903;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#24314;&#26641;&#26041;&#27861;&#8212;&#8212;DeepTree&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#26641;&#26408;&#20998;&#25903;&#32467;&#26500;&#30340;&#21457;&#23637;&#35268;&#24459;&#32780;&#38750;&#25163;&#21160;&#23450;&#20041;&#26469;&#24314;&#27169;&#26641;&#26408;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#26469;&#35757;&#32451;&#8220;&#20301;&#20110;&#24773;&#22659;&#28508;&#21464;&#37327;&#8221;&#30340;&#31354;&#38388;&#65292;&#20197;&#20415;&#23454;&#29616;&#22312;&#26641;&#27169;&#22411;&#20013;&#21333;&#20010;&#33410;&#28857;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#23616;&#37096;&#20998;&#25903;&#29983;&#38271;&#39044;&#27979;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#27169;&#25311;&#20102;&#26641;&#26408;&#30340;&#29983;&#38271;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#24418;&#29366;&#30340;&#26641;&#26408;&#65292;&#32780;&#19981;&#38656;&#35201;&#23450;&#20041;&#22797;&#26434;&#30340;&#20998;&#25903;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose DeepTree, a novel method for modeling trees based on learning developmental rules for branching structures instead of manually defining them. We call our deep neural model situated latent because its behavior is determined by the intrinsic state -encoded as a latent space of a deep neural model- and by the extrinsic (environmental) data that is situated as the location in the 3D space and on the tree structure. We use a neural network pipeline to train a situated latent space that allows us to locally predict branch growth only based on a single node in the branch graph of a tree model. We use this representation to progressively develop new branch nodes, thereby mimicking the growth process of trees. Starting from a root node, a tree is generated by iteratively querying the neural network on the newly added nodes resulting in the branching structure of the whole tree. Our method enables generating a wide variety of tree shapes without the need to define intri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#21322;&#26080;&#38480;&#23618;&#29366;&#22495;&#22320;&#38663;&#27874;&#21453;&#28436;&#20013;&#36827;&#34892;&#22320;&#19979;&#29289;&#36136;&#20998;&#24067;&#30340;&#21453;&#28436;&#65292;&#36890;&#36807;&#23558;&#21560;&#25910;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#36719;&#32422;&#26463;&#22120;&#32435;&#20837;&#32593;&#32476;&#20197;&#36991;&#20813;&#36807;&#22810;&#35745;&#31639;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05150</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;&#21322;&#26080;&#38480;&#23618;&#29366;&#22495;&#22320;&#38663;&#27874;&#21453;&#28436;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural network for seismic wave inversion in layered semi-infinite domain. (arXiv:2305.05150v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#22312;&#21322;&#26080;&#38480;&#23618;&#29366;&#22495;&#22320;&#38663;&#27874;&#21453;&#28436;&#20013;&#36827;&#34892;&#22320;&#19979;&#29289;&#36136;&#20998;&#24067;&#30340;&#21453;&#28436;&#65292;&#36890;&#36807;&#23558;&#21560;&#25910;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#36719;&#32422;&#26463;&#22120;&#32435;&#20837;&#32593;&#32476;&#20197;&#36991;&#20813;&#36807;&#22810;&#35745;&#31639;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#26377;&#25928;&#24615;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#31639;&#22320;&#19979;&#29289;&#36136;&#20998;&#24067;&#26159;&#22320;&#38663;&#23398;&#21644;&#22320;&#38663;&#24037;&#31243;&#20013;&#30340;&#38590;&#39064;&#12290;&#29289;&#29702;&#23398;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30340;&#21457;&#23637;&#20026;&#22320;&#38663;&#21453;&#28436;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PINN&#26694;&#26550;&#65292;&#29992;&#20110;&#21322;&#26080;&#38480;&#23618;&#29366;&#22495;&#22320;&#38663;&#27874;&#21453;&#28436;&#12290;&#36890;&#36807;&#23558;&#21560;&#25910;&#36793;&#30028;&#26465;&#20214;&#20316;&#20026;&#36719;&#32422;&#26463;&#22120;&#32435;&#20837;&#32593;&#32476;&#20197;&#36991;&#20813;&#36807;&#22810;&#35745;&#31639;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#32593;&#32476;&#26469;&#23398;&#20064;&#26410;&#30693;&#30340;&#29289;&#36136;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#20010;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#35299;&#20915;&#26041;&#26696;&#21464;&#37327;&#12290;&#25972;&#20010;&#32593;&#32476;&#26159;&#31471;&#21040;&#31471;&#30340;&#65292;&#24182;&#21463;&#31232;&#30095;&#27979;&#37327;&#25968;&#25454;&#21644;&#22522;&#26412;&#29289;&#29702;&#23450;&#24459;&#65288;&#21363;&#25511;&#21046;&#26041;&#31243;&#21644;&#21021;&#36793;&#30028;&#26465;&#20214;&#65289;&#30340;&#32422;&#26463;&#12290;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21453;&#28436;&#26041;&#27861;&#22312;1D&#21322;&#26080;&#38480;&#22495;&#20013;&#22320;&#38663;&#27874;&#20256;&#25773;&#21453;&#28436;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the material distribution of Earth's subsurface is a challenging task in seismology and earthquake engineering. The recent development of physics-informed neural network (PINN) has shed new light on seismic inversion. In this paper, we present a PINN framework for seismic wave inversion in layered (1D) semi-infinite domain. The absorbing boundary condition is incorporated into the network as a soft regularizer for avoiding excessive computation. In specific, we design a lightweight network to learn the unknown material distribution and a deep neural network to approximate solution variables. The entire network is end-to-end and constrained by both sparse measurement data and the underlying physical laws (i.e., governing equations and initial/boundary conditions). Various experiments have been conducted to validate the effectiveness of our proposed approach for inverse modeling of seismic wave propagation in 1D semi-infinite domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20083;&#33146;&#32959;&#22359;&#23450;&#20301;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#22823;&#31867;&#28608;&#27963;&#20540;&#23450;&#20301;&#24322;&#24120;&#21306;&#22495;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.05136</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#36138;&#23146;&#22238;&#28335;&#30340;&#20083;&#33146;X&#32447;&#25668;&#24433;&#36136;&#22359;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Localisation of Mammographic masses by Greedy Backtracking of Activations in the Stacked Auto-Encoders. (arXiv:2305.05136v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#20083;&#33146;&#32959;&#22359;&#23450;&#20301;&#26041;&#27861;&#65292;&#21033;&#29992;&#26368;&#22823;&#31867;&#28608;&#27963;&#20540;&#23450;&#20301;&#24322;&#24120;&#21306;&#22495;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#30340;&#20998;&#26512;&#38656;&#35201;&#20934;&#30830;&#30340;&#23450;&#20301;&#31361;&#20986;&#30340;&#20083;&#33146;&#32959;&#22359;&#12290;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#39046;&#22495;&#65292;&#21307;&#24072;&#36890;&#24120;&#26631;&#35760;&#32959;&#22359;&#25110;&#24863;&#20852;&#36259;&#21306;&#22495;(ROI)&#65292;&#24182;&#20174;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20083;&#33146;&#32959;&#22359;&#23450;&#20301;&#26694;&#26550;&#65292;&#22522;&#20110;&#22534;&#21472;&#33258;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#31867;&#28608;&#27963;&#20540;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#20083;&#33146;X&#32447;&#25668;&#24433;&#22270;&#20687;&#20013;&#28608;&#27963;&#24322;&#24120;&#31867;&#30340;&#22270;&#20687;&#21306;&#22495;&#23558;&#26159;&#24341;&#36215;&#24322;&#24120;&#30340;&#20083;&#33146;&#32959;&#22359;&#12290;&#23454;&#39564;&#20351;&#29992;IRMA&#20083;&#33146;X&#32447;&#25668;&#24433;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#30340;200&#20010;&#22270;&#20687;(100&#20010;&#27491;&#24120;&#21644;100&#20010;&#24322;&#24120;)&#36827;&#34892;&#12290;&#30001;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#26631;&#35760;&#30340;&#24322;&#24120;&#32959;&#22359;&#21306;&#22495;&#29992;&#20316;&#26631;&#20934;&#31572;&#26696;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#31361;&#20986;&#21306;&#22495;&#26816;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476; (DCNN)&#30340;&#25216;&#26415;&#12290;&#25152;&#25552;&#20986;&#30340;&#36138;&#23146;&#22238;&#28335;&#26041;&#27861;&#26356;&#20026;&#39640;&#25928;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mammographic image analysis requires accurate localisation of salient mammographic masses. In mammographic computer-aided diagnosis, mass or Region of Interest (ROI) is often marked by physicians and features are extracted from the marked ROI. In this paper, we present a novel mammographic mass localisation framework, based on the maximal class activations of the stacked auto-encoders. We hypothesize that the image regions activating abnormal classes in mammographic images will be the breast masses which causes the anomaly. The experiment is conducted using randomly selected 200 mammographic images (100 normal and 100 abnormal) from IRMA mammographic dataset. Abnormal mass regions marked by an expert radiologist are used as the ground truth. The proposed method outperforms existing Deep Convolutional Neural Network (DCNN) based techniques in terms of salient region detection accuracy. The proposed greedy backtracking method is more efficient and does not require a vast number of labell
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#39044;&#27979;&#30340;&#22320;&#36136;&#20449;&#24687;&#21644;&#23454;&#26102;&#30340;&#36816;&#34892;&#21442;&#25968;&#20449;&#24687;&#65292;&#20026;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#26426;&#21069;&#26041;&#22320;&#36136;&#39044;&#27979;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#32531;&#35299;&#26045;&#24037;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.05128</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#30340;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#38567;&#36947;&#23454;&#26102;&#22320;&#36136;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling. (arXiv:2305.05128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#65292;&#32467;&#21512;&#20808;&#21069;&#39044;&#27979;&#30340;&#22320;&#36136;&#20449;&#24687;&#21644;&#23454;&#26102;&#30340;&#36816;&#34892;&#21442;&#25968;&#20449;&#24687;&#65292;&#20026;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#26426;&#21069;&#26041;&#22320;&#36136;&#39044;&#27979;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#32531;&#35299;&#26045;&#24037;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Kriging-Random Forest&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;Kriging&#22806;&#25512;&#31639;&#27861;&#21644;Random Forest&#30456;&#32467;&#21512;&#65292;&#20026;&#22320;&#21387;&#24179;&#34913;&#30462;&#26500;&#26426;&#21069;&#26041;&#22320;&#36136;&#39044;&#27979;&#25552;&#20379;&#25351;&#23548;&#65292;&#20174;&#32780;&#32531;&#35299;&#26045;&#24037;&#39118;&#38505;&#12290;&#35813;&#31639;&#27861;&#21516;&#26102;&#21033;&#29992;&#20102;&#20808;&#21069;&#39044;&#27979;&#30340;&#22320;&#36136;&#20449;&#24687;&#21644;&#23454;&#26102;&#30340;&#36816;&#34892;&#21442;&#25968;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#65292;&#36816;&#29992;&#21152;&#26435;&#24179;&#22343;&#26041;&#27861;&#23558;&#39044;&#27979;&#32467;&#26524;&#32467;&#21512;&#65292;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#26368;&#23567;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A kriging-random forest hybrid model is developed for real-time ground property prediction ahead of the earth pressure balanced shield by integrating Kriging extrapolation and random forest, which can guide shield operating parameter selection thereby mitigate construction risks. The proposed KRF algorithm synergizes two types of information: prior information and real-time information. The previously predicted ground properties with EPB operating parameters are extrapolated via the Kriging algorithm to provide prior information for the prediction of currently being excavated ground properties. The real-time information refers to the real-time operating parameters of the EPB shield, which are input into random forest to provide a real-time prediction of ground properties. The integration of these two predictions is achieved by assigning weights to each prediction according to their uncertainties, ensuring the prediction of KRF with minimum uncertainty. The performance of the KRF algori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2305.05126</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#20869;&#26680;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#28041;&#21450;&#22312;&#21508;&#31181;&#31574;&#21010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32858;&#21512;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24230;&#37327;&#25351;&#26631;&#30340;&#36873;&#25321;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#29702;&#24819;&#24230;&#37327;&#19981;&#26126;&#26174;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#22270;&#29702;&#35770;&#65292;&#24182;&#20419;&#36827;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35825;&#23548;&#19968;&#32452;&#37197;&#22791;&#26377;&#19982;&#19968;&#20123;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#27169;&#22411;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.05119</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#21046;&#36896;&#20652;&#29983;&#20102;&#22797;&#26434;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#22914;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSP&#65289;&#12290;&#22312;FJSP&#20013;&#65292;&#25805;&#20316;&#21487;&#20197;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#23398;&#20064;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#65288;PDRs&#65289;&#20197;&#35299;&#20915;FJSP&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#35832;&#22914;OR-Tools&#31561;&#31934;&#30830;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#20173;&#26377;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;DRL&#36827;&#34892;&#21487;&#25193;&#23637;&#20915;&#31574;&#21046;&#23450;&#30340;&#20248;&#28857;&#12290;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#34987;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#25805;&#20316;&#20449;&#24687;&#27880;&#24847;&#22359;&#21644;&#26426;&#22120;&#20449;&#24687;&#27880;&#24847;&#22359;&#32452;&#25104;&#30340;&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DAN&#65289;&#12290;DAN&#21033;&#29992;&#36825;&#20123;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;PDRs&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
&lt;/p&gt;</description></item><item><title>Flame &#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#32423;&#25277;&#35937;&#8212;&#8212;&#35282;&#33394;&#21644;&#36890;&#36947;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25551;&#36848;&#20026;&#25299;&#25169;&#25277;&#35937;&#22270;&#65292;&#35299;&#32806; ML &#24212;&#29992;&#31243;&#24207;&#36923;&#36753;&#19982;&#24213;&#23618;&#37096;&#32626;&#32454;&#33410;&#65292;&#20351;&#24471;&#21487;&#20197;&#19987;&#38376;&#23450;&#21046;&#37096;&#32626;&#65292;&#20943;&#23569;&#24320;&#21457;&#24037;&#20316;&#65292;&#25913;&#36827;&#33258;&#21160;&#21270;&#21644;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2305.05118</link><description>&lt;p&gt;
Flame&#65306;&#31616;&#21270;&#32852;&#37030;&#23398;&#20064;&#25805;&#20316;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Federated Learning Operations Made Simple with Flame. (arXiv:2305.05118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05118
&lt;/p&gt;
&lt;p&gt;
Flame &#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#32423;&#25277;&#35937;&#8212;&#8212;&#35282;&#33394;&#21644;&#36890;&#36947;&#65292;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25551;&#36848;&#20026;&#25299;&#25169;&#25277;&#35937;&#22270;&#65292;&#35299;&#32806; ML &#24212;&#29992;&#31243;&#24207;&#36923;&#36753;&#19982;&#24213;&#23618;&#37096;&#32626;&#32454;&#33410;&#65292;&#20351;&#24471;&#21487;&#20197;&#19987;&#38376;&#23450;&#21046;&#37096;&#32626;&#65292;&#20943;&#23569;&#24320;&#21457;&#24037;&#20316;&#65292;&#25913;&#36827;&#33258;&#21160;&#21270;&#21644;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#24191;&#27867;&#20998;&#24067;&#30340;&#22522;&#30784;&#26550;&#26500;&#19978;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#26102;&#24102;&#26469;&#20102;&#35768;&#22810;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39044;&#26399;&#30340;&#25928;&#30410;&#65292;&#38656;&#35201;&#36827;&#34892;&#24212;&#29992;&#31243;&#24207;&#21644;&#37197;&#32622;&#32423;&#21035;&#30340;&#26356;&#25913;&#65292;&#36825;&#28041;&#21450;&#37096;&#32626;&#29305;&#23450;&#30340;&#32454;&#33410;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#39640;&#32423;&#21035;&#30340;&#25277;&#35937;&#8212;&#8212;&#35282;&#33394;&#21644;&#36890;&#36947;&#65292;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#36825;&#20123;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#32852;&#37030;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#25551;&#36848;&#20026;&#25299;&#25169;&#25277;&#35937;&#22270;&#65288;TAG&#65289;&#12290;TAG&#23558;ML&#24212;&#29992;&#31243;&#24207;&#36923;&#36753;&#19982;&#24213;&#23618;&#37096;&#32626;&#32454;&#33410;&#35299;&#32806;&#65292;&#20351;&#24471;&#21487;&#20197;&#19987;&#38376;&#23450;&#21046;&#24212;&#29992;&#31243;&#24207;&#37096;&#32626;&#65292;&#20174;&#32780;&#38477;&#20302;&#24320;&#21457;&#24037;&#20316;&#37327;&#65292;&#24182;&#20026;&#25913;&#36827;&#33258;&#21160;&#21270;&#21644;&#35843;&#25972;&#38138;&#24179;&#36947;&#36335;&#12290;&#25105;&#20204;&#25512;&#20986;&#20102;Flame&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#36825;&#20123;&#25277;&#35937;&#27010;&#24565;&#30340;&#31995;&#32479;&#65292;&#24182;&#28436;&#31034;&#20102;&#23427;&#23545;&#22810;&#20010;&#29992;&#20363;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed machine learning approaches, including a broad class of federated learning techniques, present a number of benefits when deploying machine learning applications over widely distributed infrastructures. To realize the expected benefits, however, introduces substantial operational challenges due to required application and configuration-level changes related to deployment-specific details. Such complexities can be greatly reduced by introducing higher-level abstractions -- role and channel -- using which federated learning applications are described as Topology Abstraction Graphs (TAGs). TAGs decouple the ML application logic from the underlying deployment details, making it possible to specialize the application deployment, thus reducing development effort and paving the way for improved automation and tuning. We present Flame, the first system that supports these abstractions, and demonstrate its benefits for several use cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30340;&#36741;&#21161;&#23545;&#25239;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MA3C&#65292;&#20197;&#22312;&#21512;&#20316;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#19979;&#23454;&#29616;&#36890;&#20449;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#36890;&#36807;&#20849;&#21516;&#30340;&#30446;&#26631;&#26368;&#23567;&#21270;&#21327;&#21516;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#36890;&#20449;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#36890;&#20449;&#25915;&#20987;&#19979;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05116</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35843;&#33410;&#30340;&#36741;&#21161;&#22810;&#20195;&#29702;&#23545;&#25239;&#29983;&#25104;&#23454;&#29616;&#36890;&#20449;&#40065;&#26834;&#30340;&#22810;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Robust Multi-Agent Learning by Adaptable Auxiliary Multi-Agent Adversary Generation. (arXiv:2305.05116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#30340;&#36741;&#21161;&#23545;&#25239;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MA3C&#65292;&#20197;&#22312;&#21512;&#20316;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#19979;&#23454;&#29616;&#36890;&#20449;&#40065;&#26834;&#24615;&#65292;&#33021;&#22815;&#36890;&#36807;&#20849;&#21516;&#30340;&#30446;&#26631;&#26368;&#23567;&#21270;&#21327;&#21516;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#36890;&#20449;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#36890;&#20449;&#25915;&#20987;&#19979;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#36890;&#20449;&#21487;&#20197;&#20419;&#36827;&#20195;&#29702;&#30340;&#21327;&#20316;&#65292;&#28982;&#32780;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#20195;&#29702;&#30340;&#36890;&#20449;&#25928;&#29575;&#65292;&#24573;&#30053;&#20102;&#30495;&#23454;&#36890;&#20449;&#20013;&#30340;&#22122;&#22768;&#25110;&#28508;&#22312;&#25915;&#20987;&#32773;&#21487;&#33021;&#23548;&#33268;&#30340;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#31574;&#30053;&#30340;&#31283;&#20581;&#24615;&#25104;&#20026;&#19968;&#20010;&#24613;&#38656;&#25506;&#31350;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36741;&#21161;&#23545;&#25239;&#26041;&#27861;&#35757;&#32451;&#30340;&#33258;&#25105;&#31995;&#32479;&#21487;&#20197;&#24212;&#23545;&#36825;&#31181;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#22810;&#20195;&#29702;&#36741;&#21161;&#23545;&#25239;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MA3C&#65292;&#20197;&#33719;&#24471;&#31283;&#20581;&#30340;&#36890;&#20449;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#36741;&#21161;&#23545;&#25163;&#30340;&#23398;&#20064;&#24314;&#27169;&#20026;&#19968;&#20010;&#21327;&#20316;&#38382;&#39064;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#20849;&#21516;&#30340;&#30446;&#26631;&#26368;&#23567;&#21270;&#33258;&#25105;&#31995;&#32479;&#30340;&#21327;&#21516;&#33021;&#21147;&#65292;&#20174;&#32780;&#20351;&#27599;&#20010;&#20449;&#24687;&#36890;&#36947;&#37117;&#21487;&#33021;&#21463;&#21040;&#19981;&#21516;&#30340;&#20449;&#24687;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36866;&#24212;&#24615;MA3C&#26694;&#26550;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#20013;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22522;&#20934;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MA3C&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#36890;&#20449;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#65292;&#22312;&#36890;&#20449;&#25915;&#20987;&#19979;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication can promote coordination in cooperative Multi-Agent Reinforcement Learning (MARL). Nowadays, existing works mainly focus on improving the communication efficiency of agents, neglecting that real-world communication is much more challenging as there may exist noise or potential attackers. Thus the robustness of the communication-based policies becomes an emergent and severe issue that needs more exploration. In this paper, we posit that the ego system trained with auxiliary adversaries may handle this limitation and propose an adaptable method of Multi-Agent Auxiliary Adversaries Generation for robust Communication, dubbed MA3C, to obtain a robust communication-based policy. In specific, we introduce a novel message-attacking approach that models the learning of the auxiliary attacker as a cooperative problem under a shared goal to minimize the coordination ability of the ego system, with which every information channel may suffer from distinct message attacks. Furthermore
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#33322;&#29677;&#36215;&#39134;&#24310;&#35823;&#12290;&#19968;&#20010;&#30001; XGB-CBR Twin &#36716;&#25442;&#26469;&#30340; CBR &#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20934;&#30830;&#30340;&#23616;&#37096;&#39044;&#27979;&#12289;&#26368;&#26131;&#35299;&#37322;&#30340;&#23616;&#37096;&#35299;&#37322;&#34920;&#31034;&#21644;&#20840;&#23616;&#37325;&#35201;&#24615;&#30340;&#32500;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.05111</link><description>&lt;p&gt;
&#24403;&#25163;&#19978;&#26377;&#19968;&#20010;CBR&#27604;&#19995;&#26519;&#37324;&#30340;&#20004;&#20010;&#26356;&#22909;&#26102;
&lt;/p&gt;
&lt;p&gt;
When a CBR in Hand is Better than Twins in the Bush. (arXiv:2305.05111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#33322;&#29677;&#36215;&#39134;&#24310;&#35823;&#12290;&#19968;&#20010;&#30001; XGB-CBR Twin &#36716;&#25442;&#26469;&#30340; CBR &#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20934;&#30830;&#30340;&#23616;&#37096;&#39044;&#27979;&#12289;&#26368;&#26131;&#35299;&#37322;&#30340;&#23616;&#37096;&#35299;&#37322;&#34920;&#31034;&#21644;&#20840;&#23616;&#37325;&#35201;&#24615;&#30340;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34987;&#31216;&#20026;&#21487;&#35299;&#37322;&#30340;AI&#26041;&#27861;&#24120;&#34987;&#25903;&#25345;&#35299;&#37322;&#24615;&#19982;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#30340;&#20154;&#36140;&#20302;&#20026;&#19981;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#26435;&#34913;&#24182;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#22238;&#24402;&#38382;&#39064;&#8212;&#8212;&#39044;&#27979;&#33322;&#29677;&#36215;&#39134;&#24310;&#35823;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;XGBoost&#23454;&#29616;&#35757;&#32451;&#20102;&#26368;&#31934;&#30830;&#30340;&#25968;&#25454;&#22238;&#24402;&#27169;&#22411;&#12290;&#32780;&#22312;&#26500;&#24314;XGB-CBR Twin&#24182;&#23558;XGBoost&#29305;&#24449;&#37325;&#35201;&#24615;&#36716;&#25442;&#20026;CBR&#27169;&#22411;&#20013;&#30340;&#20840;&#23616;&#26435;&#37325;&#26102;&#65292;&#32467;&#26524;&#21333;&#29420;&#20351;&#29992;&#30340;CBR&#27169;&#22411;&#25552;&#20379;&#20102;&#26368;&#20934;&#30830;&#30340;&#23616;&#37096;&#39044;&#27979;&#65292;&#20445;&#25345;&#20102;&#20840;&#23616;&#37325;&#35201;&#24615;&#65292;&#25552;&#20379;&#20102;&#26368;&#26131;&#35299;&#37322;&#30340;&#23616;&#37096;&#35299;&#37322;&#34920;&#31034;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;CBR&#27169;&#22411;&#25104;&#20026;&#20102;&#36825;&#20010;&#38382;&#39064;&#24773;&#22659;&#19979;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#20934;&#65292;&#20174;&#32780;&#29992;&#20110;&#35780;&#20272;&#20004;&#31181;&#28155;&#21152;&#29305;&#24449;&#23646;&#24615;&#26041;&#27861;SHAP&#21644;LIME&#26469;&#35299;&#37322;XGBoost&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI methods referred to as interpretable are often discredited as inaccurate by supporters of the existence of a trade-off between interpretability and accuracy. In many problem contexts however this trade-off does not hold. This paper discusses a regression problem context to predict flight take-off delays where the most accurate data regression model was trained via the XGBoost implementation of gradient boosted decision trees. While building an XGB-CBR Twin and converting the XGBoost feature importance into global weights in the CBR model, the resultant CBR model alone provides the most accurate local prediction, maintains the global importance to provide a global explanation of the model, and offers the most interpretable representation for local explanations. This resultant CBR model becomes a benchmark of accuracy and interpretability for this problem context, and hence it is used to evaluate the two additive feature attribute methods SHAP and LIME to explain the XGBoost regressio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;SSFL&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05110</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;&#20851;&#38190;&#35789;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Federated Learning for Keyword Spotting. (arXiv:2305.05110v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;SSFL&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#26816;&#27979;&#26159;&#31227;&#21160;&#35774;&#22791;&#21644;&#34394;&#25311;&#21161;&#25163;&#20013;&#38899;&#39057;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#32852;&#37030;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#26174;&#30528;&#25193;&#23637;&#20102;&#21033;&#29992;&#22810;&#20010;&#20998;&#24067;&#24335;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#31169;&#26377;&#25968;&#25454;&#36164;&#28304;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#35774;&#22791;&#20855;&#26377;&#20934;&#30830;&#30340;&#26631;&#31614;&#65292;&#32780;&#24403;&#28041;&#21450;&#21040;&#26412;&#22320;&#38899;&#39057;&#25968;&#25454;&#26102;&#65292;&#36825;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#19981;&#23454;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21322;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35843;&#26597;&#25193;&#23637;&#21040;&#21322;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;SSFL&#65289;&#29992;&#20110;&#20851;&#38190;&#35789;&#26816;&#27979;&#65292;&#20854;&#20013;&#35774;&#22791;&#20855;&#26377;&#23436;&#20840;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#32780;&#26381;&#21153;&#22120;&#21487;&#20197;&#35775;&#38382;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;SSL&#12289;FL&#21644;SSFL&#25216;&#26415;&#36827;&#34892;&#25968;&#23383;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#21033;&#29992;&#20016;&#23500;&#36164;&#28304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;KWS&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyword Spotting (KWS) is a critical aspect of audio-based applications on mobile devices and virtual assistants. Recent developments in Federated Learning (FL) have significantly expanded the ability to train machine learning models by utilizing the computational and private data resources of numerous distributed devices. However, existing FL methods typically require that devices possess accurate ground-truth labels, which can be both expensive and impractical when dealing with local audio data. In this study, we first demonstrate the effectiveness of Semi-Supervised Federated Learning (SSL) and FL for KWS. We then extend our investigation to Semi-Supervised Federated Learning (SSFL) for KWS, where devices possess completely unlabeled data, while the server has access to a small amount of labeled data. We perform numerical analyses using state-of-the-art SSL, FL, and SSFL techniques to demonstrate that the performance of KWS models can be significantly improved by leveraging the abun
&lt;/p&gt;</description></item><item><title>TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05105</link><description>&lt;p&gt;
&#38754;&#21521;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#26816;&#27979;&#30340;&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
TinyML Design Contest for Life-Threatening Ventricular Arrhythmia Detection. (arXiv:2305.05105v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05105
&lt;/p&gt;
&lt;p&gt;
TDC'22&#26159;&#31532;&#19968;&#23626;&#38754;&#21521;ICDs&#20302;&#21151;&#32791;&#24494;&#25511;&#21046;&#22120;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#21019;&#26032;&#31454;&#36187;&#12290;&#26412;&#27425;&#31454;&#36187;&#30340;&#25361;&#25112;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;ACM/IEEE&#24494;&#23567;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#31454;&#36187;&#65288;TDC&#65289;&#20110;2022&#24180;&#22312;&#31532;41&#23626;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#22269;&#38469;&#20250;&#35758;&#65288;ICCAD&#65289;&#19978;&#20030;&#34892;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26376;&#30740;&#21457;&#31454;&#36187;&#12290;TDC'22&#19987;&#27880;&#20110;&#38656;&#35201;&#22312;&#21487;&#26893;&#20837;&#35774;&#22791;&#19978;&#21019;&#26032;&#21644;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#65288;AI/ML&#65289;&#31639;&#27861;&#30340;&#30495;&#23454;&#21307;&#30103;&#38382;&#39064;&#12290;TDC'22&#30340;&#25361;&#25112;&#38382;&#39064;&#26159;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;AI/ML&#30340;&#26032;&#22411;&#23454;&#26102;&#26816;&#27979;&#31639;&#27861;&#65292;&#29992;&#20110;&#24515;&#33039;&#38500;&#39076;&#22120;&#65288;ICDs&#65289;&#19978;&#20351;&#29992;&#30340;&#20302;&#21151;&#29575;&#24494;&#25511;&#21046;&#22120;&#23545;&#21361;&#21450;&#29983;&#21629;&#30340;&#23460;&#24615;&#24515;&#24459;&#22833;&#24120;&#36827;&#34892;&#26816;&#27979;&#12290;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;90&#20010;&#21463;&#35797;&#32773;&#30340;8&#31181;&#19981;&#21516;&#24515;&#24459;&#31867;&#22411;&#30340;&#36229;&#36807;38,000&#20010;5&#31186;&#24515;&#20869;&#30005;&#22270;&#65288;IEGM&#65289;&#29255;&#27573;&#12290;&#19987;&#29992;&#30828;&#20214;&#24179;&#21488;&#26159;STMicroelectronics&#21046;&#36896;&#30340;NUCLEO-L432KC&#12290;TDC'22&#38754;&#21521;&#20840;&#29699;&#22810;&#20154;&#22242;&#38431;&#65292;&#21560;&#24341;&#20102;&#26469;&#33258;50&#22810;&#20010;&#32452;&#32455;&#30340;150&#22810;&#25903;&#38431;&#20237;&#21442;&#36187;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#36825;&#19968;&#21307;&#30103;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The first ACM/IEEE TinyML Design Contest (TDC) held at the 41st International Conference on Computer-Aided Design (ICCAD) in 2022 is a challenging, multi-month, research and development competition. TDC'22 focuses on real-world medical problems that require the innovation and implementation of artificial intelligence/machine learning (AI/ML) algorithms on implantable devices. The challenge problem of TDC'22 is to develop a novel AI/ML-based real-time detection algorithm for life-threatening ventricular arrhythmia over low-power microcontrollers utilized in Implantable Cardioverter-Defibrillators (ICDs). The dataset contains more than 38,000 5-second intracardiac electrograms (IEGMs) segments over 8 different types of rhythm from 90 subjects. The dedicated hardware platform is NUCLEO-L432KC manufactured by STMicroelectronics. TDC'22, which is open to multi-person teams world-wide, attracted more than 150 teams from over 50 organizations. This paper first presents the medical problem, da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;AI&#31995;&#32479;&#30340;&#26657;&#20934;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#19982;&#23454;&#38469;&#25968;&#25454;&#19981;&#31526;&#21512;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#21307;&#30103;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#27495;&#35270;&#20559;&#24046;&#30340;&#35780;&#20272;&#65292;&#32780;&#26657;&#20934;&#20559;&#24046;&#30340;&#35780;&#20272;&#20173;&#28982;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2305.05101</link><description>&lt;p&gt;
&#26088;&#22312;&#25581;&#31034;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#26657;&#20934;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Towards unraveling calibration biases in medical image analysis. (arXiv:2305.05101v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;AI&#31995;&#32479;&#30340;&#26657;&#20934;&#20559;&#24046;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#39044;&#27979;&#19982;&#23454;&#38469;&#25968;&#25454;&#19981;&#31526;&#21512;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;&#21307;&#30103;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#30740;&#31350;&#37117;&#20391;&#37325;&#20110;&#27495;&#35270;&#20559;&#24046;&#30340;&#35780;&#20272;&#65292;&#32780;&#26657;&#20934;&#20559;&#24046;&#30340;&#35780;&#20272;&#20173;&#28982;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#24212;&#29992;&#21462;&#24471;&#20102;&#24040;&#22823;&#21457;&#23637;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#37327;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#23384;&#22312;&#31995;&#32479;&#24615;&#21644;&#19981;&#20844;&#24179;&#30340;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#27495;&#35270;&#12290;&#36825;&#20004;&#20010;&#20107;&#23454;&#20419;&#20351;&#31639;&#27861;&#20844;&#24179;&#24615;&#30740;&#31350;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#20986;&#29616;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#21307;&#30103;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20391;&#37325;&#20110;&#20197;&#32463;&#20856;&#30340;&#27495;&#35270;&#25351;&#26631;&#20363;&#22914;AUC&#21644;&#20934;&#30830;&#24230;&#26469;&#35780;&#20272;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26657;&#20934;&#26041;&#38754;&#30340;&#28508;&#22312;&#20559;&#24046;&#21482;&#26159;&#26368;&#36817;&#24320;&#22987;&#24471;&#21040;&#35780;&#20272;&#12290;&#36825;&#22312;&#20351;&#29992;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26102;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26159;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#20248;&#21270;&#35780;&#20272;&#21644;&#32467;&#21512;&#22810;&#20010;&#20449;&#24687;&#26469;&#28304;&#30340;&#20851;&#38190;&#12290;&#26412;&#24037;&#20316;&#30740;&#31350;&#20102;&#33258;&#21160;&#26816;&#27979;&#24694;&#24615;&#30382;&#32932;&#30149;&#21464;&#30340;&#27169;&#22411;&#20013;&#30340;&#27495;&#35270;&#21644;&#26657;&#20934;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years the development of artificial intelligence (AI) systems for automated medical image analysis has gained enormous momentum. At the same time, a large body of work has shown that AI systems can systematically and unfairly discriminate against certain populations in various application scenarios. These two facts have motivated the emergence of algorithmic fairness studies in this field. Most research on healthcare algorithmic fairness to date has focused on the assessment of biases in terms of classical discrimination metrics such as AUC and accuracy. Potential biases in terms of model calibration, however, have only recently begun to be evaluated. This is especially important when working with clinical decision support systems, as predictive uncertainty is key for health professionals to optimally evaluate and combine multiple sources of information. In this work we study discrimination and calibration biases in models trained for automatic detection of malignant dermatol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#24341;&#20837;&#19968;&#31181;&#21453;&#24212;&#24615;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#22495;&#20559;&#31227;&#65292;&#26080;&#38656;&#20107;&#20808;&#39044;&#27979;&#25110;&#25552;&#20379;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.05100</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#30340;&#33258;&#36866;&#24212;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Domain Generalization for Digital Pathology Images. (arXiv:2305.05100v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#20013;&#30340;&#22495;&#28418;&#31227;&#38382;&#39064;&#65292;&#24341;&#20837;&#19968;&#31181;&#21453;&#24212;&#24615;&#22495;&#27867;&#21270;&#25216;&#26415;&#65292;&#22312;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#22495;&#20559;&#31227;&#65292;&#26080;&#38656;&#20107;&#20808;&#39044;&#27979;&#25110;&#25552;&#20379;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#65292;&#22495;&#20559;&#31227;&#26159;&#24120;&#35265;&#19988;&#32463;&#24471;&#36215;&#32771;&#39564;&#30340;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#26579;&#33394;&#21644;&#25195;&#25551;&#20202;&#21464;&#21270;&#65292;&#28982;&#32780;&#65292;&#36825;&#31181;&#21464;&#21270;&#24182;&#27809;&#26377;&#26174;&#31034;&#25972;&#20010;&#22270;&#26223;&#8212;&#8212;&#20559;&#31227;&#21487;&#33021;&#26159;&#20854;&#20182;&#20559;&#31227;&#30340;&#32452;&#21512;&#65292;&#25110;&#32773;&#26159;&#8220;&#38544;&#24418;&#8221;&#20559;&#31227;&#65292;&#23427;&#20204;&#34429;&#28982;&#19981;&#26126;&#26174;&#65292;&#20294;&#20173;&#28982;&#20250;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#37325;&#35201;&#30340;&#26159;&#35753;&#27169;&#22411;&#22312;&#27809;&#26377;&#26114;&#36149;&#25110;&#31232;&#32570;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#23545;&#36825;&#20123;&#21464;&#21270;&#36827;&#34892;&#27867;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#32452;&#32455;&#30149;&#29702;&#23398;&#39046;&#22495;&#65292;&#24182;&#19988;&#24076;&#26395;&#23558;&#27169;&#22411;&#37096;&#32626;&#22312;&#26356;&#22823;&#30340;&#33539;&#22260;&#20869;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#8220;&#21453;&#24212;&#24615;&#8221;&#22495;&#27867;&#21270;&#25216;&#26415;&#65306;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#22495;&#20559;&#31227;&#32780;&#19981;&#38656;&#35201;&#22312;&#35757;&#32451;&#26102;&#38388;&#39044;&#27979;&#25110;&#25552;&#20379;&#20559;&#31227;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#20171;&#32461;&#20102;&#22312;&#27979;&#35797;&#26102;&#38388;&#36866;&#24212;&#27169;&#22411;&#21442;&#25968;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23545;&#31532;&#20108;&#20010;
&lt;/p&gt;
&lt;p&gt;
In AI-based histopathology, domain shifts are common and well-studied. However, this research focuses on stain and scanner variations, which do not show the full picture-- shifts may be combinations of other shifts, or "invisible" shifts that are not obvious but still damage performance of machine learning models. Furthermore, it is important for models to generalize to these shifts without expensive or scarce annotations, especially in the histopathology space and if wanting to deploy models on a larger scale. Thus, there is a need for "reactive" domain generalization techniques: ones that adapt to domain shifts at test-time rather than requiring predictions of or examples of the shifts at training time. We conduct a literature review and introduce techniques that react to domain shifts rather than requiring a prediction of them in advance. We investigate test time training, a technique for domain generalization that adapts model parameters at test-time through optimization of a secon
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#27169;&#22411;(NAP)&#65292;&#36890;&#36807;&#32534;&#30721;&#24207;&#21015;&#30452;&#25509;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23454;&#29616;&#35299;&#30721;&#27493;&#39588;&#30340;&#35268;&#36991;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20004;&#20010;&#22330;&#26223;&#19979;&#65292;NAP&#20998;&#21035;&#21487;&#20197;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#21644;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05098</link><description>&lt;p&gt;
&#35841;&#38656;&#35201;&#35299;&#30721;&#22120;&#65311;&#39640;&#25928;&#39044;&#27979;&#24207;&#21015;&#32423;&#23646;&#24615;&#12290;&#65288;arXiv:2305.05098v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05098
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#27169;&#22411;(NAP)&#65292;&#36890;&#36807;&#32534;&#30721;&#24207;&#21015;&#30452;&#25509;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23454;&#29616;&#35299;&#30721;&#27493;&#39588;&#30340;&#35268;&#36991;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20004;&#20010;&#22330;&#26223;&#19979;&#65292;NAP&#20998;&#21035;&#21487;&#20197;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#21644;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21270;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#33258;&#22238;&#24402;&#35299;&#30721;&#65292;&#36825;&#24448;&#24448;&#38750;&#24120;&#28040;&#32791;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#36234;&#30028;&#26816;&#27979;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#38469;&#35299;&#30721;&#36755;&#20986;&#24182;&#19981;&#38656;&#35201;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#24207;&#21015;&#30340;&#26631;&#37327;&#23646;&#24615;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#19979;&#65292;&#30693;&#36947;&#31995;&#32479;&#36755;&#20986;&#36136;&#37327;&#20197;&#39044;&#27979;&#24615;&#33021;&#36739;&#24046;&#27604;&#30693;&#36947;&#36755;&#20986;&#26412;&#36523;&#26356;&#20026;&#37325;&#35201;&#65292;&#37027;&#20040;&#26159;&#21542;&#21487;&#20197;&#32469;&#36807;&#33258;&#22238;&#24402;&#35299;&#30721;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#65288;NAP&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;NAP&#30452;&#25509;&#20174;&#32534;&#30721;&#39044;&#27979;&#36825;&#20123;&#25351;&#26631;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65306;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#22312;MT&#30340;&#36234;&#30028;&#26816;&#27979;&#20013;&#65292;NAP&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#65292;&#21516;&#26102;&#36895;&#24230;&#26174;&#33879;&#26356;&#24555;&#12290;NAP&#20063;&#34987;&#35777;&#26126;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;ASR&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20363;&#22914;&#35789;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#23646;&#24615;&#21487;&#20197;&#20174;&#32534;&#30721;&#20013;&#30452;&#25509;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#65292;NAP&#20026;&#20256;&#32479;&#22522;&#20110;&#35299;&#30721;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#36739;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#65292;&#36866;&#29992;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.05097</link><description>&lt;p&gt;
&#36890;&#29992;&#22270;&#19978;&#30340;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208; - &#36890;&#36807;&#38750;&#32447;&#24615;&#39532;&#23572;&#21487;&#22827;&#38142;&#23454;&#29616;&#26368;&#23567;&#37319;&#26679;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
Self-Repellent Random Walks on General Graphs - Achieving Minimal Sampling Variance via Nonlinear Markov Chains. (arXiv:2305.05097v1 [math.PR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05097
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208;&#27169;&#22411;&#65292;&#21487;&#23454;&#29616;&#36739;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#65292;&#36866;&#29992;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65292;&#20363;&#22914;&#19968;&#33324;&#30340;&#26080;&#21521;&#22270;&#65292;&#20854;&#20013;&#38543;&#26426;&#28216;&#36208;&#35774;&#35745;&#25104;&#36890;&#36807;&#37319;&#26679;&#21644;&#37051;&#22495;&#25506;&#32034;&#26469;&#36924;&#36817;&#32593;&#32476;&#25299;&#25169;&#19978;&#30340;&#30446;&#26631;&#37327;&#65292;&#20197;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599; (MCMC) &#31243;&#24207;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#23545;&#20110;&#20219;&#20309;&#30456;&#24212;&#20110;&#30446;&#26631;&#27010;&#29575;&#20998;&#24067;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#25105;&#25490;&#26021;&#38543;&#26426;&#28216;&#36208; (SRRW)&#65292;&#23427;&#19981;&#22826;&#21487;&#33021;&#36716;&#31227;&#21040;&#36807;&#21435;&#34987;&#39640;&#24230;&#35775;&#38382;&#30340;&#33410;&#28857;&#65292;&#32780;&#26356;&#21487;&#33021;&#36716;&#31227;&#21040;&#24456;&#23569;&#34987;&#35775;&#38382;&#30340;&#33410;&#28857;&#12290;&#23545;&#20110;&#19968;&#31867;&#30001;&#27491;&#23454;&#25968; {\alpha} &#21442;&#25968;&#21270;&#30340; SRRW&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#36807;&#31243;&#30340;&#32463;&#39564;&#20998;&#24067;&#20960;&#20046;&#32943;&#23450;&#25910;&#25947;&#20110;&#24213;&#23618;&#39532;&#23572;&#21487;&#22827;&#38142;&#20869;&#26680;&#30340;&#30446;&#26631; (&#24179;&#31283;) &#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#65292;&#24182;&#25512;&#23548;&#20986;&#25152;&#24471;&#21040;&#30340;&#28176;&#36817;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#31934;&#30830;&#24418;&#24335;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#24378;&#30340;&#25490;&#26021;&#20316;&#29992; (&#36739;&#22823;&#30340; {\alpha}) &#30340; SRRW &#19968;&#23450;&#27604;&#20855;&#26377;&#36739;&#24369;&#30340;&#25490;&#26021;&#20316;&#29992; (&#36739;&#23567;&#30340; {\alpha}) &#30340; SRRW &#23454;&#29616;&#26356;&#23567;&#30340;&#28176;&#36817;&#37319;&#26679;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo (MCMC) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk (SRRW) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of SRRWs parameterized by a positive real {\alpha}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the SRRW with a stronger repellence (larger {\alpha}) always achieves a smaller asymptotic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#29616;&#24615;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#26144;&#23556;&#26469;&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20381;&#36182;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#26500;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.05090</link><description>&lt;p&gt;
&#34920;&#29616;&#24615;&#32852;&#37030;&#23398;&#20064;&#65306;&#24212;&#23545;&#27169;&#22411;&#20381;&#36182;&#21644;&#24322;&#26500;&#20998;&#24067;&#20559;&#31227;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Performative Federated Learning: A Solution to Model-Dependent and Heterogeneous Distribution Shifts. (arXiv:2305.05090v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#29616;&#24615;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#26144;&#23556;&#26469;&#24212;&#23545;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20381;&#36182;&#21644;&#23458;&#25143;&#31471;&#25968;&#25454;&#30340;&#24322;&#26500;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#23458;&#25143;&#31471;&#21644;&#19968;&#20010;&#26381;&#21153;&#22120;&#32452;&#25104;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#26088;&#22312;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#20013;&#21327;&#20316;&#23398;&#20064;&#19968;&#20010;&#20849;&#21516;&#30340;&#20915;&#31574;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20551;&#35774;&#23458;&#25143;&#31471;&#25968;&#25454;&#38745;&#24577;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#20250;&#34987;&#37096;&#32626;&#30340;&#20915;&#31574;&#27169;&#22411;&#37325;&#22609;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#34920;&#29616;&#24615;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#26144;&#23556;&#30340;&#24819;&#27861;&#65292;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#27169;&#22411;&#20381;&#36182;&#30340;&#25968;&#25454;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#29616;&#24615;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a federated learning (FL) system consisting of multiple clients and a server, where the clients aim to collaboratively learn a common decision model from their distributed data. Unlike the conventional FL framework that assumes the client's data is static, we consider scenarios where the clients' data distributions may be reshaped by the deployed decision model. In this work, we leverage the idea of distribution shift mappings in performative prediction to formalize this model-dependent data distribution shift and propose a performative federated learning framework. We first introduce necessary and sufficient conditions for the existence of a unique performative stable solution and characterize its distance to the performative optimal solution. Then we propose the performative FedAvg algorithm and show that it converges to the performative stable solution at a rate of O(1/T) under both full and partial participation schemes. In particular, we use novel proof techniques and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#21333;&#38544;&#34255;&#23618;&#21452;&#26354;&#27491;&#20999;&#32467;&#26500;&#32473;&#20986;&#20102;&#21333;&#20803;&#20887;&#20313;&#21644;&#21487;&#32422;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#30340;&#31639;&#27861;&#21051;&#30011;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#36890;&#30340;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.05089</link><description>&lt;p&gt;
&#21487;&#32422;&#30340;&#21452;&#26354;&#27491;&#20999;&#32593;&#32476;&#30340;&#21151;&#33021;&#31561;&#20215;&#21644;&#36335;&#24452;&#36830;&#36890;&#24615;
&lt;/p&gt;
&lt;p&gt;
Functional Equivalence and Path Connectivity of Reducible Hyperbolic Tangent Networks. (arXiv:2305.05089v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#21333;&#38544;&#34255;&#23618;&#21452;&#26354;&#27491;&#20999;&#32467;&#26500;&#32473;&#20986;&#20102;&#21333;&#20803;&#20887;&#20313;&#21644;&#21487;&#32422;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#30340;&#31639;&#27861;&#21051;&#30011;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#36890;&#30340;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#38656;&#35201;&#28548;&#28165;&#23398;&#20064;&#21457;&#29983;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#32467;&#26500;&#12290;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#23454;&#29616;&#30456;&#21516;&#36755;&#20837;&#36755;&#20986;&#20989;&#25968;&#30340;&#21442;&#25968;&#38598;&#21512;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19968;&#23567;&#37096;&#20998;&#21487;&#32422;&#21442;&#25968;&#65292;&#20854;&#21151;&#33021;&#31561;&#20215;&#31867;&#30001;&#32593;&#32476;&#21333;&#20803;&#20043;&#38388;&#30340;&#20887;&#20313;&#36896;&#25104;&#65292;&#22240;&#32780;&#20855;&#26377;&#26356;&#20016;&#23500;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#12290;&#26412;&#25991;&#23545;&#20110;&#21333;&#38544;&#34255;&#23618;&#21452;&#26354;&#27491;&#20999;&#32467;&#26500;&#32473;&#20986;&#20102;&#21333;&#20803;&#20887;&#20313;&#21644;&#21487;&#32422;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#30340;&#31639;&#27861;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#21151;&#33021;&#31561;&#20215;&#31867;&#26159;&#20998;&#27573;&#32447;&#24615;&#36335;&#24452;&#36830;&#36890;&#30340;&#38598;&#21512;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#22810;&#25968;&#20887;&#20313;&#21333;&#20803;&#30340;&#21442;&#25968;&#65292;&#36825;&#20123;&#38598;&#21512;&#30340;&#30452;&#24452;&#26368;&#22810;&#20026;7&#32447;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's functional equivalence class is the set of parameters implementing the same input--output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of reducible parameters, with richer functional equivalence classes caused by redundancies among the network's units.  In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#23545;&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#20013;&#21382;&#21490;&#20559;&#24046;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#31639;&#27861;&#27979;&#35797;&#26102;&#38388;&#20559;&#31227;&#65292;&#36827;&#34892;&#22238;&#39038;&#24615;&#25195;&#25551;&#20197;&#23547;&#25214;&#26102;&#38388;&#20559;&#31227;&#65292;&#24182;&#21019;&#24314;1010&#20010;&#20219;&#21153;&#26469;&#35780;&#20272;242&#39033;&#21307;&#30103;&#20445;&#20581;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;9.7%&#30340;&#20219;&#21153;&#26174;&#31034;&#20986;&#20154;&#32676;&#27700;&#24179;&#30340;&#26102;&#38388;&#20559;&#31227;&#65292;93%&#26174;&#31034;&#20986;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.05087</link><description>&lt;p&gt;
&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#20013;&#21382;&#21490;&#20559;&#24046;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Study of Temporal Shift in Health Insurance Claims. (arXiv:2305.05087v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#23545;&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#20013;&#21382;&#21490;&#20559;&#24046;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#36890;&#36807;&#26500;&#24314;&#31639;&#27861;&#27979;&#35797;&#26102;&#38388;&#20559;&#31227;&#65292;&#36827;&#34892;&#22238;&#39038;&#24615;&#25195;&#25551;&#20197;&#23547;&#25214;&#26102;&#38388;&#20559;&#31227;&#65292;&#24182;&#21019;&#24314;1010&#20010;&#20219;&#21153;&#26469;&#35780;&#20272;242&#39033;&#21307;&#30103;&#20445;&#20581;&#32467;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#26377;9.7%&#30340;&#20219;&#21153;&#26174;&#31034;&#20986;&#20154;&#32676;&#27700;&#24179;&#30340;&#26102;&#38388;&#20559;&#31227;&#65292;93%&#26174;&#31034;&#20986;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29992;&#20110;&#39044;&#27979;&#20020;&#24202;&#32467;&#26524;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#24320;&#21457;&#30340;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#37096;&#32626;&#65292;&#25968;&#25454;&#38598;&#38543;&#26102;&#38388;&#21457;&#29983;&#20559;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#29702;&#24819;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#29305;&#23450;&#26102;&#38388;&#28857;&#39044;&#27979;&#30340;&#20219;&#21153;&#65288;&#21363;&#35201;&#39044;&#27979;&#30340;&#32467;&#26524;&#65289;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#22914;&#26524;&#21382;&#21490;&#27169;&#22411;&#19981;&#20877;&#26159;&#39044;&#27979;&#35813;&#32467;&#26524;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#27979;&#35797;&#20154;&#32676;&#27700;&#24179;&#25110;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#20803;&#31639;&#27861;&#65292;&#22312;&#19968;&#32452;&#22823;&#37327;&#30340;&#20219;&#21153;&#20013;&#25191;&#34892;&#22238;&#39038;&#24615;&#25195;&#25551;&#20197;&#23547;&#25214;&#26102;&#38388;&#20559;&#31227;&#12290;&#26681;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#23545;&#21382;&#21490;&#20559;&#24046;&#30340;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;242&#39033;&#21307;&#30103;&#20445;&#20581;&#32467;&#26524;&#20174;2015&#24180;&#21040;2020&#24180;&#30340;&#20581;&#24247;&#20445;&#38505;&#32034;&#36180;&#25968;&#25454;&#38598;&#20013;&#30340;&#21382;&#21490;&#20559;&#31227;&#26469;&#21019;&#24314;1010&#20010;&#20219;&#21153;&#12290;9.7%&#30340;&#20219;&#21153;&#26174;&#31034;&#20986;&#20154;&#32676;&#27700;&#24179;&#30340;&#26102;&#38388;&#20559;&#31227;&#65292;93.0%&#26174;&#31034;&#20986;&#24050;&#21457;&#29616;&#30340;&#23376;&#20154;&#32676;&#20869;&#30340;&#26102;&#38388;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most machine learning models for predicting clinical outcomes are developed using historical data. Yet, even if these models are deployed in the near future, dataset shift over time may result in less than ideal performance. To capture this phenomenon, we consider a task--that is, an outcome to be predicted at a particular time point--to be non-stationary if a historical model is no longer optimal for predicting that outcome. We build an algorithm to test for temporal shift either at the population level or within a discovered sub-population. Then, we construct a meta-algorithm to perform a retrospective scan for temporal shift on a large collection of tasks. Our algorithms enable us to perform the first comprehensive evaluation of temporal shift in healthcare to our knowledge. We create 1,010 tasks by evaluating 242 healthcare outcomes for temporal shift from 2015 to 2020 on a health insurance claims dataset. 9.7% of the tasks show temporal shifts at the population level, and 93.0% ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#36127;&#36733;&#39044;&#27979;&#32479;&#19968;&#26694;&#26550;&#65292;&#21253;&#25324;&#26102;&#21464;&#29305;&#24449;&#21152;&#26435;&#12289;&#20998;&#23618;&#26102;&#38388;&#27880;&#24847;&#21147;&#21644;&#29305;&#24449;&#22686;&#24378;&#35823;&#24046;&#20462;&#27491;&#31561;&#27169;&#22359;&#65292;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05082</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#36127;&#36733;&#39044;&#27979;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unifying Framework of Attention-based Neural Load Forecasting. (arXiv:2305.05082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#36127;&#36733;&#39044;&#27979;&#32479;&#19968;&#26694;&#26550;&#65292;&#21253;&#25324;&#26102;&#21464;&#29305;&#24449;&#21152;&#26435;&#12289;&#20998;&#23618;&#26102;&#38388;&#27880;&#24847;&#21147;&#21644;&#29305;&#24449;&#22686;&#24378;&#35823;&#24046;&#20462;&#27491;&#31561;&#27169;&#22359;&#65292;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36127;&#36733;&#39044;&#27979;&#23545;&#20110;&#30005;&#21147;&#32593;&#32476;&#30340;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#35268;&#21010;&#21644;&#36816;&#33829;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36127;&#36733;&#39044;&#27979;&#30340;&#32479;&#19968;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26102;&#21464;&#29305;&#24449;&#21152;&#26435;&#12289;&#20998;&#23618;&#26102;&#38388;&#27880;&#24847;&#21147;&#21644;&#29305;&#24449;&#22686;&#24378;&#35823;&#24046;&#20462;&#27491;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#29305;&#24449;&#21152;&#26435;&#26426;&#21046;&#23558;&#36755;&#20837;&#29305;&#24449;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#26102;&#38388;&#26435;&#37325;&#65307;&#20854;&#27425;&#65292;&#37319;&#29992;&#36882;&#24402;&#32534;&#30721;-&#35299;&#30721;&#32467;&#26500;&#21644;&#20998;&#23618;&#27880;&#24847;&#21147;&#36827;&#34892;&#36127;&#36733;&#39044;&#27979;&#65292;&#20998;&#23618;&#27880;&#24847;&#21147;&#33021;&#22815;&#36827;&#34892;&#30456;&#20284;&#26085;&#36873;&#25321;&#65292;&#20197;&#37325;&#26032;&#35780;&#20272;&#21382;&#21490;&#20449;&#24687;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#37325;&#35201;&#24615;&#65307;&#31532;&#19977;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#35823;&#24046;&#20462;&#27491;&#27169;&#22359;&#65292;&#25506;&#32034;&#35823;&#24046;&#21644;&#23398;&#20064;&#29305;&#24449;&#38544;&#34255;&#20449;&#24687;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate load forecasting is critical for reliable and efficient planning and operation of electric power grids. In this paper, we propose a unifying deep learning framework for load forecasting, which includes time-varying feature weighting, hierarchical temporal attention, and feature-reinforced error correction. Our framework adopts a modular design with good generalization capability. First, the feature-weighting mechanism assigns input features with temporal weights. Second, a recurrent encoder-decoder structure with hierarchical attention is developed as a load predictor. The hierarchical attention enables a similar day selection, which re-evaluates the importance of historical information at each time step. Third, we develop an error correction module that explores the errors and learned feature hidden information to further improve the model's forecasting performance. Experimental results demonstrate that our proposed framework outperforms existing methods on two public dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.05080</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#26102;&#20195;&#30340;&#22320;&#29699;&#31227;&#21160;&#32773;: &#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning. (arXiv:2305.05080v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#25193;&#23637;&#20197;&#36866;&#24212;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#36755;&#36816;(OT)&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;,&#39318;&#27425;&#20986;&#29616;&#20110;18&#19990;&#32426;,&#24182;&#24341;&#21457;&#20986;&#22823;&#37327;&#26041;&#27861;&#26469;&#22238;&#31572;&#35768;&#22810;&#29702;&#35770;&#21644;&#24212;&#29992;&#38382;&#39064;&#12290;&#36807;&#21435;&#30340;&#21313;&#24180;&#35265;&#35777;&#20102;&#36825;&#20010;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#26174;&#30528;&#36129;&#29486;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#21450;&#20854;&#25193;&#23637;&#30340;&#38382;&#39064;&#12290;&#22312;&#19987;&#39064;&#19982;&#32972;&#26223;&#30340;&#20801;&#35768;&#19979;,&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#20840;&#38754;&#35843;&#26597;,&#24182;&#30830;&#20445;&#20854;&#21576;&#29616;&#20855;&#26377;&#21487;&#35775;&#38382;&#24615;&#12290;&#39318;&#20808;,&#25105;&#20204;&#35299;&#37322;&#20102;&#26368;&#20248;&#36755;&#36816;&#30340;&#32972;&#26223;,&#24182;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#31867;&#22411;&#12289;&#29305;&#24615;&#21644;&#26174;&#33879;&#24212;&#29992;&#12290;&#28982;&#21518;,&#25105;&#20204;&#30528;&#37325;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#26368;&#20248;&#36755;&#36816;&#25193;&#23637;&#20197;&#24212;&#23545;&#24403;&#21069;&#22823;&#25968;&#25454;&#21644;&#39640;&#32500;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;&#29992;&#20110;&#25193;&#23637;OT&#30340;&#25991;&#29486;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;,&#24182;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#21576;&#29616;&#32467;&#26524;&#20197;&#20419;&#36827;&#29702;&#35299;&#12290;&#26368;&#21518;,&#25105;&#20204;&#25506;&#35752;&#20102;&#21487;&#25193;&#23637;&#26368;&#20248;&#36755;&#36816;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26410;&#26469;&#30740;&#31350;&#30340;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.05065</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20351;&#29992;&#22823;&#35268;&#27169;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#25512;&#33616;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#35757;&#32451;&#21452;&#32534;&#30721;&#27169;&#22411;&#23558;&#26597;&#35810;&#21644;&#20505;&#36873;&#39033;&#23884;&#20837;&#21040;&#30456;&#21516;&#30340;&#31354;&#38388;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#26469;&#36873;&#25321;&#32473;&#23450;&#26597;&#35810;&#23884;&#20837;&#30340;&#39030;&#37096;&#20505;&#36873;&#39033;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#38454;&#27573;&#33539;&#20363;&#65306;&#29983;&#25104;&#24335;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#22238;&#24402;&#26041;&#24335;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#35299;&#30721;&#30446;&#26631;&#20505;&#36873;&#39033;&#30340;&#26631;&#35782;&#31526;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#19981;&#26159;&#20026;&#27599;&#20010;&#39033;&#30446;&#20998;&#37197;&#38543;&#26426;&#29983;&#25104;&#30340;&#21407;&#23376;ID&#65292;&#32780;&#26159;&#29983;&#25104;&#35821;&#20041;ID&#65306;&#27599;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#20803;&#32452;&#32534;&#30721;&#35789;&#65292;&#23427;&#20316;&#20026;&#20854;&#21807;&#19968;&#26631;&#35782;&#31526;&#12290;&#25105;&#20204;&#20351;&#29992;&#31216;&#20026;RQ-VAE&#30340;&#20998;&#23618;&#26041;&#27861;&#29983;&#25104;&#36825;&#20123;&#32534;&#30721;&#35789;&#12290;&#19968;&#26086;&#25105;&#20204;&#23545;&#25152;&#26377;&#39033;&#30446;&#37117;&#26377;&#20102;&#35821;&#20041;ID&#65292;&#23601;&#20250;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#39033;&#30446;&#30340;&#35821;&#20041;ID&#12290;&#30001;&#20110;&#36825;&#20010;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#30452;&#25509;&#39044;&#27979;&#26631;&#35782;&#19979;&#19968;&#20010;&#39033;&#30340;&#32534;&#30721;&#35789;&#20803;&#32452;&#65292;&#22240;&#27492;&#23427;&#21487;&#20197;&#23558;&#26816;&#32034;&#21644;&#29983;&#25104;&#32452;&#21512;&#22312;&#19968;&#36215;&#20197;&#20135;&#29983;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05027</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#30340;&#20027;&#35201;&#30446;&#26631;&#65306;&#20445;&#38556;&#32452;&#32455;&#20813;&#21463;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#65292;&#38480;&#21046;&#35775;&#38382;&#39640;&#39118;&#38505;&#25110;&#21487;&#30097;&#32593;&#31449;&#65292;&#20197;&#21450;&#20419;&#36827;&#23433;&#20840;&#30340;&#19987;&#19994;&#24037;&#20316;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#19987;&#19994;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#12290;&#22312;&#23558;&#36890;&#36807;&#22823;&#22411;&#23433;&#20840;&#20379;&#24212;&#21830;&#25910;&#38598;&#30340;&#23458;&#25143;&#36965;&#27979;&#25968;&#25454;&#30340; 30 &#20010;&#19981;&#21516;&#20869;&#23481;&#31867;&#21035;&#30340;&#32593;&#31449;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#33976;&#39311;&#32467;&#26524;&#23454;&#29616;&#20102; 9% &#30340;&#20998;&#31867;&#31934;&#24230;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#19982;&#21407;&#22987;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#20943;&#23569;&#20102; 175 &#20493;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#19979;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28304;&#22270;&#20687;&#30340;&#21487;&#35270;&#29305;&#24449;&#19982;&#20302;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05023</link><description>&lt;p&gt;
&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#19979;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning. (arXiv:2305.05023v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20998;&#36776;&#29575;&#26465;&#20214;&#19979;&#30340;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#28304;&#22270;&#20687;&#30340;&#21487;&#35270;&#29305;&#24449;&#19982;&#20302;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#30340;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#30456;&#20851;&#30340;&#32454;&#31890;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#36328;&#39046;&#22495;&#30340;&#26144;&#23556;&#65292;&#20551;&#23450;&#29992;&#20110;&#32763;&#35793;&#30340;&#22270;&#20687;&#20849;&#20139;&#20869;&#23481;&#65292;&#20294;&#20855;&#26377;&#33258;&#24049;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65288;&#21363;&#39118;&#26684;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32454;&#31890;&#24230;&#38382;&#39064;&#65292;&#20854;&#20013;&#39046;&#22495;&#30456;&#20851;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#39046;&#22495;&#26080;&#20851;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#19968;&#24133;&#22270;&#20687;&#65292;&#23558;&#28304;&#22270;&#20687;&#30340;&#21487;&#35270;&#29305;&#24449;&#19982;&#20302;&#39057;&#20449;&#24687;&#65288;&#20363;&#22914;&#23039;&#21183;&#12289;&#39068;&#33394;&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#20135;&#29983;&#21516;&#26102;&#20855;&#26377;&#28304;&#22270;&#20687;&#30340;&#29420;&#29305;&#20449;&#24687;&#21644;&#20302;&#20998;&#36776;&#29575;&#30446;&#26631;&#22270;&#20687;&#20449;&#24687;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style). Conditioned on a target image, such methods extract the target style and combine it with the source image content, keeping coherence between the domains. In our proposal, we depart from this traditional view and instead consider the scenario where the target domain is represented by a very low-resolution (LR) image, proposing a domain-agnostic i2i method for fine-grained problems, where the domains are related. More specifically, our domain-agnostic approach aims at generating an image that combines visual features from the source image with low-frequency information (e.g. pose, color) of the LR target image. To do so, we present a novel approach that relies on training the generative model to produce images that both share distinctive information of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;U-Net&#30340;&#39046;&#22495;&#29420;&#31435;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#65288;EIT&#65289;&#20013;&#26469;&#25552;&#39640;&#37325;&#24314;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#28789;&#27963;&#20960;&#20309;&#24418;&#29366;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2305.05020</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;U-Net&#30340;&#39046;&#22495;&#29420;&#31435;&#21518;&#22788;&#29702;&#65306;&#24212;&#29992;&#20110;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Domain independent post-processing with graph U-nets: Applications to Electrical Impedance Tomographic Imaging. (arXiv:2305.05020v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;U-Net&#30340;&#39046;&#22495;&#29420;&#31435;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#30005;&#38459;&#25239;&#23618;&#26512;&#25104;&#20687;&#65288;EIT&#65289;&#20013;&#26469;&#25552;&#39640;&#37325;&#24314;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#23545;&#28789;&#27963;&#20960;&#20309;&#24418;&#29366;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#36793;&#30028;&#27979;&#37327;&#20013;&#37325;&#24314;&#23618;&#26512;&#22270;&#20687;&#38656;&#35201;&#28789;&#27963;&#22788;&#29702;&#30446;&#26631;&#39046;&#22495;&#12290;&#20363;&#22914;&#65292;&#24403;&#31995;&#32479;&#26041;&#31243;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#26102;&#65292;&#37325;&#24314;&#36890;&#24120;&#22312;&#26377;&#38480;&#20803;&#65288;FE&#65289;&#32593;&#26684;&#19978;&#23436;&#25104;&#65292;&#20174;&#32780;&#20801;&#35768;&#28789;&#27963;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#22240;&#27492;&#65292;&#25152;&#24471;&#37325;&#24314;&#30340;&#20219;&#20309;&#22788;&#29702;&#26368;&#22909;&#20063;&#22312;FE&#32593;&#26684;&#19978;&#36827;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#38750;&#24120;&#25104;&#21151;&#30340;U-Net&#26550;&#26500;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#28789;&#27963;&#30340;FE&#32593;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#23558;FE&#32593;&#26684;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#24182;&#22312;&#22270;&#24418;&#19978;&#21046;&#23450;&#20855;&#26377;&#26032;&#38598;&#32676;&#27744;&#21644;&#21453;&#27744;&#30340;&#22270;&#24418;U-Net&#65292;&#27169;&#25311;&#22522;&#20110;&#26368;&#22823;&#27744;&#30340;&#32463;&#20856;&#37051;&#22495;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22270;&#24418;U-Net&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#25913;&#21892;&#20174;&#30005;&#38459;&#25239;&#23618;&#26512;&#65288;EIT&#65289;&#27979;&#37327;&#20013;&#24471;&#21040;&#30340;&#37325;&#24314;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#32447;&#24615;&#21644;&#39640;&#24230;&#30149;&#24577;&#30340;&#36870;&#38382;&#39064;&#12290;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstruction of tomographic images from boundary measurements requires flexibility with respect to target domains. For instance, when the system equations are modeled by partial differential equations the reconstruction is usually done on finite element (FE) meshes, allowing for flexible geometries. Thus, any processing of the obtained reconstructions should be ideally done on the FE mesh as well. For this purpose, we extend the hugely successful U-Net architecture that is limited to rectangular pixel or voxel domains to an equivalent that works flexibly on FE meshes. To achieve this, the FE mesh is converted into a graph and we formulate a graph U-Net with a new cluster pooling and unpooling on the graph that mimics the classic neighborhood based max-pooling. We demonstrate effectiveness and flexibility of the graph U-Net for improving reconstructions from electrical impedance tomographic (EIT) measurements, a nonlinear and highly ill-posed inverse problem. The performance is evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#20989;&#25968;PTLoss&#65292;&#36890;&#36807;&#25200;&#21160;&#32769;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05010</link><description>&lt;p&gt;
&#19981;&#35201;&#30450;&#30446;&#27169;&#20223;&#32769;&#24072;&#65306;&#20351;&#29992;&#25200;&#21160;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#20989;&#25968;PTLoss&#65292;&#36890;&#36807;&#25200;&#21160;&#32769;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#36890;&#24120;&#65292;&#23398;&#29983;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36755;&#20986;&#20998;&#24067;&#21644;&#25945;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#27169;&#20223;&#25945;&#24072;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#23398;&#20064;&#30446;&#26631;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#25945;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#24378;&#21046;&#23398;&#29983;&#30450;&#30446;&#27169;&#20223;&#19981;&#21487;&#38752;&#30340;&#25945;&#24072;&#36755;&#20986;&#20998;&#24067;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#20989;&#25968;PTLoss&#65292;&#39318;&#20808;&#36890;&#36807;Maclaurin&#32423;&#25968;&#34920;&#31034;&#39321;&#33609;KL&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#65292;&#28982;&#21518;&#25200;&#21160;&#35813;&#32423;&#25968;&#20013;&#30340;&#20027;&#23548;&#39033;&#12290;&#36825;&#31181;&#25200;&#21160;&#25439;&#22833;&#38544;&#24335;&#22320;&#23558;&#21407;&#22987;&#32769;&#24072;&#36716;&#25442;&#20026;&#20855;&#26377;&#26356;&#25509;&#36817;&#22320;&#38754;&#30495;&#23454;&#20998;&#24067;&#30340;&#20195;&#29702;&#32769;&#24072;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#39321;&#33609;KL&#33976;&#39311;&#21644;&#25200;&#21160;KL&#33976;&#39311;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340; ICU &#20020;&#24202;&#20195;&#30721;&#39044;&#27979;&#65292;&#38024;&#23545;&#19981;&#23436;&#25972;&#30340;&#20020;&#24202;&#20195;&#30721;&#28165;&#21333;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#20004;&#20010;&#24378;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20849;&#29616;&#30340;&#26041;&#27861;&#34920;&#29616;&#30053;&#24494;&#26356;&#22909;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04992</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340; ICU &#20020;&#24202;&#20195;&#30721;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based prediction of ICU clinical codes. (arXiv:2305.04992v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340; ICU &#20020;&#24202;&#20195;&#30721;&#39044;&#27979;&#65292;&#38024;&#23545;&#19981;&#23436;&#25972;&#30340;&#20020;&#24202;&#20195;&#30721;&#28165;&#21333;&#65292;&#20351;&#29992;&#20102;&#21508;&#31181;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#20004;&#20010;&#24378;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#20849;&#29616;&#30340;&#26041;&#27861;&#34920;&#29616;&#30053;&#24494;&#26356;&#22909;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;&#20013;&#35786;&#26029;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#23545;&#20110;&#24739;&#32773;&#25252;&#29702;&#20197;&#21450;&#25253;&#38144;&#30446;&#30340;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#36755;&#20837;&#21040;&#30005;&#23376;&#30149;&#21382;&#20013;&#38750;&#24120;&#32321;&#29712;&#65292;&#32780;&#19988;&#19968;&#20123;&#20020;&#24202;&#20195;&#30721;&#21487;&#33021;&#20250;&#34987;&#24573;&#30053;&#12290;&#38024;&#23545;&#19981;&#23436;&#25972;&#30340;&#20020;&#24202;&#20195;&#30721;&#28165;&#21333;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#23436;&#25972;&#20020;&#24202;&#20195;&#30721;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#21253;&#21547;&#20854;&#20182;&#20020;&#24202;&#24739;&#32773;&#25968;&#25454;&#30340;&#22686;&#21152;&#39044;&#27979;&#20215;&#20540;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102; MIMIC-III &#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23436;&#25972;&#20020;&#24202;&#20195;&#30721;&#30340;&#20219;&#21153;&#26694;&#26550;&#23450;&#20026;&#25512;&#33616;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#33258;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#20004;&#20010;&#24378;&#22522;&#20934;&#65307;&#39033;&#20849;&#29616;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#12290;&#36755;&#20837;&#21253;&#25324; 1&#65289;&#35760;&#24405;&#30340;&#24050;&#30693;&#20020;&#24202;&#20195;&#30721;&#65292;2&#65289;&#20195;&#30721;&#21152;&#21464;&#37327;&#12290;&#22522;&#20110;&#20849;&#29616;&#30340;&#26041;&#27861;&#30053;&#24494;&#34920;&#29616;&#26356;&#22909;&#65288;F1 &#20998;&#25968;=0.26&#65292;&#22343;&#20540;&#24179;&#22343;&#20934;&#30830;&#24230;[MAP]=0.19&#65289;&#65292;&#32780; SVD&#65288;F1=0.24&#65292;MAP=0.18&#65289;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24403;&#20195;&#30721;&#21152;&#21464;&#37327;&#26102;&#65292;&#23545;&#25239;&#33258;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Availability of diagnostic codes in Electronic Health Records (EHRs) is crucial for patient care as well as reimbursement purposes. However, entering them in the EHR is tedious, and some clinical codes may be overlooked. Given an in-complete list of clinical codes, we investigate the performance of ML methods on predicting the complete ones, and assess the added predictive value of including other clinical patient data in this task. We used the MIMIC-III dataset and frame the task of completing the clinical codes as a recommendation problem. We con-sider various autoencoder approaches plus two strong baselines; item co-occurrence and Singular Value Decomposition (SVD). Inputs are 1) a record's known clinical codes, 2) the codes plus variables. The co-occurrence-based ap-proach performed slightly better (F1 score=0.26, Mean Average Precision [MAP]=0.19) than the SVD (F1=0.24, MAP=0.18). However, the adversarial autoencoder achieved the best performance when using the codes plus variable
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04990</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24494;&#35843;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#24378;&#38887;
&lt;/p&gt;
&lt;p&gt;
Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#26377;&#26102;&#20250;&#23398;&#20064;&#21040;&#26631;&#31614;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#37322;&#24615;&#24494;&#35843;&#20316;&#20026;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#21482;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31572;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#24494;&#35843;&#27169;&#22411;&#20197;&#29983;&#25104;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#35813;&#35757;&#32451;&#38598;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#34394;&#20551;&#25552;&#31034;&#65292;&#24182;&#22312;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26041;&#38754;&#20351;&#27169;&#22411;&#26497;&#20854;&#24378;&#38887;&#65306;ComVE&#65288;+1.2&#65289;&#65292;CREAK&#65288;+9.1&#65289;&#65292;e-SNLI&#65288;+15.4&#65289;&#21644;SBIC&#65288;+6.5&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#21516;&#26679;&#26377;&#25928;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22359;&#22352;&#26631;&#19979;&#38477;&#20998;&#24067;&#24335;&#31639;&#27861;&#23454;&#29616;&#23545;&#23458;&#25143;&#31471;&#31169;&#26377;&#25968;&#25454;&#19981;&#36879;&#38706;&#30340;&#23398;&#20064;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#19982;&#27491;&#21017;&#21270;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.04979</link><description>&lt;p&gt;
FedHB: &#23618;&#27425;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedHB: Hierarchical Bayesian Federated Learning. (arXiv:2305.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22359;&#22352;&#26631;&#19979;&#38477;&#20998;&#24067;&#24335;&#31639;&#27861;&#23454;&#29616;&#23545;&#23458;&#25143;&#31471;&#31169;&#26377;&#25968;&#25454;&#19981;&#36879;&#38706;&#30340;&#23398;&#20064;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#19982;&#27491;&#21017;&#21270;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23618;&#27425;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#27425;&#36125;&#21494;&#26031;&#24314;&#27169;&#21512;&#29702;&#22320;&#25551;&#36848;&#20102;&#23458;&#25143;&#31471;&#26412;&#22320;&#25968;&#25454;&#30340;&#29983;&#25104;&#36807;&#31243;&#65306;&#26500;&#25104;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#30340;&#38543;&#26426;&#21464;&#37327;&#65292;&#30001;&#26356;&#39640;&#27700;&#24179;&#30340;&#20840;&#23616;&#21464;&#37327;&#36827;&#34892;&#25511;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#36125;&#21494;&#26031;&#27169;&#22411;&#20013;&#30340;&#21464;&#20998;&#25512;&#26029;&#23548;&#33268;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#22359;&#22352;&#26631;&#19979;&#38477;&#27714;&#35299;&#25104;&#20026;&#19968;&#20010;&#21487;&#20998;&#23458;&#25143;&#31471;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#36825;&#20351;&#24471;&#23458;&#25143;&#31471;&#23436;&#20840;&#19981;&#38656;&#35201;&#36879;&#38706;&#33258;&#24049;&#30340;&#31169;&#26377;&#25968;&#25454;&#65292;&#22240;&#27492;&#19982;&#32852;&#37030;&#23398;&#20064;&#23436;&#20840;&#20860;&#23481;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#65292;&#25105;&#20204;&#30340;&#22359;&#22352;&#26631;&#31639;&#27861;&#20855;&#26377;&#29305;&#23450;&#24418;&#24335;&#65292;&#21253;&#25324;Fed-Avg&#21644;Fed-Prox&#22312;&#20869;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;FL&#31639;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#20854;&#29305;&#20363;&#36827;&#34892;&#23376;&#24402;&#12290;&#38500;&#20102;&#24341;&#20837;&#26032;&#30340;&#24314;&#27169;&#21644;&#23548;&#20986;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#22359;&#22352;&#26631;FL&#31639;&#27861;&#20197;$O(1/\sqrt{t})$&#30340;&#36895;&#24230;&#25910;&#25947;&#21040;&#30446;&#26631;&#30340;&#65288;&#26412;&#22320;&#65289;&#26368;&#20248;&#35299;&#65292;&#36825;&#19982;&#27491;&#21017;&#21270;&#20855;&#26377;&#30456;&#21516;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel hierarchical Bayesian approach to Federated Learning (FL), where our model reasonably describes the generative process of clients' local data via hierarchical Bayesian modeling: constituting random variables of local models for clients that are governed by a higher-level global variate. Interestingly, the variational inference in our Bayesian model leads to an optimisation problem whose block-coordinate descent solution becomes a distributed algorithm that is separable over clients and allows them not to reveal their own private data at all, thus fully compatible with FL. We also highlight that our block-coordinate algorithm has particular forms that subsume the well-known FL algorithms including Fed-Avg and Fed-Prox as special cases. Beyond introducing novel modeling and derivations, we also offer convergence analysis showing that our block-coordinate FL algorithm converges to an (local) optimum of the objective at the rate of $O(1/\sqrt{t})$, the same rate as regul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#65292;&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;LABO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#65292;&#24182;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#26631;&#31614;&#24179;&#28369;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.04971</link><description>&lt;p&gt;
LABO: &#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#26368;&#20339;&#26631;&#31614;&#27491;&#21017;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization. (arXiv:2305.04971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#65292;&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;LABO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#65292;&#24182;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#26631;&#31614;&#24179;&#28369;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#25216;&#26415;&#23545;&#20110;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20381;&#36182;&#20110;&#26435;&#37325;&#34928;&#20943;&#12289;&#20002;&#24323;&#12289;&#25209;/&#23618;&#24402;&#19968;&#21270;&#31561;&#25216;&#26415;&#26469;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#21478;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;LS&#20551;&#35774;&#27599;&#20010;&#38750;&#30446;&#26631;&#31867;&#21035;&#20986;&#29616;&#30340;&#27010;&#29575;&#30456;&#31561;&#65292;&#19981;&#33021;&#26681;&#25454;&#23454;&#20363;&#23545;&#26631;&#31614;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#21452;&#23618;&#20248;&#21270;&#65288;LABO&#65289;&#38382;&#39064;&#26469;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20869;&#29615;&#33410;&#30340;&#30830;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#35299;&#65292;&#32780;&#26080;&#38656;&#23384;&#20648;&#32463;&#36807;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;Deep Evidence Regression&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#20449;&#29992;&#39118;&#38505;&#20013;&#30340;&#36829;&#32422;&#25439;&#22833;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.04967</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#31649;&#29702;&#20013;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65306;&#19968;&#31181;&#28145;&#24230;&#35777;&#25454;&#22238;&#24402;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UQ for Credit Risk Management: A deep evidence regression approach. (arXiv:2305.04967v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;Deep Evidence Regression&#26041;&#27861;&#65292;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#20449;&#29992;&#39118;&#38505;&#20013;&#30340;&#36829;&#32422;&#25439;&#22833;&#65307;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#20449;&#29992;&#39118;&#38505;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#30001;&#20110;&#20449;&#29992;&#39118;&#38505;&#30340;&#22266;&#26377;&#24615;&#36136;&#65292;&#37327;&#21270;&#39044;&#27979;&#39118;&#38505;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#35201;&#30340;&#65292;&#23558;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#20449;&#29992;&#39118;&#38505;&#35774;&#32622;&#20013;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#22312;&#26412;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;UQ&#24863;&#30693;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;Deep Evidence Regression&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#36829;&#32422;&#25439;&#22833;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;Deep Evidence Regression&#26041;&#27861;&#25193;&#23637;&#21040;&#36890;&#36807;Weibull&#36807;&#31243;&#29983;&#25104;&#30340;&#30446;&#26631;&#21464;&#37327;&#30340;&#23398;&#20064;&#26469;&#20026;&#25991;&#29486;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning has invariantly found its way into various Credit Risk applications. Due to the intrinsic nature of Credit Risk, quantifying the uncertainty of the predicted risk metrics is essential, and applying uncertainty-aware deep learning models to credit risk settings can be very helpful. In this work, we have explored the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression and applied it to predicting Loss Given Default. We contribute to the literature by extending the Deep Evidence Regression methodology to learning target variables generated by a Weibull process and provide the relevant learning framework. We demonstrate the application of our approach to both simulated and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#31995;&#27744;&#21270;&#21040;&#23376;&#22270;GNN&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#25552;&#39640;&#20219;&#20309;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#65292;&#36890;&#36807;&#26126;&#30830;&#26631;&#35760;&#33410;&#28857;&#20316;&#20026;&#38468;&#21152;&#29305;&#24449;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292; &#24182;&#21487;&#22312;&#35768;&#22810;syn&#19978;&#23454;&#29616;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04963</link><description>&lt;p&gt;
&#20174;&#20851;&#31995;&#27744;&#21270;&#21040;&#23376;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks. (arXiv:2305.04963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20851;&#31995;&#27744;&#21270;&#21040;&#23376;&#22270;GNN&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#25552;&#39640;&#20219;&#20309;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#65292;&#36890;&#36807;&#26126;&#30830;&#26631;&#35760;&#33410;&#28857;&#20316;&#20026;&#38468;&#21152;&#29305;&#24449;&#26469;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292; &#24182;&#21487;&#22312;&#35768;&#22810;syn&#19978;&#23454;&#29616;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#27744;&#21270;&#26159;&#29992;&#20110;&#26500;&#24314;&#26356;&#20855;&#34920;&#29616;&#21147;&#21644;&#32622;&#25442;&#19981;&#21464;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20851;&#31995;&#27744;&#21270;&#22312;&#34920;&#29616;&#21147;&#26041;&#38754;&#30340;&#30830;&#20999;&#22686;&#24378;&#21450;&#20854;&#19982;Weisfeiler-Lehman&#20998;&#23618;&#30340;&#32852;&#31995;&#30340;&#29702;&#35299;&#26159;&#26377;&#38480;&#30340;&#12290;&#20174;&#20851;&#31995;&#27744;&#21270;&#20986;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#33410;&#28857;&#26126;&#30830;&#26631;&#35760;&#20026;&#38468;&#21152;&#29305;&#24449;&#20197;&#25552;&#39640;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#29616;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#39640;&#32500;WL&#65292;&#24471;&#21040;&#19968;&#31181;&#26032;&#30340;$k,l$-WL&#31639;&#27861;&#65292;&#27604;$k$-WL&#26356;&#36890;&#29992;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;$k,l$-WL&#30456;&#23545;&#20110;$k$&#21644;$l$&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#19982;&#22823;&#37327;&#30340;&#23376;&#22270;GNN&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#35752;&#35770;&#20102;&#22797;&#26434;&#24230;&#38477;&#20302;&#26041;&#27861;&#65292;&#20197;&#26500;&#24314;&#24378;&#22823;&#32780;&#23454;&#29992;&#30340;$k,l$-GNN&#23454;&#20363;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#20860;&#23481;&#30340;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#20219;&#20309;&#22522;&#26412;GNN&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;$k,l$-GNN&#22312;&#35768;&#22810;syn&#19978;&#23454;&#29616;&#20102;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel $k,l$-WL algorithm, a more general framework than $k$-WL. Theoretically, we analyze the expressivity of $k,l$-WL with respect to $k$ and $l$ and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical $k,l$-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our $k,l$-GNNs achieve superior performance on many syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#32852;&#21512;&#35270;&#39057;&#25688;&#35201;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#32447;&#32034;&#21305;&#37197;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#23454;&#29616;&#26816;&#32034;&#35270;&#39057;&#20013;&#26368;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#26102;&#21051;&#12290;</title><link>http://arxiv.org/abs/2305.04961</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#32852;&#21512;&#26102;&#21051;&#26816;&#32034;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Joint Moment Retrieval and Highlight Detection Via Natural Language Queries. (arXiv:2305.04961v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#32852;&#21512;&#35270;&#39057;&#25688;&#35201;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#32447;&#32034;&#21305;&#37197;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#23454;&#29616;&#26816;&#32034;&#35270;&#39057;&#20013;&#26368;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#26102;&#21051;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#35270;&#39057;&#25688;&#35201;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#32852;&#21512;&#35270;&#39057;&#25688;&#35201;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20351;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#32447;&#32034;&#26469;&#21305;&#37197;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20197;&#26816;&#32034;&#35270;&#39057;&#20013;&#26368;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#26102;&#21051;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22810;&#20010;&#26368;&#36817;&#29992;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;(ViTs)&#30340;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;YouTube Highlights&#21644;TVSum&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video summarization has become an increasingly important task in the field of computer vision due to the vast amount of video content available on the internet. In this project, we propose a new method for natural language query based joint video summarization and highlight detection using multi-modal transformers. This approach will use both visual and audio cues to match a user's natural language query to retrieve the most relevant and interesting moments from a video. Our approach employs multiple recent techniques used in Vision Transformers (ViTs) to create a transformer-like encoder-decoder model. We evaluated our approach on multiple datasets such as YouTube Highlights and TVSum to demonstrate the flexibility of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04940</link><description>&lt;p&gt;
&#26089;&#36215;&#30340;&#40479;&#20799;&#25417;&#21040;&#34411;&#65306;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#31867;&#22411;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#20013;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#30340;&#26368;&#20339;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#26500;&#24314;&#20195;&#30721;&#30340;&#22797;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;12&#31181;&#21019;&#24314;&#22797;&#21512;&#34920;&#31034;&#30340;&#31574;&#30053;&#19982;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#32534;&#30721;&#22120;&#23618;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#22312;CodeBERT&#27169;&#22411;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20960;&#20010;&#26089;&#26399;&#23618;&#30340;&#32452;&#21512;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19968;&#20123;&#32452;&#21512;&#21017;&#25913;&#36827;&#20102;&#22810;&#31867;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24179;&#22343;&#26816;&#27979;&#22686;&#24378;2&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.04934</link><description>&lt;p&gt;
&#24212;&#29992;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#33258;&#22238;&#24402;Transformer&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#20998;&#26512;&#21644;&#21457;&#29616;&#26032;&#22411;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#24212;&#29992;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#32467;&#26500;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#35757;&#32451;&#21518;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#65292;&#26696;&#20363;&#39564;&#35777;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#29983;&#25104;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#30340;&#34507;&#30333;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#34507;&#30333;&#36136;&#24314;&#27169;&#20013;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#20010;&#25972;&#21512;&#20102;transformer&#21644;&#22270;&#21367;&#31215;&#30340;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#22312;&#22240;&#26524;&#22810;&#22836;&#22270;&#26426;&#21046;&#20013;&#23454;&#29616;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#39044;&#27979;&#20108;&#32423;&#32467;&#26500;&#20869;&#23481;&#65288;&#27599;&#20010;&#27531;&#22522;&#30340;&#27700;&#24179;&#21644;&#24635;&#20307;&#20869;&#23481;&#65289;&#12289;&#34507;&#30333;&#36136;&#21487;&#28342;&#24615;&#21644;&#27979;&#24207;&#20219;&#21153;&#12290;&#36827;&#19968;&#27493;&#22312;&#21453;&#21521;&#20219;&#21153;&#19978;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#35774;&#35745;&#20855;&#26377;&#36825;&#20123;&#24615;&#36136;&#20316;&#20026;&#30446;&#26631;&#29305;&#24449;&#30340;&#34507;&#30333;&#36136;&#12290;&#35813;&#27169;&#22411;&#34987;&#21046;&#23450;&#20026;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#23436;&#20840;&#22522;&#20110;&#25552;&#31034;&#65292;&#21487;&#20197;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#39069;&#22806;&#20219;&#21153;&#20250;&#20135;&#29983;&#30456;&#20114;&#21327;&#21516;&#20316;&#29992;&#65292;&#20351;&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#24471;&#21040;&#25552;&#39640;&#65292;&#36229;&#36807;&#20165;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#26696;&#20363;&#30740;&#31350;&#29992;&#20110;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#29983;&#25104;&#20855;&#26377;&#29702;&#24819;&#30446;&#26631;&#24615;&#36136;&#65292;&#21253;&#25324;&#31283;&#23450;&#24615;&#21644;&#21487;&#28342;&#24615;&#30340;&#34507;&#30333;&#36136;&#65292;&#24182;&#36827;&#34892;&#23454;&#39564;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a flexible language-model based deep learning strategy, applied here to solve complex forward and inverse problems in protein modeling, based on an attention neural network that integrates transformer and graph convolutional architectures in a causal multi-headed graph mechanism, to realize a generative pretrained model. The model is applied to predict secondary structure content (per-residue level and overall content), protein solubility, and sequencing tasks. Further trained on inverse tasks, the model is rendered capable of designing proteins with these properties as target features. The model is formulated as a general framework, completely prompt-based, and can be adapted for a variety of downstream tasks. We find that adding additional tasks yields emergent synergies that the model exploits in improving overall performance, beyond what would be possible by training a model on each dataset alone. Case studies are presented to validate the method, yielding protein designs
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25945;&#31243;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24110;&#21161;&#25552;&#21319;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36827;&#32780;&#20419;&#36827;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04933</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#24037;&#31243;&#35774;&#35745;&#19982;&#20581;&#24247;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification in Machine Learning for Engineering Design and Health Prognostics: A Tutorial. (arXiv:2305.04933v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25945;&#31243;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#24110;&#21161;&#25552;&#21319;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36827;&#32780;&#20419;&#36827;&#20854;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#22312;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#20316;&#20026;&#23433;&#20840;&#20445;&#38556;&#30340;&#22522;&#26412;&#23618;&#65292;&#21487;&#20197;&#36890;&#36807;&#21551;&#29992;&#21512;&#29702;&#30340;&#39118;&#38505;&#35780;&#20272;&#21644;&#31649;&#29702;&#26469;&#20419;&#36827;&#26356;&#21152;&#21407;&#21017;&#24615;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;UQ&#20351;&#24471;ML&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#24471;&#21040;&#25913;&#21892;&#65292;&#26377;&#28508;&#21147;&#26174;&#33879;&#20419;&#36827;&#39640;&#39118;&#38505;&#20915;&#31574;&#32972;&#26223;&#19979;&#30340;ML&#35299;&#20915;&#26041;&#26696;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#21046;&#36896;&#19994;&#21644;&#33322;&#31354;&#31561;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#26041;&#20301;&#20171;&#32461;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26032;&#20852;UQ&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21450;&#36825;&#20123;UQ&#26041;&#27861;&#22312;&#35299;&#20915;&#24037;&#31243;&#35774;&#35745;&#21644;&#39044;&#27979;&#20581;&#24247;&#31649;&#29702;&#38382;&#39064;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#28041;&#21450;ML&#27169;&#22411;UQ&#30340;&#19981;&#30830;&#23450;&#24615;&#31867;&#22411;&#65292;&#26469;&#28304;&#21644;&#21407;&#22240;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;UQ&#26041;&#27861;&#30340;&#25945;&#31243;&#24335;&#25551;&#36848;&#65306;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
On top of machine learning models, uncertainty quantification (UQ) functions as an essential layer of safety assurance that could lead to more principled decision making by enabling sound risk assessment and management. The safety and reliability improvement of ML models empowered by UQ has the potential to significantly facilitate the broad adoption of ML solutions in high-stakes decision settings, such as healthcare, manufacturing, and aviation, to name a few. In this tutorial, we aim to provide a holistic lens on emerging UQ methods for ML models with a particular focus on neural networks and the applications of these UQ methods in tackling engineering design as well as prognostics and health management problems. Toward this goal, we start with a comprehensive classification of uncertainty types, sources, and causes pertaining to UQ of ML models. Next, we provide a tutorial-style description of several state-of-the-art UQ methods: Gaussian process regression, Bayesian neural network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;&#20449;&#21495;&#30340;&#21435;&#21367;&#31215;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#35266;&#27979;&#20540;&#20013;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#25968;&#25454;&#19988;&#20449;&#21495;&#28388;&#27874;&#22120;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.04871</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#21435;&#21367;&#31215;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gaussian process deconvolution. (arXiv:2305.04871v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#36830;&#32493;&#26102;&#38388;&#20449;&#21495;&#30340;&#21435;&#21367;&#31215;&#38382;&#39064;&#65292;&#36866;&#29992;&#20110;&#35266;&#27979;&#20540;&#20013;&#21487;&#33021;&#23384;&#22312;&#32570;&#22833;&#25968;&#25454;&#19988;&#20449;&#21495;&#28388;&#27874;&#22120;&#26410;&#30693;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#21435;&#21367;&#31215;&#38382;&#39064;&#65292;&#21363;&#20174;&#21367;&#31215;&#22788;&#29702;&#30340;&#35266;&#27979;&#20540; $\mathbf{y}$ &#20013;&#24674;&#22797;&#28508;&#22312;&#20449;&#21495; $x(\cdot)$&#65292;&#20854;&#20013;&#35266;&#27979;&#20540; $\mathbf{y}$ &#21487;&#33021;&#23545;&#24212;&#20110; $y$ &#30340;&#19968;&#37096;&#20998;&#32570;&#22833;&#65292;&#28388;&#27874;&#22120; $h$ &#21487;&#33021;&#26410;&#30693;&#19988;&#22122;&#22768; $\eta$ &#21487;&#21152;&#24615;&#12290;&#24403; $x$ &#26159;&#36830;&#32493;&#26102;&#38388;&#20449;&#21495;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20808;&#39564;&#20998;&#24067;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#21435;&#21367;&#31215;&#31574;&#30053;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#30452;&#25509;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#20854;&#33391;&#22909;&#23450;&#20041;&#30340;&#26465;&#20214;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36716;&#21521;&#36870;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#65306;&#65288;i&#65289;&#19968;&#20123;&#24517;&#35201;&#26465;&#20214;&#65292;&#20351;&#24471;&#36125;&#21494;&#26031;&#21435;&#21367;&#31215;&#35745;&#31639;&#26377;&#21487;&#33021;&#25104;&#31435;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#22312;&#21738;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#28388;&#27874;&#22120; $h$&#65292;&#20197;&#21450;&#22312;&#30450;&#21435;&#21367;&#31215;&#24773;&#20917;&#19979;&#21487;&#20197;&#36817;&#20284;&#28388;&#27874;&#22120; $h$ &#30340;&#31243;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#39640;&#26031;&#36807;&#31243;&#21435;&#21367;&#31215;&#65288;GPDC&#65289;&#65292;&#24182;&#19982;&#20854;&#20182;&#21435;&#21367;&#31215;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Let us consider the deconvolution problem, that is, to recover a latent source $x(\cdot)$ from the observations $\y = [y_1,\ldots,y_N]$ of a convolution process $y = x\star h + \eta$, where $\eta$ is an additive noise, the observations in $\y$ might have missing parts with respect to $y$, and the filter $h$ could be unknown. We propose a novel strategy to address this task when $x$ is a continuous-time signal: we adopt a Gaussian process (GP) prior on the source $x$, which allows for closed-form Bayesian nonparametric deconvolution. We first analyse the direct model to establish the conditions under which the model is well defined. Then, we turn to the inverse problem, where we study i) some necessary conditions under which Bayesian deconvolution is feasible, and ii) to which extent the filter $h$ can be learnt from data or approximated for the blind deconvolution case. The proposed approach, termed Gaussian process deconvolution (GPDC) is compared to other deconvolution methods concep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Energy Twin&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#24314;&#31569;&#29289;&#33021;&#28304;&#20351;&#29992;&#27169;&#24335;&#24182;&#25552;&#20379;&#20248;&#21270;&#33021;&#28304;&#25928;&#29575;&#30340;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.04498</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#25216;&#26415;&#25552;&#39640;&#24314;&#31569;&#29289;&#33021;&#28304;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leveraging Deep Learning and Digital Twins to Improve Energy Performance of Buildings. (arXiv:2305.04498v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Energy Twin&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30456;&#32467;&#21512;&#65292;&#20197;&#35782;&#21035;&#24314;&#31569;&#29289;&#33021;&#28304;&#20351;&#29992;&#27169;&#24335;&#24182;&#25552;&#20379;&#20248;&#21270;&#33021;&#28304;&#25928;&#29575;&#30340;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31569;&#29289;&#25968;&#23383;&#21270;&#36716;&#22411;&#31215;&#32047;&#20102;&#22823;&#37327;&#36816;&#33829;&#25968;&#25454;&#65292;&#38656;&#35201;&#26234;&#33021;&#21270;&#35299;&#20915;&#26041;&#26696;&#26469;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#25552;&#39640;&#33021;&#28304;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#8220;Deep Energy Twin&#8221;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#23383;&#23402;&#29983;&#30456;&#32467;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#24314;&#31569;&#29289;&#33021;&#28304;&#20351;&#29992;&#24773;&#20917;&#65292;&#24182;&#35782;&#21035;&#25552;&#39640;&#33021;&#28304;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;&#37319;&#29992;&#26412;&#20307;&#35770;&#21019;&#24314;&#21442;&#25968;&#25968;&#23383;&#23402;&#29983;&#65292;&#20197;&#25552;&#20379;&#24314;&#31569;&#29289;&#20013;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#25968;&#25454;&#26684;&#24335;&#30340;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#21019;&#24314;&#30340;&#25968;&#23383;&#23402;&#29983;&#21644;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#27169;&#24335;&#24182;&#20026;&#33021;&#28304;&#20248;&#21270;&#25552;&#20379;&#27934;&#35265;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#26412;&#30740;&#31350;&#22312;&#29790;&#20856;&#35834;&#23572;&#32943;&#24179;&#30340;&#19968;&#24231;&#20844;&#20849;&#21382;&#21490;&#24314;&#31569;&#20013;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#22312;&#24314;&#31569;&#29289;&#33021;&#28304;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital transformation in buildings accumulates massive operational data, which calls for smart solutions to utilize these data to improve energy performance. This study has proposed a solution, namely Deep Energy Twin, for integrating deep learning and digital twins to better understand building energy use and identify the potential for improving energy efficiency. Ontology was adopted to create parametric digital twins to provide consistency of data format across different systems in a building. Based on created digital twins and collected data, deep learning methods were used for performing data analytics to identify patterns and provide insights for energy optimization. As a demonstration, a case study was conducted in a public historic building in Norrk\"oping, Sweden, to compare the performance of state-of-the-art deep learning architectures in building energy forecasting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.04152</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#32447;&#36890;&#20449;&#30340;&#36890;&#36947;&#39537;&#21160;&#38543;&#26426;&#26799;&#24230; Langevin &#21160;&#21147;&#23398;&#36125;&#21494;&#26031;&#32852;&#37030;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Bayesian Over-the-Air FedAvg via Channel Driven Stochastic Gradient Langevin Dynamics. (arXiv:2305.04152v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26080;&#32447; FALD &#21327;&#35758;&#65292;&#21487;&#20197;&#22312;&#26080;&#22122;&#22768;&#36890;&#20449;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#22312;&#26080;&#32447;&#31995;&#32479;&#20013;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20197;&#21450;&#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#24182;&#36827;&#34892;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#27861;&#30340;&#36817;&#26399;&#21457;&#23637;&#24050;&#32463;&#37325;&#26032;&#24341;&#36215;&#20102;&#23545;&#37319;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#20316;&#20026;&#20256;&#32479;&#39057;&#29575;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#30340;&#20852;&#36259;&#65292;&#20854;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#27169;&#22411;&#26657;&#20934;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;&#32852;&#37030;&#24179;&#22343; Langevin &#21160;&#21147;&#23398;(FALD)&#20316;&#20026;&#32852;&#37030;&#24179;&#22343;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#36890;&#20449;&#23384;&#22312;&#19979;&#26377;&#25928;&#22320;&#23454;&#29616;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#32447; FALD(WFALD)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#35758;&#65292;&#36890;&#36807;&#38598;&#25104;&#22522;&#20110;&#31354;&#20013;&#35745;&#31639;&#21644;&#22522;&#20110;&#36890;&#36947;&#39537;&#21160;&#30340; Monte Carlo &#26356;&#26032;&#26469;&#23454;&#29616;&#26080;&#32447;&#31995;&#32479;&#20013;&#30340; FALD&#12290;&#19982;&#20808;&#21069;&#30340;&#26080;&#32447;&#36125;&#21494;&#26031;&#23398;&#20064;&#30456;&#27604;&#65292;WFALD &#21487;&#20197;&#23454;&#29616;(i) &#22312;&#36890;&#20449;&#22238;&#21512;&#20043;&#38388;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#65307;&#24182;&#19988;(ii) &#30001;&#23567;&#25209;&#37327;&#35745;&#31639;&#30340;&#38543;&#26426;&#26799;&#24230;&#12290;&#20197; 2-Wasserstein &#36317;&#31163;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32473;&#20986;&#20102;&#26679;&#26412;&#25910;&#25947;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development of scalable Bayesian inference methods has renewed interest in the adoption of Bayesian learning as an alternative to conventional frequentist learning that offers improved model calibration via uncertainty quantification. Recently, federated averaging Langevin dynamics (FALD) was introduced as a variant of federated averaging that can efficiently implement distributed Bayesian learning in the presence of noiseless communications. In this paper, we propose wireless FALD (WFALD), a novel protocol that realizes FALD in wireless systems by integrating over-the-air computation and channel-driven sampling for Monte Carlo updates. Unlike prior work on wireless Bayesian learning, WFALD enables (\emph{i}) multiple local updates between communication rounds; and (\emph{ii}) stochastic gradients computed by mini-batch. A convergence analysis is presented in terms of the 2-Wasserstein distance between the samples produced by WFALD and the targeted global posterior distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.04111</link><description>&lt;p&gt;
&#31163;&#25955;&#25193;&#25955;&#24314;&#27169;&#19979;&#30340;&#39640;&#25928;&#21644;&#24230;&#25968;&#24341;&#23548;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Efficient and Degree-Guided Graph Generation via Discrete Diffusion Modeling. (arXiv:2305.04111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#65292;&#24182;&#36890;&#36807;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#12290;EDGE&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20165;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#65292;&#24182;&#19988;&#21487;&#20197;&#26126;&#30830;&#22320;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#23567;&#22270;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26356;&#21487;&#25193;&#23637;&#24615;&#65292;&#20197;&#29983;&#25104;&#21253;&#21547;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22270;&#24182;&#28385;&#36275;&#22270;&#32479;&#35745;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EDGE&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#22270;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#22270;&#30340;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#38271;&#38543;&#26426;&#21024;&#38500;&#36793;&#26469;&#40723;&#21169;&#22270;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#26368;&#32456;&#33719;&#24471;&#19968;&#24352;&#31354;&#30333;&#22270;&#12290;EDGE&#20165;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#20851;&#27880;&#22270;&#20013;&#19968;&#37096;&#20998;&#33410;&#28857;&#12290;&#23427;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#26356;&#23569;&#22320;&#36827;&#34892;&#36793;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;EDGE&#26126;&#30830;&#22320;&#20801;&#35768;&#23545;&#22270;&#30340;&#33410;&#28857;&#24230;&#25968;&#36827;&#34892;&#24314;&#27169;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;EDGE&#27604;&#31454;&#20105;&#26041;&#27861;&#26356;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#25968;&#21315;&#20010;&#33410;&#28857;&#30340;&#22823;&#22411;&#22270;&#12290;&#23427;&#36824;&#22312;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation qual
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2305.02640</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#23384;&#22312;&#38544;&#24615;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Learning to Recover Causal Relationship from Indefinite Data in the Presence of Latent Confounders. (arXiv:2305.02640v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#23384;&#22312;&#30340;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#21644;&#20998;&#24067;&#20551;&#35774;&#26080;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#28508;&#22312;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#25968;&#25454;&#33539;&#24335;&#65306;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#21333;&#20540;&#30340;&#21333;&#20010;&#39592;&#26550;&#32467;&#26500;&#65292;&#21644;&#19981;&#30830;&#23450;&#25968;&#25454;&#65306;&#20855;&#26377;&#35266;&#23519;&#33410;&#28857;&#22810;&#20540;&#30340;&#19968;&#32452;&#22810;&#39592;&#26550;&#32467;&#26500;&#12290;&#22810;&#20010;&#39592;&#26550;&#24341;&#20837;&#20302;&#26679;&#26412;&#21033;&#29992;&#29575;&#65292;&#22810;&#20010;&#20540;&#24341;&#20837;&#20102;&#20998;&#24067;&#20551;&#35774;&#30340;&#26080;&#33021;&#21147;&#65292;&#36825;&#20004;&#32773;&#23548;&#33268;&#20174;&#19981;&#30830;&#23450;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#20851;&#31995;&#33267;&#20170;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22240;&#26524;&#24378;&#24230;&#21464;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#24378;&#24230;&#32780;&#19981;&#26159;&#29420;&#31435;&#22122;&#22768;&#20316;&#20026;&#28508;&#21464;&#37327;&#26469;&#35843;&#33410;&#35777;&#25454;&#19979;&#30028;&#12290;&#36890;&#36807;&#36825;&#31181;&#35774;&#35745;&#24605;&#24819;&#65292;&#19981;&#21516;&#39592;&#26550;&#30340;&#22240;&#26524;&#24378;&#24230;&#34987;&#30475;&#20316;&#26159;&#19968;&#20010;&#20998;&#24067;&#65292;&#24182;&#21487;&#20197;&#34920;&#31034;&#20026;&#21333;&#20540;&#22240;&#26524;&#22270;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#65292;&#25105;&#20204;&#23558;&#22240;&#26524;&#22270;G&#20998;&#35299;&#20026;&#20004;&#20010;&#30456;&#20851;&#23376;&#22270;O&#21644;C&#12290;O&#21253;&#21547;&#35266;&#23519;&#33410;&#28857;&#20043;&#38388;&#30340;&#32431;&#20851;&#31995;&#65292;&#32780;C&#34920;&#31034;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Causal Discovery with latent variables, We define two data paradigms: definite data: a single-skeleton structure with observed nodes single-value, and indefinite data: a set of multi-skeleton structures with observed nodes multi-value. Multi,skeletons induce low sample utilization and multi values induce incapability of the distribution assumption, both leading that recovering causal relations from indefinite data is, as of yet, largely unexplored. We design the causal strength variational model to settle down these two problems. Specifically, we leverage the causal strength instead of independent noise as latent variable to mediate evidence lower bound. By this design ethos, The causal strength of different skeletons is regarded as a distribution and can be expressed as a single-valued causal graph matrix. Moreover, considering the latent confounders, we disentangle the causal graph G into two relatisubgraphs O and C. O contains pure relations between observed nodes, while C repres
&lt;/p&gt;</description></item><item><title>CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.01118</link><description>&lt;p&gt;
CSP&#65306;&#38024;&#23545;&#22320;&#29702;&#31354;&#38388;&#35270;&#35273;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#31354;&#38388;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01118
&lt;/p&gt;
&lt;p&gt;
CSP&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29702;&#20449;&#24687;&#65292;&#23398;&#20064;&#22320;&#29702;&#20301;&#32622;&#30340;&#26377;&#25928;&#34920;&#31034;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30340;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#20844;&#24320;&#21487;&#29992;&#65292;&#32780;&#23545;&#35937;&#31867;&#21035;&#31561;&#26631;&#31614;&#21017;&#30456;&#23545;&#31232;&#32570;&#19988;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#21516;&#26102;&#65292;&#23545;&#27604;&#23398;&#20064;&#22312;&#21508;&#31181;&#33258;&#28982;&#22270;&#20687;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20165;&#38656;&#24456;&#23569;&#30340;&#24102;&#26631;&#31614;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#36825;&#21487;&#33021;&#26159;&#21306;&#20998;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#23545;&#35937;&#30340;&#20851;&#38190;&#12290;&#20026;&#20102;&#22312;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#38454;&#27573;&#30452;&#25509;&#21033;&#29992;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20016;&#23500;&#22320;&#29702;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550; Contrastive Spatial Pre-Training&#65288;CSP&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#22320;&#29702;&#20301;&#32622;&#36827;&#34892;&#32534;&#30721;&#65292;&#21033;&#29992;&#23545;&#27604;&#30446;&#26631;&#20174;&#22270;&#20687;&#20013;&#23398;&#20064;&#26377;&#25928;&#30340;&#20301;&#32622;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#21487;&#20197;&#36716;&#31227;&#21040;&#19979;&#28216;&#30417;&#30563;&#20219;&#21153;&#65292;&#20363;&#22914;&#22270;&#20687;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CSP&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#26631;&#35760;&#22270;&#20687;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.00909</link><description>&lt;p&gt;
&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;&#65306;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation. (arXiv:2305.00909v2 [cs.PL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00909
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#24341;&#23548;&#30340;&#31895;-&#32454;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65292;&#25903;&#25345;&#20174;&#31895;&#21040;&#32454;&#30340;&#22810;&#27425;&#36845;&#20195;&#65292;&#23454;&#29616;&#20102;&#26356;&#21152;&#31526;&#21512;&#20154;&#33041;&#24605;&#32500;&#26041;&#24335;&#30340;&#20195;&#30721;&#32534;&#20889;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#22797;&#26434;&#31639;&#27861;&#30340;&#23454;&#29616;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#30340;&#20570;&#27861;&#36890;&#24120;&#26159;&#20808;&#27010;&#36848;&#19968;&#19979;&#25511;&#21046;&#27969;&#31243;&#65292;&#28982;&#21518;&#36845;&#20195;&#36827;&#34892;&#20016;&#23500;&#65292;&#26368;&#32456;&#29983;&#25104;&#19968;&#20123;&#31934;&#24515;&#21152;&#24037;&#30340;&#35821;&#27861;&#32467;&#26500;&#21644;&#23618;&#27425;&#21464;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#20195;&#30721;&#65292;&#27809;&#26377;&#20013;&#38388;&#29615;&#33410;&#65292;&#20197;&#21453;&#26144;"&#22823;&#32434;&#20808;&#34892;&#65292;&#32454;&#33410;&#21518;&#33267;"&#30340;&#32467;&#26500;&#21270;&#24605;&#32500;&#36807;&#31243;&#12290;&#21463;&#21040;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChainCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#31243;&#24207;&#32508;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36880;&#27493;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#21363;&#20174;&#31895;&#21040;&#32454;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#35821;&#27861;&#26641;&#35299;&#26512;&#23558;&#28304;&#20195;&#30721;&#20998;&#35299;&#20026;&#24067;&#23616;&#26694;&#26550;&#32452;&#20214;&#21644;&#38468;&#20214;&#32452;&#20214;&#65292;&#20197;&#26500;&#24314;&#23618;&#27425;&#34920;&#31034;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#39044;&#27979;&#30446;&#26631;&#37325;&#26032;&#21551;&#21160;&#65292;&#24418;&#25104;&#22810;&#27425;&#36890;&#36807;&#30446;&#26631;&#65292;&#27599;&#27425;&#29983;&#25104;&#19968;&#20010;&#23376;&#24207;&#21015;&#65292;&#36825;&#20123;&#23376;&#24207;&#21015;&#22312;&#23618;&#27425;&#32467;&#26500;&#20013;&#20018;&#32852;&#36215;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;Transformer&#20307;&#31995;&#32467;&#26500;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a complicated algorithm, its implementation by a human programmer usually starts with outlining a rough control flow followed by iterative enrichments, eventually yielding carefully generated syntactic structures and variables in a hierarchy. However, state-of-the-art large language models generate codes in a single pass, without intermediate warm-ups to reflect the structured thought process of "outline-then-detail". Inspired by the recent success of chain-of-thought prompting, we propose ChainCoder, a program synthesis language model that generates Python code progressively, i.e. from coarse to fine in multiple passes. We first decompose source code into layout frame components and accessory components via abstract syntax tree parsing to construct a hierarchical representation. We then reform our prediction target into a multi-pass objective, each pass generates a subsequence, which is concatenated in the hierarchy. Finally, a tailored transformer architecture is leveraged to joi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#31354;&#38388;&#65292;&#24182;&#19988;&#23558;&#28608;&#27963;&#20989;&#25968;&#30340;&#35282;&#33394;&#25551;&#36848;&#20026;&#25918;&#22823;&#20989;&#25968;&#65292;&#23558;&#20302;&#32500;&#32447;&#24615;&#31354;&#38388;&#26144;&#23556;&#20026;&#26080;&#38480;&#32500;&#36229;&#32423;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.00663</link><description>&lt;p&gt;
&#28608;&#27963;&#20989;&#25968;&#19981;&#20877;&#28608;&#27963;&#65306;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#21512;&#29702;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks. (arXiv:2305.00663v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#32500;&#31354;&#38388;&#65292;&#24182;&#19988;&#23558;&#28608;&#27963;&#20989;&#25968;&#30340;&#35282;&#33394;&#25551;&#36848;&#20026;&#25918;&#22823;&#20989;&#25968;&#65292;&#23558;&#20302;&#32500;&#32447;&#24615;&#31354;&#38388;&#26144;&#23556;&#20026;&#26080;&#38480;&#32500;&#36229;&#32423;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#26222;&#36941;&#35748;&#20026;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#19968;&#20010;&#39640;&#32500;&#31354;&#38388;&#65292;&#20294;&#21364;&#19981;&#33021;&#28165;&#26224;&#22320;&#23450;&#20041;&#36825;&#20010;&#31354;&#38388;&#12290;&#37027;&#20040;&#36825;&#20010;&#31354;&#38388;&#26159;&#20160;&#20040;&#65311;&#23427;&#30340;&#32500;&#25968;&#26159;&#22810;&#23569;&#65311;&#26159;&#21542;&#20855;&#26377;&#26377;&#38480;&#30340;&#32500;&#25968;&#65311;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20316;&#29992;&#30340;&#21487;&#34892;&#29702;&#35770;&#65292;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#32500;&#65288;&#26356;&#31934;&#30830;&#22320;&#65292;&#26159;&#19968;&#20010;&#26080;&#38480;&#32500;&#65289;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#35748;&#20026;&#28608;&#27963;&#20989;&#25968;&#20805;&#24403;&#20102;&#19968;&#20010;&#25918;&#22823;&#20989;&#25968;&#30340;&#20316;&#29992;&#65292;&#23558;&#20302;&#32500;&#32447;&#24615;&#31354;&#38388;&#26144;&#23556;&#25104;&#20102;&#19968;&#20010;&#26080;&#38480;&#32500;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers commonly believe that neural networks model a high-dimensional space but cannot give a clear definition of this space. What is this space? What is its dimension? And does it has finite dimensions? In this paper, we develop a plausible theory on interpreting neural networks in terms of the role of activation functions in neural networks and define a high-dimensional (more precisely, an infinite-dimensional) space. We conjunction that the activation function acts as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional space. Given a dataset with each example of $d$ features $f_1$, $f_2$, $\cdots$, $f_d$, we believe that NNs model a special space with infinite dimensions, each of which is a monomial $$\prod_{i_1, i_2, \cdots, i_d} f_1^{i_1} f_2^{i_2} \cdots f_d^{i_d}$$ for some non-negative integers ${i_1, i_2, \cdots, i_d} \in \mathbb{Z}_{0}^{+}=\{0,1,2,3,\ldots\} $. We term such an infinite-dimensional space $\textit{ Super Space (SS)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;</title><link>http://arxiv.org/abs/2305.00543</link><description>&lt;p&gt;
&#20351;&#29992;&#27169;&#31946;&#20998;&#31665;&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#65292;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#24182;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#19982;&#20256;&#32479;&#25351;&#26631;ECE&#30456;&#27604;&#65292;FCE&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;https://github.com/srdgFHE/FCE-paper&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#24448;&#24448;&#20250;&#36807;&#20110;&#33258;&#20449;&#65292;&#20854;&#21407;&#22987;&#32467;&#26524;&#30340;&#27010;&#29575;&#24182;&#19981;&#31526;&#21512;&#30495;&#23454;&#30340;&#20915;&#31574;&#27010;&#29575;&#12290;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#26159;&#23454;&#29616;&#26356;&#21487;&#38752;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20808;&#21069;&#30340;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#20027;&#35201;&#21033;&#29992;&#28165;&#26224;&#30340;&#20998;&#31665;&#25104;&#21592;&#36164;&#26684;&#24230;&#37327;&#12290;&#36825;&#21152;&#21095;&#20102;&#27169;&#22411;&#27010;&#29575;&#30340;&#20559;&#26012;&#65292;&#24182;&#25551;&#32472;&#20102;&#26657;&#20934;&#35823;&#24046;&#30340;&#19981;&#23436;&#25972;&#22270;&#20687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#31946;&#20998;&#31665;&#26041;&#27861;&#35745;&#31639;&#26657;&#20934;&#35823;&#24046;&#30340;&#27169;&#31946;&#26657;&#20934;&#35823;&#24046;&#24230;&#37327;&#65288;FCE&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#32531;&#35299;&#20102;&#27010;&#29575;&#20559;&#26012;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#27979;&#37327;&#26657;&#20934;&#35823;&#24046;&#26102;&#25552;&#20379;&#20102;&#26356;&#32039;&#23494;&#30340;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#25351;&#26631;&#19982;ECE&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#32676;&#20307;&#21644;&#31867;&#21035;&#25104;&#21592;&#36523;&#20221;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;FCE&#22312;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#31867;&#35774;&#32622;&#20013;&#65292;&#32531;&#35299;&#20102;&#27169;&#22411;&#32622;&#20449;&#24230;&#20998;&#25968;&#20559;&#26012;&#23545;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;https://github.com/srdgFHE/FCE-paper&#65292;&#20197;&#20415;&#26410;&#26469;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20351;&#29992;FCE&#36827;&#34892;&#26657;&#20934;&#35823;&#24046;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.00312</link><description>&lt;p&gt;
&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#21046;&#30340;&#22810;&#30446;&#26631;&#32852;&#37030;&#23398;&#20064;&#20013;&#20248;&#21270;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#38544;&#31169;&#27844;&#38706;&#12289;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#31561;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#20248;&#21270;&#21333;&#20010;&#30446;&#26631;&#65292;&#36890;&#24120;&#26159;&#25928;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20351;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#20540;&#24471;&#20449;&#36182;&#65292;&#23427;&#38656;&#35201;&#21516;&#26102;&#28385;&#36275;&#22810;&#20010;/&#22810;&#20010;&#30446;&#26631;&#65292;&#20363;&#22914;&#26368;&#22823;&#21270;&#27169;&#22411;&#24615;&#33021;&#12289;&#26368;&#23567;&#21270;&#38544;&#31169;&#27844;&#38706;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#23545;&#24694;&#24847;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26088;&#22312;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30456;&#20114;&#20914;&#31361;&#30340;&#30446;&#26631;&#65292;&#38750;&#24120;&#36866;&#21512;&#35299;&#20915;&#20540;&#24471;&#20449;&#36182;&#30340;&#32852;&#21512;&#23398;&#20064;&#65288;TFL&#65289;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;MOO&#21644;TFL&#32479;&#19968;&#36215;&#26469;&#65292;&#36890;&#36807;&#21046;&#23450;&#32422;&#26463;&#30340;&#22810;&#30446;&#26631;&#32852;&#21512;&#23398;&#20064;&#65288;CMOFL&#65289;&#38382;&#39064;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#21046;&#23450;&#19979;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#21487;&#20197;&#30452;&#25509;&#36866;&#29992;&#20110;TFL&#12290;&#19981;&#21516;&#20110;&#29616;&#26377;&#30340;CMOFL&#20316;&#21697;&#19987;&#27880;&#20110;&#25928;&#29992;&#12289;&#25928;&#29575;&#12289;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20248;&#21270;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#25928;&#29992;&#25439;&#22833;&#21644;&#35757;&#32451;&#25104;&#26412;&#65292;&#36825;&#26159;TFL&#31995;&#32479;&#30340;&#19977;&#20010;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25913;&#36827;&#30340;CMOFL&#31639;&#27861;&#65292;&#23427;&#20204;&#36820;&#22238;&#19968;&#32452;&#24179;&#34913;&#33391;&#22909;&#30340;&#27169;&#22411;&#65292;&#28385;&#36275;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#12290;&#22522;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2304.13836</link><description>&lt;p&gt;
&#35770;RemOve-And-Retrain&#30340;&#38519;&#38449;&#65306;&#25968;&#25454;&#22788;&#29702;&#19981;&#31561;&#24335;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
On Pitfalls of $\textit{RemOve-And-Retrain}$: Data Processing Inequality Perspective. (arXiv:2304.13836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23646;&#24615;&#21487;&#33021;&#26377;&#26356;&#23569;&#30340;&#26377;&#20851;&#20915;&#31574;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#36825;&#31181;&#20559;&#24046;&#31216;&#20026;&#27611;&#31961;&#24230;&#20559;&#24046;&#65292;&#24182;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#22312;ROAR&#25351;&#26631;&#19978;&#36827;&#34892;&#30450;&#30446;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;RemOve-And-Retrain&#65288;ROAR&#65289;&#21327;&#35758;&#30340;&#21487;&#38752;&#24615;&#65292;&#35813;&#21327;&#35758;&#29992;&#20110;&#27979;&#37327;&#29305;&#24449;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#32972;&#26223;&#21644;&#23454;&#35777;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;&#20855;&#26377;&#36739;&#23569;&#26377;&#20851;&#20915;&#31574;&#21151;&#33021;&#30340;&#20449;&#24687;&#30340;&#23646;&#24615;&#22312;ROAR&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#19982;ROAR&#30340;&#21407;&#22987;&#30446;&#30340;&#30456;&#30683;&#30462;&#12290;&#36825;&#31181;&#29616;&#35937;&#20063;&#20986;&#29616;&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#21464;&#20307;RemOve-And-Debias&#65288;ROAD&#65289;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ROAR&#24402;&#22240;&#24230;&#37327;&#20013;&#27611;&#31961;&#24230;&#20559;&#24046;&#30340;&#19968;&#33268;&#36235;&#21183;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#37266;&#20154;&#20204;&#19981;&#35201;&#30450;&#30446;&#20381;&#36182;ROAR&#30340;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper assesses the reliability of the RemOve-And-Retrain (ROAR) protocol, which is used to measure the performance of feature importance estimates. Our findings from the theoretical background and empirical experiments indicate that attributions that possess less information about the decision function can perform better in ROAR benchmarks, conflicting with the original purpose of ROAR. This phenomenon is also observed in the recently proposed variant RemOve-And-Debias (ROAD), and we propose a consistent trend of blurriness bias in ROAR attribution metrics. Our results caution against uncritical reliance on ROAR metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.13410</link><description>&lt;p&gt;
&#20351;&#29992;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#26469;&#25552;&#39640;&#25932;&#23545;&#36801;&#31227;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Adversarial Transferability by Intermediate-level Perturbation Decay. (arXiv:2304.13410v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#22312;&#36807;&#31243;&#20013;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#38388;&#23618;&#25915;&#20987;&#26159;&#25351;&#36890;&#36807;&#36981;&#24490;&#23545;&#25239;&#26041;&#21521;&#65292;&#23581;&#35797;&#25200;&#21160;&#29305;&#24449;&#34920;&#31034;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#26679;&#26412;&#20013;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#36825;&#31867;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#30001;&#20004;&#20010;&#20998;&#31163;&#30340;&#38454;&#27573;&#26500;&#25104;&#65292;&#39318;&#20808;&#38656;&#35201;&#30830;&#23450;&#19968;&#20010;&#26041;&#21521;&#21521;&#23548;&#65292;&#28982;&#21518;&#25193;&#22823;&#20013;&#38388;&#23618;&#25200;&#21160;&#23545;&#35813;&#26041;&#21521;&#21521;&#23548;&#30340;&#26631;&#37327;&#25237;&#24433;&#12290;&#28982;&#32780;&#65292;&#24471;&#21040;&#30340;&#25200;&#21160;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#38590;&#20813;&#20250;&#20559;&#31163;&#21521;&#23548;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#36825;&#31181;&#20559;&#31163;&#21487;&#33021;&#23548;&#33268;&#27425;&#20248;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;&#20013;&#38388;&#23618;&#25200;&#21160;&#34928;&#20943;&#65288;ILPD&#65289;&#30340;&#26032;&#22411;&#20013;&#38388;&#23618;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38454;&#27573;&#21046;&#20316;&#21487;&#36716;&#31227;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35813;&#26041;&#27861;&#40723;&#21169;&#20013;&#38388;&#23618;&#25200;&#21160;&#22788;&#20110;&#26377;&#25928;&#30340;&#25932;&#23545;&#26041;&#21521;&#65292;&#24182;&#21516;&#26102;&#20855;&#26377;&#24456;&#22823;&#30340;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intermediate-level attacks that attempt to perturb feature representations following an adversarial direction drastically have shown favorable performance in crafting transferable adversarial examples. Existing methods in this category are normally formulated with two separate stages, where a directional guide is required to be determined at first and the scalar projection of the intermediate-level perturbation onto the directional guide is enlarged thereafter. The obtained perturbation deviates from the guide inevitably in the feature space, and it is revealed in this paper that such a deviation may lead to sub-optimal attack. To address this issue, we develop a novel intermediate-level method that crafts adversarial examples within a single stage of optimization. In particular, the proposed method, named intermediate-level perturbation decay (ILPD), encourages the intermediate-level perturbation to be in an effective adversarial direction and to possess a great magnitude simultaneous
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.12322</link><description>&lt;p&gt;
&#20351;&#29992;Pylogik&#36827;&#34892;&#21307;&#23398;&#24433;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12322
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;Python&#26694;&#26550;&#19979;&#30340;&#24211;PyLogik&#26469;&#24110;&#21161;&#36229;&#22768;&#22270;&#20687;&#21435;&#26631;&#35782;&#21270;&#21644;&#28165;&#27927;&#21387;&#32553;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#25552;&#20379;&#22270;&#20687;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#22823;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#35760;&#24405;&#20449;&#24687;&#26041;&#38754;&#39035;&#27880;&#24847;&#65292;&#24517;&#39035;&#28165;&#27927;&#21644;&#21435;&#26631;&#35782;&#21270;&#25968;&#25454;&#12290;&#24403;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#23884;&#20837;&#22312;&#24433;&#20687;&#20803;&#25968;&#25454;&#20013;&#26102;&#65292;&#20419;&#36827;&#22810;&#20013;&#24515;&#21512;&#20316;&#20013;&#25968;&#25454;&#20849;&#20139;&#21644;&#21327;&#35843;&#21464;&#24471;&#23588;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Python&#26694;&#26550;&#19979;&#30340;&#24211;&#65292;&#31216;&#20026;PyLogik&#65292;&#24110;&#21161;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#28165;&#27927;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#22270;&#20687;&#30452;&#25509;&#21253;&#21547;&#24456;&#22810;PHI&#12290;PyLogik&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25991;&#26412;&#26816;&#27979;/&#25552;&#21462;&#12289;&#36807;&#28388;&#12289;&#38408;&#20540;&#21270;&#12289;&#24418;&#24577;&#23398;&#21644;&#36718;&#24275;&#27604;&#36739;&#22788;&#29702;&#22270;&#20687;&#20307;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#21435;&#26631;&#35782;&#21270;&#22270;&#20687;&#65292;&#20943;&#23567;&#25991;&#20214;&#22823;&#23567;&#65292;&#24182;&#20026;&#28145;&#24230;&#23398;&#20064;&#21644;&#25968;&#25454;&#20849;&#20139;&#24212;&#29992;&#20934;&#22791;&#22909;&#20102;&#22270;&#20687;&#25968;&#25454;&#12290;&#20026;&#20102;&#35780;&#20272;PyLogik&#22312;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#30340;&#35782;&#21035;&#26377;&#25928;&#24615;&#65292;&#38543;&#26426;&#25277;&#21462;&#20102;50&#24352;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;&#36229;&#22768;&#24515;&#21160;&#22270;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11042</link><description>&lt;p&gt;
&#26080;&#38656;&#21453;&#21521;&#20256;&#25773;&#30340;&#28145;&#24230;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Backpropagation-free Training of Deep Physical Neural Networks. (arXiv:2304.11042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11042
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#22522;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#35832;&#22914;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#21151;&#12290;&#36825;&#19968;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#65292;&#39044;&#35745;&#20250;&#19981;&#26029;&#22686;&#21152;&#12290;&#36825;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#22686;&#38271;&#20276;&#38543;&#30528;&#19982;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#35757;&#32451;&#12289;&#25512;&#29702;&#38454;&#27573;&#20013;&#30340;&#33021;&#32791;&#31561;&#38382;&#39064;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#38750;&#20256;&#32479;&#29289;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#26469;&#35299;&#20915;&#25512;&#29702;&#38454;&#27573;&#30340;&#33021;&#25928;&#38382;&#39064;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#20173;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#25968;&#23383;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#21453;&#21521;&#20256;&#25773;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#29289;&#29702;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23436;&#20840;&#20102;&#35299;&#25152;&#35859;&#21069;&#21521;&#20256;&#36882;&#30340;&#35745;&#31639;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#31616;&#21333;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the outstanding success of deep learning in various fields such as vision and natural language processing. This success is largely indebted to the massive size of deep learning models that is expected to increase unceasingly. This growth of the deep learning models is accompanied by issues related to their considerable energy consumption, both during the training and inference phases, as well as their scalability. Although a number of work based on unconventional physical systems have been proposed which addresses the issue of energy efficiency in the inference phase, efficient training of deep learning models has remained unaddressed. So far, training of digital deep learning models mainly relies on backpropagation, which is not suitable for physical implementation as it requires perfect knowledge of the computation performed in the so-called forward pass of the neural network. Here, we tackle this issue by proposing a simple deep neural network architectur
&lt;/p&gt;</description></item><item><title>PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2304.10255</link><description>&lt;p&gt;
PED-ANOVA: &#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#37327;&#21270;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA: Efficiently Quantifying Hyperparameter Importance in Arbitrary Subspaces. (arXiv:2304.10255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10255
&lt;/p&gt;
&lt;p&gt;
PED-ANOVA &#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#33021;&#22815;&#22312;&#20219;&#24847;&#23376;&#31354;&#38388;&#20013;&#39640;&#25928;&#22320;&#35745;&#31639;&#36229;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#21161;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#27969;&#34892;&#20351;&#24471;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#23545;&#20110;&#35757;&#32451;&#24378;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#22909;&#30340;&#36229;&#21442;&#25968;&#31354;&#38388;&#35774;&#35745;&#21448;&#20005;&#37325;&#20381;&#36182;&#20110;&#20102;&#35299;&#19981;&#21516;&#36229;&#21442;&#25968;&#30340;&#20316;&#29992;&#12290;&#36825;&#28608;&#21457;&#20102;&#20851;&#20110;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#30740;&#31350;&#65292;&#20363;&#22914;&#20351;&#29992;&#21151;&#33021;&#26041;&#24046;&#20998;&#26512; (f-ANOVA) &#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21407;&#22987;&#30340; f-ANOVA &#20844;&#24335;&#19981;&#36866;&#29992;&#20110;&#31639;&#27861;&#35774;&#35745;&#24072;&#26368;&#30456;&#20851;&#30340;&#23376;&#31354;&#38388;&#65292;&#20363;&#22914;&#30001;&#26368;&#20339;&#24615;&#33021;&#23450;&#20041;&#30340;&#23376;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#20219;&#24847;&#23376;&#31354;&#38388;&#30340; f-ANOVA &#20844;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#20351;&#29992; Pearson &#25955;&#24230; (PED) &#23454;&#29616;&#36229;&#21442;&#25968;&#37325;&#35201;&#24615;&#30340;&#38381;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#26032;&#31639;&#27861;&#65292;&#31216;&#20026; PED-ANOVA&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#35782;&#21035;&#19981;&#21516;&#23376;&#31354;&#38388;&#20013;&#37325;&#35201;&#30340;&#36229;&#21442;&#25968;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26497;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent rise in popularity of Hyperparameter Optimization (HPO) for deep learning has highlighted the role that good hyperparameter (HP) space design can play in training strong models. In turn, designing a good HP space is critically dependent on understanding the role of different HPs. This motivates research on HP Importance (HPI), e.g., with the popular method of functional ANOVA (f-ANOVA). However, the original f-ANOVA formulation is inapplicable to the subspaces most relevant to algorithm designers, such as those defined by top performance. To overcome this problem, we derive a novel formulation of f-ANOVA for arbitrary subspaces and propose an algorithm that uses Pearson divergence (PED) to enable a closed-form computation of HPI. We demonstrate that this new algorithm, dubbed PED-ANOVA, is able to successfully identify important HPs in different subspaces while also being extremely computationally efficient.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#22312;&#25932;&#23545;&#35774;&#32622;&#20013;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36798;&#21040;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2304.05219</link><description>&lt;p&gt;
BanditQ - &#22312;&#25932;&#23545;&#29615;&#22659;&#20013;&#20445;&#35777;&#29992;&#25143;&#27599;&#27425;&#22870;&#21169;&#30340;&#26080;&#24724;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BanditQ -- No-Regret Learning with Guaranteed Per-User Rewards in Adversarial Environments. (arXiv:2304.05219v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05219
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#22312;&#25932;&#23545;&#35774;&#32622;&#20013;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#36798;&#21040;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#30340;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#22914;Hedge&#22312;&#35774;&#35745;&#19978;&#20855;&#26377;&#19981;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#23581;&#35797;&#23613;&#21487;&#33021;&#22810;&#22320;&#29609;&#26368;&#20855;&#22238;&#25253;&#30340;&#33218;&#32780;&#24573;&#30053;&#27425;&#20248;&#33218;&#65292;&#20197;&#23454;&#29616;&#20122;&#32447;&#24615;&#36951;&#25022;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20855;&#26377;&#23545;&#25152;&#26377;&#33218;&#32047;&#31215;&#22870;&#21169;&#36895;&#29575;&#19979;&#30028;&#30340;&#25932;&#23545;&#35774;&#32622;&#20013;&#65292;&#20197;&#20844;&#24179;&#30340;&#22312;&#32447;&#39044;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;&#25490;&#38431;&#35770;&#19982;&#22312;&#32447;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BanditQ&#30340;&#26032;&#30340;&#22312;&#32447;&#39044;&#27979;&#31574;&#30053;&#65292;&#23427;&#22312;&#20840;&#20449;&#24687;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#30446;&#26631;&#36895;&#29575;&#32422;&#26463;&#65292;&#24182;&#23454;&#29616;&#20102;$O(T^{3/4})$&#30340;&#36951;&#25022;&#12290;BanditQ&#30340;&#35774;&#35745;&#21644;&#20998;&#26512;&#28041;&#21450;&#28508;&#22312;&#20989;&#25968;&#26041;&#27861;&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#24182;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classic online prediction algorithms, such as Hedge, are inherently unfair by design, as they try to play the most rewarding arm as many times as possible while ignoring the sub-optimal arms to achieve sublinear regret. In this paper, we consider a fair online prediction problem in the adversarial setting with hard lower bounds on the rate of accrual of rewards for all arms. By combining elementary queueing theory with online learning, we propose a new online prediction policy, called BanditQ, that achieves the target rate constraints while achieving a regret of $O(T^{3/4})$ in the full-information setting. The design and analysis of BanditQ involve a novel use of the potential function method and are of independent interest.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2304.04807</link><description>&lt;p&gt;
&#22312;&#24658;&#26143;&#21464;&#24322;&#23384;&#22312;&#19979;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#27979;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep-learning based measurement of planetary radial velocities in the presence of stellar variability. (arXiv:2304.04807v1 [astro-ph.EP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#65292;&#20026;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24658;&#26143;&#21464;&#24322;&#23384;&#22312;&#19979;&#27979;&#37327;&#23567;&#34892;&#26143;&#30340;&#24452;&#21521;&#36895;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#20943;&#23569;&#19977;&#24180;HARPS-N&#22826;&#38451;-&#26143;&#24418;&#20809;&#35889;&#20013;&#30340;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#25238;&#21160;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#38477;&#32500;&#21644;&#25968;&#25454;&#20998;&#21106;&#26041;&#27861;&#65292;&#20197;&#21450;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#65292;&#21253;&#25324;&#21333;&#32447;CNN&#12289;&#21333;&#32447;CNN&#38598;&#21512;&#21644;&#22810;&#32447;CNN&#12290;&#25105;&#20204;&#23558;&#31867;&#20284;&#20110;&#34892;&#26143;&#30340;&#24452;&#21521;&#36895;&#24230;&#27880;&#20837;&#20809;&#35889;&#20013;&#24182;&#20351;&#29992;&#32593;&#32476;&#24674;&#22797;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22810;&#32447;CNN&#33021;&#22815;&#24674;&#22797;0.2m/s&#21322;&#25391;&#24133;&#12289;50&#22825;&#21608;&#26399;&#30340;&#34892;&#26143;&#65292;&#20854;&#25391;&#24133;&#35823;&#24046;&#20026;8.8&#65285;&#65292;&#21608;&#26399;&#35823;&#24046;&#20026;0.7&#65285;&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#32531;&#35299;&#24658;&#26143;&#24452;&#21521;&#36895;&#24230;&#21464;&#24322;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#20197;&#21069;&#26080;&#27861;&#24819;&#35937;&#30340;&#23567;&#34892;&#26143;&#24452;&#21521;&#36895;&#24230;&#26816;&#27979;&#31934;&#24230;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep-learning based approach for measuring small planetary radial velocities in the presence of stellar variability. We use neural networks to reduce stellar RV jitter in three years of HARPS-N sun-as-a-star spectra. We develop and compare dimensionality-reduction and data splitting methods, as well as various neural network architectures including single line CNNs, an ensemble of single line CNNs, and a multi-line CNN. We inject planet-like RVs into the spectra and use the network to recover them. We find that the multi-line CNN is able to recover planets with 0.2 m/s semi-amplitude, 50 day period, with 8.8% error in the amplitude and 0.7% in the period. This approach shows promise for mitigating stellar RV variability and enabling the detection of small planetary RVs with unprecedented precision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17523</link><description>&lt;p&gt;
&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#25552;&#39640;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#35299;&#20915;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#21033;&#29992;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#39044;&#27979;&#37327;&#23376;&#30005;&#36335;&#30340;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#24050;&#36827;&#20837;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#26102;&#20195;&#65292;&#30446;&#21069;&#25105;&#20204;&#25317;&#26377;&#30340;&#37327;&#23376;&#22788;&#29702;&#22120;&#23545;&#36752;&#23556;&#21644;&#28201;&#24230;&#31561;&#29615;&#22659;&#21464;&#37327;&#25935;&#24863;&#65292;&#22240;&#27492;&#20250;&#20135;&#29983;&#22024;&#26434;&#30340;&#36755;&#20986;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#35768;&#22810;&#31639;&#27861;&#21644;&#24212;&#29992;&#31243;&#24207;&#29992;&#20110;NISQ&#22788;&#29702;&#22120;&#65292;&#20294;&#25105;&#20204;&#20173;&#38754;&#20020;&#30528;&#35299;&#37322;&#20854;&#22024;&#26434;&#32467;&#26524;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#25152;&#36873;&#25321;&#30340;&#37327;&#23376;&#24577;&#26377;&#22810;&#23569;&#20449;&#24515;&#65311;&#36825;&#31181;&#20449;&#24515;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;NISQ&#35745;&#31639;&#26426;&#23558;&#36755;&#20986;&#20854;&#37327;&#23376;&#20301;&#27979;&#37327;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#26377;&#26102;&#24456;&#38590;&#21306;&#20998;&#20998;&#24067;&#26159;&#21542;&#34920;&#31034;&#26377;&#24847;&#20041;&#30340;&#35745;&#31639;&#25110;&#21482;&#26159;&#38543;&#26426;&#22122;&#22768;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23558;&#37327;&#23376;&#30005;&#36335;&#20445;&#30495;&#24230;&#39044;&#27979;&#26694;&#26550;&#20026;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#38382;&#39064;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#19968;&#20010;&#23436;&#25972;&#30340;&#24037;&#20316;&#27969;&#31243;&#26469;&#26500;&#24314;&#35757;&#32451;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.15477</link><description>&lt;p&gt;
&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#19978;&#30340;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#40654;&#26364;&#24230;&#37327;&#26469;&#25913;&#36827;SPD&#31070;&#32463;&#32593;&#32476;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#24230;&#37327;&#33021;&#20351;&#32593;&#32476;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#20869;&#22312;&#33021;&#22815;&#32534;&#30721;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#30456;&#20851;&#24615;&#65292;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#30697;&#38453;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#21453;&#26144;SPD&#27969;&#24418;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25104;&#21151;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22266;&#23450;&#24230;&#37327;&#24352;&#37327;&#21487;&#33021;&#20250;&#23548;&#33268;SPD&#30697;&#38453;&#23398;&#20064;&#30340;&#27425;&#20248;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;SPD&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#21033;&#29992;&#25289;&#22238;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;SPD&#27969;&#24418;&#30340;&#40654;&#26364;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#24230;&#37327;&#25552;&#20986;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#12290;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37197;&#22791;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24230;&#37327;&#30340;SPD&#32593;&#32476;&#21487;&#20197;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DGNet-Bio&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DGNet-Bio&#22312;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11338</link><description>&lt;p&gt;
&#38754;&#21521;&#24515;&#30005;&#22270;&#21644;&#33041;&#30005;&#22270;&#20998;&#31867;&#30340;&#39046;&#22495;&#27867;&#21270;&#65306;&#31639;&#27861;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks. (arXiv:2303.11338v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#19987;&#38376;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;DGNet-Bio&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;DGNet-Bio&#22312;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#21644;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#33021;&#22815;&#22312;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#29282;&#22266;&#22320;&#30830;&#31435;&#33258;&#24049;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#65292;&#24403;&#27169;&#22411;&#38754;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#26679;&#26412;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#36825;&#23601;&#26159;&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#20010;&#35780;&#20272;DG&#31639;&#27861;&#30340;&#22522;&#20934;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#26469;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#20013;&#30340;DG&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#29983;&#29289;&#20449;&#21495;&#30340;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#65292;&#24182;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#24320;&#28304;&#29983;&#29289;&#20449;&#21495;&#39046;&#22495;&#27867;&#21270;&#35780;&#20272;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#26368;&#20808;&#36827;DG&#31639;&#27861;&#25913;&#36827;&#20026;1D&#29983;&#29289;&#20449;&#21495;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;DGNet-Bio&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#29983;&#29289;&#20449;&#21495;&#20013;&#30340;DG&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DGNet-Bio&#22312;&#26032;&#25552;&#20986;&#30340;ECG&#21644;EEG&#20998;&#31867;&#39046;&#22495;&#27867;&#21270;&#22522;&#20934;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their immense success in numerous fields, machine and deep learning systems have not have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#24310;&#36831;&#23884;&#20837;&#29702;&#35770;&#21644;&#24191;&#20041;&#23884;&#20837;&#29702;&#35770;&#65292;&#20005;&#35880;&#35777;&#26126;&#20102;RC&#26412;&#36136;&#19978;&#26159;&#21407;&#22987;&#36755;&#20837;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#39640;&#32500;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#26102;&#38388;&#24310;&#36831;&#21644;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#20102;&#27946;&#27867;&#35745;&#31639;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#27604;&#20840;&#23610;&#23544;RC&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09042</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#29702;&#35770;&#22312;&#27946;&#27867;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#21450;&#21033;&#29992;&#26102;&#38388;&#24310;&#36831;&#20943;&#23569;&#27946;&#27867;&#32593;&#32476;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Embedding Theory of Reservoir Computing and Reducing Reservoir Network Using Time Delays. (arXiv:2303.09042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#24310;&#36831;&#23884;&#20837;&#29702;&#35770;&#21644;&#24191;&#20041;&#23884;&#20837;&#29702;&#35770;&#65292;&#20005;&#35880;&#35777;&#26126;&#20102;RC&#26412;&#36136;&#19978;&#26159;&#21407;&#22987;&#36755;&#20837;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#39640;&#32500;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#26102;&#38388;&#24310;&#36831;&#21644;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#26174;&#30528;&#20943;&#23567;&#20102;&#27946;&#27867;&#35745;&#31639;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#27604;&#20840;&#23610;&#23544;RC&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27946;&#27867;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#30001;&#20110;&#22312;&#37325;&#26500;&#25110;/&#21644;&#39044;&#27979;&#22797;&#26434;&#29289;&#29702;&#31995;&#32479;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#21151;&#25928;&#21644;&#39640;&#24615;&#33021;&#65292;&#22240;&#27492;&#27491;&#22312;&#29190;&#28856;&#24615;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35302;&#21457;RC&#22914;&#27492;&#26377;&#25928;&#24212;&#29992;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#65292;&#38656;&#35201;&#28145;&#20837;&#32780;&#31995;&#32479;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#32467;&#21512;&#24310;&#36831;&#23884;&#20837;&#29702;&#35770;&#21644;&#24191;&#20041;&#23884;&#20837;&#29702;&#35770;&#65292;&#20005;&#35880;&#35777;&#26126;&#20102;RC&#26412;&#36136;&#19978;&#26159;&#21407;&#22987;&#36755;&#20837;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#30340;&#39640;&#32500;&#23884;&#20837;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#36825;&#31181;&#23884;&#20837;&#29305;&#24615;&#65292;&#25105;&#20204;&#23558;&#26631;&#20934;RC&#21644;&#26102;&#38388;&#24310;&#36831;RC&#32479;&#19968;&#21040;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#25105;&#20204;&#23545;&#32593;&#32476;&#30340;&#36755;&#20986;&#23618;&#20165;&#24341;&#20837;&#26102;&#38388;&#24310;&#36831;&#65292;&#36827;&#19968;&#27493;&#21457;&#29616;&#20102;&#26102;&#38388;&#24310;&#36831;&#21644;&#32593;&#32476;&#31070;&#32463;&#20803;&#25968;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#26174;&#30528;&#20943;&#23567;&#20102;&#27946;&#27867;&#35745;&#31639;&#32593;&#32476;&#30340;&#22823;&#23567;&#65292;&#29992;&#20110;&#37325;&#26500;&#21644;&#39044;&#27979;&#19968;&#20123;&#20195;&#34920;&#24615;&#30340;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#19988;&#26356;&#35753;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23454;&#29616;&#20102;&#27604;&#20840;&#23610;&#23544;RC&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir computing (RC), a particular form of recurrent neural network, is under explosive development due to its exceptional efficacy and high performance in reconstruction or/and prediction of complex physical systems. However, the mechanism triggering such effective applications of RC is still unclear, awaiting deep and systematic exploration. Here, combining the delayed embedding theory with the generalized embedding theory, we rigorously prove that RC is essentially a high dimensional embedding of the original input nonlinear dynamical system. Thus, using this embedding property, we unify into a universal framework the standard RC and the time-delayed RC where we novelly introduce time delays only into the network's output layer, and we further find a trade-off relation between the time delays and the number of neurons in RC. Based on this finding, we significantly reduce the network size of RC for reconstructing and predicting some representative physical systems, and, more surp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;PnP&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#36739;&#36731;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.07271</link><description>&lt;p&gt;
&#21487;&#35777;&#25910;&#25947;&#30340;&#21363;&#25554;&#21363;&#29992;&#25311;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;PnP&#26041;&#27861;&#65292;&#20351;&#29992;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#36739;&#36731;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#25554;&#21363;&#29992;&#65288;PnP&#65289;&#26041;&#27861;&#26159;&#19968;&#31867;&#39640;&#25928;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;ISTA&#25110;ADMM&#65289;&#65292;&#23558;&#25968;&#25454;&#20445;&#30495;&#24230;&#39033;&#21644;&#28145;&#24230;&#21435;&#22122;&#22120;&#30456;&#32467;&#21512;&#12290;&#29616;&#26377;&#30340;&#21487;&#35777;&#26126;&#30340;PnP&#26041;&#27861;&#23545;&#21435;&#22122;&#22120;&#25110;&#20445;&#30495;&#24230;&#20989;&#25968;&#26045;&#21152;&#20102;&#20005;&#26684;&#30340;&#38480;&#21046;&#65292;&#22914;&#38750;&#25193;&#24352;&#24615;&#25110;&#20005;&#26684;&#20984;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#30340;PnP&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#36817;&#31471;&#21435;&#22122;&#22120;&#26045;&#21152;&#30456;&#23545;&#36739;&#36731;&#30340;&#26465;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#25311;&#29275;&#39039;&#27493;&#39588;&#20197;&#22823;&#22823;&#21152;&#36895;&#25910;&#25947;&#12290;&#36890;&#36807;&#23558;&#28145;&#24230;&#21435;&#22122;&#22120;&#29305;&#21035;&#21442;&#25968;&#21270;&#20026;&#26799;&#24230;&#27493;&#39588;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25311;&#29275;&#39039;PnP&#31639;&#27861;&#30340;&#22266;&#23450;&#28857;&#34920;&#24449;&#20026;&#21487;&#33021;&#38750;&#20984;&#20989;&#25968;&#30340;&#20020;&#30028;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2302.07260</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20808;&#39564;&#32593;&#32476;&#30340;&#39640;&#32500;&#36755;&#20986;&#21487;&#25193;&#23637;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#39640;&#32500;&#36755;&#20986;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#21487;&#26377;&#25928;&#22320;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#39640;&#32500;&#24230;&#21521;&#37327;&#31354;&#38388;&#25110;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#20013;&#20063;&#33021;&#36817;&#20284;&#21151;&#33021;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#30340;&#19968;&#20123;&#22522;&#26412;&#38382;&#39064;&#28041;&#21450;&#21040;&#26410;&#30693;&#30340;&#39640;&#32500;&#24230;&#26144;&#23556;&#19968;&#32452;&#21487;&#25511;&#21464;&#37327;&#21040;&#26114;&#36149;&#23454;&#39564;&#32467;&#26524;&#30340;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#20219;&#21153;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#30446;&#26631;&#20989;&#25968;&#35780;&#20272;&#26102;&#22788;&#29702;&#20840;&#23616;&#20248;&#21270;&#38382;&#39064;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24403;&#22788;&#29702;&#39640;&#32500;&#36755;&#20986;&#26102;&#65292;&#20854;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#32500;&#24230;&#20027;&#35201;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24102;&#38543;&#26426;&#20808;&#39564;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#20030;&#38598;&#25104;&#30340;BO&#21644;&#24207;&#36143;&#20915;&#31574;&#21046;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#20351;&#29992;&#36866;&#24403;&#30340;&#20307;&#31995;&#32467;&#26500;&#36873;&#25321;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#36817;&#20284;&#35774;&#35745;&#21464;&#37327;&#21644;&#24863;&#20852;&#36259;&#37327;&#20043;&#38388;&#30340;&#21151;&#33021;&#20851;&#31995;&#65292;&#21363;&#20351;&#22312;&#21518;&#32773;&#21462;&#20540;&#20110;&#39640;&#32500;&#21521;&#37327;&#31354;&#38388;&#25110;&#29978;&#33267;&#26080;&#38480;&#32500;&#20989;&#25968;&#31354;&#38388;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32972;&#26223;&#19979;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22788;&#29702;&#39640;&#32500;&#24230;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several fundamental problems in science and engineering consist of global optimization tasks involving unknown high-dimensional (black-box) functions that map a set of controllable variables to the outcomes of an expensive experiment. Bayesian Optimization (BO) techniques are known to be effective in tackling global optimization problems using a relatively small number objective function evaluations, but their performance suffers when dealing with high-dimensional outputs. To overcome the major challenge of dimensionality, here we propose a deep learning framework for BO and sequential decision making based on bootstrapped ensembles of neural architectures with randomized priors. Using appropriate architecture choices, we show that the proposed framework can approximate functional relationships between design variables and quantities of interest, even in cases where the latter take values in high-dimensional vector spaces or even infinite-dimensional function spaces. In the context of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.06675</link><description>&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#30340;&#31526;&#21495;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21457;&#29616;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#25928;&#25628;&#32034;&#25216;&#26415;&#26469;&#25506;&#32034;&#26080;&#38480;&#21644;&#31232;&#30095;&#30340;&#31243;&#24207;&#31354;&#38388;&#12290;&#20026;&#20102;&#22635;&#34917;&#20195;&#29702;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#24040;&#22823;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31243;&#24207;&#36873;&#25321;&#21644;&#31616;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;$ \textbf {Lion} $&#65288;$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $&#65289;&#12290;&#23427;&#30340;&#35760;&#24518;&#25928;&#29575;&#27604;Adam&#26356;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#36319;&#36394;&#21160;&#37327;&#12290;&#19982;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#19981;&#21516;&#65292;&#36890;&#36807;&#31526;&#21495;&#36816;&#31639;&#35745;&#31639;&#30340;&#27599;&#20010;&#21442;&#25968;&#30340;&#26356;&#26032;&#20855;&#26377;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#23558;Lion&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#21644;Adafactor&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;Lion&#23558;&#22312;ImageNet&#19978;ViT&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;2&#65285;&#65292;&#24182;&#33410;&#30465;&#20102;&#22810;&#36798;5&#20493;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#22312;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;FCL&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06089</link><description>&lt;p&gt;
&#38754;&#21521;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Federated contrastive learning models for prostate cancer diagnosis and Gleason grading. (arXiv:2302.06089v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#22312;&#21069;&#21015;&#33146;&#30284;&#35786;&#26029;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;FCL&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#25928;&#26524;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#31283;&#20581;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#25968;&#25454;&#25910;&#38598;&#38754;&#20020;&#27807;&#36890;&#12289;&#20262;&#29702;&#21644;&#38544;&#31169;&#20445;&#25252;&#31561;&#38480;&#21046;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#21327;&#35843;&#22810;&#20010;&#23458;&#25143;&#31471;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#22823;&#35268;&#27169;&#30149;&#29702;&#22270;&#20687;&#21644;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#32852;&#37030;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;FCL&#65289;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26412;&#22320;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#27169;&#22411;&#38388;&#30340;&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#32531;&#35299;&#21442;&#25968;&#20256;&#36755;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#24182;&#39564;&#35777;FCL&#30340;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#36890;&#36807;&#28155;&#21152;&#22122;&#38899;&#36827;&#19968;&#27493;&#20445;&#25252;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;19,635&#20010;&#26469;&#33258;&#22810;&#20010;&#23458;&#25143;&#31471;&#30340;&#21069;&#21015;&#33146;&#30284;WSI&#19978;&#35780;&#20272;&#20102;FCL&#22312;&#30284;&#30151;&#35786;&#26029;&#20219;&#21153;&#21644;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#35786;&#26029;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21306;&#20998;&#30284;&#24615;&#21644;&#38750;&#30284;&#24615;WSI&#30340;0.99&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#12290;&#22312;&#26684;&#37324;&#26862;&#20998;&#32423;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;FCL&#27169;&#22411;&#23454;&#29616;&#20102;&#19968;&#20010;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20026;0.143&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#38477;&#20302;&#20102;21.7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application effect of artificial intelligence (AI) in the field of medical imaging is remarkable. Robust AI model training requires large datasets, but data collection faces communication, ethics, and privacy protection constraints. Fortunately, federated learning can solve the above problems by coordinating multiple clients to train the model without sharing the original data. In this study, we design a federated contrastive learning framework (FCL) for large-scale pathology images and the heterogeneity challenges. It enhances the model's generalization ability by maximizing the attention consistency between the local client and server models. To alleviate the privacy leakage problem when transferring parameters and verify the robustness of FCL, we use differential privacy to further protect the model by adding noise. We evaluate the effectiveness of FCL on the cancer diagnosis task and Gleason grading task on 19,635 prostate cancer WSIs from multiple clients. In the diagnosis tas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.01798</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35299;&#37322;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Interpretations of Domain Adaptations via Layer Variational Analysis. (arXiv:2302.01798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study establishes the theory of transfer learning in deep learning through formal derivations and heuristic analysis, proving that the success of transfer learning can be guaranteed with corresponding data conditions. An alternative method for network-based transfer learning is proposed, which shows an increase in efficiency and accuracy for domain adaptation.
&lt;/p&gt;
&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#24615;&#33021;&#65292;&#20294;&#26377;&#38480;&#30340;&#25991;&#29486;&#25253;&#36947;&#20102;&#20854;&#32972;&#21518;&#30340;&#26426;&#21046;&#12290;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#27491;&#24335;&#30340;&#25512;&#23548;&#21644;&#21551;&#21457;&#24335;&#20998;&#26512;&#65292;&#20197;&#21046;&#23450;&#28145;&#24230;&#23398;&#20064;&#20013;&#36716;&#31227;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#23618;&#21464;&#20998;&#20998;&#26512;&#35777;&#26126;&#20102;&#36716;&#31227;&#23398;&#20064;&#30340;&#25104;&#21151;&#21487;&#20197;&#36890;&#36807;&#30456;&#24212;&#30340;&#25968;&#25454;&#26465;&#20214;&#24471;&#21040;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#35745;&#31639;&#20135;&#29983;&#20102;&#23545;&#30693;&#35782;&#36716;&#31227;&#36807;&#31243;&#30340;&#30452;&#35266;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#36716;&#31227;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#30340;&#25552;&#39640;&#12290;&#24403;&#36866;&#24212;&#26399;&#38388;&#30340;&#26032;&#39046;&#22495;&#25968;&#25454;&#36275;&#22815;&#31232;&#30095;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#20248;&#21183;&#12290;&#23545;&#21508;&#31181;&#20219;&#21153;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#27604;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning is known to perform efficiently in many applications empirically, yet limited literature reports the mechanism behind the scene. This study establishes both formal derivations and heuristic analysis to formulate the theory of transfer learning in deep learning. Our framework utilizing layer variational analysis proves that the success of transfer learning can be guaranteed with corresponding data conditions. Moreover, our theoretical calculation yields intuitive interpretations towards the knowledge transfer process. Subsequently, an alternative method for network-based transfer learning is derived. The method shows an increase in efficiency and accuracy for domain adaptation. It is particularly advantageous when new domain data is sufficiently sparse during adaptation. Numerical experiments over diverse tasks validated our theory and verified that our analytic expression achieved better performance in domain adaptation than the gradient descent method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;P-&#23398;&#20064;&#22120;&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22788;&#29702;&#25928;&#26524;&#24322;&#36136;&#24615;&#30340;&#23450;&#21046;&#20004;&#38454;&#27573;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#20381;&#38752;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.10913</link><description>&lt;p&gt;
&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;&#30340;&#36817;&#22240;&#22240;&#26524;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Learning of Conditional Average Treatment Effects. (arXiv:2301.10913v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;P-&#23398;&#20064;&#22120;&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#22788;&#29702;&#25928;&#26524;&#24322;&#36136;&#24615;&#30340;&#23450;&#21046;&#20004;&#38454;&#27573;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#20381;&#38752;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#28789;&#27963;&#22320;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#30340;&#24322;&#36136;&#24615;&#26159;&#20174;&#21307;&#23398;&#21040;&#24066;&#22330;&#31561;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#26377;&#35768;&#22810;&#26377;&#21069;&#36884;&#30340;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20272;&#35745;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#27979;&#37327;&#21040;&#30340;&#21327;&#21464;&#37327;&#36275;&#20197;&#35777;&#26126;&#26465;&#20214;&#20132;&#25442;&#24615;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;P-&#23398;&#20064;&#22120;&#65292;&#21463;&#21040;R-&#23398;&#20064;&#22120;&#21644;DR-&#23398;&#20064;&#22120;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#22312;&#35266;&#23519;&#21040;&#30340;&#21327;&#21464;&#37327;&#32473;&#23450;&#21487;&#20132;&#25442;&#24615;&#26159;&#19981;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#24322;&#36136;&#24615;&#22788;&#29702;&#25928;&#26524;&#30340;&#23450;&#21046;&#20004;&#38454;&#27573;&#25439;&#22833;&#20989;&#25968;&#65292;&#24182;&#24076;&#26395;&#20381;&#38752;&#20195;&#29702;&#21464;&#37327;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#21487;&#20197;&#36890;&#36807;&#29616;&#25104;&#30340;&#25439;&#22833;&#26368;&#23567;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#65292;&#22312;&#26680;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#65292;&#21482;&#35201;&#22952;&#30861;&#32452;&#20214;&#24471;&#21040;&#21512;&#29702;&#30340;&#20272;&#35745;&#65292;&#23601;&#33021;&#28385;&#36275;&#20272;&#35745;&#35823;&#24046;&#30340;oracle&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently and flexibly estimating treatment effect heterogeneity is an important task in a wide variety of settings ranging from medicine to marketing, and there are a considerable number of promising conditional average treatment effect estimators currently available. These, however, typically rely on the assumption that the measured covariates are enough to justify conditional exchangeability. We propose the P-learner, motivated by the R- and DR-learner, a tailored two-stage loss function for learning heterogeneous treatment effects in settings where exchangeability given observed covariates is an implausible assumption, and we wish to rely on proxy variables for causal inference. Our proposed estimator can be implemented by off-the-shelf loss-minimizing machine learning methods, which in the case of kernel regression satisfies an oracle bound on the estimated error as long as the nuisance components are estimated reasonably well.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.07068</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#23433;&#20840;&#36755;&#20837;&#35745;&#25968;&#30340;#DNN-Verification&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21487;&#20197;&#29992;&#26469;&#26816;&#26597;DNN&#26159;&#21542;&#19981;&#23433;&#20840;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#19981;&#23433;&#20840;&#30340;&#36755;&#20837;&#37197;&#32622;&#65292;&#20294;&#23427;&#20204;&#30340;&#26159;/&#21542;&#36755;&#20986;&#23545;&#20110;&#20854;&#20182;&#30446;&#30340;&#65288;&#22914;&#23631;&#34109;&#12289;&#27169;&#22411;&#36873;&#25321;&#25110;&#22521;&#35757;&#25913;&#36827;&#65289;&#30340;&#20449;&#24687;&#19981;&#36275;&#22815;&#35814;&#32454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#35745;&#31639;&#23548;&#33268;DNN&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#36820;&#22238;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#30340;#P&#23436;&#22791;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#27491;&#30830;&#35745;&#25968;&#30340;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#21576;&#29616;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20551;&#35774;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#21644;&#20027;&#21160;&#23398;&#20064;&#20803;&#32032;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21270;&#23398;&#31354;&#38388;&#20013;&#24555;&#36895;&#25506;&#32034;&#21644;&#39044;&#27979;&#20998;&#23376;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2301.02665</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20551;&#35774;&#30340;&#20027;&#21160;&#23398;&#20064;&#65292;&#25506;&#32034;&#20998;&#23376;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Discovery of structure-property relations for molecules via hypothesis-driven active learning over the chemical space. (arXiv:2301.02665v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02665
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20551;&#35774;&#23398;&#20064;&#25216;&#26415;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#21644;&#20027;&#21160;&#23398;&#20064;&#20803;&#32032;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21270;&#23398;&#31354;&#38388;&#20013;&#24555;&#36895;&#25506;&#32034;&#21644;&#39044;&#27979;&#20998;&#23376;&#30340;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#12289;&#29983;&#29289;&#20998;&#23376;&#31995;&#32479;&#12289;&#20652;&#21270;&#21058;&#12289;&#20809;&#20239;&#12289;&#26377;&#26426;&#30005;&#23376;&#21644;&#30005;&#27744;&#30340;&#20998;&#23376;&#20505;&#36873;&#32773;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#24555;&#36895;&#25506;&#32034;&#21270;&#23398;&#31354;&#38388;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#38024;&#23545;&#25152;&#38656;&#30340;&#21151;&#33021;&#36827;&#34892;&#30446;&#26631;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20551;&#35774;&#23398;&#20064;&#30340;&#21270;&#23398;&#31354;&#38388;&#20027;&#21160;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20851;&#20110;&#24863;&#20852;&#36259;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#20043;&#38388;&#21487;&#33021;&#20851;&#31995;&#30340;&#20551;&#35774;&#65292;&#22522;&#20110;&#23569;&#37327;&#30340;&#25968;&#25454;&#65292;&#24182;&#23558;&#23427;&#20204;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#20013;&#20316;&#20026;&#65288;&#27010;&#29575;&#65289;&#22343;&#20540;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65288;&#22914;SISSO&#65289;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#20803;&#32032;&#32467;&#21512;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#22312;&#20027;&#21160;&#23398;&#20064;&#29615;&#22659;&#20013;&#36924;&#36817;&#29289;&#29702;&#23450;&#24459;&#65292;&#20197;&#23454;&#29616;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#20256;&#32479;&#30340;&#20445;&#30041;&#38598;&#35780;&#20272;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovery of the molecular candidates for applications in drug targets, biomolecular systems, catalysts, photovoltaics, organic electronics, and batteries, necessitates development of machine learning algorithms capable of rapid exploration of the chemical spaces targeting the desired functionalities. Here we introduce a novel approach for the active learning over the chemical spaces based on hypothesis learning. We construct the hypotheses on the possible relationships between structures and functionalities of interest based on a small subset of data and introduce them as (probabilistic) mean functions for the Gaussian process. This approach combines the elements from the symbolic regression methods such as SISSO and active learning into a single framework. The primary focus of constructing this framework is to approximate physical laws in an active learning regime toward a more robust predictive performance, as traditional evaluation on hold-out sets in machine learning doesn't accou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30830;&#23450;&#24182;&#34920;&#24449;&#20102;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#65292;&#21516;&#26102;&#20063;&#24341;&#39046;&#20102;&#23545;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26041;&#27861;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.13881</link><description>&lt;p&gt;
&#28145;&#24230;&#20840;&#36830;&#25509;&#32593;&#32476;&#21644;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26426;&#22120;&#30340;&#29305;&#24449;&#23398;&#20064;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. (arXiv:2212.13881v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30830;&#23450;&#24182;&#34920;&#24449;&#20102;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#24182;&#35299;&#37322;&#20102;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#65292;&#21516;&#26102;&#20063;&#24341;&#39046;&#20102;&#23545;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26041;&#27861;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#25216;&#26415;&#21644;&#31185;&#23398;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#33258;&#21160;&#36873;&#25321;&#29992;&#20110;&#39044;&#27979;&#30340;&#29305;&#24449;&#25110;&#25968;&#25454;&#27169;&#24335;&#30340;&#26426;&#21046;&#20173;&#19981;&#28165;&#26970;&#12290;&#30830;&#23450;&#36825;&#26679;&#30340;&#26426;&#21046;&#26159;&#25512;&#21160;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#20419;&#36827;&#36825;&#20123;&#27169;&#22411;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#21487;&#38752;&#37319;&#29992;&#30340;&#20851;&#38190;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#34920;&#24449;&#20102;&#28145;&#24230;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#29305;&#24449;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#31070;&#32463;&#29305;&#24449;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#34920;&#26126;&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#26159;&#36890;&#36807;&#23454;&#29616;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#26469;&#21152;&#24378;&#19982;&#27169;&#22411;&#36755;&#20986;&#23494;&#20999;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#25581;&#31034;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#29616;&#35937;&#65292;&#21253;&#25324;&#20551;&#29305;&#24449;&#30340;&#20986;&#29616;&#21644;&#31616;&#21333;&#24615;&#20559;&#24046;&#20197;&#21450;&#22914;&#20309;&#20462;&#21098;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#12298;&#24425;&#31080;&#20551;&#35774;&#12299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#26426;&#21046;&#20063;&#24341;&#39046;&#20102;&#23545;&#36882;&#24402;&#23398;&#20064;&#29305;&#24449;&#30340;&#26680;&#26041;&#27861;&#20013;&#29305;&#24449;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years neural networks have achieved impressive results on many technological and scientific tasks. Yet, the mechanism through which these models automatically select features, or patterns in data, for prediction remains unclear. Identifying such a mechanism is key to advancing performance and interpretability of neural networks and promoting reliable adoption of these models in scientific applications. In this paper, we identify and characterize the mechanism through which deep fully connected neural networks learn features. We posit the Deep Neural Feature Ansatz, which states that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output. Our ansatz sheds light on various deep learning phenomena including emergence of spurious features and simplicity biases and how pruning networks can increase performance, the "lottery ticket hypothesis." Moreover, the mechanism identified in our work leads to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#36335;&#30001;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;BiGS&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;BERT&#39044;&#35757;&#32451;&#20934;&#30830;&#24230;&#30456;&#24403;&#30340;GLUE&#27979;&#35797;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2212.10544</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#36335;&#30001;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;BiGS&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;BERT&#39044;&#35757;&#32451;&#20934;&#30830;&#24230;&#30456;&#24403;&#30340;GLUE&#27979;&#35797;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;Transformer&#27169;&#22411;&#26159;&#39044;&#35757;&#32451;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#34429;&#28982;&#20063;&#26377;&#20854;&#20182;&#26550;&#26500;&#34987;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#20294;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#35201;&#20040;&#26174;&#33879;&#19979;&#38477;&#65292;&#35201;&#20040;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;&#25165;&#33021;&#36798;&#21040;&#26631;&#20934;&#27979;&#35797;&#30340;&#22522;&#20934;&#65288;&#22914;GLUE&#65289;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#26368;&#36817;&#22312;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#24207;&#21015;&#36335;&#30001;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Bidirectional Gated SSM&#65288;BiGS&#65289;&#32467;&#21512;&#20102;SSM&#23618;&#21644;&#20056;&#24615;&#38376;&#25511;&#26550;&#26500;&#65292;&#36825;&#22312;&#31616;&#21270;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#19981;&#32771;&#34385;&#25104;&#23545;&#20132;&#20114;&#30340;&#38745;&#24577;&#23618;&#12290;&#21363;&#20351;&#22914;&#27492;&#65292;BiGS&#33021;&#22815;&#36798;&#21040;&#19982;BERT&#39044;&#35757;&#32451;&#20934;&#30830;&#24230;&#30456;&#24403;&#30340;GLUE&#27979;&#35797;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36817;&#20284;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;4096&#20010;&#26631;&#35760;&#30340;&#38271;&#24418;&#24335;&#39044;&#35757;&#32451;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20294;&#19982;BERT&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20132;&#20114;&#21644;&#21477;&#27861;&#34920;&#31034;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#25152;&#26377;&#27169;&#22411;&#21487;&#22312; https://git &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.16715</link><description>&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Policy Optimization over General State and Action Spaces. (arXiv:2211.16715v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#20989;&#25968;&#36817;&#20284;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#21452;&#24179;&#22343;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#37117;&#21487;&#20197;&#24212;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#30340;RL&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#38236;&#20687;&#19979;&#38477;&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36890;&#36807;Hopfield&#27169;&#22411;&#20316;&#20026;&#21407;&#22411;&#65292;&#25104;&#21151;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21487;&#20197;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15880</link><description>&lt;p&gt;
Hopfield&#27169;&#22411;&#30340;&#38236;&#20687;&#19979;&#38477;&#20248;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Mirror descent of Hopfield model. (arXiv:2211.15880v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#38236;&#20687;&#19979;&#38477;&#25216;&#26415;&#26469;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36890;&#36807;Hopfield&#27169;&#22411;&#20316;&#20026;&#21407;&#22411;&#65292;&#25104;&#21151;&#35757;&#32451;&#27169;&#22411;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21487;&#20197;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38236;&#20687;&#19979;&#38477;&#26159;&#19968;&#31181;&#20248;&#38597;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#21442;&#25968;&#27169;&#22411;&#30340;&#23545;&#20598;&#31354;&#38388;&#26469;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#12290;&#34429;&#28982;&#26368;&#21021;&#26159;&#20026;&#20984;&#20248;&#21270;&#32780;&#24320;&#21457;&#30340;&#65292;&#20294;&#23427;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#38236;&#20687;&#19979;&#38477;&#26469;&#21021;&#22987;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20351;&#29992;Hopfield&#27169;&#22411;&#20316;&#20026;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#22411;&#65292;&#38236;&#20687;&#19979;&#38477;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#27604;&#20381;&#36182;&#20110;&#38543;&#26426;&#21442;&#25968;&#21021;&#22987;&#21270;&#30340;&#20256;&#32479;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#26174;&#20102;&#38236;&#20687;&#19979;&#38477;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#21021;&#22987;&#21270;&#25216;&#26415;&#65292;&#21487;&#20197;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mirror descent is an elegant optimization technique that leverages a dual space of parametric models to perform gradient descent. While originally developed for convex optimization, it has increasingly been applied in the field of machine learning. In this study, we propose a novel approach for utilizing mirror descent to initialize the parameters of neural networks. Specifically, we demonstrate that by using the Hopfield model as a prototype for neural networks, mirror descent can effectively train the model with significantly improved performance compared to traditional gradient descent methods that rely on random parameter initialization. Our findings highlight the potential of mirror descent as a promising initialization technique for enhancing the optimization of machine learning models.
&lt;/p&gt;</description></item><item><title>ReLM&#26159;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#39564;&#35777;&#21644;&#26597;&#35810;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;LLM&#25968;&#25454;&#35760;&#24518;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#35821;&#35328;&#29702;&#35299;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#24191;&#27867;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15458</link><description>&lt;p&gt;
&#20351;&#29992;ReLM&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Validating Large Language Models with ReLM. (arXiv:2211.15458v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15458
&lt;/p&gt;
&lt;p&gt;
ReLM&#26159;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#39564;&#35777;&#21644;&#26597;&#35810;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;LLM&#25968;&#25454;&#35760;&#24518;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#35821;&#35328;&#29702;&#35299;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20026;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#30340;&#25991;&#26412;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#20294;&#26159;&#36234;&#26469;&#36234;&#22810;&#20154;&#20851;&#27880;LLM&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#22914;&#25968;&#25454;&#35760;&#24518;&#12289;&#20559;&#35265;&#21644;&#19981;&#24688;&#24403;&#35821;&#35328;&#20351;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#30340;&#22797;&#26434;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#20351;&#24471;&#39564;&#35777;&#65288;&#21644;&#32416;&#27491;&#65289;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ReLM&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#34920;&#36798;&#24335;&#39564;&#35777;&#21644;&#26597;&#35810;LLM&#30340;&#31995;&#32479;&#12290;ReLM&#23558;&#24191;&#27867;&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24418;&#24335;&#21270;&#24182;&#21551;&#29992;&#65292;&#23558;&#22797;&#26434;&#30340;&#35780;&#20272;&#35268;&#21017;&#31616;&#21270;&#20026;&#31616;&#21333;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25506;&#32034;&#20102;&#20851;&#20110;&#35760;&#24518;&#12289;&#24615;&#21035;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#26597;&#35810;&#65292;&#26174;&#31034;ReLM&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#26597;&#35810;&#25216;&#26415;&#36798;&#21040;&#20102;&#39640;&#36798;15&#20493;&#30340;&#31995;&#32479;&#25928;&#29575;&#12289;2.5&#20493;&#30340;&#25968;&#25454;&#25928;&#29575;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#32479;&#35745;&#21644;&#25552;&#31034;&#35843;&#25972;&#35206;&#30422;&#33539;&#22260;&#12290;ReLM&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;LLM&#39564;&#35777;&#38382;&#39064;&#25552;&#20379;&#20102;&#31454;&#20105;&#24615;&#21644;&#36890;&#29992;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language. Unfortunately, the complexity and generation capacities of LLMs make validating (and correcting) such concerns difficult. In this work, we introduce ReLM, a system for validating and querying LLMs using standard regular expressions. ReLM formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. Our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that ReLM achieves up to 15x higher system efficiency, 2.5x data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers a competitive and general baseline for the increasingly important problem of LLM valida
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24863;&#30693;&#26680;&#35843;&#21046;&#19979;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#36873;&#25321;&#28304;&#27169;&#22411;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16559</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#24863;&#30693;&#26680;&#35843;&#21046;&#19979;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Few-shot Image Generation via Adaptation-Aware Kernel Modulation. (arXiv:2210.16559v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24863;&#30693;&#26680;&#35843;&#21046;&#19979;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#36873;&#25321;&#28304;&#27169;&#22411;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#22270;&#20687;&#29983;&#25104;&#26088;&#22312;&#22312;&#32473;&#23450;&#26497;&#20854;&#26377;&#38480;&#30340;&#22495;&#20869;&#26679;&#26412;&#30340;&#21069;&#25552;&#19979;&#29983;&#25104;&#26032;&#30340;&#19981;&#21516;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#24050;&#39044;&#35757;&#32451;&#22312;&#22823;&#35268;&#27169;&#26469;&#28304;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;GAN&#27169;&#22411;&#65292;&#24182;&#26681;&#25454;&#26497;&#23569;&#30340;&#30446;&#26631;&#22495;&#26679;&#26412;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#26368;&#36817;&#30340;FSIG&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#30693;&#35782;&#20445;&#30041;&#20934;&#21017;&#65292;&#23427;&#20204;&#26088;&#22312;&#36873;&#25321;&#35201;&#20445;&#30041;&#21040;&#36866;&#24212;&#27169;&#22411;&#20013;&#30340;&#28304;&#27169;&#22411;&#30693;&#35782;&#23376;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#30340;&#30693;&#35782;&#20445;&#30041;&#20934;&#21017;&#21482;&#32771;&#34385;&#28304;&#22495;/&#28304;&#20219;&#21153;&#65292;&#27809;&#26377;&#32771;&#34385;&#30446;&#26631;&#22495;/&#36866;&#24212;&#20219;&#21153;&#22312;&#36873;&#25321;&#28304;&#27169;&#22411;&#30693;&#35782;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#23545;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#30340;&#35774;&#32622;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#37325;&#26032;&#23457;&#35270;&#26368;&#36817;&#30340;FSIG&#20316;&#21697;&#21450;&#20854;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot image generation (FSIG) aims to learn to generate new and diverse samples given an extremely limited number of samples from a domain, e.g., 10 training samples. Recent work has addressed the problem using transfer learning approach, leveraging a GAN pretrained on a large-scale source domain dataset and adapting that model to the target domain based on very limited target domain samples. Central to recent FSIG methods are knowledge preserving criteria, which aim to select a subset of source model's knowledge to be preserved into the adapted model. However, a major limitation of existing methods is that their knowledge preserving criteria consider only source domain/source task, and they fail to consider target domain/adaptation task in selecting source model's knowledge, casting doubt on their suitability for setups of different proximity between source and target domain. Our work makes two contributions. As our first contribution, we re-visit recent FSIG works and their experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraph MPNNs&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Subgraph MPNNs&#19981;&#33021;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#35745;&#25968;&#36229;&#36807;4&#20010;&#30340;&#29615;&#65292;&#36825;&#23545;&#20110;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2210.13978</link><description>&lt;p&gt;
&#25552;&#21319;I$^2$-GNN&#22312;&#24490;&#29615;&#35745;&#25968;&#26041;&#38754;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Boosting the Cycle Counting Power of Graph Neural Networks with I$^2$-GNNs. (arXiv:2210.13978v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;Subgraph MPNNs&#30340;GNN&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;Subgraph MPNNs&#19981;&#33021;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#35745;&#25968;&#36229;&#36807;4&#20010;&#30340;&#29615;&#65292;&#36825;&#23545;&#20110;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;(MPNNs)&#26159;&#19968;&#31867;&#34987;&#24191;&#27867;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#12290;&#28982;&#32780;&#65292;MPNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#21551;&#21457;&#25105;&#20204;&#30740;&#31350;&#21487;&#35777;&#26126;&#20855;&#26377;&#26356;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;GNN&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#30740;&#31350;&#23376;&#22270;MPNNs&#30340;&#35745;&#25968;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31867;&#26368;&#26032;&#21644;&#24120;&#29992;&#30340;&#24378;&#22823;GNN&#27169;&#22411;&#65292;&#20854;&#20174;&#27599;&#20010;&#33410;&#28857;&#25552;&#21462;&#26681;&#25454;&#23376;&#22270;&#65292;&#22312;&#26681;&#33410;&#28857;&#20998;&#37197;&#21807;&#19968;&#26631;&#35782;&#31526;&#24182;&#22312;&#20854;&#26681;&#25454;&#23376;&#22270;&#20013;&#32534;&#30721;&#26681;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#23376;&#22270;MPNNs&#19981;&#33021;&#22312;&#33410;&#28857;&#32423;&#21035;&#19978;&#35745;&#25968;&#36229;&#36807;4&#20010;&#30340;&#29615;&#65292;&#36825;&#24847;&#21619;&#30528;&#33410;&#28857;&#34920;&#31034;&#19981;&#33021;&#27491;&#30830;&#22320;&#32534;&#30721;&#21608;&#22260;&#30340;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Message Passing Neural Networks (MPNNs) are a widely used class of Graph Neural Networks (GNNs). The limited representational power of MPNNs inspires the study of provably powerful GNN architectures. However, knowing one model is more powerful than another gives little insight about what functions they can or cannot express. It is still unclear whether these models are able to approximate specific functions such as counting certain graph substructures, which is essential for applications in biology, chemistry and social network analysis. Motivated by this, we propose to study the counting power of Subgraph MPNNs, a recent and popular class of powerful GNN models that extract rooted subgraphs for each node, assign the root node a unique identifier and encode the root node's representation within its rooted subgraph. Specifically, we prove that Subgraph MPNNs fail to count more-than-4-cycles at node level, implying that node representations cannot correctly encode the surrounding substru
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#20010;DL&#27169;&#22411;&#26694;&#26550;&#65292;&#21517;&#20026;&#22240;&#26524;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#20197;&#36924;&#36817;&#22240;&#26524;&#31639;&#23376;&#65288;CO&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;CNO&#27169;&#22411;&#21487;&#20197;&#22312;&#32039;&#33268;&#38598;&#19978;&#19968;&#33268;&#36924;&#36817;H&#246;lder&#25110;&#24179;&#28369;&#36857;&#31867;&#31639;&#23376;&#12290;</title><link>http://arxiv.org/abs/2210.13300</link><description>&lt;p&gt;
&#35774;&#35745;&#36890;&#29992;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65306;&#20197;&#38543;&#26426;&#20998;&#26512;&#20013;&#30340;&#26080;&#38480;&#32500;&#21160;&#24577;&#31995;&#32479;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Designing Universal Causal Deep Learning Models: The Case of Infinite-Dimensional Dynamical Systems from Stochastic Analysis. (arXiv:2210.13300v2 [math.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13300
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#20010;DL&#27169;&#22411;&#26694;&#26550;&#65292;&#21517;&#20026;&#22240;&#26524;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#65292;&#20197;&#36924;&#36817;&#22240;&#26524;&#31639;&#23376;&#65288;CO&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;CNO&#27169;&#22411;&#21487;&#20197;&#22312;&#32039;&#33268;&#38598;&#19978;&#19968;&#33268;&#36924;&#36817;H&#246;lder&#25110;&#24179;&#28369;&#36857;&#31867;&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#31639;&#23376;&#65288;CO&#65289;&#22312;&#24403;&#20195;&#38543;&#26426;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#20363;&#22914;&#21508;&#31181;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#19968;&#20010;&#33021;&#22815;&#36924;&#36817;CO&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#30340;&#35268;&#33539;&#26694;&#26550;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;DL&#27169;&#22411;&#35774;&#35745;&#26694;&#26550;&#26469;&#25552;&#20986;&#19968;&#20010;&#8220;&#20960;&#20309;&#24863;&#30693;&#8221;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26694;&#26550;&#20197;&#21512;&#36866;&#30340;&#26080;&#38480;&#32500;&#32447;&#24615;&#24230;&#37327;&#31354;&#38388;&#20026;&#36755;&#20837;&#65292;&#24182;&#36820;&#22238;&#36866;&#24212;&#36825;&#20123;&#32447;&#24615;&#20960;&#20309;&#30340;&#36890;&#29992;&#36830;&#32493;&#24207;&#21015;DL&#27169;&#22411;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#27169;&#22411;&#20026;&#22240;&#26524;&#31070;&#32463;&#31639;&#23376;&#65288;CNO&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25152;&#20135;&#29983;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#32039;&#33268;&#38598;&#19978;&#21644;&#36328;&#20219;&#24847;&#26377;&#38480;&#26102;&#38388;&#35270;&#37326;&#19978;&#19968;&#33268;&#36924;&#36817;H&#246;lder&#25110;&#24179;&#28369;&#36857;&#31867;&#31639;&#23376;&#65292;&#36825;&#20123;&#31639;&#23376;&#22240;&#26524;&#22320;&#26144;&#23556;&#32473;&#23450;&#32447;&#24615;&#24230;&#37327;&#31354;&#38388;&#20043;&#38388;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20851;&#20110;CNO&#30340;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#30340;&#26032;&#23450;&#37327;&#20851;&#31995;&#65292;&#29978;&#33267;&#23545;&#20110;&#65288;&#32463;&#20856;&#30340;&#65289;&#26377;&#38480;&#32500;DL&#27169;&#22411;&#20063;&#26377;&#26032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal operators (CO), such as various solution operators to stochastic differential equations, play a central role in contemporary stochastic analysis; however, there is still no canonical framework for designing Deep Learning (DL) models capable of approximating COs. This paper proposes a "geometry-aware'" solution to this open problem by introducing a DL model-design framework that takes suitable infinite-dimensional linear metric spaces as inputs and returns a universal sequential DL model adapted to these linear geometries. We call these models Causal Neural Operators (CNOs). Our main result states that the models produced by our framework can uniformly approximate on compact sets and across arbitrarily finite-time horizons H\"older or smooth trace class operators, which causally map sequences between given linear metric spaces. Our analysis uncovers new quantitative relationships on the latent state-space dimension of CNOs which even have new implications for (classical) finite-d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#23618;&#32423;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#25910;&#32553;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#24037;&#20316;&#27531;&#38556;&#21151;&#33021;&#35780;&#20272;&#30005;&#27744;&#30340;&#23610;&#24230;&#20998;&#35299;&#12289;&#26465;&#30446;&#36873;&#25321;&#12289;&#21442;&#25968;&#35782;&#21035;&#21644;&#21709;&#24212;&#35780;&#20998;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.10952</link><description>&lt;p&gt;
&#33258;&#32534;&#30721;&#31232;&#30095;&#36125;&#21494;&#26031;&#30340;IRT&#22240;&#23376;&#20998;&#35299;&#65292;&#26657;&#20934;&#65292;&#21644;&#25705;&#23572;&#28363;&#20114;&#21453;&#25512;&#26029;&#65306;&#20197;&#24037;&#20316;&#27531;&#38556;&#21151;&#33021;&#35780;&#20272;&#30005;&#27744; (WD-FAB) &#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Autoencoded sparse Bayesian in-IRT factorization, calibration, and amortized inference for the Work Disability Functional Assessment Battery. (arXiv:2210.10952v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#23618;&#32423;&#27169;&#22411;&#65292;&#21033;&#29992;&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#25910;&#32553;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#20102;&#24037;&#20316;&#27531;&#38556;&#21151;&#33021;&#35780;&#20272;&#30005;&#27744;&#30340;&#23610;&#24230;&#20998;&#35299;&#12289;&#26465;&#30446;&#36873;&#25321;&#12289;&#21442;&#25968;&#35782;&#21035;&#21644;&#21709;&#24212;&#35780;&#20998;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#27531;&#38556;&#21151;&#33021;&#35780;&#20272;&#30005;&#27744; (WD-FAB) &#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#30446;&#24211;&#30340;&#65292;&#29992;&#20110;&#35780;&#20272;&#32844;&#19994;&#30456;&#20851;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#21151;&#33021;&#30340;&#22810;&#32500;&#29289;&#21697;&#21453;&#24212;&#29702;&#35770; (IRT) &#24037;&#20855;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#23618;&#32423;&#27169;&#22411;&#65292;&#23454;&#29616;&#20197;&#19979;&#21516;&#26102;&#20219;&#21153;&#65306;&#23610;&#24230;&#20998;&#35299;&#65292;&#26465;&#30446;&#36873;&#25321;&#65292;&#21442;&#25968;&#35782;&#21035;&#21644;&#21709;&#24212;&#35780;&#20998;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#25910;&#32553;&#65292;&#28040;&#38500;&#20102;&#32447;&#24615;&#22240;&#23376;&#21270;/IRT&#27169;&#22411;&#30340;&#39640;&#24230;&#21442;&#25968;&#21270;&#38382;&#39064;&#65292;&#32780;&#21518;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#25705;&#23572;&#28363;&#20114;&#21453;&#25512;&#26029;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#26041;&#27861;&#22312;WD-FAB&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#27604;&#36739;&#34920;&#26126;&#20102;&#20854;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Work Disability Functional Assessment Battery (WD-FAB) is a multidimensional item response theory (IRT) instrument designed for assessing work-related mental and physical function based on responses to an item bank. In prior iterations it was developed using traditional means -- linear factorization and null hypothesis statistical testing for item partitioning/selection, and finally, posthoc calibration of disjoint unidimensional IRT models. As a result, the WD-FAB, like many other IRT instruments, is a posthoc model. Its item partitioning, based on exploratory factor analysis, is blind to the final nonlinear IRT model and is not performed in a manner consistent with goodness of fit to the final model. In this manuscript, we develop a Bayesian hierarchical model for self-consistently performing the following simultaneous tasks: scale factorization, item selection, parameter identification, and response scoring. This method uses sparsity-based shrinkage to obviate the linear factori
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#26469;&#20102;&#35299;&#25919;&#27835;&#27963;&#21160;&#30340;&#29305;&#28857;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2210.10669</link><description>&lt;p&gt;
&#38024;&#23545;Facebook&#19978;&#30340;&#25919;&#27835;&#27963;&#21160;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#26469;&#20102;&#35299;&#25919;&#27835;&#27963;&#21160;&#30340;&#29305;&#28857;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30446;&#21069;&#26159;&#25919;&#27835;&#20449;&#24687;&#20256;&#25773;&#30340;&#20027;&#35201;&#28192;&#36947;&#65292;&#25919;&#27835;&#23478;&#20204;&#33021;&#22815;&#36890;&#36807;&#36825;&#20123;&#24179;&#21488;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#36827;&#34892;&#23459;&#20256;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#24212;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20351;&#36825;&#31181;&#20132;&#27969;&#36879;&#26126;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20449;&#24687;&#20256;&#25773;&#19982;&#30446;&#26631;&#21463;&#20247;&#32039;&#23494;&#30456;&#36830;&#65292;&#24182;&#32463;&#24120;&#34987;&#22810;&#20010;&#21033;&#30410;&#25912;&#20851;&#26041;&#20849;&#21516;&#20256;&#25773;&#12290;&#26412;&#25991;&#26088;&#22312;&#31532;&#19968;&#27493;&#20102;&#35299;&#36825;&#20123;&#39640;&#24230;&#20998;&#25955;&#30340;&#25919;&#27835;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#24182;&#20998;&#26512;&#25919;&#27835;&#27963;&#21160;&#22914;&#20309;&#20351;&#29992;&#26576;&#31181;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#22914;&#20301;&#32622;&#12289;&#24615;&#21035;&#25110;&#24180;&#40836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36873;&#20030;&#27665;&#24847;&#35843;&#26597;&#20013;&#25919;&#27835;&#24191;&#21578;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ScionFL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#25928;&#36816;&#34892;&#22312;&#37327;&#21270;&#36755;&#20837;&#19978;&#24182;&#21516;&#26102;&#25552;&#20379;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#24378;&#20581;&#24615;&#30340;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.07376</link><description>&lt;p&gt;
ScionFL: &#39640;&#25928;&#19988;&#24378;&#20581;&#30340;&#23433;&#20840;&#37327;&#21270;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
ScionFL: Efficient and Robust Secure Quantized Aggregation. (arXiv:2210.07376v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ScionFL&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#39640;&#25928;&#36816;&#34892;&#22312;&#37327;&#21270;&#36755;&#20837;&#19978;&#24182;&#21516;&#26102;&#25552;&#20379;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#24378;&#20581;&#24615;&#30340;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20197;&#20943;&#36731;&#19982;&#20013;&#22830;&#32858;&#21512;&#22120;&#30475;&#21040;&#25152;&#26377;&#21442;&#25968;&#26356;&#26032;&#26377;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#24573;&#30053;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#27491;&#20132;&#30740;&#31350;&#26041;&#21521;&#65292;&#26088;&#22312;&#65288;i&#65289;&#26174;&#30528;&#20943;&#23569;&#23458;&#25143;&#31471;-&#26381;&#21153;&#22120;&#36890;&#20449;&#21644;&#65288;ii&#65289;&#20943;&#36731;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#39069;&#22806;&#30340;&#23646;&#24615;&#23545;&#20110;&#23454;&#29616;&#36328;&#35774;&#22791;&#30340;&#32852;&#37030;&#23398;&#20064;&#19982;&#25104;&#21315;&#19978;&#19975;&#29978;&#33267;&#30334;&#19975;&#65288;&#31227;&#21160;&#65289;&#21442;&#19982;&#32773;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;ScionFL&#65292;&#32852;&#21512;&#20102;&#20004;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#36825;&#26159;FL&#30340;&#31532;&#19968;&#20010;&#23433;&#20840;&#32858;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#37327;&#21270;&#36755;&#20837;&#19978;&#26377;&#25928;&#36816;&#34892;&#65292;&#21516;&#26102;&#25552;&#20379;&#23545;&#24694;&#24847;&#23458;&#25143;&#31471;&#30340;&#24378;&#20581;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#65288;&#26032;&#39062;&#30340;&#65289;&#22810;&#26041;&#35745;&#31639;&#65288;MPC&#65289;&#25216;&#26415;&#65292;&#24182;&#25903;&#25345;&#22810;&#20010;&#32447;&#24615;&#65288;1&#27604;&#29305;&#65289;&#37327;&#21270;&#26041;&#26696;&#65292;&#21253;&#25324;&#21033;&#29992;&#38543;&#26426;&#21704;&#36798;&#29595;&#21464;&#25442;&#21644;&#21345;&#30003;&#34920;&#31034;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Secure aggregation is commonly used in federated learning (FL) to alleviate privacy concerns related to the central aggregator seeing all parameter updates in the clear. Unfortunately, most existing secure aggregation schemes ignore two critical orthogonal research directions that aim to (i) significantly reduce client-server communication and (ii) mitigate the impact of malicious clients. However, both of these additional properties are essential to facilitate cross-device FL with thousands or even millions of (mobile) participants. In this paper, we unite both research directions by introducing ScionFL, the first secure aggregation framework for FL that operates efficiently on quantized inputs and simultaneously provides robustness against malicious clients. Our framework leverages (novel) multi-party computation (MPC) techniques and supports multiple linear (1-bit) quantization schemes, including ones that utilize the randomized Hadamard transform and Kashin's representation. Our th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#19968;&#33324;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#26680;&#24515;&#38598;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2210.04260</link><description>&lt;p&gt;
Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#30340;&#26680;&#24515;&#38598;
&lt;/p&gt;
&lt;p&gt;
Coresets for Wasserstein Distributionally Robust Optimization Problems. (arXiv:2210.04260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26500;&#24314;&#19968;&#33324;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#26680;&#24515;&#38598;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;WDRO&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#21547;&#31946;&#25968;&#25454;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#40065;&#26834;&#24615;&#30340;&#27969;&#34892;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;WDRO&#30340;&#22797;&#26434;&#24230;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#26159;&#31105;&#27490;&#24615;&#30340;&#65292;&#22240;&#20026;&#35299;&#20915;&#20854;&#8220;&#26497;&#23567;&#26497;&#22823;&#8221;&#34920;&#36798;&#24335;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#38024;&#23545;&#29305;&#23450;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65288;&#20363;&#22914;&#36923;&#36753;&#22238;&#24402;&#65289;&#30340;&#24555;&#36895;WDRO&#35757;&#32451;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#22823;&#35268;&#27169;WDRO&#30340;&#35774;&#35745;&#39640;&#25928;&#31639;&#27861;&#30340;&#30740;&#31350;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#26680;&#24515;&#38598;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#21387;&#32553;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#20943;&#23569;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26500;&#24314;&#19968;&#33324;WDRO&#38382;&#39064;&#30340;$\epsilon$-coreset&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23613;&#31649;&#30001;&#20110;&#19981;&#30830;&#23450;&#24615;&#32780;&#33719;&#21462;WDRO&#30340;&#20256;&#32479;&#26680;&#24515;&#38598;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wasserstein distributionally robust optimization (\textsf{WDRO}) is a popular model to enhance the robustness of machine learning with ambiguous data. However, the complexity of \textsf{WDRO} can be prohibitive in practice since solving its ``minimax'' formulation requires a great amount of computation. Recently, several fast \textsf{WDRO} training algorithms for some specific machine learning tasks (e.g., logistic regression) have been developed. However, the research on designing efficient algorithms for general large-scale \textsf{WDRO}s is still quite limited, to the best of our knowledge. \textit{Coreset} is an important tool for compressing large dataset, and thus it has been widely applied to reduce the computational complexities for many optimization problems. In this paper, we introduce a unified framework to construct the $\epsilon$-coreset for the general \textsf{WDRO} problems. Though it is challenging to obtain a conventional coreset for \textsf{WDRO} due to the uncertaint
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DSpERT&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#24230;Transformer&#36880;&#23618;&#32858;&#21512;&#26631;&#35760;&#34920;&#31034;&#20316;&#20026;&#38190;&#21644;&#20540;&#65292;&#20135;&#29983;&#20102;&#28145;&#23618;&#35821;&#20041;&#30340;&#36328;&#24230;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#20013;&#38271;&#36328;&#24230;&#23454;&#20307;&#26174;&#30528;&#26080;&#25928;&#24615;&#21644;&#37325;&#21472;&#36328;&#24230;&#34920;&#31034;&#30340;&#32806;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSpERT&#22312;&#20843;&#20010;NER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#39640;&#20110;&#25110;&#19982;&#26368;&#26032;&#26368;&#20808;&#36827;&#31995;&#32479;&#31454;&#20105;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.04182</link><description>&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#28145;&#24230;&#36328;&#24230;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DSpERT&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#24230;Transformer&#36880;&#23618;&#32858;&#21512;&#26631;&#35760;&#34920;&#31034;&#20316;&#20026;&#38190;&#21644;&#20540;&#65292;&#20135;&#29983;&#20102;&#28145;&#23618;&#35821;&#20041;&#30340;&#36328;&#24230;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#20013;&#38271;&#36328;&#24230;&#23454;&#20307;&#26174;&#30528;&#26080;&#25928;&#24615;&#21644;&#37325;&#21472;&#36328;&#24230;&#34920;&#31034;&#30340;&#32806;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSpERT&#22312;&#20843;&#20010;NER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#39640;&#20110;&#25110;&#19982;&#26368;&#26032;&#26368;&#20808;&#36827;&#31995;&#32479;&#31454;&#20105;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24230;&#22522;&#30784;&#27169;&#22411;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26368;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290; &#29616;&#26377;&#30340;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#23558;&#26631;&#35760;&#34920;&#31034;&#27973;&#23618;&#32858;&#21512;&#21040;&#36328;&#24230;&#34920;&#31034;&#20013;&#12290; &#20294;&#26159;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#38271;&#36328;&#24230;&#23454;&#20307;&#30340;&#26174;&#30528;&#26080;&#25928;&#24615;&#65292;&#37325;&#21472;&#36328;&#24230;&#34920;&#31034;&#30340;&#32806;&#21512;&#65292;&#26368;&#32456;&#24615;&#33021;&#19979;&#38477;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DSpERT&#65288;&#26469;&#33258;Transformer&#30340;&#28145;&#24230;&#36328;&#24230;&#32534;&#30721;&#22120;&#34920;&#31034;&#65289;&#65292;&#23427;&#30001;&#26631;&#20934;Transformer&#21644;&#36328;&#24230;Transformer&#32452;&#25104;&#12290; &#21518;&#32773;&#20351;&#29992;&#20302;&#23618;&#27425;&#30340;&#36328;&#24230;&#34920;&#31034;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#20174;&#24213;&#37096;&#21040;&#39030;&#37096;&#36880;&#23618;&#32858;&#21512;&#26631;&#35760;&#34920;&#31034;&#20316;&#20026;&#38190;&#21644;&#20540;&#65292;&#22240;&#27492;&#65292;DSpERT&#20135;&#29983;&#20102;&#28145;&#23618;&#35821;&#20041;&#30340;&#36328;&#24230;&#34920;&#31034;&#12290; &#20511;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;DSpERT&#22312;&#20843;&#20010;NER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20110;&#25110;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290; &#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#28145;&#24230;&#23545;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long-span entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics.  With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on eight NER benchmarks. Experimental results verify the importance of the depth for s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26500;&#24314;&#26082;&#31934;&#30830;&#21448;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#22522;&#26412;&#19981;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#23398;&#19979;&#38480;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.15259</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19981;&#21487;&#33021;&#23433;&#20840;&#30340;&#35770;&#36848;
&lt;/p&gt;
&lt;p&gt;
On the Impossible Safety of Large AI Models. (arXiv:2209.15259v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26500;&#24314;&#26082;&#31934;&#30830;&#21448;&#23433;&#20840;&#30340;&#27169;&#22411;&#30340;&#22522;&#26412;&#19981;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#32479;&#35745;&#23398;&#19979;&#38480;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;(LAIMs)&#65292;&#20854;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#26368;&#31361;&#20986;&#30340;&#26368;&#36817;&#30340;&#20363;&#23376;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#23427;&#20204;&#23384;&#22312;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#26412;&#25991;&#31995;&#32479;&#21270;&#22320;&#24635;&#32467;&#20102;&#26377;&#20851;&#26500;&#24314;&#20219;&#24847;&#20934;&#30830;&#21644;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22522;&#26412;&#19981;&#21487;&#33021;&#24615;&#30340;&#30693;&#35782;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20170;&#22825;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#35774;&#32622;&#30340;&#20851;&#38190;&#25361;&#25112;&#29305;&#24449;&#12290;&#21363;&#65292;&#39640;&#31934;&#24230;&#20284;&#20046;&#38656;&#35201;&#35760;&#20303;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#26159;&#29992;&#25143;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#39640;&#24230;&#24322;&#26500;&#65292;&#21253;&#25324;&#25935;&#24863;&#20449;&#24687;&#21644;&#20551;&#29992;&#25143;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#32479;&#35745;&#19979;&#38480;&#65292;&#35748;&#20026;&#36825;&#26500;&#25104;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#35774;&#35745;&#20855;&#26377;&#24378;&#23433;&#20840;&#20445;&#35777;&#30340;&#39640;&#31934;&#24230;LAIMs&#30340;&#21487;&#33021;&#24615;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today's machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12336</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#30340;&#24418;&#24335;&#21270;&#23433;&#20840;&#20445;&#38556;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Generating Formal Safety Assurances for High-Dimensional Reachability. (arXiv:2209.12336v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#35813;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#33258;&#20027;&#31995;&#32479;&#25552;&#20379;&#27491;&#24335;&#30340;&#23433;&#20840;&#21644;&#24615;&#33021;&#20445;&#35777;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#21704;&#23494;&#39039;-&#38597;&#31185;&#27604;&#65288;HJ&#65289;&#21487;&#36798;&#24615;&#20998;&#26512;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#25552;&#20379;&#36825;&#20123;&#20445;&#35777;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#38750;&#32447;&#24615;&#31995;&#32479;&#21160;&#24577;&#12289;&#26377;&#30028;&#23545;&#25239;&#31995;&#32479;&#24178;&#25200;&#20197;&#21450;&#29366;&#24577;&#21644;&#36755;&#20837;&#32422;&#26463;&#12290;&#20294;&#26159;&#65292;&#23427;&#28041;&#21450;&#21040;&#27714;&#35299;PDE&#65292;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#38543;&#30528;&#29366;&#24577;&#32500;&#24230;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#31995;&#32479;&#19978;&#30340;&#30452;&#25509;&#20351;&#29992;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;DeepReach&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#27491;&#24358;&#31070;&#32463;PDE&#27714;&#35299;&#22120;&#26469;&#20811;&#26381;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#35299;&#20915;&#39640;&#32500;&#21487;&#36798;&#24615;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#35201;&#27714;&#38543;&#21487;&#36798;&#31649;&#22797;&#26434;&#24615;&#32780;&#19981;&#26159;&#29366;&#24577;&#31354;&#38388;&#32500;&#24230;&#32780;&#21464;&#21270;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#22240;&#27492;&#35745;&#31639;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#23433;&#20840;&#65292;&#36825;&#27809;&#26377;&#36798;&#21040;&#25105;&#20204;&#25552;&#20379;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#30340;&#24635;&#20307;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Safe DeepReach&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;DeepReach&#26041;&#27861;&#29983;&#25104;&#27491;&#24335;&#23433;&#20840;&#20445;&#38556;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#26032;&#39062;&#30340;Lipschitz&#36830;&#32493;&#24615;&#20998;&#26512;&#19982;&#21306;&#38388;&#36793;&#30028;&#20256;&#25773;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#20197;&#26377;&#25928;&#21644;&#21487;&#20280;&#32553;&#30340;&#26041;&#24335;&#20445;&#35777;&#35299;&#20915;&#26041;&#26696;&#30340;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#31034;&#20363;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#22522;&#20110;&#24863;&#30693;&#30340;&#39640;&#32500;&#36710;&#36947;&#20445;&#25345;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing formal safety and performance guarantees for autonomous systems is becoming increasingly important. Hamilton-Jacobi (HJ) reachability analysis is a popular formal verification tool for providing these guarantees, since it can handle general nonlinear system dynamics, bounded adversarial system disturbances, and state and input constraints. However, it involves solving a PDE, whose computational and memory complexity scales exponentially with respect to the state dimensionality, making its direct use on large-scale systems intractable. A recently proposed method called DeepReach overcomes this challenge by leveraging a sinusoidal neural PDE solver for high-dimensional reachability problems, whose computational requirements scale with the complexity of the underlying reachable tube rather than the state space dimension. Unfortunately, neural networks can make errors and thus the computed solution may not be safe, which falls short of achieving our overarching goal to provide fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#35843;&#21046;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;DT-JSCC&#65292;&#39318;&#20808;&#25506;&#35752;&#32534;&#30721;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#22312;&#25509;&#25910;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#22833;&#30495;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#22266;&#26377;&#26435;&#34913;&#65292;&#35299;&#20915;&#20102;&#20165;&#20256;&#36755;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#21644;JSCC&#22312;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#19978;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.10382</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#20013;&#30340;&#40065;&#26834;&#20449;&#24687;&#29942;&#39048;&#19982;&#25968;&#23383;&#35843;&#21046;
&lt;/p&gt;
&lt;p&gt;
Robust Information Bottleneck for Task-Oriented Communication with Digital Modulation. (arXiv:2209.10382v2 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#35843;&#21046;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;DT-JSCC&#65292;&#39318;&#20808;&#25506;&#35752;&#32534;&#30721;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#22312;&#25509;&#25910;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#22833;&#30495;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#22266;&#26377;&#26435;&#34913;&#65292;&#35299;&#20915;&#20102;&#20165;&#20256;&#36755;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#21644;JSCC&#22312;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#19978;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;&#8212;&#20449;&#36947;&#32534;&#30721;(JSCC)&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#20256;&#36755;&#21040;&#25509;&#25910;&#26041;&#65292;&#35774;&#35745;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#36793;&#32536;&#25512;&#26029;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20165;&#20256;&#36755;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#32780;&#19981;&#24341;&#20837;&#20219;&#20309;&#20887;&#20313;&#23548;&#33268;&#30340;&#20449;&#24687;&#21464;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#32780;&#30452;&#25509;&#23558;&#28304;&#25968;&#25454;&#26144;&#23556;&#20026;&#36830;&#32493;&#30340;&#20449;&#36947;&#36755;&#20837;&#31526;&#21495;&#30340;JSCC&#22312;&#29616;&#26377;&#25968;&#23383;&#36890;&#20449;&#31995;&#32479;&#19978;&#23384;&#22312;&#20860;&#23481;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#25506;&#35752;&#32534;&#30721;&#34920;&#31034;&#30340;&#20449;&#24687;&#37327;&#21644;&#22312;&#25509;&#25910;&#34920;&#31034;&#20013;&#30340;&#20449;&#24687;&#22833;&#30495;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#22266;&#26377;&#26435;&#34913;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23383;&#35843;&#21046;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#36890;&#20449;&#26041;&#26696;&#65292;&#31216;&#20026;&#31163;&#25955;&#20219;&#21153;&#23548;&#21521;&#30340;JSCC (DT-JSCC)&#65292;&#20854;&#20013;&#21457;&#23556;&#26426;&#23558;&#29305;&#24449;&#32534;&#30721;&#20026;&#31163;&#25955;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#20256;&#36755;&#21040;&#25509;&#25910;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented communications, mostly using learning-based joint source-channel coding (JSCC), aim to design a communication-efficient edge inference system by transmitting task-relevant information to the receiver. However, only transmitting task-relevant information without introducing any redundancy may cause robustness issues in learning due to the channel variations, and the JSCC which directly maps the source data into continuous channel input symbols poses compatibility issues on existing digital communication systems. In this paper, we address these two issues by first investigating the inherent tradeoff between the informativeness of the encoded representations and the robustness to information distortion in the received representations, and then propose a task-oriented communication scheme with digital modulation, named discrete task-oriented JSCC (DT-JSCC), where the transmitter encodes the features into a discrete representation and transmits it to the receiver with the digi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PATRON&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;few-shot&#24615;&#33021;&#65292;&#22312;&#20845;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20919;&#21551;&#21160;&#25968;&#25454;&#36873;&#25321;&#22522;&#32447;&#65292;&#19988;&#20165;&#20351;&#29992;128&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;91.0%&#21644;92.1%&#30340;&#23436;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06995</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65306;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#20256;&#36882;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;few-shot&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PATRON&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;few-shot&#24615;&#33021;&#65292;&#22312;&#20845;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20919;&#21551;&#21160;&#25968;&#25454;&#36873;&#25321;&#22522;&#32447;&#65292;&#19988;&#20165;&#20351;&#29992;128&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;91.0%&#21644;92.1%&#30340;&#23436;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;few-shot&#24615;&#33021;&#65292;&#20294;&#24615;&#33021;&#23545;&#20110;few-shot&#23454;&#20363;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PATRON&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#36873;&#25321;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#25968;&#25454;&#65292;&#22312;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#27809;&#26377;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;PATRON&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#65288;1&#65289;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#26041;&#27861;&#26469;&#20272;&#35745;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#21644;&#65288;2&#65289;&#19968;&#31181;&#20998;&#21106;-&#37325;&#20889;&#65288;PTR&#65289;&#31574;&#30053;&#65292;&#20197;&#22312;&#26597;&#35810;&#27880;&#37322;&#26102;&#20419;&#36827;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#20845;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PATRON&#30340;&#34920;&#29616;&#27604;&#26368;&#24378;&#30340;&#20919;&#21551;&#21160;&#25968;&#25454;&#36873;&#25321;&#22522;&#32447;&#20248;&#36234;&#20102;6.9%&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;128&#26631;&#31614;&#65292;PATRON&#22522;&#20110;&#26222;&#36890;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#23398;&#20064;&#20998;&#21035;&#36798;&#21040;&#20102;91.0%&#21644;92.1%&#30340;&#23436;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;PATRON&#23454;&#29616;&#21487;&#22312;\url{https://github.com/yueyu1030/Patron}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \url{https://github.com/yueyu1030/Patron}.
&lt;/p&gt;</description></item><item><title>EDeNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26356;&#25509;&#36817;&#21407;&#22987;&#20107;&#20214;&#25968;&#25454;&#27969;&#65292;&#36991;&#20813;&#20102;&#31215;&#32047;&#20107;&#20214;&#21040;&#22270;&#20687;&#24103;&#30340;&#36807;&#31243;&#65292;&#23637;&#29616;&#20102;&#22312;&#35282;&#36895;&#24230;&#22238;&#24402;&#21644;&#31454;&#20105;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.04362</link><description>&lt;p&gt;
EDeNN: &#20107;&#20214;&#34928;&#20943;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20302;&#24310;&#36831;&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
EDeNN: Event Decay Neural Networks for low latency vision. (arXiv:2209.04362v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.04362
&lt;/p&gt;
&lt;p&gt;
EDeNN&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26356;&#25509;&#36817;&#21407;&#22987;&#20107;&#20214;&#25968;&#25454;&#27969;&#65292;&#36991;&#20813;&#20102;&#31215;&#32047;&#20107;&#20214;&#21040;&#22270;&#20687;&#24103;&#30340;&#36807;&#31243;&#65292;&#23637;&#29616;&#20102;&#22312;&#35282;&#36895;&#24230;&#22238;&#24402;&#21644;&#31454;&#20105;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#25968;&#23383;&#8220;&#31070;&#32463;&#20803;&#8221;&#26159;&#23545;&#29983;&#29289;&#31070;&#32463;&#20803;&#30340;&#19968;&#31181;&#38750;&#24120;&#19981;&#31934;&#30830;&#30340;&#36817;&#20284;&#12290;&#24403;&#21069;&#30340;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#22312;&#25968;&#23383;&#35774;&#22791;&#19978;&#36827;&#34892;&#65292;&#20855;&#26377;&#20687;&#22270;&#20687;&#24103;&#36825;&#26679;&#30340;&#25968;&#23383;&#25968;&#25454;&#34920;&#31034;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#29983;&#29289;&#35270;&#35273;&#31995;&#32479;&#36890;&#24120;&#27604;&#26368;&#20808;&#36827;&#30340;&#25968;&#23383;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#26356;&#20855;&#33021;&#21147;&#21644;&#25928;&#29575;&#12290;&#20107;&#20214;&#30456;&#26426;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20256;&#24863;&#22120;&#25216;&#26415;&#65292;&#23427;&#27169;&#20223;&#29983;&#29289;&#35270;&#35273;&#65292;&#20351;&#29992;&#24322;&#27493;&#35302;&#21457;&#30340;&#20687;&#32032;&#65292;&#25918;&#24323;&#20102;&#22270;&#20687;&#24103;&#30340;&#27010;&#24565;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#65292;&#35768;&#22810;&#22522;&#20110;&#20107;&#20214;&#30340;&#31639;&#27861;&#19981;&#24471;&#19981;&#23558;&#20107;&#20214;&#31215;&#32047;&#22238;&#22270;&#20687;&#24103;&#65292;&#20174;&#32780;&#28010;&#36153;&#20102;&#20107;&#20214;&#30456;&#26426;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#37319;&#29992;&#30456;&#21453;&#30340;&#29702;&#24565;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#26356;&#25509;&#36817;&#21407;&#22987;&#20107;&#20214;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#22312;&#35282;&#36895;&#24230;&#22238;&#24402;&#21644;&#31454;&#20105;&#20809;&#27969;&#20272;&#35745;&#26041;&#38754;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#31215;&#32047;&#20107;&#20214;&#21040;&#22270;&#20687;&#24103;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of neural networks in computer vision tasks, digital 'neurons' are a very loose approximation of biological neurons. Today's learning approaches are designed to function on digital devices with digital data representations such as image frames. In contrast, biological vision systems are generally much more capable and efficient than state-of-the-art digital computer vision algorithms. Event cameras are an emerging sensor technology which imitates biological vision with asynchronously firing pixels, eschewing the concept of the image frame. To leverage modern learning techniques, many event-based algorithms are forced to accumulate events back to image frames, somewhat squandering the advantages of event cameras.  We follow the opposite paradigm and develop a new type of neural network which operates closer to the original event data stream. We demonstrate state-of-the-art performance in angular velocity regression and competitive optical flow estimation, while avoid
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23454;&#26102;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;EdgeSRGAN&#65292;&#20351;&#29992;&#27169;&#22411;&#37327;&#21270;&#25552;&#39640;&#20102;CPU&#21644;Edge TPU&#35774;&#22791;&#19978;&#30340;&#25191;&#34892;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#36827;&#19968;&#27493;&#20248;&#21270;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#36739;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.03355</link><description>&lt;p&gt;
&#24102;&#26377;&#30693;&#35782;&#33976;&#39311;&#30340;&#36793;&#32536;&#29983;&#25104;&#23545;&#25239;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation. (arXiv:2209.03355v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03355
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23454;&#26102;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;EdgeSRGAN&#65292;&#20351;&#29992;&#27169;&#22411;&#37327;&#21270;&#25552;&#39640;&#20102;CPU&#21644;Edge TPU&#35774;&#22791;&#19978;&#30340;&#25191;&#34892;&#25928;&#29575;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#36827;&#19968;&#27493;&#20248;&#21270;&#27169;&#22411;&#65292;&#20445;&#30041;&#20102;&#36739;&#22909;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#24133;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#21487;&#25903;&#25345;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#38656;&#35201;&#21487;&#38752;&#30340;&#35270;&#35273;&#27969;&#20197;&#30417;&#35270;&#20219;&#21153;&#12289;&#22788;&#29702;&#36828;&#31243;&#25805;&#32437;&#25110;&#30740;&#31350;&#30456;&#20851;&#35270;&#35273;&#32454;&#33410;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;&#12289;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#23454;&#26102;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#31216;&#20026;EdgeSRGAN&#65288;&#20195;&#30721;&#21487;&#22312;https://github.com/PIC4SeR/EdgeSRGAN&#33719;&#24471;&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#21407;&#22987;SRGAN&#30340;&#23450;&#21046;&#21270;&#26550;&#26500;&#21644;&#27169;&#22411;&#37327;&#21270;&#65292;&#20197;&#25552;&#39640;CPU&#21644;Edge TPU&#35774;&#22791;&#19978;&#30340;&#25191;&#34892;&#25928;&#29575;&#65292;&#23454;&#29616;&#26368;&#39640;&#36798;200fps&#30340;&#25512;&#29702;&#35745;&#31639;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#23558;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#21040;&#32593;&#32476;&#30340;&#36739;&#23567;&#29256;&#26412;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#30456;&#27604;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26356;&#37325;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#27169;&#22411;&#20445;&#30041;&#20102;&#30456;&#24403;&#20196;&#20154;&#28385;&#24847;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#24102;&#26377;&#24102;&#23485;&#38477;&#32423;&#30340;&#22270;&#20687;&#20256;&#36755;&#23454;&#39564;&#65292;&#20197;&#31361;&#26174;&#35813;&#31995;&#32479;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real-time Super-Resolution, called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#39592;&#24178;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#22495;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29616;&#20854;&#20934;&#30830;&#24615;&#19982;&#21333;&#22495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#26377;&#26174;&#33879;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.01121</link><description>&lt;p&gt;
&#37325;&#24402;&#39592;&#24178;&#65306;&#37325;&#26032;&#21457;&#29616;&#39592;&#24178;&#22312;&#22495;&#27867;&#21270;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Back-to-Bones: Rediscovering the Role of Backbones in Domain Generalization. (arXiv:2209.01121v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#39592;&#24178;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#22495;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#22120;&#21457;&#29616;&#20854;&#20934;&#30830;&#24615;&#19982;&#21333;&#22495;&#20998;&#31867;&#20219;&#21153;&#24615;&#33021;&#26377;&#26174;&#33879;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#27867;&#21270;(DG)&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#20110;&#36229;&#20986;&#35757;&#32451;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#25991;&#29486;&#20013;&#20805;&#26021;&#30528;&#22768;&#31216;&#33021;&#22815;&#33719;&#24471;&#26356;&#25277;&#35937;&#21644;&#31283;&#20581;&#30340;&#25968;&#25454;&#34920;&#31034;&#20197;&#24212;&#23545;&#22495;&#20559;&#31227;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20026;DG&#25552;&#20379;&#20102;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#65292;&#25351;&#20986;&#20102;&#22825;&#30495;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#20173;&#28982;&#22362;&#25345;&#20351;&#29992;&#30456;&#21516;&#36807;&#26102;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#27880;&#24847;&#21040;&#19981;&#21516;&#39592;&#24178;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#22238;&#21040;&#39592;&#24178;&#65292;&#25552;&#20986;&#20102;&#23545;&#23427;&#20204;&#20869;&#22312;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20174;&#26631;&#20934;&#30340;&#27531;&#24046;&#35299;&#20915;&#26041;&#26696;&#21040;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#21457;&#29616;&#20102;&#19968;&#20010;&#26126;&#26174;&#30340;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#23601;&#26159;&#35828;&#65292;&#22823;&#35268;&#27169;&#21333;&#22495;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#21487;&#20197;&#36890;&#36807;&#39592;&#24178;&#30340;&#20934;&#30830;&#24615;&#20984;&#26174;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain Generalization (DG) studies the capability of a deep learning model to generalize to out-of-training distributions. In the last decade, literature has been massively filled with training methodologies that claim to obtain more abstract and robust data representations to tackle domain shifts. Recent research has provided a reproducible benchmark for DG, pointing out the effectiveness of naive empirical risk minimization (ERM) over existing algorithms. Nevertheless, researchers persist in using the same outdated feature extractors, and no attention has been given to the effects of different backbones yet. In this paper, we start back to the backbones proposing a comprehensive analysis of their intrinsic generalization capabilities, which so far have been ignored by the research community. We evaluate a wide variety of feature extractors, from standard residual solutions to transformer-based architectures, finding an evident linear correlation between large-scale single-domain clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#19982;&#20154;&#31867;&#35268;&#33539;&#36827;&#34892;&#23545;&#27604;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;LLMs&#22312;&#26576;&#20123;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#20013;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#23384;&#22312;&#24369;&#28857;&#65292;&#20363;&#22914;&#22312;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2208.11981</link><description>&lt;p&gt;
&#35770;&#29616;&#23454;&#21644;&#35821;&#35328;&#25968;&#25454;&#38480;&#21046;&#65306;&#23558;LLMs&#19982;&#20154;&#31867;&#35268;&#33539;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#19982;&#20154;&#31867;&#35268;&#33539;&#36827;&#34892;&#23545;&#27604;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;LLMs&#22312;&#26576;&#20123;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#20013;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#23384;&#22312;&#24369;&#28857;&#65292;&#20363;&#22914;&#22312;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21033;&#29992;&#22823;&#37327;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#20851;&#32852;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#26469;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#20173;&#26377;&#30097;&#38382;&#12290;&#22312;&#22238;&#39038;&#29616;&#26377;&#21327;&#35758;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#35268;&#33539;&#19982;GPT-3&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36890;&#24120;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#31867;&#21035;&#20197;&#21450;&#24369;&#28857;&#25152;&#22312;&#12290;GPT-3&#20026;&#21253;&#25324;&#21516;&#20041;&#35789;&#12289;&#21453;&#20041;&#35789;&#21644;&#40664;&#35748;&#32487;&#25215;&#22312;&#20869;&#30340;&#20960;&#20010;&#20851;&#31995;&#26041;&#38754;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#24403;&#30340;&#21475;&#22836;&#25512;&#29702;&#35777;&#25454;&#12290;&#27809;&#26377;&#26469;&#33258;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;GPT-3&#22312;&#20855;&#26377;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#30340;&#21306;&#38388;&#19979;&#38480;&#22788;&#12290;&#22312;&#24517;&#35201;&#21697;&#36136;&#12289;&#22823;&#23567;&#39034;&#24207;&#21644;&#24378;&#24230;&#39034;&#24207;&#31561;&#26041;&#38754;&#20063;&#35266;&#23519;&#21040;&#20102;&#19981;&#36275;&#20043;&#22788;&#12290;&#25226;LLMs&#19982;&#35937;&#24449;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#19981;&#30456;&#23481;&#24615;&#27979;&#37327;&#25351;&#26631;&#8220;&#20849;&#21516;&#26412;&#24449;&#31354;&#38388;&#24178;&#25200;&#8221;&#65292;&#24182;&#36827;&#19968;&#27493;&#36890;&#36807;&#37327;&#23376;&#24320;&#20851;&#23454;&#29616;&#37327;&#21270;&#65292;&#26041;&#20415;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#31639;&#27861;&#21487;&#20197;&#23545;&#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#65292;&#30830;&#23450;&#20849;&#20139;&#30456;&#20284;&#27979;&#37327;&#29615;&#22659;&#30340;&#35266;&#23519;&#32773;&#32452;&#12290;</title><link>http://arxiv.org/abs/2208.06210</link><description>&lt;p&gt;
&#29992;&#37327;&#23376;&#24320;&#20851;&#27979;&#37327;&#19981;&#30456;&#23481;&#24615;&#24182;&#23545;&#37327;&#23376;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Measuring incompatibility and clustering quantum observables with a quantum switch. (arXiv:2208.06210v3 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#23376;&#19981;&#30456;&#23481;&#24615;&#27979;&#37327;&#25351;&#26631;&#8220;&#20849;&#21516;&#26412;&#24449;&#31354;&#38388;&#24178;&#25200;&#8221;&#65292;&#24182;&#36827;&#19968;&#27493;&#36890;&#36807;&#37327;&#23376;&#24320;&#20851;&#23454;&#29616;&#37327;&#21270;&#65292;&#26041;&#20415;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#22788;&#29702;&#12290;&#21516;&#26102;&#65292;&#31639;&#27861;&#21487;&#20197;&#23545;&#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#65292;&#30830;&#23450;&#20849;&#20139;&#30456;&#20284;&#27979;&#37327;&#29615;&#22659;&#30340;&#35266;&#23519;&#32773;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30456;&#23481;&#35266;&#27979;&#32467;&#26524;&#26159;&#37327;&#23376;&#21147;&#23398;&#30340;&#22522;&#30707;&#65292;&#24182;&#19988;&#26159;&#37327;&#23376;&#25216;&#26415;&#20013;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30456;&#23481;&#24615;&#27979;&#37327;&#25351;&#26631;&#65292;&#21363;&#8220;&#20849;&#21516;&#26412;&#24449;&#31354;&#38388;&#24178;&#25200;&#8221;&#65292;&#21487;&#20197;&#37327;&#21270;&#19968;&#20010;&#31934;&#30830;&#35266;&#27979;&#37327;&#23545;&#21478;&#19968;&#20010;&#26412;&#24449;&#31354;&#38388;&#30340;&#24178;&#25200;&#37327;&#12290;&#35813;&#25351;&#26631;&#25552;&#20379;&#20102; von Neumann &#27979;&#37327;&#31354;&#38388;&#30340;&#24230;&#37327;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#31216;&#20026;&#8220;&#37327;&#23376;&#24320;&#20851;&#8221;&#30340;&#35774;&#32622;&#65292;&#35753;&#27979;&#37327;&#36807;&#31243;&#20197;&#19981;&#30830;&#23450;&#30340;&#39034;&#24207;&#36827;&#34892;&#65292;&#20174;&#32780;&#39640;&#25928;&#22320;&#36827;&#34892;&#20272;&#35745;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#65292;MED&#21487;&#20197;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26080;&#30417;&#30563;&#31639;&#27861;&#65292;&#23545;&#26410;&#30693;&#30340;von Neumann &#27979;&#37327;&#32467;&#26524;&#36827;&#34892;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23545;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#21487;&#29992;&#20110;&#30830;&#23450;&#20849;&#20139;&#36817;&#20284;&#30456;&#21516;&#27979;&#37327;&#29615;&#22659;&#30340;&#35266;&#23519;&#32773;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of incompatible observables is a cornerstone of quantum mechanics and a valuable resource in quantum technologies. Here we introduce a measure of incompatibility, called the mutual eigenspace disturbance (MED), which quantifies the amount of disturbance induced by the measurement of a sharp observable on the eigenspaces of another. The MED provides a metric on the space of von Neumann measurements, and can be efficiently estimated by letting the measurement processes act in an indefinite order, using a setup known as the quantum switch, which also allows one to quantify the noncommutativity of arbitrary quantum processes. Thanks to these features, the MED can be used in quantum machine learning tasks. We demonstrate this application by providing an unsupervised algorithm that clusters unknown von Neumann measurements. Our algorithm is robust to noise can be used to identify groups of observers that share approximately the same measurement context.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309; $\beta$-&#24046;&#24322;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2207.06316</link><description>&lt;p&gt;
&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Majorization-minimization for Sparse Nonnegative Matrix Factorization with the $\beta$-divergence. (arXiv:2207.06316v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102; $\beta$-&#24046;&#24322;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20854;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309; $\beta$-&#24046;&#24322;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20803;&#20056;&#27861;&#26356;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377; $\beta$-&#24046;&#24322;&#21644;&#20004;&#20010;&#22240;&#23376;&#20013;&#30340;&#19968;&#20010;&#65288;&#27604;&#22914;&#35828;&#65292;&#28608;&#27963;&#30697;&#38453;&#65289;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#12290;&#26631;&#20934;&#30340;&#20570;&#27861;&#26159;&#38480;&#21046;&#23383;&#20856;&#30340;&#21015;&#20855;&#26377;&#21333;&#20301;&#33539;&#25968;&#65292;&#20174;&#32780;&#25511;&#21046;&#21478;&#19968;&#20010;&#22240;&#23376;&#65288;&#23383;&#20856;&#30697;&#38453;&#65289;&#30340;&#33539;&#25968;&#65292;&#20197;&#36991;&#20813;&#30149;&#24577;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21407;&#38382;&#39064;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#31561;&#20215;&#30340;&#26631;&#24230;&#19981;&#21464;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#22359;&#19979;&#38477;&#20027;&#23548;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#23545;&#20110; $\ell_{1}$-&#27491;&#21017;&#21270;&#25110;&#26356; "&#28608;&#36827;" &#30340;&#23545;&#25968;&#27491;&#21017;&#21270;&#37117;&#21487;&#20197;&#20135;&#29983;&#31616;&#21333;&#30340;&#22810;&#20803;&#20056;&#27861;&#26356;&#26032;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20219;&#20309; $\beta$-&#24046;&#24322;&#65288;&#21363;&#20219;&#20309; $\beta$ &#30340;&#20540;&#65289;&#21644;&#20854;&#20182;&#31232;&#30095;&#32422;&#26463;&#19978;&#20063;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article introduces new multiplicative updates for nonnegative matrix factorization with the $\beta$-divergence and sparse regularization of one of the two factors (say, the activation matrix). It is well known that the norm of the other factor (the dictionary matrix) needs to be controlled in order to avoid an ill-posed formulation. Standard practice consists in constraining the columns of the dictionary to have unit norm, which leads to a nontrivial optimization problem. Our approach leverages a reparametrization of the original problem into the optimization of an equivalent scale-invariant objective function. From there, we derive block-descent majorization-minimization algorithms that result in simple multiplicative updates for either $\ell_{1}$-regularization or the more "aggressive" log-regularization. In contrast with other state-of-the-art methods, our algorithms are universal in the sense that they can be applied to any $\beta$-divergence (i.e., any value of $\beta$) and t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.03075</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards the Practical Utility of Federated Learning in the Medical Domain. (arXiv:2207.03075v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24212;&#29992;&#32852;&#37030;&#23398;&#20064;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#23454;&#29992;&#25351;&#21335;&#65292;&#21253;&#25324;&#19977;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#65292;&#26088;&#22312;&#25552;&#39640;&#21307;&#20445;&#19994;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#24418;&#25104;&#36866;&#29992;&#20110;&#20840;&#34892;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#21307;&#23398;&#39046;&#22495;&#26159;&#37319;&#29992;FL&#30340;&#26368;&#36866;&#21512;&#39046;&#22495;&#20043;&#19968;&#65292;&#22240;&#20026;&#24517;&#39035;&#23562;&#37325;&#24739;&#32773;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25552;&#20379;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#24212;&#29992;FL&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#21363;&#38271;&#26399;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12289;&#30382;&#32932;&#30284;&#22270;&#20687;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#65292;&#25552;&#20986;&#32463;&#39564;&#22522;&#20934;&#21644;&#23454;&#39564;&#35774;&#32622;&#12290;&#28508;&#22312;&#30340;FL&#29992;&#25143;&#65292;&#22914;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#65292;&#21487;&#20197;&#23558;&#36825;&#20123;&#22522;&#20934;&#20316;&#20026;&#37319;&#29992;FL&#30340;&#25351;&#21335;&#65292;&#24182;&#23613;&#21487;&#33021;&#20943;&#23569;&#35797;&#38169;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#20197;&#20445;&#30041;&#29616;&#23454;&#19990;&#30028;&#30340;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20845;&#31181;&#38024;&#23545;&#23458;&#25143;&#31471;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;FL&#31639;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#20004;&#31181;&#20856;&#22411;FL&#31639;&#27861;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#31639;&#27861;&#12290;&#22522;&#20110;&#19977;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#26356;&#22797;&#26434;&#31639;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#21307;&#30103;&#26426;&#26500;&#21644;IT&#20844;&#21496;&#25552;&#20379;&#20102;&#22312;&#23433;&#20840;&#39640;&#25928;&#30340;&#26041;&#24335;&#19979;&#65292;&#24212;&#29992;FL&#20174;&#32780;&#25913;&#21892;&#21307;&#30103;&#20445;&#20581;&#30340;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorith
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#35757;&#32451;&#26679;&#26412;&#36951;&#24536;&#31243;&#24230;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#26631;&#20934;&#30340;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#26102;&#38388;&#19978;&#30830;&#23454;&#20250;&#36951;&#24536;&#31034;&#20363;&#65292;&#30830;&#23450;&#24615;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20250;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2207.00099</link><description>&lt;p&gt;
&#35760;&#24518;&#21270;&#35757;&#32451;&#26679;&#26412;&#30340;&#36951;&#24536;&#31243;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Forgetting of Memorized Training Examples. (arXiv:2207.00099v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#35757;&#32451;&#26679;&#26412;&#36951;&#24536;&#31243;&#24230;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#26631;&#20934;&#30340;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#26102;&#38388;&#19978;&#30830;&#23454;&#20250;&#36951;&#24536;&#31034;&#20363;&#65292;&#30830;&#23450;&#24615;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20250;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#20004;&#31181;&#20284;&#20046;&#30683;&#30462;&#30340;&#29616;&#35937;: &#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#21270;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#36951;&#24536;&#12290;&#22312;&#35760;&#24518;&#21270;&#20013;&#65292;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#23450;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#24182;&#21464;&#24471;&#23481;&#26131;&#21463;&#21040;&#38544;&#31169;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36951;&#24536;&#20013;&#65292;&#20986;&#29616;&#22312;&#35757;&#32451;&#26089;&#26399;&#30340;&#26679;&#26412;&#26368;&#32456;&#20250;&#34987;&#36951;&#24536;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#29616;&#35937;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25216;&#26415;&#26469;&#34913;&#37327;&#27169;&#22411;&#8220;&#36951;&#24536;&#8221;&#35757;&#32451;&#26679;&#26412;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#21464;&#24471;&#19981;&#22826;&#23481;&#26131;&#21463;&#21040;&#26368;&#36817;&#27809;&#26377;&#30475;&#21040;&#30340;&#26679;&#26412;&#30340;&#38544;&#31169;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#34429;&#28982;&#38750;&#20984;&#27169;&#22411;&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#27704;&#20037;&#35760;&#24518;&#21270;&#25968;&#25454;&#65292;&#20294;&#26631;&#20934;&#30340;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#26102;&#38388;&#19978;&#30830;&#23454;&#20250;&#36951;&#24536;&#31034;&#20363;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38750;&#30830;&#23450;&#24615;&#20316;&#20026;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#34920;&#26126;&#30830;&#23450;&#24615;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20250;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;&#26497;&#22823;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#35266;&#23519;&#21040;&#30340;&#38544;&#31169;&#21487;&#33021;&#20250;&#20986;&#29616;&#22312;&#35757;&#32451;&#26089;&#26399;&#30475;&#21040;&#30340;&#26679;&#26412;&#19978;&#65292;&#20363;&#22914;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models exhibit two seemingly contradictory phenomena: training data memorization, and various forms of forgetting. In memorization, models overfit specific training examples and become susceptible to privacy attacks. In forgetting, examples which appeared early in training are forgotten by the end. In this work, we connect these phenomena. We propose a technique to measure to what extent models "forget" the specifics of training examples, becoming less susceptible to privacy attacks on examples they have not seen recently. We show that, while non-convex models can memorize data forever in the worst-case, standard image, speech, and language models empirically do forget examples over time. We identify nondeterminism as a potential explanation, showing that deterministically trained models do not forget. Our results suggest that examples seen early when training with extremely large datasets - for instance those examples used to pre-train a model - may observe privacy be
&lt;/p&gt;</description></item><item><title>MOOD&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#24322;&#20998;&#24067;&#25193;&#25955;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23646;&#24615;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#20351;&#24471;&#36870;&#21521;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#25351;&#23548;&#30446;&#26631;&#29305;&#24615;&#21040;&#39640;&#20998;&#25968;&#21306;&#22495;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#26032;&#39062;&#19988;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2206.07632</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#24322;&#20998;&#24067;&#29983;&#25104;&#25506;&#32034;&#21270;&#23398;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Exploring Chemical Space with Score-based Out-of-distribution Generation. (arXiv:2206.07632v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07632
&lt;/p&gt;
&lt;p&gt;
MOOD&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#24322;&#20998;&#24067;&#25193;&#25955;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#23646;&#24615;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#20351;&#24471;&#36870;&#21521;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#25351;&#23548;&#30446;&#26631;&#29305;&#24615;&#21040;&#39640;&#20998;&#25968;&#21306;&#22495;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25628;&#32034;&#26032;&#39062;&#19988;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20998;&#23376;&#29983;&#25104;&#27169;&#22411;&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#23616;&#38480;&#24615;&#26159;&#29983;&#25104;&#30340;&#20998;&#23376;&#19982;&#35757;&#32451;&#38598;&#20013;&#30340;&#20998;&#23376;&#39640;&#24230;&#30456;&#20284;&#12290;&#20026;&#20102;&#29983;&#25104;&#20840;&#26032;&#30340;&#20998;&#23376;&#20197;&#23547;&#25214;&#26356;&#22909;&#30340;&#26032;&#39062;&#33647;&#29289;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#21270;&#23398;&#31354;&#38388;&#25506;&#32034;&#25216;&#26415;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25968;-based&#24322;&#20998;&#24067;&#25193;&#25955;&#31574;&#30053;(MOOD)&#65292;&#35813;&#31574;&#30053;&#22312;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#30340;&#29983;&#25104;&#20013;&#32467;&#21512;&#20102;&#24322;&#20998;&#24067;(OOD)&#25511;&#21046;&#65292;&#21516;&#26102;&#36890;&#36807;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#25511;&#21046;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#30001;&#20110;&#19968;&#20123;&#26032;&#39062;&#20998;&#23376;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#29616;&#23454;&#33647;&#29289;&#30340;&#22522;&#26412;&#35201;&#27714;&#65292;MOOD&#21033;&#29992;&#23646;&#24615;&#39044;&#27979;&#22120;&#30340;&#26799;&#24230;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#20174;&#32780;&#20351;&#24471;&#36870;&#21521;&#26102;&#38388;&#25193;&#25955;&#36807;&#31243;&#36890;&#36807;&#25351;&#23548;&#30446;&#26631;&#29305;&#24615;&#65288;&#22914;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12289;&#33647;&#29289;&#26679;&#24615;&#21644;&#21487;&#21512;&#25104;&#24615;&#65289;&#21040;&#39640;&#20998;&#25968;&#21306;&#22495;&#65292;&#20174;&#32780;&#20801;&#35768;MOOD&#25628;&#32034;&#26032;&#39062;&#19988;&#26377;&#24847;&#20041;&#30340;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
A well-known limitation of existing molecular generative models is that the generated molecules highly resemble those in the training set. To generate truly novel molecules that may have even better properties for de novo drug discovery, more powerful exploration in the chemical space is necessary. To this end, we propose Molecular Out-Of-distribution Diffusion(MOOD), a score-based diffusion scheme that incorporates out-of-distribution (OOD) control in the generative stochastic differential equation (SDE) with simple control of a hyperparameter, thus requires no additional costs. Since some novel molecules may not meet the basic requirements of real-world drugs, MOOD performs conditional generation by utilizing the gradients from a property predictor that guides the reverse-time diffusion process to high-scoring regions according to target properties such as protein-ligand interactions, drug-likeness, and synthesizability. This allows MOOD to search for novel and meaningful molecules r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FL-FDMS&#65292;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#36864;&#24441;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26102;&#25214;&#21040;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2205.13222</link><description>&lt;p&gt;
&#36890;&#36807;&#21451;&#22909;&#27169;&#22411;&#26367;&#25442;&#35299;&#20915;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#36864;&#24441;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Combating Client Dropout in Federated Learning via Friend Model Substitution. (arXiv:2205.13222v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FL-FDMS&#65292;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#36864;&#24441;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#26102;&#25214;&#21040;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#26469;&#26367;&#20195;&#32570;&#22833;&#30340;&#23458;&#25143;&#31471;&#30340;&#26356;&#26032;&#65292;&#23454;&#29616;&#26356;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20854;&#25968;&#25454;&#38544;&#31169;&#21644;&#36890;&#20449;&#25928;&#29575;&#30340;&#20248;&#21183;&#32780;&#38395;&#21517;&#12290;&#30001;&#20110;&#35768;&#22810;&#24773;&#20917;&#19979;&#23436;&#20840;&#23458;&#25143;&#31471;&#21442;&#19982;&#19981;&#21487;&#34892;&#65292;&#30740;&#31350;&#20102;&#20027;&#21160;&#36873;&#25321;/&#37319;&#26679;&#19968;&#37096;&#20998;&#23458;&#25143;&#31471;&#30340;&#37096;&#20998;&#21442;&#19982;FL&#31639;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#25509;&#36817;&#20840;&#21442;&#19982;&#24773;&#20917;&#30340;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#34987;&#21160;&#37096;&#20998;&#23458;&#25143;&#31471;&#21442;&#19982;&#24773;&#20917;&#65292;&#36825;&#31181;&#24773;&#20917;&#29702;&#35299;&#19981;&#36275;&#24471;&#22810;&#65292;&#37096;&#20998;&#21442;&#19982;&#26159;&#30001;&#20110;&#22806;&#37096;&#20107;&#20214;&#65288;&#21363;&#23458;&#25143;&#31471;&#36864;&#20986;&#65289;&#32780;&#19981;&#26159;FL&#31639;&#27861;&#30340;&#20915;&#23450;&#32780;&#23548;&#33268;&#30340;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#23458;&#25143;&#31471;&#36864;&#24441;&#30340;FL&#35270;&#20026;FL&#38382;&#39064;&#20013;&#30340;&#19968;&#31867;&#29305;&#27530;&#24773;&#20917;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#20197;&#25552;&#20132;&#26367;&#20195;&#65288;&#21487;&#33021;&#19981;&#20934;&#30830;&#65289;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;FL-FDMS&#65292;&#21487;&#20197;&#21363;&#26102;&#21457;&#29616;&#23458;&#25143;&#31471;&#30340;&#26379;&#21451;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#30456;&#20284;&#30340;&#23458;&#25143;&#31471;&#65289;&#65292;&#24182;&#20351;&#29992;&#26379;&#21451;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#26469;&#26367;&#20195;&#25918;&#24323;&#30340;&#23458;&#25143;&#31471;&#30340;&#32570;&#22833;&#26356;&#26032;&#12290;&#25105;&#20204;&#22312;&#20551;&#35774;&#19979;&#35777;&#26126;FL-FDMS&#22312;&#29702;&#35770;&#19978;&#25910;&#25947;&#21040;&#26368;&#20248;&#20840;&#23616;&#27169;&#22411;&#65292;&#35813;&#20551;&#35774;&#36275;&#20197;&#28085;&#30422;&#35768;&#22810;&#29616;&#26377;&#30340;FL&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#20010;&#22522;&#32447;&#30456;&#27604;&#65292;&#24403;&#23458;&#25143;&#31471;&#36864;&#20986;&#29575;&#36866;&#20013;&#21040;&#39640;&#26102;&#65292;FL-FDMS&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#23398;&#20064;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local
&lt;/p&gt;</description></item><item><title>FedAdapter&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#36866;&#37197;&#22120;&#37197;&#32622;&#20197;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#26377;&#21161;&#20110;&#38754;&#21521;&#29616;&#20195;NLP&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.10162</link><description>&lt;p&gt;
FedAdapter: &#38754;&#21521;&#29616;&#20195; NLP &#30340;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedAdapter: Efficient Federated Learning for Modern NLP. (arXiv:2205.10162v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10162
&lt;/p&gt;
&lt;p&gt;
FedAdapter&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#36866;&#37197;&#22120;&#37197;&#32622;&#20197;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#30340;&#25928;&#29575;&#65292;&#26377;&#21161;&#20110;&#38754;&#21521;&#29616;&#20195;NLP&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20026; NLP &#24102;&#26469;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#21644;&#36890;&#29992;&#24615;&#12290;&#20294;&#26159;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#38656;&#35201;&#31169;&#26377;&#25968;&#25454;&#65292;&#32780;&#32852;&#37030;&#23398;&#20064;&#26159;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#40644;&#37329;&#26041;&#27861;&#65288;&#21363;FedNLP&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#27979;&#37327;&#34920;&#26126;&#65292;&#30001;&#20110;&#22823;&#22411;&#27169;&#22411;&#30340;&#23384;&#22312;&#20197;&#21450;&#30456;&#24212;&#30340;&#39640;&#32593;&#32476;/&#35745;&#31639;&#25104;&#26412;&#65292;FedNLP&#26080;&#27861;&#36827;&#34892;&#12290;&#20026;&#20102;&#23454;&#29616;&#23454;&#29992;&#30340;FedNLP&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36866;&#37197;&#22120;&#20316;&#20026;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#36825;&#26159;&#19968;&#31181;&#25554;&#20837;&#21508;&#31181;&#27169;&#22411;&#23618;&#30340;&#23567;&#29942;&#39048;&#27169;&#22359;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#27491;&#30830;&#37197;&#32622;&#36866;&#37197;&#22120;&#30340;&#28145;&#24230;&#21644;&#23485;&#24230;&#65292;&#36825;&#23545;&#35757;&#32451;&#36895;&#24230;&#21644;&#25928;&#29575;&#38750;&#24120;&#25935;&#24863;&#12290;&#24182;&#19981;&#23384;&#22312;&#36866;&#29992;&#20110;&#25152;&#26377;&#24773;&#20917;&#30340;&#26368;&#20339;&#37197;&#32622;&#65306;&#26368;&#20339;&#36873;&#25321;&#22240;&#19979;&#28216;NLP&#20219;&#21153;&#12289;&#25152;&#38656;&#27169;&#22411;&#31934;&#24230;&#21644;&#31227;&#21160;&#36164;&#28304;&#32780;&#24322;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36866;&#37197;&#22120;&#37197;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; FedAdapter&#65292;&#36825;&#26159;&#19968;&#20010;&#22686;&#24378;&#29616;&#26377; FedNLP &#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#20004;&#20010;&#26032;&#39062;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models have revolutionized NLP for superior performance and generality. Fine-tuning pre-trained models for downstream tasks often requires private data, for which federated learning is the de-facto approach (i.e., FedNLP). However, our measurements show that FedNLP is prohibitively slow due to the large model sizes and the resultant high network/computation cost. Towards practical FedNLP, we identify as the key building blocks adapters, small bottleneck modules inserted at a variety of model layers. A key challenge is to properly configure the depth and width of adapters, to which the training speed and efficiency is highly sensitive. No silver-bullet configuration exists: the optimal choice varies across downstream NLP tasks, desired model accuracy, and mobile resources. To automate adapter configuration, we propose FedAdapter, a framework that enhances the existing FedNLP with two novel designs. First, FedAdapter progressively upgrades the adapter config
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29616;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20498;&#31435;&#25670;&#21644;&#22235;&#26059;&#32764;&#31283;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#23433;&#20840;&#30340;&#19988;&#34920;&#29616;&#19982;&#19981;&#23433;&#20840;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2205.06750</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#24378;&#21270;&#23398;&#20064;&#65306;&#29702;&#35770;&#21644;&#23454;&#39564;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Provably Safe Reinforcement Learning: A Theoretical and Experimental Comparison. (arXiv:2205.06750v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.06750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#29616;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#24182;&#22312;&#20498;&#31435;&#25670;&#21644;&#22235;&#26059;&#32764;&#31283;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#36825;&#20123;&#26041;&#27861;&#37117;&#26159;&#23433;&#20840;&#30340;&#19988;&#34920;&#29616;&#19982;&#19981;&#23433;&#20840;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#23433;&#20840;&#24615;&#23545;&#20110;&#24320;&#21457;&#20854;&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#30340;RL&#24182;&#19981;&#20445;&#35777;&#23433;&#20840;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#35774;&#35745;&#26469;&#25552;&#20379;RL&#30340;&#23433;&#20840;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#36824;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#20171;&#32461;&#20102;&#36830;&#32493;&#21644;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#36825;&#20123;&#26041;&#27861;&#26681;&#25454;&#23433;&#20840;&#26041;&#27861;&#22914;&#20309;&#36866;&#24212;&#21160;&#20316;&#36827;&#34892;&#20998;&#31867;&#65306;&#21160;&#20316;&#26367;&#25442;&#12289;&#21160;&#20316;&#25237;&#24433;&#21644;&#21160;&#20316;&#25513;&#34109;&#12290;&#25105;&#20204;&#22312;&#20498;&#31435;&#25670;&#21644;&#22235;&#26059;&#32764;&#31283;&#23450;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#26377;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#26041;&#27861;&#30830;&#23454;&#26159;&#23433;&#20840;&#30340;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#34920;&#29616;&#21487;&#19982;&#19981;&#23433;&#20840;&#30340;&#22522;&#32447;&#30456;&#23218;&#32654;&#12290;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#24212;&#36873;&#25321;&#19981;&#21516;&#30340;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring safety of reinforcement learning (RL) algorithms is crucial to unlock their potential for many real-world tasks. However, vanilla RL does not guarantee safety. In recent years, several methods have been proposed to provide safety guarantees for RL by design. Yet, there is no comprehensive comparison of these provably safe RL methods. We therefore introduce a categorization of existing provably safe RL methods, present the theoretical foundations for both continuous and discrete action spaces, and benchmark the methods' performance empirically. The methods are categorized based on how the action is adapted by the safety method: action replacement, action projection, and action masking. Our experiments on an inverted pendulum and quadrotor stabilization task show that all provably safe methods are indeed always safe. Furthermore, their trained performance is comparable to unsafe baselines. The benchmarking suggests that different provably safe RL approaches should be selected de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#38454;&#27573;&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#20132;&#21449;&#22330;&#25152;&#30340;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;MA-Echo&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35843;&#21644;&#26368;&#20248;&#21306;&#22495;&#36845;&#20195;&#26356;&#26032;&#25152;&#26377;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#23558;&#23427;&#20204;&#25289;&#36817;&#21040;&#19968;&#20010;&#20302;&#25439;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#33258;&#36523;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.12493</link><description>&lt;p&gt;
&#26080;&#26381;&#21153;&#22120;&#31471;&#35757;&#32451;&#30340;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-shot Federated Learning without Server-side Training. (arXiv:2204.12493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12493
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#38454;&#27573;&#21644;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#20132;&#21449;&#22330;&#25152;&#30340;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;MA-Echo&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#35843;&#21644;&#26368;&#20248;&#21306;&#22495;&#36845;&#20195;&#26356;&#26032;&#25152;&#26377;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#23558;&#23427;&#20204;&#25289;&#36817;&#21040;&#19968;&#20010;&#20302;&#25439;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#22312;&#33258;&#36523;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26088;&#22312;&#20445;&#25252;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20256;&#32479;&#32852;&#37030;&#23398;&#20064;&#30340;&#39640;&#36890;&#20449;&#20195;&#20215;&#65292;&#19968;&#27425;&#24615;&#32852;&#37030;&#23398;&#20064;&#24320;&#22987;&#21463;&#21040;&#20851;&#27880;&#65292;&#20197;&#20943;&#23569;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#26381;&#21153;&#22120;&#31471;&#35757;&#32451;&#30340;&#20132;&#21449;&#22330;&#25152;&#30340;&#26377;&#25928;&#31639;&#27861;MA-Echo&#65292;&#36890;&#36807;&#25506;&#32034;&#20844;&#20849;&#35843;&#21644;&#26368;&#20248;&#21306;&#22495;&#65292;&#36845;&#20195;&#26356;&#26032;&#25152;&#26377;&#26412;&#22320;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#23558;&#23427;&#20204;&#25289;&#36817;&#21040;&#25439;&#22833;&#34920;&#38754;&#19978;&#30340;&#19968;&#20010;&#20302;&#25439;&#22833;&#21306;&#22495;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#23427;&#20204;&#22312;&#20854;&#33258;&#36523;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has recently made significant progress as a new machine learning paradigm for privacy protection. Due to the high communication cost of traditional FL, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. Most of the existing one-shot FL methods are based on Knowledge Distillation; however, {distillation based approach requires an extra training phase and depends on publicly available data sets or generated pseudo samples.} In this work, we consider a novel and challenging cross-silo setting: performing a single round of parameter aggregation on the local models without server-side training. In this setting, we propose an effective algorithm for Model Aggregation via Exploring Common Harmonized Optima (MA-Echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#22522;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#27010;&#24565;&#25512;&#33616;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21521;&#19981;&#21516;&#19987;&#19994;&#27700;&#24179;&#30340;&#29992;&#25143;&#31934;&#32454;&#25512;&#33616;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2203.11011</link><description>&lt;p&gt;
&#24378;&#21270;&#22411;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#19979;MOOC&#27010;&#24565;&#25512;&#33616;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reinforced MOOCs Concept Recommendation in Heterogeneous Information Networks. (arXiv:2203.11011v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#19979;&#22522;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#30340;&#27010;&#24565;&#25512;&#33616;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21521;&#19981;&#21516;&#19987;&#19994;&#27700;&#24179;&#30340;&#29992;&#25143;&#31934;&#32454;&#25512;&#33616;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#36890;&#36807;&#20114;&#32852;&#32593;&#25552;&#20379;&#24320;&#25918;&#35775;&#38382;&#21644;&#24191;&#27867;&#30340;&#20114;&#21160;&#21442;&#19982;&#65292;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#22312;&#32447;&#21644;&#36828;&#31243;&#23398;&#20064;&#30340;&#39318;&#36873;&#26041;&#24335;&#12290;&#35768;&#22810;MOOC&#24179;&#21488;&#20026;&#29992;&#25143;&#25552;&#20379;&#35838;&#31243;&#25512;&#33616;&#26381;&#21153;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#30340;&#23398;&#20064;&#20307;&#39564;&#12290;&#23613;&#31649;&#36825;&#39033;&#26381;&#21153;&#24456;&#26377;&#29992;&#65292;&#20294;&#22914;&#26524;&#30452;&#25509;&#21521;&#29992;&#25143;&#25512;&#33616;&#35838;&#31243;&#21487;&#33021;&#20250;&#24573;&#35270;&#20182;&#20204;&#19981;&#21516;&#30340;&#19987;&#19994;&#27700;&#24179;&#65292;&#22240;&#27492;&#26412;&#25991;&#32771;&#34385;&#20102;&#27010;&#24565;&#25512;&#33616;&#36825;&#20010;&#38382;&#39064;&#65292;&#21487;&#20197;&#31934;&#32454;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;HinCRec-RL&#26469;&#35299;&#20915;MOOC&#20013;&#30340;&#27010;&#24565;&#25512;&#33616;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#27010;&#24565;&#25512;&#33616;&#38382;&#39064;&#22609;&#36896;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#20197;&#34920;&#24449;&#29992;&#25143;&#21644;&#30693;&#35782;&#27010;&#24565;&#20043;&#38388;&#30340;&#21160;&#24577;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs), which offer open access and widespread interactive participation through the internet, are quickly becoming the preferred method for online and remote learning. Several MOOC platforms offer the service of course recommendation to users, to improve the learning experience of users. Despite the usefulness of this service, we consider that recommending courses to users directly may neglect their varying degrees of expertise. To mitigate this gap, we examine an interesting problem of concept recommendation in this paper, which can be viewed as recommending knowledge to users in a fine-grained way. We put forward a novel approach, termed HinCRec-RL, for Concept Recommendation in MOOCs, which is based on Heterogeneous Information Networks and Reinforcement Learning. In particular, we propose to shape the problem of concept recommendation within a reinforcement learning framework to characterize the dynamic interaction between users and knowledge concepts 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2202.11046</link><description>&lt;p&gt;
&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A policy gradient approach for optimization of smooth risk measures. (arXiv:2202.11046v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#65292;&#24182;&#36866;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#21253;&#25324;on-policy&#21644;off-policy&#24773;&#20917;&#12290;&#25105;&#20204;&#32771;&#34385;&#26102;&#38388;&#27573;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#32047;&#31215;&#25240;&#25187;&#22870;&#21169;&#30340;&#24191;&#20041;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#26469;&#24314;&#27169;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#26495;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20998;&#21035;&#22312;on-policy&#21644;off-policy RL&#24773;&#20917;&#19979;&#20248;&#21270;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#38750;&#28176;&#36827;&#24615;&#30028;&#65292;&#37327;&#21270;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#25910;&#25947;&#21040;&#24179;&#28369;&#39118;&#38505;&#24230;&#37327;&#30340;&#31283;&#24577;&#28857;&#30340;&#36895;&#29575;&#12290;&#20316;&#20026;&#29305;&#20363;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#21035;&#24212;&#29992;&#20110;&#22343;&#20540;-&#26041;&#24046;&#21644;&#30072;&#21464;&#39118;&#38505;&#24230;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose policy gradient algorithms for solving a risk-sensitive reinforcement learning problem in on-policy as well as off-policy settings. We consider episodic Markov decision processes, and model the risk using the broad class of smooth risk measures of the cumulative discounted reward. We propose two template policy gradient algorithms that optimize a smooth risk measure in on-policy and off-policy RL settings, respectively. We derive non-asymptotic bounds that quantify the rate of convergence to our proposed algorithms to a stationary point of the smooth risk measure. As special cases, we establish that our algorithms apply to the optimization of mean-variance and distortion risk measures, respectively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wi-Fi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#22788;&#29702;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.22%&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GUI&#24212;&#29992;&#31243;&#24207;&#26041;&#20415;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.08146</link><description>&lt;p&gt;
&#22522;&#20110;Wi-Fi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;&#35782;&#21035;&#30340;&#21069;&#30651;&#24615;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#20855;&#26377;GUI&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#30340;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(arXiv:2202.08146v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation. (arXiv:2202.08146v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08146
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Wi-Fi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#22788;&#29702;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98.22%&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;GUI&#24212;&#29992;&#31243;&#24207;&#26041;&#20415;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#36817;&#30340;&#25216;&#26415;&#36827;&#27493;&#12289;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#12289;&#26234;&#33021;&#22478;&#24066;&#21644;&#31038;&#20250;&#32463;&#27982;&#21464;&#38761;&#30340;&#38656;&#35201;&#65292;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;(HAR)&#30740;&#31350;&#24050;&#32463;&#33719;&#24471;&#20102;&#37325;&#35201;&#30340;&#21160;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20256;&#24863;&#22120;&#30340;HAR&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#38544;&#31169;&#38382;&#39064;&#12289;&#23384;&#20648;&#21644;&#21151;&#29575;&#28040;&#32791;&#38382;&#39064;&#20197;&#21450;&#20329;&#25140;&#20256;&#24863;&#22120;&#30340;&#19981;&#36866;&#24863;&#65292;&#36825;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#35266;&#23519;&#21040;HAR&#30740;&#31350;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#22522;&#20110;WiFi&#30340;HAR&#22240;&#20854;&#26356;&#31895;&#31890;&#24230;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#21487;&#29992;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;WiFi&#30340;HAR&#26041;&#27861;&#20165;&#38480;&#20110;&#23545;&#22312;&#30456;&#31561;&#26102;&#38388;&#20869;&#25191;&#34892;&#30340;&#29420;&#31435;&#21644;&#38750;&#24182;&#21457;&#20154;&#31867;&#27963;&#21160;&#36827;&#34892;&#20998;&#31867;&#12290;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#36890;&#20449;&#38142;&#36335;&#65292;&#20854;&#20013;&#21457;&#23556;&#22120;&#26159;WiFi&#36335;&#30001;&#22120;&#65292;&#25509;&#25910;&#22120;&#26159;&#37197;&#22791;&#20102;Intel 5300 NIC&#30340;&#26234;&#33021;&#25163;&#26426;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(ABiGRNN)&#30340;WiFi&#20449;&#36947;&#25968;&#25454;&#30340;&#20154;&#19982;&#20154;&#20114;&#21160;(HHI)&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#39640;&#31934;&#24230;&#30340;&#23454;&#26102;&#22788;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;HHI&#27963;&#21160;&#25968;&#25454;&#38598;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;98.22%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#24212;&#29992;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#36731;&#26494;&#23454;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;HHI&#12290;
&lt;/p&gt;
&lt;p&gt;
Human Activity Recognition (HAR) research has gained significant momentum due to recent technological advancements, artificial intelligence algorithms, the need for smart cities, and socioeconomic transformation. However, existing computer vision and sensor-based HAR solutions have limitations such as privacy issues, memory and power consumption, and discomfort in wearing sensors for which researchers are observing a paradigm shift in HAR research. In response, WiFi-based HAR is gaining popularity due to the availability of more coarse-grained Channel State Information. However, existing WiFi-based HAR approaches are limited to classifying independent and non-concurrent human activities performed within equal time duration. Recent research commonly utilizes a Single Input Multiple Output communication link with a WiFi signal of 5 GHz channel frequency, using two WiFi routers or two Intel 5300 NICs as transmitter-receiver. Our study, on the other hand, utilizes a Multiple Input Multiple
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20445;&#35777;&#32852;&#37030;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615; (PF) &#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26131;&#20110;&#23454;&#29616;&#30340;&#31639;&#27861; PropFair&#65292;&#33021;&#22815;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#24046; 10% &#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2202.01666</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Proportional Fairness in Federated Learning. (arXiv:2202.01666v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.01666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#20445;&#35777;&#32852;&#37030;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#27604;&#20363;&#20844;&#24179;&#24615; (PF) &#27010;&#24565;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26131;&#20110;&#23454;&#29616;&#30340;&#31639;&#27861; PropFair&#65292;&#33021;&#22815;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#24046; 10% &#23458;&#25143;&#31471;&#24179;&#22343;&#24615;&#33021;&#20043;&#38388;&#36798;&#21040;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#20445;&#35777;&#20844;&#24179;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#20294;&#20063;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#21363;&#38656;&#35201;&#20026;&#20247;&#22810;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#25552;&#20379;&#21512;&#29702;&#28385;&#24847;&#30340;&#34920;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#21363;&#27604;&#20363;&#20844;&#24179;&#24615; (PF)&#65292;&#23427;&#22522;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#24615;&#33021;&#30340;&#30456;&#23545;&#21464;&#21270;&#12290;&#36890;&#36807;&#19982;&#20132;&#26131;&#21338;&#24328;&#30340;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; PropFair&#65292;&#19968;&#31181;&#26032;&#39062;&#19988;&#26131;&#20110;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23547;&#25214;&#27604;&#20363;&#20844;&#24179;&#35299;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#36890;&#36807;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126; PropFair &#33021;&#22815;&#22823;&#33268;&#25214;&#21040; PF &#35299;&#65292;&#24182;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#24046; 10% &#23458;&#25143;&#31471;&#30340;&#24179;&#22343;&#24615;&#33021;&#20043;&#38388;&#23454;&#29616;&#33391;&#22909;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL} &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients. Our code is available at \url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;gDOC&#26041;&#27861;&#21644;gDOC+&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#20998;&#24067;&#21644;&#26032;&#31867;&#21035;&#32422;&#26463;&#19979;&#36827;&#34892;&#28436;&#21270;&#22270;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.10558</link><description>&lt;p&gt;
&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#32422;&#26463;&#19979;&#23545;&#28436;&#21270;&#22270;&#36827;&#34892;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Lifelong Learning on Evolving Graphs Under the Constraints of Imbalanced Classes and New Classes. (arXiv:2112.10558v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;gDOC&#26041;&#27861;&#21644;gDOC+&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#20998;&#24067;&#21644;&#26032;&#31867;&#21035;&#32422;&#26463;&#19979;&#36827;&#34892;&#28436;&#21270;&#22270;&#32456;&#36523;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#22270;&#23398;&#20064;&#35299;&#20915;&#20102;&#19981;&#26029;&#36866;&#24212;&#28436;&#21270;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#35299;&#20915;&#20102;&#32456;&#36523;&#22270;&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#22788;&#29702;&#26032;&#30340;&#31867;&#21035;&#21644;&#24212;&#23545;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#36825;&#20004;&#20010;&#25361;&#25112;&#30340;&#32467;&#21512;&#23588;&#20854;&#30456;&#20851;&#65292;&#22240;&#20026;&#26032;&#20986;&#29616;&#30340;&#31867;&#21035;&#36890;&#24120;&#21482;&#21344;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#21152;&#21095;&#20102;&#21407;&#26412;&#23601;&#20559;&#26012;&#30340;&#31867;&#21035;&#20998;&#24067;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20960;&#39033;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#25968;&#37327;&#23545;&#32467;&#26524;&#27809;&#26377;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#32456;&#36523;&#23398;&#20064;&#30340;&#20851;&#38190;&#21069;&#25552;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26497;&#23569;&#37327;&#30340;&#26631;&#27880;&#33410;&#28857;&#23601;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gDOC&#26041;&#27861;&#65292;&#22312;&#19981;&#24179;&#34913;&#31867;&#21035;&#20998;&#24067;&#30340;&#32422;&#26463;&#19979;&#26816;&#27979;&#26032;&#31867;&#21035;&#12290;&#20851;&#38190;&#30340;&#37096;&#20998;&#26159;&#32771;&#34385;&#21040;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#21152;&#26435;&#20108;&#20803;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#12290;&#31532;&#22235;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;gDOC+&#26041;&#27861;&#65292;&#23558;gDOC&#19982;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30456;&#32467;&#21512;&#65292;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#26377;&#38480;&#30340;&#26631;&#27880;&#25968;&#25454;&#26816;&#27979;&#26032;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28436;&#21270;&#22270;&#30340;&#32456;&#36523;&#23398;&#20064;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22810;&#20010;&#22522;&#32447;&#21644;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong graph learning deals with the problem of continually adapting graph neural network (GNN) models to changes in evolving graphs. We address two critical challenges of lifelong graph learning in this work: dealing with new classes and tackling imbalanced class distributions. The combination of these two challenges is particularly relevant since newly emerging classes typically resemble only a tiny fraction of the data, adding to the already skewed class distribution. We make several contributions: First, we show that the amount of unlabeled data does not influence the results, which is an essential prerequisite for lifelong learning on a sequence of tasks. Second, we experiment with different label rates and show that our methods can perform well with only a tiny fraction of annotated nodes. Third, we propose the gDOC method to detect new classes under the constraint of having an imbalanced class distribution. The critical ingredient is a weighted binary cross-entropy loss functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#37096;&#20998;&#21382;&#21490;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#20272;&#31639;&#22120;&#65292;&#21487;&#19968;&#33268;&#22320;&#20272;&#35745;&#30446;&#26631;&#31574;&#30053;&#30340;&#31283;&#24577;&#24179;&#22343;&#22238;&#25253;&#65292;&#22312;POMDP&#20013;&#36827;&#34892;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#27604;&#22312;&#65288;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20294;&#27604;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#33258;&#30001;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#26356;&#23481;&#26131;&#12290;</title><link>http://arxiv.org/abs/2110.12343</link><description>&lt;p&gt;
&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Evaluation in Partially Observed Markov Decision Processes under Sequential Ignorability. (arXiv:2110.12343v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22522;&#20110;&#37096;&#20998;&#21382;&#21490;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#30340;&#20272;&#31639;&#22120;&#65292;&#21487;&#19968;&#33268;&#22320;&#20272;&#35745;&#30446;&#26631;&#31574;&#30053;&#30340;&#31283;&#24577;&#24179;&#22343;&#22238;&#25253;&#65292;&#22312;POMDP&#20013;&#36827;&#34892;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#27604;&#22312;&#65288;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20294;&#27604;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#33258;&#30001;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#26356;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30528;&#30524;&#20110;&#21160;&#24577;&#27835;&#30103;&#35268;&#21017;&#19979;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#20013;&#30340;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#65292;&#22522;&#20110;&#20551;&#35774;&#31995;&#32479;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#31639;&#22120;&#65292;&#21363;&#22522;&#20110;&#37096;&#20998;&#21382;&#21490;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#20174;&#34892;&#20026;&#31574;&#30053;&#20013;&#33719;&#24471;&#36275;&#22815;&#22810;&#30340;&#25968;&#25454;&#21518;&#65292;&#35813;&#20272;&#31639;&#22120;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#30446;&#26631;&#31574;&#30053;&#30340;&#31283;&#24577;&#24179;&#22343;&#22238;&#25253;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20272;&#31639;&#22120;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#20854;&#22810;&#39033;&#24335;&#34928;&#20943;&#20110;&#35266;&#27979;&#37327;&#30340;&#25968;&#37327;&#65288;&#21363;&#36712;&#36857;&#30340;&#25968;&#37327;&#20056;&#20197;&#23427;&#20204;&#30340;&#38271;&#24230;&#65289;&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#30446;&#26631;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#30340;&#37325;&#21472;&#65292;&#20197;&#21450;&#22522;&#30784;&#31995;&#32479;&#30340;&#28151;&#21512;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#25910;&#25947;&#36895;&#29575;&#22312;&#28151;&#21512;&#21644;&#37325;&#21472;&#26041;&#38754;&#30340;&#20551;&#23450;&#19979;&#26159;&#26497;&#23567;&#21270;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;POMDP&#20013;&#36827;&#34892;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#27604;&#22312;&#65288;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#65289;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#36827;&#34892;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#20294;&#27604;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#33258;&#30001;&#20559;&#24046;&#25511;&#21046;&#24335;&#35780;&#20272;&#26356;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider off-policy evaluation of dynamic treatment rules under sequential ignorability, given an assumption that the underlying system can be modeled as a partially observed Markov decision process (POMDP). We propose an estimator, partial history importance weighting, and show that it can consistently estimate the stationary mean rewards of a target policy given long enough draws from the behavior policy. We provide an upper bound on its error that decays polynomially in the number of observations (i.e., the number of trajectories times their length), with an exponent that depends on the overlap of the target and behavior policies, and on the mixing time of the underlying system. Furthermore, we show that this rate of convergence is minimax given only our assumptions on mixing and overlap. Our results establish that off-policy evaluation in POMDPs is strictly harder than off-policy evaluation in (fully observed) Markov decision processes, but strictly easier than model-free off-po
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#26045;RBF&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#37325;&#21472;&#26102;&#38388;&#30340;&#27010;&#24565;&#26469;&#20248;&#21270;&#24658;&#27969;&#24182;&#32852;&#26426;&#26800;&#20301;&#31227;&#24494;&#22411;&#27893;&#65292;&#22312;&#23558;&#24038;&#21491;&#27893;&#20114;&#25442;&#35282;&#33394;&#30340;&#24448;&#22797;&#36816;&#21160;&#26399;&#38388;&#26368;&#23567;&#21270;&#21387;&#21147;&#33033;&#20914;&#12290;</title><link>http://arxiv.org/abs/2109.08717</link><description>&lt;p&gt;
&#20351;&#29992;RBF&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#24658;&#27969;&#24182;&#32852;&#24494;&#22411;&#27893;
&lt;/p&gt;
&lt;p&gt;
The Optimization of the Constant Flow Parallel Micropump Using RBF Neural Network. (arXiv:2109.08717v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.08717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#26045;RBF&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20986;&#20102;&#37325;&#21472;&#26102;&#38388;&#30340;&#27010;&#24565;&#26469;&#20248;&#21270;&#24658;&#27969;&#24182;&#32852;&#26426;&#26800;&#20301;&#31227;&#24494;&#22411;&#27893;&#65292;&#22312;&#23558;&#24038;&#21491;&#27893;&#20114;&#25442;&#35282;&#33394;&#30340;&#24448;&#22797;&#36816;&#21160;&#26399;&#38388;&#26368;&#23567;&#21270;&#21387;&#21147;&#33033;&#20914;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20248;&#21270;&#20855;&#26377;&#24182;&#32852;&#27893;&#33108;&#21644;&#34987;&#21160;&#27490;&#22238;&#38400;&#30340;&#24658;&#27969;&#24182;&#32852;&#26426;&#26800;&#20301;&#31227;&#24494;&#22411;&#27893;&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#20219;&#21153;&#26159;&#22312;&#24038;&#21491;&#27893;&#20114;&#25442;&#21560;&#20837;&#21644;&#36755;&#36865;&#35282;&#33394;&#26102;&#30340;&#24448;&#22797;&#36816;&#21160;&#26399;&#38388;&#65292;&#23558;&#30001;&#21453;&#27969;&#24341;&#36215;&#30340;&#21387;&#21147;&#33033;&#20914;&#26368;&#23567;&#21270;&#65292;&#36825;&#23545;&#31283;&#23450;&#30340;&#24658;&#27969;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#23581;&#35797;&#36890;&#36807;&#34987;&#21160;&#27490;&#22238;&#38400;&#30340;&#26426;&#26800;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#37325;&#21472;&#26102;&#38388;&#30340;&#26032;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#23454;&#26045;RBF&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#20174;&#25511;&#21046;&#29702;&#35770;&#35282;&#24230;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20351;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#23545;&#20854;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21387;&#21147;&#33033;&#20914;&#22312;0.15-0.25 MPa&#30340;&#33539;&#22260;&#20869;&#24471;&#21040;&#20102;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#26368;&#22823;&#27893;&#24037;&#20316;&#21387;&#21147;40 MPa&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this work is to optimize the performance of a constant flow parallel mechanical displacement micropump, which has parallel pump chambers and incorporates passive check valves. The critical task is to minimize the pressure pulse caused by regurgitation, which negatively impacts the constant flow rate, during the reciprocating motion when the left and right pumps interchange their role of aspiration and transfusion. Previous works attempt to solve this issue via the mechanical design of passive check valves. In this work, the novel concept of overlap time is proposed, and the issue is solved from the aspect of control theory by implementing a RBF neural network trained by both unsupervised and supervised learning. The experimental results indicate that the pressure pulse is optimized in the range of 0.15 - 0.25 MPa, which is a significant improvement compared to the maximum pump working pressure of 40 MPa.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#20302;&#19988;&#39640;&#31934;&#24230;&#30340;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#35782;&#21035;COVID-19&#24739;&#32773;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2108.05660</link><description>&lt;p&gt;
&#21033;&#29992;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#24320;&#21457;&#26080;&#39118;&#38505;&#30340;COVID-19&#31579;&#26597;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning. (arXiv:2108.05660v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#20302;&#19988;&#39640;&#31934;&#24230;&#30340;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#35782;&#21035;COVID-19&#24739;&#32773;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#36716;&#24405;&#32858;&#21512;&#37238;&#38142;&#21453;&#24212;&#65288;RT-PCR&#65289;&#26159;&#36776;&#21035;COVID-19&#24863;&#26579;&#30340;&#38134;&#24377;&#35786;&#26029;&#26816;&#27979;&#12290;&#24555;&#36895;&#25239;&#21407;&#26816;&#27979;&#26159;&#19968;&#31181;&#31579;&#26597;&#26816;&#27979;&#65292;&#21487;&#22312;15&#20998;&#38047;&#20869;&#35782;&#21035;COVID-19&#38451;&#24615;&#24739;&#32773;&#65292;&#20294;&#20854;&#28789;&#25935;&#24230;&#20302;&#20110;PCR&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;COVID-19&#24739;&#32773;&#20813;&#30123;&#21644;&#34880;&#28082;&#23398;&#36164;&#26009;&#30340;&#21442;&#25968;&#20559;&#24046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#20302;&#19988;&#39640;&#31934;&#24230;&#30340;&#22534;&#21472;&#38598;&#25104;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#24120;&#35268;&#34880;&#28082;&#26816;&#26597;&#20013;&#35782;&#21035;COVID-19&#24739;&#32773;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Reverse Transcription Polymerase Chain Reaction (RTPCR)} test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected and either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological and hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;$M^3-UCRL$&#65292;&#22312;&#26410;&#30693;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#38382;&#39064;&#27714;&#35299;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2107.04050</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2107.04050v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;$M^3-UCRL$&#65292;&#22312;&#26410;&#30693;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#21487;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#24182;&#23454;&#29616;&#20102;&#21487;&#35777;&#26126;&#30340;&#38382;&#39064;&#27714;&#35299;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23398;&#20064;&#20805;&#28385;&#25361;&#25112;&#65292;&#21253;&#25324;&#26234;&#33021;&#20307;&#30456;&#20114;&#20316;&#29992;&#25152;&#24341;&#20837;&#30340;&#38750;&#31283;&#24577;&#21644;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#32452;&#21512;&#24615;&#36136;&#31561;&#22810;&#20010;&#22240;&#32032;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20551;&#35774;&#23384;&#22312;&#26080;&#38480;&#25968;&#37327;&#30340;&#30456;&#21516;&#26234;&#33021;&#20307;&#65292;&#26088;&#22312;&#20849;&#21516;&#26368;&#22823;&#21270;&#25910;&#30410;&#12290;&#38024;&#23545;&#26410;&#30693;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#24773;&#20917;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;$M^3-UCRL$&#65292;&#22312;&#31574;&#30053;&#23398;&#20064;&#26399;&#38388;&#24179;&#34913;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#24182;&#23454;&#29616;&#20102;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#35777;&#26126;&#27714;&#35299;&#12290;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning in multi-agent systems is highly challenging due to several factors including the non-stationarity introduced by agents' interactions and the combinatorial nature of their state and action spaces. In particular, we consider the Mean-Field Control (MFC) problem which assumes an asymptotically infinite population of identical agents that aim to collaboratively maximize the collective reward. In many cases, solutions of an MFC problem are good approximations for large systems, hence, efficient learning for MFC is valuable for the analogous discrete agent setting with many agents. Specifically, we focus on the case of unknown system dynamics where the goal is to simultaneously optimize for the rewards and learn from experience. We propose an efficient model-based reinforcement learning algorithm, $M^3-UCRL$, that runs in episodes, balances between exploration and exploitation during policy learning, and provably solves this problem. Our main theoretical contributions are the first
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;Blotto&#21338;&#24328;&#20013;&#26377;&#38480;&#36164;&#28304;&#30340;&#25112;&#30053;&#20998;&#37197;&#38382;&#39064;&#65292;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#21644;&#24102;&#32972;&#21253;&#21644;&#32452;&#21512;&#30340;&#36172;&#21338;&#26426;&#20998;&#26512;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2103.12833</link><description>&lt;p&gt;
&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#21160;&#24577;Blotto&#21338;&#24328;&#22312;&#32447;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Learning in Budget-Constrained Dynamic Colonel Blotto Games. (arXiv:2103.12833v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.12833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21160;&#24577;Blotto&#21338;&#24328;&#20013;&#26377;&#38480;&#36164;&#28304;&#30340;&#25112;&#30053;&#20998;&#37197;&#38382;&#39064;&#65292;&#20351;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#21644;&#24102;&#32972;&#21253;&#21644;&#32452;&#21512;&#30340;&#36172;&#21338;&#26426;&#20998;&#26512;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#19979;&#20351;&#29992;Blotto&#21338;&#24328;(CBG)&#23545;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#25112;&#30053;&#20998;&#37197;&#65292;&#24182;&#37319;&#29992;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#20998;&#26512;&#20102;&#35813;&#38382;&#39064;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#19968;&#26041;&#26159;&#25317;&#26377;&#26377;&#38480;&#20891;&#38431;&#30340;&#23398;&#20064;&#32773;&#65292;&#21478;&#19968;&#26041;&#26159;&#23545;&#25163;&#12290;&#22312;&#27599;&#36718;&#20013;&#65292;&#23398;&#20064;&#32773;&#19982;&#23545;&#25163;&#36827;&#34892;&#21333;&#27425;Blotto&#21338;&#24328;&#65292;&#24182;&#26681;&#25454;&#36807;&#21435;&#30340;&#35266;&#23519;&#32467;&#26524;&#25112;&#30053;&#24615;&#22320;&#30830;&#23450;&#22312;&#25112;&#22330;&#19978;&#30340;&#20891;&#38431;&#20998;&#37197;&#12290;&#23545;&#25163;&#38543;&#26426;&#36873;&#25321;&#20854;&#20998;&#37197;&#34892;&#21160;&#65292;&#20854;&#20998;&#24067;&#23545;&#23398;&#20064;&#32773;&#19981;&#21487;&#30693;&#12290;&#23398;&#20064;&#32773;&#30340;&#30446;&#26631;&#26159;&#22312;&#19981;&#36829;&#21453;&#39044;&#31639;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36981;&#24490;&#23398;&#20064;&#31639;&#27861;&#23454;&#29616;&#30340;&#32047;&#31215;&#22870;&#21169;&#19982;&#26368;&#20339;&#28151;&#21512;&#31574;&#30053;&#30340;&#32047;&#31215;&#22870;&#21169;&#20043;&#38388;&#30340;&#24046;&#24322;&#26368;&#23567;&#21270;&#12290; &#22312;&#21160;&#24577;CBG&#20013;&#30340;&#23398;&#20064;&#26159;&#22312;&#32452;&#21512;&#36172;&#21338;&#26426;&#21644;&#32972;&#21253;&#36172;&#21338;&#26426;&#30340;&#26694;&#26550;&#19979;&#20998;&#26512;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the strategic allocation of limited resources using a Colonel Blotto game (CBG) under a dynamic setting and analyze the problem using an online learning approach. In this model, one of the players is a learner who has limited troops to allocate over a finite time horizon, and the other player is an adversary. In each round, the learner plays a one-shot Colonel Blotto game with the adversary and strategically determines the allocation of troops among battlefields based on past observations. The adversary chooses its allocation action randomly from some fixed distribution that is unknown to the learner. The learner's objective is to minimize its regret, which is the difference between the cumulative reward of the best mixed strategy and the realized cumulative reward by following a learning algorithm while not violating the budget constraint. The learning in dynamic CBG is analyzed under the framework of combinatorial bandits and bandits with knapsacks. We first c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#24403;&#38454;&#27573;&#25968;&#22686;&#21152;&#26102;&#65292;&#26576;&#20123;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#30053;&#24494;&#22686;&#21152;&#65292;&#32780;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24230;&#21017;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/1912.07702</link><description>&lt;p&gt;
&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Complexity of Stochastic Dual Dynamic Programming. (arXiv:1912.07702v9 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.07702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#21457;&#29616;&#24403;&#38454;&#27573;&#25968;&#22686;&#21152;&#26102;&#65292;&#26576;&#20123;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#30053;&#24494;&#22686;&#21152;&#65292;&#32780;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#30340;&#22797;&#26434;&#24230;&#21017;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#30340;&#21106;&#24179;&#38754;&#31867;&#22411;&#31639;&#27861;&#65292;&#28304;&#20110;30&#24180;&#21069;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#20219;&#20309;&#20851;&#20110;&#35813;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#25968;&#23398;&#24037;&#20855;&#65292;&#21253;&#25324;&#25628;&#32034;&#28857;&#30340;&#39281;&#21644;&#24615;&#65292;&#20026;&#35299;&#20915;&#30456;&#23545;&#31616;&#21333;&#30340;&#22810;&#38454;&#27573;&#20248;&#21270;&#38382;&#39064;&#30340;&#22522;&#26412;&#21160;&#24577;&#21106;&#24179;&#38754;&#26041;&#27861;&#24314;&#31435;&#20102;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#21363;&#36845;&#20195;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#31934;&#21270;&#36825;&#20123;&#22522;&#26412;&#24037;&#20855;&#65292;&#38024;&#23545;&#26631;&#20934;&#38454;&#27573;&#29420;&#31435;&#24615;&#20551;&#35774;&#19979;&#30340;&#26356;&#19968;&#33324;&#30340;&#22810;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#26041;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26576;&#20123;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#22797;&#26434;&#24230;&#38543;&#30528;&#38454;&#27573;&#25968;$T$&#30340;&#22686;&#21152;&#32780;&#36731;&#24494;&#22686;&#21152;&#65292;&#20107;&#23454;&#19978;&#23545;&#20110;&#25240;&#25187;&#38382;&#39064;&#26159;$T$&#30340;&#32447;&#24615;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38543;&#26426;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#23545;&#38454;&#27573;&#25968;&#26377;&#25351;&#25968;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#30830;&#23450;&#24615;&#23545;&#20598;&#21160;&#24577;&#35268;&#21010;&#30340;&#32447;&#24615;&#22686;&#38271;&#30830;&#23454;&#21487;&#33021;&#22312;&#23454;&#36341;&#20013;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic dual dynamic programming is a cutting plane type algorithm for multi-stage stochastic optimization originated about 30 years ago. In spite of its popularity in practice, there does not exist any analysis on the convergence rates of this method. In this paper, we first establish the number of iterations, i.e., iteration complexity, required by a basic dynamic cutting plane method for solving relatively simple multi-stage optimization problems, by introducing novel mathematical tools including the saturation of search points. We then refine these basic tools and establish the iteration complexity for both deterministic and stochastic dual dynamic programming methods for solving more general multi-stage stochastic optimization problems under the standard stage-wise independence assumption. Our results indicate that the complexity of some deterministic variants of these methods mildly increases with the number of stages $T$, in fact linearly dependent on $T$ for discounted probl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26680;Stein&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/1907.00586</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#26680;Stein&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
A Kernel Stein Test for Comparing Latent Variable Models. (arXiv:1907.00586v5 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1907.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26680;Stein&#26816;&#39564;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#28508;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#30340;&#38750;&#21442;&#25968;&#25311;&#21512;&#20248;&#24230;&#26816;&#39564;&#65292;&#26088;&#22312;&#27604;&#36739;&#20004;&#20010;&#27169;&#22411;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#21487;&#33021;&#20855;&#26377;&#26410;&#35266;&#27979;&#30340;&#28508;&#21464;&#37327;&#65292;&#19988;&#35266;&#27979;&#21464;&#37327;&#30340;&#36793;&#32536;&#20998;&#24067;&#19981;&#21487;&#35266;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#23558;&#26368;&#36817;&#25552;&#20986;&#30340;&#26680;Stein&#36317;&#31163;(KSD)&#26816;&#39564;(Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018)&#25512;&#24191;&#21040;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#26159;&#19968;&#20010;&#27604;&#20043;&#21069;&#22788;&#29702;&#30340;&#20840;&#35266;&#27979;&#27169;&#22411;&#26356;&#21152;&#36890;&#29992;&#30340;&#31867;&#21035;&#12290;&#26032;&#30340;&#26816;&#39564;&#36890;&#36807;&#36866;&#24403;&#30340;&#26657;&#20934;&#38408;&#20540;&#65292;&#24471;&#21040;&#33391;&#22909;&#25511;&#21046;&#30340;&#19968;&#31867;&#38169;&#35823;&#12290;&#23545;&#20110;&#26576;&#20123;&#20855;&#26377;&#20302;&#32500;&#28508;&#22312;&#32467;&#26500;&#21644;&#39640;&#32500;&#35266;&#27979;&#30340;&#27169;&#22411;&#65292;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#20013;&#65292;&#26174;&#33879;&#20248;&#20110;&#22522;&#20110;&#27169;&#22411;&#26679;&#26412;&#21644;&#19981;&#21033;&#29992;&#28508;&#22312;&#32467;&#26500;&#30340;&#30456;&#23545;&#26368;&#22823;&#24179;&#22343;&#36317;&#31163;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a kernel-based nonparametric test of relative goodness of fit, where the goal is to compare two models, both of which may have unobserved latent variables, such that the marginal distribution of the observed variables is intractable. The proposed test generalizes the recently proposed kernel Stein discrepancy (KSD) tests (Liu et al., 2016, Chwialkowski et al., 2016, Yang et al., 2018) to the case of latent variable models, a much more general class than the fully observed models treated previously. The new test, with a properly calibrated threshold, has a well-controlled type-I error. In the case of certain models with low-dimensional latent structure and high-dimensional observations, our test significantly outperforms the relative Maximum Mean Discrepancy test, which is based on samples from the models and does not exploit the latent structure.
&lt;/p&gt;</description></item></channel></rss>