<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2401.03350</link><description>&lt;p&gt;
&#20934;&#30830;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2401.03350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03350
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#29992;&#20110;&#33410;&#28857;&#21644;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;GNN&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#20173;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#20107;&#23454;&#19978;&#65292;&#34429;&#28982;&#20107;&#21518;&#26657;&#20934;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20869;&#37096;&#20998;&#24067;&#26657;&#20934;&#65292;&#20294;&#23427;&#20204;&#19981;&#19968;&#23450;&#20063;&#33021;&#25913;&#36827;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20869;&#37096;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#25216;&#26415;&#23588;&#20854;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38543;&#21518;&#19982;&#20107;&#21518;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;G-$\Delta$UQ&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20869;&#22312;&#30340;GNN&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#26032;&#39062;&#30340;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#21407;&#21017;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;&#34429;&#28982;&#20027;&#27969;&#35266;&#28857;&#26159;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#20272;&#35745;&#65292;&#38656;&#35201;&#23436;&#20840;&#38543;&#26426;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#21151;&#33021;&#22810;&#26679;&#24615;&#24341;&#20837;&#30340;&#20013;&#35266;&#38170;&#23450;&#21487;&#20197;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35299;&#27010;&#29575;&#27169;&#22411;&#31934;&#30830;&#35745;&#31639;&#32422;&#26463;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#19968;&#20449;&#21495;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#22270;&#20687;&#20462;&#22797;&#30340;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03349</link><description>&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#36890;&#36807;&#21487;&#25511;&#25193;&#25955;&#27169;&#22411;&#30340;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Image Inpainting via Tractable Steering of Diffusion Models. (arXiv:2401.03349v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#35299;&#27010;&#29575;&#27169;&#22411;&#31934;&#30830;&#35745;&#31639;&#32422;&#26463;&#21518;&#39564;&#30340;&#26041;&#27861;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#19968;&#20449;&#21495;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#65292;&#20174;&#32780;&#25913;&#36827;&#22270;&#20687;&#20462;&#22797;&#30340;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#32422;&#26463;&#30340;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#20462;&#22797;&#65292;&#25511;&#21046;&#25277;&#26679;&#36807;&#31243;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23545;&#36825;&#20123;&#32422;&#26463;&#30340;&#31934;&#30830;&#26465;&#20214;&#35774;&#23450;&#26159;&#19981;&#21487;&#35299;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#21487;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;(TPMs)&#30340;&#33021;&#21147;&#26469;&#31934;&#30830;&#19988;&#26377;&#25928;&#22320;&#35745;&#31639;&#21463;&#32422;&#26463;&#30340;&#21518;&#39564;&#65292;&#24182;&#21033;&#29992;&#35813;&#20449;&#21495;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#21435;&#22122;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#31867;&#34920;&#36798;&#21147;&#36739;&#24378;&#30340;TPMs&#65292;&#31216;&#20026;&#27010;&#29575;&#30005;&#36335;(PCs)&#12290;&#22522;&#20110;&#20808;&#21069;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#22823;&#20102;PCs&#30340;&#35268;&#27169;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#19977;&#20010;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#21363;CelebA-H&#65289;&#20013;&#25345;&#32493;&#25913;&#36827;&#20462;&#22797;&#22270;&#20687;&#30340;&#25972;&#20307;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are the current state of the art for generating photorealistic images. Controlling the sampling process for constrained image generation tasks such as inpainting, however, remains challenging since exact conditioning on such constraints is intractable. While existing methods use various techniques to approximate the constrained posterior, this paper proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to exactly and efficiently compute the constrained posterior, and to leverage this signal to steer the denoising process of diffusion models. Specifically, this paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs). Building upon prior advances, we further scale up PCs and make them capable of guiding the image generation process of diffusion models. Empirical results suggest that our approach can consistently improve the overall quality and semantic coherence of inpainted images across three natural image datasets (i.e., CelebA-H
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#32780;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20316;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.03346</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#32780;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#20316;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#26377;&#25928;&#25552;&#31034;&#36825;&#20123;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24694;&#24847;&#35328;&#35770;&#24050;&#32463;&#25104;&#20026;&#22256;&#25200;&#25105;&#20204;&#31038;&#20132;&#31354;&#38388;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#19978;&#24050;&#32463;&#20570;&#20986;&#20102;&#19968;&#20123;&#26174;&#33879;&#30340;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#25928;&#26816;&#27979;&#22312;&#32447;&#24694;&#24847;&#35328;&#35770;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#20010;&#39640;&#24230;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#24694;&#24847;&#35328;&#35770;&#30340;&#19978;&#19979;&#25991;&#20197;&#36827;&#34892;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;LLMs&#32463;&#36807;&#22823;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35757;&#32451;&#65292;&#20351;&#20854;&#33021;&#22815;&#25484;&#25569;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30693;&#35782;&#24211;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;LLMs&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#27809;&#26377;&#20851;&#20110;&#26377;&#25928;&#25552;&#31034;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30740;&#31350;&#65292;&#35843;&#26597;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#24341;&#36215;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#19981;&#36830;&#32493;&#24615;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03341</link><description>&lt;p&gt;
&#24369;&#22686;&#24378;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection. (arXiv:2401.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#30001;&#20110;&#25968;&#25454;&#31232;&#32570;&#24341;&#36215;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#19981;&#36830;&#32493;&#24615;&#23548;&#33268;&#30340;&#37325;&#24314;&#19981;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#26080;&#30417;&#30563;&#35757;&#32451;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#28145;&#24230;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#24050;&#32463;&#25104;&#20026;&#22522;&#20110;&#37325;&#24314;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;VAE&#30340;TSAD&#26041;&#27861;&#65292;&#19981;&#35770;&#26159;&#32479;&#35745;&#26041;&#27861;&#36824;&#26159;&#28145;&#24230;&#26041;&#27861;&#65292;&#37117;&#35843;&#25972;&#20803;&#20808;&#39564;&#20197;&#20272;&#35745;&#26377;&#25928;&#25429;&#33719;&#25968;&#25454;&#20013;&#30340;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#20284;&#28982;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#30528;&#20869;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#36825;&#22312;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;&#36825;&#31181;&#31232;&#32570;&#23481;&#26131;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28508;&#22312;&#31354;&#27934;&#65292;&#21363;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#19981;&#36830;&#32493;&#21306;&#22495;&#65292;&#23548;&#33268;&#22312;&#36825;&#20123;&#19981;&#36830;&#32493;&#30340;&#31354;&#38388;&#19978;&#30340;&#38750;&#40065;&#26834;&#37325;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;VAEs&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#32467;&#21512;&#30340;&#26032;&#39062;&#29983;&#25104;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detection tasks. Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22825;&#25991;&#23398;&#20013;&#32852;&#21512;&#25552;&#21462;&#20809;&#35889;&#21644;&#28304;&#25968;&#20998;&#24067;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2401.03336</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22312;&#22825;&#25991;&#23398;&#20013;&#32852;&#21512;&#25552;&#21462;&#20809;&#35889;&#21644;&#28304;&#25968;&#20998;&#24067;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A deep learning framework for jointly extracting spectra and source-count distributions in astronomy. (arXiv:2401.03336v1 [astro-ph.IM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03336
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22825;&#25991;&#23398;&#20013;&#32852;&#21512;&#25552;&#21462;&#20809;&#35889;&#21644;&#28304;&#25968;&#20998;&#24067;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#25991;&#35266;&#27979;&#36890;&#24120;&#25552;&#20379;&#19977;&#32500;&#22320;&#22270;&#65292;&#32534;&#30721;&#20102;&#35266;&#27979;&#27969;&#37327;&#22312;(1)&#22825;&#29699;&#30340;&#20004;&#20010;&#35282;&#24230;&#21644;(2)&#33021;&#37327;/&#39057;&#29575;&#19978;&#30340;&#20998;&#24067;&#12290;&#20851;&#20110;&#36825;&#31181;&#22320;&#22270;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#26159;&#32479;&#35745;&#22320;&#34920;&#24449;&#37027;&#20123;&#22826;&#26263;&#20197;&#33267;&#20110;&#26080;&#27861;&#20010;&#21035;&#26816;&#27979;&#21040;&#30340;&#28857;&#28304;&#30340;&#31181;&#32676;&#12290;&#30001;&#20110;&#23545;&#21333;&#20010;&#20809;&#24369;&#28304;&#30340;&#24615;&#36136;&#20102;&#35299;&#26377;&#38480;&#65292;&#36890;&#24120;&#25105;&#20204;&#20250;&#23558;&#25972;&#20010;&#31181;&#32676;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#36827;&#34892;&#30740;&#31350;&#65292;&#25512;&#26029;&#20986;&#19968;&#20010;&#28304;&#25968;&#20998;&#24067; (SCD)&#65292;&#35813;&#20998;&#24067;&#25551;&#36848;&#20102;&#28304;&#30340;&#25968;&#23494;&#24230;&#38543;&#20142;&#24230;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#34429;&#28982;&#23384;&#22312;&#29992;&#20110;&#24674;&#22797; SCD &#30340;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23436;&#20840;&#24573;&#30053;&#20102;&#19982;&#27969;&#37327;&#33021;&#35889;&#30456;&#20851;&#30340;&#20809;&#35889;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#37325;&#26500;&#19981;&#21516;&#21457;&#23556;&#25104;&#20998;&#30340;&#20809;&#35889;&#21644;&#28857;&#28304;&#31181;&#32676;&#30340;SCD&#12290;&#22312;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#30340;&#20363;&#23376;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#20934;&#30830;&#25552;&#21462;&#22797;&#26434;&#24418;&#29366;&#30340;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Astronomical observations typically provide three-dimensional maps, encoding the distribution of the observed flux in (1) the two angles of the celestial sphere and (2) energy/frequency. An important task regarding such maps is to statistically characterize populations of point sources too dim to be individually detected. As the properties of a single dim source will be poorly constrained, instead one commonly studies the population as a whole, inferring a source-count distribution (SCD) that describes the number density of sources as a function of their brightness. Statistical and machine learning methods for recovering SCDs exist; however, they typically entirely neglect spectral information associated with the energy distribution of the flux. We present a deep learning framework able to jointly reconstruct the spectra of different emission components and the SCD of point-source populations. In a proof-of-concept example, we show that our method accurately extracts even complex-shape
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20809;&#35889;&#21512;&#25104;&#22270;&#20687;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26680;&#26691;&#26816;&#27979;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03331</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20809;&#35889;&#21512;&#25104;&#22270;&#20687;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#26680;&#26691;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Walnut Detection Through Deep Learning Enhanced by Multispectral Synthetic Images. (arXiv:2401.03331v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20809;&#35889;&#21512;&#25104;&#22270;&#20687;&#22686;&#24378;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#26680;&#26691;&#26816;&#27979;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26816;&#27979;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#37492;&#23450;&#26524;&#22253;&#20869;&#30340;&#26680;&#26691;&#31181;&#26893;&#24102;&#26469;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#26497;&#22823;&#25552;&#39640;&#20102;&#26680;&#26691;&#26524;&#22253;&#31649;&#29702;&#30340;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#28982;&#32780;&#65292;&#26680;&#26691;&#26641;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21363;&#26680;&#26691;&#19982;&#21494;&#23376;&#20043;&#38388;&#30340;&#24418;&#29366;&#12289;&#39068;&#33394;&#21644;&#32441;&#29702;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20351;&#20854;&#22312;&#27880;&#37322;&#36807;&#31243;&#20013;&#31934;&#30830;&#21306;&#20998;&#23427;&#20204;&#25104;&#20026;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26680;&#26691;&#26816;&#27979;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#22270;&#20687;&#38598;&#20013;&#35757;&#32451;&#30340;YOLOv5&#65292;&#35813;&#38598;&#21512;&#21253;&#25324;&#20102;&#30495;&#23454;&#21644;&#21512;&#25104;&#30340;RGB&#21644;NIR&#22270;&#20687;&#12290;&#25105;&#20204;&#23545;&#21407;&#22987;&#25968;&#25454;&#38598;&#21644;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#21512;&#25104;&#22270;&#20687;&#26102;&#26816;&#27979;&#25928;&#26524;&#26126;&#26174;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate identification of walnuts within orchards brings forth a plethora of advantages, profoundly amplifying the efficiency and productivity of walnut orchard management. Nevertheless, the unique characteristics of walnut trees, characterized by their closely resembling shapes, colors, and textures between the walnuts and leaves, present a formidable challenge in precisely distinguishing between them during the annotation process. In this study, we present a novel approach to improve walnut detection efficiency, utilizing YOLOv5 trained on an enriched image set that incorporates both real and synthetic RGB and NIR images. Our analysis comparing results from our original and augmented datasets shows clear improvements in detection when using the synthetic images.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#31383;&#21475;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03322</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#26080;&#30417;&#30563;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection. (arXiv:2401.03322v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#30340;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#36890;&#36807;&#32467;&#21512;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#31383;&#21475;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#26080;&#30417;&#30563;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#12290;&#33258;&#32534;&#30721;&#22120;&#25429;&#25417;&#30701;&#23884;&#20837;&#20013;&#30340;&#23616;&#37096;&#32467;&#26500;&#27169;&#24335;&#65292;&#32780;&#27880;&#24847;&#21147;&#27169;&#22411;&#23398;&#20064;&#38271;&#26399;&#29305;&#24449;&#65292;&#36890;&#36807;&#20301;&#32622;&#32534;&#30721;&#23454;&#29616;&#24182;&#34892;&#35745;&#31639;&#12290;&#22312;&#26041;&#27861;&#19978;&#29420;&#29305;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#27169;&#22411;&#39318;&#27425;&#22312;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#32467;&#21512;&#20102;&#27880;&#24847;&#21147;&#21644;&#33258;&#32534;&#30721;&#22120;&#12290;&#23427;&#37319;&#29992;&#20102;&#31867;&#20284;&#28145;&#24230;&#21464;&#25442;&#22120;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#23545;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39044;&#27979;&#19979;&#19968;&#20010;&#26102;&#38388;&#27493;&#39588;&#31383;&#21475;&#36827;&#34892;&#20102;&#20851;&#38190;&#26550;&#26500;&#20462;&#25913;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#39564;&#35777;&#25968;&#25454;&#38598;&#20013;&#30340;&#38408;&#20540;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#26512;&#35823;&#24046;&#30340;&#31532;&#19968;&#32479;&#35745;&#30697;&#26041;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#32780;&#19981;&#20381;&#36182;&#20110;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22810;&#26679;&#30340;&#23454;&#38469;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#19982;&#20854;&#20182;&#24191;&#20026;&#35748;&#21487;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#35777;&#23454;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a hybrid attention and autoencoder (AE) model for unsupervised online anomaly detection in time series. The autoencoder captures local structural patterns in short embeddings, while the attention model learns long-term features, facilitating parallel computing with positional encoding. Unique in its approach, our proposed hybrid model combines attention and autoencoder for the first time in time series anomaly detection. It employs an attention-based mechanism, akin to the deep transformer model, with key architectural modifications for predicting the next time step window in the autoencoder's latent space. The model utilizes a threshold from the validation dataset for anomaly detection and introduces an alternative method based on analyzing the first statistical moment of error, improving accuracy without dependence on a validation dataset. Evaluation on diverse real-world benchmark datasets and comparing with other well-established models, confirms the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24494;&#26381;&#21153;&#35843;&#29992;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#27169;&#22411;&#22312;&#20943;&#23567;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#21103;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.03319</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#22797;&#21046;&#22312;&#20113;&#20013;&#30340;&#35843;&#29992;&#29575;&#39044;&#27979;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparison of Microservice Call Rate Predictions for Replication in the Cloud. (arXiv:2401.03319v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32447;&#24615;&#22238;&#24402;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#24494;&#26381;&#21153;&#35843;&#29992;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#27169;&#22411;&#22312;&#20943;&#23567;&#35823;&#24046;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#21103;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#29992;&#25143;&#22312;&#19968;&#32676;&#20113;&#26426;&#22120;&#19978;&#37096;&#32626;&#22522;&#20110;&#24494;&#26381;&#21153;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#36825;&#20123;&#24212;&#29992;&#31243;&#24207;&#20855;&#26377;&#21508;&#31181;&#30456;&#20114;&#36830;&#25509;&#65292;&#24182;&#21463;&#21040;&#21160;&#24577;&#29992;&#25143;&#38656;&#27714;&#30340;&#38543;&#26426;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#24494;&#26381;&#21153;&#26102;&#38388;&#39044;&#27979;&#24494;&#26381;&#21153;&#35843;&#29992;&#29575;&#65292;&#24182;&#26088;&#22312;&#20272;&#35745;&#21487;&#25193;&#23637;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#38463;&#37324;&#24052;&#24052;&#30340;&#24494;&#26381;&#21153;&#36319;&#36394;&#25968;&#25454;&#19978;&#24212;&#29992;&#20102;&#32447;&#24615;&#22238;&#24402;&#65288;LR&#65289;&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;&#26799;&#24230;&#25552;&#21319;&#22238;&#24402;&#65288;GBR&#65289;&#27169;&#22411;&#12290;&#39044;&#27979;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;GBR&#21644;MLP&#27169;&#22411;&#30456;&#27604;&#65292;LR&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#28982;&#32780;&#65292;&#19982;LR&#21644;MLP&#27169;&#22411;&#30456;&#27604;&#65292;GBR&#27169;&#22411;&#38477;&#20302;&#20102;&#22343;&#26041;&#24046;&#21644;&#24179;&#22343;&#32477;&#23545;&#30334;&#20998;&#27604;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#39044;&#27979;&#32467;&#26524;&#26174;&#31034;&#65292;&#26799;&#24230;&#25552;&#21319;&#27169;&#22411;&#23545;&#27599;&#20010;&#24494;&#26381;&#21153;&#25152;&#38656;&#30340;&#21103;&#26412;&#25968;&#37327;&#19982;&#23454;&#38469;&#27979;&#35797;&#25968;&#25454;&#38750;&#24120;&#25509;&#36817;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Today, many users deploy their microservice-based applications with various interconnections on a cluster of Cloud machines, subject to stochastic changes due to dynamic user requirements. To address this problem, we compare three machine learning (ML) models for predicting the microservice call rates based on the microservice times and aiming at estimating the scalability requirements. We apply the linear regression (LR), multilayer perception (MLP), and gradient boosting regression (GBR) models on the Alibaba microservice traces. The prediction results reveal that the LR model reaches a lower training time than the GBR and MLP models. However, the GBR reduces the mean absolute error and the mean absolute percentage error compared to LR and MLP models. Moreover, the prediction results show that the required number of replicas for each microservice by the gradient boosting model is close to the actual test data without any prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#22686;&#24378;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#22686;&#21152;&#25968;&#25454;&#25110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2401.03314</link><description>&lt;p&gt;
&#25552;&#21319;&#23545;&#27604;&#24230;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19978;&#19979;&#25991;&#22686;&#24378;&#26469;&#25552;&#21319;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#22686;&#21152;&#25968;&#25454;&#25110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21463;&#30410;&#20110;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#20351;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30446;&#26631;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#36825;&#31181;&#34920;&#31034;&#30340;&#22823;&#24133;&#36827;&#23637;&#12290;&#35821;&#35328;&#24314;&#27169;&#30340;&#20381;&#36182;&#24615;&#20351;&#24471;&#22312;&#23398;&#20064;&#34920;&#31034;&#30340;&#36890;&#29992;&#24615;&#21644;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#34429;&#28982;&#23545;&#27604;&#23398;&#20064;&#25913;&#36827;&#20102;&#24615;&#33021;&#65292;&#20294;&#20854;&#25104;&#21151;&#19981;&#33021;&#20165;&#24402;&#22240;&#20110;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#27493;&#39588;&#65292;&#36890;&#36807;&#20351;&#29992;&#24052;&#27931;&#21452;&#32990;&#32974;&#25439;&#22833;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#26159;&#26174;&#24335;&#22320;&#22686;&#21152;&#25968;&#25454;&#65292;&#32780;&#26159;&#23558;&#35821;&#35328;&#35270;&#20026;&#38544;&#21547;&#30340;&#22686;&#24378;&#65292;&#28040;&#38500;&#20102;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20250;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#23884;&#20837;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20219;&#20309;&#19968;&#32452;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation benefits from semantically rich representations. Considerable progress in learning such representations has been achieved by language modelling and mutual information maximization objectives using contrastive learning. The language-dependent nature of language modelling introduces a trade-off between the universality of the learned representations and the model's performance on the language modelling tasks. Although contrastive learning improves performance, its success cannot be attributed to mutual information alone. We propose a novel Context Enhancement step to improve performance on neural machine translation by maximizing mutual information using the Barlow Twins loss. Unlike other approaches, we do not explicitly augment the data but view languages as implicit augmentations, eradicating the risk of disrupting semantic information. Further, our method does not learn embeddings from scratch and can be generalised to any set of pre-trained embeddings. Fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#20540;&#25193;&#23637;&#21644;&#31574;&#30053;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24494;&#35843;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12289;&#31163;&#25955;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#38750;&#31283;&#24577;&#22870;&#21169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03306</link><description>&lt;p&gt;
MOTO: &#31163;&#32447;&#39044;&#35757;&#32451;&#19982;&#22312;&#32447;&#24494;&#35843;&#29992;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#20540;&#25193;&#23637;&#21644;&#31574;&#30053;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#36827;&#34892;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#31163;&#32447;&#21040;&#22312;&#32447;&#24494;&#35843;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#12289;&#31163;&#25955;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#38750;&#31283;&#24577;&#22870;&#21169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#65292;&#20174;&#39640;&#32500;&#35266;&#27979;&#20013;&#36827;&#34892;&#31163;&#32447;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#31163;&#32447;&#26080;&#27169;&#22411;&#26041;&#27861;&#25104;&#21151;&#22320;&#20351;&#29992;&#22312;&#32447;&#24494;&#35843;&#26469;&#25552;&#39640;&#26234;&#33021;&#20307;&#22312;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#19978;&#30340;&#24615;&#33021;&#65292;&#25110;&#32773;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#35299;&#20915;&#30340;&#20219;&#21153;&#22797;&#26434;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#24494;&#35843;&#26041;&#38754;&#20173;&#28982;&#34987;&#20302;&#20272;&#20102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#39640;&#32500;&#39046;&#22495;&#20013;&#19981;&#36866;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24494;&#35843;&#65292;&#21407;&#22240;&#26159;&#20998;&#24067;&#20559;&#31227;&#12289;&#31163;&#25955;&#21160;&#21147;&#23398;&#25968;&#25454;&#21644;&#38750;&#31283;&#24577;&#22870;&#21169;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#20540;&#25193;&#23637;&#21644;&#31574;&#30053;&#27491;&#21017;&#21270;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#37325;&#29992;&#20808;&#21069;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#36890;&#36807;&#25511;&#21046;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#26469;&#38450;&#27490;&#27169;&#22411;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of offline pre-training and online fine-tuning for reinforcement learning from high-dimensional observations in the context of realistic robot tasks. Recent offline model-free approaches successfully use online fine-tuning to either improve the performance of the agent over the data collection policy or adapt to novel tasks. At the same time, model-based RL algorithms have achieved significant progress in sample efficiency and the complexity of the tasks they can solve, yet remain under-utilized in the fine-tuning setting. In this work, we argue that existing model-based offline RL methods are not suitable for offline-to-online fine-tuning in high-dimensional domains due to issues with distribution shifts, off-dynamics data, and non-stationary rewards. We propose an on-policy model-based method that can efficiently reuse prior data through model-based value expansion and policy regularization, while preventing model exploitation by controlling epistemic uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03302</link><description>&lt;p&gt;
&#34892;&#21160;&#20013;&#30340;&#29616;&#23454;&#20027;&#20041;&#65306;&#20351;&#29992;YOLOv8&#21644;DeiT&#20174;&#21307;&#23398;&#22270;&#20687;&#20013;&#35786;&#26029;&#33041;&#32959;&#30244;&#30340;&#24322;&#24120;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#32597;&#35265;&#24773;&#20917;&#19979;&#30340;&#32959;&#30244;&#26816;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20462;&#25913;&#26679;&#26412;&#25968;&#37327;&#21644;&#24739;&#32773;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31185;&#23398;&#39046;&#22495;&#65292;&#30001;&#20110;&#33041;&#32959;&#30244;&#22312;&#24739;&#32773;&#20013;&#30340;&#32597;&#35265;&#31243;&#24230;&#65292;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#24322;&#24120;&#24773;&#20917;&#19979;&#26816;&#27979;&#32959;&#30244;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#21644;&#20998;&#31867;&#33041;&#32959;&#30244;&#12290;&#26469;&#33258;&#22269;&#23478;&#33041;&#26144;&#23556;&#23454;&#39564;&#23460;&#65288;NBML&#65289;&#30340;&#31934;&#36873;&#25968;&#25454;&#38598;&#21253;&#25324;81&#21517;&#24739;&#32773;&#65292;&#20854;&#20013;&#21253;&#25324;30&#20363;&#32959;&#30244;&#30149;&#20363;&#21644;51&#20363;&#27491;&#24120;&#30149;&#20363;&#12290;&#26816;&#27979;&#21644;&#20998;&#31867;&#27969;&#31243;&#34987;&#20998;&#20026;&#20004;&#20010;&#36830;&#32493;&#30340;&#20219;&#21153;&#12290;&#26816;&#27979;&#38454;&#27573;&#21253;&#25324;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;&#21644;&#39044;&#22788;&#29702;&#65292;&#20197;&#20462;&#25913;&#22270;&#20687;&#26679;&#26412;&#21644;&#27599;&#20010;&#31867;&#21035;&#30340;&#24739;&#32773;&#25968;&#37327;&#65292;&#20197;&#31526;&#21512;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#20998;&#24067;&#65288;9&#20010;&#27491;&#24120;&#26679;&#26412;&#23545;&#24212;1&#20010;&#32959;&#30244;&#26679;&#26412;&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#27979;&#35797;&#20013;&#38500;&#20102;&#24120;&#35265;&#30340;&#35780;&#20272;&#25351;&#26631;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;... [&#25688;&#35201;&#38271;&#24230;&#24050;&#36798;&#21040;&#19978;&#38480;]
&lt;/p&gt;
&lt;p&gt;
In the field of medical sciences, reliable detection and classification of brain tumors from images remains a formidable challenge due to the rarity of tumors within the population of patients. Therefore, the ability to detect tumors in anomaly scenarios is paramount for ensuring timely interventions and improved patient outcomes. This study addresses the issue by leveraging deep learning (DL) techniques to detect and classify brain tumors in challenging situations. The curated data set from the National Brain Mapping Lab (NBML) comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The detection and classification pipelines are separated into two consecutive tasks. The detection phase involved comprehensive data analysis and pre-processing to modify the number of image samples and the number of patients of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with real world scenarios. Next, in addition to common evaluation metrics for the testing, we emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03301</link><description>&lt;p&gt;
&#22312;&#26679;&#26412;&#39640;&#25928;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65306;&#25968;&#25454;&#22810;&#26679;&#24615;&#12289;&#21518;&#39564;&#37319;&#26679;&#65292;&#20197;&#21450;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#32479;&#19968;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#21644;&#21518;&#39564;&#37319;&#26679;&#30340;&#31639;&#27861;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35797;&#22270;&#29702;&#35299;&#20160;&#20040;&#20419;&#36827;&#20102;&#23545;&#20110;&#24207;&#36125;&#21494;&#26031;&#20915;&#31574;&#30340;&#21382;&#21490;&#25968;&#25454;&#38598;&#36827;&#34892;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#65292;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#34987;&#31216;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20110;&#22312;&#21033;&#29992;&#65288;&#20540;&#65289;&#20989;&#25968;&#36924;&#36817;&#30340;&#21516;&#26102;&#20139;&#21463;&#26679;&#26412;&#25928;&#29575;&#30340;&#31639;&#27861;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21253;&#25324;&#31163;&#32447;RL&#20013;&#35206;&#30422;&#24230;&#37327;&#30340;&#20808;&#21069;&#27010;&#24565;&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#65292;&#24182;&#19988;&#21033;&#29992;&#36825;&#20010;&#27010;&#24565;&#23558;&#22522;&#20110;&#29256;&#26412;&#31354;&#38388;&#65288;VS&#65289;&#12289;&#27491;&#21017;&#21270;&#20248;&#21270;&#65288;RO&#65289;&#21644;&#21518;&#39564;&#37319;&#26679;&#65288;PS&#65289;&#30340;&#19977;&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#31163;&#32447;RL&#31639;&#27861;&#36827;&#34892;&#32479;&#19968;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#35777;&#26126;&#65292;&#22522;&#20110;VS&#12289;&#22522;&#20110;RO&#21644;&#22522;&#20110;PS&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;\emph{&#21487;&#27604;}&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#36825;&#24674;&#22797;&#20102;&#22312;&#26377;&#38480;&#21644;&#32447;&#24615;&#27169;&#22411;&#31867;&#21035;&#19979;&#30340;&#26368;&#20248;&#24615;&#30340;&#26631;&#20934;&#20551;&#35774;&#30340;&#36793;&#30028;&#12290;&#36825;&#20010;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#19981;&#20855;&#26377;&#26377;&#21033;&#24615;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to understand what facilitates sample-efficient learning from historical datasets for sequential decision-making, a problem that is popularly known as offline reinforcement learning (RL). Further, we are interested in algorithms that enjoy sample efficiency while leveraging (value) function approximation. In this paper, we address these fundamental questions by (i) proposing a notion of data diversity that subsumes the previous notions of coverage measures in offline RL and (ii) using this notion to {unify} three distinct classes of offline RL algorithms based on version spaces (VS), regularized optimization (RO), and posterior sampling (PS). We establish that VS-based, RO-based, and PS-based algorithms, under standard assumptions, achieve \emph{comparable} sample efficiency, which recovers the state-of-the-art sub-optimality bounds for finite and linear model classes with the standard assumptions. This result is surprising, given that the prior work suggested an unfavorable sa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24212;&#29992;CNN-DNN&#32593;&#32476;&#34701;&#21512;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20351;&#29992;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;Monte-Carlo&#27979;&#35797;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03267</link><description>&lt;p&gt;
&#22797;&#26434;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24212;&#29992;CNN-DNN&#32593;&#32476;&#34701;&#21512;&#36827;&#34892;&#33258;&#20027;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#20223;&#23398;&#20064;&#20351;&#29992;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;Monte-Carlo&#27979;&#35797;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#24212;&#29992;CNN-DNN&#32593;&#32476;&#34701;&#21512;&#26469;&#26500;&#24314;&#26426;&#22120;&#20154;&#23548;&#33322;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#12290;&#27169;&#25311;&#29615;&#22659;&#29992;&#20110;&#27169;&#25311;&#22320;&#19979;&#25937;&#25588;&#24773;&#26223;&#65292;&#23558;&#33258;&#20027;&#20195;&#29702;&#20219;&#21153;&#35774;&#23450;&#20026;&#22312;&#26410;&#30693;&#30340;&#27934;&#31348;&#31995;&#32479;&#20013;&#23547;&#25214;&#30446;&#26631;&#12290;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#26469;&#35757;&#32451;&#25511;&#21046;&#31639;&#27861;&#20197;&#36827;&#34892;&#31354;&#38388;&#23548;&#33322;&#24182;&#25214;&#21040;&#30446;&#26631;&#12290;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36890;&#36807;Monte-Carlo&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the application of CNN-DNN network fusion to construct a robot navigation controller within a simulated environment. The simulated environment is constructed to model a subterranean rescue situation, such that an autonomous agent is tasked with finding a goal within an unknown cavernous system. Imitation learning is used to train the control algorithm to use LiDAR and camera data to navigate the space and find the goal. The trained model is then tested for robustness using Monte-Carlo.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03253</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#22312;&#36328;&#39046;&#22495;&#20219;&#21153;&#20013;&#20943;&#23569;&#39046;&#22495;&#36716;&#31227;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#30340;&#26368;&#26032;&#36827;&#23637;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#38754;&#23545;&#39046;&#22495;&#36716;&#31227;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#36328;&#39046;&#22495;&#23398;&#20064;&#26088;&#22312;&#25552;&#21462;&#39046;&#22495;&#19981;&#21464;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#35757;&#32451;&#19982;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#20013;&#65292;&#20256;&#32479;&#26041;&#27861;&#20165;&#20851;&#27880;&#22270;&#20687;&#27169;&#24577;&#65292;&#24573;&#35270;&#20102;&#20351;&#29992;&#25991;&#26412;&#27169;&#24577;&#26469;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35270;&#35273;&#36328;&#39046;&#22495;&#23398;&#20064;&#22120;&#65288;LLaVO&#65289;&#12290;LLaVO&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23558;&#22270;&#20687;&#36716;&#25442;&#20026;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#22312;&#32463;&#36807;&#35774;&#35745;&#30340;&#25351;&#23548;&#27169;&#26495;&#29983;&#25104;&#30340;&#28304;&#22495;/&#30446;&#26631;&#22495;&#30340;&#25991;&#26412;&#25551;&#36848;&#19978;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#21644;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#35774;&#32622;&#19979;&#36827;&#34892;&#30340;&#21508;&#31181;&#36328;&#39046;&#22495;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances achieved by deep learning models rely on the independent and identically distributed assumption, hindering their applications in real-world scenarios with domain shifts. To address the above issues, cross-domain learning aims at extracting domain-invariant knowledge to reduce the domain shift between training and testing data. However, in visual cross-domain learning, traditional methods concentrate solely on the image modality, neglecting the use of the text modality to alleviate the domain shift. In this work, we propose Large Language models as Visual cross-dOmain learners (LLaVO). LLaVO uses vision-language models to convert images into detailed textual descriptions. A large language model is then finetuned on textual descriptions of the source/target domain generated by a designed instruction template. Extensive experimental results on various cross-domain tasks under the domain generalization and unsupervised domain adaptation settings have demonstrated the effect
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#31471;&#21040;&#31471;ASR&#20449;&#20219;&#24230;&#30340;&#26102;&#24577;&#35789;&#20803;&#30456;&#20284;&#24230;&#20998;&#25968;TeLeS&#65292;&#24182;&#29992;&#32553;&#20943;&#25439;&#22833;&#26469;&#35299;&#20915;CEM&#35757;&#32451;&#20013;&#30446;&#26631;&#24471;&#20998;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03251</link><description>&lt;p&gt;
TeLeS&#65306;&#29992;&#20110;&#20272;&#35745;&#31471;&#21040;&#31471;ASR&#20449;&#20219;&#24230;&#30340;&#26102;&#24577;&#35789;&#20803;&#30456;&#20284;&#24230;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR. (arXiv:2401.03251v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20272;&#35745;&#31471;&#21040;&#31471;ASR&#20449;&#20219;&#24230;&#30340;&#26102;&#24577;&#35789;&#20803;&#30456;&#20284;&#24230;&#20998;&#25968;TeLeS&#65292;&#24182;&#29992;&#32553;&#20943;&#25439;&#22833;&#26469;&#35299;&#20915;CEM&#35757;&#32451;&#20013;&#30446;&#26631;&#24471;&#20998;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#39044;&#27979;&#20013;&#24471;&#20986;&#20449;&#24515;&#20272;&#35745;&#26377;&#21161;&#20110;ASR&#30340;&#19979;&#28216;&#21644;&#19978;&#28216;&#20219;&#21153;&#12290;&#22522;&#20110;&#31867;&#21035;&#27010;&#29575;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#19981;&#33021;&#20934;&#30830;&#22320;&#34920;&#31034;&#36807;&#20110;&#33258;&#20449;&#30340;ASR&#39044;&#27979;&#30340;&#36136;&#37327;&#12290;&#36741;&#21161;&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;&#65288;CEM&#65289;&#21487;&#26657;&#20934;&#36825;&#20123;&#39044;&#27979;&#12290;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#20351;&#29992;&#20108;&#36827;&#21046;&#30446;&#26631;&#24471;&#20998;&#36827;&#34892;CEM&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20108;&#36827;&#21046;&#26631;&#31614;&#19981;&#33021;&#25581;&#31034;&#39044;&#27979;&#35789;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#22914;&#21442;&#32771;&#35821;&#38899;&#21644;&#20551;&#35774;&#35821;&#38899;&#20043;&#38388;&#30340;&#26102;&#24577;&#23545;&#40784;&#20197;&#21450;&#39044;&#27979;&#35789;&#26159;&#21542;&#23436;&#20840;&#38169;&#35823;&#25110;&#21253;&#21547;&#25340;&#20889;&#38169;&#35823;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#24577;&#35789;&#20803;&#30456;&#20284;&#24230;&#65288;TeLeS&#65289;&#32622;&#20449;&#24230;&#20998;&#25968;&#26469;&#35757;&#32451;CEM&#12290;&#20026;&#20102;&#35299;&#20915;CEM&#35757;&#32451;&#20013;&#30446;&#26631;&#24471;&#20998;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#32553;&#20943;&#25439;&#22833;&#26469;&#32858;&#28966;&#20110;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#28857;&#24182;&#26368;&#23567;&#21270;&#36731;&#26131;&#23398;&#20064;&#30340;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#35821;&#35328;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Confidence estimation of predictions from an End-to-End (E2E) Automatic Speech Recognition (ASR) model benefits ASR's downstream and upstream tasks. Class-probability-based confidence scores do not accurately represent the quality of overconfident ASR predictions. An ancillary Confidence Estimation Model (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use binary target scores for CEM training. However, the binary labels do not reveal the granular information of predicted words, such as temporal alignment between reference and hypothesis and whether the predicted word is entirely incorrect or contains spelling errors. Addressing this issue, we propose a novel Temporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address the data imbalance of target scores while training CEM, we use shrinkage loss to focus on hard-to-learn data points and minimise the impact of easily learned data points. We conduct experiments with ASR models trained in three languages
&lt;/p&gt;</description></item><item><title>SeqNAS&#26159;&#19968;&#31181;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#34920;&#36798;&#21147;&#24378;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#24120;&#29992;&#30340;&#26500;&#24314;&#22359;&#36827;&#34892;&#25628;&#32034;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.03246</link><description>&lt;p&gt;
SeqNAS: &#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SeqNAS: Neural Architecture Search for Event Sequence Classification. (arXiv:2401.03246v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03246
&lt;/p&gt;
&lt;p&gt;
SeqNAS&#26159;&#19968;&#31181;&#38024;&#23545;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#34920;&#36798;&#21147;&#24378;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#24120;&#29992;&#30340;&#26500;&#24314;&#22359;&#36827;&#34892;&#25628;&#32034;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(NAS)&#26041;&#27861;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#34892;&#19994;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#12290;&#20107;&#20214;&#24207;&#21015;&#22312;&#21508;&#31181;&#24037;&#19994;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#20351;&#29992;&#65292;&#21253;&#25324;&#27969;&#22833;&#39044;&#27979;&#12289;&#23458;&#25143;&#20998;&#21106;&#12289;&#27450;&#35784;&#26816;&#27979;&#21644;&#25925;&#38556;&#35786;&#26029;&#31561;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#25324;&#20855;&#26377;&#19981;&#35268;&#21017;&#26102;&#38388;&#25139;&#30340;&#20998;&#31867;&#21644;&#23454;&#25968;&#20540;&#32452;&#20214;&#12290;&#23613;&#31649;NAS&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#65292;&#20294;&#20043;&#21069;&#30340;&#26041;&#27861;&#20165;&#24212;&#29992;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#26102;&#38388;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;SeqNAS&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#35813;&#31639;&#27861;&#19987;&#38376;&#29992;&#20110;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#34920;&#36798;&#21147;&#24378;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#21033;&#29992;&#24120;&#29992;&#30340;&#20107;&#20214;&#24207;&#21015;&#20998;&#31867;&#26500;&#24314;&#22359;&#65292;&#21253;&#25324;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21367;&#31215;&#21644;&#24490;&#29615;&#21333;&#20803;&#12290;&#20026;&#20102;&#36827;&#34892;&#25628;&#32034;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#39034;&#24207;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#20808;&#21069;&#35757;&#32451;&#36807;&#30340;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) methods are widely used in various industries to obtain high quality taskspecific solutions with minimal human intervention. Event Sequences find widespread use in various industrial applications including churn prediction customer segmentation fraud detection and fault diagnosis among others. Such data consist of categorical and real-valued components with irregular timestamps. Despite the usefulness of NAS methods previous approaches only have been applied to other domains images texts or time series. Our work addresses this limitation by introducing a novel NAS algorithm SeqNAS specifically designed for event sequence classification. We develop a simple yet expressive search space that leverages commonly used building blocks for event sequence classification including multihead self attention convolutions and recurrent cells. To perform the search we adopt sequential Bayesian Optimization and utilize previously trained models as an ensemble of teache
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#32553;&#25918;&#35299;&#37322;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#26080;&#23398;&#20064;&#29575;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#24182;&#22686;&#24378;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03240</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#25968;&#32553;&#25918;&#35299;&#37322;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20197;&#23454;&#29616;&#38646;&#23398;&#20064;&#29575;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization. (arXiv:2401.03240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#21442;&#25968;&#32553;&#25918;&#35299;&#37322;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#26080;&#23398;&#20064;&#29575;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#24182;&#22686;&#24378;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20272;&#35745;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#23398;&#20064;&#29575;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26080;&#23398;&#20064;&#29575;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#38024;&#23545;&#26368;&#38497;&#19979;&#38477;&#27861;&#35774;&#35745;&#30340;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#38497;&#19979;&#38477;&#27861;&#23545;&#20110;&#23547;&#25214;&#26497;&#23567;&#20540;&#25552;&#20379;&#20102;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#20294;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#38656;&#35201;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#26412;&#25991;&#23558;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#35299;&#37322;&#20026;&#24212;&#29992;&#20110;&#21442;&#25968;&#32553;&#25918;&#32593;&#32476;&#19978;&#30340;&#26368;&#38497;&#19979;&#38477;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26080;&#23398;&#20064;&#29575;&#30340;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#23637;&#31034;&#20986;&#19982;&#25163;&#21160;&#35843;&#25972;&#23398;&#20064;&#29575;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#25193;&#23637;&#20102;&#26080;&#23398;&#20064;&#29575;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#22686;&#24378;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the challenge of estimating the learning rate for adaptive gradient methods used in training deep neural networks. While several learning-rate-free approaches have been proposed, they are typically tailored for steepest descent. However, although steepest descent methods offer an intuitive approach to finding minima, many deep learning applications require adaptive gradient methods to achieve faster convergence. In this paper, we interpret adaptive gradient methods as steepest descent applied on parameter-scaled networks, proposing learning-rate-free adaptive gradient methods. Experimental results verify the effectiveness of this approach, demonstrating comparable performance to hand-tuned learning rates across various scenarios. This work extends the applicability of learning-rate-free methods, enhancing training with adaptive gradient methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03233</link><description>&lt;p&gt;
&#22522;&#20110;Split Learning&#30340;&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;Split Learning&#32908;&#30005;&#20551;&#32930;&#25511;&#21046;&#20013;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36807;&#31243;&#25552;&#39640;&#20102;&#20551;&#32930;&#25511;&#21046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Split Learning (SL)&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#20110;&#22522;&#20110;&#32908;&#30005;&#30340;&#20551;&#32930;&#25511;&#21046;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#21644;&#32852;&#37030;&#23398;&#20064;&#31561;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SL&#33021;&#22815;&#25552;&#20379;&#26356;&#20248;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20551;&#32930;&#35774;&#22791;&#22312;&#22788;&#29702;&#33021;&#21147;&#21644;&#30005;&#27744;&#23551;&#21629;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;SL&#30340;&#21487;&#34892;&#24615;&#28304;&#20110;&#20854;&#22266;&#26377;&#30340;&#27169;&#22411;&#20998;&#21106;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#25191;&#34892;&#36739;&#23567;&#30340;&#27169;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#19981;&#24688;&#24403;&#30340;&#20999;&#23618;&#20250;&#38459;&#30861;SL&#31995;&#32479;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#27169;&#22411;&#25910;&#25947;&#36895;&#29575;&#30340;&#20999;&#23618;&#36873;&#25321;&#31639;&#27861;&#12290;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#25913;&#21892;&#20551;&#32930;&#25511;&#21046;&#30340;&#32908;&#30005;&#27169;&#24335;&#35782;&#21035;&#20219;&#21153;&#20013;&#26174;&#33879;&#21152;&#36895;&#20102;&#25910;&#25947;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Split Learning (SL) is a promising Distributed Learning approach in electromyography (EMG) based prosthetic control, due to its applicability within resource-constrained environments. Other learning approaches, such as Deep Learning and Federated Learning (FL), provide suboptimal solutions, since prosthetic devices are extremely limited in terms of processing power and battery life. The viability of implementing SL in such scenarios is caused by its inherent model partitioning, with clients executing the smaller model segment. However, selecting an inadequate cut layer hinders the training process in SL systems. This paper presents an algorithm for optimal cut layer selection in terms of maximizing the convergence rate of the model. The performance evaluation demonstrates that the proposed algorithm substantially accelerates the convergence in an EMG pattern recognition task for improving prosthetic device control.
&lt;/p&gt;</description></item><item><title>FedTGP &#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36793;&#32536;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064; (ACL) &#26469;&#23398;&#20064;&#21487;&#35757;&#32451;&#30340;&#20840;&#23616;&#21407;&#22411; (TGP)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21407;&#22411;&#30340;&#21487;&#20998;&#31163;&#24615;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.03230</link><description>&lt;p&gt;
FedTGP: &#29992;&#33258;&#36866;&#24212;&#36793;&#32536;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#21487;&#35757;&#32451;&#20840;&#23616;&#21407;&#22411;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning. (arXiv:2401.03230v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03230
&lt;/p&gt;
&lt;p&gt;
FedTGP &#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36793;&#32536;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064; (ACL) &#26469;&#23398;&#20064;&#21487;&#35757;&#32451;&#30340;&#20840;&#23616;&#21407;&#22411; (TGP)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21407;&#22411;&#30340;&#21487;&#20998;&#31163;&#24615;&#21644;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#25903;&#25345;&#24322;&#26500;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#65288;HtFL&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#20943;&#23569;&#22312;HtFL&#20013;&#20256;&#36755;&#27169;&#22411;&#21442;&#25968;&#30340;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#25552;&#20986;&#20102;&#22522;&#20110;&#21407;&#22411;&#30340;HtFL&#26041;&#27861;&#65292;&#23558;&#31867;&#21035;&#20195;&#34920;&#65292;&#21363;&#21407;&#22411;&#65292;&#20165;&#22312;&#24322;&#26500;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#65292;&#21516;&#26102;&#20445;&#25252;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21407;&#22411;&#22312;&#26381;&#21153;&#22120;&#19978;&#36890;&#36807;&#21152;&#26435;&#24179;&#22343;&#20540;&#31616;&#21333;&#22320;&#32858;&#21512;&#25104;&#20840;&#23616;&#21407;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#23376;&#20248;&#21270;&#30340;&#20840;&#23616;&#30693;&#35782;&#65292;&#20174;&#32780;&#23545;&#23458;&#25143;&#31471;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;HtFL&#26041;&#27861;&#65292;&#31216;&#20026;FedTGP&#65292;&#23427;&#21033;&#29992;&#20102;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36793;&#32536;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#65288;ACL&#65289;&#22312;&#26381;&#21153;&#22120;&#19978;&#23398;&#20064;&#21487;&#35757;&#32451;&#30340;&#20840;&#23616;&#21407;&#22411;&#65288;TGP&#65289;&#12290;&#36890;&#36807;&#32467;&#21512;ACL&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#21407;&#22411;&#30340;&#21487;&#20998;&#31163;&#24615;&#21516;&#26102;&#20445;&#30041;&#35821;&#20041;&#21547;&#20041;&#12290;&#23545;&#20110;&#21313;&#20108;&#20010;&#24322;&#26500;&#27169;&#22411;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due to its ability to support heterogeneous models and data. To reduce the high communication cost of transmitting model parameters, a major challenge in HtFL, prototype-based HtFL methods are proposed to solely share class representatives, a.k.a, prototypes, among heterogeneous clients while maintaining the privacy of clients' models. However, these prototypes are naively aggregated into global prototypes on the server using weighted averaging, resulting in suboptimal global knowledge which negatively impacts the performance of clients. To overcome this challenge, we introduce a novel HtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the server. By incorporating ACL, our approach enhances prototype separability while preserving semantic meaning. Extensive experiments with twelve heterogeneous models demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26377;&#30028;&#22495;&#20869;&#29983;&#25104;&#25968;&#25454;&#30340;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#24102;&#26377;&#36793;&#30028;&#26465;&#20214;&#30340;&#21453;&#23556;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#35299;&#20915;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#22312;&#36866;&#24212;&#22810;&#26679;&#24615;&#39046;&#22495;&#26102;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.03228</link><description>&lt;p&gt;
&#26377;&#32422;&#26463;&#29983;&#25104;&#24314;&#27169;&#30340;&#21453;&#23556;Schr\"odinger&#26725;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reflected Schr\"odinger Bridge for Constrained Generative Modeling. (arXiv:2401.03228v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26377;&#30028;&#22495;&#20869;&#29983;&#25104;&#25968;&#25454;&#30340;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#24102;&#26377;&#36793;&#30028;&#26465;&#20214;&#30340;&#21453;&#23556;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#35299;&#20915;&#20102;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#22312;&#36866;&#24212;&#22810;&#26679;&#24615;&#39046;&#22495;&#26102;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#30340;&#39318;&#36873;&#26041;&#27861;&#12290;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#28041;&#21450;&#22312;&#26377;&#30028;&#22495;&#20869;&#38480;&#21046;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#36890;&#24120;&#38656;&#35201; ad-hoc &#38408;&#20540;&#25216;&#26415;&#26469;&#24378;&#21046;&#36793;&#30028;&#12290;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#30001;&#21453;&#23556;&#24067;&#26391;&#36816;&#21160;&#25511;&#21046;&#30340;&#21518;&#21521;&#36807;&#31243;&#29983;&#25104;&#25968;&#25454;&#20998;&#24067;&#65292;&#20197;&#22686;&#21152;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#21453;&#23556;&#25193;&#25955;&#27169;&#22411;&#21487;&#33021;&#19981;&#23481;&#26131;&#36866;&#24212;&#21508;&#31181;&#39046;&#22495;&#65292;&#38656;&#35201;&#27491;&#30830;&#23548;&#20986;&#30340;&#24046;&#21516;&#32986;&#26144;&#23556;&#65292;&#24182;&#19988;&#19981;&#33021;&#20445;&#35777;&#26368;&#20248;&#36755;&#36816;&#29305;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#23556;Schr\"odinger&#26725;&#31639;&#27861;&#65306;&#19968;&#31181;&#29992;&#20110;&#22312;&#21508;&#31181;&#26377;&#30028;&#22495;&#20869;&#29983;&#25104;&#25968;&#25454;&#30340;&#29109;&#27491;&#21017;&#21270;&#26368;&#20248;&#36755;&#36816;&#26041;&#27861;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#24102;&#26377; Neumann &#21644; Robin &#36793;&#30028;&#26465;&#20214;&#30340;&#20248;&#38597;&#21453;&#23556;&#27491;&#21521;-&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#24182;&#23558;&#22522;&#20110;&#25955;&#24230;&#30340;&#20284;&#28982;&#35757;&#32451;&#25193;&#23637;&#21040;&#26377;&#30028;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become the go-to method for large-scale generative models in real-world applications. These applications often involve data distributions confined within bounded domains, typically requiring ad-hoc thresholding techniques for boundary enforcement. Reflected diffusion models (Lou23) aim to enhance generalizability by generating the data distribution through a backward process governed by reflected Brownian motion. However, reflected diffusion models may not easily adapt to diverse domains without the derivation of proper diffeomorphic mappings and do not guarantee optimal transport properties. To overcome these limitations, we introduce the Reflected Schrodinger Bridge algorithm: an entropy-regularized optimal transport approach tailored for generating data within diverse bounded domains. We derive elegant reflected forward-backward stochastic differential equations with Neumann and Robin boundary conditions, extend divergence-based likelihood training to bounded d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31471;&#21040;&#31471;&#21453;&#21518;&#38376;&#23398;&#20064;&#65288;E2ABL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#20197;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#21453;&#21518;&#38376;&#23398;&#20064;&#65288;ABL&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;E2ABL&#36890;&#36807;&#39069;&#22806;&#30340;&#20998;&#31867;&#22836;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27973;&#23618;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#24182;&#26377;&#25928;&#22320;&#37492;&#21035;&#28145;&#23618;&#29305;&#24449;&#31354;&#38388;&#20013;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2401.03215</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#21453;&#21518;&#38376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
End-to-End Anti-Backdoor Learning on Images and Time Series. (arXiv:2401.03215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31471;&#21040;&#31471;&#21453;&#21518;&#38376;&#23398;&#20064;&#65288;E2ABL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#22270;&#20687;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#40065;&#26834;&#35757;&#32451;&#20197;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#12290;&#19982;&#20256;&#32479;&#30340;&#21453;&#21518;&#38376;&#23398;&#20064;&#65288;ABL&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;E2ABL&#36890;&#36807;&#39069;&#22806;&#30340;&#20998;&#31867;&#22836;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27973;&#23618;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#24182;&#26377;&#25928;&#22320;&#37492;&#21035;&#28145;&#23618;&#29305;&#24449;&#31354;&#38388;&#20013;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#23041;&#32961;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#29992;&#20110;&#23433;&#20840;&#21644;&#20445;&#23494;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#23884;&#20837;&#38544;&#34255;&#35302;&#21457;&#22120;&#26469;&#25805;&#32437;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#22312;&#25512;&#26029;&#26102;&#20801;&#35768;&#23545;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#26410;&#32463;&#25480;&#26435;&#30340;&#25511;&#21046;&#12290;&#23613;&#31649;&#23384;&#22312;&#22823;&#37327;&#38754;&#21521;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#20294;&#40092;&#26377;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#65292;&#20063;&#27809;&#26377;&#33021;&#22815;&#22312;&#21463;&#27745;&#26579;&#25968;&#25454;&#19978;&#36827;&#34892;&#24178;&#20928;&#27169;&#22411;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#22312;&#21453;&#21518;&#38376;&#23398;&#20064;&#65288;ABL&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#31471;&#21040;&#31471;&#21453;&#21518;&#38376;&#23398;&#20064;&#65288;E2ABL&#65289;&#65292;&#29992;&#20110;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#40065;&#26834;&#35757;&#32451;&#12290;&#19982;&#21407;&#22987;&#30340;ABL&#19981;&#21516;&#65292;E2ABL&#36890;&#36807;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#27973;&#23618;&#36830;&#25509;&#30340;&#38468;&#21152;&#20998;&#31867;&#22836;&#23454;&#29616;&#31471;&#21040;&#31471;&#35757;&#32451;&#12290;&#36825;&#20010;&#36741;&#21161;&#22836;&#31215;&#26497;&#22320;&#25506;&#32034;&#28145;&#23618;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#24178;&#20928;&#26679;&#26412;&#21644;&#27745;&#26579;&#26679;&#26412;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks present a substantial security concern for deep learning models, especially those utilized in applications critical to safety and security. These attacks manipulate model behavior by embedding a hidden trigger during the training phase, allowing unauthorized control over the model's output during inference time. Although numerous defenses exist for image classification models, there is a conspicuous absence of defenses tailored for time series data, as well as an end-to-end solution capable of training clean models on poisoned data. To address this gap, this paper builds upon Anti-Backdoor Learning (ABL) and introduces an innovative method, End-to-End Anti-Backdoor Learning (E2ABL), for robust training against backdoor attacks. Unlike the original ABL, which employs a two-stage training procedure, E2ABL accomplishes end-to-end training through an additional classification head linked to the shallow layers of a Deep Neural Network (DNN). This secondary head actively ide
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#27425;&#20934;&#30830;&#20998;&#26512;&#38750;&#32447;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22312;&#29702;&#35299;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2401.03214</link><description>&lt;p&gt;
&#29702;&#35299;&#38750;&#32447;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26159;&#31532;&#19968;&#27425;&#20934;&#30830;&#20998;&#26512;&#38750;&#32447;&#24615;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#27169;&#22411;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#22312;&#29702;&#35299;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#32463;&#39564;&#35777;&#26126;&#20854;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#12290;&#20851;&#20110;&#25968;&#25454;&#34920;&#31034;&#21487;&#23398;&#20064;&#24615;&#30340;&#29702;&#35770;&#30740;&#31350;&#24456;&#23569;&#65292;&#20854;&#20013;&#35768;&#22810;&#23558;&#38750;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#8220;&#40657;&#31665;&#8221;&#65292;&#20165;&#20851;&#27880;&#26368;&#32456;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#23398;&#20064;&#32467;&#26524;&#23545;&#20110;&#25551;&#36848;SSL&#27169;&#22411;&#23398;&#21040;&#30340;&#25968;&#25454;&#20998;&#24067;&#29305;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26159;&#39318;&#27425;&#20934;&#30830;&#20998;&#26512;&#38750;&#32447;&#24615;SSL&#27169;&#22411;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#20010;&#29305;&#24449;&#30340;&#29609;&#20855;&#25968;&#25454;&#20998;&#24067;&#65306;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#29305;&#24449;&#21644;&#38544;&#34255;&#29305;&#24449;&#12290;&#19982;&#20197;&#24448;&#30340;&#20381;&#36182;&#20110;&#38381;&#24335;&#35299;&#30340;&#32447;&#24615;&#35774;&#32622;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#29305;&#23450;&#30340;&#21021;&#22987;&#21270;&#21306;&#22495;&#19979;&#35757;&#32451;&#19968;&#20010;1&#23618;&#38750;&#32447;&#24615;SSL&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#27169;&#22411;&#25910;&#25947;&#21040;&#19968;&#20010;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#27492;&#22806;&#65292;&#19982;&#22797;&#26434;&#30340;&#36845;&#20195;&#20998;&#26512;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#26512;&#36807;&#31243;&#65292;&#20351;&#29992;t
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has empirically shown its data representation learnability in many downstream tasks. There are only a few theoretical works on data representation learnability, and many of those focus on final data representation, treating the nonlinear neural network as a ``black box". However, the accurate learning results of neural networks are crucial for describing the data distribution features learned by SSL models. Our paper is the first to analyze the learning results of the nonlinear SSL model accurately. We consider a toy data distribution that contains two features: the label-related feature and the hidden feature. Unlike previous linear setting work that depends on closed-form solutions, we use the gradient descent algorithm to train a 1-layer nonlinear SSL model with a certain initialization region and prove that the model converges to a local minimum. Furthermore, different from the complex iterative analysis, we propose a new analysis process which uses t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;Robbins-Monro&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20808;&#39564;&#20449;&#24687;&#30340;Robbins-Monro&#24207;&#21015;&#27604;&#26631;&#20934;&#24207;&#21015;&#25910;&#25947;&#26356;&#24555;&#65292;&#29305;&#21035;&#26159;&#22312;&#21069;&#20960;&#27493;&#12290;</title><link>http://arxiv.org/abs/2401.03206</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#20197;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#30340;Robbins-Monro&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence. (arXiv:2401.03206v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20808;&#39564;&#20449;&#24687;&#65292;&#25913;&#36827;&#20102;Robbins-Monro&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20808;&#39564;&#20449;&#24687;&#30340;Robbins-Monro&#24207;&#21015;&#27604;&#26631;&#20934;&#24207;&#21015;&#25910;&#25947;&#26356;&#24555;&#65292;&#29305;&#21035;&#26159;&#22312;&#21069;&#20960;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#28857;&#30340;&#20808;&#39564;&#20449;&#24687;&#24341;&#20837;Robbins-Monro&#36845;&#20195;&#26469;&#25913;&#21892;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19981;&#38656;&#35201;&#22238;&#24402;&#27169;&#22411;&#30340;&#20808;&#39564;&#20449;&#24687;&#30340;&#34701;&#21512;&#65292;&#36825;&#20063;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20808;&#39564;&#20449;&#24687;&#30340;Robbins-Monro&#24207;&#21015;&#23545;&#20110;&#24191;&#27867;&#30340;&#20808;&#39564;&#20998;&#24067;&#37117;&#26159;&#25910;&#25947;&#30340;&#65292;&#21363;&#20351;&#26159;&#38169;&#35823;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#22914;&#39640;&#26031;&#20998;&#24067;&#12289;&#39640;&#26031;&#20998;&#24067;&#30340;&#21152;&#26435;&#21644;&#65288;&#20363;&#22914;&#22312;&#26680;&#23494;&#24230;&#20272;&#35745;&#20013;&#65289;&#65292;&#20197;&#21450;&#22823;&#20110;&#38646;&#30340;&#26377;&#30028;&#20219;&#24847;&#20998;&#24067;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25968;&#20540;&#20998;&#26512;&#26469;&#20102;&#35299;&#24207;&#21015;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20808;&#39564;&#20449;&#24687;&#30340;Robbins-Monro&#24207;&#21015;&#27604;&#26631;&#20934;&#24207;&#21015;&#25910;&#25947;&#26356;&#24555;&#65292;&#29305;&#21035;&#26159;&#22312;&#21069;&#20960;&#27493;&#65292;&#36825;&#23545;&#20110;&#27979;&#37327;&#20989;&#25968;&#25968;&#30446;&#26377;&#38480;&#21644;&#35823;&#24046;&#36739;&#22823;&#30340;&#24212;&#29992;&#29305;&#21035;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method to improve the convergence speed of the Robbins-Monro algorithm by introducing prior information about the target point into the Robbins-Monro iteration. We achieve the incorporation of prior information without the need of a -- potentially wrong -- regression model, which would also entail additional constraints. We show that this prior-information Robbins-Monro sequence is convergent for a wide range of prior distributions, even wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel density estimate, as well as bounded arbitrary distribution functions greater than zero. We furthermore analyse the sequence numerically to understand its performance and the influence of parameters. The results demonstrate that the prior-information Robbins-Monro sequence converges faster than the standard one, especially during the first steps, which are particularly important for applications where the number of function measurements is limited, and when the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#38477;&#20302;&#25968;&#25454;&#38598;&#32500;&#24230;&#30340;&#23398;&#20064;&#22686;&#24378;K-Means&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;k&#20540;&#20026;10&#21644;25&#26102;&#33719;&#24471;&#36739;&#20302;&#30340;&#32858;&#31867;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.03198</link><description>&lt;p&gt;
&#20351;&#29992;&#38477;&#32500;&#30340;&#23398;&#20064;&#22686;&#24378;K-Means&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented K-Means Clustering Using Dimensional Reduction. (arXiv:2401.03198v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#38477;&#20302;&#25968;&#25454;&#38598;&#32500;&#24230;&#30340;&#23398;&#20064;&#22686;&#24378;K-Means&#32858;&#31867;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;k&#20540;&#20026;10&#21644;25&#26102;&#33719;&#24471;&#36739;&#20302;&#30340;&#32858;&#31867;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#26041;&#27861;&#25110;&#27169;&#22411;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#39044;&#27979;&#21644;&#27867;&#21270;&#25968;&#25454;&#25110;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#25110;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#21644;&#20854;&#20182;&#22240;&#32032;&#27979;&#35797;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#12290;&#32858;&#31867;&#26159;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#30784;&#65292;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#29992;&#20110;&#20102;&#35299;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#12290;&#23613;&#31649;k-means&#31639;&#27861;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#20294;&#20173;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26469;&#38477;&#20302;&#25968;&#25454;&#38598;&#32500;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning augmented is a machine learning concept built to improve the performance of a method or model, such as enhancing its ability to predict and generalize data or features, or testing the reliability of the method by introducing noise and other factors. On the other hand, clustering is a fundamental aspect of data analysis and has long been used to understand the structure of large datasets. Despite its long history, the k-means algorithm still faces challenges. One approach, as suggested by Ergun et al,is to use a predictor to minimize the sum of squared distances between each data point and a specified centroid. However, it is known that the computational cost of this algorithm increases with the value of k, and it often gets stuck in local minima. In response to these challenges, we propose a solution to reduce the dimensionality of the dataset using Principal Component Analysis (PCA). It is worth noting that when using k values of 10 and 25, the proposed algorithm yields lower
&lt;/p&gt;</description></item><item><title>&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.03197</link><description>&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#19982;&#31574;&#30053;&#22686;&#24378;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03197
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#37325;&#35201;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#30528;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36830;&#32493;&#20915;&#31574;&#21046;&#23450;&#12290;&#38024;&#23545;&#36825;&#31867;&#38382;&#39064;&#65292;&#20256;&#32479;&#30340;&#26041;&#27861;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#21644;&#22312;&#32447;&#25628;&#32034;&#65288;&#22914;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65289;&#12290;&#21069;&#32773;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65288;&#36890;&#24120;&#22312;&#25191;&#34892;&#20043;&#21069;&#23436;&#25104;&#65289;&#65292;&#32780;&#21518;&#32773;&#22312;&#20915;&#31574;&#26102;&#20351;&#29992;&#29615;&#22659;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#37319;&#26679;&#26377;&#21069;&#26223;&#30340;&#34892;&#21160;&#36712;&#36857;&#12290;&#22312;&#38750;&#31283;&#23450;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#21046;&#23450;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20195;&#29702;&#25805;&#20316;&#30340;&#29615;&#22659;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#20004;&#31181;&#26041;&#27861;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#37117;&#23384;&#22312;&#32570;&#38519;--&#19968;&#26041;&#38754;&#65292;&#25191;&#34892;&#20043;&#21069;&#23398;&#20064;&#30340;&#31574;&#30053;&#22312;&#29615;&#22659;&#25913;&#21464;&#26102;&#21464;&#24471;&#38472;&#26087;&#65292;&#37325;&#26032;&#23398;&#20064;&#38656;&#35201;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#32447;&#25628;&#32034;&#22312;&#20801;&#35768;&#30340;&#36816;&#34892;&#26102;&#38388;&#26377;&#38480;&#26102;&#21487;&#33021;&#20250;&#36820;&#22238;&#27425;&#20248;&#34892;&#21160;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;--&#31574;&#30053;&#22686;&#24378;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;PA-MCTS&#65289;&#65292;&#23427;&#23558;&#22312;&#32447;&#25628;&#32034;&#19982;&#31574;&#30053;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential decision-making under uncertainty is present in many important problems. Two popular approaches for tackling such problems are reinforcement learning and online search (e.g., Monte Carlo tree search). While the former learns a policy by interacting with the environment (typically done before execution), the latter uses a generative model of the environment to sample promising action trajectories at decision time. Decision-making is particularly challenging in non-stationary environments, where the environment in which an agent operates can change over time. Both approaches have shortcomings in such settings -- on the one hand, policies learned before execution become stale when the environment changes and relearning takes both time and computational effort. Online search, on the other hand, can return sub-optimal actions when there are limitations on allowed runtime. In this paper, we introduce \textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines actio
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#30340;&#27604;&#29305;&#29575;&#21644;&#22797;&#26434;&#24230;&#39640;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#21644;&#39044;&#27979;&#30340;&#26368;&#23567;&#27604;&#29305;&#29575;&#26469;&#25913;&#36827;&#27604;&#29305;&#29575;&#26799;&#24230;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#22312;&#35270;&#39057;&#34892;&#19994;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#35270;&#39057;&#24182;&#20445;&#25345;&#27604;&#29305;&#29575;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03195</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#26500;&#24314;&#39640;&#25928;&#30340;&#27604;&#29305;&#29575;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features. (arXiv:2401.03195v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03195
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#30340;&#27604;&#29305;&#29575;&#21644;&#22797;&#26434;&#24230;&#39640;&#25928;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#21644;&#39044;&#27979;&#30340;&#26368;&#23567;&#27604;&#29305;&#29575;&#26469;&#25913;&#36827;&#27604;&#29305;&#29575;&#26799;&#24230;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#22312;&#35270;&#39057;&#34892;&#19994;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#35270;&#39057;&#24182;&#20445;&#25345;&#27604;&#29305;&#29575;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#39057;&#34892;&#19994;&#37324;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35270;&#39057;&#24182;&#20445;&#25345;&#27604;&#29305;&#29575;&#30340;&#25928;&#29575;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#19968;&#20992;&#20999;&#27604;&#29305;&#29575;&#26799;&#24230;&#26041;&#26696;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#30001;&#20110;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#32534;&#30721;&#65292;&#20351;&#24471;&#35745;&#31639;&#26368;&#20339;&#20869;&#23481;&#24863;&#30693;&#20915;&#31574;&#25104;&#20026;&#19981;&#29616;&#23454;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26102;&#31354;&#29305;&#24449;&#30340;&#27604;&#29305;&#29575;&#21644;&#22797;&#26434;&#24230;&#39640;&#25928;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#26041;&#27861;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26469;&#33258;&#30693;&#21517;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#26469;&#39044;&#27979;&#27604;&#29305;&#29575;&#36136;&#37327;&#34892;&#20026;&#65292;&#24182;&#38480;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#65307;&#65288;2&#65289;&#36890;&#36807;&#39044;&#27979;&#39030;&#37096;&#36136;&#37327;&#30340;&#26368;&#23567;&#27604;&#29305;&#29575;&#26469;&#25913;&#36827;&#26368;&#39640;&#36136;&#37327;&#30340;&#26799;&#24230;&#25928;&#29575;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39030;&#37096;step&#12290;&#35813;&#26041;&#27861;&#22312;102&#20010;&#35270;&#39057;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;1.71%&#30340;BD-Rate&#24320;&#38144;&#19979;&#65292;&#19982;&#34542;&#21147;&#26041;&#27861;&#30456;&#27604;&#65292;&#22797;&#26434;&#24230;&#38477;&#20302;&#20102;94.1%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#22235;&#20010;&#32593;&#32476;&#21644;&#28040;&#34701;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#36801;&#31227;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#25345;&#20037;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26368;&#23567;&#21270;&#25299;&#25169;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03194</link><description>&lt;p&gt;
&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#25345;&#20037;&#31038;&#21306;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis. (arXiv:2401.03194v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#23398;&#20064;&#21160;&#24577;&#32593;&#32476;&#20013;&#30340;&#25345;&#20037;&#31038;&#21306;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#26368;&#23567;&#21270;&#25299;&#25169;&#21464;&#21270;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31038;&#21306;&#26816;&#27979;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#30830;&#20445;&#26102;&#38388;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#23545;&#32593;&#32476;&#28436;&#21270;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#22270;&#32858;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#23567;&#26102;&#38388;&#38388;&#38548;&#20869;&#30340;&#31038;&#21306;&#38388;&#32467;&#26500;&#36827;&#34892;&#25299;&#25169;&#21464;&#21270;&#30340;&#26368;&#23567;&#21270;&#65292;&#26469;&#20445;&#35777;&#26102;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35299;&#20915;&#34920;&#31034;&#22604;&#32553;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#28145;&#24230;&#22270;&#32858;&#31867;&#31639;&#27861;MFC&#65292;&#20445;&#30041;&#20102;&#33410;&#28857;&#23884;&#20837;&#12290;&#22522;&#20110;&#38745;&#24577;&#32858;&#31867;&#32467;&#26524;&#65292;&#25105;&#20204;&#26500;&#24314;&#27010;&#29575;&#31038;&#21306;&#32593;&#32476;&#65292;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#25345;&#20037;&#21516;&#35843;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25299;&#25169;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#20043;&#38388;&#30340;&#32467;&#26500;&#30456;&#20284;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#26041;&#27861;TopoReg&#65292;&#29992;&#20110;&#30830;&#20445;&#31038;&#21306;&#38388;&#32467;&#26500;&#30340;&#25299;&#25169;&#30456;&#20284;&#24615;&#38543;&#26102;&#38388;&#38388;&#38548;&#30340;&#20445;&#25345;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#30495;&#23454;&#32593;&#32476;&#19978;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#32858;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic community detection methods often lack effective mechanisms to ensure temporal consistency, hindering the analysis of network evolution. In this paper, we propose a novel deep graph clustering framework with temporal consistency regularization on inter-community structures, inspired by the concept of minimal network topological changes within short intervals. Specifically, to address the representation collapse problem, we first introduce MFC, a matrix factorization-based deep graph clustering algorithm that preserves node embedding. Based on static clustering results, we construct probabilistic community networks and compute their persistence homology, a robust topological measure, to assess structural similarity between them. Moreover, a novel neural network regularization TopoReg is introduced to ensure the preservation of topological similarity between inter-community structures over time intervals. Our approach enhances temporal consistency and clustering accuracy on real-
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;(DMD)&#23545;&#33258;&#20276;&#38543;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#20102;&#20851;&#20110;&#35889;&#27979;&#24230;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;HDMD&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#25910;&#25947;&#21040;&#22522;&#30784;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2401.03192</link><description>&lt;p&gt;
&#20851;&#20110;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Hermitian Dynamic Mode Decomposition. (arXiv:2401.03192v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03192
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;(DMD)&#23545;&#33258;&#20276;&#38543;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#20102;&#20851;&#20110;&#35889;&#27979;&#24230;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#35777;&#26126;&#20102;HDMD&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#25910;&#25947;&#21040;&#22522;&#30784;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hermitian&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;(DMD)&#23545;&#33258;&#20276;&#38543;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#12290;Hermitian DMD&#26159;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31163;&#25955;&#26102;&#38388;&#24555;&#29031;&#36817;&#20284;&#34920;&#31034;&#19982;&#26410;&#30693;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#30456;&#20851;&#30340;Koopman&#31639;&#23376;&#65292;&#21516;&#26102;&#22312;&#20854;&#26377;&#38480;&#32500;&#36817;&#20284;&#20013;&#20445;&#25345;&#31639;&#23376;&#30340;&#33258;&#20276;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;HDMD&#30340;&#29305;&#24449;&#20540;&#21644;&#29305;&#24449;&#20989;&#25968;&#25910;&#25947;&#21040;&#22522;&#30784;Koopman&#31639;&#23376;&#30340;&#35889;&#24615;&#36136;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20851;&#20110;&#35889;&#27979;&#24230;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#23450;&#29702;&#65292;&#24182;&#22312;&#20108;&#32500;&#34203;&#23450;&#35860;&#26041;&#31243;&#19978;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the convergence of Hermitian Dynamic Mode Decomposition (DMD) to the spectral properties of self-adjoint Koopman operators. Hermitian DMD is a data-driven method for approximating the Koopman operator associated with an unknown nonlinear dynamical system from discrete-time snapshots, while preserving the self-adjointness of the operator on its finite-dimensional approximations. We show that, under suitable conditions, the eigenvalues and eigenfunctions of HDMD converge to the spectral properties of the underlying Koopman operator. Along the way, we establish a general theorem on the convergence of spectral measures, and demonstrate our results numerically on the two-dimensional Schr\"odinger equation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bodo&#35821;&#35328;&#27169;&#22411;BodoBERT&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;BiLSTM&#12289;CRF&#21644;BodoBERT&#19982;BytePairEmbeddings&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;Bodo&#65292;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.03175</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;Bodo&#35821;&#35789;&#24615;&#26631;&#27880;&#22120;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Bodo&#35821;&#35328;&#27169;&#22411;BodoBERT&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;BiLSTM&#12289;CRF&#21644;BodoBERT&#19982;BytePairEmbeddings&#30340;&#32452;&#21512;&#12290;&#23613;&#31649;&#30740;&#31350;&#24050;&#32463;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#22914;Bodo&#65292;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#22914;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#24314;&#27169;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#19968;&#20123;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;Bodo&#12289;Mizo&#12289;Nagamese&#31561;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#30740;&#31350;&#35201;&#20040;&#23578;&#26410;&#24320;&#22987;&#65292;&#35201;&#20040;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#12290;&#35821;&#35328;&#27169;&#22411;&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#20110;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20687;Bodo&#12289;Rabha&#21644;Mising&#36825;&#26679;&#30340;&#35821;&#35328;&#20173;&#28982;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#20102;BodoBERT&#65292;&#19968;&#20010;&#29992;&#20110;Bodo&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#20026;Bodo&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#30340;&#21162;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#30340;Bodo&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#12290;&#35813;&#35789;&#24615;&#26631;&#27880;&#27169;&#22411;&#22522;&#20110;BiLSTM&#21644;CRF&#30340;&#32452;&#21512;&#65292;&#20197;&#21450;BodoBERT&#19982;BytePairEmbeddings&#30340;&#22534;&#21472;&#23884;&#20837;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#35328;...&#65288;&#25688;&#35201;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
Language Processing systems such as Part-of-speech tagging, Named entity recognition, Machine translation, Speech recognition, and Language modeling (LM) are well-studied in high-resource languages. Nevertheless, research on these systems for several low-resource languages, including Bodo, Mizo, Nagamese, and others, is either yet to commence or is in its nascent stages. Language model plays a vital role in the downstream tasks of modern NLP. Extensive studies are carried out on LMs for high-resource languages. Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack coverage. In this study, we first present BodoBERT, a language model for the Bodo language. To the best of our knowledge, this work is the first such effort to develop a language model for Bodo. Secondly, we present an ensemble DL-based POS tagging model for Bodo. The POS tagging model is based on combinations of BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We cover several lan
&lt;/p&gt;</description></item><item><title>UGGNet&#36890;&#36807;&#32467;&#21512;U-Net&#21644;VGG&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2401.03173</link><description>&lt;p&gt;
UGGNet&#65306;&#36830;&#25509;U-Net&#21644;VGG&#20197;&#25552;&#39640;&#20083;&#33146;&#30284;&#35786;&#26029;&#30340;&#20808;&#36827;&#24615;
&lt;/p&gt;
&lt;p&gt;
UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis. (arXiv:2401.03173v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03173
&lt;/p&gt;
&lt;p&gt;
UGGNet&#36890;&#36807;&#32467;&#21512;U-Net&#21644;VGG&#26550;&#26500;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20840;&#38754;&#35299;&#20915;&#26041;&#26696;&#26469;&#22686;&#24378;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20083;&#33146;&#30284;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#65292;&#20083;&#33146;&#36229;&#22768;&#25104;&#20026;&#26089;&#26399;&#20083;&#33146;&#30284;&#26816;&#27979;&#30340;&#37325;&#35201;&#35786;&#26029;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#35786;&#26029;&#21463;&#21040;&#21307;&#29983;&#32463;&#39564;&#30340;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#30830;&#23450;&#21463;&#24433;&#21709;&#21306;&#22495;&#20301;&#32622;&#21644;&#30149;&#21464;&#31243;&#24230;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UGGNet&#30340;&#26032;&#22411;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;U-Net&#21644;VGG&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#20197;&#22686;&#24378;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#20998;&#26512;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#30340;U-Net&#32452;&#20214;&#21487;&#20197;&#31934;&#30830;&#20998;&#21106;&#30149;&#21464;&#65292;&#32780;VGG&#32452;&#20214;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#23618;&#25552;&#21462;&#29305;&#24449;&#12290;UGGNet&#20013;&#36825;&#20004;&#31181;&#26550;&#26500;&#30340;&#34701;&#21512;&#26088;&#22312;&#20248;&#21270;&#20998;&#21106;&#21644;&#29305;&#24449;&#34920;&#31034;&#65292;&#20026;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#20934;&#30830;&#35786;&#26029;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UGGNet&#27169;&#22411;&#22312;&#8220;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#25968;&#25454;&#38598;&#8221;&#19978;&#21462;&#24471;&#20102;78.2&#65285;&#30340;&#26174;&#33879;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of medical imaging, breast ultrasound has emerged as a crucial diagnostic tool for early detection of breast cancer. However, the accuracy of diagnosing the location of the affected area and the extent of the disease depends on the experience of the physician. In this paper, we propose a novel model called UGGNet, combining the power of the U-Net and VGG architectures to enhance the performance of breast ultrasound image analysis. The U-Net component of the model helps accurately segment the lesions, while the VGG component utilizes deep convolutional layers to extract features. The fusion of these two architectures in UGGNet aims to optimize both segmentation and feature representation, providing a comprehensive solution for accurate diagnosis in breast ultrasound images. Experimental results have demonstrated that the UGGNet model achieves a notable accuracy of 78.2% on the "Breast Ultrasound Images Dataset."
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#38738;&#23569;&#24180;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#65292;&#20851;&#27880;&#20799;&#31461;&#23545;&#25233;&#37057;&#30340;&#32463;&#21382;&#21644;&#26085;&#24120;&#29983;&#27963;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.03171</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#21475;&#26222;&#26597;&#35843;&#26597;&#21644;&#19968;&#33324;&#29983;&#27963;&#38382;&#39064;&#30340;&#38738;&#23569;&#24180;&#25233;&#37057;&#39118;&#38505;&#39044;&#27979;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues. (arXiv:2401.03171v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#38738;&#23569;&#24180;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#65292;&#20851;&#27880;&#20799;&#31461;&#23545;&#25233;&#37057;&#30340;&#32463;&#21382;&#21644;&#26085;&#24120;&#29983;&#27963;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#31038;&#20250;&#65292;&#29983;&#27963;&#21644;&#24037;&#20316;&#30340;&#21387;&#21147;&#19981;&#26029;&#22686;&#21152;&#65292;&#24515;&#29702;&#30142;&#30149;&#24050;&#25104;&#20026;&#29616;&#20195;&#20581;&#24247;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#32780;COVID-19&#30123;&#24773;&#36827;&#19968;&#27493;&#20984;&#26174;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#38738;&#23569;&#24180;&#25233;&#37057;&#30340;&#24739;&#30149;&#29575;&#27491;&#22312;&#31283;&#27493;&#19978;&#21319;&#65292;&#32780;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#65292;&#22914;&#37327;&#34920;&#25110;&#38754;&#35797;&#65292;&#23545;&#20110;&#26816;&#27979;&#38738;&#23569;&#24180;&#25233;&#37057;&#26174;&#24471;&#23588;&#20026;&#19981;&#36275;&#12290;&#38754;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#26469;&#36741;&#21161;&#35786;&#26029;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#37327;&#34920;&#23384;&#22312;&#30340;&#22522;&#26412;&#38382;&#39064;&#19978;&#65292;&#25110;&#32773;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#22914;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#12290;&#26681;&#25454;&#26085;&#24120;&#20064;&#24815;&#21644;&#34892;&#20026;&#26469;&#35786;&#26029;&#25233;&#37057;&#39118;&#38505;&#30340;&#30740;&#31350;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#38738;&#23569;&#24180;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#26469;&#39044;&#27979;&#25233;&#37057;&#39118;&#38505;&#65292;&#37325;&#28857;&#20851;&#27880;&#20799;&#31461;&#23545;&#25233;&#37057;&#30340;&#32463;&#21382;&#21644;&#20182;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In contemporary society, the escalating pressures of life and work have propelled psychological disorders to the forefront of modern health concerns, an issue that has been further accentuated by the COVID-19 pandemic. The prevalence of depression among adolescents is steadily increasing, and traditional diagnostic methods, which rely on scales or interviews, prove particularly inadequate for detecting depression in young people. Addressing these challenges, numerous AI-based methods for assisting in the diagnosis of mental health issues have emerged. However, most of these methods center around fundamental issues with scales or use multimodal approaches like facial expression recognition. Diagnosis of depression risk based on everyday habits and behaviors has been limited to small-scale qualitative studies. Our research leverages adolescent census data to predict depression risk, focusing on children's experiences with depression and their daily life situations. We introduced a method
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;(DG)&#35774;&#32622;&#20013;&#65292;&#33258;&#30417;&#30563;&#27169;&#22411;&#19981;&#22914;&#26377;&#30417;&#30563;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#24471;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38745;&#40664;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#35748;&#20026;&#20445;&#30041;&#36825;&#20123;&#38745;&#40664;&#29305;&#24449;&#21487;&#20197;&#38477;&#20302;&#27979;&#35797;&#39046;&#22495;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.03170</link><description>&lt;p&gt;
&#20445;&#30041;&#38745;&#40664;&#29305;&#24449;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Preserving Silent Features for Domain Generalization. (arXiv:2401.03170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#39046;&#22495;&#27867;&#21270;(DG)&#35774;&#32622;&#20013;&#65292;&#33258;&#30417;&#30563;&#27169;&#22411;&#19981;&#22914;&#26377;&#30417;&#30563;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#24471;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38745;&#40664;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#35748;&#20026;&#20445;&#30041;&#36825;&#20123;&#38745;&#40664;&#29305;&#24449;&#21487;&#20197;&#38477;&#20302;&#27979;&#35797;&#39046;&#22495;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#22312;&#26410;&#30693;&#27979;&#35797;&#39046;&#22495;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29992;&#24050;&#30693;&#30340;&#22810;&#20010;&#35757;&#32451;&#39046;&#22495;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;DG&#35774;&#32622;&#20013;&#24182;&#27809;&#26377;&#27604;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#25552;&#21462;&#30340;&#26356;&#20016;&#23500;&#30340;&#31867;&#20869;&#21306;&#20998;&#29305;&#24449;&#65292;&#21363;&#25105;&#20204;&#25152;&#31216;&#30340;&#38745;&#40664;&#29305;&#24449;&#65292;&#22312;&#26377;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#20013;&#34987;&#25233;&#21046;&#20102;&#12290;&#36825;&#20123;&#38745;&#40664;&#29305;&#24449;&#24456;&#21487;&#33021;&#21253;&#21547;&#20102;&#26356;&#20855;&#27867;&#21270;&#24615;&#30340;&#29305;&#24449;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#31181;&#29305;&#24449;&#25233;&#21046;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#21644;&#20998;&#26512;&#65292;&#24182;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#29702;&#35770;&#19978;&#35777;&#26126;&#20445;&#30041;&#38745;&#40664;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#39044;&#26399;&#27979;&#35797;&#39046;&#22495;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to improve the generalization ability of the model trained on several known training domains over unseen test domains. Previous work has shown that self-supervised contrastive pre-training improves the robustness of the model on downstream tasks. However, in this paper, we find that self-supervised models do not exhibit better generalization performance than supervised models pre-trained on the same dataset in the DG setting. We argue that this is owing to the fact that the richer intra-class discriminative features extracted by self-supervised contrastive learning, which we term silent features, are suppressed during supervised fine-tuning. These silent features are likely to contain features that are more generalizable on the test domain. In this work, we model and analyze this feature suppression phenomenon and theoretically prove that preserving silent features can achieve lower expected test domain risk under certain conditions. In light of this, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#21033;&#29992;Q&#20540;&#19982;&#25928;&#29992;&#20989;&#25968;&#25193;&#23637;&#20256;&#32479;Q&#23398;&#20064;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#23384;&#22312;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26680;&#24515;&#31639;&#27861;&#21464;&#20307;&#21644;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#21457;&#29616;&#22122;&#22768;Q&#20540;&#20272;&#35745;&#38382;&#39064;&#23545;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#26377;&#20851;&#38190;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.03163</link><description>&lt;p&gt;
&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments. (arXiv:2401.03163v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#21033;&#29992;Q&#20540;&#19982;&#25928;&#29992;&#20989;&#25968;&#25193;&#23637;&#20256;&#32479;Q&#23398;&#20064;&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#23384;&#22312;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#26680;&#24515;&#31639;&#27861;&#21464;&#20307;&#21644;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#21457;&#29616;&#22122;&#22768;Q&#20540;&#20272;&#35745;&#38382;&#39064;&#23545;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#26377;&#20851;&#38190;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#36890;&#36807;&#20351;&#29992;&#30690;&#37327;Q&#20540;&#19982;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;Q&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#20248;&#21270;&#26631;&#37327;&#21270;&#39044;&#26399;&#22238;&#25253;&#65288;SER&#65289;&#20934;&#21017;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#23545;&#24433;&#21709;&#22522;&#20110;&#20215;&#20540;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;Q&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;SER&#26368;&#20248;&#31574;&#30053;&#39057;&#29575;&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#35814;&#32454;&#32771;&#23519;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;&#20960;&#31181;&#26680;&#24515;&#22810;&#30446;&#26631;Q&#23398;&#20064;&#31639;&#27861;&#30340;&#21464;&#20307;&#20197;&#21450;&#22870;&#21169;&#24037;&#31243;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22122;&#22768;Q&#20540;&#20272;&#35745;&#38382;&#39064;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25910;&#25947;&#24615;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
One common approach to solve multi-objective reinforcement learning (MORL) problems is to extend conventional Q-learning by using vector Q-values in combination with a utility function. However issues can arise with this approach in the context of stochastic environments, particularly when optimising for the Scalarised Expected Reward (SER) criterion. This paper extends prior research, providing a detailed examination of the factors influencing the frequency with which value-based MORL Q-learning algorithms learn the SER-optimal policy for an environment with stochastic state transitions. We empirically examine several variations of the core multi-objective Q-learning algorithm as well as reward engineering approaches, and demonstrate the limitations of these methods. In particular, we highlight the critical impact of the noisy Q-value estimates issue on the stability and convergence of these algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QoS&#24863;&#30693;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;QAGCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#38543;&#26426;&#24615;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#22270;&#26469;&#35299;&#20915;&#32593;&#32476;&#26381;&#21153;&#25512;&#33616;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03162</link><description>&lt;p&gt;
QoS&#24863;&#30693;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#32593;&#32476;&#26381;&#21153;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
QoS-Aware Graph Contrastive Learning for Web Service Recommendation. (arXiv:2401.03162v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QoS&#24863;&#30693;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;QAGCL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#38543;&#26426;&#24615;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#22270;&#26469;&#35299;&#20915;&#32593;&#32476;&#26381;&#21153;&#25512;&#33616;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#26381;&#21153;&#25216;&#26415;&#30340;&#36827;&#27493;&#65292;&#20113;&#26381;&#21153;&#30340;&#24555;&#36895;&#22686;&#38271;&#20351;&#24471;&#20174;&#20247;&#22810;&#36873;&#39033;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#26381;&#21153;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#36136;&#37327;&#26381;&#21153;&#65288;QoS&#65289;&#35299;&#20915;&#32593;&#32476;&#26381;&#21153;&#25512;&#33616;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QoS&#24863;&#30693;&#30340;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;QAGCL&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#36827;&#34892;&#32593;&#32476;&#26381;&#21153;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#22788;&#29702;&#20919;&#21551;&#21160;&#38382;&#39064;&#24182;&#26377;&#25928;&#25552;&#39640;&#25512;&#33616;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#22320;&#29702;&#20301;&#32622;&#20449;&#24687;&#21644;&#38543;&#26426;&#24615;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#22270;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#35270;&#35282;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#22270;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#25105;&#20204;&#20174;&#36825;&#20123;&#22686;&#24378;&#22270;&#20013;&#23398;&#20064;&#29992;&#25143;&#21644;&#26381;&#21153;&#30340;&#23884;&#20837;&#12290;&#28982;&#21518;&#21033;&#29992;&#23398;&#21040;&#30340;&#23884;&#20837;&#23558;QoS&#32771;&#34385;&#26080;&#32541;&#38598;&#25104;&#21040;&#25512;&#33616;&#36807;&#31243;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#21644;&#20919;&#21551;&#21160;&#38382;&#39064;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of cloud services driven by advancements in web service technology, selecting a high-quality service from a wide range of options has become a complex task. This study aims to address the challenges of data sparsity and the cold-start problem in web service recommendation using Quality of Service (QoS). We propose a novel approach called QoS-aware graph contrastive learning (QAGCL) for web service recommendation. Our model harnesses the power of graph contrastive learning to handle cold-start problems and improve recommendation accuracy effectively. By constructing contextually augmented graphs with geolocation information and randomness, our model provides diverse views. Through the use of graph convolutional networks and graph contrastive learning techniques, we learn user and service embeddings from these augmented graphs. The learned embeddings are then utilized to seamlessly integrate QoS considerations into the recommendation process. Experimental results de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#36710;&#32852;&#32593;&#20013;&#20445;&#25345;&#27963;&#21160;&#29366;&#24577;&#30340;&#25104;&#26412;&#12290;&#36890;&#36807;&#36873;&#25321;&#37051;&#23621;&#20013;&#35780;&#20272;&#26368;&#39640;&#30340;&#23458;&#25143;&#31471;&#65292;&#32771;&#34385;&#26679;&#26412;&#25968;&#37327;&#12289;&#21534;&#21520;&#37327;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#26412;&#22320;&#25968;&#25454;&#38598;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#31946;&#36923;&#36753;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25552;&#35758;&#25509;&#36817;c</title><link>http://arxiv.org/abs/2401.03159</link><description>&lt;p&gt;
&#22312;&#36741;&#21161;&#29289;&#32852;&#32593;&#36710;&#32852;&#32593;&#20013;&#20855;&#26377;&#22810;&#30446;&#26631;&#30340;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Distributed client selection with multi-objective in federated learning assisted Internet of Vehicles. (arXiv:2401.03159v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#36710;&#32852;&#32593;&#20013;&#20445;&#25345;&#27963;&#21160;&#29366;&#24577;&#30340;&#25104;&#26412;&#12290;&#36890;&#36807;&#36873;&#25321;&#37051;&#23621;&#20013;&#35780;&#20272;&#26368;&#39640;&#30340;&#23458;&#25143;&#31471;&#65292;&#32771;&#34385;&#26679;&#26412;&#25968;&#37327;&#12289;&#21534;&#21520;&#37327;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#26412;&#22320;&#25968;&#25454;&#38598;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#31946;&#36923;&#36753;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25552;&#35758;&#25509;&#36817;c
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#36710;&#32852;&#32593;&#20013;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36710;&#32852;&#32593;&#20013;&#65292;&#25968;&#30334;&#19975;&#36742;&#36710;&#24895;&#24847;&#35757;&#32451;&#27169;&#22411;&#20197;&#20849;&#20139;&#23427;&#20204;&#30340;&#30693;&#35782;&#12290;&#20445;&#25345;&#27963;&#21160;&#29366;&#24577;&#24847;&#21619;&#30528;&#21442;&#19982;&#32773;&#24517;&#39035;&#22312;&#22266;&#23450;&#38388;&#38548;&#20869;&#26356;&#26032;&#20854;&#29366;&#24577;&#24182;&#21442;&#19982;&#21040;&#19979;&#19968;&#36718;&#20013;&#12290;&#28982;&#32780;&#65292;&#24403;&#26377;&#22823;&#37327;&#21442;&#19982;&#30340;&#36710;&#36742;&#26102;&#65292;&#20445;&#25345;&#27963;&#21160;&#29366;&#24577;&#30340;&#25104;&#26412;&#26159;&#38750;&#24120;&#22823;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#36873;&#25321;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;&#25152;&#26377;&#21442;&#19982;&#32773;&#32500;&#25252;&#27963;&#21160;&#29366;&#24577;&#30340;&#25104;&#26412;&#12290;&#36873;&#25321;&#20855;&#26377;&#26368;&#39640;&#35780;&#20272;&#30340;&#37051;&#23621;&#23458;&#25143;&#31471;&#12290;&#22312;&#35780;&#20272;&#22120;&#20013;&#65292;&#32771;&#34385;&#20102;&#22235;&#20010;&#21464;&#37327;&#65292;&#21253;&#25324;&#26679;&#26412;&#25968;&#37327;&#12289;&#21487;&#29992;&#21534;&#21520;&#37327;&#12289;&#35745;&#31639;&#33021;&#21147;&#21644;&#26412;&#22320;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#30001;&#20110;&#22235;&#20010;&#21464;&#37327;&#30340;&#38381;&#24335;&#35299;&#19981;&#23384;&#22312;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#31946;&#36923;&#36753;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#24191;&#27867;&#30340;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#25552;&#35758;&#19982;c
&lt;/p&gt;
&lt;p&gt;
Federated learning is an emerging distributed machine learning framework in the Internet of Vehicles (IoV). In IoV, millions of vehicles are willing to train the model to share their knowledge. Maintaining an active state means the participants must update their state to the FL server in a fixed interval and participate to next round. However, the cost by maintaining an active state is very large when there are a huge number of participating vehicles. In this paper, we proposed a distributed client selection scheme to reduce the cost of maintaining the active state for all participants. The clients with the highest evaluation are elected among the neighbours. In the evaluator, four variables are considered including sample quantity, throughput available, computational capability and the quality of the local dataset. We adopted fuzzy logic as the evaluator since the closed-form solution over four variables does not exist. Extensive simulation results show our proposal approximates the c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21253;&#21547;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#21644;&#23545;&#25239;&#39044;&#31639;&#30340;&#21464;&#21270;&#23545;&#40065;&#26834;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#65292;&#24471;&#21040;&#20102;&#19982;&#22343;&#21248;&#31283;&#23450;&#24615;&#30456;&#24403;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2401.03156</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#30340;&#25968;&#25454;&#30456;&#20851;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data-Dependent Stability Analysis of Adversarial Training. (arXiv:2401.03156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21253;&#21547;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#20449;&#24687;&#65292;&#36890;&#36807;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#21644;&#23545;&#25239;&#39044;&#31639;&#30340;&#21464;&#21270;&#23545;&#40065;&#26834;&#27867;&#21270;&#24046;&#36317;&#30340;&#24433;&#21709;&#65292;&#24471;&#21040;&#20102;&#19982;&#22343;&#21248;&#31283;&#23450;&#24615;&#30456;&#24403;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#24615;&#20998;&#26512;&#26159;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#28041;&#21450;&#21040;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#35757;&#32451;&#31639;&#27861;&#30340;&#25512;&#23548;&#27867;&#21270;&#30028;&#38480;&#12290;&#23545;&#25239;&#35757;&#32451;&#26159;&#26368;&#24120;&#29992;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#23545;&#25239;&#35757;&#32451;&#27867;&#21270;&#30028;&#38480;&#24182;&#26410;&#21253;&#21547;&#25968;&#25454;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#23545;&#25239;&#35757;&#32451;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#32435;&#20837;&#20102;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#12290;&#25105;&#20204;&#21033;&#29992;&#24179;&#22343;&#31283;&#23450;&#24615;&#21644;&#39640;&#38454;&#36817;&#20284;Lipschitz&#26465;&#20214;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#21644;&#23545;&#25239;&#39044;&#31639;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#40065;&#26834;&#27867;&#21270;&#24046;&#36317;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#20984;&#25439;&#22833;&#21644;&#38750;&#20984;&#25439;&#22833;&#30340;&#27867;&#21270;&#30028;&#38480;&#33267;&#23569;&#19982;&#19981;&#21253;&#21547;&#25968;&#25454;&#20998;&#24067;&#20449;&#24687;&#30340;&#22343;&#21248;&#31283;&#23450;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stability analysis is an essential aspect of studying the generalization ability of deep learning, as it involves deriving generalization bounds for stochastic gradient descent-based training algorithms. Adversarial training is the most widely used defense against adversarial example attacks. However, previous generalization bounds for adversarial training have not included information regarding the data distribution. In this paper, we fill this gap by providing generalization bounds for stochastic gradient descent-based adversarial training that incorporate data distribution information. We utilize the concepts of on-average stability and high-order approximate Lipschitz conditions to examine how changes in data distribution and adversarial budget can affect robust generalization gaps. Our derived generalization bounds for both convex and non-convex losses are at least as good as the uniform stability-based counterparts which do not include data distribution information. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2401.03154</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#24403;&#30446;&#26631;&#36229;&#36807;&#26234;&#33021;&#20307;&#25968;&#37327;&#26102;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22788;&#29702;&#22810;&#30446;&#26631;&#36319;&#36394;&#30340;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22312;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#23454;&#29616;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#65292;&#24182;&#20351;&#29992;&#24322;&#27493;&#26234;&#33021;&#20307;&#36890;&#20449;&#26469;&#21327;&#35843;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#22810;&#30446;&#26631;&#36319;&#36394;&#22312;&#37326;&#29983;&#21160;&#29289;&#24033;&#36923;&#12289;&#23433;&#20840;&#30417;&#25511;&#25110;&#29615;&#22659;&#30417;&#27979;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#29616;&#26377;&#31639;&#27861;&#24120;&#24120;&#20570;&#20986;&#19968;&#20123;&#38480;&#21046;&#24615;&#20551;&#35774;&#65306;&#30446;&#26631;&#25968;&#37327;&#21644;&#21021;&#22987;&#20301;&#32622;&#24050;&#30693;&#65292;&#25110;&#32773;&#26234;&#33021;&#20307;&#24050;&#34987;&#39044;&#20998;&#37197;&#21040;&#30417;&#25511;&#29615;&#22659;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#65292;&#20943;&#36731;&#20102;&#25506;&#32034;&#30340;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#23569;&#20110;&#30446;&#26631;&#25968;&#37327;&#26102;&#65292;&#36825;&#31181;&#20551;&#35774;&#20250;&#38480;&#21046;&#31639;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#26234;&#33021;&#20307;&#26080;&#27861;&#25345;&#32493;&#36319;&#36394;&#20854;&#35270;&#37326;&#20013;&#30340;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#22810;&#26234;&#33021;&#20307;&#36319;&#36394;&#31639;&#27861;&#36824;&#20551;&#35774;&#26234;&#33021;&#20307;&#38388;&#35266;&#27979;&#30340;&#21516;&#27493;&#65292;&#25110;&#32773;&#38656;&#35201;&#19968;&#20010;&#20013;&#22830;&#25511;&#21046;&#22120;&#26469;&#21327;&#35843;&#32852;&#21512;&#21160;&#20316;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#20998;&#25955;&#24335;&#22810;&#26234;&#33021;&#20307;&#12289;&#22810;&#30446;&#26631;&#12289;&#21516;&#26102;&#20027;&#21160;&#25628;&#32034;&#21644;&#36319;&#36394;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#38388;&#36890;&#20449;&#26159;&#24322;&#27493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;DecSTER&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#20551;&#35774;&#23494;&#24230;&#28388;&#27874;&#22120;&#30340;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent multi-target tracking has a wide range of applications, including wildlife patrolling, security surveillance or environment monitoring. Such algorithms often make restrictive assumptions: the number of targets and/or their initial locations may be assumed known, or agents may be pre-assigned to monitor disjoint partitions of the environment, reducing the burden of exploration. This also limits applicability when there are fewer agents than targets, since agents are unable to continuously follow the targets in their fields of view. Multi-agent tracking algorithms additionally assume inter-agent synchronization of observations, or the presence of a central controller to coordinate joint actions. Instead, we focus on the setting of decentralized multi-agent, multi-target, simultaneous active search-and-tracking with asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a sequential monte carlo implementation of the probability hypothesis density filter fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#22312;&#24037;&#19994;&#25968;&#25454;&#19978;&#23454;&#29616;&#21487;&#25511;&#30340;&#22270;&#20687;&#21512;&#25104;&#65292;&#20174;&#32780;&#20801;&#35768;&#29983;&#25104;&#33258;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#24037;&#19994;&#25968;&#25454;&#35757;&#32451;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#28385;&#36275;&#29305;&#23450;&#25299;&#25169;&#29305;&#24449;&#21644;&#20960;&#20309;&#32570;&#38519;&#20301;&#32622;&#35201;&#27714;&#30340;&#24037;&#19994;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2401.03152</link><description>&lt;p&gt;
&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#30340;&#24037;&#19994;&#25968;&#25454;&#21487;&#25511;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controllable Image Synthesis of Industrial Data Using Stable Diffusion. (arXiv:2401.03152v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03152
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#22312;&#24037;&#19994;&#25968;&#25454;&#19978;&#23454;&#29616;&#21487;&#25511;&#30340;&#22270;&#20687;&#21512;&#25104;&#65292;&#20174;&#32780;&#20801;&#35768;&#29983;&#25104;&#33258;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#38024;&#23545;&#24037;&#19994;&#25968;&#25454;&#35757;&#32451;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#28385;&#36275;&#29305;&#23450;&#25299;&#25169;&#29305;&#24449;&#21644;&#20960;&#20309;&#32570;&#38519;&#20301;&#32622;&#35201;&#27714;&#30340;&#24037;&#19994;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#30417;&#30563;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#32570;&#38519;&#26816;&#27979;&#21644;&#20998;&#21106;&#38656;&#35201;&#22823;&#35268;&#27169;&#23436;&#20840;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#36825;&#31181;&#25968;&#25454;&#38598;&#24448;&#24448;&#24456;&#38590;&#33719;&#24471;&#29978;&#33267;&#19981;&#21487;&#33021;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#19968;&#31181;&#25193;&#22823;&#23567;&#35268;&#27169;&#24037;&#19994;&#25968;&#25454;&#38598;&#30340;&#26426;&#20250;&#65292;&#20174;&#32780;&#20351;&#24471;&#24037;&#19994;&#30028;&#33021;&#22815;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;&#26041;&#27861;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#22909;&#30340;&#29983;&#25104;&#27169;&#22411;&#20063;&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#65292;&#32780;&#24037;&#19994;&#25968;&#25454;&#38598;&#24448;&#24448;&#24456;&#23567;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24037;&#19994;&#25968;&#25454;&#19978;&#37325;&#22797;&#20351;&#29992;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#32456;&#23454;&#29616;&#29983;&#25104;&#33258;&#26631;&#35760;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35753;&#27169;&#22411;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#65292;&#28041;&#21450;&#26032;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36843;&#20351;&#23427;&#23398;&#20064;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#26465;&#20214;&#38480;&#21046;&#65292;&#20135;&#29983;&#20855;&#26377;&#33391;&#22909;&#23450;&#20041;&#30340;&#25299;&#25169;&#29305;&#24449;&#24182;&#19988;&#20855;&#26377;&#32473;&#23450;&#20960;&#20309;&#24418;&#29366;&#21644;&#20301;&#32622;&#32570;&#38519;&#30340;&#24037;&#19994;&#22270;&#20687;&#12290;&#20026;&#20102;&#31361;&#20986;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;
Training supervised deep neural networks that perform defect detection and segmentation requires large-scale fully-annotated datasets, which can be hard or even impossible to obtain in industrial environments. Generative AI offers opportunities to enlarge small industrial datasets artificially, thus enabling the usage of state-of-the-art supervised approaches in the industry. Unfortunately, also good generative models need a lot of data to train, while industrial datasets are often tiny. Here, we propose a new approach for reusing general-purpose pre-trained generative models on industrial data, ultimately allowing the generation of self-labelled defective images. First, we let the model learn the new concept, entailing the novel data distribution. Then, we force it to learn to condition the generative process, producing industrial images that satisfy well-defined topological characteristics and show defects with a given geometry and location. To highlight the advantage of our approach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DQNLog&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;DQN&#31639;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#35760;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#24322;&#24120;&#29615;&#22659;&#20132;&#20114;&#21644;&#20027;&#21160;&#25506;&#32034;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#23398;&#20064;&#24050;&#30693;&#30340;&#24322;&#24120;&#24182;&#21457;&#29616;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2401.03151</link><description>&lt;p&gt;
&#36890;&#36807;DQN&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning via DQN for log anomaly detection. (arXiv:2401.03151v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;DQNLog&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;DQN&#31639;&#27861;&#65292;&#21033;&#29992;&#23569;&#37327;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#35760;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#24322;&#24120;&#29615;&#22659;&#20132;&#20114;&#21644;&#20027;&#21160;&#25506;&#32034;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#23398;&#20064;&#24050;&#30693;&#30340;&#24322;&#24120;&#24182;&#21457;&#29616;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#22312;&#20445;&#38556;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#32500;&#25252;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#65292;&#26816;&#27979;&#26085;&#24535;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#36890;&#36807;&#30417;&#30563;&#24335;&#24322;&#24120;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24448;&#24448;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#32467;&#21512;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;DQN&#31639;&#27861;&#65292;&#31216;&#20026;DQNLog&#12290;DQNLog&#21033;&#29992;&#23569;&#37327;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21644;&#22823;&#35268;&#27169;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#26631;&#35760;&#25968;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#36890;&#36807;&#19982;&#20559;&#21521;&#20110;&#24322;&#24120;&#30340;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#23398;&#20064;&#24050;&#30693;&#30340;&#24322;&#24120;&#65292;&#36824;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#26080;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#21457;&#29616;&#26410;&#30693;&#30340;&#24322;&#24120;&#12290;&#27492;&#22806;&#65292;DQNLog&#36824;&#24341;&#20837;&#20102;&#20132;&#21449;&#29109;&#25439;&#22833;&#39033;&#65292;&#38450;&#27490;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#27169;&#22411;&#36807;&#39640;&#20272;&#35745;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Log anomaly detection plays a critical role in ensuring the security and maintenance of modern software systems. At present, the primary approach for detecting anomalies in log data is through supervised anomaly detection. Nonetheless, existing supervised methods heavily rely on labeled data, which can be frequently limited in real-world scenarios. In this paper, we propose a semi-supervised log anomaly detection method that combines the DQN algorithm from deep reinforcement learning, which is called DQNLog. DQNLog leverages a small amount of labeled data and a large-scale unlabeled dataset, effectively addressing the challenges of imbalanced data and limited labeling. This approach not only learns known anomalies by interacting with an environment biased towards anomalies but also discovers unknown anomalies by actively exploring the unlabeled dataset. Additionally, DQNLog incorporates a cross-entropy loss term to prevent model overestimation during Deep Reinforcement Learning (DRL). 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2401.03140</link><description>&lt;p&gt;
&#36890;&#36807;&#20999;&#25442;&#26426;&#21046;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20844;&#24179;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#30340;&#20844;&#24179;&#25277;&#26679;&#26426;&#21046;&#65292;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#33391;&#22909;&#36924;&#36817;&#28508;&#22312;&#27010;&#29575;&#20998;&#24067;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#20844;&#24179;&#24615;&#26041;&#38754;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#30340;&#20869;&#22312;&#20559;&#24046;&#30340;&#25918;&#22823;&#12290;&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#30340;&#25277;&#26679;&#36807;&#31243;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#24341;&#23548;&#26469;&#25511;&#21046;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25214;&#21040;&#23454;&#35777;&#24341;&#23548;&#26469;&#23454;&#29616;&#23450;&#37327;&#20844;&#24179;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#23646;&#24615;&#20999;&#25442;&#8221;&#26426;&#21046;&#30340;&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#25277;&#26679;&#26041;&#27861;&#65292;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#25277;&#26679;&#26041;&#27861;&#21487;&#20197;&#22312;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#28151;&#28102;&#25935;&#24863;&#23646;&#24615;&#65292;&#32780;&#19981;&#20381;&#36182;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#20174;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;(i)&#29983;&#25104;&#20844;&#24179;&#25968;&#25454;&#21644;(ii)&#20445;&#25345;&#29983;&#25104;&#25968;&#25454;&#30340;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown their effectiveness in generation tasks by well-approximating the underlying probability distribution. However, diffusion models are known to suffer from an amplified inherent bias from the training data in terms of fairness. While the sampling process of diffusion models can be controlled by conditional guidance, previous works have attempted to find empirical guidance to achieve quantitative fairness. To address this limitation, we propose a fairness-aware sampling method called \textit{attribute switching} mechanism for diffusion models. Without additional training, the proposed sampling can obfuscate sensitive attributes in generated data without relying on classifiers. We mathematically prove and experimentally demonstrate the effectiveness of the proposed method on two key aspects: (i) the generation of fair data and (ii) the preservation of the utility of the generated data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25968;&#25454;&#26469;&#25913;&#36827;&#20132;&#36890;&#35780;&#20272;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#21464;&#37327;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#38754;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25972;&#21512;&#21040;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.03138</link><description>&lt;p&gt;
TelTrans&#65306;&#36890;&#36807;&#22810;&#26041;&#38754;&#30340;&#22270;&#27169;&#22411;&#23558;&#22810;&#31181;&#31867;&#22411;&#30340;&#30005;&#20449;&#25968;&#25454;&#24212;&#29992;&#20110;&#20132;&#36890;&#35780;&#20272;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling. (arXiv:2401.03138v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03138
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25968;&#25454;&#26469;&#25913;&#36827;&#20132;&#36890;&#35780;&#20272;&#21644;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#22810;&#21464;&#37327;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#38754;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#25972;&#21512;&#21040;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22522;&#20110;&#20301;&#32622;&#36793;&#30028;&#26816;&#27979;&#22120;&#30340;&#20132;&#36890;&#39044;&#27979;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22320;&#29702;&#21333;&#20803;&#20132;&#36890;&#27969;&#65288;GCT&#65289;&#27969;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#24191;&#27867;&#30340;&#34562;&#31389;&#20132;&#36890;&#35206;&#30422;&#26469;&#25429;&#25417;&#31227;&#21160;&#27169;&#24335;&#30340;&#26032;&#22411;&#25968;&#25454;&#26469;&#28304;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#20998;&#26512;&#39564;&#35777;&#20102;&#23427;&#22312;&#20132;&#36890;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#38024;&#23545;&#19982;&#36710;&#36742;&#30456;&#20851;&#30340;GCT&#27969;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#32467;&#21512;&#20102;&#22810;&#21464;&#37327;&#12289;&#26102;&#38388;&#21644;&#31354;&#38388;&#26041;&#38754;&#65292;&#20197;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#20248;&#36234;&#24615;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#23558;GCT&#27969;&#25972;&#21512;&#21040;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the limitations of traffic prediction from location-bound detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data source that leverages the extensive coverage of cellular traffic to capture mobility patterns. Our extensive analysis validates its potential for transportation. Focusing on vehicle-related GCT flow prediction, we propose a graph neural network that integrates multivariate, temporal, and spatial facets for improved accuracy. Experiments reveal our model's superiority over baselines, especially in long-term predictions. We also highlight the potential for GCT flow integration into transportation systems.
&lt;/p&gt;</description></item><item><title>SPQR&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#23792;&#38543;&#26426;&#27169;&#22411;&#26469;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;&#20013;Q-&#38598;&#21512;&#30340;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#20811;&#26381;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.03137</link><description>&lt;p&gt;
SPQR:&#20351;&#29992;&#23574;&#23792;&#38543;&#26426;&#27169;&#22411;&#25511;&#21046;Q-&#38598;&#21512;&#30340;&#29420;&#31435;&#24615;&#65292;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03137
&lt;/p&gt;
&lt;p&gt;
SPQR&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23574;&#23792;&#38543;&#26426;&#27169;&#22411;&#26469;&#25511;&#21046;&#24378;&#21270;&#23398;&#20064;&#20013;Q-&#38598;&#21512;&#30340;&#29420;&#31435;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#26469;&#20811;&#26381;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#26159;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26356;&#22797;&#26434;&#20219;&#21153;&#25110;&#21253;&#21547;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#25104;&#21151;&#34920;&#29616;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#65292;&#30740;&#31350;&#20102;Q-learning&#30340;&#38598;&#25104;&#26041;&#27861;&#26469;&#21033;&#29992;&#22810;&#20010;Q&#20989;&#25968;&#30340;&#22810;&#26679;&#24615;&#12290;&#30001;&#20110;&#32593;&#32476;&#21021;&#22987;&#21270;&#19968;&#30452;&#26159;&#20419;&#36827;Q&#20989;&#25968;&#22810;&#26679;&#24615;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22240;&#27492;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#20102;&#21551;&#21457;&#24335;&#35774;&#35745;&#30340;&#22810;&#26679;&#24615;&#27880;&#20837;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24182;&#26410;&#23581;&#35797;&#20174;&#29702;&#35770;&#35282;&#24230;&#20445;&#35777;&#38598;&#25104;&#30340;&#29420;&#31435;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;Q-&#38598;&#21512;&#29420;&#31435;&#24615;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23574;&#23792;Wishart Q-&#38598;&#21512;&#29420;&#31435;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65288;SPQR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alleviating overestimation bias is a critical challenge for deep reinforcement learning to achieve successful performance on more complex tasks or offline datasets containing out-of-distribution data. In order to overcome overestimation bias, ensemble methods for Q-learning have been investigated to exploit the diversity of multiple Q-functions. Since network initialization has been the predominant approach to promote diversity in Q-functions, heuristically designed diversity injection methods have been studied in the literature. However, previous studies have not attempted to approach guaranteed independence over an ensemble from a theoretical perspective. By introducing a novel regularization loss for Q-ensemble independence based on random matrix theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR) for reinforcement learning. Specifically, we modify the intractable hypothesis testing criterion for the Q-ensemble independence into a tractable KL divergence 
&lt;/p&gt;</description></item><item><title>TimeGraphs&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#20132;&#20114;&#24314;&#27169;&#20026;&#20998;&#23618;&#26102;&#38388;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.03134</link><description>&lt;p&gt;
TimeGraphs: &#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
TimeGraphs: Graph-based Temporal Reasoning. (arXiv:2401.03134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03134
&lt;/p&gt;
&lt;p&gt;
TimeGraphs&#26159;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21160;&#24577;&#20132;&#20114;&#24314;&#27169;&#20026;&#20998;&#23618;&#26102;&#38388;&#22270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#23637;&#31034;&#20102;&#26102;&#38388;&#19978;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#36825;&#20123;&#34892;&#20026;&#21487;&#20197;&#34987;&#25429;&#25417;&#20026;&#22797;&#26434;&#20195;&#29702;&#20132;&#20114;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#20026;&#20102;&#36827;&#34892;&#26102;&#38388;&#25512;&#29702;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#31616;&#21333;&#30340;&#24207;&#21015;&#27169;&#22411;&#26469;&#32534;&#30721;&#26102;&#38388;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26377;&#25928;&#25429;&#25417;&#36755;&#20837;&#20013;&#20016;&#23500;&#21160;&#24577;&#30340;&#20840;&#35889;&#26102;&#24448;&#24448;&#22833;&#36133;&#65292;&#22240;&#20026;&#21160;&#24577;&#24182;&#19981;&#22343;&#21248;&#20998;&#24067;&#12290;&#29305;&#21035;&#26159;&#65292;&#26377;&#20851;&#20449;&#24687;&#21487;&#33021;&#26356;&#38590;&#25552;&#21462;&#65292;&#24182;&#19988;&#21363;&#20351;&#23427;&#20204;&#19981;&#21253;&#21547;&#37325;&#22823;&#21464;&#21270;&#25110;&#26032;&#20449;&#24687;&#65292;&#20063;&#20250;&#28010;&#36153;&#35745;&#31639;&#33021;&#21147;&#26469;&#22788;&#29702;&#25152;&#26377;&#21333;&#29420;&#30340;&#26102;&#38388;&#27493;&#38271;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeGraphs&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#20132;&#20114;&#20316;&#20026;&#20998;&#23618;&#26102;&#38388;&#22270;&#26469;&#34920;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#39034;&#24207;&#34920;&#31034;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#32039;&#20945;&#30340;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26469;&#24314;&#27169;&#20132;&#20114;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#25512;&#29702;&#12290;&#37319;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;TimeGraphs&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#20107;&#20214;&#23618;&#27425;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Many real-world systems exhibit temporal, dynamic behaviors, which are captured as time series of complex agent interactions. To perform temporal reasoning, current methods primarily encode temporal dynamics through simple sequence-based models. However, in general these models fail to efficiently capture the full spectrum of rich dynamics in the input, since the dynamics is not uniformly distributed. In particular, relevant information might be harder to extract and computing power is wasted for processing all individual timesteps, even if they contain no significant changes or no new information. Here we propose TimeGraphs, a novel approach that characterizes dynamic interactions as a hierarchical temporal graph, diverging from traditional sequential representations. Our approach models the interactions using a compact graph-based representation, enabling adaptive reasoning across diverse time scales. Adopting a self-supervised method, TimeGraphs constructs a multi-level event hierar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Vision Transformers&#21644;Bi-LSTM&#22788;&#29702;MRI&#22270;&#20687;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#24182;&#20445;&#25345;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#22312;AD&#30340;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03132</link><description>&lt;p&gt;
Vision Transformers&#21644;Bi-LSTM&#29992;&#20110;3D MRI&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers and Bi-LSTM for Alzheimer's Disease Diagnosis from 3D MRI. (arXiv:2401.03132v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Vision Transformers&#21644;Bi-LSTM&#22788;&#29702;MRI&#22270;&#20687;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#35786;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29305;&#24449;&#24182;&#20445;&#25345;&#29305;&#24449;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#22312;AD&#30340;&#35786;&#26029;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#38543;&#26102;&#38388;&#24694;&#21270;&#24182;&#24433;&#21709;&#35760;&#24518;&#12289;&#24605;&#32500;&#21644;&#34892;&#20026;&#30340;&#22823;&#33041;&#30142;&#30149;&#12290;&#22914;&#26524;&#26089;&#26399;&#35786;&#26029;&#65292;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#26159;&#21487;&#20197;&#27835;&#30103;&#21644;&#31649;&#29702;&#30340;&#65292;&#21487;&#20197;&#20943;&#32531;&#30151;&#29366;&#30340;&#36827;&#23637;&#24182;&#25913;&#21892;&#29983;&#27963;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;Visual Transformer&#65288;ViT&#65289;&#21644;bi-LSTM&#26469;&#22788;&#29702;MRI&#22270;&#20687;&#36827;&#34892;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#35786;&#26029;&#12290;&#25105;&#20204;&#20351;&#29992;ViT&#20174;MRI&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#28982;&#21518;&#23558;&#20854;&#26144;&#23556;&#21040;&#29305;&#24449;&#24207;&#21015;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;Bi-LSTM&#24207;&#21015;&#24314;&#27169;&#26469;&#20445;&#25345;&#30456;&#20851;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#31070;&#32463;&#24433;&#20687;&#35745;&#21010;&#65288;ADNI&#65289;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#23545;AD&#24739;&#32773;&#30340;&#20108;&#20998;&#31867;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#25991;&#29486;&#20013;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#22312;AD&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#12289;&#31934;&#30830;&#24230;&#12289;F-score&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's is a brain disease that gets worse over time and affects memory, thinking, and behavior. Alzheimer's disease (AD) can be treated and managed if it is diagnosed early, which can slow the progression of symptoms and improve quality of life. In this study, we suggested using the Visual Transformer (ViT) and bi-LSTM to process MRI images for diagnosing Alzheimer's disease. We used ViT to extract features from the MRI and then map them to a feature sequence. Then, we used Bi-LSTM sequence modeling to keep the interdependencies between related features. In addition, we evaluated the performance of the proposed model for the binary classification of AD patients using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our method against other deep learning models in the literature. The proposed method performs well in terms of accuracy, precision, F-score, and recall for the diagnosis of AD.
&lt;/p&gt;</description></item><item><title>EdGeo&#24037;&#20855;&#21253;&#21033;&#29992;&#29289;&#29702;&#21407;&#29702;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22768;&#27874;&#26041;&#31243;&#29983;&#25104;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#26469;&#25913;&#21892;&#27169;&#22411;&#20462;&#21098;&#21518;&#30340;ML&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03131</link><description>&lt;p&gt;
&#29992;&#29289;&#29702;&#25351;&#23548;&#30340;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21253;&#36827;&#34892;&#22320;&#29699;&#29289;&#29702;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03131
&lt;/p&gt;
&lt;p&gt;
EdGeo&#24037;&#20855;&#21253;&#21033;&#29992;&#29289;&#29702;&#21407;&#29702;&#25351;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#22320;&#19979;&#36895;&#24230;&#22270;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#22768;&#27874;&#26041;&#31243;&#29983;&#25104;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#26469;&#25913;&#21892;&#27169;&#22411;&#20462;&#21098;&#21518;&#30340;ML&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#27874;&#24418;&#21453;&#28436;(FWI)&#22312;&#22320;&#29699;&#31185;&#23398;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#29992;&#20110;&#25506;&#32034;&#22320;&#19979;&#12290;&#23427;&#21033;&#29992;&#22320;&#38663;&#27874;&#26469;&#25104;&#20687;&#22320;&#19979;&#36895;&#24230;&#22270;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;(ML)&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20351;&#29992;ML&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#29992;&#20110;FWI&#20219;&#21153;&#65292;&#30456;&#27604;&#20256;&#32479;&#22522;&#20110;&#29289;&#29702;&#30340;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#22320;&#29699;&#31185;&#23398;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#27809;&#26377;&#25968;&#25454;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;ML&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#27169;&#22411;&#20462;&#21098;&#36807;&#31243;&#20013;&#21464;&#24471;&#26356;&#21152;&#20005;&#37325;&#65292;&#36825;&#26159;&#22320;&#29699;&#31185;&#23398;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#19968;&#27493;&#65292;&#22240;&#20026;&#29615;&#22659;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EdGeo&#24037;&#20855;&#21253;&#65292;&#23427;&#20351;&#29992;&#29289;&#29702;&#21407;&#29702;&#24341;&#23548;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#36895;&#24230;&#22270;&#12290;&#35813;&#24037;&#20855;&#21253;&#20351;&#29992;&#22768;&#27874;&#26041;&#31243;&#26469;&#29983;&#25104;&#30456;&#24212;&#30340;&#22320;&#38663;&#27874;&#24418;&#25968;&#25454;&#65292;&#20415;&#20110;&#20462;&#21098;&#21518;&#30340;ML&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65292;SSIM&#20998;&#25968;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;MAE&#21644;MSE&#12290;
&lt;/p&gt;
&lt;p&gt;
Full-waveform inversion (FWI) plays a vital role in geoscience to explore the subsurface. It utilizes the seismic wave to image the subsurface velocity map. As the machine learning (ML) technique evolves, the data-driven approaches using ML for FWI tasks have emerged, offering enhanced accuracy and reduced computational cost compared to traditional physics-based methods. However, a common challenge in geoscience, the unprivileged data, severely limits ML effectiveness. The issue becomes even worse during model pruning, a step essential in geoscience due to environmental complexities. To tackle this, we introduce the EdGeo toolkit, which employs a diffusion-based model guided by physics principles to generate high-fidelity velocity maps. The toolkit uses the acoustic wave equation to generate corresponding seismic waveform data, facilitating the fine-tuning of pruned ML models. Our results demonstrate significant improvements in SSIM scores and reduction in both MAE and MSE across vario
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#22810;&#20803;&#22238;&#24402;&#38382;&#39064;&#65292;&#21487;&#20197;&#28789;&#27963;&#24314;&#27169;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26465;&#20214;&#22343;&#20540;&#20989;&#25968;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#21464;&#37327;&#36873;&#25321;&#22312;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03123</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
A least distance estimator for a multivariate regression model using deep neural networks. (arXiv:2401.03123v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03123
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#23567;&#36317;&#31163;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#22810;&#20803;&#22238;&#24402;&#38382;&#39064;&#65292;&#21487;&#20197;&#28789;&#27963;&#24314;&#27169;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26465;&#20214;&#22343;&#20540;&#20989;&#25968;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25429;&#25417;&#20381;&#36182;&#20851;&#31995;&#21644;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#21464;&#37327;&#36873;&#25321;&#22312;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26368;&#23567;&#36317;&#31163;(LD)&#20272;&#35745;&#22120;(DNN-LD)&#29992;&#20110;&#22810;&#20803;&#22238;&#24402;&#38382;&#39064;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#30001;&#20110;DNN&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#24314;&#27169;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26465;&#20214;&#22343;&#20540;&#20989;&#25968;&#65292;&#24182;&#19988;&#21482;&#38656;&#22312;&#36755;&#20986;&#23618;&#28155;&#21152;&#39069;&#22806;&#33410;&#28857;&#21363;&#21487;&#23454;&#29616;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#27604;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#26356;&#26377;&#25928;&#22320;&#25429;&#25417;&#21709;&#24212;&#20043;&#38388;&#30340;&#20381;&#36182;&#32467;&#26500;&#65292;&#24182;&#19988;&#23545;&#24322;&#24120;&#20540;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#21464;&#37327;&#36873;&#25321;&#22312;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#20851;&#38190;&#24615;&#65292;&#24341;&#20837;&#20102;L1&#22411;&#24809;&#32602;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;(A)GDNN-LD&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#23558;(&#33258;&#36866;&#24212;)&#20998;&#32452;Lasso&#24809;&#32602;&#24212;&#29992;&#20110;DNN&#32467;&#26500;&#20013;&#30340;&#26435;&#37325;&#21442;&#25968;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#21464;&#37327;&#36873;&#25321;&#21644;&#27169;&#22411;&#20272;&#35745;&#12290;&#20026;&#20102;&#36827;&#34892;&#35745;&#31639;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#27425;&#24179;&#28369;&#36817;&#20284;&#26041;&#27861;&#65292;&#20197;&#20415;&#20248;&#21270;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a deep neural network (DNN) based least distance (LD) estimator (DNN-LD) for a multivariate regression problem, addressing the limitations of the conventional methods. Due to the flexibility of a DNN structure, both linear and nonlinear conditional mean functions can be easily modeled, and a multivariate regression model can be realized by simply adding extra nodes at the output layer. The proposed method is more efficient in capturing the dependency structure among responses than the least squares loss, and robust to outliers. In addition, we consider $L_1$-type penalization for variable selection, crucial in analyzing high-dimensional data. Namely, we propose what we call (A)GDNN-LD estimator that enjoys variable selection and model estimation simultaneously, by applying the (adaptive) group Lasso penalty to weight parameters in the DNN structure. For the computation, we propose a quadratic smoothing approximation method to facilitate optimizing the non-smooth objective fu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;DDoS&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#36776;&#21035;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.03116</link><description>&lt;p&gt;
&#25552;&#21319;DDoS&#25915;&#20987;&#26816;&#27979;&#65306;&#20351;&#29992;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#21512;&#25104;&#36807;&#37319;&#26679;&#30340;&#21327;&#21516;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing DDoS Attack Detection: A Synergistic Approach Using Deep Residual Neural Networks and Synthetic Oversampling. (arXiv:2401.03116v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;DDoS&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#36776;&#21035;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#25298;&#32477;&#26381;&#21153;&#65288;DDoS&#65289;&#25915;&#20987;&#23545;&#22312;&#32447;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26377;&#25928;&#21644;&#26089;&#26399;&#26816;&#27979;&#27492;&#31867;&#25915;&#20987;&#23545;&#20110;&#20445;&#25252;&#32593;&#32476;&#30340;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#65288;ResNets&#65289;&#21644;&#21512;&#25104;&#36807;&#37319;&#26679;&#25216;&#26415;&#30340;&#33021;&#21147;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;DDoS&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#12290;&#30001;&#20110;&#35768;&#22810;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#22266;&#26377;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#20256;&#32479;&#26041;&#27861;&#24120;&#24120;&#22312;&#23558;&#32454;&#24494;&#30340;DDoS&#27169;&#24335;&#35823;&#21028;&#20026;&#33391;&#24615;&#26102;&#22256;&#38590;&#37325;&#37325;&#12290;&#36890;&#36807;&#23558;&#21512;&#25104;&#23569;&#25968;&#31867;&#36807;&#37319;&#26679;&#25216;&#26415;&#65288;SMOTE&#65289;&#24212;&#29992;&#20110;CICIDS&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#33391;&#24615;&#21644;&#24694;&#24847;&#25968;&#25454;&#28857;&#30340;&#34920;&#31034;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#36776;&#21035;&#25915;&#20987;&#30340;&#22797;&#26434;&#27169;&#24335;&#12290;&#25105;&#20204;&#38024;&#23545;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#23450;&#21046;&#30340;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#36827;&#19968;&#27493;&#23436;&#21892;&#20102;&#26816;&#27979;&#36807;&#31243;&#12290;&#23545;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DDoS&#25915;&#20987;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Denial of Service (DDoS) attacks pose a significant threat to the stability and reliability of online systems. Effective and early detection of such attacks is pivotal for safeguarding the integrity of networks. In this work, we introduce an enhanced approach for DDoS attack detection by leveraging the capabilities of Deep Residual Neural Networks (ResNets) coupled with synthetic oversampling techniques. Because of the inherent class imbalance in many cyber-security datasets, conventional methods often struggle with false negatives, misclassifying subtle DDoS patterns as benign. By applying the Synthetic Minority Over-sampling Technique (SMOTE) to the CICIDS dataset, we balance the representation of benign and malicious data points, enabling the model to better discern intricate patterns indicative of an attack. Our deep residual network, tailored for this specific task, further refines the detection process. Experimental results on a real-world dataset demonstrate that our
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#22686;&#38271;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26410;&#32771;&#34385;&#36825;&#19968;&#25928;&#26524;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#24863;&#30693;&#30340;&#22686;&#38271;&#26102;&#26426;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2401.03104</link><description>&lt;p&gt;
&#20309;&#26102;&#36827;&#34892;&#31070;&#32463;&#22686;&#38271;&#65311;&#19968;&#31181;&#36866;&#24212;&#39118;&#38505;&#24863;&#30693;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#22686;&#38271;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
When To Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep Neural Networks. (arXiv:2401.03104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#22686;&#38271;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#26410;&#32771;&#34385;&#36825;&#19968;&#25928;&#26524;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#24863;&#30693;&#30340;&#22686;&#38271;&#26102;&#26426;&#31574;&#30053;&#65292;&#20197;&#24212;&#23545;&#27424;&#25311;&#21512;&#21644;&#36807;&#25311;&#21512;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#22686;&#38271;&#26159;&#23558;&#23567;&#22411;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#20026;&#22823;&#22411;&#32593;&#32476;&#30340;&#36807;&#31243;&#65292;&#24050;&#34987;&#29992;&#20110;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#12290;&#31070;&#32463;&#22686;&#38271;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#23450;&#26368;&#20339;&#22686;&#38271;&#26102;&#26426;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#31995;&#32479;&#22320;&#35843;&#26597;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#22686;&#38271;&#26412;&#36136;&#19978;&#20855;&#26377;&#19968;&#31181;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#20854;&#24378;&#24230;&#21463;&#22686;&#38271;&#26102;&#26426;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#36825;&#31181;&#27491;&#21017;&#21270;&#25928;&#26524;&#21487;&#20197;&#32531;&#35299;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#39118;&#38505;&#65292;&#20294;&#22312;&#27169;&#22411;&#27424;&#25311;&#21512;&#26102;&#21487;&#33021;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24403;&#21069;&#26041;&#27861;&#26410;&#32771;&#34385;&#31070;&#32463;&#22686;&#38271;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#22240;&#27492;&#26410;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#21463;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#38505;&#24863;&#30693;&#30340;&#22686;&#38271;&#26102;&#26426;&#31574;&#30053;&#65292;&#26681;&#25454;&#28508;&#22312;&#30340;&#27424;&#25311;&#21512;/&#36807;&#25311;&#21512;&#39118;&#38505;&#27700;&#24179;&#33258;&#21160;&#35843;&#25972;&#22686;&#38271;&#26102;&#26426;&#65292;&#20197;&#24212;&#23545;&#20004;&#31181;&#39118;&#38505;&#12290;&#36890;&#36807;&#20351;&#29992;CIFAR-10/10&#36827;&#34892;&#20840;&#38754;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#31181;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural growth is the process of growing a small neural network to a large network and has been utilized to accelerate the training of deep neural networks. One crucial aspect of neural growth is determining the optimal growth timing. However, few studies investigate this systematically. Our study reveals that neural growth inherently exhibits a regularization effect, whose intensity is influenced by the chosen policy for growth timing. While this regularization effect may mitigate the overfitting risk of the model, it may lead to a notable accuracy drop when the model underfits. Yet, current approaches have not addressed this issue due to their lack of consideration of the regularization effect from neural growth. Motivated by these findings, we propose an under/over fitting risk-aware growth timing policy, which automatically adjusts the growth timing informed by the level of potential under/overfitting risks to address both risks. Comprehensive experiments conducted using CIFAR-10/10
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;AdaBoost&#65288;FAB&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20844;&#24179;&#24863;&#30693;&#30340;&#22522;&#20998;&#31867;&#22120;&#37325;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;AdaBoost&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#32780;&#23545;&#39044;&#27979;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#29306;&#29298;&#12290;</title><link>http://arxiv.org/abs/2401.03097</link><description>&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24863;&#30693;&#37325;&#21152;&#26435;&#25216;&#26415;&#30340;&#33258;&#36866;&#24212;Boosting&#29992;&#20110;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification. (arXiv:2401.03097v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;AdaBoost&#65288;FAB&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20844;&#24179;&#24863;&#30693;&#30340;&#22522;&#20998;&#31867;&#22120;&#37325;&#21152;&#26435;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#20445;&#25345;AdaBoost&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#32780;&#23545;&#39044;&#27979;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;AdaBoost&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#21307;&#30103;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#31561;&#22810;&#20010;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#30340;&#21508;&#31181;&#20998;&#31867;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#32463;&#20856;&#31639;&#27861;&#22914;AdaBoost&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#31867;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#21644;&#27495;&#35270;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24179;AdaBoost&#65288;FAB&#65289;&#26041;&#27861;&#65292;&#23427;&#26159;AdaBoost&#30340;&#21487;&#35299;&#37322;&#24615;&#25913;&#36827;&#21464;&#20307;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#20851;&#27880;&#19977;&#31181;&#19981;&#21516;&#25351;&#26631;&#30340;&#20844;&#24179;&#24615;&#65288;&#21363;&#20934;&#30830;&#29575;&#12289;&#20551;&#38451;&#24615;&#29575;&#21644;&#20551;&#38452;&#24615;&#29575;&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#30340;&#22522;&#20998;&#31867;&#22120;&#37325;&#21152;&#26435;&#25216;&#26415;&#65292;&#25552;&#20986;&#30340;FAB&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;AdaBoost&#20248;&#21183;&#30340;&#21516;&#26102;&#23454;&#29616;&#20844;&#24179;&#20998;&#31867;&#65292;&#32780;&#23545;&#39044;&#27979;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#29306;&#29298;&#12290;&#21478;&#22806;&#65292;&#24341;&#20837;&#20102;FAB&#20013;&#30340;&#19968;&#20010;&#36229;&#21442;&#25968;&#65292;&#29992;&#20110;&#23637;&#31034;&#23545;&#20844;&#24179;&#24615;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods based on AdaBoost have been widely applied to various classification problems across many mission-critical applications including healthcare, law and finance. However, there is a growing concern about the unfairness and discrimination of data-driven classification models, which is inevitable for classical algorithms including AdaBoost. In order to achieve fair classification, a novel fair AdaBoost (FAB) approach is proposed that is an interpretable fairness-improving variant of AdaBoost. We mainly investigate binary classification problems and focus on the fairness of three different indicators (i.e., accuracy, false positive rate and false negative rate). By utilizing a fairness-aware reweighting technique for base classifiers, the proposed FAB approach can achieve fair classification while maintaining the advantage of AdaBoost with negligible sacrifice of predictive performance. In addition, a hyperparameter is introduced in FAB to show preferences for the fa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31614;&#21517;&#39564;&#35777;&#30340;&#19968;&#33268;&#38408;&#20540;&#20934;&#21017;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#20943;&#23569;&#20266;&#36896;&#31614;&#21517;&#30340;&#35823;&#35782;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.03085</link><description>&lt;p&gt;
&#31163;&#32447;&#31614;&#21517;&#39564;&#35777;&#20013;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#34920;&#31034;&#30340;&#19968;&#33268;&#38408;&#20540;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Consensus-Threshold Criterion for Offline Signature Verification using Convolutional Neural Network Learned Representations. (arXiv:2401.03085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#31614;&#21517;&#39564;&#35777;&#30340;&#19968;&#33268;&#38408;&#20540;&#20934;&#21017;&#65292;&#36890;&#36807;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#21487;&#20197;&#20943;&#23569;&#20266;&#36896;&#31614;&#21517;&#30340;&#35823;&#35782;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#31614;&#21517;&#21363;&#20351;&#22312;&#30701;&#26102;&#38388;&#38388;&#38548;&#20869;&#20063;&#26159;&#33258;&#28982;&#19981;&#31283;&#23450;&#30340;&#65292;&#32780;&#19987;&#23478;&#32423;&#20266;&#36896;&#32773;&#24635;&#26159;&#35797;&#22270;&#23436;&#32654;&#27169;&#20223;&#30495;&#23454;&#31614;&#21517;&#12290;&#36825;&#23601;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#20351;&#24471;&#30495;&#23454;&#31614;&#21517;&#32773;&#38754;&#20020;&#34987;&#25298;&#32477;&#35775;&#38382;&#30340;&#39118;&#38505;&#65292;&#32780;&#20266;&#36896;&#31614;&#21517;&#32773;&#21017;&#34987;&#25480;&#26435;&#35775;&#38382;&#12290;&#36825;&#24847;&#21619;&#30528;&#39640;&#35823;&#35782;&#29575;&#65288;FAR&#65289;&#65292;&#21363;&#34987;&#38169;&#35823;&#20998;&#31867;&#20026;&#23646;&#20110;&#30495;&#23454;&#31867;&#21035;&#30340;&#20266;&#36896;&#31614;&#21517;&#30340;&#30334;&#20998;&#27604;&#12290;&#29616;&#26377;&#24037;&#20316;&#21482;&#26159;&#27973;&#23581;&#36740;&#27490;&#22320;&#36827;&#34892;&#20102;&#31614;&#21517;&#39564;&#35777;&#65292;&#22240;&#20026;&#35823;&#20998;&#31867;&#38169;&#35823;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31163;&#32447;&#20889;&#23383;&#32773;&#20381;&#36182;&#30340;&#31614;&#21517;&#39564;&#35777;&#30340;&#19968;&#33268;&#38408;&#20540;&#22522;&#20110;&#36317;&#31163;&#30340;&#20998;&#31867;&#22120;&#20934;&#21017;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;SigNet&#21644;SigNet-F&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25152;&#25552;&#20986;&#30340;&#20998;&#31867;&#22120;&#26368;&#23567;&#21270;&#20102;FAR&#12290;&#36890;&#36807;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#65306;GPDS-300&#12289;MCYT&#12289;CEDAR&#21644;&#24052;&#35199;PUC-PR&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#26469;&#35777;&#26126;&#36825;&#19968;&#28857;&#12290;&#22312;GPDS-300&#19978;&#65292;&#19968;&#33268;&#38408;&#20540;&#20998;&#31867;&#22120;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
A genuine signer's signature is naturally unstable even at short time-intervals whereas, expert forgers always try to perfectly mimic a genuine signer's signature. This presents a challenge which puts a genuine signer at risk of being denied access, while a forge signer is granted access. The implication is a high false acceptance rate (FAR) which is the percentage of forge signature classified as belonging to a genuine class. Existing work have only scratched the surface of signature verification because the misclassification error remains high. In this paper, a consensus-threshold distance-based classifier criterion is proposed for offline writer-dependent signature verification. Using features extracted from SigNet and SigNet-F deep convolutional neural network models, the proposed classifier minimizes FAR. This is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR and Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier improves the state-of-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22270;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#28151;&#21512;&#30697;&#38453;&#65292;&#20197;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#33021;&#25928;&#12290;&#22312;&#29305;&#27530;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#26377;&#20445;&#35777;&#24615;&#33021;&#21644;&#36138;&#24515;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03083</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31232;&#30095;&#21270;&#30340;&#33021;&#25928;&#20248;&#21270;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Energy-efficient Decentralized Learning via Graph Sparsification. (arXiv:2401.03083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22270;&#31232;&#30095;&#21270;&#30340;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#28151;&#21512;&#30697;&#38453;&#65292;&#20197;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#33021;&#25928;&#12290;&#22312;&#29305;&#27530;&#21644;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#26377;&#20445;&#35777;&#24615;&#33021;&#21644;&#36138;&#24515;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#20223;&#30495;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#28151;&#21512;&#30697;&#38453;&#26469;&#25552;&#39640;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#30340;&#33021;&#25928;&#65292;&#28151;&#21512;&#30697;&#38453;&#25511;&#21046;&#20102;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#38656;&#27714;&#12290;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20005;&#26684;&#20998;&#26512;&#65292;&#23558;&#38382;&#39064;&#34920;&#31034;&#20026;&#21452;&#23618;&#20248;&#21270;&#65292;&#20854;&#20013;&#24213;&#23618;&#36890;&#36807;&#22270;&#31232;&#30095;&#21270;&#35299;&#20915;&#12290;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#22522;&#30784;&#25299;&#25169;&#32467;&#26500;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#38024;&#23545;&#19968;&#33324;&#24773;&#20917;&#25552;&#20986;&#20102;&#19968;&#31181;&#36138;&#24515;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22522;&#20110;&#30495;&#23454;&#25299;&#25169;&#32467;&#26500;&#21644;&#25968;&#25454;&#38598;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#23558;&#26368;&#32321;&#24537;&#33410;&#28857;&#30340;&#33021;&#32791;&#38477;&#20302;54%-76%&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims at improving the energy efficiency of decentralized learning by optimizing the mixing matrix, which controls the communication demands during the learning process. Through rigorous analysis based on a state-of-the-art decentralized learning algorithm, the problem is formulated as a bi-level optimization, with the lower level solved by graph sparsification. A solution with guaranteed performance is proposed for the special case of fully-connected base topology and a greedy heuristic is proposed for the general case. Simulations based on real topology and dataset show that the proposed solution can lower the energy consumption at the busiest node by 54%-76% while maintaining the quality of the trained model.
&lt;/p&gt;</description></item><item><title>StreamVC&#26159;&#19968;&#31181;&#23454;&#26102;&#20302;&#24310;&#36831;&#35821;&#38899;&#36716;&#25442;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#31227;&#21160;&#24179;&#21488;&#19978;&#36827;&#34892;&#23454;&#26102;&#36890;&#20449;&#65292;&#24182;&#23454;&#29616;&#22768;&#38899;&#21311;&#21517;&#21270;&#12290;&#23427;&#20445;&#30041;&#20102;&#28304;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#38901;&#24459;&#65292;&#21516;&#26102;&#21305;&#37197;&#20102;&#30446;&#26631;&#35821;&#38899;&#30340;&#38899;&#36136;&#65292;&#24182;&#19988;&#36890;&#36807;&#36719;&#35821;&#38899;&#21333;&#20803;&#30340;&#23398;&#20064;&#21644;&#25552;&#20379;&#30333;&#21270;&#30340;&#22522;&#39057;&#20449;&#24687;&#26469;&#25913;&#21892;&#38899;&#39640;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.03078</link><description>&lt;p&gt;
StreamVC&#65306;&#23454;&#26102;&#20302;&#24310;&#36831;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
StreamVC: Real-Time Low-Latency Voice Conversion. (arXiv:2401.03078v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03078
&lt;/p&gt;
&lt;p&gt;
StreamVC&#26159;&#19968;&#31181;&#23454;&#26102;&#20302;&#24310;&#36831;&#35821;&#38899;&#36716;&#25442;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#31227;&#21160;&#24179;&#21488;&#19978;&#36827;&#34892;&#23454;&#26102;&#36890;&#20449;&#65292;&#24182;&#23454;&#29616;&#22768;&#38899;&#21311;&#21517;&#21270;&#12290;&#23427;&#20445;&#30041;&#20102;&#28304;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#38901;&#24459;&#65292;&#21516;&#26102;&#21305;&#37197;&#20102;&#30446;&#26631;&#35821;&#38899;&#30340;&#38899;&#36136;&#65292;&#24182;&#19988;&#36890;&#36807;&#36719;&#35821;&#38899;&#21333;&#20803;&#30340;&#23398;&#20064;&#21644;&#25552;&#20379;&#30333;&#21270;&#30340;&#22522;&#39057;&#20449;&#24687;&#26469;&#25913;&#21892;&#38899;&#39640;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;StreamVC&#65292;&#36825;&#26159;&#19968;&#20010;&#27969;&#24335;&#35821;&#38899;&#36716;&#25442;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#30041;&#20219;&#20309;&#28304;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#38901;&#24459;&#65292;&#21516;&#26102;&#21305;&#37197;&#20219;&#20309;&#30446;&#26631;&#35821;&#38899;&#30340;&#38899;&#36136;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;StreamVC&#29978;&#33267;&#21487;&#20197;&#22312;&#31227;&#21160;&#24179;&#21488;&#19978;&#20197;&#20302;&#24310;&#36831;&#20174;&#36755;&#20837;&#20449;&#21495;&#20013;&#29983;&#25104;&#32467;&#26524;&#27874;&#24418;&#65292;&#36825;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#26102;&#36890;&#20449;&#22330;&#26223;&#65292;&#22914;&#30005;&#35805;&#21644;&#35270;&#39057;&#20250;&#35758;&#65292;&#24182;&#35299;&#20915;&#36825;&#20123;&#22330;&#26223;&#19979;&#30340;&#22768;&#38899;&#21311;&#21517;&#21270;&#31561;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#35774;&#35745;&#20805;&#20998;&#21033;&#29992;&#20102;SoundStream&#31070;&#32463;&#38899;&#39057;&#32534;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#36731;&#37327;&#32423;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23398;&#20064;&#36719;&#35821;&#38899;&#21333;&#20803;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#21450;&#36890;&#36807;&#25552;&#20379;&#30333;&#21270;&#30340;&#22522;&#39057;&#20449;&#24687;&#26469;&#25913;&#21892;&#38899;&#39640;&#31283;&#23450;&#24615;&#32780;&#19981;&#27844;&#28431;&#28304;&#38899;&#36136;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present StreamVC, a streaming voice conversion solution that preserves the content and prosody of any source speech while matching the voice timbre from any target speech. Unlike previous approaches, StreamVC produces the resulting waveform at low latency from the input signal even on a mobile platform, making it applicable to real-time communication scenarios like calls and video conferencing, and addressing use cases such as voice anonymization in these scenarios. Our design leverages the architecture and training strategy of the SoundStream neural audio codec for lightweight high-quality speech synthesis. We demonstrate the feasibility of learning soft speech units causally, as well as the effectiveness of supplying whitened fundamental frequency information to improve pitch stability without leaking the source timbre information.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#36830;&#32493;&#22270;&#23398;&#20064;&#26694;&#26550;TA$\mathbb{CO}$&#65292;&#36890;&#36807;&#22270;&#20943;&#31895;&#21644;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#22312;&#22788;&#29702;&#27969;&#24335;&#22270;&#26102;&#30340;&#20302;&#25928;&#29575;&#21644;&#26080;&#27861;&#25429;&#25417;&#20219;&#21153;&#20851;&#32852;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03077</link><description>&lt;p&gt;
&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#36830;&#32493;&#22270;&#23398;&#20064;&#30340;&#22270;&#20943;&#31895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Topology-aware Graph Coarsening Framework for Continual Graph Learning. (arXiv:2401.03077v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03077
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#36830;&#32493;&#22270;&#23398;&#20064;&#26694;&#26550;TA$\mathbb{CO}$&#65292;&#36890;&#36807;&#22270;&#20943;&#31895;&#21644;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#22312;&#22788;&#29702;&#27969;&#24335;&#22270;&#26102;&#30340;&#20302;&#25928;&#29575;&#21644;&#26080;&#27861;&#25429;&#25417;&#20219;&#21153;&#20851;&#32852;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#36830;&#32493;&#23398;&#20064;&#35299;&#20915;&#20102;&#22312;&#25968;&#25454;&#20197;&#27969;&#24335;&#26041;&#24335;&#21040;&#36798;&#19988;&#27169;&#22411;&#22312;&#26356;&#26032;&#26102;&#23481;&#26131;&#24536;&#35760;&#20197;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#35757;&#32451;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#36830;&#32493;&#23398;&#20064;&#31574;&#30053;&#65292;&#22914;&#32463;&#39564;&#22238;&#25918;&#65292;&#21487;&#20197;&#36866;&#24212;&#27969;&#24335;&#22270;&#65292;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38754;&#20020;&#20445;&#30041;&#22270;&#25299;&#25169;&#30340;&#20302;&#25928;&#29575;&#21644;&#25429;&#25417;&#26087;&#20219;&#21153;&#19982;&#26032;&#20219;&#21153;&#20043;&#38388;&#20851;&#32852;&#24615;&#30340;&#26080;&#33021;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TA$\mathbb{CO}$&#65292;&#19968;&#20010;&#38754;&#21521;&#25299;&#25169;&#24863;&#30693;&#30340;&#22270;&#20943;&#31895;&#21644;&#36830;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20197;&#21069;&#20219;&#21153;&#30340;&#20449;&#24687;&#23384;&#20648;&#20026;&#19968;&#20010;&#20943;&#23569;&#30340;&#22270;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#36825;&#20010;&#20943;&#23569;&#30340;&#22270;&#36890;&#36807;&#19982;&#26032;&#22270;&#21512;&#24182;&#24182;&#23545;&#40784;&#20849;&#20139;&#33410;&#28857;&#26469;&#25193;&#23637;&#65292;&#28982;&#21518;&#36827;&#34892;&#8220;&#32553;&#23567;&#8221;&#36807;&#31243;&#20197;&#20445;&#25345;&#31283;&#23450;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#34920;&#31034;&#25509;&#36817;&#24230;&#30340;&#22270;&#20943;&#31895;&#31639;&#27861;&#26469;&#39640;&#25928;&#22320;&#20943;&#23569;&#22270;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning on graphs tackles the problem of training a graph neural network (GNN) where graph data arrive in a streaming fashion and the model tends to forget knowledge from previous tasks when updating with new data. Traditional continual learning strategies such as Experience Replay can be adapted to streaming graphs, however, these methods often face challenges such as inefficiency in preserving graph topology and incapability of capturing the correlation between old and new tasks. To address these challenges, we propose TA$\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual learning framework that stores information from previous tasks as a reduced graph. At each time period, this reduced graph expands by combining with a new graph and aligning shared nodes, and then it undergoes a "zoom out" process by reduction to maintain a stable size. We design a graph coarsening algorithm based on node representation proximities to efficiently reduce a graph and pres
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03069</link><description>&lt;p&gt;
&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#36719;&#20214;&#31995;&#32479;&#19968;&#26679;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20063;&#23384;&#22312;Bug&#65292;&#36825;&#21487;&#33021;&#23545;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;Bug&#30340;&#35299;&#20915;&#12290;&#29616;&#26377;&#25991;&#29486;&#25351;&#20986;&#65292;&#20165;&#26377;3%&#30340;&#28145;&#24230;&#23398;&#20064;Bug&#26159;&#21487;&#22797;&#29616;&#30340;&#65292;&#36825;&#20984;&#26174;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#32771;&#23519;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#35782;&#21035;&#21487;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Stack Overflow&#21644;Defects4ML&#30340;3&#20010;&#26694;&#26550;&#21644;22&#20010;&#26550;&#26500;&#30340;668&#20010;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#36873;&#25321;&#20102;102&#20010;Bug&#65292;&#24182;&#23581;&#35797;&#30830;&#23450;&#23427;&#20204;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#22312;&#22797;&#29616;&#36825;&#20123;Bug&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Deep learning has achieved remarkable progress in various domains. However, like traditional software systems, deep learning systems contain bugs, which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which hinders resolving them. Existing literature suggests that only 3% of deep learning bugs are reproducible, underscoring the need for further research.  Objective: This paper examines the reproducibility of deep learning bugs. We identify edit actions and useful information that could improve deep learning bug reproducibility.  Method: First, we construct a dataset of 668 deep learning bugs from Stack Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we select 102 bugs using stratified sampling and try to determine their reproducibility. While reproducing these bugs, we identify edit actions and us
&lt;/p&gt;</description></item><item><title>CRUXEval&#26159;&#19968;&#20010;&#21253;&#21547;800&#20010;Python&#20989;&#25968;&#30340;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35780;&#20272;&#20108;&#21313;&#20010;&#20195;&#30721;&#27169;&#22411;&#65292;&#21457;&#29616;&#35768;&#22810;&#22312;HumanEval&#19978;&#24471;&#20998;&#39640;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#27809;&#26377;&#30456;&#21516;&#30340;&#25913;&#36827;&#12290;&#20351;&#29992;CoT&#30340;GPT-4&#23637;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#19982;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2401.03065</link><description>&lt;p&gt;
CRUXEval: &#19968;&#20010;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03065
&lt;/p&gt;
&lt;p&gt;
CRUXEval&#26159;&#19968;&#20010;&#21253;&#21547;800&#20010;Python&#20989;&#25968;&#30340;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#36890;&#36807;&#35780;&#20272;&#20108;&#21313;&#20010;&#20195;&#30721;&#27169;&#22411;&#65292;&#21457;&#29616;&#35768;&#22810;&#22312;HumanEval&#19978;&#24471;&#20998;&#39640;&#30340;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#27979;&#35797;&#19978;&#27809;&#26377;&#30456;&#21516;&#30340;&#25913;&#36827;&#12290;&#20351;&#29992;CoT&#30340;GPT-4&#23637;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#20294;&#20173;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#19982;&#24320;&#28304;&#27169;&#22411;&#30456;&#27604;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;CRUXEval&#65288;&#20195;&#30721;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#25191;&#34892;&#35780;&#20272;&#65289;&#65292;&#19968;&#20010;&#21253;&#21547;800&#20010;Python&#20989;&#25968;&#65288;3-13&#34892;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#27599;&#20010;&#20989;&#25968;&#37117;&#26377;&#19968;&#20010;&#36755;&#20837;&#36755;&#20986;&#23545;&#65292;&#21487;&#20197;&#36827;&#34892;&#20004;&#20010;&#33258;&#28982;&#20219;&#21153;&#65306;&#36755;&#20837;&#39044;&#27979;&#21644;&#36755;&#20986;&#39044;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#25104;&#25191;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#26469;&#21019;&#24314;&#26410;&#26469;&#21464;&#31181;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#20108;&#21313;&#20010;&#20195;&#30721;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#35768;&#22810;&#26368;&#36817;&#22312;HumanEval&#19978;&#24471;&#20998;&#39640;&#30340;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#27809;&#26377;&#21462;&#24471;&#30456;&#21516;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;CoT&#21644;&#24494;&#35843;&#26041;&#26696;&#21487;&#20197;&#25913;&#21892;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#38382;&#39064;&#12290;&#26368;&#20339;&#35774;&#32622;&#65292;&#20351;&#29992;CoT&#30340;GPT-4&#65292;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#39044;&#27979;&#19978;&#30340;pass@1&#20998;&#21035;&#36798;&#21040;&#20102;75%&#21644;81%&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;Code Llama 34B&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#39044;&#27979;&#19978;&#30340;pass@1&#20998;&#21035;&#20026;50%&#21644;46%&#65292;&#31361;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#38381;&#28304;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;QoS-aware UE&#25509;&#20837;&#25511;&#21046;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20851;&#32852;URRLC UE&#19982;&#23567;&#21306;&#20043;&#21069;&#65292;&#20934;&#30830;&#20272;&#35745;QoS&#24182;&#36991;&#20813;&#23567;&#21306;&#36807;&#36733;&#12290;</title><link>http://arxiv.org/abs/2401.03059</link><description>&lt;p&gt;
&#20026;URLLC&#27969;&#37327;&#20248;&#21270;&#21487;&#38752;&#24615;&#30340;&#29992;&#25143;&#25509;&#20837;&#25511;&#21046;&#65306;&#19968;&#31181;&#31070;&#32463;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach. (arXiv:2401.03059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;QoS-aware UE&#25509;&#20837;&#25511;&#21046;&#31574;&#30053;&#65292;&#26088;&#22312;&#22312;&#20851;&#32852;URRLC UE&#19982;&#23567;&#21306;&#20043;&#21069;&#65292;&#20934;&#30830;&#20272;&#35745;QoS&#24182;&#36991;&#20813;&#23567;&#21306;&#36807;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#65288;URLLC&#65289;&#26159;&#19979;&#19968;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#24191;&#27867;&#33539;&#22260;&#30340;&#26032;&#20852;&#26381;&#21153;&#30340;&#22522;&#30707;&#12290;URLLC&#22522;&#26412;&#19978;&#20381;&#36182;&#20110;&#32593;&#32476;&#33021;&#22815;&#20027;&#21160;&#30830;&#23450;&#26159;&#21542;&#26377;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#25903;&#25345;URLLC&#27969;&#37327;&#65292;&#24182;&#22240;&#27492;&#38450;&#27490;&#25152;&#35859;&#30340;&#23567;&#21306;&#36807;&#36733;&#12290;&#28982;&#32780;&#65292;&#20026;URLLC&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#23454;&#29616;&#20934;&#30830;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#39044;&#27979;&#24182;&#38450;&#27490;&#23567;&#21306;&#36229;&#36733;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#26159;&#30001;&#20110;QoS&#25351;&#26631;&#65288;&#24310;&#36831;&#21644;&#21487;&#38752;&#24615;&#65289;&#20381;&#36182;&#20110;&#27969;&#37327;&#21644;&#20449;&#36947;&#32479;&#35745;&#25968;&#25454;&#12289;&#29992;&#25143;&#30340;&#31227;&#21160;&#24615;&#21644;UE&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;QoS&#24863;&#30693;UE&#25509;&#20837;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23558;&#20854;&#19982;&#23567;&#21306;&#20851;&#32852;&#20043;&#21069;&#65292;&#20027;&#21160;&#20272;&#35745;URLLC UE&#30340;QoS&#65292;&#24182;&#30456;&#24212;&#22320;&#20165;&#25509;&#32435;&#19981;&#20250;&#23548;&#33268;&#23567;&#21306;&#36807;&#36733;&#30340;UE&#23376;&#38598;&#12290;&#20026;&#27492;&#65292;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#25214;&#21040;&#19968;&#31181;&#26377;&#25928;&#30340;UE&#25509;&#20837;&#25511;&#21046;&#31574;&#30053;&#65292;
&lt;/p&gt;
&lt;p&gt;
Ultra-reliable low-latency communication (URLLC) is the cornerstone for a broad range of emerging services in next-generation wireless networks. URLLC fundamentally relies on the network's ability to proactively determine whether sufficient resources are available to support the URLLC traffic, and thus, prevent so-called cell overloads. Nonetheless, achieving accurate quality-of-service (QoS) predictions for URLLC user equipment (UEs) and preventing cell overloads are very challenging tasks. This is due to dependency of the QoS metrics (latency and reliability) on traffic and channel statistics, users' mobility, and interdependent performance across UEs. In this paper, a new QoS-aware UE admission control approach is developed to proactively estimate QoS for URLLC UEs, prior to associating them with a cell, and accordingly, admit only a subset of UEs that do not lead to a cell overload. To this end, an optimization problem is formulated to find an efficient UE admission control policy,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23376;&#31354;&#38388;&#31435;&#26041;&#27491;&#21017;&#21270;&#29275;&#39039;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#26102;&#23454;&#29616;&#26080;&#32500;&#24230;&#30456;&#20851;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#20108;&#38454;&#26356;&#26032;&#65292;&#20811;&#26381;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03058</link><description>&lt;p&gt;
Krylov&#31435;&#26041;&#27491;&#21017;&#21270;&#29275;&#39039;&#27861;&#65306;&#20855;&#26377;&#26080;&#32500;&#25910;&#25947;&#36895;&#24230;&#30340;&#23376;&#31354;&#38388;&#20108;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate. (arXiv:2401.03058v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23376;&#31354;&#38388;&#31435;&#26041;&#27491;&#21017;&#21270;&#29275;&#39039;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#26102;&#23454;&#29616;&#26080;&#32500;&#24230;&#30456;&#20851;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#36890;&#36807;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#19978;&#36827;&#34892;&#20108;&#38454;&#26356;&#26032;&#65292;&#20811;&#26381;&#20102;&#39640;&#32500;&#38382;&#39064;&#20013;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22914;&#31435;&#26041;&#27491;&#21017;&#21270;&#29275;&#39039;&#27861;&#65292;&#20197;&#20854;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#32780;&#38395;&#21517;&#65307;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#65292;&#23427;&#20204;&#21464;&#24471;&#19981;&#23454;&#29992;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#25191;&#34892;&#20108;&#38454;&#26356;&#26032;&#65292;&#20174;&#32780;&#20135;&#29983;&#23376;&#31354;&#38388;&#20108;&#38454;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#23376;&#31354;&#38388;&#20108;&#38454;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#23376;&#31354;&#38388;&#65292;&#22240;&#27492;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#65292;&#36825;&#21462;&#20915;&#20110;&#38382;&#39064;&#30340;&#32500;&#24230;d&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#31354;&#38388;&#31435;&#26041;&#27491;&#21017;&#21270;&#29275;&#39039;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#36798;&#21040;&#20102;&#19968;&#20010;&#32500;&#24230;&#26080;&#20851;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#20026;${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$&#12290;&#36825;&#37324;&#65292;m&#34920;&#31034;&#23376;&#31354;&#38388;&#32500;&#24230;&#65292;&#21487;&#20197;&#26174;&#33879;&#23567;&#20110;d&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21019;&#26032;&#19981;&#26159;&#37319;&#29992;&#38543;&#26426;&#23376;&#31354;&#38388;&#65292;&#32780;&#26159;&#36827;&#34892;&#31435;&#26041;&#27491;&#21017;&#21270;...
&lt;/p&gt;
&lt;p&gt;
Second-order optimization methods, such as cubic regularized Newton methods, are known for their rapid convergence rates; nevertheless, they become impractical in high-dimensional problems due to their substantial memory requirements and computational costs. One promising approach is to execute second-order updates within a lower-dimensional subspace, giving rise to subspace second-order methods. However, the majority of existing subspace second-order methods randomly select subspaces, consequently resulting in slower convergence rates depending on the problem's dimension $d$. In this paper, we introduce a novel subspace cubic regularized Newton method that achieves a dimension-independent global convergence rate of ${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$ for solving convex optimization problems. Here, $m$ represents the subspace dimension, which can be significantly smaller than $d$. Instead of adopting a random subspace, our primary innovation involves performing the cubic regul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21322;&#26080;&#30417;&#30563;&#26657;&#20934;&#31639;&#27861;&#65292;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#35266;&#28857;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03051</link><description>&lt;p&gt;
&#21322;&#26080;&#30417;&#30563;&#26657;&#20934;&#36890;&#36807;&#20808;&#39564;&#36866;&#24212;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm. (arXiv:2401.03051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21322;&#26080;&#30417;&#30563;&#26657;&#20934;&#31639;&#27861;&#65292;&#20351;&#29992;&#21160;&#21147;&#31995;&#32479;&#30340;&#35266;&#28857;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#12290;Semi Unsupervised Calibration through Prior Adaptation (SUCPA)&#26159;&#19968;&#31181;&#26657;&#20934;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#23450;&#20041;&#20026;&#19968;&#20010;&#19968;&#38454;&#24046;&#20998;&#26041;&#31243;&#32452;&#12290;&#35813;&#26041;&#31243;&#32452;&#23548;&#20986;&#30340;&#26144;&#23556;&#20855;&#26377;&#38750;&#21452;&#26354;&#24615;&#29305;&#24449;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#32452;&#38750;&#23396;&#31435;&#19981;&#30028;&#23450;&#30340;&#22266;&#23450;&#28857;&#12290;&#26412;&#25991;&#20174;&#21160;&#21147;&#31995;&#32479;&#30340;&#35270;&#35282;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#22810;&#20010;&#25910;&#25947;&#24615;&#36136;&#12290;&#23545;&#20110;&#20108;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#21487;&#20197;&#35777;&#26126;&#35813;&#31639;&#27861;&#24635;&#26159;&#25910;&#25947;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#35813;&#26144;&#23556;&#26159;&#20840;&#23616;&#28176;&#36817;&#31283;&#23450;&#30340;&#65292;&#36712;&#36947;&#25910;&#25947;&#21040;&#19968;&#26465;&#22266;&#23450;&#28857;&#30452;&#32447;&#19978;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#23454;&#38469;&#24212;&#29992;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#25903;&#25345;&#25152;&#25552;&#20986;&#30340;&#32467;&#26524;&#12290;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;&#32593;&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibration is an essential key in machine leaning. Semi Unsupervised Calibration through Prior Adaptation (SUCPA) is a calibration algorithm used in (but not limited to) large-scale language models defined by a {system of first-order difference equation. The map derived by this system} has the peculiarity of being non-hyperbolic {with a non-bounded set of non-isolated fixed points}. In this work, we prove several convergence properties of this algorithm from the perspective of dynamical systems. For a binary classification problem, it can be shown that the algorithm always converges, {more precisely, the map is globally asymptotically stable, and the orbits converge} to a single line of fixed points. Finally, we perform numerical experiments on real-world application to support the presented results. Experiment codes are available online.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;AccidentGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#33258;&#21160;&#37325;&#24314;&#20107;&#25925;&#36807;&#31243;&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;AccidentGPT&#36824;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25552;&#31034;&#19982;&#21453;&#39304;&#12289;&#28151;&#21512;&#35757;&#32451;&#27169;&#24335;&#21644;&#36793;&#32536;-&#20113;&#20998;&#21106;&#37197;&#32622;&#20197;&#22686;&#24378;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.03040</link><description>&lt;p&gt;
AccidentGPT:&#29992;&#20110;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;AccidentGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#12290;&#23427;&#21033;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#33258;&#21160;&#37325;&#24314;&#20107;&#25925;&#36807;&#31243;&#35270;&#39057;&#65292;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;AccidentGPT&#36824;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25552;&#31034;&#19982;&#21453;&#39304;&#12289;&#28151;&#21512;&#35757;&#32451;&#27169;&#24335;&#21644;&#36793;&#32536;-&#20113;&#20998;&#21106;&#37197;&#32622;&#20197;&#22686;&#24378;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20844;&#20849;&#23433;&#20840;&#21644;&#21046;&#23450;&#36947;&#36335;&#35268;&#31456;&#21046;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#34429;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#24120;&#24120;&#21463;&#38480;&#20110;&#25163;&#21160;&#20998;&#26512;&#36807;&#31243;&#12289;&#20027;&#35266;&#20915;&#31574;&#12289;&#21333;&#27169;&#24577;&#36755;&#20986;&#20197;&#21450;&#19982;&#25935;&#24863;&#25968;&#25454;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AccidentGPT&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#23427;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#29992;&#20110;&#33258;&#21160;&#37325;&#24314;&#20107;&#25925;&#36807;&#31243;&#35270;&#39057;&#24182;&#25552;&#20379;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#12290;AccidentGPT&#30340;&#35774;&#35745;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#25552;&#31034;&#19982;&#21453;&#39304;&#65292;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;&#36866;&#24212;&#24615;&#65292;&#28151;&#21512;&#35757;&#32451;&#27169;&#24335;&#20197;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20197;&#21450;&#36793;&#32536;-&#20113;&#20998;&#21106;&#37197;&#32622;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#35813;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#30740;&#31350;&#26426;&#20250;&#12290;&#26412;&#25991;&#23558;&#22635;&#34917;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic accident analysis is pivotal for enhancing public safety and developing road regulations. Traditional approaches, although widely used, are often constrained by manual analysis processes, subjective decisions, uni-modal outputs, as well as privacy issues related to sensitive data. This paper introduces the idea of AccidentGPT, a foundation model of traffic accident analysis, which incorporates multi-modal input data to automatically reconstruct the accident process video with dynamics details, and furthermore provide multi-task analysis with multi-modal outputs. The design of the AccidentGPT is empowered with a multi-modality prompt with feedback for task-oriented adaptability, a hybrid training schema to leverage labelled and unlabelled data, and a edge-cloud split configuration for data privacy. To fully realize the functionalities of this model, we proposes several research opportunities. This paper serves as the stepping stone to fill the gaps in traditional approaches of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.03006</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#20852;&#36215;
&lt;/p&gt;
&lt;p&gt;
The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#21644;&#35814;&#32454;&#35828;&#26126;&#65292;&#21516;&#26102;&#20063;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#20854;&#36129;&#29486;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#36825;&#26159;&#19968;&#20221;&#23545;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#21253;&#25324;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20840;&#38754;&#32972;&#26223;&#20449;&#24687;&#65292;&#35814;&#32454;&#20171;&#32461;&#20854;&#26465;&#20214;&#26041;&#27861;&#65292;&#24182;&#23457;&#26597;&#20102;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20998;&#26512;&#28085;&#30422;&#20102;11&#20010;&#20855;&#20307;&#30340;&#26102;&#38388;&#24207;&#21015;&#23454;&#29616;&#65292;&#23427;&#20204;&#30340;&#30452;&#35273;&#21644;&#29702;&#35770;&#22522;&#30784;&#65292;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#24444;&#27492;&#20043;&#38388;&#30340;&#27604;&#36739;&#12290;&#35813;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#24212;&#29992;&#30340;&#24443;&#24213;&#25506;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25353;&#26102;&#38388;&#39034;&#24207;&#25490;&#24207;&#30340;&#27169;&#22411;&#27010;&#36848;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#36827;&#34892;&#20102;&#28145;&#20837;&#35752;&#35770;&#65292;&#24182;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#36825;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20221;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#23545;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#28165;&#26224;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey delves into the application of diffusion models in time-series forecasting. Diffusion models are demonstrating state-of-the-art results in various fields of generative AI. The paper includes comprehensive background information on diffusion models, detailing their conditioning methods and reviewing their use in time-series forecasting. The analysis covers 11 specific time-series implementations, the intuition and theory behind them, the effectiveness on different datasets, and a comparison among each other. Key contributions of this work are the thorough exploration of diffusion models' applications in time-series forecasting and a chronologically ordered overview of these models. Additionally, the paper offers an insightful discussion on the current state-of-the-art in this domain and outlines potential future research directions. This serves as a valuable resource for researchers in AI and time-series analysis, offering a clear view of the latest advancements and future p
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>UnetTSF&#26159;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;U-Net&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;FPN&#25216;&#26415;&#25552;&#21462;&#29305;&#24449;&#21644;&#35774;&#35745;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#34701;&#21512;&#32467;&#26500;&#65292;&#30456;&#27604;&#20110;DLiner&#21644;PatchTST&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#27979;&#35797;&#39033;&#30446;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03001</link><description>&lt;p&gt;
UnetTSF:&#19968;&#20010;&#24615;&#33021;&#26356;&#22909;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UnetTSF: A Better Performance Linear Complexity Time Series Prediction Model. (arXiv:2401.03001v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03001
&lt;/p&gt;
&lt;p&gt;
UnetTSF&#26159;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;U-Net&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;FPN&#25216;&#26415;&#25552;&#21462;&#29305;&#24449;&#21644;&#35774;&#35745;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#34701;&#21512;&#32467;&#26500;&#65292;&#30456;&#27604;&#20110;DLiner&#21644;PatchTST&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#27979;&#35797;&#39033;&#30446;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24050;&#32463;&#25104;&#20026;Dlinear&#22522;&#20934;&#27169;&#22411;&#20043;&#22806;&#30340;&#19968;&#31181;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#30340;U-Net&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;(UnetTSF)&#65292;&#37319;&#29992;&#20102;U-Net&#26550;&#26500;&#12290;&#25105;&#20204;&#26159;&#39318;&#27425;&#20351;&#29992;FPN&#25216;&#26415;&#20174;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#21462;&#20195;&#20102;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20998;&#35299;&#20026;&#36235;&#21183;&#21644;&#23395;&#33410;&#39033;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#34701;&#21512;&#32467;&#26500;&#12290;&#22312;8&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#21518;&#65292;&#19982;&#26368;&#20339;&#32447;&#24615;&#27169;&#22411;DLiner&#30456;&#27604;&#65292;31&#20010;&#27979;&#35797;&#39033;&#30446;&#20013;&#26377;32&#20010;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#24179;&#22343;mse&#19979;&#38477;10.1&#65285;&#65292;&#24179;&#22343;mae&#19979;&#38477;9.1&#65285;&#12290;&#19982;&#22797;&#26434;&#30340;&#22522;&#20110;Transformer&#30340;PatchTST&#30456;&#27604;&#65292;UnetTSF&#22312;32&#20010;&#27979;&#35797;&#39033;&#30446;&#20013;&#33719;&#24471;&#20102;9&#20010;mse&#30340;&#26368;&#20339;&#32467;&#26524;&#21644;15&#20010;mae&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Transformer-base models have made significant progress in the field of time series prediction which have achieved good results and become baseline models beyond Dlinear. The paper proposes an U-Net time series prediction model (UnetTSF) with linear complexity, which adopts the U-Net architecture. We are the first to use FPN technology to extract features from time series data, replacing the method of decomposing time series data into trend and seasonal terms, while designing a fusion structure suitable for time series data. After testing on 8 open-source datasets, compared to the best linear model DLiner. Out of 32 testing projects, 31 achieved the best results. The average decrease in mse is 10.1%, while the average decrease in mae is 9.1%. Compared with the complex transformer-base PatchTST, UnetTSF obtained 9 optimal results for mse and 15 optimal results for mae in 32 testing projects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#36716;&#21270;&#20026;&#26356;&#23454;&#29992;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#21333;&#27169;&#24577;&#12289;&#20165;&#35821;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#25216;&#26415;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03000</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#36830;&#25509;&#65306;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#36716;&#21270;&#20026;&#21333;&#27169;&#24577;&#12289;&#20165;&#35821;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition. (arXiv:2401.03000v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#36716;&#21270;&#20026;&#26356;&#23454;&#29992;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#21333;&#27169;&#24577;&#12289;&#20165;&#35821;&#38899;&#30340;&#24773;&#24863;&#35782;&#21035;&#12290;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#25216;&#26415;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#25152;&#20381;&#36182;&#30340;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23558;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#36716;&#21270;&#20026;&#26356;&#23454;&#29992;&#19988;&#36164;&#28304;&#39640;&#25928;&#30340;&#21333;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#20855;&#20307;&#20851;&#27880;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#12290;&#20174;&#35821;&#38899;&#20449;&#21495;&#20013;&#35782;&#21035;&#24773;&#24863;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#12289;&#24773;&#24863;&#35745;&#31639;&#21644;&#24515;&#29702;&#20581;&#24247;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#21253;&#25324;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#20449;&#24687;&#65292;&#22914;&#38754;&#37096;&#34920;&#24773;&#21644;&#25163;&#21183;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#26131;&#33719;&#24471;&#25110;&#19981;&#21487;&#34892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#23631;&#34109;&#35757;&#32451;&#25216;&#26415;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative approach to address the challenges of translating multi-modal emotion recognition models to a more practical and resource-efficient uni-modal counterpart, specifically focusing on speech-only emotion recognition. Recognizing emotions from speech signals is a critical task with applications in human-computer interaction, affective computing, and mental health assessment. However, existing state-of-the-art models often rely on multi-modal inputs, incorporating information from multiple sources such as facial expressions and gestures, which may not be readily available or feasible in real-world scenarios. To tackle this issue, we propose a novel framework that leverages knowledge distillation and masked training techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bias Free Network (RBFNet) &#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#36731;&#28151;&#28102;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#20934;&#30830;&#19988;&#26080;&#20559;&#20506;&#30340;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#35786;&#26029;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;COVID-19&#25968;&#25454;&#38598;&#32435;&#20837;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#35813;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02996</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#21683;&#22013;&#38899;&#39057;&#30340;AI&#36741;&#21161;&#26080;&#20559;&#24046;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65306;&#20197;COVID-19&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough Audio: A Case Study for COVID-19. (arXiv:2401.02996v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Bias Free Network (RBFNet) &#30340;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20943;&#36731;&#28151;&#28102;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#20934;&#30830;&#19988;&#26080;&#20559;&#20506;&#30340;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#35786;&#26029;&#29305;&#24449;&#12290;&#36890;&#36807;&#23558;COVID-19&#25968;&#25454;&#38598;&#32435;&#20837;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#35813;&#27169;&#22411;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#22522;&#20110;&#21683;&#22013;&#30340;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#35786;&#26029;&#24341;&#36215;&#20102;&#24456;&#22823;&#20851;&#27880;&#65292;&#20294;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#24573;&#35270;&#20102;&#20854;&#39044;&#27979;&#27169;&#22411;&#20013;&#30340;&#28151;&#28102;&#21464;&#37327;&#12290;&#36825;&#20123;&#21464;&#37327;&#21487;&#33021;&#20250;&#25197;&#26354;&#21683;&#22013;&#35760;&#24405;&#65288;&#36755;&#20837;&#25968;&#25454;&#65289;&#21644;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#29366;&#24577;&#65288;&#36755;&#20986;&#21464;&#37327;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23548;&#33268;&#26377;&#20559;&#20506;&#30340;&#20851;&#32852;&#21644;&#19981;&#30495;&#23454;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#20559;&#20506;&#32593;&#32476;&#65288;RBFNet&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#20013;&#28151;&#28102;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;RBFNet&#30830;&#20445;&#20102;&#20934;&#30830;&#19988;&#26080;&#20559;&#20506;&#30340;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#35786;&#26029;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23558;COVID-19&#25968;&#25454;&#38598;&#32435;&#20837;&#26412;&#30740;&#31350;&#26469;&#24378;&#35843;&#20854;&#30456;&#20851;&#24615;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20811;&#26381;&#28151;&#28102;&#22240;&#32032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25552;&#20986;&#20102;RBFNet&#30340;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22359;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#28151;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cough-based diagnosis for Respiratory Diseases (RDs) using Artificial Intelligence (AI) has attracted considerable attention, yet many existing studies overlook confounding variables in their predictive models. These variables can distort the relationship between cough recordings (input data) and RD status (output variable), leading to biased associations and unrealistic model performance. To address this gap, we propose the Bias Free Network (RBFNet), an end to end solution that effectively mitigates the impact of confounders in the training data distribution. RBFNet ensures accurate and unbiased RD diagnosis features, emphasizing its relevance by incorporating a COVID19 dataset in this study. This approach aims to enhance the reliability of AI based RD diagnosis models by navigating the challenges posed by confounding variables. A hybrid of a Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks is proposed for the feature encoder module of RBFNet. An additio
&lt;/p&gt;</description></item><item><title>GLIDE-RL&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;-&#25351;&#23548;&#21592;-&#23398;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#25351;&#20196;&#30340;RL&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.02991</link><description>&lt;p&gt;
GLIDE-RL&#65306;&#22522;&#20110;&#28436;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
GLIDE-RL: Grounded Language Instruction through DEmonstration in RL. (arXiv:2401.02991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02991
&lt;/p&gt;
&lt;p&gt;
GLIDE-RL&#26159;&#19968;&#31181;&#22522;&#20110;&#28436;&#31034;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;-&#25351;&#23548;&#21592;-&#23398;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#65292;&#35757;&#32451;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#25351;&#20196;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#31995;&#32479;&#30340;&#26368;&#21518;&#19968;&#39033;&#25361;&#25112;&#26159;AI&#20195;&#29702;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#24182;&#30456;&#24212;&#22320;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#27495;&#20041;&#24615;&#20197;&#21450;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#35757;&#32451;&#26377;&#25928;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#25361;&#25112;&#12290;&#24378;&#21270;&#23398;&#20064;&#12289;&#35838;&#31243;&#23398;&#20064;&#12289;&#25345;&#32493;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#39033;&#36827;&#23637;&#20998;&#21035;&#20026;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#35757;&#32451;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20195;&#29702;&#20570;&#20986;&#20102;&#26377;&#25928;&#36129;&#29486;&#12290;&#22312;&#36825;&#20123;&#21457;&#23637;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;GLIDE-RL&#65292;&#36890;&#36807;&#24341;&#20837;&#25945;&#24072;-&#25351;&#23548;&#21592;-&#23398;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#26694;&#26550;&#26469;&#22521;&#35757;&#19968;&#20010;&#33021;&#22815;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#19988;&#33021;&#22815;&#25512;&#24191;&#21040;&#20043;&#21069;&#26410;&#35265;&#30340;&#35821;&#35328;&#25351;&#20196;&#30340;RL&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the final frontiers in the development of complex human - AI collaborative systems is the ability of AI agents to comprehend the natural language and perform tasks accordingly. However, training efficient Reinforcement Learning (RL) agents grounded in natural language has been a long-standing challenge due to the complexity and ambiguity of the language and sparsity of the rewards, among other factors. Several advances in reinforcement learning, curriculum learning, continual learning, language models have independently contributed to effective training of grounded agents in various environments. Leveraging these developments, we present a novel algorithm, Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that introduces a teacher-instructor-student curriculum learning framework for training an RL agent capable of following natural language instructions that can generalize to previously unseen language instructions. In this multi-agent framework, the teacher a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#29289;&#31181;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20266;&#32570;&#22833;&#30340;&#36873;&#25321;&#21644;&#25928;&#26524;&#12290;&#20316;&#32773;&#25351;&#20986;&#29289;&#31181;&#20986;&#29616;&#21644;&#20266;&#32570;&#22833;&#20043;&#38388;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20266;&#32570;&#22833;&#36873;&#25321;&#23545;&#32467;&#26524;&#20135;&#29983;&#20102;&#22797;&#26434;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.02989</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#29289;&#31181;&#20998;&#24067;&#24314;&#27169;&#20013;&#20266;&#32570;&#22833;&#36873;&#25321;&#21644;&#25928;&#26524;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning. (arXiv:2401.02989v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02989
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#29289;&#31181;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20266;&#32570;&#22833;&#30340;&#36873;&#25321;&#21644;&#25928;&#26524;&#12290;&#20316;&#32773;&#25351;&#20986;&#29289;&#31181;&#20986;&#29616;&#21644;&#20266;&#32570;&#22833;&#20043;&#38388;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20266;&#32570;&#22833;&#36873;&#25321;&#23545;&#32467;&#26524;&#20135;&#29983;&#20102;&#22797;&#26434;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#31181;&#20998;&#24067;&#24314;&#27169;&#26159;&#19968;&#31181;&#38750;&#24120;&#22810;&#21151;&#33021;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29702;&#35299;&#29615;&#22659;&#26465;&#20214;&#21644;&#29289;&#31181;&#20986;&#29616;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#36890;&#24120;&#32570;&#20047;&#20851;&#20110;&#30830;&#35748;&#30340;&#29289;&#31181;&#32570;&#22833;&#30340;&#20449;&#24687;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#26426;&#20250;&#24615;&#25277;&#26679;&#30340;&#20986;&#29616;&#35266;&#23519;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20266;&#32570;&#22833;&#65292;&#23427;&#20204;&#26159;&#25351;&#23450;&#20026;&#36127;&#26679;&#26412;&#30340;&#29305;&#23450;&#22320;&#29702;&#20301;&#32622;&#12290;&#34429;&#28982;&#20266;&#32570;&#22833;&#22312;&#21333;&#29289;&#31181;&#20998;&#24067;&#27169;&#22411;&#20013;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#22312;&#22810;&#29289;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29289;&#31181;&#23384;&#22312;&#20986;&#29616;&#21644;&#20266;&#32570;&#22833;&#20043;&#38388;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#20266;&#32570;&#22833;&#65288;&#20363;&#22914;&#65292;&#38543;&#26426;&#21644;&#30446;&#26631;&#32676;&#20307;&#32972;&#26223;&#28857;&#65289;&#30340;&#23384;&#22312;&#20351;&#36873;&#25321;&#36807;&#31243;&#21464;&#24471;&#22797;&#26434;&#12290;&#30830;&#23450;&#20266;&#32570;&#22833;&#31867;&#22411;&#30340;&#26368;&#20339;&#32452;&#21512;&#26159;&#22256;&#38590;&#30340;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Species distribution modeling is a highly versatile tool for understanding the intricate relationship between environmental conditions and species occurrences. However, the available data often lacks information on confirmed species absence and is limited to opportunistically sampled, presence-only observations. To overcome this limitation, a common approach is to employ pseudo-absences, which are specific geographic locations designated as negative samples. While pseudo-absences are well-established for single-species distribution models, their application in the context of multi-species neural networks remains underexplored. Notably, the significant class imbalance between species presences and pseudo-absences is often left unaddressed. Moreover, the existence of different types of pseudo-absences (e.g., random and target-group background points) adds complexity to the selection process. Determining the optimal combination of pseudo-absences types is difficult and depends on the char
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#25193;&#23637;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;XGAN&#65289;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#20248;&#21270;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#35774;&#35745;&#12290;XGAN&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#20934;&#30830;&#22320;&#29983;&#25104;&#20803;&#34920;&#38754;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.02961</link><description>&lt;p&gt;
&#22522;&#20110;&#20195;&#29702;&#30340;&#25193;&#23637;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#35774;&#35745;&#20013;&#30340;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Surrogate-Assisted Extended Generative Adversarial Network for Parameter Optimization in Free-Form Metasurface Design. (arXiv:2401.02961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#30340;&#25193;&#23637;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;XGAN&#65289;&#65292;&#29992;&#20110;&#21152;&#36895;&#21644;&#20248;&#21270;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#35774;&#35745;&#12290;XGAN&#36890;&#36807;&#29289;&#29702;&#32422;&#26463;&#20934;&#30830;&#22320;&#29983;&#25104;&#20803;&#34920;&#38754;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#34920;&#38754;&#22312;&#31532;&#20116;&#20195;&#65288;5G&#65289;&#24494;&#27874;&#36890;&#20449;&#20013;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#20803;&#34920;&#38754;&#23478;&#26063;&#20013;&#65292;&#19982;&#24120;&#35268;&#24418;&#29366;&#30456;&#27604;&#65292;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#22312;&#23454;&#29616;&#22797;&#26434;&#30340;&#20809;&#35889;&#21709;&#24212;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#25968;&#20540;&#26041;&#27861;&#32791;&#26102;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#23545;&#20110;&#21152;&#36895;&#21644;&#20248;&#21270;&#20803;&#34920;&#38754;&#35774;&#35745;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;XGAN&#65292;&#19968;&#31181;&#24102;&#26377;&#20195;&#29702;&#30340;&#25193;&#23637;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#30340;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#20026;XGAN&#25552;&#20379;&#29289;&#29702;&#32422;&#26463;&#65292;&#20197;&#20415;XGAN&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#36755;&#20837;&#30340;&#20809;&#35889;&#21709;&#24212;&#20013;&#29983;&#25104;&#20803;&#34920;&#38754;&#12290;&#22312;&#28041;&#21450;20000&#20010;&#33258;&#30001;&#24418;&#24577;&#20803;&#34920;&#38754;&#35774;&#35745;&#30340;&#27604;&#36739;&#23454;&#39564;&#20013;&#65292;XGAN&#23454;&#29616;&#20102;0.9734&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#27604;&#20256;&#32479;&#26041;&#27861;&#24555;500&#20493;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#20803;&#34920;&#38754;&#24211;&#30340;&#24314;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metasurfaces have widespread applications in fifth-generation (5G) microwave communication. Among the metasurface family, free-form metasurfaces excel in achieving intricate spectral responses compared to regular-shape counterparts. However, conventional numerical methods for free-form metasurfaces are time-consuming and demand specialized expertise. Alternatively, recent studies demonstrate that deep learning has great potential to accelerate and refine metasurface designs. Here, we present XGAN, an extended generative adversarial network (GAN) with a surrogate for high-quality free-form metasurface designs. The proposed surrogate provides a physical constraint to XGAN so that XGAN can accurately generate metasurfaces monolithically from input spectral responses. In comparative experiments involving 20000 free-form metasurface designs, XGAN achieves 0.9734 average accuracy and is 500 times faster than the conventional methodology. This method facilitates the metasurface library buildi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.02740</link><description>&lt;p&gt;
&#20026;&#22810;&#20219;&#21153;&#32852;&#37030;&#23398;&#20064;&#25552;&#20379;&#20844;&#24179;&#24615;&#24863;&#30693;&#30340;&#20316;&#19994;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#33021;&#22815;&#22312;&#19981;&#27844;&#38706;&#25935;&#24863;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22404;&#26029;&#22330;&#26223;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#21333;&#20010;FL&#26381;&#21153;&#22120;&#22312;&#27599;&#36718;&#35757;&#32451;&#20013;&#36873;&#25321;&#19968;&#37096;&#20998;FL&#23458;&#25143;&#31471;&#26469;&#26356;&#26032;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#23454;&#38469;&#19978;&#65292;&#21487;&#33021;&#20250;&#26377;&#22810;&#20010;FL&#26381;&#21153;&#22120;&#21516;&#26102;&#23581;&#35797;&#20174;&#21516;&#19968;&#20010;&#27744;&#20013;&#36873;&#25321;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39318;&#21019;&#30340;&#20844;&#24179;&#24863;&#30693;&#32852;&#37030;&#20316;&#19994;&#35843;&#24230;&#65288;FairFedJS&#65289;&#26041;&#27861;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#22522;&#20110;Lyapunov&#20248;&#21270;&#65292;&#23427;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#24403;&#21069;&#38656;&#27714;&#21644;&#20316;&#19994;&#20184;&#27454;&#20986;&#20215;&#65292;&#30830;&#20445;&#23558;&#39640;&#38656;&#27714;&#30340;FL&#23458;&#25143;&#31471;&#25968;&#25454;&#38598;&#20844;&#24179;&#20998;&#37197;&#32473;&#38656;&#35201;&#23427;&#20204;&#30340;FL&#20316;&#19994;&#65292;&#20197;&#38450;&#27490;&#31561;&#24453;&#26102;&#38388;&#36807;&#38271;&#12290;&#22522;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;FairFedJS&#19982;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#20248;&#21183;&#12290;&#23427;&#22312;&#24179;&#22343;&#19978;&#20987;&#36133;&#20102;&#26368;&#20339;&#22522;&#20934;&#32447;31.9%&#21644;1.0%&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to collaboratively train machine learning models without disclosing sensitive private data. Existing FL research mostly focuses on the monopoly scenario in which a single FL server selects a subset of FL clients to update their local models in each round of training. In practice, there can be multiple FL servers simultaneously trying to select clients from the same pool. In this paper, we propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS) approach to bridge this gap. Based on Lyapunov optimization, it ensures fair allocation of high-demand FL client datasets to FL jobs in need of them, by jointly considering the current demand and the job payment bids, in order to prevent prolonged waiting. Extensive experiments comparing FairFedJS against four state-of-the-art approaches on two datasets demonstrate its significant advantages. It outperforms the best baseline by 31.9% and 1.0% on avera
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#25429;&#25417;&#22122;&#22768;&#24182;&#35843;&#25972;&#21442;&#25968;&#26469;&#22686;&#24378;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#35777;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.02325</link><description>&lt;p&gt;
&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#20855;&#26377;&#21487;&#35299;&#37322;&#21442;&#25968;&#35843;&#25972;&#30340;&#40065;&#26834;&#20998;&#20301;&#25968;Huber&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#25429;&#25417;&#22122;&#22768;&#24182;&#35843;&#25972;&#21442;&#25968;&#26469;&#22686;&#24378;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#35777;&#27979;&#35797;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#20989;&#25968;&#26469;&#20272;&#35745;&#22238;&#25253;&#20998;&#24067;&#65292;&#35813;&#20989;&#25968;&#20174;&#39640;&#26031;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#35745;&#31639;&#20013;&#20135;&#29983;&#65292;&#25429;&#25417;&#21040;&#24403;&#21069;&#21644;&#30446;&#26631;&#20998;&#20301;&#25968;&#20540;&#20013;&#30340;&#22122;&#22768;&#12290;&#19982;&#32463;&#20856;&#30340;&#20998;&#20301;&#25968;Huber&#25439;&#22833;&#30456;&#27604;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#20102;&#23545;&#24322;&#24120;&#20540;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#36817;&#20284;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#25968;&#37327;&#26469;&#35843;&#25972;&#21442;&#25968;&#12290;&#23454;&#35777;&#27979;&#35797;&#22312;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#30340;&#24120;&#35265;&#24212;&#29992;Atari&#28216;&#25103;&#21644;&#26368;&#36817;&#30340;&#23545;&#20914;&#31574;&#30053;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values. Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers. Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the eff
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#22270;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#35270;&#22270;&#21644;&#22270;&#27169;&#24335;&#26469;&#35299;&#37322;&#29305;&#23450;&#31867;&#21035;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02086</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#22270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02086
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#22270;&#30340;&#35299;&#37322;&#26041;&#27861;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#34892;&#20026;&#65292;&#36890;&#36807;&#29983;&#25104;&#35299;&#37322;&#35270;&#22270;&#21644;&#22270;&#27169;&#24335;&#26469;&#35299;&#37322;&#29305;&#23450;&#31867;&#21035;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#29983;&#25104;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#35299;&#37322;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22270;&#20998;&#31867;&#31561;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26088;&#22312;&#29702;&#35299;GNNs&#30340;&#25972;&#20307;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#33021;&#36820;&#22238;&#38590;&#20197;&#35775;&#38382;&#25110;&#30452;&#25509;&#26597;&#35810;&#30340;&#35299;&#37322;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33539;&#24335;GVEX&#65292;&#29992;&#20110;&#29983;&#25104;&#22270;&#35299;&#37322;&#30340;&#22270;&#35270;&#22270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#23618;&#30340;&#35299;&#37322;&#32467;&#26500;&#65292;&#31216;&#20026;&#35299;&#37322;&#35270;&#22270;&#12290;&#35299;&#37322;&#35270;&#22270;&#21253;&#25324;&#19968;&#32452;&#22270;&#27169;&#24335;&#21644;&#19968;&#32452;&#35825;&#21457;&#30340;&#35299;&#37322;&#23376;&#22270;&#12290;&#32473;&#23450;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#22270;&#21644;&#30001;&#22522;&#20110;GNN&#30340;&#20998;&#31867;&#22120;M&#20998;&#37197;&#30340;&#29305;&#23450;&#31867;&#21035;&#26631;&#31614;l&#30340;&#25968;&#25454;&#24211;G&#65292;&#23427;&#31616;&#27905;&#22320;&#25551;&#36848;&#20102;&#26368;&#22909;&#35299;&#37322;&#20026;&#20160;&#20040;l&#30001;M&#20998;&#37197;&#30340;G&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#21046;&#23450;&#20102;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#26469;&#35745;&#31639;GNN&#35299;&#37322;&#30340;&#26368;&#20339;&#35299;&#37322;&#35270;&#22270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#38382;&#39064;&#26159;&#931;^2_P&#38590;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating explanations for graph neural networks (GNNs) has been studied to understand their behavior in analytical tasks such as graph classification. Existing approaches aim to understand the overall results of GNNs rather than providing explanations for specific class labels of interest, and may return explanation structures that are hard to access, nor directly queryable.  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1) We design a two-tier explanation structure called explanation views. An explanation view consists of a set of graph patterns and a set of induced explanation subgraphs. Given a database G of multiple graphs and a specific class label l assigned by a GNN-based classifier M, it concisely describes the fraction of G that best explains why l is assigned by M. (2) We propose quality measures and formulate an optimization problem to compute optimal explanation views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3) 
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01923</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;: &#35270;&#37326;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01923
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#24863;&#30693;&#12289;&#32593;&#32476;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#22914;&#26234;&#33021;&#25163;&#26426;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#26234;&#33021;&#38899;&#31665;&#21644;&#23478;&#24237;&#26426;&#22120;&#20154;&#65292;&#24050;&#32463;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#30340;&#36827;&#23637;&#65292;&#22914;GPT&#12289;LLaMA&#12289;DALL-E&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#65292;&#32473;&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#25105;&#20204;&#23545;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#24102;&#26469;&#30340;&#22909;&#22788;&#30340;&#30475;&#27861;&#21644;&#24895;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;Generative AI&#22312;&#29289;&#32852;&#32593;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#20123;&#37325;&#35201;&#24212;&#29992;&#12290;&#20805;&#20998;&#21033;&#29992;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;Generative AI&#27169;&#22411;&#30340;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#21368;&#36733;&#12289;&#35774;&#22791;&#31471;&#24494;&#35843;&#12289;&#32852;&#37030;&#23398;&#20064;&#12289;&#23433;&#20840;&#20197;&#21450;&#24320;&#21457;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#24046;&#36317;&#20197;&#21450;&#20351;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#30340;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#33021;&#22815;&#28608;&#21457;&#26032;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
&lt;/p&gt;</description></item><item><title>SCALA&#26159;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#21892;&#32593;&#32476;&#30340;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#21270;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01625</link><description>&lt;p&gt;
SCALA: &#22522;&#20110;&#31232;&#30095;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2401.01625v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01625
&lt;/p&gt;
&lt;p&gt;
SCALA&#26159;&#19968;&#31181;&#20351;&#29992;&#31232;&#30095;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23646;&#24615;&#32593;&#32476;&#20013;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#21892;&#32593;&#32476;&#30340;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#21270;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23646;&#24615;&#32593;&#32476;&#19978;&#30340;&#24322;&#24120;&#26816;&#27979;&#26088;&#22312;&#25214;&#21040;&#19982;&#20854;&#20182;&#22823;&#22810;&#25968;&#33410;&#28857;&#34892;&#20026;&#26126;&#26174;&#19981;&#21516;&#30340;&#33410;&#28857;&#12290;&#36890;&#24120;&#65292;&#32593;&#32476;&#25968;&#25454;&#21253;&#21547;&#23454;&#20307;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#24322;&#24120;&#36890;&#24120;&#20307;&#29616;&#22312;&#36825;&#20123;&#20851;&#31995;&#20013;&#12290;&#22240;&#27492;&#65292;&#22914;&#20309;&#20840;&#38754;&#22320;&#24314;&#27169;&#32593;&#32476;&#20013;&#22797;&#26434;&#30340;&#20132;&#20114;&#27169;&#24335;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#12290;&#21487;&#20197;&#35266;&#23519;&#21040;&#65292;&#22312;&#32593;&#32476;&#20013;&#65292;&#24322;&#24120;&#36829;&#21453;&#20102;&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#21482;&#26159;&#38388;&#25509;&#22320;&#32771;&#34385;&#20102;&#36825;&#31181;&#29616;&#35937;&#65292;&#32780;&#19981;&#26159;&#26174;&#24335;&#22320;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#27491;&#24120;&#23454;&#20307;&#30340;&#33410;&#28857;&#34920;&#31034;&#24456;&#23481;&#26131;&#21463;&#21040;&#24322;&#24120;&#33410;&#28857;&#24341;&#20837;&#30340;&#22122;&#22768;&#20851;&#31995;&#30340;&#24178;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#23646;&#24615;&#32593;&#32476;&#24322;&#24120;&#26816;&#27979;&#26694;&#26550;SCALA&#65292;&#26088;&#22312;&#25913;&#21892;&#32593;&#32476;&#30340;&#23884;&#20837;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#21270;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#25552;&#20379;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#24322;&#24120;&#24471;&#20998;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes. Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships. Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus. It can be observed that anomalies in networks violate the homophily assumption. However, most existing studies only considered this phenomenon obliquely rather than explicitly. Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes. To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;</title><link>http://arxiv.org/abs/2401.01519</link><description>&lt;p&gt;
&#25506;&#32034;LLMs&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65306;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#65292;&#21253;&#25324;&#22914;&#20309;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#12289;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#36827;&#34892;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#23398;&#24212;&#29992;&#20013;&#30340;&#21069;&#27839;&#12290;&#24515;&#29702;&#23398;&#32463;&#21382;&#20102;&#20960;&#27425;&#29702;&#35770;&#21464;&#38761;&#65292;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;LLMs&#30340;&#20351;&#29992;&#26377;&#26395;&#24320;&#21551;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#35814;&#32454;&#25506;&#35752;&#20102;LLMs&#22914;ChatGPT&#22312;&#24515;&#29702;&#23398;&#30740;&#31350;&#20013;&#30340;&#36716;&#21464;&#12290;&#25991;&#31456;&#35752;&#35770;&#20102;LLMs&#22312;&#35748;&#30693;&#19982;&#34892;&#20026;&#24515;&#29702;&#23398;&#12289;&#20020;&#24202;&#19982;&#21672;&#35810;&#24515;&#29702;&#23398;&#12289;&#25945;&#32946;&#19982;&#21457;&#23637;&#24515;&#29702;&#23398;&#20197;&#21450;&#31038;&#20250;&#19982;&#25991;&#21270;&#24515;&#29702;&#23398;&#31561;&#24515;&#29702;&#23398;&#20998;&#25903;&#20013;&#30340;&#24433;&#21709;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#25991;&#26412;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#20026;&#24515;&#29702;&#23398;&#20013;&#30340;&#25991;&#29486;&#22238;&#39038;&#12289;&#20551;&#35774;&#29983;&#25104;&#12289;&#23454;&#39564;&#35774;&#35745;&#12289;&#23454;&#39564;&#23545;&#35937;&#12289;&#25968;&#25454;&#20998;&#26512;&#12289;&#23398;&#26415;&#20889;&#20316;&#21644;&#21516;&#34892;&#35780;&#23457;&#31561;&#25552;&#20379;&#21019;&#26032;&#24037;&#20855;&#12290;&#34429;&#28982;LLMs&#22312;&#25512;&#21160;&#30740;&#31350;&#26041;&#27861;&#23398;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01470</link><description>&lt;p&gt;
&#39640;&#25928;&#35270;&#35273;Transformer&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#36890;&#36807;&#32467;&#21512;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35270;&#35273;Transformer&#30340;&#25928;&#29575;&#21644;&#20196;&#29260;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViTs&#65289;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#19978;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#37319;&#29992;&#36880;&#28176;&#20943;&#23569;&#20196;&#29260;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20551;&#35774;&#19968;&#20010;&#23618;&#20013;&#30340;&#20196;&#29260;&#20887;&#20313;&#24847;&#21619;&#30528;&#25152;&#26377;&#21518;&#32493;&#23618;&#20013;&#20063;&#26377;&#20887;&#20313;&#12290;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#36825;&#20010;&#20551;&#35774;&#36890;&#24120;&#26159;&#19981;&#27491;&#30830;&#30340;&#65292;&#21363;&#19968;&#20010;&#23618;&#20013;&#22810;&#20313;&#30340;&#20196;&#29260;&#22312;&#21518;&#38754;&#30340;&#23618;&#20013;&#21487;&#20197;&#26159;&#26377;&#29992;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20196;&#29260;&#20256;&#25773;&#25511;&#21046;&#22120;&#65288;TPC&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20196;&#29260;&#20998;&#24067;&#65292;&#21363;&#26242;&#20572;&#27010;&#29575;&#21644;&#37325;&#26032;&#24320;&#22987;&#27010;&#29575;&#65292;&#29992;&#26469;&#25511;&#21046;&#20196;&#29260;&#30340;&#20943;&#23569;&#21644;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#20196;&#29260;&#21033;&#29992;&#12290;&#20026;&#20102;&#25913;&#21892;&#20196;&#29260;&#20998;&#24067;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#26426;&#21046;&#65292;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#26377;&#21161;&#20110;&#21435;&#38500;&#22122;&#22768;&#24322;&#24120;&#20540;&#12290;&#27492;&#22806;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;</title><link>http://arxiv.org/abs/2401.01404</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23376;&#20108;&#27425;&#26102;&#38388;&#32593;&#32476;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#37325;&#24314;&#26159;&#25351;&#22312;&#21482;&#26377;&#20851;&#20110;&#26465;&#20214;&#20598;&#32852;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#25110;&#22270;&#27169;&#22411;&#30340;&#29420;&#31435;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;N&#20010;&#33410;&#28857;&#20043;&#38388;&#26410;&#35266;&#27979;&#21040;&#30340;&#25104;&#23545;&#32806;&#21512;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20284;&#20046;&#26080;&#27861;&#36991;&#20813;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;O(N^2)&#65292;&#21363;&#35201;&#32771;&#34385;&#27599;&#31181;&#21487;&#33021;&#30340;&#25104;&#23545;&#32806;&#21512;&#33267;&#23569;&#19968;&#27425;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#24863;&#20852;&#36259;&#30340;&#32593;&#32476;&#37117;&#26159;&#31232;&#30095;&#30340;&#65292;&#38750;&#38646;&#32806;&#21512;&#30340;&#25968;&#37327;&#21482;&#26377;O(N)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#37325;&#24314;&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#20854;&#22312;&#23376;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#20854;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24230;&#23485;&#26494;&#19978;&#30028;&#20026;O(N^(3/2)logN)&#65292;&#20294;&#20855;&#26377;&#26356;&#20856;&#22411;&#30340;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;O(Nlog^2 N)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#65292;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22238;&#28335;&#26032;&#30340;Q-Newton&#26041;&#27861;&#65288;BNQN&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;Newton&#26041;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#21457;&#29616;BNQN&#22312;&#22810;&#39033;&#24335;&#21644;&#20122;&#32431;&#20989;&#25968;&#30340;&#26681;&#25628;&#32034;&#20013;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#21560;&#24341;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;Newton&#27969;&#21644;Voronoi&#22270;&#30340;&#32852;&#31995;&#25552;&#20986;&#20102;&#19968;&#20123;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#38754;&#23545;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;BNQN&#27604;Newton&#26041;&#27861;&#21644;&#38543;&#26426;&#26494;&#24347;Newton&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01393</link><description>&lt;p&gt;
&#22238;&#28335;&#26032;Q-Newton&#26041;&#27861;&#65292;Newton&#27969;&#65292;Voronoi&#22270;&#21644;&#38543;&#26426;&#27714;&#26681;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and Stochastic root finding. (arXiv:2401.01393v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22238;&#28335;&#26032;&#30340;Q-Newton&#26041;&#27861;&#65288;BNQN&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;Newton&#26041;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#22312;&#23454;&#39564;&#20013;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#21457;&#29616;BNQN&#22312;&#22810;&#39033;&#24335;&#21644;&#20122;&#32431;&#20989;&#25968;&#30340;&#26681;&#25628;&#32034;&#20013;&#20855;&#26377;&#26356;&#24179;&#28369;&#30340;&#21560;&#24341;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;Newton&#27969;&#21644;Voronoi&#22270;&#30340;&#32852;&#31995;&#25552;&#20986;&#20102;&#19968;&#20123;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#38754;&#23545;&#38543;&#26426;&#25200;&#21160;&#26102;&#65292;BNQN&#27604;Newton&#26041;&#27861;&#21644;&#38543;&#26426;&#26494;&#24347;Newton&#26041;&#27861;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#31532;&#19977;&#20316;&#32773;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22238;&#28335;&#26032;Q-Newton&#26041;&#27861;(BNQN)&#30340;Newton&#26041;&#27861;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20808;&#21069;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#20351;&#29992;BNQN&#21487;&#20197;&#25214;&#21040;&#22810;&#39033;&#24335;&#21644;&#20122;&#32431;&#20989;&#25968;&#30340;&#26681;&#26102;&#65292;&#21560;&#24341;&#22495;&#20855;&#26377;&#19968;&#20123;&#26174;&#33879;&#30340;&#29305;&#24615;&#12290;&#24635;&#20307;&#19978;&#30475;&#65292;&#23427;&#20204;&#27604;Newton&#26041;&#27861;&#30340;&#21560;&#24341;&#22495;&#26356;&#21152;&#24179;&#28369;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32487;&#32493;&#28145;&#20837;&#23454;&#39564;&#30740;&#31350;&#36825;&#19968;&#26174;&#33879;&#29616;&#35937;&#65292;&#24182;&#23558;BNQN&#19982;Newton&#27969;&#21644;Voronoi&#22270;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#31181;&#32852;&#31995;&#32473;&#20986;&#20102;&#19968;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38590;&#39064;&#38656;&#35201;&#35299;&#37322;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#19982;Newton&#26041;&#27861;&#21644;&#38543;&#26426;&#26494;&#24347;Newton&#26041;&#27861;&#30456;&#27604;&#65292;BNQN&#23545;&#38543;&#26426;&#25200;&#21160;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new variant of Newton's method - named Backtracking New Q-Newton's method (BNQN) - which has strong theoretical guarantee, is easy to implement, and has good experimental performance, was recently introduced by the third author.  Experiments performed previously showed some remarkable properties of the basins of attractions for finding roots of polynomials and meromorphic functions, with BNQN. In general, they look more smooth than that of Newton's method.  In this paper, we continue to experimentally explore in depth this remarkable phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link poses a couple of challenging puzzles to be explained. Experiments also indicate that BNQN is more robust against random perturbations than Newton's method and Random Relaxed Newton's method.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.01383</link><description>&lt;p&gt;
&#20351;&#29992;&#31232;&#32570;&#25968;&#25454;&#21644;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01383
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#37030;&#22810;&#36712;&#36857;GNN&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#32570;&#25968;&#25454;&#39044;&#27979;&#23156;&#20799;&#33041;&#36830;&#25509;&#24615;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#36890;&#36807;&#32858;&#21512;&#22810;&#20010;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35782;&#21035;&#26089;&#26399;&#33041;&#36830;&#25509;&#24615;&#21457;&#23637;&#30340;&#21160;&#24577;&#36807;&#31243;&#65292;&#20102;&#35299;&#23156;&#20799;&#33041;&#32593;&#32476;&#22312;&#20986;&#29983;&#21518;&#30340;&#31532;&#19968;&#24180;&#20013;&#30340;&#22797;&#26434;&#28436;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#19981;&#33021;&#27867;&#21270;&#21040;&#22810;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#36712;&#36857;&#23545;&#24212;&#20110;&#29305;&#23450;&#30340;&#25104;&#20687;&#27169;&#24577;&#25110;&#36830;&#25509;&#31867;&#22411;&#65288;&#20363;&#22914;T1-w MRI&#65289;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#24448;&#24448;&#24456;&#38590;&#33719;&#21462;&#12290;&#31532;&#19977;&#65292;&#23427;&#20204;&#19981;&#33021;&#26377;&#25928;&#21033;&#29992;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedGmTE-Net++&#65292;&#19968;&#31181;&#32852;&#37030;&#22270;&#24418;&#22810;&#36712;&#36857;&#28436;&#21270;&#32593;&#32476;&#12290;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#25105;&#20204;&#22312;&#26377;&#38480;&#30340;&#21307;&#38498;&#25968;&#25454;&#38598;&#20013;&#32858;&#21512;&#20102;&#19981;&#21516;&#21307;&#38498;&#30340;&#26412;&#22320;&#23398;&#20064;&#32467;&#26524;&#12290;&#32467;&#26524;&#21363;&#21487;&#25552;&#39640;&#27599;&#20010;&#21307;&#38498;&#26412;&#22320;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#32602;&#20989;&#25968;&#27861;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#22815;&#20135;&#29983;&#19981;&#38169;&#30340;&#36817;&#20284;&#35299;&#12290;&#22312;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26159;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26356;&#26032;&#35268;&#21017;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2401.01306</link><description>&lt;p&gt;
&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23398;&#20064;&#19968;&#20123;&#29609;&#20855;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces. (arXiv:2401.01306v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#36816;&#29992;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#32602;&#20989;&#25968;&#27861;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#22312;&#29609;&#20855;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#22815;&#20135;&#29983;&#19981;&#38169;&#30340;&#36817;&#20284;&#35299;&#12290;&#22312;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26159;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26356;&#26032;&#35268;&#21017;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26080;&#38480;&#32500;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20004;&#31181;&#27969;&#34892;&#30340;&#29702;&#35770;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#8212;&#8212;&#32602;&#20989;&#25968;&#27861;&#21644;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#27861;&#30340;&#28145;&#24230;&#23398;&#20064;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20123;&#28304;&#33258;&#21464;&#20998;&#27861;&#25110;&#29289;&#29702;&#23398;&#30340;&#29609;&#20855;&#38382;&#39064;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#31639;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#22815;&#20135;&#29983;&#23545;&#27979;&#35797;&#38382;&#39064;&#30340;&#19981;&#38169;&#36817;&#20284;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#35823;&#24046;&#26041;&#38754;&#26159;&#21487;&#27604;&#36739;&#30340;&#12290;&#36890;&#36807;&#21033;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#26356;&#26032;&#35268;&#21017;&#22312;&#35745;&#31639;&#19978;&#27604;&#27714;&#35299;&#32602;&#20989;&#25968;&#27861;&#20013;&#30340;&#23376;&#38382;&#39064;&#26356;&#31616;&#21333;&#30340;&#26222;&#36941;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#36755;&#20986;&#32422;&#26463;&#20989;&#25968;&#26412;&#36523;&#26159;&#19968;&#20010;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present deep learning implementations of two popular theoretical constrained optimization algorithms in infinite dimensional Hilbert spaces, namely, the penalty and the augmented Lagrangian methods. We test these algorithms on some toy problems originating in either calculus of variations or physics. We demonstrate that both methods are able to produce decent approximations for the test problems and are comparable in terms of different errors. Leveraging the common occurrence of the Lagrange multiplier update rule being computationally less expensive than solving subproblems in the penalty method, we achieve significant speedups in cases when the output of the constraint function is itself a function.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01286</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#65292;&#26088;&#22312;&#26377;&#25928;&#20462;&#25913;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#19982;&#20154;&#31867;&#20132;&#27969;&#32039;&#23494;&#30456;&#20284;&#30340;&#25991;&#26412;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20854;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26174;&#33879;&#35745;&#31639;&#38656;&#27714;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#36896;&#25104;&#30340;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#20110;&#19990;&#30028;&#30340;&#21160;&#24577;&#24615;&#65292;&#38656;&#35201;&#39057;&#32321;&#26356;&#26032;LLM&#20197;&#20462;&#27491;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#38598;&#25104;&#26032;&#30693;&#35782;&#65292;&#20174;&#32780;&#30830;&#20445;&#20854;&#25345;&#32493;&#30340;&#30456;&#20851;&#24615;&#12290;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#36827;&#34892;&#25345;&#32493;&#30340;&#27169;&#22411;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#32570;&#38519;&#25110;&#19981;&#33391;&#34892;&#20026;&#12290;&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;LLM&#30340;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#30340;&#20852;&#36259;&#36234;&#26469;&#36234;&#39640;&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#20869;&#26377;&#25928;&#22320;&#20462;&#25913;LLM&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#22312;&#21508;&#31181;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#20041;&#20102;&#30693;&#35782;&#32534;&#36753;&#30340;&#30446;&#26631;&#21644;&#25361;&#25112;&#65292;&#28982;&#21518;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#24212;&#29992;&#21644;&#26410;&#26469;&#21457;&#23637;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the kno
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#30340;&#35282;&#24230;&#28145;&#20837;&#30740;&#31350;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#8212;&#8212;&#36317;&#31163;&#38598;&#20013;&#21644;&#27969;&#24418;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;Minkowski&#36317;&#31163;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;NNS&#65289;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.00422</link><description>&lt;p&gt;
&#20174;&#36317;&#31163;&#38598;&#20013;&#21644;&#27969;&#24418;&#25928;&#24212;&#35299;&#35835;&#32500;&#24230;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect. (arXiv:2401.00422v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#30340;&#35282;&#24230;&#28145;&#20837;&#30740;&#31350;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#8212;&#8212;&#36317;&#31163;&#38598;&#20013;&#21644;&#27969;&#24418;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20351;&#29992;Minkowski&#36317;&#31163;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;NNS&#65289;&#22312;&#39640;&#32500;&#25968;&#25454;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32500;&#24230;&#30340;&#22686;&#21152;&#65292;&#25968;&#25454;&#30340;&#29305;&#24449;&#22914;&#20998;&#24067;&#21644;&#24322;&#36136;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#36829;&#21453;&#30452;&#35273;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#32500;&#24230;&#35781;&#21650;&#65292;&#20302;&#32500;&#31354;&#38388;&#20013;&#25104;&#31435;&#30340;&#24120;&#35265;&#27169;&#24335;&#21644;&#20851;&#31995;&#65288;&#20363;&#22914;&#20869;&#37096;&#21644;&#36793;&#30028;&#27169;&#24335;&#65289;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#21487;&#33021;&#26080;&#25928;&#12290;&#36825;&#23548;&#33268;&#22238;&#24402;&#12289;&#20998;&#31867;&#25110;&#32858;&#31867;&#27169;&#22411;&#25110;&#31639;&#27861;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#32500;&#24230;&#35781;&#21650;&#21487;&#20197;&#24402;&#22240;&#20110;&#35768;&#22810;&#21407;&#22240;&#12290;&#26412;&#25991;&#39318;&#20808;&#24635;&#32467;&#20102;&#19982;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#30456;&#20851;&#30340;&#20116;&#20010;&#25361;&#25112;&#65292;&#24182;&#35299;&#37322;&#20102;&#22238;&#24402;&#12289;&#20998;&#31867;&#25110;&#32858;&#31867;&#20219;&#21153;&#22833;&#36133;&#30340;&#28508;&#22312;&#21407;&#22240;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#28145;&#20837;&#30740;&#31350;&#20102;&#32500;&#24230;&#35781;&#21650;&#30340;&#20004;&#20010;&#20027;&#35201;&#21407;&#22240;&#65292;&#21363;&#36317;&#31163;&#38598;&#20013;&#21644;&#27969;&#24418;&#25928;&#24212;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#19977;&#31181;&#20856;&#22411;&#30340;&#36317;&#31163;&#27979;&#37327;&#36827;&#34892;&#26368;&#36817;&#37051;&#25628;&#32034;&#65288;NNS&#65289;&#26102;&#65292;Minkowski&#36317;&#31163;&#30340;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
The characteristics of data like distribution and heterogeneity, become more complex and counterintuitive as the dimensionality increases. This phenomenon is known as curse of dimensionality, where common patterns and relationships (e.g., internal and boundary pattern) that hold in low-dimensional space may be invalid in higher-dimensional space. It leads to a decreasing performance for the regression, classification or clustering models or algorithms. Curse of dimensionality can be attributed to many causes. In this paper, we first summarize five challenges associated with manipulating high-dimensional data, and explains the potential causes for the failure of regression, classification or clustering tasks. Subsequently, we delve into two major causes of the curse of dimensionality, distance concentration and manifold effect, by performing theoretical and empirical analyses. The results demonstrate that nearest neighbor search (NNS) using three typical distance measurements, Minkowski
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TSPP&#65292;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#35774;&#32622;&#19979;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#22256;&#38590;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#24320;&#21457;&#24037;&#20316;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#31934;&#24515;&#23454;&#26045;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#19982;&#38656;&#35201;&#22823;&#37327;&#29305;&#24449;&#24037;&#31243;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30456;&#23218;&#32654;&#65292;&#32780;&#21482;&#38656;&#20184;&#20986;&#26368;&#23567;&#30340;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.17100</link><description>&lt;p&gt;
TSPP&#65306;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
TSPP: A Unified Benchmarking Tool for Time-series Forecasting. (arXiv:2312.17100v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;TSPP&#65292;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#22522;&#20934;&#27979;&#35797;&#24037;&#20855;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#35774;&#32622;&#19979;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#27604;&#36739;&#22256;&#38590;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#24320;&#21457;&#24037;&#20316;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#31934;&#24515;&#23454;&#26045;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#19982;&#38656;&#35201;&#22823;&#37327;&#29305;&#24449;&#24037;&#31243;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30456;&#23218;&#32654;&#65292;&#32780;&#21482;&#38656;&#20184;&#20986;&#26368;&#23567;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#37325;&#28857;&#20027;&#35201;&#38598;&#20013;&#22312;&#25968;&#25454;&#33719;&#21462;&#21644;&#27169;&#22411;&#21019;&#24314;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#25972;&#20010;&#27969;&#31243;&#30340;&#26631;&#20934;&#21270;&#12290;&#36825;&#31181;&#38656;&#27714;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#23588;&#20026;&#36843;&#20999;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#35774;&#32622;&#22952;&#30861;&#20102;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26377;&#24847;&#20041;&#30340;&#27604;&#36739;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20844;&#24320;&#20102;&#24320;&#21457;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#20851;&#38190;&#24314;&#27169;&#21644;&#26426;&#22120;&#23398;&#20064;&#20915;&#31574;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#24110;&#21161;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24320;&#23637;&#20182;&#20204;&#30340;&#24320;&#21457;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#35813;&#26694;&#26550;&#20869;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#31934;&#24515;&#23454;&#26045;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#19982;&#38656;&#35201;&#22823;&#37327;&#29305;&#24449;&#24037;&#31243;&#21644;&#19987;&#23478;&#30693;&#35782;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30456;&#23218;&#32654;&#65292;&#32780;&#21482;&#38656;&#20184;&#20986;&#26368;&#23567;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While machine learning has witnessed significant advancements, the emphasis has largely been on data acquisition and model creation. However, achieving a comprehensive assessment of machine learning solutions in real-world settings necessitates standardization throughout the entire pipeline. This need is particularly acute in time series forecasting, where diverse settings impede meaningful comparisons between various methods. To bridge this gap, we propose a unified benchmarking framework that exposes the crucial modelling and machine learning decisions involved in developing time series forecasting models. This framework fosters seamless integration of models and datasets, aiding both practitioners and researchers in their development efforts. We benchmark recently proposed models within this framework, demonstrating that carefully implemented deep learning models with minimal effort can rival gradient-boosting decision trees requiring extensive feature engineering and expert knowled
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16430</link><description>&lt;p&gt;
&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20174;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#23398;&#20064;&#65292;&#39318;&#20808;&#25311;&#21512;&#20559;&#22909;&#20998;&#25968;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;RLHF&#30340;&#22788;&#29702;&#36807;&#31243;&#22797;&#26434;&#12289;&#32791;&#26102;&#19988;&#19981;&#31283;&#23450;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#20351;&#29992;&#31163;&#31574;&#30053;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;DPO&#20351;&#29992;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#21644;&#23545;&#25968;&#25439;&#22833;&#65292;&#23548;&#33268;&#22312;&#20559;&#22909;&#25509;&#36817;&#30830;&#23450;&#24615;&#26102;&#24573;&#30053;&#20102;KL&#27491;&#21017;&#21270;&#39033;&#32780;&#36807;&#24230;&#25311;&#21512;&#20559;&#22909;&#25968;&#25454;&#12290;IPO&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26681;&#26597;&#25214;&#30340;&#25104;&#23545;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#26469;&#35299;&#20915;&#24573;&#30053;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#24182;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#26159;IPO&#30340;&#25104;&#23545;&#25439;&#22833;&#20173;&#28982;&#26080;&#27861;&#20351;KL&#27491;&#21017;&#21270;&#29983;&#25928;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20559;&#22909;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#33539;&#24335;&#12290;&#20351;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23545;&#22810;&#26679;&#24615;&#20559;&#31227;&#21644;&#30456;&#20851;&#24615;&#20559;&#31227;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25945;&#23398;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2312.16242</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#33539;&#24335;&#12290;&#20351;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23545;&#22810;&#26679;&#24615;&#20559;&#31227;&#21644;&#30456;&#20851;&#24615;&#20559;&#31227;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25945;&#23398;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23558;&#22823;&#22411;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23567;&#22411;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#38754;&#23545;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#26426;&#21046;&#12290;&#20998;&#24067;&#20559;&#31227;&#26159;&#25351;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#30340;&#28418;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20559;&#31227;&#24773;&#20917;&#19979;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#32780;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20004;&#31181;&#24120;&#35265;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#21363;&#22810;&#26679;&#24615;&#20559;&#31227;&#21644;&#30456;&#20851;&#24615;&#20559;&#31227;&#65292;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#35813;&#35780;&#20272;&#22522;&#20934;&#35206;&#30422;&#20102;&#26469;&#33258;&#31639;&#27861;&#12289;&#25968;&#25454;&#39537;&#21160;&#21644;&#20248;&#21270;&#35270;&#35282;&#30340;30&#22810;&#31181;&#26041;&#27861;&#65292;&#38024;&#23545;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25945;&#23398;&#24615;&#33021;&#36739;&#24046;&#30340;&#26377;&#36259;&#35266;&#23519;&#32467;&#26524;&#65307;&#29305;&#21035;&#26159;&#65292;&#22797;&#26434;&#30340;&#31639;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#23545;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation transfers knowledge from large models into small models, and has recently made remarkable achievements. However, few studies has investigated the mechanism of knowledge distillation against distribution shift. Distribution shift refers to the data distribution drifts between training and testing phases. In this paper, we reconsider the paradigm of knowledge distillation by reformulating the objective function in shift situations. Under the real scenarios, we propose a unified and systematic framework to benchmark knowledge distillation against two general distributional shifts including diversity and correlation shift. The evaluation benchmark covers more than 30 methods from algorithmic, data-driven, and optimization perspectives for five benchmark datasets. Overall, we conduct extensive experiments on the student model. We reveal intriguing observations of poor teaching performance under distribution shifts; in particular, complex algorithms and data augmentati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21464;&#24418;&#38899;&#39057;Transformer&#65292;&#21629;&#21517;&#20026;DATAR&#65292;&#29992;&#20110;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#37329;&#23383;&#22612;Transformer&#39592;&#26550;&#30340;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22270;&#35745;&#31639;&#21487;&#33021;&#36807;&#20110;&#31616;&#21270;&#36755;&#20837;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2312.16228</link><description>&lt;p&gt;
&#21487;&#21464;&#24418;&#38899;&#39057;Transformer&#29992;&#20110;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deformable Audio Transformer for Audio Event Detection. (arXiv:2312.16228v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16228
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#21464;&#24418;&#38899;&#39057;Transformer&#65292;&#21629;&#21517;&#20026;DATAR&#65292;&#29992;&#20110;&#38899;&#39057;&#20107;&#20214;&#26816;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20855;&#26377;&#37329;&#23383;&#22612;Transformer&#39592;&#26550;&#30340;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22270;&#35745;&#31639;&#21487;&#33021;&#36807;&#20110;&#31616;&#21270;&#36755;&#20837;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#29305;&#24449;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#33258;&#25105;&#27880;&#24847;&#35745;&#31639;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#24212;&#29992;&#30340;&#33539;&#22260;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#21644;&#31227;&#21160;&#35774;&#22791;&#25110;&#36793;&#32536;&#35774;&#22791;&#20013;&#12290;&#29616;&#26377;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#21033;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25163;&#24037;&#21046;&#20316;&#30340;&#27169;&#24335;&#26159;&#25968;&#25454;&#26080;&#20851;&#30340;&#65292;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#21487;&#33021;&#20943;&#23569;&#20102;&#30456;&#20851;&#30340;&#38190;&#25110;&#20540;&#65292;&#32780;&#37325;&#35201;&#24615;&#36739;&#20302;&#30340;&#38190;&#25110;&#20540;&#20173;&#28982;&#20445;&#30041;&#12290;&#22522;&#20110;&#36825;&#20010;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#38899;&#39057;&#35782;&#21035;&#30340;&#21487;&#21464;&#24418;&#38899;&#39057;Transformer&#65292;&#21629;&#21517;&#20026;DATAR&#65292;&#20854;&#20013;&#26500;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#37329;&#23383;&#22612;Transformer&#39592;&#26550;&#30340;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#65292;&#24182;&#19988;&#21487;&#23398;&#20064;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20363;&#22914;&#20107;&#20214;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#22270;&#35745;&#31639;&#21487;&#33021;&#36807;&#20110;&#31616;&#21270;&#36755;&#20837;&#29305;&#24449;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved promising results on a variety of tasks. However, the quadratic complexity in self-attention computation has limited the applications, especially in low-resource settings and mobile or edge devices. Existing works have proposed to exploit hand-crafted attention patterns to reduce computation complexity. However, such hand-crafted patterns are data-agnostic and may not be optimal. Hence, it is likely that relevant keys or values are being reduced, while less important ones are still preserved. Based on this key insight, we propose a novel deformable audio Transformer for audio recognition, named DATAR, where a deformable attention equipping with a pyramid transformer backbone is constructed and learnable. Such an architecture has been proven effective in prediction tasks,~\textit{e.g.}, event classification. Moreover, we identify that the deformable attention map computation may over-simplify the input feature, which can be further enhanced. Hence, we introduc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25353;&#38656;&#20849;&#20056;&#36710;&#36742;&#27966;&#36963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#31639;&#27861;&#20013;&#21482;&#32771;&#34385;&#25910;&#20837;&#26368;&#22823;&#21270;&#32780;&#26080;&#27861;&#28385;&#36275;&#24322;&#24120;&#20998;&#24067;&#35831;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15195</link><description>&lt;p&gt;
&#20114;&#20449;&#24687;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#29992;&#20110;&#25353;&#38656;&#20849;&#20056;&#30340;&#36710;&#36742;&#27966;&#36963;
&lt;/p&gt;
&lt;p&gt;
Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling. (arXiv:2312.15195v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25353;&#38656;&#20849;&#20056;&#36710;&#36742;&#27966;&#36963;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20114;&#20449;&#24687;&#20316;&#20026;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#31639;&#27861;&#20013;&#21482;&#32771;&#34385;&#25910;&#20837;&#26368;&#22823;&#21270;&#32780;&#26080;&#27861;&#28385;&#36275;&#24322;&#24120;&#20998;&#24067;&#35831;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25353;&#38656;&#20849;&#20056;&#26381;&#21153;&#30340;&#20986;&#29616;&#20801;&#35768;&#27599;&#36742;&#36710;&#21516;&#26102;&#20026;&#22810;&#21517;&#20056;&#23458;&#25552;&#20379;&#26381;&#21153;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#21496;&#26426;&#30340;&#25910;&#20837;&#65292;&#24182;&#20351;&#20056;&#23458;&#33021;&#20197;&#36739;&#20302;&#30340;&#20215;&#26684;&#26053;&#34892;&#65292;&#32780;&#19981;&#20687;UberX&#21644;Lyft&#31561;&#20986;&#31199;&#36710;/&#25353;&#38656;&#26381;&#21153;&#21482;&#33021;&#20026;&#19968;&#21517;&#20056;&#23458;&#20998;&#37197;&#19968;&#36742;&#36710;&#12290;&#23613;&#31649;&#25353;&#38656;&#20849;&#20056;&#26381;&#21153;&#21487;&#20197;&#24102;&#26469;&#22914;&#27492;&#22810;&#30340;&#22909;&#22788;&#65292;&#20294;&#20849;&#20056;&#26381;&#21153;&#38656;&#35201;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#21305;&#37197;&#31574;&#30053;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#20026;&#25152;&#26377;&#21508;&#26041;&#65288;&#20056;&#23458;&#65292;&#21496;&#26426;&#65292;&#32858;&#21512;&#20844;&#21496;&#21644;&#29615;&#22659;&#65289;&#25552;&#20379;&#21033;&#30410;&#65292;&#20854;&#20013;&#21306;&#22495;&#35843;&#24230;&#36710;&#36742;&#23545;&#21305;&#37197;&#21644;&#25910;&#20837;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#29616;&#26377;&#31639;&#27861;&#36890;&#24120;&#20165;&#32771;&#34385;&#25910;&#20837;&#26368;&#22823;&#21270;&#65292;&#36825;&#20351;&#24471;&#35831;&#27714;&#20855;&#26377;&#24322;&#24120;&#20998;&#24067;&#30340;&#20056;&#23458;&#38590;&#20197;&#33719;&#24471;&#20056;&#36710;&#12290;&#22914;&#20309;&#22312;&#30830;&#20445;&#21512;&#29702;&#35831;&#27714;&#20998;&#37197;&#30340;&#21516;&#26102;&#22686;&#21152;&#25910;&#20837;&#65292;&#23545;&#20849;&#20056;&#26381;&#21153;&#20844;&#21496;&#65288;&#32858;&#21512;&#20844;&#21496;&#65289;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36710;&#36742;&#27966;&#36963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#20056;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of on-demand ride pooling services allows each vehicle to serve multiple passengers at a time, thus increasing drivers' income and enabling passengers to travel at lower prices than taxi/car on-demand services (only one passenger can be assigned to a car at a time like UberX and Lyft). Although on-demand ride pooling services can bring so many benefits, ride pooling services need a well-defined matching strategy to maximize the benefits for all parties (passengers, drivers, aggregation companies and environment), in which the regional dispatching of vehicles has a significant impact on the matching and revenue. Existing algorithms often only consider revenue maximization, which makes it difficult for requests with unusual distribution to get a ride. How to increase revenue while ensuring a reasonable assignment of requests brings a challenge to ride pooling service companies (aggregation companies). In this paper, we propose a framework for vehicle dispatching for ride po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#24322;&#27493;&#31232;&#30095;&#21270;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;TEASQ-Fed&#65289;&#65292;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#24182;&#34892;&#21442;&#19982;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35774;&#22791;&#25302;&#24930;&#35757;&#32451;&#21644;&#36890;&#20449;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.15186</link><description>&lt;p&gt;
&#39640;&#25928;&#24322;&#27493;&#31232;&#30095;&#21270;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#24322;&#27493;&#31232;&#30095;&#21270;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;TEASQ-Fed&#65289;&#65292;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#30340;&#24182;&#34892;&#21442;&#19982;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35774;&#22791;&#25302;&#24930;&#35757;&#32451;&#21644;&#36890;&#20449;&#29942;&#39048;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20998;&#24067;&#22312;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#29992;&#21442;&#25968;&#26381;&#21153;&#22120;&#21644;&#22823;&#37327;&#36793;&#32536;&#35774;&#22791;&#26469;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#27599;&#36718;&#36873;&#25321;&#20960;&#20010;&#35774;&#22791;&#21442;&#19982;&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#35774;&#22791;&#21487;&#33021;&#20250;&#25302;&#24930;&#35757;&#32451;&#36807;&#31243;&#29978;&#33267;&#23548;&#33268;&#31995;&#32479;&#23849;&#28291;&#65292;&#32780;&#20854;&#20182;&#31354;&#38386;&#35774;&#22791;&#21017;&#38386;&#32622;&#19981;&#29992;&#12290;&#30001;&#20110;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#24102;&#23485;&#30456;&#23545;&#36739;&#20302;&#65292;&#20013;&#38388;&#25968;&#25454;&#30340;&#36890;&#20449;&#25104;&#20026;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#30340;&#26102;&#38388;&#39640;&#25928;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;TEASQ-Fed&#65289;&#65292;&#23427;&#21487;&#20197;&#20805;&#20998;&#21033;&#29992;&#36793;&#32536;&#35774;&#22791;&#20027;&#21160;&#21442;&#19982;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#21033;&#29992;&#25511;&#21046;&#21442;&#25968;&#26469;&#36873;&#25321;&#36866;&#24403;&#25968;&#37327;&#30340;&#24182;&#34892;&#36793;&#32536;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;
While data is distributed in multiple edge devices, Federated Learning (FL) is attracting more and more attention to collaboratively train a machine learning model without transferring raw data. FL generally exploits a parameter server and a large number of edge devices during the whole process of the model training, while several devices are selected in each round. However, straggler devices may slow down the training process or even make the system crash during training. Meanwhile, other idle edge devices remain unused. As the bandwidth between the devices and the server is relatively low, the communication of intermediate data becomes a bottleneck. In this paper, we propose Time-Efficient Asynchronous federated learning with Sparsification and Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to asynchronously participate in the training process by actively applying for tasks. We utilize control parameters to choose an appropriate number of parallel edge device
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;&#27700;&#21360;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24377;&#24615;&#65292;&#38450;&#24481;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#22312;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#26696;&#20214;&#20013;&#25552;&#20379;&#30830;&#20991;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2312.14260</link><description>&lt;p&gt;
&#25552;&#21319;&#38450;&#24481;&#33021;&#21147;&#65306;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;&#27700;&#21360;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20026;&#27169;&#22411;&#30340;&#24377;&#24615;&#25552;&#20379;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience. (arXiv:2312.14260v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#21644;&#27700;&#21360;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#24377;&#24615;&#65292;&#38450;&#24481;&#35268;&#36991;&#25915;&#20987;&#65292;&#24182;&#22312;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#26696;&#20214;&#20013;&#25552;&#20379;&#30830;&#20991;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#38190;&#24212;&#29992;&#20013;&#34987;&#20351;&#29992;&#65292;&#22240;&#27492;&#30830;&#20445;&#20854;&#23436;&#25972;&#24615;&#21644;&#25152;&#26377;&#26435;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#25239;&#35757;&#32451;&#21644;&#27700;&#21360;&#25216;&#26415;&#23384;&#22312;&#20914;&#31361;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#19982;&#27700;&#21360;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#23545;&#35268;&#36991;&#25915;&#20987;&#30340;&#38450;&#24481;&#33021;&#21147;&#65292;&#24182;&#22312;&#30693;&#35782;&#20135;&#26435;&#30423;&#31363;&#26696;&#20214;&#20013;&#25552;&#20379;&#30830;&#20991;&#30340;&#27169;&#22411;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#23545;&#25239;&#27700;&#21360;&#26469;&#35757;&#32451;&#19968;&#20010;&#24378;&#22823;&#30340;&#27700;&#21360;&#27169;&#22411;&#12290;&#20851;&#38190;&#30340;&#26041;&#27861;&#26159;&#22312;&#29983;&#25104;&#23545;&#25239;&#27700;&#21360;&#26102;&#20351;&#29992;&#26356;&#39640;&#30340;&#25200;&#21160;&#39044;&#31639;&#65292;&#19982;&#29992;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#39044;&#31639;&#36991;&#20813;&#20914;&#31361;&#12290;&#25105;&#20204;&#20351;&#29992;MNIST&#21644;Fashion-MNIST&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#23545;&#21508;&#31181;&#27169;&#22411;&#31363;&#21462;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#22312;&#40065;&#26834;&#24615;&#24615;&#33021;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36825;&#31181;&#38450;&#24481;&#30340;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#29305;&#24449;&#24182;&#22312;&#22810;&#20010;&#23567;&#22411;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#26399;&#26395;&#20540;&#38598;&#21512;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#21487;&#33021;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#21333;&#19968;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2312.13650</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#20998;&#21106;&#29305;&#24449;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Quantum Neural Networks via Partitioned Features Encoding. (arXiv:2312.13650v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#24067;&#24335;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#29305;&#24449;&#24182;&#22312;&#22810;&#20010;&#23567;&#22411;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#19978;&#36827;&#34892;&#26399;&#26395;&#20540;&#38598;&#21512;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#35813;&#26041;&#27861;&#21487;&#33021;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;&#21333;&#19968;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#34987;&#35748;&#20026;&#26159;&#36817;&#26399;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#24212;&#29992;&#65292;&#20294;&#26159;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#26799;&#24230;&#28040;&#22833;&#20197;&#21450;&#21463;&#38480;&#21046;&#30340;Qubit&#25968;&#37327;&#21644;&#27973;&#23618;&#30005;&#36335;&#23548;&#33268;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#23567;&#30005;&#36335;&#36924;&#36817;&#22823;&#30005;&#36335;&#30340;&#36755;&#20986;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36924;&#36817;&#22823;&#30005;&#36335;&#38656;&#35201;&#25351;&#25968;&#32423;&#30340;&#23567;&#30005;&#36335;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#20998;&#21106;&#29305;&#24449;&#20998;&#24067;&#22312;&#22810;&#20010;&#23567;&#22411;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#19978;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#30340;&#26399;&#26395;&#20540;&#38598;&#21512;&#26469;&#29983;&#25104;&#39044;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;Semeion&#21644;MNIST&#25163;&#20889;&#25968;&#23383;&#25968;&#25454;&#38598;&#30340;&#21313;&#31867;&#20998;&#31867;&#23454;&#39564;&#12290;Semeion&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#22312;&#20998;&#31867;&#20013;&#21487;&#33021;&#20248;&#20110;&#21333;&#19968;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum neural networks are expected to be a promising application in near-term quantum computing, but face challenges such as vanishing gradients during optimization and limited expressibility by a limited number of qubits and shallow circuits. To mitigate these challenges, an approach using distributed quantum neural networks has been proposed to make a prediction by approximating outputs of a large circuit using multiple small circuits. However, the approximation of a large circuit requires an exponential number of small circuit evaluations. Here, we instead propose to distribute partitioned features over multiple small quantum neural networks and use the ensemble of their expectation values to generate predictions. To verify our distributed approach, we demonstrate ten class classification of the Semeion and MNIST handwritten digit datasets. The results of the Semeion dataset imply that while our distributed approach may outperform a single quantum neural network in classification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#37492;&#21035;&#22120;&#31867;&#21035;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2312.13314</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#39592;&#24178;&#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#37492;&#21035;&#22120;&#31867;&#21035;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20132;&#21449;&#27880;&#24847;&#21147;&#25216;&#26415;&#65292;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#26465;&#20214;&#24615;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#35821;&#20041;&#26631;&#31614;&#22270;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#25511;&#21046;&#29983;&#25104;&#22270;&#20687;&#30340;&#20869;&#23481;&#21644;&#31354;&#38388;&#24067;&#23616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GAN&#37492;&#21035;&#22120;&#31867;&#21035;&#65292;&#29992;&#20110;&#35821;&#20041;&#22270;&#20687;&#21512;&#25104;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#24449;&#39592;&#24178;&#32593;&#32476;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#22120;&#26550;&#26500;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23558;&#22122;&#22768;&#27880;&#20837;&#28508;&#21464;&#37327;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic image synthesis, i.e., generating images from user-provided semantic label maps, is an important conditional image generation task as it allows to control both the content as well as the spatial layout of generated images. Although diffusion models have pushed the state of the art in generative image modeling, the iterative nature of their inference process makes them computationally demanding. Other approaches such as GANs are more efficient as they only need a single feed-forward pass for generation, but the image quality tends to suffer on large and diverse datasets. In this work, we propose a new class of GAN discriminators for semantic image synthesis that generates highly realistic images by exploiting feature backbone networks pre-trained for tasks such as image classification. We also introduce a new generator architecture with better context modeling and using cross-attention to inject noise into latent variables, leading to more diverse generated images. Our model, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;</title><link>http://arxiv.org/abs/2312.11973</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;: &#38754;&#21521;&#35270;&#39057;&#34920;&#31034;&#30340;&#20813;&#36951;&#24536;&#20248;&#32988;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#23376;&#32593;&#32476;&#21644;FSO&#36827;&#34892;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#21644;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65292;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#21644;&#26377;&#25928;&#30340;&#26435;&#37325;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;"&#24425;&#31080;&#31080;&#25454;&#20551;&#35774;"&#65288;LTH&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#20551;&#35774;&#24378;&#35843;&#22312;&#36739;&#22823;&#30340;&#23494;&#38598;&#32593;&#32476;&#20013;&#23384;&#22312;&#39640;&#25928;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;&#22312;&#36866;&#24403;&#30340;&#31232;&#30095;&#26465;&#20214;&#19979;&#34920;&#29616;&#20248;&#31168;&#30340;&#20248;&#32988;&#23376;&#32593;&#32476;&#65288;WSN&#65289;&#22312;&#21508;&#31181;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#23427;&#21033;&#29992;&#26469;&#33258;&#23494;&#38598;&#32593;&#32476;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#26435;&#37325;&#65292;&#22312;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#65288;TIL&#65289;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#12290;&#22312;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;FSCIL&#65289;&#20013;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;&#36719;&#23376;&#32593;&#32476;&#65288;SoftNet&#65289;&#30340;WSN&#21464;&#20307;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#26679;&#26412;&#31232;&#32570;&#26102;&#30340;&#36807;&#25311;&#21512;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20102;WSN&#26435;&#37325;&#30340;&#31232;&#30095;&#37325;&#29992;&#65292;&#29992;&#20110;&#35270;&#39057;&#22686;&#37327;&#23398;&#20064;&#65288;VIL&#65289;&#12290;&#32771;&#34385;&#20102;&#22312;WSN&#20013;&#20351;&#29992;&#20613;&#31435;&#21494;&#23376;&#31070;&#32463;&#36816;&#31639;&#22120;&#65288;FSO&#65289;&#65292;&#23427;&#33021;&#22815;&#23545;&#35270;&#39057;&#36827;&#34892;&#32039;&#20945;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#24102;&#23485;&#19979;&#35782;&#21035;&#21487;&#37325;&#29992;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#23558;FSO&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#26550;&#26500;&#20013;&#65292;&#21253;&#25324;VIL&#12289;TIL&#21644;FSCIL&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the existence of efficient subnetworks within larger, dense networks, a high-performing Winning Subnetwork (WSN) in terms of task performance under appropriate sparsity conditions is considered for various continual learning tasks. It leverages pre-existing weights from dense networks to achieve efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is designed to prevent overfitting when the data samples are scarce. Furthermore, the sparse reuse of WSN weights is considered for Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It enables compact encoding of videos and identifies reusable subnetworks across varying bandwidths. We have integrated FSO into different architectural frameworks for continual learning, including VIL, TIL, and FSCIL. Our c
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.11509</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33647;&#29289;&#35843;&#25972;&#31995;&#32479;&#20197;&#20943;&#23569;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20026;&#24739;&#26377;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#34394;&#25311;&#24739;&#32773;&#24320;&#20855;&#33647;&#29289;&#22788;&#26041;&#65292;&#24182;&#26681;&#25454;&#38646;&#25104;&#26412;&#39057;&#32321;&#27979;&#37327;&#32467;&#26524;&#65292;&#35843;&#25972;&#33647;&#29289;&#21644;&#21058;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#35780;&#20272;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#33391;&#22909;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#20851;&#20110;&#33647;&#29289;&#27835;&#30103;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#25928;&#26524;&#30340;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#20449;&#30340;&#24739;&#32773;&#27169;&#25311;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#23545;&#21487;&#33021;&#23384;&#22312;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20154;&#32676;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#27880;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;:
&lt;/p&gt;
&lt;p&gt;
We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
&lt;/p&gt;</description></item><item><title>SAME&#26159;&#19968;&#31181;&#38450;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#26679;&#26412;&#37325;&#24314;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35775;&#38382;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23454;&#29992;&#30340;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2312.10578</link><description>&lt;p&gt;
SAME: &#38450;&#33539;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#26679;&#26412;&#37325;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10578
&lt;/p&gt;
&lt;p&gt;
SAME&#26159;&#19968;&#31181;&#38450;&#24481;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#26032;&#26041;&#27861;&#65292;&#22522;&#20110;&#26679;&#26412;&#37325;&#24314;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35775;&#38382;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#23454;&#29992;&#30340;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#21644;&#20808;&#36827;&#30340;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#65288;MLaaS&#65289;&#24212;&#36816;&#32780;&#29983;&#65292;&#38477;&#20302;&#20102;&#29992;&#25143;&#21457;&#24067;&#25110;&#20135;&#21697;&#21270;&#20182;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#38376;&#27099;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#19982;MLaaS&#30456;&#20851;&#30340;&#28508;&#22312;&#38544;&#31169;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#23041;&#32961;&#26159;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#38450;&#24481;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#37117;&#23384;&#22312;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#20351;&#23427;&#20204;&#23545;&#21487;&#38752;&#30340;&#20445;&#25252;&#19981;&#22815;&#23454;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#38480;&#21046;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#37325;&#24314;&#27010;&#24565;&#30340;&#26032;&#22411;&#38450;&#24481;&#26426;&#21046;SAME&#12290;&#35813;&#31574;&#30053;&#23545;&#38450;&#24481;&#32773;&#30340;&#33021;&#21147;&#35201;&#27714;&#26368;&#23567;&#65292;&#28040;&#38500;&#20102;&#23545;&#36741;&#21161;&#30340;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#12289;&#29992;&#25143;&#26597;&#35810;&#21382;&#21490;&#12289;&#30333;&#30418;&#27169;&#22411;&#35775;&#38382;&#21644;&#39069;&#22806;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have shown significant performance across various domains, their deployment needs extensive resources and advanced computing infrastructure. As a solution, Machine Learning as a Service (MLaaS) has emerged, lowering the barriers for users to release or productize their deep learning models. However, previous studies have highlighted potential privacy and security concerns associated with MLaaS, and one primary threat is model extraction attacks. To address this, there are many defense solutions but they suffer from unrealistic assumptions and generalization issues, making them less practical for reliable protection. Driven by these limitations, we introduce a novel defense mechanism, SAME, based on the concept of sample reconstruction. This strategy imposes minimal prerequisites on the defender's capabilities, eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user query history, white-box model access, and additional intervention during m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#36807;&#31243;&#23545;&#39640;&#21487;&#38752;&#24615;&#27494;&#22120;&#31995;&#32479;&#30340;&#25925;&#38556;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#26631;&#20934;&#20998;&#31867;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LaplaceNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10494</link><description>&lt;p&gt;
&#26159;&#21542;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#20102;&#27494;&#22120;&#31995;&#32479;&#39044;&#27979;&#24615;&#32500;&#25252;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Bayesian Neural Networks Improve Weapon System Predictive Maintenance?. (arXiv:2312.10494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#36807;&#31243;&#23545;&#39640;&#21487;&#38752;&#24615;&#27494;&#22120;&#31995;&#32479;&#30340;&#25925;&#38556;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#25913;&#21892;&#39044;&#27979;&#24615;&#32500;&#25252;&#12290;&#36890;&#36807;&#20998;&#26512;&#21644;&#26631;&#20934;&#20998;&#31867;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;LaplaceNN&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36807;&#31243;&#26469;&#24314;&#31435;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23545;&#39640;&#21487;&#38752;&#24615;&#27494;&#22120;&#31995;&#32479;&#30340;&#25925;&#38556;&#26102;&#38388;&#36827;&#34892;&#24314;&#27169;&#65292;&#35813;&#25968;&#25454;&#20855;&#26377;&#21306;&#38388;&#25130;&#26029;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#21327;&#21464;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#20998;&#31867;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#12289;&#31934;&#30830;&#24230;-&#21484;&#22238;&#29575;&#26354;&#32447;&#65288;PR&#65289;&#12289;&#21487;&#38752;&#24615;&#26354;&#32447;&#21487;&#35270;&#21270;&#65289;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;LaplaceNN&#36827;&#34892;&#20998;&#26512;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
We implement a Bayesian inference process for Neural Networks to model the time to failure of highly reliable weapon systems with interval-censored data and time-varying covariates. We analyze and benchmark our approach, LaplaceNN, on synthetic and real datasets with standard classification metrics such as Receiver Operating Characteristic (ROC) Area Under Curve (AUC) Precision-Recall (PR) AUC, and reliability curve visualizations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#32974;&#20799;&#22810;&#26222;&#21202;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#21363;&#26102;&#25552;&#20379;&#21453;&#39304;&#21644;&#32416;&#27491;&#25968;&#25454;&#28304;&#12290;</title><link>http://arxiv.org/abs/2312.09433</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#32974;&#20799;&#22810;&#26222;&#21202;&#36229;&#22768;&#22312;&#29616;&#22330;&#23454;&#26102;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Point-of-Care Real-Time Signal Quality for Fetal Doppler Ultrasound Using a Deep Learning Approach. (arXiv:2312.09433v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09433
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#32974;&#20799;&#22810;&#26222;&#21202;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#23454;&#26102;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#21363;&#26102;&#25552;&#20379;&#21453;&#39304;&#21644;&#32416;&#27491;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#19982;&#25105;&#20204;&#20043;&#21069;&#24320;&#21457;&#30340;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22823;&#35268;&#27169;1D&#32974;&#20799;&#22810;&#26222;&#21202;&#25968;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#36136;&#37327;&#12290;&#36825;&#20010;&#31995;&#32479;&#19987;&#20026;&#20302;&#36164;&#28304;&#31038;&#21306;&#30340;&#20256;&#32479;&#22303;&#33879;&#21161;&#20135;&#22763;&#35774;&#35745;&#65292;&#21033;&#29992;&#19968;&#27454;&#20215;&#26684;&#23454;&#24800;&#30340;&#23433;&#21331;&#25163;&#26426;&#25913;&#21892;&#35760;&#24405;&#20449;&#21495;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;&#22810;&#26222;&#21202;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#22312;&#22922;&#23072;&#26399;&#38388;&#35782;&#21035;&#32974;&#20799;&#29983;&#38271;&#21463;&#38480;&#12289;&#39640;&#34880;&#21387;&#21644;&#20854;&#20182;&#30456;&#20851;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20449;&#21495;&#30340;&#36136;&#37327;&#21462;&#20915;&#20110;&#35768;&#22810;&#22240;&#32032;&#65292;&#21253;&#25324;&#23556;&#39057;&#24178;&#25200;&#12289;&#32974;&#20799;&#20301;&#32622;&#12289;&#23381;&#22919;&#20307;&#22411;&#21644;&#21161;&#20135;&#22763;&#20351;&#29992;&#22810;&#26222;&#21202;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#33021;&#22815;&#21363;&#26102;&#25552;&#20379;&#21453;&#39304;&#65292;&#32416;&#27491;&#25968;&#25454;&#28304;&#65292;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22312;&#25163;&#26426;&#19978;&#23454;&#26102;&#36816;&#34892;&#30340;&#20449;&#21495;&#36136;&#37327;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;191&#20010;DUS&#20449;&#21495;&#65292;&#20027;&#35201;&#25345;&#32493;&#26102;&#38388;&#22312;5&#21040;10&#20998;&#38047;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a deep learning framework designed to integrate with our previously developed system that facilitates large-scale 1D fetal Doppler data collection, aiming to enhance data quality. This system, tailored for traditional Indigenous midwives in low-resource communities, leverages a cost-effective Android phone to improve the quality of recorded signals. We have shown that the Doppler data can be used to identify fetal growth restriction, hypertension, and other concerning issues during pregnancy. However, the quality of the signal is dependent on many factors, including radio frequency interference, position of the fetus, maternal body habitus, and usage of the Doppler by the birth attendants. In order to provide instant feedback to allow correction of the data at source, a signal quality metric is required that can run in real-time on the mobile phone.  In this study, 191 DUS signals with durations mainly in the range between 5 to 10 minutes were evaluated for qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#21253;&#25324;&#38381;&#28304;&#27169;&#22411;&#12289;&#25968;&#25454;&#27844;&#28431;&#21644;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#38024;&#23545;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20154;&#21592;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#21830;&#30340;&#25351;&#21335;&#26469;&#20943;&#36731;&#36825;&#20123;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2312.08055</link><description>&lt;p&gt;
&#25171;&#30772;&#27785;&#40664;&#65306;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#20351;&#29992;LLMs&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Breaking the Silence: the Threats of Using LLMs in Software Engineering. (arXiv:2312.08055v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#36719;&#20214;&#24037;&#31243;&#20013;&#20351;&#29992;LLMs&#30340;&#28508;&#22312;&#23041;&#32961;&#65292;&#21253;&#25324;&#38381;&#28304;&#27169;&#22411;&#12289;&#25968;&#25454;&#27844;&#28431;&#21644;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#38024;&#23545;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20154;&#21592;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#21830;&#30340;&#25351;&#21335;&#26469;&#20943;&#36731;&#36825;&#20123;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#24433;&#21709;&#30528;&#20195;&#30721;&#34917;&#20840;&#12289;&#27979;&#35797;&#29983;&#25104;&#12289;&#31243;&#24207;&#20462;&#22797;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#20855;&#26377;&#24456;&#22823;&#28508;&#21147;&#65292;&#20294;&#30740;&#31350;&#32773;&#20204;&#22312;&#28041;&#21450;LLMs&#30340;&#23454;&#39564;&#20013;&#20173;&#38656;&#35880;&#24910;&#65292;&#22240;&#20026;&#35768;&#22810;&#22797;&#26434;&#22240;&#32032;&#21487;&#33021;&#20250;&#24433;&#21709;&#23454;&#39564;&#32467;&#26524;&#12290;&#26412;&#25991;&#23545;LLM&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#28508;&#22312;&#23041;&#32961;&#36827;&#34892;&#20102;&#24320;&#25918;&#35752;&#35770;&#65292;&#21253;&#25324;&#38381;&#28304;&#27169;&#22411;&#12289;LLM&#35757;&#32451;&#25968;&#25454;&#19982;&#30740;&#31350;&#35780;&#20272;&#20043;&#38388;&#30340;&#21487;&#33021;&#25968;&#25454;&#27844;&#28431;&#20197;&#21450;LLM&#30456;&#20851;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#37325;&#22797;&#24615;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#22871;&#38024;&#23545;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20154;&#21592;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#21830;&#30340;&#25351;&#21335;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#25285;&#24551;&#12290;&#25351;&#21335;&#30340;&#24433;&#21709;&#36890;&#36807;&#29616;&#26377;&#30340;LLM&#25552;&#20379;&#21830;&#30340;&#33391;&#22909;&#23454;&#36341;&#21644;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#20154;&#21592;&#22312;&#27979;&#35797;&#29992;&#20363;&#29983;&#25104;&#26041;&#38754;&#30340;&#23454;&#38469;&#31034;&#20363;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have gained considerable traction within the Software Engineering (SE) community, impacting various SE tasks from code completion to test generation, from program repair to code summarization. Despite their promise, researchers must still be careful as numerous intricate factors can influence the outcomes of experiments involving LLMs. This paper initiates an open discussion on potential threats to the validity of LLM-based research including issues such as closed-source models, possible data leakage between LLM training data and research evaluation, and the reproducibility of LLM-based findings. In response, this paper proposes a set of guidelines tailored for SE researchers and Language Model (LM) providers to mitigate these concerns. The implications of the guidelines are illustrated using existing good practices followed by LLM providers and a practical example for SE researchers in the context of test case generation.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#40065;&#26834;&#30340;&#39640;&#26031;&#36807;&#31243;&#22343;&#21248;&#35823;&#24046;&#30028;&#38480;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#22312;&#32447;&#20248;&#21270;&#20013;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.07281</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Safe Multi-Task Bayesian Optimization. (arXiv:2312.07281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#40065;&#26834;&#30340;&#39640;&#26031;&#36807;&#31243;&#22343;&#21248;&#35823;&#24046;&#30028;&#38480;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#22312;&#32447;&#20248;&#21270;&#20013;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#25104;&#20026;&#23433;&#20840;&#22312;&#32447;&#31995;&#32479;&#20248;&#21270;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20854;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#22122;&#22768;&#20581;&#22766;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#24555;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#20943;&#23569;&#30340;&#29289;&#29702;&#27169;&#22411;&#32435;&#20837;&#20248;&#21270;&#36807;&#31243;&#20013;&#20197;&#21152;&#36895;&#36807;&#31243;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#23545;&#23454;&#38469;&#31995;&#32479;&#30340;&#36817;&#20284;&#65292;&#24182;&#19988;&#20174;&#20013;&#36827;&#34892;&#37319;&#26679;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#27169;&#22411;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30001;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#34920;&#31034;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23398;&#20064;&#12290;&#23433;&#20840;&#24615;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#22312;&#32447;&#20248;&#21270;&#26041;&#27861;&#30340;&#37325;&#35201;&#26631;&#20934;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#24050;&#32463;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#24182;&#22312;&#24050;&#30693;&#36229;&#21442;&#25968;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#36825;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#40065;&#26834;&#39640;&#26031;&#36807;&#31243;&#22343;&#21248;&#35823;&#24046;&#30028;&#38480;&#65292;&#20197;&#28385;&#36275;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#28041;&#21450;&#20174;&#36229;&#21442;&#25968;&#21518;&#39564;&#20998;&#24067;&#35745;&#31639;&#32622;&#20449;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has become a powerful tool for safe online optimization of systems, due to its high sample efficiency and noise robustness. For further speed-up reduced physical models of the system can be incorporated into the optimization to accelerate the process, since the models are able to offer an approximation of the actual system, and sampling from them is significantly cheaper. The similarity between model and reality is represented by additional hyperparameters and learned within the optimization process. Safety is an important criteria for online optimization methods like Bayesian optimization, which has been addressed by recent literature, which provide safety guarantees under the assumption of known hyperparameters. However, in practice this is not applicable. Therefore, we extend the robust Gaussian process uniform error bounds to meet the multi-task setting, which involves the calculation of a confidence region from the hyperparameter posterior distribution utiliz
&lt;/p&gt;</description></item><item><title>MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2312.06786</link><description>&lt;p&gt;
&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06786
&lt;/p&gt;
&lt;p&gt;
MoLE&#26159;&#19968;&#31181;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#22810;&#20010;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;&#21644;&#19968;&#20010;&#36335;&#30001;&#27169;&#22411;&#65292;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#20102;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;(LTSF)&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#36807;&#21435;&#20540;&#30340;&#26102;&#38388;&#24207;&#21015;&#30340;&#26410;&#26469;&#20540;&#12290;&#24403;&#21069;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;(SOTA)&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#30001;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20855;&#26377;&#32447;&#24615;&#26144;&#23556;&#23618;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#31616;&#21333;&#24615;&#65292;&#23427;&#20204;&#19981;&#33021;&#22815;&#36866;&#24212;&#26102;&#38388;&#24207;&#21015;&#27169;&#24335;&#30340;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#39118;&#26684;&#30340;&#22686;&#24378;&#32447;&#24615;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#32447;&#24615;&#19987;&#23478;(MoLE)&#12290;MoLE&#19981;&#26159;&#35757;&#32451;&#21333;&#20010;&#27169;&#22411;&#65292;&#32780;&#26159;&#35757;&#32451;&#22810;&#20010;&#20197;&#32447;&#24615;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;(&#21363;&#19987;&#23478;)&#21644;&#19968;&#20010;&#26435;&#34913;&#21644;&#28151;&#21512;&#20854;&#36755;&#20986;&#30340;&#36335;&#30001;&#27169;&#22411;&#12290;&#34429;&#28982;&#25972;&#20010;&#26694;&#26550;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#20294;&#27599;&#20010;&#19987;&#23478;&#37117;&#23398;&#20250;&#19987;&#38376;&#22788;&#29702;&#29305;&#23450;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#32780;&#36335;&#30001;&#27169;&#22411;&#21017;&#23398;&#20250;&#33258;&#36866;&#24212;&#22320;&#32452;&#21512;&#19987;&#23478;&#20204;&#30340;&#36755;&#20986;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MoLE&#38477;&#20302;&#20102;&#32447;&#24615;&#20013;&#24515;&#27169;&#22411;(DLinear&#65292;RLinear&#21644;RMLP)&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term time series forecasting (LTSF) aims to predict future values of a time series given the past values. The current state-of-the-art (SOTA) on this problem is attained in some cases by linear-centric models, which primarily feature a linear mapping layer. However, due to their inherent simplicity, they are not able to adapt their prediction rules to periodic changes in time series patterns. To address this challenge, we propose a Mixture-of-Experts-style augmentation for linear-centric models and propose Mixture-of-Linear-Experts (MoLE). Instead of training a single model, MoLE trains multiple linear-centric models (i.e., experts) and a router model that weighs and mixes their outputs. While the entire framework is trained end-to-end, each expert learns to specialize in a specific temporal pattern, and the router model learns to compose the experts adaptively. Experiments show that MoLE reduces forecasting error of linear-centric models, including DLinear, RLinear, and RMLP, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CPDM&#65289;&#26469;&#35299;&#20915;Continual Learning&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;CPDM&#36890;&#36807;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#20943;&#23569;&#20102;&#20998;&#31867;&#22120;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2312.06710</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#22238;&#25918;&#30340;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;Continual Learning
&lt;/p&gt;
&lt;p&gt;
Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CPDM&#65289;&#26469;&#35299;&#20915;Continual Learning&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;CPDM&#36890;&#36807;&#25552;&#39640;&#29983;&#25104;&#22120;&#30340;&#22270;&#20687;&#36136;&#37327;&#65292;&#20943;&#23569;&#20102;&#20998;&#31867;&#22120;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;Continual Learning&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38590;&#39064;&#12290;&#28145;&#24230;&#29983;&#25104;&#24335;&#22238;&#25918;&#65288;GR&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#20808;&#21069;&#20219;&#21153;&#20013;&#29983;&#25104;&#26679;&#26412;&#26469;&#22686;&#24378;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#20174;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21457;&#23637;&#21040;&#20102;&#26368;&#26032;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#12290;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#25968;&#25454;&#36136;&#37327;&#19982;&#21407;&#22987;&#25968;&#25454;&#30456;&#27604;&#20250;&#36880;&#28176;&#19979;&#38477;&#65292;&#22240;&#20026;&#29983;&#25104;&#22120;&#19981;&#26029;&#20174;&#20854;&#36755;&#20986;&#20013;&#36827;&#34892;&#33258;&#25105;&#23398;&#20064;&#12290;&#36825;&#31181;&#36864;&#21270;&#21487;&#33021;&#23548;&#33268;&#20998;&#31867;&#22120;&#20013;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#21407;&#22411;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65288;CPDM&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#24335;&#22238;&#25918;&#30340;Continual Learning&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25552;&#39640;&#29983;&#25104;&#22120;&#20013;&#30340;&#22270;&#20687;&#36136;&#37327;&#20174;&#32780;&#20943;&#23569;&#20102;&#20998;&#31867;&#22120;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;CPDM&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#31867;&#21407;&#22411;&#65292;&#23427;&#25429;&#25417;&#20102;&#32473;&#23450;&#31867;&#21035;&#20013;&#22270;&#20687;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mitigating catastrophic forgetting is a key hurdle in continual learning. Deep Generative Replay (GR) provides techniques focused on generating samples from prior tasks to enhance the model's memory capabilities. With the progression in generative AI, generative models have advanced from Generative Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major issue is the deterioration in the quality of generated data compared to the original, as the generator continuously self-learns from its outputs. This degradation can lead to the potential risk of catastrophic forgetting occurring in the classifier. To address this, we propose the Class-Prototype Conditional Diffusion Model (CPDM), a GR-based approach for continual learning that enhances image quality in generators and thus reduces catastrophic forgetting in classifiers. The cornerstone of CPDM is a learnable class-prototype that captures the core characteristics of images in a given class. This prototype, integra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20083;&#33146;&#30284;HER2&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26631;&#31614;&#20998;&#24067;&#31574;&#30053;&#21644;&#36741;&#21161;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#21644;&#21033;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.06454</link><description>&lt;p&gt;
&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#27927;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20083;&#33146;&#30284;HER2&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images. (arXiv:2312.06454v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20083;&#33146;&#30284;HER2&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#26631;&#31614;&#20998;&#24067;&#31574;&#30053;&#21644;&#36741;&#21161;&#20998;&#31867;&#22120;&#65292;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#21644;&#21033;&#29992;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#25509;&#20174;&#24191;&#27867;&#21487;&#24471;&#30340;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#20154;&#31867;&#34920;&#30382;&#29983;&#38271;&#22240;&#23376;&#21463;&#20307;2&#65288;HER2&#65289;&#29366;&#24577;&#21487;&#20197;&#38477;&#20302;&#25216;&#26415;&#25104;&#26412;&#65292;&#21152;&#24555;&#27835;&#30103;&#36873;&#25321;&#36895;&#24230;&#12290;&#20934;&#30830;&#39044;&#27979;HER2&#38656;&#35201;&#22823;&#37327;&#30340;&#22810;&#22320;&#28857;&#20840;&#20999;&#29255;&#22270;&#20687;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#22312;&#19981;&#20256;&#36755;&#21315;&#20806;&#23383;&#33410;&#22823;&#23567;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#65292;&#21327;&#21516;&#35757;&#32451;&#36825;&#20123;&#20840;&#20999;&#29255;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#22810;&#22320;&#28857;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#26631;&#31614;&#19981;&#24179;&#34913;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#19981;&#33021;&#21516;&#26102;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#31449;&#28857;&#31471;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28857;&#21464;&#25442;&#22120;&#32467;&#21512;&#32852;&#37030;&#23398;&#20064;&#20174;&#21994;&#21644;&#22055;&#21878;&#27861;&#26579;&#33394;&#20840;&#20999;&#29255;&#22270;&#20687;&#20013;&#39044;&#27979;&#22810;&#22320;&#28857;HER2&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#39033;&#26032;&#35774;&#35745;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#26631;&#31614;&#20998;&#24067;&#31574;&#30053;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Directly predicting human epidermal growth factor receptor 2 (HER2) status from widely available hematoxylin and eosin (HE)-stained whole slide images (WSIs) can reduce technical costs and expedite treatment selection. Accurately predicting HER2 requires large collections of multi-site WSIs. Federated learning enables collaborative training of these WSIs without gigabyte-size WSIs transportation and data privacy concerns. However, federated learning encounters challenges in addressing label imbalance in multi-site WSIs from the real world. Moreover, existing WSI classification methods cannot simultaneously exploit local context information and long-range dependencies in the site-end feature representation of federated learning. To address these issues, we present a point transformer with federated learning for multi-site HER2 status prediction from HE-stained WSIs. Our approach incorporates two novel designs. We propose a dynamic label distribution strategy and an auxiliary classifier,
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#24341;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2312.05910</link><description>&lt;p&gt;
&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#19982;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#38750;&#22343;&#22330;&#21644;&#22312;&#32447;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05910
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#24341;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#26377;&#25928;&#22320;&#21033;&#29992;&#20102;&#28508;&#22312;&#29366;&#24577;&#21644;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSMs&#65289;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#21407;&#21017;&#24615;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GPSSMs&#21464;&#20998;&#23398;&#20064;&#21644;&#25512;&#29702;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20248;&#21270;&#22823;&#37327;&#21464;&#20998;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#25928;&#29575;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#38598;&#21512;&#21345;&#23572;&#26364;&#28388;&#27874;&#65288;EnKF&#65289;&#65292;&#19968;&#31181;&#25104;&#29087;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#28388;&#27874;&#25216;&#26415;&#65292;&#32435;&#20837;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#20013;&#65292;&#20197;&#36817;&#20284;&#28508;&#22312;&#29366;&#24577;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#36825;&#31181;&#21033;&#29992;EnKF&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#28508;&#22312;&#29366;&#24577;&#21644;GP&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#23545;&#21464;&#20998;&#20998;&#24067;&#36827;&#34892;&#21442;&#25968;&#21270;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#21464;&#20998;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#23545;&#22810;&#20010;&#39033;&#36827;&#34892;&#27714;&#21644;&#26469;&#30452;&#25509;&#35780;&#20272;&#21464;&#20998;&#25512;&#29702;&#20013;&#30340;&#36817;&#20284;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian process state-space models (GPSSMs) are a versatile and principled family of nonlinear dynamical system models. However, existing variational learning and inference methods for GPSSMs often necessitate optimizing a substantial number of variational parameters, leading to inadequate performance and efficiency. To overcome this issue, we propose incorporating the ensemble Kalman filter (EnKF), a well-established model-based filtering technique, into the variational inference framework to approximate the posterior distribution of latent states. This utilization of EnKF can effectively exploit the dependencies between latent states and GP dynamics, while eliminating the need for parameterizing the variational distribution, thereby significantly reducing the number of variational parameters. Moreover, we show that our proposed algorithm allows straightforward evaluation of an approximated evidence lower bound (ELBO) in variational inference via simply summating multiple terms with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;MobileNetV2&#27169;&#22411;&#65292;&#20351;&#29992;1576&#24352;&#36229;&#22768;&#22270;&#20687;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;&#20083;&#33146;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#22312;&#27491;&#24120;&#12289;&#33391;&#24615;&#12289;&#24694;&#24615;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;ROC-AUC&#12289;PR-AUC&#21644;MCC&#20998;&#21035;&#36798;&#21040;0.82&#12289;0.83&#12289;0.81&#12289;0.94&#12289;0.88&#21644;0.74&#12290;&#30740;&#31350;&#36824;&#25913;&#36827;&#20102;&#22270;&#20687;&#24378;&#24230;&#20998;&#24067;&#21644;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;MobileNetV2&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#32959;&#30244;&#35786;&#26029;&#30340;&#31934;&#24230;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;&#21478;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#22522;&#20110;Streamlit&#30340;&#23454;&#26102;&#32959;&#30244;&#20998;&#31867;&#37096;&#32626;&#65292;&#20026;&#26410;&#26469;&#30340;&#30284;&#30151;&#30740;&#31350;&#35774;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2312.03020</link><description>&lt;p&gt;
&#20351;&#29992;MobileNetV2&#22686;&#24378;&#30340;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#31867;&#65306;&#23545;&#22270;&#20687;&#24378;&#24230;&#12289;&#38169;&#35823;&#32531;&#35299;&#21644;Streamlit&#39537;&#21160;&#30340;&#23454;&#26102;&#37096;&#32626;&#30340;&#35814;&#32454;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment. (arXiv:2312.03020v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;MobileNetV2&#27169;&#22411;&#65292;&#20351;&#29992;1576&#24352;&#36229;&#22768;&#22270;&#20687;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#23545;&#20083;&#33146;&#30284;&#32959;&#30244;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#27169;&#22411;&#22312;&#27491;&#24120;&#12289;&#33391;&#24615;&#12289;&#24694;&#24615;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;ROC-AUC&#12289;PR-AUC&#21644;MCC&#20998;&#21035;&#36798;&#21040;0.82&#12289;0.83&#12289;0.81&#12289;0.94&#12289;0.88&#21644;0.74&#12290;&#30740;&#31350;&#36824;&#25913;&#36827;&#20102;&#22270;&#20687;&#24378;&#24230;&#20998;&#24067;&#21644;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;MobileNetV2&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#32959;&#30244;&#35786;&#26029;&#30340;&#31934;&#24230;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;&#21478;&#22806;&#65292;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#22522;&#20110;Streamlit&#30340;&#23454;&#26102;&#32959;&#30244;&#20998;&#31867;&#37096;&#32626;&#65292;&#20026;&#26410;&#26469;&#30340;&#30284;&#30151;&#30740;&#31350;&#35774;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Google&#30340;MobileNetV2&#30340;&#22797;&#26434;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#20083;&#33146;&#30284;&#32959;&#30244;&#20998;&#31867;&#20026;&#27491;&#24120;&#12289;&#33391;&#24615;&#21644;&#24694;&#24615;&#19977;&#31867;&#65292;&#21033;&#29992;&#20102;1576&#24352;&#36229;&#22768;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65288;265&#24352;&#27491;&#24120;&#12289;891&#24352;&#33391;&#24615;&#12289;420&#24352;&#24694;&#24615;&#65289;&#12290;&#35813;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#20026;0.82&#65292;&#31934;&#30830;&#29575;&#20026;0.83&#65292;&#21484;&#22238;&#29575;&#20026;0.81&#65292;ROC-AUC&#20026;0.94&#65292;PR-AUC&#20026;0.88&#65292;MCC&#20026;0.74&#12290;&#30740;&#31350;&#36824;&#26816;&#26597;&#20102;&#22270;&#20687;&#24378;&#24230;&#20998;&#24067;&#21644;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24212;&#29992;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;&#38024;&#23545;&#25968;&#25454;&#38598;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#35813;&#30740;&#31350;&#30830;&#20445;&#20102;&#19968;&#20010;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#22467;&#21450;&#24320;&#32599;&#24052;&#24076;&#20122;&#21307;&#38498;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;Walid Al-Dhabyani&#31561;&#20154;&#32534;&#21046;&#65292;&#24378;&#35843;&#20102;MobileNetV2&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#28508;&#21147;&#65292;&#26088;&#22312;&#25552;&#39640;&#32959;&#30244;&#35786;&#26029;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#22522;&#20110;Streamlit&#30340;&#23454;&#26102;&#32959;&#30244;&#20998;&#31867;&#37096;&#32626;&#65292;&#23637;&#31034;&#20102;MobileNetV2&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20026;&#30284;&#30151;&#30740;&#31350;&#35774;&#23450;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research introduces a sophisticated transfer learning model based on Google's MobileNetV2 for breast cancer tumor classification into normal, benign, and malignant categories, utilizing a dataset of 1576 ultrasound images (265 normal, 891 benign, 420 malignant). The model achieves an accuracy of 0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and MCC of 0.74. It examines image intensity distributions and misclassification errors, offering improvements for future applications. Addressing dataset imbalances, the study ensures a generalizable model. This work, using a dataset from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al., emphasizes MobileNetV2's potential in medical imaging, aiming to improve diagnostic precision in oncology. Additionally, the paper explores Streamlit-based deployment for real-time tumor classification, demonstrating MobileNetV2's applicability in medical imaging and setting a benchmark for future research in onco
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#24615;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#26631;&#35760;&#20998;&#21106;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#24494;&#35843;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.02380</link><description>&lt;p&gt;
FaultFormer: &#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#36866;&#24212;&#24615;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification. (arXiv:2312.02380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#24615;&#36724;&#25215;&#25925;&#38556;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#26631;&#35760;&#20998;&#21106;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#33021;&#22815;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;&#24494;&#35843;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#28040;&#36153;&#30340;&#22686;&#38271;&#25512;&#21160;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#26234;&#33021;&#21046;&#36896;&#21644;&#26426;&#22120;&#20581;&#24247;&#30417;&#27979;&#26041;&#38754;&#30340;&#37325;&#35201;&#24212;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25391;&#21160;&#25968;&#25454;&#25552;&#20379;&#20102;&#20016;&#23500;&#21487;&#38752;&#30340;&#20449;&#24687;&#65292;&#33021;&#22815;&#23545;&#26426;&#22120;&#20581;&#24247;&#21644;&#39044;&#27979;&#24615;&#32500;&#25252;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27934;&#23519;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#36724;&#25215;&#25925;&#38556;&#35782;&#21035;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26694;&#26550;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#26631;&#35760;&#20998;&#21106;&#21644;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#25391;&#21160;&#20449;&#21495;&#30340;&#25513;&#30721;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21450;&#20854;&#22312;&#20302;&#25968;&#25454;&#29615;&#22659;&#12289;&#20219;&#21153;&#36866;&#24212;&#21644;&#25968;&#25454;&#38598;&#36866;&#24212;&#20013;&#30340;&#24212;&#29992;&#12290;&#39044;&#35757;&#32451;&#33021;&#22815;&#25552;&#21319;&#22312;&#31232;&#32570;&#26410;&#35265;&#35757;&#32451;&#26679;&#26412;&#19978;&#30340;10&#31867;&#36724;&#25215;&#20998;&#31867;&#24615;&#33021;&#12290;&#24403;&#22312;&#39044;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25925;&#38556;&#31867;&#21035;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;Transformer&#27169;&#22411;&#20063;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of global consumption has motivated important applications of deep learning to smart manufacturing and machine health monitoring. In particular, vibration data offers a rich and reliable source to provide meaningful insights into machine health and predictive maintenance. In this work, we present pretraining and fine-tuning frameworks for identifying bearing faults based on transformer models. In particular, we investigate different tokenization and data augmentation strategies to improve performance and reach state of the art accuracies. Furthermore, we demonstrate masked self-supervised pretraining for vibration signals and its application to low-data regimes, task adaptation, and dataset adaptation. Pretraining is able to improve performance on 10-way bearing classification on scarce, unseen training samples. Transformer models also benefit from pretraining when fine-tuning on fault classes outside of the pretraining distribution. Lastly, pretrained transformers are shown
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;OTTA&#65289;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#26657;&#20934;&#19988;&#20445;&#25252;&#38544;&#31169;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#36830;&#32493;&#33258;&#36866;&#24212;&#27169;&#22411;&#36816;&#34892;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36731;&#37327;&#32423;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;OTTA&#25216;&#26415;&#26469;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2311.18520</link><description>&lt;p&gt;
&#26080;&#38656;&#26657;&#20934;&#30340;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#24212;&#29992;&#20110;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;OTTA&#65289;&#30340;&#27010;&#24565;&#65292;&#26080;&#38656;&#26657;&#20934;&#19988;&#20445;&#25252;&#38544;&#31169;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#36830;&#32493;&#33258;&#36866;&#24212;&#27169;&#22411;&#36816;&#34892;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#36731;&#37327;&#32423;&#26550;&#26500;&#21644;&#19981;&#21516;&#30340;OTTA&#25216;&#26415;&#26469;&#25552;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#22312;&#35299;&#30721;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#20026;&#23558;&#20154;&#31867;&#22823;&#33041;&#19982;&#22806;&#37096;&#35774;&#22791;&#30456;&#36830;&#25552;&#20379;&#20102;&#19968;&#26465;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#65292;&#36825;&#20027;&#35201;&#24471;&#30410;&#20110;&#24840;&#21457;&#22797;&#26434;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#27979;&#35797;&#21644;&#21463;&#35797;&#32773;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25506;&#35752;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;OTTA&#65289;&#30340;&#27010;&#24565;&#65292;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#25345;&#32493;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#35775;&#38382;&#28304;&#25968;&#25454;&#26469;&#20445;&#35777;&#38544;&#31169;&#30340;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;OTTA&#36890;&#36807;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#23450;&#20110;&#20250;&#35805;&#25110;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#26469;&#23454;&#29616;&#26080;&#38656;&#26657;&#20934;&#36816;&#34892;&#12290;&#25105;&#20204;&#23558;&#20351;&#29992;&#36731;&#37327;&#32423;&#26550;&#26500;&#20197;&#21450;&#19981;&#21516;&#30340;OTTA&#25216;&#26415;&#65288;&#22914;&#23545;&#40784;&#65292;&#33258;&#36866;&#24212;&#25209;&#37327;&#24402;&#19968;&#21270;&#65289;&#26469;&#30740;&#31350;&#33041;&#30005;&#36816;&#21160;&#24819;&#35937;&#35299;&#30721;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing a promising pathway to link the human brain with external devices, Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding capabilities, primarily driven by increasingly sophisticated techniques, especially deep learning. However, achieving high accuracy in real-world scenarios remains a challenge due to the distribution shift between sessions and subjects. In this paper we will explore the concept of online test-time adaptation (OTTA) to continuously adapt the model in an unsupervised fashion during inference time. Our approach guarantees the preservation of privacy by eliminating the requirement to access the source data during the adaptation process. Additionally, OTTA achieves calibration-free operation by not requiring any session- or subject-specific data. We will investigate the task of electroencephalography (EEG) motor imagery decoding using a lightweight architecture together with different OTTA techniques like alignment, adaptive batch normaliza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20809;&#29031;&#19981;&#21464;&#30340;&#32769;&#34382;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;EnlightenGAN&#21644;YOLOv8&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;61%&#30340;mAP&#24471;&#20998;&#65292;&#24182;&#36890;&#36807;&#20809;&#29031;&#22686;&#24378;&#25552;&#39640;&#20102;0.7%&#30340;mAP&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;ATRW&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.17552</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#37326;&#29983;&#21160;&#29289;&#30417;&#27979;&#30340;&#39640;&#25928;&#20809;&#29031;&#19981;&#21464;&#30340;&#32769;&#34382;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance. (arXiv:2311.17552v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20809;&#29031;&#19981;&#21464;&#30340;&#32769;&#34382;&#26816;&#27979;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;EnlightenGAN&#21644;YOLOv8&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;61%&#30340;mAP&#24471;&#20998;&#65292;&#24182;&#36890;&#36807;&#20809;&#29031;&#22686;&#24378;&#25552;&#39640;&#20102;0.7%&#30340;mAP&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;ATRW&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;&#20102;&#24403;&#21069;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32769;&#34382;&#20445;&#25252;&#38656;&#35201;&#22810;&#26041;&#38754;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#20445;&#25252;&#29983;&#24577;&#26646;&#24687;&#22320;&#12289;&#21453;&#20599;&#29454;&#25514;&#26045;&#20197;&#21450;&#31038;&#21306;&#21442;&#19982;&#65292;&#20197;&#23454;&#29616;&#32769;&#34382;&#31181;&#32676;&#30340;&#21487;&#25345;&#32493;&#22686;&#38271;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#21033;&#29992;&#30446;&#26631;&#26816;&#27979;&#33258;&#21160;&#21270;&#36827;&#34892;&#32769;&#34382;&#30417;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EnlightenGAN&#21644;YOLOv8&#30340;&#20934;&#30830;&#20809;&#29031;&#19981;&#21464;&#26694;&#26550;&#29992;&#20110;&#32769;&#34382;&#26816;&#27979;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;YOLOv8&#27169;&#22411;&#22312;&#27809;&#26377;&#20809;&#29031;&#22686;&#24378;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;61&#65285;&#30340;mAP&#24471;&#20998;&#12290;&#20809;&#29031;&#22686;&#24378;&#21487;&#20197;&#25552;&#39640;0.7&#65285;&#30340;mAP&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;ATRW&#25968;&#25454;&#38598;&#30340;&#26368;&#26032;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;6&#65285;&#33267;7&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tiger conservation necessitates the strategic deployment of multifaceted initiatives encompassing the preservation of ecological habitats, anti-poaching measures, and community involvement for sustainable growth in the tiger population. With the advent of artificial intelligence, tiger surveillance can be automated using object detection. In this paper, an accurate illumination invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without illumination enhancement. The illumination enhancement improves the mAP by 0.7%. The approaches elevate the state-of-the-art performance on the ATRW dataset by approximately 6% to 7%.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2311.16167</link><description>&lt;p&gt;
&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;PDE&#30340;&#31227;&#21160;&#37319;&#26679;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(MMPDE-Net)&#65292;&#36890;&#36807;&#35299;&#20915;&#31227;&#21160;&#32593;&#26684;PDE&#26469;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#65292;&#24182;&#19988;&#32467;&#21512;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#30340;&#26694;&#26550;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31227;&#21160;&#32593;&#26684;&#26041;&#27861;&#30340;&#31471;&#21040;&#31471;&#33258;&#36866;&#24212;&#37319;&#26679;&#31070;&#32463;&#32593;&#32476;&#65288;MMPDE-Net&#65289;&#65292;&#36890;&#36807;&#27714;&#35299;&#31227;&#21160;&#32593;&#26684;PDE&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#29983;&#25104;&#26032;&#30340;&#37319;&#26679;&#28857;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#25913;&#21892;&#37319;&#26679;&#28857;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;MMPDE-Net&#24320;&#21457;&#20102;&#19968;&#31181;&#36845;&#20195;&#31639;&#27861;&#65292;&#20351;&#24471;&#37319;&#26679;&#28857;&#26356;&#21152;&#31934;&#30830;&#21644;&#21487;&#25511;&#12290;&#30001;&#20110;MMPDE-Net&#26159;&#29420;&#31435;&#20110;&#28145;&#24230;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#31227;&#21160;&#37319;&#26679;PINN&#65288;MS-PINN&#65289;&#65292;&#24182;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#36890;&#36807;&#35823;&#24046;&#20998;&#26512;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#20856;&#22411;&#23454;&#20363;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;MS-PINN&#30456;&#23545;&#20110;PINN&#30340;&#24615;&#33021;&#25913;&#21892;&#65292;&#20174;&#32780;&#25968;&#20540;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose an end-to-end adaptive sampling neural network (MMPDE-Net) based on the moving mesh method, which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes the sampling points more precise and controllable. Since MMPDE-Net is a framework independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error analysis under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#23398;&#20064;&#29575;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#26174;&#31034;&#20986;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#65292;&#23558;&#36845;&#20195;&#26397;&#30528;&#27604;&#26799;&#24230;&#19979;&#38477;&#21457;&#29616;&#30340;&#26356;&#24179;&#32531;&#30340;&#26497;&#23567;&#20540;&#26041;&#21521;&#25512;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.15051</link><description>&lt;p&gt;
&#24102;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#30340;&#22823;&#22411;&#24377;&#23556;&#27010;&#24565;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study. (arXiv:2311.15051v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.15051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#23398;&#20064;&#29575;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#26174;&#31034;&#20986;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#65292;&#23558;&#36845;&#20195;&#26397;&#30528;&#27604;&#26799;&#24230;&#19979;&#38477;&#21457;&#29616;&#30340;&#26356;&#24179;&#32531;&#30340;&#26497;&#23567;&#20540;&#26041;&#21521;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20854;&#23545;&#35757;&#32451;&#36712;&#36857;&#30340;&#24433;&#21709;&#30340;&#20855;&#20307;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#24102;&#26377;&#22823;&#23398;&#20064;&#29575;&#21644;&#23398;&#20064;&#29575;&#39044;&#28909;&#30340;&#21160;&#37327;&#26799;&#24230;&#19979;&#38477;&#26174;&#31034;&#20986;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#65292;&#23558;&#36845;&#20195;&#26397;&#30528;&#27604;&#26799;&#24230;&#19979;&#38477;&#21457;&#29616;&#30340;&#26356;&#24179;&#32531;&#30340;&#26497;&#23567;&#20540;&#26041;&#21521;&#25512;&#36827;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#21644;&#29702;&#35770;&#30452;&#35273;&#65292;&#34920;&#26126;&#22823;&#22411;&#24377;&#23556;&#25928;&#24212;&#26159;&#30001;&#20110;&#21160;&#37327;&#8220;&#25918;&#22823;&#8221;&#20102;&#33258;&#31283;&#23450;&#25928;&#24212;&#65288;Damian&#31561;&#65292;2023&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although gradient descent with momentum is widely used in modern deep learning, a concrete understanding of its effects on the training trajectory still remains elusive. In this work, we empirically show that momentum gradient descent with a large learning rate and learning rate warmup displays large catapults, driving the iterates towards flatter minima than those found by gradient descent. We then provide empirical evidence and theoretical intuition that the large catapult is caused by momentum "amplifying" the self-stabilization effect (Damian et al., 2023).B.1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#24322;&#26500;&#39046;&#22495;&#36866;&#24212;&#30340;&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#65288;OSDN&#65289;&#29992;&#20110;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20174;&#30693;&#35782;&#20016;&#23500;&#30340;&#28304;&#32593;&#32476;&#20837;&#20405;&#39046;&#22495;&#36827;&#34892;&#20837;&#20405;&#30693;&#35782;&#20256;&#36755;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;&#22312;&#24320;&#25918;&#38598;&#35774;&#32622;&#19979;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;&#22312;&#28304;&#39046;&#22495;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#20852;&#30446;&#26631;&#39046;&#22495;&#20837;&#20405;&#12290;</title><link>http://arxiv.org/abs/2311.11249</link><description>&lt;p&gt;
&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#29992;&#20110;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Open Set Dandelion Network for IoT Intrusion Detection. (arXiv:2311.11249v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#24322;&#26500;&#39046;&#22495;&#36866;&#24212;&#30340;&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#65288;OSDN&#65289;&#29992;&#20110;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20174;&#30693;&#35782;&#20016;&#23500;&#30340;&#28304;&#32593;&#32476;&#20837;&#20405;&#39046;&#22495;&#36827;&#34892;&#20837;&#20405;&#30693;&#35782;&#20256;&#36755;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;&#22312;&#24320;&#25918;&#38598;&#35774;&#32622;&#19979;&#65292;&#23427;&#33021;&#22815;&#26816;&#27979;&#21040;&#22312;&#28304;&#39046;&#22495;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#20852;&#30446;&#26631;&#39046;&#22495;&#20837;&#20405;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20445;&#25252;&#23427;&#20204;&#20813;&#21463;&#24694;&#24847;&#20837;&#20405;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#38480;&#21046;&#20102;&#20256;&#32479;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#36825;&#20123;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#22522;&#20110;&#26080;&#30417;&#30563;&#24322;&#26500;&#39046;&#22495;&#36866;&#24212;&#30340;&#24320;&#25918;&#38598;dandelion&#32593;&#32476;&#65288;OSDN&#65289;&#12290;OSDN&#27169;&#22411;&#36890;&#36807;&#20174;&#30693;&#35782;&#20016;&#23500;&#30340;&#28304;&#32593;&#32476;&#20837;&#20405;&#39046;&#22495;&#36827;&#34892;&#20837;&#20405;&#30693;&#35782;&#20256;&#36755;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25968;&#25454;&#31232;&#32570;&#30340;&#30446;&#26631;&#29289;&#32852;&#32593;&#20837;&#20405;&#39046;&#22495;&#30340;&#26356;&#20934;&#30830;&#30340;&#20837;&#20405;&#26816;&#27979;&#12290;&#22312;&#24320;&#25918;&#38598;&#35774;&#32622;&#19979;&#65292;&#23427;&#36824;&#33021;&#26816;&#27979;&#21040;&#22312;&#28304;&#39046;&#22495;&#20013;&#26410;&#35266;&#27979;&#21040;&#30340;&#26032;&#20852;&#30446;&#26631;&#39046;&#22495;&#20837;&#20405;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;OSDN&#27169;&#22411;&#23558;&#28304;&#39046;&#22495;&#24418;&#25104;&#19968;&#20010;&#31867;&#20284;&#33970;&#20844;&#33521;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20854;&#20013;&#27599;&#20010;&#20837;&#20405;&#31867;&#21035;&#34987;&#32039;&#23494;&#20998;&#32452;&#24182;&#19988;&#19981;&#21516;&#30340;&#20837;&#20405;&#31867;&#21035;&#34987;&#20998;&#38548;&#24320;&#65292;&#21363;&#21516;&#26102;&#24378;&#35843;&#20102;&#31867;&#21035;&#38388;&#30340;&#21487;&#20998;&#24615;&#21644;&#31867;&#21035;&#20869;&#30340;&#32039;&#20945;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As IoT devices become widely, it is crucial to protect them from malicious intrusions. However, the data scarcity of IoT limits the applicability of traditional intrusion detection methods, which are highly data-dependent. To address this, in this paper we propose the Open-Set Dandelion Network (OSDN) based on unsupervised heterogeneous domain adaptation in an open-set manner. The OSDN model performs intrusion knowledge transfer from the knowledge-rich source network intrusion domain to facilitate more accurate intrusion detection for the data-scarce target IoT intrusion domain. Under the open-set setting, it can also detect newly-emerged target domain intrusions that are not observed in the source domain. To achieve this, the OSDN model forms the source domain into a dandelion-like feature space in which each intrusion category is compactly grouped and different intrusion categories are separated, i.e., simultaneously emphasising inter-category separability and intra-category compactn
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#24322;&#27493;&#26412;&#22320;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20843;&#21350;&#24322;&#27493;&#36890;&#20449;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#21033;&#29992;&#24555;&#36895;&#35745;&#31639;&#23454;&#29616;&#21327;&#20316;&#23398;&#20064;&#26410;&#30693;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2311.03496</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#24322;&#27493;&#26412;&#22320;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Local Computations in Distributed Bayesian Learning. (arXiv:2311.03496v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#24335;&#36125;&#21494;&#26031;&#23398;&#20064;&#20013;&#30340;&#24322;&#27493;&#26412;&#22320;&#35745;&#31639;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20843;&#21350;&#24322;&#27493;&#36890;&#20449;&#26469;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#21033;&#29992;&#24555;&#36895;&#35745;&#31639;&#23454;&#29616;&#21327;&#20316;&#23398;&#20064;&#26410;&#30693;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#20256;&#24863;&#22120;&#32593;&#32476;&#65292;&#21327;&#20316;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#39046;&#22495;&#30340;&#24212;&#29992;&#33539;&#22260;&#19981;&#26029;&#25193;&#22823;&#65292;&#20998;&#24067;&#24335;&#25512;&#29702;&#31639;&#27861;&#30340;&#37096;&#32626;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#36825;&#20123;&#31639;&#27861;&#28041;&#21450;&#21040;&#36890;&#36807;&#22810;&#20010;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#20998;&#25955;&#25968;&#25454;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#26410;&#30693;&#21442;&#25968;&#12290;&#22312;&#36825;&#26679;&#30340;&#31639;&#27861;&#20013;&#23384;&#22312;&#20004;&#20010;&#31454;&#20105;&#30340;&#26041;&#38754;&#65292;&#21363;&#26234;&#33021;&#20307;&#20869;&#37096;&#35745;&#31639;&#21644;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#20256;&#32479;&#19978;&#65292;&#31639;&#27861;&#34987;&#35774;&#35745;&#25104;&#21516;&#26102;&#25191;&#34892;&#36825;&#20004;&#20010;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#36890;&#20449;&#20449;&#36947;&#19981;&#21487;&#38752;&#12289;&#32791;&#26102;&#25110;&#36164;&#28304;&#26114;&#36149;&#65292;&#38656;&#35201;&#33410;&#32422;&#20351;&#29992;&#36890;&#20449;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20843;&#21350;&#30340;&#24322;&#27493;&#36890;&#20449;&#26469;&#21516;&#26102;&#21033;&#29992;&#24555;&#36895;&#35745;&#31639;&#21644;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36830;&#32493;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26412;&#22320;&#26234;&#33021;&#20307;&#35745;&#31639;&#23545;&#20132;&#20114;&#26234;&#33021;&#20307;&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#26412;&#22320;&#35745;&#31639;&#37096;&#20998;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#37319;&#26679;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the expanding scope of machine learning (ML) to the fields of sensor networking, cooperative robotics and many other multi-agent systems, distributed deployment of inference algorithms has received a lot of attention. These algorithms involve collaboratively learning unknown parameters from dispersed data collected by multiple agents. There are two competing aspects in such algorithms, namely, intra-agent computation and inter-agent communication. Traditionally, algorithms are designed to perform both synchronously. However, certain circumstances need frugal use of communication channels as they are either unreliable, time-consuming, or resource-expensive. In this paper, we propose gossip-based asynchronous communication to leverage fast computations and reduce communication overhead simultaneously. We analyze the effects of multiple (local) intra-agent computations by the active agents between successive inter-agent communications. For local computations, Bayesian sampling via 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#32423;&#32508;&#21512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30828;&#20214;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21151;&#33021;&#21644;&#36136;&#37327;&#65292;&#24182;&#35760;&#24405;&#20102;&#25152;&#26377;&#30456;&#20851;&#30340;&#24037;&#20855;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#26041;&#27861;&#23558;&#22312;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2311.03489</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#32423;&#32508;&#21512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#27169;&#25311;&#21644;&#37096;&#32626;&#32479;&#19968;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#30828;&#20214;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v4 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03489
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39640;&#32423;&#32508;&#21512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30828;&#20214;&#35774;&#35745;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#21151;&#33021;&#21644;&#36136;&#37327;&#65292;&#24182;&#35760;&#24405;&#20102;&#25152;&#26377;&#30456;&#20851;&#30340;&#24037;&#20855;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#19968;&#26041;&#27861;&#23558;&#22312;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#20013;&#20135;&#29983;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#32423;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24037;&#20855;&#26469;&#29983;&#25104;&#30828;&#20214;&#35774;&#35745;&#12290;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;&#65292;&#19981;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#20197;&#29983;&#25104;&#20855;&#26377;wishbone&#25509;&#21475;&#30340;&#32622;&#25442;&#21516;&#20313;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20223;&#30495;&#21644;Dieharder&#38543;&#26426;&#24615;&#27979;&#35797;&#22871;&#20214;&#39564;&#35777;&#20102;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#35774;&#35745;&#30340;&#21151;&#33021;&#21644;&#36136;&#37327;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#26696;&#20363;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#35760;&#24405;&#12289;Python&#33050;&#26412;&#12289;Verilog&#33050;&#26412;&#21644;&#20223;&#30495;&#32467;&#26524;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#30828;&#20214;&#35774;&#35745;&#29983;&#25104;&#26041;&#27861;&#19982;&#24320;&#28304;&#30789;130&#32435;&#31859;&#35774;&#35745;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#23558;&#25913;&#21464;&#24212;&#29992;&#29305;&#23450;&#38598;&#25104;&#30005;&#36335;&#35774;&#35745;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26500;&#24314;&#29289;&#32852;&#32593;&#30340;&#39046;&#22495;&#19987;&#29992;&#35745;&#31639;&#21152;&#36895;&#22120;&#21644;&#27010;&#24565;&#39564;&#35777;&#21407;&#22411;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new high-level synthesis methodology for using large language model tools to generate hardware designs. The methodology uses exclusively open-source tools excluding the large language model. As a case study, we use our methodology to generate a permuted congruential random number generator design with a wishbone interface. We verify the functionality and quality of the random number generator design using large language model-generated simulations and the Dieharder randomness test suite. We document all the large language model chat logs, Python scripts, Verilog scripts, and simulation results used in the case study. We believe that our method of hardware design generation coupled with the open source silicon 130 nm design tools will revolutionize application-specific integrated circuit design. Our methodology significantly lowers the bar to entry when building domain-specific computing accelerators for the Internet of Things and proof of concept prototypes for later fabri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2310.19802</link><description>&lt;p&gt;
&#23398;&#20064;&#29983;&#25104;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#38543;&#26426;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Stochastic Thermodynamics of Learning Generative Parametric Probabilistic Models. (arXiv:2310.19802v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#35270;&#20026;&#21442;&#25968;&#27010;&#29575;&#27169;&#22411;&#30340;&#26102;&#38388;&#28436;&#21270;&#36807;&#31243;&#65292;&#36890;&#36807;&#30740;&#31350;&#27169;&#22411;&#21442;&#25968;&#19982;&#29983;&#25104;&#26679;&#26412;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#65292;&#21457;&#29616;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#28909;&#24211;&#23384;&#20648;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#36825;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#29983;&#25104;&#24335;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#21442;&#25968;&#21270;&#27010;&#29575;&#27169;&#22411;&#65288;PPM&#65289;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#20174;&#26412;&#36136;&#19978;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#20010;&#28909;&#21147;&#23398;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27169;&#22411;&#21442;&#25968;&#65288;&#35760;&#20026;$\Theta$&#65289;&#19982;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#65288;&#35760;&#20026;$X$&#65289;&#20043;&#38388;&#30340;&#28909;&#21147;&#23398;&#20132;&#25442;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20248;&#21270;&#22120;&#30340;&#20316;&#29992;&#26159;&#39537;&#21160;&#36825;&#20004;&#20010;&#23376;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#30340;&#33021;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#29983;&#25104;&#26679;&#26412;$X$&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#36890;&#36807;&#32791;&#25955;&#28909;&#37327;&#26469;&#23398;&#20064;&#65292;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;$\Theta$&#30340;&#29109;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#23376;&#31995;&#32479;&#20805;&#24403;&#20102;&#19968;&#20010;&#28909;&#24211;&#65292;&#26377;&#25928;&#22320;&#23384;&#20648;&#20102;&#23398;&#21040;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#21442;&#25968;&#20316;&#20026;&#28909;&#24211;&#30340;&#35282;&#33394;&#20026;&#36229;&#21442;&#25968;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#28909;&#21147;&#23398;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#19988;&#19968;&#33268;&#30340;&#26041;&#24335;&#26469;&#29702;&#35299;&#29983;&#25104;&#27169;&#22411;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28909;&#21147;&#23398;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We have formulated generative machine learning problems as the time evolution of Parametric Probabilistic Models (PPMs), inherently rendering a thermodynamic process. Then, we have studied the thermodynamic exchange between the model's parameters, denoted as $\Theta$, and the model's generated samples, denoted as $X$. We demonstrate that the training dataset and the action of the Stochastic Gradient Descent (SGD) optimizer serve as a work source that governs the time evolution of these two subsystems. Our findings reveal that the model learns through the dissipation of heat during the generation of samples $X$, leading to an increase in the entropy of the model's parameters, $\Theta$. Thus, the parameter subsystem acts as a heat reservoir, effectively storing the learned information. Furthermore, the role of the model's parameters as a heat reservoir provides valuable thermodynamic insights into the generalization power of over-parameterized models. This approach offers an unambiguous 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#25490;&#21015;&#26816;&#39564;&#30340;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#30340;&#38750;&#31169;&#26377;&#25490;&#21015;&#26816;&#39564;&#65292;&#20197;&#22312;&#31169;&#26377;&#29615;&#22659;&#20013;&#20445;&#25345;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#35813;&#26816;&#39564;&#30340;&#21151;&#29575;&#21462;&#20915;&#20110;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#33324;&#26465;&#20214;&#26469;&#20445;&#35777;&#19968;&#33268;&#24615;&#21644;&#38750;&#28176;&#36827;&#22343;&#21248;&#30340;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.19043</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#25490;&#21015;&#26816;&#39564;&#65306;&#24212;&#29992;&#20110;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Permutation Tests: Applications to Kernel Methods. (arXiv:2310.19043v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24046;&#20998;&#38544;&#31169;&#25490;&#21015;&#26816;&#39564;&#30340;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#30340;&#38750;&#31169;&#26377;&#25490;&#21015;&#26816;&#39564;&#65292;&#20197;&#22312;&#31169;&#26377;&#29615;&#22659;&#20013;&#20445;&#25345;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#35813;&#26816;&#39564;&#30340;&#21151;&#29575;&#21462;&#20915;&#20110;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#33324;&#26465;&#20214;&#26469;&#20445;&#35777;&#19968;&#33268;&#24615;&#21644;&#38750;&#28176;&#36827;&#22343;&#21248;&#30340;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#25935;&#24863;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#36234;&#26469;&#36234;&#20851;&#27880;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#20316;&#20026;&#19968;&#31181;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#26694;&#26550;&#24212;&#36816;&#32780;&#29983;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24191;&#27867;&#35748;&#21487;&#12290;&#23613;&#31649;&#22312;&#31169;&#26377;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#23384;&#22312;&#19981;&#23454;&#29992;&#25110;&#26126;&#26174;&#30340;&#32479;&#35745;&#25928;&#29575;&#25439;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#24046;&#20998;&#38544;&#31169;&#25490;&#21015;&#26816;&#39564;&#26469;&#32531;&#35299;&#36825;&#20123;&#25285;&#24551;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#32463;&#20856;&#30340;&#38750;&#31169;&#26377;&#25490;&#21015;&#26816;&#39564;&#25193;&#23637;&#21040;&#31169;&#26377;&#29615;&#22659;&#20013;&#65292;&#20197;&#20005;&#26684;&#30340;&#26041;&#24335;&#20445;&#25345;&#26377;&#38480;&#26679;&#26412;&#26377;&#25928;&#24615;&#21644;&#24046;&#20998;&#38544;&#31169;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#30340;&#21151;&#29575;&#21462;&#20915;&#20110;&#19968;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#36873;&#25321;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#33324;&#26465;&#20214;&#20445;&#35777;&#20102;&#19968;&#33268;&#24615;&#21644;&#38750;&#28176;&#36827;&#22343;&#21248;&#30340;&#21151;&#29575;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26694;&#26550;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#37325;&#29616;&#26680;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed growing concerns about the privacy of sensitive data. In response to these concerns, differential privacy has emerged as a rigorous framework for privacy protection, gaining widespread recognition in both academic and industrial circles. While substantial progress has been made in private data analysis, existing methods often suffer from impracticality or a significant loss of statistical efficiency. This paper aims to alleviate these concerns in the context of hypothesis testing by introducing differentially private permutation tests. The proposed framework extends classical non-private permutation tests to private settings, maintaining both finite-sample validity and differential privacy in a rigorous manner. The power of the proposed test depends on the choice of a test statistic, and we establish general conditions for consistency and non-asymptotic uniform power. To demonstrate the utility and practicality of our framework, we focus on reproducing kerne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#26041;&#27861;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;</title><link>http://arxiv.org/abs/2310.17848</link><description>&lt;p&gt;
&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#25193;&#23637;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#26469;&#25552;&#21319;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#20808;&#36827;&#27169;&#22411;&#29983;&#25104;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#37319;&#29992;&#32479;&#35745;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#32479;&#35745;&#26041;&#27861;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20316;&#20026;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30707;&#65292;&#22312;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#26085;&#30410;&#37325;&#35201;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#32479;&#35745;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19982;&#21407;&#22987;&#25968;&#25454;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#39640;&#36924;&#30495;&#24230;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#36890;&#36807;&#20808;&#36827;&#27169;&#22411;&#22914;&#34920;&#26684;&#25193;&#25955;&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#29983;&#25104;&#65292;&#24182;&#32467;&#21512;&#30456;&#20851;&#30740;&#31350;&#27934;&#23519;&#36827;&#19968;&#27493;&#22686;&#24378;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#21457;&#29616;&#26159;&#29983;&#25104;&#25928;&#24212;&#65306;&#32479;&#35745;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#38169;&#35823;&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#30340;&#22686;&#21152;&#19968;&#24320;&#22987;&#20943;&#23569;&#65292;&#20294;&#26368;&#32456;&#21487;&#33021;&#20250;&#22686;&#21152;&#25110;&#20572;&#28382;&#12290;&#36825;&#20010;&#29616;&#35937;&#26681;&#28304;&#20110;&#22797;&#21046;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data generation, a cornerstone of Generative Artificial Intelligence, signifies a paradigm shift in data science by addressing data scarcity and privacy while enabling unprecedented performance. As synthetic data gains prominence, questions arise concerning the accuracy of statistical methods when applied to synthetic data compared to raw data. In this article, we introduce the Synthetic Data Generation for Analytics framework. This framework employs statistical methods on high-fidelity synthetic data generated by advanced models such as tabular diffusion and Generative Pre-trained Transformer models. These models, trained on raw data, are further enhanced with insights from pertinent studies. A significant discovery within this framework is the generational effect: the error of a statistical method on synthetic data initially diminishes with added synthetic data but may eventually increase or plateau. This phenomenon, rooted in the complexities of replicating raw data distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#20027;&#21160;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65288;CRCL&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#20027;&#21160;&#20114;&#34917;&#25439;&#22833;&#65288;ACL&#65289;&#21644;&#39640;&#25928;&#30340;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;&#20462;&#27491;&#65288;SCC&#65289;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17468</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#20027;&#21160;&#20114;&#34917;&#23398;&#20064;&#19982;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#20027;&#21160;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65288;CRCL&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#20027;&#21160;&#20114;&#34917;&#25439;&#22833;&#65288;ACL&#65289;&#21644;&#39640;&#25928;&#30340;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;&#20462;&#27491;&#65288;SCC&#65289;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21305;&#37197;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#29702;&#35299;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#28508;&#22312;&#23545;&#24212;&#20851;&#31995;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38544;&#24335;&#20551;&#35774;&#35757;&#32451;&#23545;&#26159;&#23545;&#40784;&#33391;&#22909;&#30340;&#65292;&#32780;&#24573;&#30053;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#27880;&#37322;&#22122;&#38899;&#65292;&#21363;&#22122;&#22768;&#23545;&#24212;&#65288;NC&#65289;&#65292;&#20174;&#32780;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#35299;&#20915;&#36825;&#31181;&#22122;&#22768;&#65292;&#20294;&#20173;&#38754;&#20020;&#20004;&#20010;&#25361;&#25112;&#24615;&#38382;&#39064;&#65306;&#36807;&#24230;&#35760;&#24518;/&#36807;&#25311;&#21512;&#21644;&#23545;&#20110;NC&#30340;&#19981;&#21487;&#38752;&#20462;&#27491;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#22122;&#22768;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36328;&#27169;&#24577;&#40065;&#26834;&#20114;&#34917;&#23398;&#20064;&#26694;&#26550;&#65288;CRCL&#65289;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#20114;&#34917;&#25439;&#22833;&#65288;ACL&#65289;&#21644;&#39640;&#25928;&#30340;&#33258;&#25105;&#23436;&#21892;&#23545;&#24212;&#20851;&#31995;&#20462;&#27491;&#65288;SCC&#65289;&#26469;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ACL&#21033;&#29992;&#20027;&#21160;&#21644;&#20114;&#34917;&#30340;&#23398;&#20064;&#25439;&#22833;&#26469;&#20943;&#23569;&#25552;&#20379;&#38169;&#35823;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, image-text matching has attracted more and more attention from academia and industry, which is fundamental to understanding the latent correspondence across visual and textual modalities. However, most existing methods implicitly assume the training pairs are well-aligned while ignoring the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby inevitably leading to a performance drop. Although some methods attempt to address such noise, they still face two challenging problems: excessive memorizing/overfitting and unreliable correction for NC, especially under high noise. To address the two problems, we propose a generalized Cross-modal Robust Complementary Learning framework (CRCL), which benefits from a novel Active Complementary Loss (ACL) and an efficient Self-refining Correspondence Correction (SCC) to improve the robustness of existing methods. Specifically, ACL exploits active and complementary learning losses to reduce the risk of providing erroneous s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.16781</link><description>&lt;p&gt;
Kiki&#36824;&#26159;Bouba&#65311;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#35937;&#24449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16781
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#21457;&#29616;&#23427;&#20204;&#26174;&#31034;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#30340;&#27169;&#24335;&#65292;&#36827;&#19968;&#27493;&#35777;&#23454;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#36328;&#27169;&#24577;&#20851;&#32852;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#31867;&#35821;&#35328;&#20013;&#30340;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#30340;&#26144;&#23556;&#34987;&#35748;&#20026;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38543;&#26426;&#30340;&#65292;&#20294;&#35748;&#30693;&#31185;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#21644;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#29305;&#23450;&#22768;&#38899;&#21644;&#24847;&#20041;&#20043;&#38388;&#23384;&#22312;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#22768;&#38899;&#35937;&#24449;&#24615;&#12290;&#22312;&#35768;&#22810;&#24847;&#20041;&#32500;&#24230;&#20013;&#65292;&#22768;&#38899;&#35937;&#24449;&#24615;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#20851;&#32852;&#26041;&#38754;&#23588;&#20026;&#26174;&#33879;&#21644;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22768;&#38899;&#35937;&#24449;&#24615;&#26159;&#21542;&#22312;CLIP&#21644;Stable Diffusion&#31561;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20307;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#30693;&#35782;&#25506;&#27979;&#26469;&#35843;&#26597;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#22312;&#30693;&#35782;&#65292;&#25105;&#20204;&#21457;&#29616;&#24378;&#26377;&#21147;&#30340;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#30830;&#23454;&#26174;&#31034;&#20102;&#36825;&#31181;&#27169;&#24335;&#65292;&#19982;&#24515;&#29702;&#35821;&#35328;&#23398;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;kiki-bouba&#25928;&#24212;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#20351;&#29992;&#35745;&#31639;&#24037;&#20855;&#26469;&#23637;&#31034;&#22768;&#38899;&#35937;&#24449;&#24615;&#24182;&#29702;&#35299;&#20854;&#26412;&#36136;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#23618;&#23884;&#22871;&#24490;&#29615;&#23545;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#65292;&#24182;&#20351;&#29992;&#20869;&#24490;&#29615;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#24471;&#21040;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2310.13807</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning to (Learn at Test Time). (arXiv:2310.13807v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#23618;&#23884;&#22871;&#24490;&#29615;&#23545;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#37325;&#26032;&#23450;&#20041;&#65292;&#24182;&#20351;&#29992;&#20869;&#24490;&#29615;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#24471;&#21040;&#25913;&#36827;&#12290;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#36890;&#36807;&#20004;&#23618;&#23884;&#22871;&#24490;&#29615;&#36827;&#34892;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#20869;&#24490;&#29615;&#22312;&#27599;&#20010;&#20010;&#20307;&#23454;&#20363;&#19978;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#28982;&#21518;&#36827;&#34892;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#22806;&#24490;&#29615;&#23398;&#20064;&#20869;&#24490;&#29615;&#20351;&#29992;&#30340;&#33258;&#30417;&#30563;&#20219;&#21153;&#65292;&#20197;&#25913;&#36827;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#24403;&#20869;&#24490;&#29615;&#23398;&#20064;&#22120;&#20026;&#32447;&#24615;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#30340;&#20869;&#24490;&#29615;&#31561;&#21516;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#65307;&#24403;&#20869;&#24490;&#29615;&#23398;&#20064;&#22120;&#20026;&#26680;&#20272;&#35745;&#22120;&#26102;&#65292;&#31561;&#21516;&#20110;&#33258;&#27880;&#24847;&#21147;&#12290;&#20026;&#20102;&#19982;&#32447;&#24615;&#25110;&#33258;&#27880;&#24847;&#21147;&#23618;&#36827;&#34892;&#23454;&#38469;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;transformer&#20013;&#29992;&#20869;&#24490;&#29615;&#26367;&#20195;&#20102;&#27599;&#20010;&#32447;&#24615;&#25110;&#33258;&#27880;&#24847;&#21147;&#23618;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#22806;&#24490;&#29615;&#31561;&#21516;&#20110;&#23545;&#26550;&#26500;&#36827;&#34892;&#35757;&#32451;&#12290;&#24403;&#27599;&#20010;&#20869;&#24490;&#29615;&#23398;&#20064;&#22120;&#20026;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#22312;ImageNet&#30340;224 x 224&#21407;&#22987;&#20687;&#32032;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#21644;FLOPs&#26041;&#38754;&#36828;&#36828;&#20248;&#20110;&#20855;&#26377;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;transformer&#65292;&#32780;&#65288;&#24120;&#35268;&#30340;&#65289;transformer&#26080;&#27861;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We reformulate the problem of supervised learning as learning to learn with two nested loops (i.e. learning problems). The inner loop learns on each individual instance with self-supervision before final prediction. The outer loop learns the self-supervised task used by the inner loop, such that its final prediction improves. Our inner loop turns out to be equivalent to linear attention when the inner-loop learner is only a linear model, and to self-attention when it is a kernel estimator. For practical comparison with linear or self-attention layers, we replace each of them in a transformer with an inner loop, so our outer loop is equivalent to training the architecture. When each inner-loop learner is a neural network, our approach vastly outperforms transformers with linear attention on ImageNet from 224 x 224 raw pixels in both accuracy and FLOPs, while (regular) transformers cannot run.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2310.11571</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#19968;&#20010;&#22909;&#38382;&#39064;&#65311;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#19982;&#20107;&#23454;&#32423;&#36974;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;
What is a good question? Task-oriented asking with fact-level masking. (arXiv:2310.11571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11571
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#30340;&#27010;&#24565;&#21644;&#26694;&#26550;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#23545;&#25512;&#29702;&#20219;&#21153;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#38382;&#26159;&#29616;&#23454;&#29983;&#27963;&#20013;&#21512;&#20316;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#38382;&#31572;&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#27861;&#24459;&#21161;&#25163;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#27809;&#26377;&#29992;&#25143;&#24773;&#20917;&#30340;&#20855;&#20307;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#26080;&#27861;&#25552;&#20379;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20250;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#25512;&#29702;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#21521;&#29992;&#25143;&#25110;&#31532;&#19977;&#26041;&#25552;&#20986;&#21518;&#32493;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#22522;&#20110;&#20219;&#21153;&#30340;&#35810;&#38382;&#65288;TOA&#65289;&#12290;&#38646;-shot&#32842;&#22825;&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;TOA&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20027;&#35201;&#22522;&#20110;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#32780;&#19981;&#26159;&#38382;&#39064;&#26159;&#21542;&#23545;&#25104;&#21151;&#30340;&#21512;&#20316;&#26377;&#24110;&#21161;&#12290;&#20026;&#20102;&#33021;&#22815;&#35757;&#32451;&#21644;&#35780;&#20272;TOA&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#23548;&#21521;&#35810;&#38382;&#30340;&#23450;&#20041;&#21644;&#26694;&#26550;&#65292;&#21363;&#29983;&#25104;&#33021;&#22815;&#20026;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#31572;&#26696;&#30340;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20107;&#23454;&#32423;&#36974;&#34109;&#65288;FLM&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30465;&#30053;&#29305;&#23450;&#30340;&#37096;&#20998;&#23558;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#38598;&#36716;&#25442;&#20026;&#33258;&#25105;&#30417;&#30563;&#30340;TOA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asking questions is an important element of real-life collaboration on reasoning tasks like question answering. For example, a legal assistant chatbot may be unable to make accurate recommendations without specific information on the user's circumstances. However, large language models are usually deployed to solve reasoning tasks directly without asking follow-up questions to the user or third parties. We term this problem task-oriented asking (TOA). Zero-shot chat models can perform TOA, but their training is primarily based on next-token prediction rather than whether questions contribute to successful collaboration. To enable the training and evaluation of TOA models, we present a definition and framework for natural language task-oriented asking, the problem of generating questions that result in answers useful for a reasoning task. We also present fact-level masking (FLM), a procedure for converting natural language datasets into self-supervised TOA datasets by omitting particula
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.09866</link><description>&lt;p&gt;
&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Objective Learning. (arXiv:2310.09866v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#22312;&#28385;&#36275;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#25903;&#25345;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#12290;&#36890;&#36807;&#24341;&#20837;&#32852;&#37030;&#23398;&#20064;&#30340;&#33539;&#24335;&#65292;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20316;&#20026;&#35768;&#22810;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#22522;&#30784;&#38382;&#39064;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MOO&#31639;&#27861;&#20173;&#23616;&#38480;&#20110;&#38598;&#20013;&#24335;&#23398;&#20064;&#29615;&#22659;&#65292;&#26080;&#27861;&#28385;&#36275;&#36825;&#20123;&#22810;&#20195;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#24212;&#29992;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#23398;&#20064;&#65288;FMOL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#20445;&#25345;&#20182;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#31169;&#23494;&#30340;&#21516;&#26102;&#65292;&#20998;&#24067;&#24335;&#21327;&#20316;&#35299;&#20915;&#19968;&#20010;MOO&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;FMOL&#26694;&#26550;&#20801;&#35768;&#19981;&#21516;&#23458;&#25143;&#31471;&#19978;&#30340;&#19981;&#21516;&#30446;&#26631;&#20989;&#25968;&#38598;&#21512;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#39318;&#27425;&#23558;MOO&#24418;&#24335;&#21270;&#25512;&#24191;&#21040;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#20013;&#12290;&#23545;&#20110;&#36825;&#20010;FMOL&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#32852;&#37030;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;FMOO&#65289;&#31639;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#22810;&#26799;&#24230;&#19979;&#38477;&#24179;&#22343;&#65288;FMGDA&#65289;&#21644;&#32852;&#37030;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;Federated SGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, multi-objective optimization (MOO) emerges as a foundational problem underpinning many multi-agent multi-task learning applications. However, existing algorithms in MOO literature remain limited to centralized learning settings, which do not satisfy the distributed nature and data privacy needs of such multi-agent multi-task learning applications. This motivates us to propose a new federated multi-objective learning (FMOL) framework with multiple clients distributively and collaboratively solving an MOO problem while keeping their training data private. Notably, our FMOL framework allows a different set of objective functions across different clients to support a wide range of applications, which advances and generalizes the MOO formulation to the federated learning paradigm for the first time. For this FMOL framework, we propose two new federated multi-objective optimization (FMOO) algorithms called federated multi-gradient descent averaging (FMGDA) and federated stoc
&lt;/p&gt;</description></item><item><title>&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2310.07535</link><description>&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#20165;&#20973;&#23569;&#37327;&#27979;&#35797;&#26679;&#26412;&#25913;&#21892;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Improving Fairness-Accuracy tradeoff with few Test Samples under Covariate Shift. (arXiv:2310.07535v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07535
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#31639;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26410;&#32463;&#30740;&#31350;&#30340;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#25968;&#25454;&#20013;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#34920;&#29616;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30830;&#20445;&#19981;&#21516;&#25935;&#24863;&#32676;&#20307;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#28041;&#21450;&#21040;&#35832;&#22914;&#21009;&#20107;&#21496;&#27861;&#31561;&#31038;&#20250;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#25805;&#20316;&#65292;&#21482;&#26377;&#19968;&#23567;&#32452;&#26080;&#26631;&#31614;&#30340;&#27979;&#35797;&#26679;&#26412;&#21644;&#19968;&#20010;&#24102;&#26631;&#31614;&#30340;&#35757;&#32451;&#38598;&#21487;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#36129;&#29486;&#12290;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#22522;&#20110;&#26032;&#22411;&#22797;&#21512;&#21152;&#26435;&#29109;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#34920;&#31034;&#21305;&#37197;&#25439;&#22833;&#26469;&#20248;&#21270;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#20960;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#22312;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#20248;&#20110;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22312;&#25105;&#20204;&#25152;&#30693;&#30340;&#33539;&#22260;&#20869;&#23578;&#26410;&#30740;&#31350;&#36807;&#38750;&#23545;&#31216;&#21327;&#21464;&#37327;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Covariate shift in the test data can significantly downgrade both the accuracy and the fairness performance of the model. Ensuring fairness across different sensitive groups in such settings is of paramount importance due to societal implications like criminal justice. We operate under the unsupervised regime where only a small set of unlabeled test samples along with a labeled training set is available. Towards this problem, we make three contributions. First is a novel composite weighted entropy based objective for prediction accuracy which is optimized along with a representation matching loss for fairness. We experimentally verify that optimizing with our loss formulation outperforms a number of state-of-the-art baselines in the pareto sense with respect to the fairness-accuracy tradeoff on several standard datasets. Our second contribution is a new setting we term Asymmetric Covariate Shift that, to the best of our knowledge, has not been studied before. Asymmetric covariate shift
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#20154;&#31867;&#34892;&#20026;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#26469;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25913;&#36827;Web&#27983;&#35272;&#25110;&#20132;&#36890;&#23548;&#33322;&#31561;&#24212;&#29992;&#30340;&#24213;&#23618;&#22522;&#30784;&#35774;&#26045;&#25110;&#29992;&#25143;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.04477</link><description>&lt;p&gt;
&#39640;&#38454;DeepTrails&#65306;&#23545;*Trails&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Higher-Order DeepTrails: Unified Approach to *Trails. (arXiv:2310.04477v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#20154;&#31867;&#34892;&#20026;&#30340;&#32479;&#19968;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#26469;&#25429;&#25417;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#25913;&#36827;Web&#27983;&#35272;&#25110;&#20132;&#36890;&#23548;&#33322;&#31561;&#24212;&#29992;&#30340;&#24213;&#23618;&#22522;&#30784;&#35774;&#26045;&#25110;&#29992;&#25143;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#12289;&#29702;&#35299;&#21644;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#22914;&#32593;&#32476;&#27983;&#35272;&#25110;&#20132;&#36890;&#23548;&#33322;&#12290;&#29702;&#35299;&#20154;&#31867;&#34892;&#20026;&#33258;&#28982;&#26377;&#21161;&#20110;&#25913;&#36827;&#21644;&#20248;&#21270;&#24213;&#23618;&#22522;&#30784;&#35774;&#26045;&#25110;&#29992;&#25143;&#30028;&#38754;&#12290;&#36890;&#24120;&#65292;&#20154;&#31867;&#23548;&#33322;&#26159;&#30001;&#29366;&#24577;&#38388;&#36716;&#25442;&#30340;&#24207;&#21015;&#34920;&#31034;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24314;&#35758;&#20351;&#29992;&#20551;&#35774;&#26469;&#20998;&#26512;&#36825;&#20123;&#36716;&#25442;&#65292;&#20195;&#34920;&#19981;&#21516;&#30340;&#23548;&#33322;&#30452;&#35266;&#12290;&#20026;&#20102;&#22312;&#25968;&#23398;&#19978;&#25235;&#20303;&#36825;&#20010;&#35774;&#32622;&#65292;&#20351;&#29992;&#19968;&#38454;&#39532;&#23572;&#21487;&#22827;&#38142;&#26469;&#25429;&#25417;&#34892;&#20026;&#65292;&#22240;&#27492;&#21487;&#20197;&#24212;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#27604;&#36739;&#65292;&#20294;&#26159;&#20250;&#26377;&#25439;&#22833;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20449;&#24687;&#30340;&#22266;&#26377;&#32570;&#28857;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#26512;&#25972;&#20010;&#24207;&#21015;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#24207;&#21015;&#20013;&#30340;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#19981;&#21516;&#30340;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing, understanding, and describing human behavior is advantageous in different settings, such as web browsing or traffic navigation. Understanding human behavior naturally helps to improve and optimize the underlying infrastructure or user interfaces. Typically, human navigation is represented by sequences of transitions between states. Previous work suggests to use hypotheses, representing different intuitions about the navigation to analyze these transitions. To mathematically grasp this setting, first-order Markov chains are used to capture the behavior, consequently allowing to apply different kinds of graph comparisons, but comes with the inherent drawback of losing information about higher-order dependencies within the sequences. To this end, we propose to analyze entire sequences using autoregressive language models, as they are traditionally used to model higher-order dependencies in sequences. We show that our approach can be easily adapted to model different settings in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.02489</link><description>&lt;p&gt;
ResidualTransformer&#65306;&#24102;&#26377;&#26435;&#37325;&#20849;&#20139;&#30340;&#27531;&#24046;&#20302;&#31209;&#23398;&#20064;&#30340;Transformer&#23618;
&lt;/p&gt;
&lt;p&gt;
ResidualTransformer: Residual Low-rank Learning with Weight-sharing for Transformer Layers. (arXiv:2310.02489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#23558;&#27169;&#22411;&#30340;&#22823;&#23567;&#20943;&#23567;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37096;&#32626;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#21040;&#22987;&#32456;&#24320;&#21551;&#35774;&#22791;&#19978;&#26102;&#65292;&#20869;&#23384;&#38480;&#21046;&#26159;&#19968;&#20010;&#20027;&#35201;&#20851;&#27880;&#28857;&#20043;&#19968;&#12290;&#34429;&#28982;&#20351;&#29992;&#36275;&#22815;&#22823;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#24471;&#21040;&#30340;&#26356;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#65292;&#20294;&#20351;&#20854;&#36866;&#24212;&#35774;&#22791;&#20869;&#23384;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;Transformer&#32534;&#30721;&#22120;&#23618;&#20043;&#38388;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24182;&#20551;&#35774;&#29305;&#27530;&#30340;&#26435;&#37325;&#32452;&#21512;&#21644;&#32467;&#26500;&#65292;&#26469;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#21463;ResNet&#21644;&#26368;&#26032;&#30340;LoRA&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ResidualTransformer&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;Transformer&#23618;&#20013;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#21253;&#25324;1&#65289;&#19982;&#20854;&#30456;&#37051;&#23618;&#20849;&#20139;&#30340;&#28385;&#31209;&#32452;&#20214;&#65292;&#21644;2&#65289;&#20165;&#23646;&#20110;&#23427;&#33258;&#24049;&#30340;&#29420;&#29305;&#20302;&#31209;&#32452;&#20214;&#12290;&#20302;&#31209;&#30697;&#38453;&#21482;&#21344;&#27169;&#22411;&#22823;&#23567;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28155;&#21152;&#23545;&#35282;&#32447;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#20302;&#31209;&#30697;&#38453;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;10k&#23567;&#26102;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ResidualTransformer&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;Transformer&#27169;&#22411;&#65292;&#19988;&#27169;&#22411;&#22823;&#23567;&#24471;&#21040;&#20102;&#26174;&#33879;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory constraint of always-on devices is one of the major concerns when deploying speech processing models on these devices. While larger models trained with sufficiently large amount of data generally perform better, making them fit in the device memory is a demanding challenge. In this paper, we aim to reduce model size by reparameterizing model weights across Transformer encoder layers and assuming a special weight composition and structure. More specifically, inspired by ResNet and the more recent LoRA work, we propose an approach named ResidualTransformer, where each weight matrix in a Transformer layer comprises 1) a shared full-rank component with its adjacent layers, and 2) a unique low-rank component to itself. The low-rank matrices only account for a small amount of model size increase. In addition, we add diagonal weight matrices to improve modeling capacity of the low-rank matrices. Experiments of our 10k-hour speech recognition and speech translation tasks show that the T
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#25913;&#21892;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01119</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01119
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21487;&#20197;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#25913;&#21892;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#20351;&#23427;&#20204;&#33021;&#22815;&#20197;&#30456;&#23545;&#36739;&#23569;&#30340;&#26631;&#35760;&#26679;&#26412;&#25512;&#24191;&#21040;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#37096;&#32626;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#29992;&#36275;&#22815;&#22810;&#30340;&#26631;&#35760;&#26679;&#26412;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#23427;&#20204;&#21487;&#20197;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26679;&#26412;&#33719;&#21462;&#36215;&#26469;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#36861;&#27714;&#20004;&#20840;&#20854;&#32654;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;&#25945;&#24072;LLMs&#29983;&#25104;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#21512;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#25913;&#21892;&#36739;&#23567;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#21644;&#20004;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#29983;&#25104;&#21644;&#27880;&#37322;&#37117;&#26174;&#33879;&#25552;&#39640;&#20102;&#30456;&#24212;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#21482;&#38656;&#35201;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25935;&#24863;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#21098;&#20999;&#38408;&#20540;&#21644;&#25104;&#26412;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36991;&#20813;&#22312;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20013;&#20135;&#29983;&#30340;&#38544;&#31169;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#20102;&#21160;&#24577;&#35843;&#25972;&#21098;&#20999;&#38408;&#20540;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00829</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#25935;&#24863;&#24230;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Sensitivity Optimization in Differentially Private Learning. (arXiv:2310.00829v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#25935;&#24863;&#24230;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#21098;&#20999;&#38408;&#20540;&#21644;&#25104;&#26412;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#36991;&#20813;&#22312;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20013;&#20135;&#29983;&#30340;&#38544;&#31169;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#20102;&#21160;&#24577;&#35843;&#25972;&#21098;&#20999;&#38408;&#20540;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38480;&#21046;&#20010;&#20307;&#23545;&#20248;&#21270;&#36807;&#31243;&#30340;&#36129;&#29486;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#24179;&#22343;&#21644;&#25209;&#27425;&#28040;&#27602;&#20043;&#21069;&#65292;&#23558;&#20010;&#20307;&#30340;&#26799;&#24230;&#30340;2-&#33539;&#25968;&#21098;&#20999;&#21040;&#39044;&#23450;&#30340;&#38408;&#20540;&#12290;&#36825;&#31181;&#36873;&#25321;&#22312;&#20004;&#20010;&#30456;&#23545;&#30340;&#26041;&#38754;&#23545;&#20248;&#21270;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65306;&#22312;&#36739;&#20302;&#20540;&#26102;&#36807;&#24230;&#21098;&#20999;&#21152;&#21095;&#20102;&#20559;&#24046;&#65292;&#32780;&#22312;&#36739;&#39640;&#20540;&#26102;&#22686;&#21152;&#20102;&#28040;&#27602;&#22122;&#22768;&#12290;&#36825;&#20010;&#36873;&#25321;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#26550;&#26500;&#65292;&#29978;&#33267;&#22312;&#21516;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#20013;&#20063;&#26377;&#25152;&#19981;&#21516;&#65292;&#38656;&#35201;&#36890;&#36807;&#32593;&#26684;&#25628;&#32034;&#26469;&#36827;&#34892;&#32454;&#33268;&#35843;&#25972;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#36229;&#21442;&#25968;&#35843;&#20248;&#20013;&#20135;&#29983;&#30340;&#38544;&#31169;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#20248;&#21270;&#21098;&#20999;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38408;&#20540;&#35270;&#20026;&#39069;&#22806;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#24314;&#31435;&#20102;&#38408;&#20540;&#21644;&#25104;&#26412;&#20989;&#25968;&#20043;&#38388;&#30340;&#28165;&#26224;&#20851;&#31995;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#21069;&#32773;&#65292;&#33258;&#21160;&#33719;&#24471;&#19968;&#20010;&#35843;&#25972;&#22909;&#30340;&#21098;&#20999;&#38408;&#20540;&#65292;&#20174;&#32780;&#22312;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#20013;&#38477;&#20302;&#20102;&#35843;&#20248;&#30340;&#38544;&#31169;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training differentially private machine learning models requires constraining an individual's contribution to the optimization process. This is achieved by clipping the $2$-norm of their gradient at a predetermined threshold prior to averaging and batch sanitization. This selection adversely influences optimization in two opposing ways: it either exacerbates the bias due to excessive clipping at lower values, or augments sanitization noise at higher values. The choice significantly hinges on factors such as the dataset, model architecture, and even varies within the same optimization, demanding meticulous tuning usually accomplished through a grid search. In order to circumvent the privacy expenses incurred in hyperparameter tuning, we present a novel approach to dynamically optimize the clipping threshold. We treat this threshold as an additional learnable parameter, establishing a clean relationship between the threshold and the cost function. This allows us to optimize the former wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00806</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00806
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#35774;&#35745;&#21407;&#21017;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#24335;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#30340;&#36890;&#29992;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#65292;&#23454;&#29616;&#20102;&#26080;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#31867;&#22411;&#31639;&#27861;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#30340;&#27867;&#21270;&#21644;&#20248;&#21270;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#29702;&#35770;&#65292;&#29992;&#20110;&#20248;&#21270;&#39057;&#29575;&#35823;&#24046;&#27714;&#21644;&#30340;&#24207;&#36143;&#23398;&#20064;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#21407;&#21017;&#24471;&#21040;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#21644;&#36172;&#21338;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#27599;&#19968;&#36718;&#29983;&#25104;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#21518;&#39564;&#36827;&#34892;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20248;&#21270;&#30446;&#26631;&#26159;&#21019;&#24314;&#8220;&#31639;&#27861;&#20449;&#24565;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#31639;&#27861;&#20449;&#24687;&#27604;&#8221;&#65292;&#26377;&#25928;&#22320;&#34920;&#24449;&#20102;&#20219;&#20309;&#31639;&#27861;&#30340;&#39057;&#29575;&#35823;&#24046;&#30340;&#20869;&#22312;&#22797;&#26434;&#24230;&#24230;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#24335;&#31639;&#27861;&#26080;&#20808;&#39564;&#22320;&#24182;&#19988;&#22312;&#23545;&#25239;&#24615;&#29615;&#22659;&#20013;&#20197;&#36890;&#29992;&#21644;&#26368;&#20248;&#26041;&#24335;&#24212;&#29992;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31639;&#27861;&#31616;&#21333;&#19988;&#36890;&#24120;&#23481;&#26131;&#23454;&#29616;&#12290;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#22312;&#38543;&#26426;&#12289;&#23545;&#25239;&#21644;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We develop a general theory to optimize the frequentist regret for sequential learning problems, where efficient bandit and reinforcement learning algorithms can be derived from unified Bayesian principles. We propose a novel optimization approach to generate "algorithmic beliefs" at each round, and use Bayesian posteriors to make decisions. The optimization objective to create "algorithmic beliefs," which we term "Algorithmic Information Ratio," represents an intrinsic complexity measure that effectively characterizes the frequentist regret of any algorithm. To the best of our knowledge, this is the first systematical approach to make Bayesian-type algorithms prior-free and applicable to adversarial settings, in a generic and optimal manner. Moreover, the algorithms are simple and often efficient to implement. As a major application, we present a novel algorithm for multi-armed bandits that achieves the "best-of-all-worlds" empirical performance in the stochastic, adversarial, and non
&lt;/p&gt;</description></item><item><title>USM-SCD&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08023</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65288;USM-SCD&#65289;
&lt;/p&gt;
&lt;p&gt;
USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08023
&lt;/p&gt;
&lt;p&gt;
USM-SCD&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#65288;USM-SCD&#65289;&#65292;&#21487;&#20197;&#21516;&#26102;&#26816;&#27979;&#28436;&#35762;&#32773;&#36716;&#25442;&#24182;&#20026;96&#31181;&#35821;&#35328;&#25191;&#34892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#26159;&#20174;&#19968;&#20010;&#32463;&#36807;&#22823;&#37327;&#21463;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#25968;&#25454;&#35757;&#32451;&#30340;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#32780;&#26469;&#30340;&#65292;&#23637;&#31034;&#20102;&#20174;&#22823;&#22411;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#35843;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#22810;&#35821;&#31181;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;USM-SCD&#27169;&#22411;&#21487;&#20197;&#22312;&#30001;&#26469;&#33258;96&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#26500;&#25104;&#30340;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;&#36229;&#36807;75%&#30340;&#24179;&#22343;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;F1&#24471;&#20998;&#12290;&#22312;&#32654;&#24335;&#33521;&#35821;&#19978;&#65292;USM-SCD&#27169;&#22411;&#21487;&#20197;&#22312;&#21508;&#31181;&#20844;&#20849;&#21644;&#20869;&#37096;&#27979;&#35797;&#38598;&#19978;&#23454;&#29616;85.8%&#30340;&#28436;&#35762;&#32773;&#36716;&#25442;&#26816;&#27979;F1&#24471;&#20998;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;&#21333;&#35821;&#35328;&#22522;&#20934;&#27169;&#22411;&#25552;&#39640;&#20102;21%&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21482;&#38656;&#35201;&#24494;&#35843;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#22235;&#20998;&#20043;&#19968;&#23601;&#21487;&#20197;&#23454;&#29616;&#26368;&#20339;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a multilingual speaker change detection model (USM-SCD) that can simultaneously detect speaker turns and perform ASR for 96 languages. This model is adapted from a speech foundation model trained on a large quantity of supervised and unsupervised data, demonstrating the utility of fine-tuning from a large generic foundation model for a downstream task. We analyze the performance of this multilingual speaker change detection model through a series of ablation studies. We show that the USM-SCD model can achieve more than 75% average speaker change detection F1 score across a test set that consists of data from 96 languages. On American English, the USM-SCD model can achieve an 85.8% speaker change detection F1 score across various public and internal test sets, beating the previous monolingual baseline model by 21% relative. We also show that we only need to fine-tune one-quarter of the trainable model parameters to achieve the best model performance. The USM-SCD model exhib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.04339</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20984;&#20248;&#21270;&#23454;&#29616;&#22312;&#32447;&#23376;&#27169;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#24182;&#23558;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;&#38382;&#39064;&#20013;&#12290;&#36825;&#31181;&#24402;&#32422;&#26041;&#24335;&#21487;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35774;&#32622;&#19979;&#30340;&#19968;&#33324;&#24615;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#22312;&#19968;&#33324;&#24615;&#27169;&#24615;&#32422;&#26463;&#19979;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#20248;&#21270;&#19968;&#31867;&#22823;&#22411;&#23376;&#27169;&#20989;&#25968;&#65292;&#21363;&#21152;&#26435;&#38408;&#20540;&#21183;&#20989;&#25968;&#65292;&#21487;&#20197;&#24402;&#32422;&#21040;&#22312;&#32447;&#20984;&#20248;&#21270;(OCO)&#38382;&#39064;&#12290;&#36825;&#26159;&#22240;&#20026;&#36825;&#20010;&#31867;&#21035;&#30340;&#20989;&#25968;&#21487;&#20197;&#36827;&#34892;&#20985;&#26494;&#24347;;&#22240;&#27492;&#65292;&#32467;&#21512;&#36866;&#24403;&#30340;&#33293;&#20837;&#26041;&#26696;&#65292;OCO&#31574;&#30053;&#21487;&#20197;&#22312;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31616;&#21270;&#26041;&#24335;&#21487;&#20197;&#24212;&#29992;&#22312;&#35768;&#22810;&#19981;&#21516;&#29256;&#26412;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#21253;&#25324;&#21160;&#24577;&#36951;&#25022;&#12289;&#24378;&#30423;&#21644;&#20048;&#35266;&#23398;&#20064;&#31561;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study monotone submodular maximization under general matroid constraints in the online setting. We prove that online optimization of a large class of submodular functions, namely, weighted threshold potential functions, reduces to online convex optimization (OCO). This is precisely because functions in this class admit a concave relaxation; as a result, OCO policies, coupled with an appropriate rounding scheme, can be used to achieve sublinear regret in the combinatorial setting. We show that our reduction extends to many different versions of the online learning problem, including the dynamic regret, bandit, and optimistic-learning settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#21442;&#25968;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#27169;&#22411;&#35757;&#32451;&#19981;&#21463;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#24212;&#29992;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#26085;&#26412;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.03447</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#23454;&#29616;&#23485;&#24102;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;: &#24320;&#21457;&#19982;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#21442;&#25968;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#26550;&#26500;&#65292;&#27169;&#22411;&#35757;&#32451;&#19981;&#21463;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#39564;&#35777;&#21644;&#24212;&#29992;&#23454;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#21487;&#29992;&#20110;&#29983;&#25104;&#26085;&#26412;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#31639;&#23376;&#65288;GANO&#65289;&#30340;&#25968;&#25454;&#39537;&#21160;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#24320;&#25918;&#33719;&#21462;&#30340;&#24378;&#38663;&#21160;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26681;&#25454;&#30697;&#38663;&#32423;&#65288;M&#65289;&#12289;&#26029;&#35010;&#36317;&#31163;&#65288;R_{rup}&#65289;&#12289;&#39030;&#37096;30m&#22788;&#30340;&#26102;&#38388;&#24179;&#22343;&#21098;&#20999;&#27874;&#36895;&#24230;&#65288;V_{S30}&#65289;&#21644;&#26500;&#36896;&#29615;&#22659;&#25110;&#26029;&#23618;&#31867;&#22411;&#29983;&#25104;&#19977;&#20998;&#37327;&#21152;&#36895;&#24230;&#26102;&#38388;&#21382;&#21490;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#31639;&#23376;&#65292;&#36825;&#26159;&#19968;&#31181;&#20998;&#36776;&#29575;&#26080;&#20851;&#30340;&#26550;&#26500;&#65292;&#21487;&#20197;&#20445;&#35777;&#27169;&#22411;&#35757;&#32451;&#19982;&#25968;&#25454;&#37319;&#26679;&#39057;&#29575;&#26080;&#20851;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26465;&#20214;&#22320;&#38754;&#36816;&#21160;&#21512;&#25104;&#31639;&#27861;&#65288;&#20197;&#19979;&#31616;&#31216;cGM-GANO&#65289;&#24182;&#35752;&#35770;&#20854;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#21335;&#21152;&#24030;&#22320;&#38663;&#20013;&#24515;&#65288;SCEC&#65289;&#23485;&#24102;&#24179;&#21488;&#65288;BBP&#65289;&#20135;&#29983;&#30340;&#27169;&#25311;&#22320;&#38663;&#21160;&#39564;&#35777;&#20102;cGM-GANO&#26694;&#26550;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#26085;&#26412;&#30340;KiK-net&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;cGM-GANO&#65292;&#34920;&#26126;&#35813;&#26694;&#26550;&#21487;&#20197;&#37325;&#26032;&#29983;&#25104;&#22320;&#38663;&#21160;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a data-driven model for ground-motion synthesis using a Generative Adversarial Neural Operator (GANO) that combines recent advancements in machine learning and open access strong motion data sets to generate three-component acceleration time histories conditioned on moment magnitude ($M$), rupture distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$ ($V_{S30}$), and tectonic environment or style of faulting. We use Neural Operators, a resolution invariant architecture that guarantees that the model training is independent of the data sampling frequency. We first present the conditional ground-motion synthesis algorithm (referred to heretofore as cGM-GANO) and discuss its advantages compared to previous work. Next, we verify the cGM-GANO framework using simulated ground motions generated with the Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the framework can rec
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#65292;&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.02045</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#22686;&#24378;&#35780;&#35770;&#25991;&#26412;&#30340;&#22810;&#39046;&#22495;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02045
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#65292;&#26412;&#35770;&#25991;&#25506;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24212;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#36890;&#36807;&#25552;&#31034;&#31574;&#30053;&#36827;&#19968;&#27493;&#22686;&#24378;LLMs&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#24212;&#29992;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;LLMs&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25552;&#31034;&#36807;&#31243;&#36827;&#34892;&#20102;&#24314;&#27169;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#30340;&#26032;&#39062;&#31574;&#30053;&#65306;&#35282;&#33394;&#25198;&#28436;&#65288;RP&#65289;&#25552;&#31034;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;RP-CoT&#25552;&#31034;&#31574;&#30053;&#65292;&#23427;&#26159;RP&#25552;&#31034;&#21644;CoT&#25552;&#31034;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#24773;&#24863;&#20998;&#26512;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#31354;&#38388;&#21644;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#27492;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#24046;&#36317;&#20381;&#36182;&#30340;&#19978;&#30028;&#65292;&#24182;&#22312;&#36951;&#25022;&#19978;&#30028;&#26041;&#38754;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15107</link><description>&lt;p&gt;
&#24102;&#26377;&#20391;&#38754;&#35266;&#27979;&#30340;&#38543;&#26426;&#22270;&#36172;&#21338;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stochastic Graph Bandit Learning with Side-Observations. (arXiv:2308.15107v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#31354;&#38388;&#21644;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#27492;&#21069;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#22870;&#21169;&#24046;&#36317;&#20381;&#36182;&#30340;&#19978;&#30028;&#65292;&#24182;&#22312;&#36951;&#25022;&#19978;&#30028;&#26041;&#38754;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#20989;&#25968;&#31354;&#38388;&#21644;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#28508;&#22312;&#30340;&#22270;&#32467;&#26500;&#21644;&#22870;&#21169;&#24046;&#36317;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#22312;&#36825;&#20010;&#38543;&#26426;&#35774;&#32622;&#19979;&#25552;&#20379;&#22870;&#21169;&#24046;&#36317;&#20381;&#36182;&#30340;&#19978;&#30028;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;[35]&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#19982;[31,33,35]&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#22270;&#24418;&#25968;&#37327;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36951;&#25022;&#19978;&#30028;&#26041;&#38754;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#25105;&#20204;&#31639;&#27861;&#22312;&#25512;&#36827;&#20855;&#26377;&#22270;&#21453;&#39304;&#30340;&#38543;&#26426;&#32972;&#26223;&#36172;&#21338;&#39046;&#22495;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#21508;&#20010;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#24320;&#36767;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the stochastic contextual bandit with general function space and graph feedback. We propose an algorithm that addresses this problem by adapting to both the underlying graph structures and reward gaps. To the best of our knowledge, our algorithm is the first to provide a gap-dependent upper bound in this stochastic setting, bridging the research gap left by the work in [35]. In comparison to [31,33,35], our method offers improved regret upper bounds and does not require knowledge of graphical quantities. We conduct numerical experiments to demonstrate the computational efficiency and effectiveness of our approach in terms of regret upper bounds. These findings highlight the significance of our algorithm in advancing the field of stochastic contextual bandits with graph feedback, opening up avenues for practical applications in various domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31283;&#23450;&#20108;&#27425;&#27169;&#22411;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#30340;&#20108;&#27425;&#31995;&#32479;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13819</link><description>&lt;p&gt;
&#20445;&#35777;&#31283;&#23450;&#30340;&#20108;&#27425;&#27169;&#22411;&#21450;&#20854;&#22312;SINDy&#21644;&#25805;&#20316;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference. (arXiv:2308.13819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31283;&#23450;&#20108;&#27425;&#27169;&#22411;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#26500;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#31283;&#23450;&#24615;&#30340;&#20108;&#27425;&#31995;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#23427;&#23558;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12289;&#22522;&#20110;&#29289;&#29702;&#24314;&#27169;&#21644;&#32463;&#39564;&#30693;&#35782;&#30456;&#32467;&#21512;&#12290;&#22312;&#24037;&#31243;&#35774;&#35745;&#24490;&#29615;&#21644;&#25968;&#23383;&#21452;&#32990;&#32974;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#20027;&#35201;&#20851;&#27880;&#19968;&#31181;&#25805;&#20316;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#27169;&#22411;&#32467;&#26500;&#19978;&#25552;&#20986;&#20808;&#39564;&#20551;&#35774;&#65288;&#36890;&#24120;&#30001;&#24050;&#30693;&#29289;&#29702;&#23398;&#25110;&#19987;&#23478;&#32473;&#20986;&#65289;&#65292;&#20174;&#32780;&#26500;&#24314;&#20302;&#32500;&#21160;&#24577;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#32622;&#36866;&#24403;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#23398;&#20064;&#27169;&#22411;&#30340;&#36816;&#31639;&#31526;&#20197;&#36827;&#34892;&#25512;&#26029;&#12290;&#21160;&#24577;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#26159;&#31283;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#27169;&#22411;&#26080;&#27861;&#20445;&#35777;&#36825;&#31181;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#20108;&#27425;&#27169;&#22411;&#30340;&#25512;&#26029;&#20844;&#24335;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35774;&#35745;&#19978;&#26159;&#31283;&#23450;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#31283;&#23450;&#30340;&#20108;&#27425;&#31995;&#32479;&#30340;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#27809;&#26377;&#31283;&#23450;&#28857;&#20294;&#26377;&#30028;&#30340;&#20108;&#27425;&#31995;&#32479;&#65306;
&lt;/p&gt;
&lt;p&gt;
Scientific machine learning for learning dynamical systems is a powerful tool that combines data-driven modeling models, physics-based modeling, and empirical knowledge. It plays an essential role in an engineering design cycle and digital twinning. In this work, we primarily focus on an operator inference methodology that builds dynamical models, preferably in low-dimension, with a prior hypothesis on the model structure, often determined by known physics or given by experts. Then, for inference, we aim to learn the operators of a model by setting up an appropriate optimization problem. One of the critical properties of dynamical systems is{stability. However, such a property is not guaranteed by the inferred models. In this work, we propose inference formulations to learn quadratic models, which are stable by design. Precisely, we discuss the parameterization of quadratic systems that are locally and globally stable. Moreover, for quadratic systems with no stable point yet bounded (e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STEM&#30340;&#26032;&#33539;&#20363;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#36127;&#20256;&#36882;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;STEM&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#20013;&#27491;&#21453;&#39304;&#25968;&#37327;&#30340;&#30456;&#23545;&#27604;&#20363;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13537</link><description>&lt;p&gt;
STEM:&#37322;&#25918;Embedding&#22312;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
STEM: Unleashing the Power of Embeddings for Multi-task Recommendation. (arXiv:2308.13537v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;STEM&#30340;&#26032;&#33539;&#20363;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#25512;&#33616;&#20013;&#30340;&#36127;&#20256;&#36882;&#38382;&#39064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;STEM&#36890;&#36807;&#26681;&#25454;&#26679;&#26412;&#20013;&#27491;&#21453;&#39304;&#25968;&#37327;&#30340;&#30456;&#23545;&#27604;&#20363;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#22810;&#20010;&#30446;&#26631;&#12290;MTL&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#36127;&#20256;&#36882;&#30340;&#21457;&#29983;&#65292;&#21363;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#30340;&#20914;&#31361;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#36807;&#23558;&#25152;&#26377;&#26679;&#26412;&#35270;&#20026;&#19968;&#20010;&#25972;&#20307;&#26469;&#25506;&#32034;&#36127;&#20256;&#36882;&#65292;&#24573;&#35270;&#20102;&#20854;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26681;&#25454;&#20219;&#21153;&#20043;&#38388;&#27491;&#21453;&#39304;&#30340;&#30456;&#23545;&#25968;&#37327;&#23558;&#26679;&#26412;&#36827;&#34892;&#32454;&#20998;&#65292;&#28145;&#20837;&#30740;&#31350;&#26679;&#26412;&#30340;&#22797;&#26434;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#29616;&#26377;MTL&#26041;&#27861;&#22312;&#25910;&#21040;&#21508;&#20219;&#21153;&#31867;&#20284;&#21453;&#39304;&#30340;&#26679;&#26412;&#19978;&#20173;&#28982;&#23384;&#22312;&#36127;&#20256;&#36882;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#20849;&#20139;&#23884;&#20837;&#30340;&#33539;&#20363;&#65292;&#24182;&#19988;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#30340;&#22833;&#36133;&#21487;&#20197;&#24402;&#22240;&#20110;&#20351;&#29992;&#36825;&#31181;&#36890;&#29992;&#23884;&#20837;&#26469;&#24314;&#27169;&#19981;&#21516;&#29992;&#25143;&#20559;&#22909;&#30340;&#26377;&#38480;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has gained significant popularity in recommendation systems as it enables the simultaneous optimization of multiple objectives. A key challenge in MTL is the occurrence of negative transfer, where the performance of certain tasks deteriorates due to conflicts between tasks. Existing research has explored negative transfer by treating all samples as a whole, overlooking the inherent complexities within them. To this end, we delve into the intricacies of samples by splitting them based on the relative amount of positive feedback among tasks. Surprisingly, negative transfer still occurs in existing MTL methods on samples that receive comparable feedback across tasks. It is worth noting that existing methods commonly employ a shared-embedding paradigm, and we hypothesize that their failure can be attributed to the limited capacity of modeling diverse user preferences across tasks using such universal embeddings.  In this paper, we introduce a novel paradigm called
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#19978;&#37319;&#26679;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2308.11700</link><description>&lt;p&gt;
SuperCalo: &#33021;&#37327;&#27785;&#31215;&#37327;&#27169;&#25311;&#30340;&#36229;&#20998;&#36776;&#29575;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SuperCalo: Calorimeter shower super-resolution. (arXiv:2308.11700v1 [physics.ins-det])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#24555;&#36895;&#19978;&#37319;&#26679;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#26159;&#22823;&#22411;&#24378;&#23376;&#23545;&#25758;&#26426;&#35745;&#31639;&#27969;&#31243;&#20013;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#24037;&#20316;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26469;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#35768;&#22810;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#38388;&#19978;&#26080;&#27861;&#24456;&#22909;&#22320;&#36866;&#24212;&#39640;&#32500;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SuperCalo&#30340;&#22522;&#20110;&#27969;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#39640;&#32500;&#32454;&#31890;&#24230;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#21487;&#20197;&#20174;&#31895;&#31890;&#24230;&#27169;&#25311;&#20013;&#24555;&#36895;&#19978;&#37319;&#26679;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#19982;&#24555;&#36895;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#27169;&#22411;&#30456;&#20851;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#20869;&#23384;&#38656;&#27714;&#21644;&#29983;&#25104;&#26102;&#38388;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#30001;SuperCalo&#19978;&#37319;&#26679;&#24471;&#21040;&#30340;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#20855;&#26377;&#39640;&#24230;&#21464;&#21270;&#30340;&#29305;&#28857;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#20174;&#36739;&#23569;&#30340;&#31895;&#31890;&#24230;&#27169;&#25311;&#20013;&#19978;&#37319;&#26679;&#20986;&#22810;&#20010;&#39640;&#32500;&#33021;&#37327;&#27785;&#31215;&#27169;&#25311;&#65292;&#20197;&#39640;&#20445;&#30495;&#24230;&#30340;&#26041;&#24335;&#36827;&#19968;&#27493;&#20943;&#23569;&#29983;&#25104;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calorimeter shower simulation is a major bottleneck in the Large Hadron Collider computational pipeline. There have been recent efforts to employ deep-generative surrogate models to overcome this challenge. However, many of best performing models have training and generation times that do not scale well to high-dimensional calorimeter showers. In this work, we introduce SuperCalo, a flow-based super-resolution model, and demonstrate that high-dimensional fine-grained calorimeter showers can be quickly upsampled from coarse-grained showers. This novel approach presents a way to reduce computational cost, memory requirements and generation time associated with fast calorimeter simulation models. Additionally, we show that the showers upsampled by SuperCalo possess a high degree of variation. This allows a large number of high-dimensional calorimeter showers to be upsampled from much fewer coarse showers with high-fidelity, which results in additional reduction in generation time.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#35813;&#31639;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#19982;&#31169;&#23494;&#30340;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2308.03735</link><description>&lt;p&gt;
&#38543;&#26426;&#31639;&#27861;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Randomized algorithms for precise measurement of differentially-private, personalized recommendations. (arXiv:2308.03735v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03735
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#27979;&#37327;&#24046;&#20998;&#38544;&#31169;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#36890;&#36807;&#31163;&#32447;&#23454;&#39564;&#65292;&#35813;&#31639;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#19982;&#31169;&#23494;&#30340;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#26159;&#24403;&#20170;&#20114;&#32852;&#32593;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#24110;&#21161;&#33402;&#26415;&#23478;&#21644;&#21019;&#20316;&#32773;&#21560;&#24341;&#24863;&#20852;&#36259;&#30340;&#29992;&#25143;&#65292;&#21516;&#26102;&#20063;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#26032;&#30340;&#26377;&#36259;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20010;&#20154;&#25968;&#25454;&#21644;&#25968;&#25454;&#38544;&#31169;&#30340;&#21382;&#21490;&#19978;&#31895;&#24515;&#23545;&#24453;&#65292;&#35768;&#22810;&#29992;&#25143;&#23545;&#20010;&#24615;&#21270;&#25512;&#33616;&#24179;&#21488;&#25345;&#24576;&#30097;&#24577;&#24230;&#12290;&#29616;&#22312;&#65292;&#20381;&#36182;&#20110;&#20010;&#24615;&#21270;&#25512;&#33616;&#30340;&#20225;&#19994;&#27491;&#36827;&#20837;&#19968;&#20010;&#26032;&#30340;&#33539;&#20363;&#65292;&#38656;&#35201;&#23545;&#20182;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#25913;&#36827;&#65292;&#20197;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25512;&#33616;&#31639;&#27861;&#65292;&#26082;&#21487;&#20197;&#23454;&#29616;&#31934;&#30830;&#27979;&#37327;&#65292;&#21448;&#21487;&#20197;&#20445;&#25252;&#24046;&#20998;&#38544;&#31169;&#12290;&#25105;&#20204;&#20197;&#24191;&#21578;&#20026;&#20363;&#24212;&#29992;&#65292;&#24182;&#36827;&#34892;&#31163;&#32447;&#23454;&#39564;&#65292;&#37327;&#21270;&#25552;&#20986;&#30340;&#38544;&#31169;&#20445;&#25252;&#31639;&#27861;&#23545;&#29992;&#25143;&#20307;&#39564;&#12289;&#24191;&#21578;&#21830;&#20215;&#20540;&#21644;&#24179;&#21488;&#25910;&#20837;&#31561;&#20851;&#38190;&#25351;&#26631;&#30340;&#24433;&#21709;&#65292;&#19982;&#65288;&#31169;&#23494;&#30340;&#65289;&#38750;&#20010;&#24615;&#21270;&#21644;&#38750;&#31169;&#23494;&#30340;&#20010;&#24615;&#21270;&#23454;&#29616;&#30340;&#26497;&#31471;&#24773;&#20917;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations form an important part of today's internet ecosystem, helping artists and creators to reach interested users, and helping users to discover new and engaging content. However, many users today are skeptical of platforms that personalize recommendations, in part due to historically careless treatment of personal data and data privacy. Now, businesses that rely on personalized recommendations are entering a new paradigm, where many of their systems must be overhauled to be privacy-first. In this article, we propose an algorithm for personalized recommendations that facilitates both precise and differentially-private measurement. We consider advertising as an example application, and conduct offline experiments to quantify how the proposed privacy-preserving algorithm affects key metrics related to user experience, advertiser value, and platform revenue compared to the extremes of both (private) non-personalized and non-private, personalized implementations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#22686;&#24378;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#26080;&#32447;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#25910;&#25947;&#36895;&#24230;&#30340;&#26368;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.01562</link><description>&lt;p&gt;
&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65306;&#21098;&#26525;&#35299;&#20915;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21098;&#26525;&#22686;&#24378;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#24102;&#23485;&#31232;&#32570;&#21644;&#31995;&#32479;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#22411;&#21098;&#26525;&#21644;&#26080;&#32447;&#36890;&#20449;&#30340;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#25910;&#25947;&#36895;&#24230;&#30340;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#32456;&#31471;&#29992;&#25143;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#20043;&#38388;&#23384;&#22312;&#22810;&#20010;&#23618;&#32423;&#65292;&#29992;&#25143;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#30005;&#27744;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;&#26381;&#21153;&#22522;&#31449;&#20855;&#26377;&#22266;&#23450;&#30340;&#24102;&#23485;&#12290;&#37492;&#20110;&#36825;&#20123;&#23454;&#38469;&#32422;&#26463;&#21644;&#31995;&#32479;&#27169;&#22411;&#65292;&#26412;&#25991;&#21033;&#29992;&#27169;&#22411;&#21098;&#26525;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#24322;&#26500;&#32593;&#32476;&#30340;&#21098;&#26525;&#22686;&#24378;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;PHFL&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27169;&#22411;&#21098;&#26525;&#21644;&#23458;&#25143;&#31471;&#19982;&#20851;&#32852;&#22522;&#31449;&#20043;&#38388;&#30340;&#26080;&#32447;&#36890;&#20449;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21098;&#26525;&#27604;&#29575;&#12289;&#23458;&#25143;&#31471;&#30340;&#20013;&#22830;&#22788;&#29702;&#22120;&#65288;CPU&#65289;&#39057;&#29575;&#21644;&#20256;&#36755;&#21151;&#29575;&#65292;&#20197;&#22312;&#20005;&#26684;&#30340;&#24310;&#36831;&#21644;&#33021;&#32791;&#32422;&#26463;&#19979;&#26368;&#23567;&#21270;&#25910;&#25947;&#30028;&#30340;&#21487;&#25511;&#39033;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21407;&#22987;&#38382;&#39064;&#19981;&#26159;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#36880;&#27493;&#20984;&#36924;&#36817;&#65288;SCA&#65289;&#26041;&#27861;&#65292;&#32852;&#21512;&#20248;&#21270;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a practical wireless network has many tiers where end users do not directly communicate with the central server, the users' devices have limited computation and battery powers, and the serving base station (BS) has a fixed bandwidth. Owing to these practical constraints and system models, this paper leverages model pruning and proposes a pruning-enabled hierarchical federated learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper bound of the convergence rate that clearly demonstrates the impact of the model pruning and wireless communications between the clients and the associated BS. Then we jointly optimize the model pruning ratio, central processing unit (CPU) frequency and transmission power of the clients in order to minimize the controllable terms of the convergence bound under strict delay and energy constraints. However, since the original problem is not convex, we perform successive convex approximation (SCA) and jointly optimize the parameters fo
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#20316;&#20026;&#20803;&#23398;&#20064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#28145;&#24230;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#26799;&#24230;&#35745;&#31639;&#21644;&#28040;&#22833;&#39118;&#38505;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16424</link><description>&lt;p&gt;
MetaDiff: &#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning. (arXiv:2307.16424v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16424
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26465;&#20214;&#25193;&#25955;&#20316;&#20026;&#20803;&#23398;&#20064;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#28145;&#24230;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#36935;&#21040;&#30340;&#26799;&#24230;&#35745;&#31639;&#21644;&#28040;&#22833;&#39118;&#38505;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#35753;&#28145;&#24230;&#27169;&#22411;&#20855;&#22791;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20174;&#23569;&#37327;&#26679;&#26412;&#24555;&#36895;&#23398;&#20064;&#65292;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#20197;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#24335;&#23398;&#20064;&#28145;&#24230;&#27169;&#22411;&#65292;&#20854;&#20013;&#22806;&#37096;&#24490;&#29615;&#36807;&#31243;&#23398;&#20064;&#20849;&#20139;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65288;&#21363;&#20854;&#36229;&#21442;&#25968;&#65289;&#65292;&#32780;&#20869;&#37096;&#24490;&#29615;&#36807;&#31243;&#21033;&#29992;&#23427;&#26469;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#25968;&#25454;&#23545;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#12290;&#23613;&#31649;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22806;&#37096;&#24490;&#29615;&#36807;&#31243;&#38656;&#35201;&#27839;&#30528;&#20869;&#37096;&#20248;&#21270;&#36335;&#24452;&#35745;&#31639;&#20108;&#38454;&#23548;&#25968;&#65292;&#36825;&#20250;&#24102;&#26469;&#21487;&#35266;&#30340;&#20869;&#23384;&#36127;&#25285;&#21644;&#26799;&#24230;&#28040;&#22833;&#30340;&#39118;&#38505;&#12290;&#21463;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21457;&#29616;&#20869;&#37096;&#24490;&#29615;&#30340;&#26799;&#24230;&#19979;&#38477;&#36807;&#31243;&#23454;&#38469;&#19978;&#21487;&#20197;&#30475;&#20316;&#26159;&#19968;&#20010;&#25193;&#25955;&#30340;&#21453;&#36807;&#31243;&#65288;&#21363;&#21435;&#22122;&#65289;&#65292;&#36825;&#20010;&#25193;&#25955;&#30340;&#30446;&#26631;&#26159;&#24674;&#22797;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipping a deep model the abaility of few-shot learning, i.e., learning quickly from only few examples, is a core challenge for artificial intelligence. Gradient-based meta-learning approaches effectively address the challenge by learning how to learn novel tasks. Its key idea is learning a deep model in a bi-level optimization manner, where the outer-loop process learns a shared gradient descent algorithm (i.e., its hyperparameters), while the inner-loop process leverage it to optimize a task-specific model by using only few labeled data. Although these existing methods have shown superior performance, the outer-loop process requires calculating second-order derivatives along the inner optimization path, which imposes considerable memory burdens and the risk of vanishing gradients. Drawing inspiration from recent progress of diffusion models, we find that the inner-loop gradient descent process can be actually viewed as a reverse process (i.e., denoising) of diffusion where the targe
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09312</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65306;&#25972;&#21512;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#21464;&#25442;&#22120;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09312
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120; (mDT) &#26159;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#20167;&#24680;&#35328;&#35770;&#30340;&#26032;&#39062;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;mDT&#36890;&#36807;&#25972;&#20307;&#20998;&#26512;&#25991;&#26412;&#21644;&#22270;&#20687;&#65292;&#32467;&#21512;&#22270;&#21464;&#25442;&#22120;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#20132;&#32455;&#34701;&#21512;&#23618;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#21487;&#20197;&#26497;&#22823;&#22320;&#25552;&#39640;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22522;&#20110;&#22270;&#30340;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21517;&#20026;&#22810;&#27169;&#24577;&#35752;&#35770;&#21464;&#25442;&#22120;&#65288;mDT&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;&#25991;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#35780;&#35770;&#20026;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#22260;&#32469;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20307;&#20998;&#26512;&#23637;&#24320;&#12290;&#36825;&#26159;&#36890;&#36807;&#21033;&#29992;&#22270;&#21464;&#25442;&#22120;&#26469;&#25429;&#25417;&#35780;&#35770;&#21608;&#22260;&#25972;&#20010;&#35752;&#35770;&#20013;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#65292;&#24182;&#37319;&#29992;&#20132;&#32455;&#34701;&#21512;&#23618;&#26469;&#32452;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#22788;&#29702;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20165;&#22788;&#29702;&#25991;&#26412;&#30340;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#65292;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#25552;&#20379;&#31038;&#20250;&#20215;&#20540;&#30340;&#26410;&#26469;&#24037;&#20316;&#65292;&#24182;&#35748;&#20026;&#25429;&#25417;&#23545;&#35805;&#30340;&#25972;&#20307;&#35270;&#22270;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#26816;&#27979;&#21453;&#31038;&#20250;&#34892;&#20026;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Multi-Modal Discussion Transformer (mDT), a novel multi-modal graph-based transformer model for detecting hate speech in online social networks. In contrast to traditional text-only methods, our approach to labelling a comment as hate speech centers around the holistic analysis of text and images. This is done by leveraging graph transformers to capture the contextual relationships in the entire discussion that surrounds a comment, with interwoven fusion layers to combine text and image embeddings instead of processing different modalities separately. We compare the performance of our model to baselines that only process text; we also conduct extensive ablation studies. We conclude with future work for multimodal solutions to deliver social value in online contexts, arguing that capturing a holistic view of a conversation greatly advances the effort to detect anti-social behavior.
&lt;/p&gt;</description></item><item><title>&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#30740;&#31350;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.08919</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Accuracy versus time frontiers of semi-supervised and self-supervised learning on medical images. (arXiv:2307.08919v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08919
&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21307;&#23398;&#22270;&#20687;&#19978;&#30340;&#20934;&#30830;&#24615;&#19982;&#26102;&#38388;&#21069;&#27839;&#36827;&#34892;&#27604;&#36739;&#65292;&#36890;&#36807;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#30740;&#31350;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#24212;&#29992;&#26469;&#35828;&#65292;&#27599;&#20010;&#22270;&#20687;&#37117;&#24456;&#38590;&#25110;&#26114;&#36149;&#22320;&#33719;&#24471;&#19968;&#20010;&#21487;&#20449;&#30340;&#26631;&#31614;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#27809;&#26377;&#26631;&#31614;&#30340;&#22270;&#20687;&#26356;&#23481;&#26131;&#33719;&#21462;&#12290;&#20004;&#20010;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#37117;&#25215;&#35834;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65306;&#33258;&#30417;&#30563;&#23398;&#20064;&#20165;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#26377;&#29992;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26631;&#35760;&#38598;&#23545;&#36825;&#20123;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#20998;&#31867;&#22120;&#65307;&#21322;&#30417;&#30563;&#23398;&#20064;&#21516;&#26102;&#22312;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#30452;&#25509;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20174;&#20004;&#20010;&#26041;&#21521;&#19978;&#37117;&#22768;&#31216;&#22312;&#38750;&#21307;&#23398;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#30410;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#65292;&#24182;&#19988;&#22823;&#22810;&#21482;&#19982;&#21516;&#19968;&#26041;&#21521;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#26469;&#22238;&#31572;&#20174;&#19994;&#32773;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22312;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#26377;&#38480;&#30340;&#22521;&#35757;&#26102;&#38388;&#39044;&#31639;&#19979;&#65292;&#39069;&#22806;&#30340;&#26080;&#26631;&#31614;&#22270;&#20687;&#33021;&#22815;&#20135;&#29983;&#22810;&#22823;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many applications of classifiers to medical images, a trustworthy label for each image can be difficult or expensive to obtain. In contrast, images without labels are more readily available. Two major research directions both promise that additional unlabeled data can improve classifier performance: self-supervised learning pretrains useful representations on unlabeled data only, then fine-tunes a classifier on these representations via the labeled set; semi-supervised learning directly trains a classifier on labeled and unlabeled data simultaneously. Recent methods from both directions have claimed significant gains on non-medical tasks, but do not systematically assess medical images and mostly compare only to methods in the same direction. This study contributes a carefully-designed benchmark to help answer a practitioner's key question: given a small labeled dataset and a limited budget of hours to spend on training, what gains from additional unlabeled images are possible and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;$k$-means&#21644;$k$-median&#32858;&#31867;&#30340;&#24046;&#20998;&#38544;&#31169;&#27969;&#31639;&#27861;&#65292;&#22312;&#27969;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07449</link><description>&lt;p&gt;
&#25968;&#25454;&#27969;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Clustering in Data Streams. (arXiv:2307.07449v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#38024;&#23545;$k$-means&#21644;$k$-median&#32858;&#31867;&#30340;&#24046;&#20998;&#38544;&#31169;&#27969;&#31639;&#27861;&#65292;&#22312;&#27969;&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#25454;&#38544;&#31169;&#30340;&#20445;&#25252;&#65292;&#24182;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#27169;&#22411;&#26159;&#22788;&#29702;&#22823;&#35268;&#27169;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#27969;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#28857;&#20381;&#27425;&#27969;&#20837;&#65292;&#31639;&#27861;&#21482;&#33021;&#23545;&#25968;&#25454;&#27969;&#36827;&#34892;&#19968;&#27425;&#36941;&#21382;&#65292;&#30446;&#26631;&#26159;&#22312;&#20351;&#29992;&#23613;&#21487;&#33021;&#23569;&#30340;&#31354;&#38388;&#30340;&#21516;&#26102;&#65292;&#22312;&#27969;&#20013;&#36827;&#34892;&#19968;&#20123;&#20998;&#26512;&#12290;&#32858;&#31867;&#38382;&#39064;&#26159;&#22522;&#26412;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#21407;&#35821;&#65292;&#36807;&#21435;&#24050;&#32463;&#23545;&#27969;&#32858;&#31867;&#31639;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#38544;&#31169;&#24050;&#25104;&#20026;&#19968;&#20010;&#26680;&#24515;&#20851;&#27880;&#28857;&#65292;&#38750;&#31169;&#26377;&#32858;&#31867;&#31639;&#27861;&#22312;&#35768;&#22810;&#22330;&#26223;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;$k$-means&#21644;$k$-median&#32858;&#31867;&#30340;&#24046;&#20998;&#31169;&#26377;&#27969;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#38271;&#24230;&#26368;&#22810;&#20026;$T$&#30340;&#27969;&#19978;&#20351;&#29992;$poly(k,d,\log(T))$&#30340;&#31354;&#38388;&#26469;&#23454;&#29616;&#19968;&#20010;&#8220;&#24120;&#25968;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The streaming model is an abstraction of computing over massive data streams, which is a popular way of dealing with large-scale modern data analysis. In this model, there is a stream of data points, one after the other. A streaming algorithm is only allowed one pass over the data stream, and the goal is to perform some analysis during the stream while using as small space as possible.  Clustering problems (such as $k$-means and $k$-median) are fundamental unsupervised machine learning primitives, and streaming clustering algorithms have been extensively studied in the past. However, since data privacy becomes a central concern in many real-world applications, non-private clustering algorithms are not applicable in many scenarios.  In this work, we provide the first differentially private streaming algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean data points over a stream with length at most $T$ using $poly(k,d,\log(T))$ space to achieve a {\it constant} 
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.11335</link><description>&lt;p&gt;
RM-PRT: &#30495;&#23454;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#21644;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks. (arXiv:2306.11335v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#36924;&#30495;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#30340;&#22522;&#20934;&#35780;&#20272;RM-PRT&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#35268;&#27169;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#21644;&#22810;&#27169;&#24577;&#25552;&#31034;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#26174;&#30528;&#25512;&#36827;&#20102;&#26426;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36825;&#19968;&#31361;&#30772;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36825;&#20123;&#24320;&#28304;LLM&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#32479;&#19968;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#20013;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#20934;&#30830;&#29702;&#35299;&#21644;&#25191;&#34892;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#27169;&#25311;&#22120;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#28176;&#36827;&#25512;&#29702;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;RM-PRT&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;RM-PRT&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;Unreal Engine 5&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#20445;&#30495;&#25968;&#23383;&#21452;&#32990;&#32974;&#22330;&#26223;&#65292;&#20854;&#20013;&#21253;&#25324;782&#20010;&#31867;&#21035;&#65292;2023&#20010;&#29289;&#20307;&#65292;&#24182;&#20351;&#29992;ChatGPT&#29983;&#25104;&#20102;15,000&#20010;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#20197;&#35814;&#32454;&#35780;&#20272;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;RM-PRT&#22522;&#20934;&#35780;&#20272;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#25509;&#21463;&#21253;&#21547;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#33258;&#21160;&#36755;&#20986;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the advent of pre-trained large-scale language models (LLMs) like ChatGPT and GPT-4 have significantly advanced the machine's natural language understanding capabilities. This breakthrough has allowed us to seamlessly integrate these open-source LLMs into a unified robot simulator environment to help robots accurately understand and execute human natural language instructions. To this end, in this work, we introduce a realistic robotic manipulation simulator and build a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark on this basis. Specifically, the RM-PRT benchmark builds a new high-fidelity digital twin scene based on Unreal Engine 5, which includes 782 categories, 2023 objects, and 15K natural language instructions generated by ChatGPT for a detailed evaluation of robot manipulation. We propose a general pipeline for the RM-PRT benchmark that takes as input multimodal prompts containing natural language instructions and automatically outputs action
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#22411;&#38169;&#35823;&#25351;&#23450;&#38382;&#39064;&#65292;&#21457;&#29616;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#22312;&#31574;&#30053;&#20989;&#25968;&#36924;&#36817;&#23384;&#22312;&#36739;&#22823;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#20540;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#20013;&#26159;&#21542;&#33021;&#23454;&#29616;&#31867;&#20284;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;</title><link>http://arxiv.org/abs/2306.10694</link><description>&lt;p&gt;
&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#22411;&#38169;&#35823;&#25351;&#23450;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Model-Misspecification in Reinforcement Learning. (arXiv:2306.10694v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10694
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#27169;&#22411;&#38169;&#35823;&#25351;&#23450;&#38382;&#39064;&#65292;&#21457;&#29616;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#22312;&#31574;&#30053;&#20989;&#25968;&#36924;&#36817;&#23384;&#22312;&#36739;&#22823;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#20540;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#20013;&#26159;&#21542;&#33021;&#23454;&#29616;&#31867;&#20284;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#23384;&#22312;&#30097;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#30340;&#25104;&#21151;&#20851;&#38190;&#22312;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#30495;&#23454;&#27169;&#22411;&#26102;&#30340;&#26377;&#25928;&#20989;&#25968;&#36924;&#36817;&#12290;&#29616;&#26377;&#30340;&#26679;&#26412;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20027;&#35201;&#37319;&#29992;&#19977;&#31181;&#20989;&#25968;&#36924;&#36817;&#26041;&#27861;&#65306;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#12289;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#38754;&#23545;&#27169;&#22411;&#38169;&#35823;&#25351;&#23450;&#30340;&#24773;&#20917;&#19979;&#65288;&#30495;&#23454;&#27169;&#22411;&#19982;&#26368;&#20248;&#20989;&#25968;&#36924;&#36817;&#22120;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#65289;&#65292;&#30740;&#31350;&#34920;&#26126;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#21363;&#20351;&#22312;&#31574;&#30053;&#20989;&#25968;&#36924;&#36817;&#23384;&#22312;&#36739;&#22823;&#23616;&#37096;&#26377;&#30028;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#31283;&#20581;&#24615;&#12290;&#20854;&#20013;&#20989;&#25968;&#31867;&#21487;&#33021;&#22312;&#29305;&#23450;&#29366;&#24577;&#21644;&#21160;&#20316;&#19979;&#21576;&#29616;$&#937;(1)$&#30340;&#36924;&#36817;&#35823;&#24046;&#65292;&#20294;&#22312;&#30001;&#31574;&#30053;&#24341;&#36215;&#30340;&#29366;&#24577;&#20998;&#24067;&#19979;&#20445;&#25345;&#36739;&#23567;&#30340;&#24179;&#22343;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;&#20540;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#20013;&#26159;&#21542;&#33021;&#23454;&#29616;&#31867;&#20284;&#30340;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of reinforcement learning (RL) crucially depends on effective function approximation when dealing with complex ground-truth models. Existing sample-efficient RL algorithms primarily employ three approaches to function approximation: policy-based, value-based, and model-based methods. However, in the face of model misspecification (a disparity between the ground-truth and optimal function approximators), it is shown that policy-based approaches can be robust even when the policy function approximation is under a large locally-bounded misspecification error, with which the function class may exhibit a $\Omega(1)$ approximation error in specific states and actions, but remains small on average within a policy-induced state distribution. Yet it remains an open question whether similar robustness can be achieved with value-based and model-based approaches, especially with general function approximation.  To bridge this gap, in this paper we present a unified theoretical framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32039;&#26680;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23454;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;</title><link>http://arxiv.org/abs/2306.10592</link><description>&lt;p&gt;
&#22522;&#20110;&#32039;&#26680;&#30340;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Conditional expectation via compact kernels. (arXiv:2306.10592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32039;&#26680;&#30340;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#26399;&#26395;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23454;&#29616;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#19988;&#25104;&#21151;&#24212;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#12289;&#26465;&#20214;&#26399;&#26395;&#21644;&#27969;&#24418;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#21487;&#20197;&#22312;&#23547;&#25214;&#20004;&#20010;&#38543;&#26426;&#21464;&#37327;&#31215;&#30340;&#26465;&#20214;&#26399;&#26395;&#30340;&#20844;&#20849;&#29615;&#22659;&#19979;&#34920;&#36848;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20010;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#19968;&#31181;&#31639;&#23376;&#29702;&#35770;&#26041;&#27861;&#26469;&#20272;&#35745;&#26465;&#20214;&#26399;&#26395;&#12290;&#26680;&#31215;&#20998;&#31639;&#23376;&#34987;&#29992;&#20316;&#32039;&#33268;&#21270;&#24037;&#20855;&#65292;&#23558;&#20272;&#35745;&#38382;&#39064;&#35774;&#32622;&#20026;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#35813;&#26041;&#31243;&#30340;&#35299;&#34987;&#35777;&#26126;&#23545;&#25968;&#20540;&#36924;&#36817;&#26159;&#31283;&#23450;&#30340;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#25968;&#25454;&#39537;&#21160;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#12290;&#24635;&#20307;&#25216;&#26415;&#26131;&#20110;&#23454;&#29616;&#65292;&#36824;&#23637;&#31034;&#20102;&#20854;&#22312;&#19968;&#20123;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#25104;&#21151;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The separate tasks of denoising, conditional expectation and manifold learning can often be posed in a common setting of finding the conditional expectations arising from a product of two random variables. This paper focuses on this more general problem and describes an operator theoretic approach to estimating the conditional expectation. Kernel integral operators are used as a compactification tool, to set up the estimation problem as a linear inverse problem in a reproducing kernel Hilbert space. This equation is shown to have solutions that are stable to numerical approximation, thus guaranteeing the convergence of data-driven implementations. The overall technique is easy to implement, and their successful application to some real-world problems are also shown.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2306.01951</link><description>&lt;p&gt;
GAD-NR: &#36890;&#36807;&#37051;&#22495;&#37325;&#26500;&#23454;&#29616;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;GAD-NR&#65292;&#19982;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;GAD-NR&#37319;&#29992;&#37051;&#22495;&#37325;&#26500;&#30340;&#26041;&#24335;&#26469;&#26816;&#27979;&#26356;&#22797;&#26434;&#38750;&#32858;&#31867;&#30340;&#32467;&#26500;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#65288;GAD&#65289;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#22270;&#20013;&#30340;&#24322;&#24120;&#33410;&#28857;&#65292;&#22312;&#32593;&#32476;&#23433;&#20840;&#12289;&#27450;&#35784;&#26816;&#27979;&#12289;&#31038;&#20132;&#23186;&#20307;&#22403;&#22334;&#26816;&#27979;&#21644;&#20854;&#20182;&#21508;&#31181;&#39046;&#22495;&#20013;&#26377;&#24212;&#29992;&#12290;GAD&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#65292;&#23427;&#23558;&#22270;&#24418;&#25968;&#25454;&#32534;&#30721;&#25104;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#26681;&#25454;&#36825;&#20123;&#34920;&#31034;&#26469;&#35780;&#20272;&#22270;&#24418;&#30340;&#37325;&#26500;&#36136;&#37327;&#65292;&#20197;&#35782;&#21035;&#24322;&#24120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GAE&#27169;&#22411;&#20027;&#35201;&#38024;&#23545;&#30452;&#25509;&#38142;&#25509;&#37325;&#26500;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36830;&#25509;&#22270;&#20013;&#30340;&#33410;&#28857;&#34987;&#32858;&#31867;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#25797;&#38271;&#26816;&#27979;&#32858;&#31867;&#22411;&#32467;&#26500;&#24322;&#24120;&#65292;&#20294;&#23545;&#19981;&#31526;&#21512;&#32858;&#31867;&#30340;&#26356;&#22797;&#26434;&#30340;&#32467;&#26500;&#24322;&#24120;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#31216;&#20026;GAD-NR&#65292;&#23427;&#26159;GAE&#30340;&#19968;&#20010;&#26032;&#21464;&#20307;&#65292;&#34701;&#21512;&#37051;&#22495;&#37325;&#26500;&#36827;&#34892;&#22270;&#24418;&#24322;&#24120;&#26816;&#27979;&#12290;GAD-NR&#30340;&#30446;&#26631;&#26159;&#37325;&#26500;&#33410;&#28857;&#30340;&#25972;&#20010;&#37051;&#22495;&#65292;&#28085;&#30422;&#26412;&#22320;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes within graphs, finding applications in network security, fraud detection, social media spam detection, and various other domains. A common method for GAD is Graph Auto-Encoders (GAEs), which encode graph data into node representations and identify anomalies by assessing the reconstruction quality of the graphs based on these representations. However, existing GAE models are primarily optimized for direct link reconstruction, resulting in nodes connected in the graph being clustered in the latent space. As a result, they excel at detecting cluster-type structural anomalies but struggle with more complex structural anomalies that do not conform to clusters. To address this limitation, we propose a novel solution called GAD-NR, a new variant of GAE that incorporates neighborhood reconstruction for graph anomaly detection. GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the local structu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.14387</link><description>&lt;p&gt;
AlpacaFarm: &#19968;&#31181;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#27169;&#25311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AlpacaFarm&#30340;&#20302;&#25104;&#26412;&#27169;&#25311;&#22120;&#65292;&#35813;&#27169;&#25311;&#22120;&#20026;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#35774;&#35745;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#24182;&#25552;&#20379;&#21442;&#32771;&#23454;&#29616;&#65292;&#20811;&#26381;&#20102;&#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#12289;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22240;&#20854;&#33391;&#22909;&#30340;&#25351;&#20196;&#36319;&#38543;&#33021;&#21147;&#32780;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#24320;&#21457;&#36825;&#20123;LLMs&#38656;&#35201;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#30340;&#22797;&#26434;&#19988;&#23578;&#19981;&#26126;&#30830;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#23558;&#27492;&#25351;&#20196;&#36319;&#38543;&#36807;&#31243;&#22797;&#21046;&#21644;&#29702;&#35299;&#38754;&#20020;&#19977;&#22823;&#25361;&#25112;&#65306; &#25968;&#25454;&#25910;&#38598;&#30340;&#39640;&#26114;&#25104;&#26412;&#65292;&#32570;&#20047;&#21487;&#20449;&#30340;&#35780;&#20272;&#21644;&#32570;&#20047;&#21442;&#32771;&#26041;&#27861;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;AlpacaFarm&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#36825;&#26159;&#19968;&#20010;&#20302;&#25104;&#26412;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#29992;&#20110;&#20174;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LLM&#25552;&#31034;&#26469;&#27169;&#25311;&#20154;&#31867;&#21453;&#39304;&#65292;&#20854;&#25104;&#26412;&#27604;&#20247;&#21253;&#24037;&#20316;&#32773;&#20415;&#23452;45&#20493;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#21453;&#39304;&#20855;&#26377;&#39640;&#24230;&#19968;&#33268;&#24615;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#30495;&#23454;&#19990;&#30028;&#20132;&#20114;&#20013;&#33719;&#24471;&#30340;&#20154;&#31867;&#25351;&#20196;&#36827;&#34892;&#39564;&#35777;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20026;&#20960;&#31181;&#20174;&#37197;&#23545;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;PPO&#65292;best-of-n&#65292;expert iteration&#31561;&#65289;&#25552;&#20379;&#20102;&#21442;&#32771;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;</title><link>http://arxiv.org/abs/2305.05465</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#21160;&#24577;&#20013;&#30340;&#32858;&#31867;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#23454;&#20102;&#24403;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#65292;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#65292;&#20195;&#34920;token&#30340;&#31890;&#23376;&#20250;&#32858;&#38598;&#22312;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#38468;&#36817;&#65292;&#36825;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;Transformer&#35270;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24403;&#26435;&#37325;&#19981;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20195;&#34920;token&#30340;&#31890;&#23376;&#38543;&#30528;&#26102;&#38388;&#36235;&#20110;&#26080;&#31351;&#22823;&#32780;&#36235;&#21521;&#20110;&#29305;&#23450;&#30340;&#26497;&#38480;&#23545;&#35937;&#12290;&#20986;&#29616;&#30340;&#26497;&#38480;&#23545;&#35937;&#31867;&#22411;&#21462;&#20915;&#20110;&#20215;&#20540;&#30697;&#38453;&#30340;&#35889;&#12290;&#27492;&#22806;&#65292;&#22312;&#19968;&#32500;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#27880;&#24847;&#21147;&#30697;&#38453;&#25910;&#25947;&#20110;&#20302;&#31209;&#24067;&#23572;&#30697;&#38453;&#12290;&#36825;&#20123;&#32467;&#26524;&#30340;&#32452;&#21512;&#22312;&#25968;&#23398;&#19978;&#35777;&#23454;&#20102;Vaswani&#31561;&#20154;&#30340;&#32463;&#39564;&#35266;&#23519;&#65292;&#21363;Transformer&#22788;&#29702;&#19968;&#31995;&#21015;token&#26102;&#20250;&#20986;&#29616;&#8220;&#39046;&#23548;&#32773;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2305.05126</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#20869;&#26680;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#22522;&#20110;&#25968;&#25454;&#20869;&#26680;&#30340;&#26041;&#27861;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#65292;&#19981;&#21463;&#24230;&#37327;&#25351;&#26631;&#30340;&#32422;&#26463;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#23454;&#29616;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#65292;&#24182;&#25104;&#21151;&#35825;&#23548;&#20102;&#19968;&#32452;&#19982;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#27169;&#22411;&#36317;&#31163;&#20989;&#25968;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#20027;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#25193;&#23637;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#22320;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#30446;&#21069;&#27604;&#36739;&#22522;&#30784;&#27169;&#22411;&#30340;&#33539;&#24335;&#28041;&#21450;&#22312;&#21508;&#31181;&#31574;&#21010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#32858;&#21512;&#25351;&#26631;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#24230;&#37327;&#25351;&#26631;&#30340;&#36873;&#25321;&#65292;&#36825;&#20351;&#24471;&#23427;&#22312;&#29702;&#24819;&#24230;&#37327;&#19981;&#26126;&#26174;&#25110;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27809;&#26377;&#24230;&#37327;&#25351;&#26631;&#30340;&#22522;&#30784;&#27169;&#22411;&#27604;&#36739;&#26041;&#27861;&#65292;&#36890;&#36807;&#23427;&#20204;&#30340;&#23884;&#20837;&#31354;&#38388;&#20960;&#20309;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#38543;&#26426;&#22270;&#29702;&#35770;&#65292;&#24182;&#20419;&#36827;&#28857;&#23545;&#28857;&#21644;&#22810;&#27169;&#22411;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#35825;&#23548;&#19968;&#32452;&#37197;&#22791;&#26377;&#19982;&#19968;&#20123;&#19979;&#28216;&#25351;&#26631;&#24378;&#30456;&#20851;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#27169;&#22411;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in self-supervised learning and neural network scaling have enabled the creation of large models -- known as foundation models -- which can be easily adapted to a wide range of downstream tasks. The current paradigm for comparing foundation models involves benchmarking them with aggregate metrics on various curated datasets. Unfortunately, this method of model comparison is heavily dependent on the choice of metric, which makes it unsuitable for situations where the ideal metric is either not obvious or unavailable. In this work, we present a metric-free methodology for comparing foundation models via their embedding space geometry. Our methodology is grounded in random graph theory, and facilitates both pointwise and multi-model comparison. Further, we demonstrate how our framework can be used to induce a manifold of models equipped with a distance function that correlates strongly with several downstream metrics.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#19968;&#31181;&#26082;&#20445;&#25252;&#20102;&#38544;&#31169;&#21448;&#20445;&#23384;&#20102;&#23454;&#29992;&#31243;&#24207;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03711</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26159;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#30340;&#38134;&#24377;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is dataset condensation a silver bullet for healthcare data sharing?. (arXiv:2305.03711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03711
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#25552;&#20379;&#20102;&#19968;&#31181;&#26082;&#20445;&#25252;&#20102;&#38544;&#31169;&#21448;&#20445;&#23384;&#20102;&#23454;&#29992;&#31243;&#24207;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21307;&#30103;&#25968;&#25454;&#20849;&#20139;&#65292;&#20445;&#25252;&#20010;&#20154;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#19968;&#20010;&#19975;&#26080;&#19968;&#22833;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26032;&#25216;&#26415;&#8212;&#8212;&#25968;&#25454;&#38598;&#21387;&#32553;&#65292;&#20197;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20849;&#20139;&#25968;&#25454;&#65292;&#32467;&#26524;&#34920;&#26126;&#21069;&#26223;&#24191;&#38420;&#12290;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#25688;&#35201;&#20102;&#21407;&#22987;&#35760;&#24405;&#65292;&#19981;&#21487;&#36870;&#22320;&#38544;&#34255;&#20102;&#20010;&#20307;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#36798;&#21040;&#20102;&#30495;&#27491;&#30340;&#21435;&#35782;&#21035;&#21270;&#65292;&#20801;&#35768;&#33258;&#30001;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#21387;&#32553;&#25968;&#25454;&#20013;&#20445;&#30041;&#20102;&#21407;&#22987;&#28145;&#24230;&#23398;&#20064;&#23454;&#29992;&#31243;&#24207;&#65292;&#19988;&#25968;&#25454;&#37327;&#36739;&#23567;&#19988;&#27169;&#22411;&#25910;&#25947;&#21152;&#36895;&#12290;&#22312;PhysioNet-2012&#20013;&#65292;20&#20010;&#26679;&#26412;&#30340;&#21387;&#32553;&#25968;&#25454;&#33021;&#22815;&#20351;&#28145;&#24230;&#27169;&#22411;&#36798;&#21040;&#20102;80.3%&#30340;&#27515;&#20129;&#39044;&#27979;&#27979;&#35797;AUC&#65288;&#32780;&#21407;&#22987;&#35760;&#24405;&#20026;5120&#20010;&#26679;&#26412;&#30340;85.8%&#65289;&#65292;&#36825;&#19968;&#21457;&#29616;&#20063;&#36866;&#29992;&#20110;MIMIC-III&#21644;Coswara&#31561;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#32463;&#39564;&#35777;&#26126;&#20102;DC&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#12290;&#25968;&#25454;&#38598;&#21387;&#32553;&#20026;&#20849;&#20139;&#21307;&#30103;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26082;&#20445;&#25252;&#20102;&#38544;&#31169;&#21448;&#20445;&#23384;&#20102;&#23454;&#29992;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safeguarding personal information is paramount for healthcare data sharing, a challenging issue without any silver bullet thus far. We study the prospect of a recent deep-learning advent, dataset condensation (DC), in sharing healthcare data for AI research, and the results are promising. The condensed data abstracts original records and irreversibly conceals individual-level knowledge to achieve a bona fide de-identification, which permits free sharing. Moreover, the original deep-learning utilities are well preserved in the condensed data with compressed volume and accelerated model convergences. In PhysioNet-2012, a condensed dataset of 20 samples can orient deep models attaining 80.3% test AUC of mortality prediction (versus 85.8% of 5120 original records), an inspiring discovery generalised to MIMIC-III and Coswara datasets. We also interpret the inhere privacy protections of DC through theoretical analysis and empirical evidence. Dataset condensation opens a new gate to sharing h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.02657</link><description>&lt;p&gt;
&#28145;&#24230;&#23485;&#26494;&#24347;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#35745;&#20248;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;
Statistical Optimality of Deep Wide Neural Networks. (arXiv:2305.02657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#35777;&#26126;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#65292;&#20294;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23450;&#20041;&#22312;&#26377;&#30028;&#22495;$\mathcal X \subset \mathbb R^{d}$&#19978;&#30340;&#28145;&#24230;&#23485;&#26494;&#24347;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#39318;&#20808;&#35777;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#20197;&#34987;&#30456;&#24212;&#30340;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22238;&#24402;&#25152;&#23436;&#20840;&#25551;&#32472;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#30340;&#35889;&#29305;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#28145;&#24230;&#31070;&#32463;&#20999;&#21521;&#26680;&#22312;$\mathcal{X}$&#19978;&#20026;&#27491;&#23450;&#65292;&#20854;&#29305;&#24449;&#20540;&#34928;&#20943;&#29575;&#20026;$(d+1)/d$&#12290;&#30001;&#20110;&#26680;&#22238;&#24402;&#20013;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#36866;&#24403;&#26089;&#20572;&#30340;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#26497;&#22823;&#29575;&#65292;&#21069;&#25552;&#26159;&#22238;&#24402;&#20989;&#25968;&#22312;&#23545;&#24212;&#30340;NTK&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#36807;&#24230;&#25311;&#21512;&#30340;&#22810;&#23618;&#23485;&#31070;&#32463;&#32593;&#32476;&#22312;$\mathbb S^{d}$&#19978;&#19981;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the generalization ability of deep wide feedforward ReLU neural networks defined on a bounded domain $\mathcal X \subset \mathbb R^{d}$. We first demonstrate that the generalization ability of the neural network can be fully characterized by that of the corresponding deep neural tangent kernel (NTK) regression. We then investigate on the spectral properties of the deep NTK and show that the deep NTK is positive definite on $\mathcal{X}$ and its eigenvalue decay rate is $(d+1)/d$. Thanks to the well established theories in kernel regression, we then conclude that multilayer wide neural networks trained by gradient descent with proper early stopping achieve the minimax rate, provided that the regression function lies in the reproducing kernel Hilbert space (RKHS) associated with the corresponding NTK. Finally, we illustrate that the overfitted multilayer wide neural networks can not generalize well on $\mathbb S^{d}$.
&lt;/p&gt;</description></item><item><title>MaskSearch&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#36807;&#28388;&#22120;&#39564;&#35777;&#26597;&#35810;&#25191;&#34892;&#26694;&#26550;&#65292;&#21152;&#36895;&#23545;&#22270;&#20687;&#25513;&#27169;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65292;&#21487;&#20197;&#23558;&#20010;&#20307;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.02375</link><description>&lt;p&gt;
MaskSearch&#65306;&#35268;&#27169;&#21270;&#26597;&#35810;&#22270;&#20687;&#25513;&#27169;
&lt;/p&gt;
&lt;p&gt;
MaskSearch: Querying Image Masks at Scale. (arXiv:2305.02375v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02375
&lt;/p&gt;
&lt;p&gt;
MaskSearch&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#39640;&#25928;&#30340;&#36807;&#28388;&#22120;&#39564;&#35777;&#26597;&#35810;&#25191;&#34892;&#26694;&#26550;&#65292;&#21152;&#36895;&#23545;&#22270;&#20687;&#25513;&#27169;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#65292;&#21487;&#20197;&#23558;&#20010;&#20307;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#25968;&#25454;&#24211;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#20250;&#29983;&#25104;&#27880;&#37322;&#22270;&#20687;&#20869;&#23481;&#30340;&#25513;&#27169;&#65288;&#20363;&#22914;&#26174;&#30528;&#24615;&#22320;&#22270;&#65292;&#20998;&#21106;&#22320;&#22270;&#65289;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#65288;&#20363;&#22914;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#20102;&#34394;&#20551;&#20851;&#32852;&#65292;&#25110;&#32773;&#22270;&#20687;&#26159;&#21542;&#34987;&#24694;&#24847;&#20462;&#25913;&#20197;&#35823;&#23548;&#27169;&#22411;&#65289;&#12290;&#23613;&#31649;&#22522;&#20110;&#25513;&#27169;&#23646;&#24615;&#26816;&#32034;&#31034;&#20363;&#30340;&#26597;&#35810;&#23545;&#23454;&#36341;&#32773;&#24456;&#26377;&#20215;&#20540;&#65292;&#20294;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#39640;&#25928;&#22320;&#25903;&#25345;&#27492;&#31867;&#26597;&#35810;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#30830;&#23450;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;MaskSearch&#65292;&#30528;&#37325;&#20110;&#21152;&#36895;&#23545;&#22270;&#20687;&#25513;&#27169;&#25968;&#25454;&#24211;&#30340;&#26597;&#35810;&#12290;MaskSearch&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32034;&#24341;&#25216;&#26415;&#21644;&#19968;&#31181;&#39640;&#25928;&#30340;&#36807;&#28388;&#22120;&#39564;&#35777;&#26597;&#35810;&#25191;&#34892;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21407;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#32034;&#24341;&#22823;&#23567;&#32422;&#20026;&#25968;&#25454;5%&#30340;MaskSearch&#21487;&#20197;&#23558;&#20010;&#20307;&#26597;&#35810;&#21152;&#36895;&#39640;&#36798;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#25506;&#32034;&#30340;&#21508;&#31181;&#22810;&#26597;&#35810;&#24037;&#20316;&#36127;&#36733;&#19978;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning tasks over image databases often generate masks that annotate image content (e.g., saliency maps, segmentation maps) and enable a variety of applications (e.g., determine if a model is learning spurious correlations or if an image was maliciously modified to mislead a model). While queries that retrieve examples based on mask properties are valuable to practitioners, existing systems do not support such queries efficiently. In this paper, we formalize the problem and propose a system, MaskSearch, that focuses on accelerating queries over databases of image masks. MaskSearch leverages a novel indexing technique and an efficient filter-verification query execution framework. Experiments on real-world datasets with our prototype show that MaskSearch, using indexes approximately 5% the size of the data, accelerates individual queries by up to two orders of magnitude and consistently outperforms existing methods on various multi-query workloads that simulate dataset explora
&lt;/p&gt;</description></item><item><title>&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2304.08242</link><description>&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#24102;&#26377;&#25991;&#26412;&#36793;&#30340;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08242
&lt;/p&gt;
&lt;p&gt;
&#28145;&#23618;&#28508;&#22312;&#20301;&#32622;&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#32593;&#32476;&#32858;&#31867;&#21644;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#21644;&#27010;&#29575;&#27169;&#22411;&#23545;&#33410;&#28857;&#21644;&#36793;&#36827;&#34892;&#32852;&#21512;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#36827;&#34892;&#21442;&#25968;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#20540;&#20132;&#20114;&#23548;&#33268;&#29992;&#25143;&#20849;&#20139;&#20854;&#20182;&#20154;&#21457;&#24067;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#33258;&#28982;&#22320;&#30001;&#23558;&#20010;&#20307;&#19982;&#33410;&#28857;&#20851;&#32852;&#21644;&#20132;&#25442;&#30340;&#25991;&#26412;&#23450;&#20041;&#20026;&#36793;&#30340;&#32593;&#32476;&#26469;&#34920;&#31034;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#20123;&#24322;&#26500;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#23558;&#33410;&#28857;&#32858;&#31867;&#20026;&#21516;&#31867;&#32676;&#32452;&#20197;&#21450;&#21576;&#29616;&#21487;&#29702;&#35299;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Deep-LPTM&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#32858;&#31867;&#31574;&#30053;&#65292;&#20381;&#36182;&#20110;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#20197;&#21450;&#27010;&#29575;&#27169;&#22411;&#26469;&#25551;&#36848;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;Deep-LPTM&#20801;&#35768;&#22312;&#20004;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#26500;&#24314;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#34920;&#31034;&#12290;&#21442;&#25968;&#20351;&#29992;&#21464;&#20998;&#25512;&#26029;&#31639;&#27861;&#36827;&#34892;&#25512;&#26029;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;IC2L&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#36873;&#25321;&#20855;&#26377;&#30456;&#20851;&#32858;&#31867;&#21644;&#21487;&#35270;&#21270;&#23646;&#24615;&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#36873;&#25321;&#20934;&#21017;&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Numerical interactions leading to users sharing textual content published by others are naturally represented by a network where the individuals are associated with the nodes and the exchanged texts with the edges. To understand those heterogeneous and complex data structures, clustering nodes into homogeneous groups as well as rendering a comprehensible visualisation of the data is mandatory. To address both issues, we introduce Deep-LPTM, a model-based clustering strategy relying on a variational graph auto-encoder approach as well as a probabilistic model to characterise the topics of discussion. Deep-LPTM allows to build a joint representation of the nodes and of the edges in two embeddings spaces. The parameters are inferred using a variational inference algorithm. We also introduce IC2L, a model selection criterion specifically designed to choose models with relevant clustering and visualisation properties. An extensive benchmark study on synthetic data is provided. In particular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;</title><link>http://arxiv.org/abs/2303.13850</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22240;&#26524;&#24402;&#22240;&#23398;&#20064;&#65306;&#36229;&#36234;&#30452;&#25509;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Learning Causal Attributions in Neural Networks: Beyond Direct Effects. (arXiv:2303.13850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19981;&#20165;&#32771;&#34385;&#30452;&#25509;&#24433;&#21709;&#65292;&#36824;&#33021;&#32771;&#34385;&#38388;&#25509;&#24433;&#21709;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#37327;&#21270;&#22240;&#26524;&#24402;&#22240;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25429;&#25417;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#22240;&#26524;&#26041;&#27861;&#20272;&#35745;&#21644;&#32500;&#25252;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23646;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20551;&#35774;&#36755;&#20837;&#21464;&#37327;&#29420;&#31435;&#65288;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65289;&#65292;&#22240;&#27492;&#20165;&#30740;&#31350;&#30452;&#25509;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#22312;&#36755;&#20837;&#29305;&#24449;&#20013;&#24341;&#20837;&#36793;&#32536;&#20197;&#25429;&#25417;&#21644;&#32500;&#25252;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#25928;&#24212;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#26377;&#25928;&#30340;&#36817;&#20284;&#31574;&#30053;&#26469;&#37327;&#21270;&#39640;&#32500;&#25968;&#25454;&#30340;&#22240;&#26524;&#24402;&#22240;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#25509;&#36817;&#22522;&#26412;&#20107;&#23454;&#25928;&#26524;&#30340;&#30452;&#25509;&#21644;&#38388;&#25509;&#22240;&#26524;&#24402;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing interest in capturing and maintaining causal relationships in Neural Network (NN) models in recent years. We study causal approaches to estimate and maintain input-output attributions in NN models in this work. In particular, existing efforts in this direction assume independence among input variables (by virtue of the NN architecture), and hence study only direct causal effects. Viewing an NN as a structural causal model (SCM), we instead focus on going beyond direct effects, introduce edges among input features, and provide a simple yet effective methodology to capture and maintain direct and indirect causal effects while training an NN model. We also propose effective approximation strategies to quantify causal attributions in high dimensional data. Our wide range of experiments on synthetic and real-world datasets show that the proposed ante-hoc method learns causal attributions for both direct and indirect causal effects close to the ground truth effects.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2302.03068</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03068
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#65292;&#25552;&#20986;&#22235;&#20010;&#35823;&#24046;&#37096;&#20998;&#35780;&#20272;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;169&#20010;&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#20026;SSL&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#27969;&#31243;&#35774;&#35745;&#28041;&#21450;&#26550;&#26500;&#12289;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#31561;&#35832;&#22810;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;SSL&#36890;&#24120;&#20351;&#29992;&#21333;&#19968;&#24230;&#37327;&#26469;&#35780;&#20272;&#65292;&#36825;&#24182;&#19981;&#33021;&#25552;&#20379;&#28145;&#20837;&#30340;&#27934;&#23519;&#21644;&#25913;&#36827;&#26041;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SSL&#39118;&#38505;&#20998;&#35299;&#65292;&#20174;&#36924;&#36817;&#12289;&#34920;&#31034;&#21487;&#29992;&#24615;&#12289;&#25506;&#38024;&#27867;&#21270;&#21644;&#32534;&#30721;&#22120;&#27867;&#21270;&#31561;&#35282;&#24230;&#23545;&#38169;&#35823;&#36827;&#34892;&#20998;&#35299;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;30&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;169&#20010;&#22312;ImageNet&#19978;&#35780;&#20272;&#30340;SSL&#35270;&#35273;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#27599;&#20010;&#32452;&#20214;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#20272;&#35745;&#22120;&#65292;&#20026;SSL&#27169;&#22411;&#30340;&#35774;&#35745;&#21644;&#20351;&#29992;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) pipelines differ in many design choices such as the architecture, augmentations, or pretraining data. Yet SSL is typically evaluated using a single metric: linear probing on ImageNet. This does not provide much insight into why or when a model is better, now how to improve it. To address this, we propose an SSL risk decomposition, which generalizes the classical supervised approximation-estimation decomposition by considering errors arising from the representation learning step. Our decomposition consists of four error components: approximation, representation usability, probe generalization, and encoder generalization. We provide efficient estimators for each component and use them to analyze the effect of 30 design choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives valuable insights for designing and using SSL models. For example, it highlights the main sources of error and shows how to improve SSL in specific settings (full- vs 
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.00997</link><description>&lt;p&gt;
&#21463;&#38480;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#65306;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33719;&#24471;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00997
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#32047;&#35745;&#30446;&#26631;&#20540;&#26368;&#23567;&#21270;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#32467;&#26524;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#37319;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#22312;&#32447;&#20004;&#38454;&#27573;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;$T$&#26399;&#32039;&#32422;&#26463;&#26465;&#20214;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#65292;&#25105;&#20204;&#20808;&#20316;&#20986;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#65292;&#28982;&#21518;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#23454;&#29616;&#65292;&#26368;&#21518;&#20174;&#21462;&#20915;&#20110;&#31532;&#19968;&#38454;&#27573;&#20915;&#31574;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#21487;&#34892;&#38598;&#20013;&#20570;&#20986;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#12290;&#25105;&#20204;&#26088;&#22312;&#26368;&#23567;&#21270;&#32047;&#35745;&#30446;&#26631;&#20540;&#65292;&#21516;&#26102;&#20445;&#35777;&#38271;&#26399;&#24179;&#22343;&#30340;&#31532;&#20108;&#38454;&#27573;&#20915;&#31574;&#23646;&#20110;&#19968;&#20010;&#38598;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#20174;&#22312;&#32447;&#20004;&#38454;&#27573;&#38382;&#39064;&#20013;&#24320;&#21457;&#22312;&#32447;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#21487;&#20197;&#38477;&#33267;&#23884;&#20837;&#23545;&#25239;&#24615;&#23398;&#20064;&#31639;&#27861;&#30340;&#36951;&#25022;&#30028;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#25105;&#20204;&#37117;&#33719;&#24471;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#24403;&#27599;&#20010;&#26102;&#38388;&#27573;&#30340;&#27169;&#22411;&#21442;&#25968;&#37117;&#26159;&#20174;&#30456;&#21516;&#30340;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;$O&#65288;\sqrt{T}&#65289;$&#36951;&#25022;&#30028;&#65292;&#36825;&#27604;&#20043;&#21069;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#30028;&#26377;&#25152;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36824;&#21487;&#20197;&#25269;&#25239;&#27169;&#22411;&#30340;&#25932;&#23545;&#24615;&#25200;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider an online two-stage stochastic optimization with long-term constraints over a finite horizon of $T$ periods. At each period, we take the first-stage action, observe a model parameter realization and then take the second-stage action from a feasible set that depends both on the first-stage decision and the model parameter. We aim to minimize the cumulative objective value while guaranteeing that the long-term average second-stage decision belongs to a set. We develop online algorithms for the online two-stage problem from adversarial learning algorithms. Also, the regret bound of our algorithm cam be reduced to the regret bound of embedded adversarial learning algorithms. Based on our framework, we obtain new results under various settings. When the model parameter at each period is drawn from identical distributions, we derive state-of-art $O(\sqrt{T})$ regret that improves previous bounds under special cases. Our algorithm is also robust to adversarial corruptions of model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QCS-SGM+&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#36825;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#31895;&#31961;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00919</link><description>&lt;p&gt;
QCM-SGM+: &#22522;&#20110;&#24471;&#20998;&#29983;&#25104;&#27169;&#22411;&#30340;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models. (arXiv:2302.00919v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;QCS-SGM+&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#36825;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#31895;&#31961;&#37327;&#21270;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#25361;&#25112;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#21387;&#32553;&#24863;&#30693;&#36807;&#31243;&#20013;&#65292;&#33719;&#24471;&#30340;&#27979;&#37327;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#22312;&#20256;&#36755;&#25110;&#23384;&#20648;&#21069;&#38480;&#21046;&#20026;&#26377;&#38480;&#27604;&#29305;&#30340;&#37327;&#21270;&#12290;&#36825;&#20010;&#38750;&#32447;&#24615;&#37327;&#21270;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#24674;&#22797;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#24230;&#31895;&#31961;&#30340;&#37327;&#21270;&#22914;1&#27604;&#29305;&#19979;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;QCS-SGM&#30340;&#26377;&#25928;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGM)&#20316;&#20026;&#38544;&#24335;&#20808;&#39564;&#36827;&#34892;&#37327;&#21270;&#21387;&#32553;&#24863;&#30693;(QCS)&#12290;&#30001;&#20110;SGM&#22312;&#25429;&#25417;&#33258;&#28982;&#20449;&#21495;&#30340;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;QCS-SGM&#26126;&#26174;&#20248;&#20110;&#20197;&#21069;&#30340;QCS&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;QCS-SGM&#23616;&#38480;&#20110;(&#36817;&#20284;)&#34892;&#27491;&#20132;&#20256;&#24863;&#30697;&#38453;&#65292;&#21542;&#21017;&#21487;&#33021;&#20250;&#26080;&#27861;&#35745;&#31639;&#20284;&#28982;&#20998;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;QCS-SGM+&#30340;&#39640;&#32423;&#21464;&#20307;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#19968;&#33324;&#30697;&#38453;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20284;&#28982;&#20998;&#25968;&#35745;&#31639;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#35266;&#28857;&#65292;&#20854;&#20013;&#35745;&#31639;&#26399;&#26395;&#24471;&#20998;&#20197;&#35299;&#20915;&#27599;&#20010;&#27979;&#37327;&#30340;&#32467;&#26500;&#38750;&#27491;&#20132;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical compressed sensing (CS), the obtained measurements typically necessitate quantization to a limited number of bits prior to transmission or storage. This nonlinear quantization process poses significant recovery challenges, particularly with extreme coarse quantization such as 1-bit. Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS (QCS) which utilizes score-based generative models (SGM) as an implicit prior. Due to the adeptness of SGM in capturing the intricate structures of natural signals, QCS-SGM substantially outperforms previous QCS methods. However, QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as the computation of the likelihood score becomes intractable otherwise. To address this limitation, we introduce an advanced variant of QCS-SGM, termed QCS-SGM+, capable of handling general matrices effectively. The key idea is a Bayesian inference perspective on the likelihood score computation, wherein an expectatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20013;&#20351;&#29992;&#20146;&#21644;&#21147;&#20449;&#24687;&#26469;&#25366;&#25496;&#22256;&#38590;&#36127;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#22256;&#38590;&#36127;&#26679;&#26412;&#35782;&#21035;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13340</link><description>&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#20013;&#22522;&#20110;&#20146;&#21644;&#21147;&#19981;&#30830;&#23450;&#24615;&#30340;&#36127;&#26679;&#26412;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning. (arXiv:2301.13340v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20013;&#20351;&#29992;&#20146;&#21644;&#21147;&#20449;&#24687;&#26469;&#25366;&#25496;&#22256;&#38590;&#36127;&#26679;&#26412;&#65292;&#20197;&#35299;&#20915;&#22270;&#25968;&#25454;&#20013;&#22256;&#38590;&#36127;&#26679;&#26412;&#35782;&#21035;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#20013;&#65292;&#21253;&#25324;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20013;&#65292;&#22256;&#38590;&#36127;&#26679;&#26412;&#25366;&#25496;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#25928;&#22686;&#24378;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22256;&#38590;&#24230;&#30340;CL&#26041;&#27861;&#36890;&#24120;&#23558;&#19982;&#38170;&#28857;&#23454;&#20363;&#26368;&#30456;&#20284;&#30340;&#36127;&#23454;&#20363;&#35270;&#20026;&#22256;&#38590;&#36127;&#20363;&#65292;&#36825;&#26377;&#21161;&#20110;&#25913;&#21892;CL&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22270;&#20687;&#25968;&#25454;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#25968;&#25454;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#35782;&#21035;&#20986;&#22256;&#38590;&#36127;&#20363;&#65292;&#21453;&#32780;&#20250;&#23548;&#33268;&#24456;&#22810;&#38169;&#35823;&#30340;&#36127;&#20363;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23398;&#21040;&#30340;&#22270;&#34920;&#31034;&#22312;&#22270;&#25968;&#25454;&#20013;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#34920;&#31034;&#21644;/&#25110;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;non-i.i.d.&#65289;&#38382;&#39064;&#32780;&#19981;&#22815;&#26377;&#21306;&#21035;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#22312;&#32676;&#20307;&#20146;&#21644;&#21147;&#20449;&#24687;&#65288;&#21363;&#36127;&#23454;&#20363;&#19982;&#38170;&#28857;&#23454;&#20363;&#20043;&#38388;&#30340;&#20004;&#32452;&#25104;&#23545;&#20146;&#21644;&#21147;&#65289;&#19978;&#26500;&#24314;&#20102;&#19968;&#20010;&#26377;&#21306;&#21035;&#24615;&#30340;&#27169;&#22411;&#26469;&#25366;&#25496;GCL&#20013;&#30340;&#22256;&#38590;&#36127;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hard negative mining has shown effective in enhancing self-supervised contrastive learning (CL) on diverse data types, including graph CL (GCL). The existing hardness-aware CL methods typically treat negative instances that are most similar to the anchor instance as hard negatives, which helps improve the CL performance, especially on image data. However, this approach often fails to identify the hard negatives but leads to many false negatives on graph data. This is mainly due to that the learned graph representations are not sufficiently discriminative due to oversmooth representations and/or non-independent and identically distributed (non-i.i.d.) issues in graph data. To tackle this problem, this article proposes a novel approach that builds a discriminative model on collective affinity information (i.e., two sets of pairwise affinities between the negative instances and the anchor instance) to mine hard negatives in GCL. In particular, the proposed approach evaluates how confident
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CycleGAN&#23454;&#29616;&#26080;&#30417;&#30563;&#26579;&#33394;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#22312;&#20083;&#33146;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#24212;&#29992;&#20110;&#20405;&#34989;&#24615;&#30284;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#23558;CycleGAN&#24212;&#29992;&#20110;&#26579;&#33394;&#26080;&#20851;&#29305;&#24449;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.13128</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#26579;&#33394;&#36866;&#24212;&#30340;&#26631;&#20934;&#21270;CycleGAN&#35757;&#32451;&#26041;&#27861;&#22312;&#20083;&#33146;&#32452;&#32455;&#30149;&#29702;&#23398;&#20405;&#34989;&#24615;&#30284;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology. (arXiv:2301.13128v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13128
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;CycleGAN&#23454;&#29616;&#26080;&#30417;&#30563;&#26579;&#33394;&#36716;&#25442;&#30340;&#26041;&#27861;&#65292;&#22312;&#20083;&#33146;&#32452;&#32455;&#30149;&#29702;&#23398;&#20013;&#24212;&#29992;&#20110;&#20405;&#34989;&#24615;&#30284;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20316;&#32773;&#35777;&#26126;&#20102;&#23558;CycleGAN&#24212;&#29992;&#20110;&#26579;&#33394;&#26080;&#20851;&#29305;&#24449;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27867;&#21270;&#24615;&#26159;&#35745;&#31639;&#30149;&#29702;&#23398;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#20999;&#29255;&#21046;&#22791;&#24322;&#36136;&#24615;&#21644;&#25195;&#25551;&#20202;&#30340;&#22810;&#26679;&#24615;&#23548;&#33268;&#22312;&#26410;&#21442;&#19982;&#35757;&#32451;&#30340;&#21307;&#30103;&#20013;&#24515;&#20013;&#20351;&#29992;&#27169;&#22411;&#26102;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#20083;&#33146;&#20405;&#34989;&#24615;&#30284;&#22359;&#30340;&#26579;&#33394;&#26080;&#20851;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;CycleGAN&#23454;&#29616;&#20102;&#19968;&#31181;&#26579;&#33394;&#36716;&#25442;&#31574;&#30053;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#22270;&#20687;&#36716;&#25442;&#12290;&#25105;&#20204;&#23558;&#19977;&#31181;&#22522;&#20110;CycleGAN&#30340;&#26041;&#27861;&#19982;&#27809;&#26377;&#20219;&#20309;&#26579;&#33394;&#26080;&#20851;&#24615;&#31574;&#30053;&#30340;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20854;&#20013;&#20004;&#31181;&#26041;&#27861;&#22312;&#25512;&#26029;&#25110;&#35757;&#32451;&#20013;&#20351;&#29992;CycleGAN&#30340;&#36716;&#25442;&#26469;&#26500;&#24314;&#26579;&#33394;&#29305;&#24322;&#24615;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#26368;&#21518;&#19968;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#26579;&#33394;&#25968;&#25454;&#22686;&#24378;&#65292;&#38480;&#21046;&#20102;&#20998;&#31867;&#27169;&#22411;&#23398;&#20064;&#26579;&#33394;&#26080;&#20851;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#21442;&#32771;&#26579;&#33394;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#22522;&#20934;&#20998;&#31867;&#27169;&#22411;&#26469;&#35774;&#32622;&#22522;&#20934;&#24230;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;H&#65286;
&lt;/p&gt;
&lt;p&gt;
Generalization is one of the main challenges of computational pathology. Slide preparation heterogeneity and the diversity of scanners lead to poor model performance when used on data from medical centers not seen during training. In order to achieve stain invariance in breast invasive carcinoma patch classification, we implement a stain translation strategy using cycleGANs for unsupervised image-to-image translation. We compare three cycleGAN-based approaches to a baseline classification model obtained without any stain invariance strategy. Two of the proposed approaches use cycleGAN's translations at inference or training in order to build stain-specific classification models. The last method uses them for stain data augmentation during training. This constrains the classification model to learn stain-invariant features. Baseline metrics are set by training and testing the baseline classification model on a reference stain. We assessed performances using three medical centers with H&amp;
&lt;/p&gt;</description></item><item><title>LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2301.13126</link><description>&lt;p&gt;
LEXTREME&#65306;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13126
&lt;/p&gt;
&lt;p&gt;
LEXTREME&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#20219;&#21153;&#30340;&#27861;&#24459;&#39046;&#22495;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#25552;&#20379;&#20102;11&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#27979;&#35780;&#65292;&#26368;&#20339;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#20351;&#24471;LEXTREME&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#19988;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;transformer&#26550;&#26500;&#30340;&#26174;&#33879;&#36827;&#23637;&#25512;&#21160;&#19979;&#65292;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#22686;&#38271;&#12290;&#20026;&#20102;&#34913;&#37327;&#36827;&#23637;&#65292;&#31934;&#24515;&#31574;&#21010;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#21482;&#33021;&#22788;&#29702;&#33521;&#25991;&#65292;&#32780;&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23578;&#26410;&#26377;&#22810;&#35821;&#35328;&#22522;&#20934;&#21487;&#29992;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22522;&#20934;&#24050;&#32463;&#39281;&#21644;&#65292;&#26368;&#20339;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#20154;&#31867;&#65292;&#24182;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25991;&#29486;&#65292;&#24182;&#36873;&#25321;&#20102;11&#20010;&#28085;&#30422;24&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#65292;&#21019;&#24314;&#20102;LEXTREME&#12290;&#20026;&#20102;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32508;&#21512;&#35780;&#20998;&#65292;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#38598;&#65292;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#12290;&#26368;&#20339;&#22522;&#32447;&#27169;&#22411;&#65288;XLM-R large&#65289;&#22312;&#25968;&#25454;&#38598;&#32508;&#21512;&#35780;&#20998;&#21644;&#35821;&#35328;&#32508;&#21512;&#35780;&#20998;&#19978;&#22343;&#36798;&#21040;&#20102;61.3&#12290;&#36825;&#34920;&#26126;LEXTREME&#20173;&#28982;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#20026;&#25913;&#36827;&#30041;&#19979;&#20102;&#20805;&#36275;&#31354;&#38388;&#12290;&#20026;&#20102;&#26041;&#20415;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#20351;&#29992;&#65292;&#25105;&#20204;&#23558;LEXTREME&#19982;&#25152;&#26377;&#25968;&#25454;&#19968;&#36215;&#21457;&#24067;&#22312;huggingface&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20801;&#35768;&#22312;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#19978;&#20445;&#25345;&#25511;&#21046;&#65292;&#24182;&#33719;&#24471;&#20102;&#32039;&#23494;&#30340;&#26377;&#38480;&#26679;&#26412;&#36793;&#30028;&#26469;&#35780;&#20272;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#12290;&#36825;&#23545;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#21644;&#38169;&#35823;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2301.12767</link><description>&lt;p&gt;
&#21387;&#32553;&#12289;&#27867;&#21270;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#65292;&#20801;&#35768;&#22312;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#19978;&#20445;&#25345;&#25511;&#21046;&#65292;&#24182;&#33719;&#24471;&#20102;&#32039;&#23494;&#30340;&#26377;&#38480;&#26679;&#26412;&#36793;&#30028;&#26469;&#35780;&#20272;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#12290;&#36825;&#23545;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#38169;&#35823;&#20998;&#31867;&#21644;&#38169;&#35823;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#20989;&#25968;&#26159;&#19968;&#31181;&#23558;&#35266;&#27979;&#38598;&#32553;&#23567;&#20026;&#23610;&#23544;&#20943;&#23567;&#30340;&#23376;&#38598;&#30340;&#26144;&#23556;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#20449;&#24687;&#20869;&#23481;&#12290;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#65292;&#26032;&#35266;&#27979;&#20351;&#21387;&#32553;&#38598;&#21457;&#29983;&#21464;&#21270;&#30340;&#26465;&#20214;&#34987;&#35299;&#37322;&#20026;&#26032;&#35266;&#27979;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#22312;&#23398;&#20064;&#29702;&#35770;&#20013;&#65292;&#36825;&#23545;&#24212;&#20110;&#38169;&#35823;&#20998;&#31867;&#25110;&#38169;&#35823;&#39044;&#27979;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#29702;&#35770;&#30340;&#22522;&#30784;&#65292;&#20801;&#35768;&#22312;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#19978;&#20445;&#25345;&#25511;&#21046;&#65288;&#19982;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#32479;&#35745;&#8220;&#39118;&#38505;&#8221;&#30456;&#23545;&#24212;&#65289;&#12290;&#22312;&#36866;&#24403;&#30340;&#26465;&#20214;&#19979;&#65292;&#21387;&#32553;&#38598;&#30340;&#22522;&#25968;&#34987;&#35777;&#26126;&#26159;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#30340;&#19968;&#33268;&#20272;&#35745;&#37327;&#65288;&#19981;&#23545;&#21387;&#32553;&#38598;&#30340;&#23610;&#23544;&#35774;&#32622;&#19978;&#38480;&#65289;&#65307;&#27492;&#22806;&#65292;&#22312;&#26222;&#36941;&#36866;&#29992;&#30340;&#20559;&#22909;&#26465;&#20214;&#19979;&#33719;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#32039;&#23494;&#30340;&#26377;&#38480;&#26679;&#26412;&#36793;&#30028;&#26469;&#35780;&#20272;&#21387;&#32553;&#30340;&#25913;&#21464;&#27010;&#29575;&#12290;&#25152;&#26377;&#32467;&#26524;&#37117;&#21487;&#20197;&#22312;&#23436;&#20840;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compression function is a map that slims down an observational set into a subset of reduced size, while preserving its informational content. In multiple applications, the condition that one new observation makes the compressed set change is interpreted that this observation brings in extra information and, in learning theory, this corresponds to misclassification, or misprediction. In this paper, we lay the foundations of a new theory that allows one to keep control on the probability of change of compression (which maps into the statistical "risk" in learning applications). Under suitable conditions, the cardinality of the compressed set is shown to be a consistent estimator of the probability of change of compression (without any upper limit on the size of the compressed set); moreover, unprecedentedly tight finite-sample bounds to evaluate the probability of change of compression are obtained under a generally applicable condition of preference. All results are usable in a fully 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;NN-PMP-Gradient&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21644;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#21033;&#29992;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#36824;&#39640;&#25928;&#22320;&#24674;&#22797;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2212.14566</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#20248;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Pontryagin Optimal Control via Neural Networks. (arXiv:2212.14566v2 [eess.SY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#21407;&#29702;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;NN-PMP-Gradient&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21644;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#19981;&#20165;&#21033;&#29992;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#36824;&#39640;&#25928;&#22320;&#24674;&#22797;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#21644;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22797;&#26434;&#30340;&#39640;&#32500;&#31995;&#32479;&#21160;&#21147;&#23398;&#36890;&#24120;&#23545;&#20915;&#31574;&#32773;&#26469;&#35828;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#25214;&#21040;&#26368;&#20248;&#25511;&#21046;&#21160;&#20316;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24314;&#27169;&#21644;&#35745;&#31639;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#24222;&#29305;&#37324;&#20122;&#37329;&#26368;&#22823;&#21407;&#29702;&#65288;PMP&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;NN-PMP-Gradient&#12290;&#25152;&#24471;&#21040;&#30340;&#25511;&#21046;&#22120;&#21487;&#20197;&#29992;&#20110;&#20855;&#26377;&#26410;&#30693;&#21644;&#22797;&#26434;&#21160;&#21147;&#23398;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#36845;&#20195;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#19981;&#20165;&#21033;&#29992;&#20102;&#30001;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20934;&#30830;&#26367;&#20195;&#27169;&#22411;&#65292;&#36824;&#36890;&#36807;PMP&#26465;&#20214;&#39640;&#25928;&#22320;&#24674;&#22797;&#20102;&#26368;&#20248;&#24615;&#26465;&#20214;&#20197;&#21450;&#26368;&#20248;&#21160;&#20316;&#24207;&#21015;&#12290;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#12289;&#19982;&#32593;&#26684;&#36830;&#25509;&#30340;&#26377;&#25439;&#30005;&#27744;&#30340;&#33021;&#28304;&#22871;&#21033;&#12289;&#21333;&#25670;&#30340;&#25511;&#21046;&#21644;&#20004;&#20010;MuJoCo&#36816;&#21160;&#20219;&#21153;&#30340;&#25968;&#20540;&#20223;&#30495;&#20013;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;NN-PMP-Gradient&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#21644;&#22810;&#21151;&#33021;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving real-world optimal control problems are challenging tasks, as the complex, high-dimensional system dynamics are usually unrevealed to the decision maker. It is thus hard to find the optimal control actions numerically. To deal with such modeling and computation challenges, in this paper, we integrate Neural Networks with the Pontryagin's Maximum Principle (PMP), and propose a sample efficient framework NN-PMP-Gradient. The resulting controller can be implemented for systems with unknown and complex dynamics. By taking an iterative approach, the proposed framework not only utilizes the accurate surrogate models parameterized by neural networks, it also efficiently recovers the optimality conditions along with the optimal action sequences via PMP conditions. Numerical simulations on Linear Quadratic Regulator, energy arbitrage of grid-connected lossy battery, control of single pendulum, and two MuJoCo locomotion tasks demonstrate our proposed NN-PMP-Gradient is a general and vers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11870</link><description>&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#30340;&#19981;&#21487;&#33021;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38416;&#36848;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#37117;&#26080;&#27861;&#32988;&#20219;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#65292;&#21551;&#31034;&#25105;&#20204;&#22312;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#35201;&#27762;&#21462;&#32463;&#39564;&#65292;&#37319;&#29992;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#30340;&#31616;&#21333;&#30452;&#25509;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#35768;&#22810;&#21487;&#20135;&#29983;&#21512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#20294;&#35813;&#39046;&#22495;&#20063;&#32463;&#39564;&#24615;&#22320;&#30475;&#21040;&#20102;&#35768;&#22810;&#22833;&#36133;&#26696;&#20363;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#23545;&#20110;&#23454;&#36341;&#32773;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#26041;&#24335;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#24182;&#22312;&#23427;&#20204;&#20043;&#38388;&#36827;&#34892;&#36873;&#25321;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#27169;&#22411;&#31867;&#65288;&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#28385;&#36275;&#65289;&#65292;&#20219;&#20309;&#23436;&#25972;&#30340;&#32447;&#24615;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#21644;SHAP&#65289;&#21487;&#20197;&#34987;&#35777;&#26126;&#23545;&#20110;&#25512;&#26029;&#27169;&#22411;&#34892;&#20026;&#30340;&#25913;&#36827;&#37117;&#26080;&#27861;&#32988;&#20219;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#24120;&#35265;&#30340;&#26368;&#32456;&#20219;&#21153;&#65292;&#22914;&#25551;&#36848;&#23616;&#37096;&#27169;&#22411;&#34892;&#20026;&#12289;&#35782;&#21035;&#34394;&#20551;&#29305;&#24449;&#21644;&#31639;&#27861;&#22238;&#28335;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#20010;&#37325;&#35201;&#21551;&#31034;&#26159;&#20855;&#20307;&#23450;&#20041;&#26368;&#32456;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65306;&#19968;&#26086;&#36825;&#26679;&#30340;&#26368;&#32456;&#20219;&#21153;&#34987;&#23450;&#20041;&#65292;&#19968;&#20010;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#26041;&#27861;&#8212;&#8212;&#37325;&#22797;&#27169;&#22411;&#35780;&#20272;&#8212;&#8212;&#21487;&#20197;&#32988;&#36807;&#35768;&#22810;&#20854;&#20182;&#22797;&#26434;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;RA&#65292;&#29992;&#20110;&#37327;&#21270;&#22312;&#21457;&#29983;&#30636;&#24577;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#20316;&#32773;&#21457;&#29616;&#29616;&#26377;&#30340;RA&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#27491;&#30830;&#20272;&#35745;RA&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.02649</link><description>&lt;p&gt;
Thales:&#20026;DNN&#21152;&#36895;&#22120;&#21046;&#23450;&#21644;&#20272;&#35745;&#26550;&#26500;&#28431;&#27934;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Thales: Formulating and Estimating Architectural Vulnerability Factors for DNN Accelerators. (arXiv:2212.02649v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#26631;RA&#65292;&#29992;&#20110;&#37327;&#21270;&#22312;&#21457;&#29983;&#30636;&#24577;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#20316;&#32773;&#21457;&#29616;&#29616;&#26377;&#30340;RA&#24418;&#24335;&#21270;&#26041;&#27861;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#27491;&#30830;&#20272;&#35745;RA&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#29983;&#29289;&#35782;&#21035;&#31561;&#23433;&#20840;&#20851;&#38190;&#21644;&#38544;&#31169;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37096;&#32626;&#65292;&#20102;&#35299;DNN&#30340;&#23481;&#38169;&#24615;&#36136;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#22833;&#25928;&#26102;&#38388;(FIT)&#29575;&#21644;&#38745;&#40664;&#25968;&#25454;&#25439;&#22351;(SDC)&#29575;&#31561;&#25351;&#26631;&#65292;&#29992;&#26469;&#34913;&#37327;&#35774;&#22791;&#25925;&#38556;&#30340;&#39057;&#29575;&#12290;&#30456;&#21453;&#65292;&#26412;&#25991;&#30528;&#37325;&#20110;&#22312;&#21457;&#29983;&#30636;&#24577;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#37327;&#21270;DNN&#30340;&#20934;&#30830;&#24615;&#65292;&#20063;&#23601;&#26159;&#21578;&#35785;&#25105;&#20204;&#32593;&#32476;&#22312;&#21457;&#29983;&#30636;&#24577;&#38169;&#35823;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#25105;&#20204;&#31216;&#36825;&#20010;&#25351;&#26631;&#20026;&#24377;&#24615;&#20934;&#30830;&#24615;(RA)&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;RA&#24418;&#24335;&#21270;&#22522;&#26412;&#19978;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#38169;&#35823;&#22320;&#20551;&#35774;&#22312;&#30828;&#20214;&#30636;&#24577;&#25925;&#38556;&#19979;&#65292;&#36719;&#20214;&#21464;&#37327;(&#27169;&#22411;&#26435;&#37325;/&#28608;&#27963;)&#20855;&#26377;&#30456;&#31561;&#30340;&#25925;&#38556;&#27010;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#25429;&#25417;&#20102;&#22312;&#30636;&#24577;&#25925;&#38556;&#19979;DNN&#21464;&#37327;&#30340;&#25925;&#38556;&#27010;&#29575;&#65292;&#22240;&#27492;&#25552;&#20379;&#20102;&#30001;&#30828;&#20214;&#39564;&#35777;&#30340;&#27491;&#30830;&#30340;RA&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Deep Neural Networks (DNNs) are increasingly deployed in safety critical and privacy sensitive applications such as autonomous driving and biometric authentication, it is critical to understand the fault-tolerance nature of DNNs. Prior work primarily focuses on metrics such as Failures In Time (FIT) rate and the Silent Data Corruption (SDC) rate, which quantify how often a device fails. Instead, this paper focuses on quantifying the DNN accuracy given that a transient error has occurred, which tells us how well a network behaves when a transient error occurs. We call this metric Resiliency Accuracy (RA). We show that existing RA formulation is fundamentally inaccurate, because it incorrectly assumes that software variables (model weights/activations) have equal faulty probability under hardware transient faults. We present an algorithm that captures the faulty probabilities of DNN variables under transient faults and, thus, provides correct RA estimations validated by hardware. To a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21306;&#38388;&#25311;&#24230;&#37327;&#23884;&#20837;&#65288;IQE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38750;&#23545;&#31216;&#36317;&#31163;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#28385;&#36275;&#22235;&#20010;&#26399;&#26395;&#23646;&#24615;&#65292;IQEs&#22312;&#25311;&#24230;&#37327;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36817;&#20284;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24102;&#26469;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.15120</link><description>&lt;p&gt;
&#21033;&#29992;&#21306;&#38388;&#25311;&#24230;&#37327;&#23884;&#20837;&#25913;&#36827;&#38750;&#23545;&#31216;&#36317;&#31163;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Improved Representation of Asymmetrical Distances with Interval Quasimetric Embeddings. (arXiv:2211.15120v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15120
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#21306;&#38388;&#25311;&#24230;&#37327;&#23884;&#20837;&#65288;IQE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38750;&#23545;&#31216;&#36317;&#31163;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#28385;&#36275;&#22235;&#20010;&#26399;&#26395;&#23646;&#24615;&#65292;IQEs&#22312;&#25311;&#24230;&#37327;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36817;&#20284;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24102;&#26469;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#39640;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#23545;&#31216;&#36317;&#31163;&#32467;&#26500;&#65288;&#25311;&#24230;&#37327;&#65289;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24182;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#27169;&#22411;&#34920;&#31034;&#20013;&#24341;&#20837;&#36825;&#26679;&#30340;&#25311;&#24230;&#37327;&#32467;&#26500;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25913;&#21892;&#35768;&#22810;&#20219;&#21153;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21644;&#22240;&#26524;&#20851;&#31995;&#23398;&#20064;&#31561;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#36825;&#31181;&#25311;&#24230;&#37327;&#27169;&#22411;&#20013;&#30340;&#26399;&#26395;&#23646;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#20854;&#20013;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21306;&#38388;&#25311;&#24230;&#37327;&#23884;&#20837;&#65288;IQE&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#34987;&#35774;&#35745;&#20026;&#28385;&#36275;&#25152;&#26377;&#22235;&#20010;&#20934;&#21017;&#12290;&#22312;&#19977;&#20010;&#25311;&#24230;&#37327;&#23398;&#20064;&#23454;&#39564;&#20013;&#65292;IQEs&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36817;&#20284;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Asymmetrical distance structures (quasimetrics) are ubiquitous in our lives and are gaining more attention in machine learning applications. Imposing such quasimetric structures in model representations has been shown to improve many tasks, including reinforcement learning (RL) and causal relation learning. In this work, we present four desirable properties in such quasimetric models, and show how prior works fail at them. We propose Interval Quasimetric Embedding (IQE), which is designed to satisfy all four criteria. On three quasimetric learning experiments, IQEs show strong approximation and generalization abilities, leading to better performance and improved efficiency over prior methods.  Project Page: https://www.tongzhouwang.info/interval_quasimetric_embedding  Quasimetric Learning Code Package: https://www.github.com/quasimetric-learning/torch-quasimetric
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#21453;&#28436;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#21183;SpinGNN++&#65292;&#29992;&#20110;&#30913;&#24615;&#31995;&#32479;&#30340;&#20840;&#38754;&#21407;&#23376;&#38388;&#21183;&#33021;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#33258;&#26059;-&#26684;&#23376;&#21160;&#21147;&#23398;&#35745;&#31639;&#12290;&#36890;&#36807;&#24341;&#20837;&#22797;&#26434;&#30913;&#24615;&#27169;&#22411;&#25968;&#25454;&#24182;&#39564;&#35777;&#20854;&#33021;&#21147;&#65292;SpinGNN++&#23637;&#31034;&#20102;&#31934;&#30830;&#25551;&#36848;&#30913;&#24615;&#26448;&#26009;&#33258;&#26059;-&#26684;&#23376;&#32806;&#21512;&#30340;&#33021;&#21147;&#65292;&#20026;&#30456;&#20851;&#24615;&#36136;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2211.11403</link><description>&lt;p&gt;
&#30913;&#24615;&#26448;&#26009;&#30340;&#26102;&#38388;&#21453;&#28436;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#21183;
&lt;/p&gt;
&lt;p&gt;
General time-reversal equivariant neural network potential for magnetic materials. (arXiv:2211.11403v3 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26102;&#38388;&#21453;&#28436;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#21183;SpinGNN++&#65292;&#29992;&#20110;&#30913;&#24615;&#31995;&#32479;&#30340;&#20840;&#38754;&#21407;&#23376;&#38388;&#21183;&#33021;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#33258;&#26059;-&#26684;&#23376;&#21160;&#21147;&#23398;&#35745;&#31639;&#12290;&#36890;&#36807;&#24341;&#20837;&#22797;&#26434;&#30913;&#24615;&#27169;&#22411;&#25968;&#25454;&#24182;&#39564;&#35777;&#20854;&#33021;&#21147;&#65292;SpinGNN++&#23637;&#31034;&#20102;&#31934;&#30830;&#25551;&#36848;&#30913;&#24615;&#26448;&#26009;&#33258;&#26059;-&#26684;&#23376;&#32806;&#21512;&#30340;&#33021;&#21147;&#65292;&#20026;&#30456;&#20851;&#24615;&#36136;&#30340;&#25506;&#32034;&#25552;&#20379;&#20102;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#26102;&#38388;&#21453;&#28436;E(3)&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#21644;SpinGNN++&#26694;&#26550;&#65292;&#29992;&#20110;&#26500;&#24314;&#30913;&#24615;&#31995;&#32479;&#30340;&#20840;&#38754;&#21407;&#23376;&#38388;&#21183;&#33021;&#65292;&#21253;&#25324;&#33258;&#26059;&#36712;&#36947;&#32806;&#21512;&#21644;&#38750;&#20849;&#32447;&#30913;&#30697;&#12290;SpinGNN++&#23558;&#22810;&#20219;&#21153;&#33258;&#26059;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#19982;&#26174;&#24335;&#33258;&#26059;&#26684;&#23376;&#39033;&#30456;&#32467;&#21512;&#65292;&#21253;&#25324;&#28023;&#26862;&#22561;&#12289;Dzyaloshinskii-Moriya&#12289;Kitaev&#12289;&#21333;&#31163;&#23376;&#21508;&#21521;&#24322;&#24615;&#21644;&#20108;&#27425;&#39033;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#37319;&#29992;&#26102;&#38388;&#21453;&#28436;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#39640;&#38454;&#33258;&#26059;-&#26684;&#23376;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#39564;&#35777;SpinGNN++&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#30913;&#24615;&#27169;&#22411;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#24182;&#29992;&#23427;&#26469;&#23637;&#31034;&#20854;&#33021;&#21147;&#12290;SpinGNN++&#33021;&#22815;&#20934;&#30830;&#25551;&#36848;&#21333;&#23618;CrI$_3$&#21644;CrTe$_2$&#20013;&#22797;&#26434;&#30340;&#33258;&#26059;-&#26684;&#23376;&#32806;&#21512;&#65292;&#36798;&#21040;&#20122;&#27627;&#30005;&#23376;&#20239;&#29305;&#30340;&#35823;&#24046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#33258;&#26059;-&#26684;&#23376;&#21160;&#21147;&#23398;&#35745;&#31639;&#65292;&#20174;&#32780;&#23454;&#29616;&#30456;&#20851;&#24615;&#36136;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces time-reversal E(3)-equivariant neural network and SpinGNN++ framework for constructing a comprehensive interatomic potential for magnetic systems, encompassing spin-orbit coupling and noncollinear magnetic moments. SpinGNN++ integrates multitask spin equivariant neural network with explicit spin-lattice terms, including Heisenberg, Dzyaloshinskii-Moriya, Kitaev, single-ion anisotropy, and biquadratic interactions, and employs time-reversal equivariant neural network to learn high-order spin-lattice interactions using time-reversal E(3)-equivariant convolutions. To validate SpinGNN++, a complex magnetic model dataset is introduced as a benchmark and employed to demonstrate its capabilities. SpinGNN++ provides accurate descriptions of the complex spin-lattice coupling in monolayer CrI$_3$ and CrTe$_2$, achieving sub-meV errors. Importantly, it facilitates large-scale parallel spin-lattice dynamics, thereby enabling the exploration of associated properties, including
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20020;&#24202;&#20915;&#31574;&#20013;&#27835;&#30103;&#20998;&#37197;&#20844;&#24179;&#24615;&#30340;&#22240;&#26524;&#20844;&#24179;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#32676;&#20307;&#30340;&#24322;&#36136;&#24615;&#24182;&#23545;&#20855;&#26377;&#30456;&#21516;&#21487;&#33021;&#24615;&#30340;&#24739;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#23450;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11183</link><description>&lt;p&gt;
&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#35780;&#20272;&#27835;&#30103;&#20998;&#37197;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Fairness Assessment of Treatment Allocation with Electronic Health Records. (arXiv:2211.11183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20020;&#24202;&#20915;&#31574;&#20013;&#27835;&#30103;&#20998;&#37197;&#20844;&#24179;&#24615;&#30340;&#22240;&#26524;&#20844;&#24179;&#31639;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#32676;&#20307;&#30340;&#24322;&#36136;&#24615;&#24182;&#23545;&#20855;&#26377;&#30456;&#21516;&#21487;&#33021;&#24615;&#30340;&#24739;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#23450;&#28508;&#22312;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#39046;&#22495;&#19968;&#30452;&#38754;&#20020;&#30528;&#27835;&#30103;&#24046;&#24322;&#30340;&#25345;&#32493;&#38382;&#39064;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#20020;&#24202;&#23454;&#36341;&#20013;&#27835;&#30103;&#20998;&#37197;&#20844;&#24179;&#24615;&#30340;&#25285;&#24551;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#20294;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#22522;&#20110;&#22240;&#26524;&#24615;&#30340;&#20844;&#24179;&#27010;&#24565;&#19978;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20943;&#36731;&#28151;&#26434;&#25928;&#24212;&#24182;&#25512;&#26029;&#20559;&#24046;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#20020;&#24202;&#20915;&#31574;&#20844;&#24179;&#24615;&#26102;&#65292;&#22240;&#26524;&#20844;&#24179;&#27010;&#24565;&#30340;&#24212;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#32463;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#22312;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#35780;&#20272;&#27835;&#30103;&#20998;&#37197;&#30340;&#22240;&#26524;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#26041;&#27861;&#35770;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#26524;&#20844;&#24179;&#31639;&#27861;&#26469;&#35780;&#20272;&#20020;&#24202;&#20915;&#31574;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#32771;&#34385;&#20102;&#24739;&#32773;&#32676;&#20307;&#30340;&#24322;&#36136;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#20855;&#26377;&#30456;&#21516;&#21487;&#33021;&#24615;&#30340;&#24739;&#32773;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#30830;&#23450;&#27835;&#30103;&#20998;&#37197;&#20013;&#30340;&#28508;&#22312;&#19981;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare continues to grapple with the persistent issue of treatment disparities, sparking concerns regarding the equitable allocation of treatments in clinical practice. While various fairness metrics have emerged to assess fairness in decision-making processes, a growing focus has been on causality-based fairness concepts due to their capacity to mitigate confounding effects and reason about bias. However, the application of causal fairness notions in evaluating the fairness of clinical decision-making with electronic health record (EHR) data remains an understudied domain. This study aims to address the methodological gap in assessing causal fairness of treatment allocation with electronic health records data. We propose a causal fairness algorithm to assess fairness in clinical decision-making. Our algorithm accounts for the heterogeneity of patient populations and identifies potential unfairness in treatment allocation by conditioning on patients who have the same likelihood to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#25968;&#25454;&#28857;&#29305;&#24449;&#30340;&#28789;&#27963;&#30340;&#19987;&#23478;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22312;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#19987;&#23478;&#25968;&#30446;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2211.09940</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#22522;&#20110;&#36755;&#20837;&#30340;&#19987;&#23478;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Entry Dependent Expert Selection in Distributed Gaussian Processes Using Multilabel Classification. (arXiv:2211.09940v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36755;&#20837;&#25968;&#25454;&#28857;&#29305;&#24449;&#30340;&#28789;&#27963;&#30340;&#19987;&#23478;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22312;&#20998;&#24067;&#24335;&#39640;&#26031;&#36807;&#31243;&#20013;&#30340;&#19987;&#23478;&#25968;&#30446;&#24182;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#24067;&#24335;&#35757;&#32451;&#36807;&#31243;&#65292;&#23616;&#37096;&#36817;&#20284;&#38477;&#20302;&#20102;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#30340;&#25104;&#26412;&#12290;&#38598;&#25104;&#25216;&#26415;&#23558;&#19981;&#21516;&#25968;&#25454;&#20998;&#21306;&#19978;&#35757;&#32451;&#30340;&#39640;&#26031;&#19987;&#23478;&#30340;&#23616;&#37096;&#39044;&#27979;&#36827;&#34892;&#32452;&#21512;&#12290;&#38598;&#25104;&#26041;&#27861;&#36890;&#36807;&#20551;&#35774;&#23616;&#37096;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#23436;&#32654;&#24046;&#24322;&#24615;&#26469;&#32858;&#21512;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#34429;&#28982;&#23427;&#20445;&#25345;&#20102;&#21487;&#22788;&#29702;&#30340;&#32858;&#21512;&#24615;&#36136;&#65292;&#20294;&#36825;&#20010;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#34987;&#36829;&#21453;&#12290;&#23613;&#31649;&#38598;&#25104;&#26041;&#27861;&#36890;&#36807;&#20551;&#35774;&#19987;&#23478;&#20043;&#38388;&#23384;&#22312;&#20381;&#36182;&#20851;&#31995;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#19982;&#25152;&#28041;&#21450;&#30340;&#19987;&#23478;&#25968;&#37327;&#25104;&#31435;&#26041;&#20851;&#32852;&#12290;&#36890;&#36807;&#23454;&#26045;&#19968;&#31181;&#19987;&#23478;&#36873;&#25321;&#31574;&#30053;&#65292;&#26368;&#32456;&#30340;&#32858;&#21512;&#27493;&#39588;&#20351;&#29992;&#36739;&#23569;&#30340;&#19987;&#23478;&#24182;&#19988;&#26356;&#39640;&#25928;&#12290;&#28982;&#32780;&#65292;&#23558;&#19968;&#20010;&#22266;&#23450;&#30340;&#19987;&#23478;&#38598;&#20998;&#37197;&#32473;&#27599;&#20010;&#26032;&#25968;&#25454;&#28857;&#30340;&#36873;&#25321;&#26041;&#27861;&#19981;&#33021;&#32534;&#30721;&#27599;&#20010;&#21807;&#19968;&#25968;&#25454;&#28857;&#30340;&#29305;&#23450;&#23646;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20837;&#21475;&#25968;&#25454;&#28857;&#29305;&#24449;&#30340;&#28789;&#27963;&#30340;&#19987;&#23478;&#36873;&#25321;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;
&lt;/p&gt;
&lt;p&gt;
By distributing the training process, local approximation reduces the cost of the standard Gaussian Process. An ensemble technique combines local predictions from Gaussian experts trained on different partitions of the data. Ensemble methods aggregate models' predictions by assuming a perfect diversity of local predictors. Although it keeps the aggregation tractable, this assumption is often violated in practice. Even though ensemble methods provide consistent results by assuming dependencies between experts, they have a high computational cost, which is cubic in the number of experts involved. By implementing an expert selection strategy, the final aggregation step uses fewer experts and is more efficient. However, a selection approach that assigns a fixed set of experts to each new data point cannot encode the specific properties of each unique data point. This paper proposes a flexible expert selection approach based on the characteristics of entry data points. To this end, we inves
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#35814;&#32454;&#12289;&#22810;&#26679;&#21644;&#36924;&#30495;&#30340;&#35821;&#20041;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#31867;&#21035;&#65292;&#26469;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09782</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#22522;&#30784;&#35843;&#25972;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Neural Network Robustness via Adversarial Pivotal Tuning. (arXiv:2211.09782v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#35814;&#32454;&#12289;&#22810;&#26679;&#21644;&#36924;&#30495;&#30340;&#35821;&#20041;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#31867;&#21035;&#65292;&#26469;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#35780;&#20272;&#20854;&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25805;&#32437;&#25110;&#20559;&#31163;&#33021;&#21147;&#22240;&#27492;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#20462;&#25913;&#36890;&#24120;&#20165;&#21253;&#25324;&#26368;&#23567;&#30340;&#21464;&#21270;&#65292;&#20173;&#33021;&#27450;&#39575;&#20998;&#31867;&#22120;&#65292;&#32780;&#29616;&#20195;&#26041;&#27861;&#23545;&#27492;&#36234;&#26469;&#36234;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#22270;&#20687;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#35821;&#20041;&#25805;&#20316;&#30340;&#26041;&#24335;&#22312;&#27492;&#30446;&#30340;&#19978;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20027;&#35201;&#23616;&#38480;&#20110;&#26679;&#24335;&#12289;&#39068;&#33394;&#25110;&#23646;&#24615;&#30340;&#21464;&#21270;&#12290;&#34429;&#28982;&#34920;&#36798;&#21147;&#24378;&#65292;&#20294;&#36825;&#20123;&#25805;&#20316;&#24182;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20840;&#37096;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#22312;&#35814;&#32454;&#12289;&#22810;&#26679;&#21644;&#36924;&#30495;&#30340;&#26041;&#24335;&#19978;&#23545;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#22270;&#20687;&#30340;&#31867;&#21035;&#12290;&#21463;&#26368;&#36817;&#30340;&#22522;&#20110;GAN&#30340;&#22270;&#20687;&#21453;&#28436;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Adversarial Pivo&#8221;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The robustness of image classifiers is essential to their deployment in the real world. The ability to assess this resilience to manipulations or deviations from the training data is thus crucial. These modifications have traditionally consisted of minimal changes that still manage to fool classifiers, and modern approaches are increasingly robust to them. Semantic manipulations that modify elements of an image in meaningful ways have thus gained traction for this purpose. However, they have primarily been limited to style, color, or attribute changes. While expressive, these manipulations do not make use of the full capabilities of a pretrained generative model. In this work, we aim to bridge this gap. We show how a pretrained image generator can be used to semantically manipulate images in a detailed, diverse, and photorealistic way while still preserving the class of the original image. Inspired by recent GAN-based image inversion methods, we propose a method called Adversarial Pivo
&lt;/p&gt;</description></item><item><title>NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.04370</link><description>&lt;p&gt;
NESTER&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04370
&lt;/p&gt;
&lt;p&gt;
NESTER&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31070;&#32463;&#31526;&#21495;&#21270;&#26041;&#27861;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#65292;&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#36827;&#34892;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#12290;&#22522;&#20110;&#28508;&#22312;&#32467;&#26524;&#26694;&#26550;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#27599;&#31181;&#29616;&#26377;&#30340;&#25216;&#26415;&#37117;&#36890;&#36807;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#27491;&#21017;&#21270;&#22120;&#26469;&#35299;&#20915;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#29305;&#23450;&#26041;&#38754;&#65292;&#20363;&#22914;&#25511;&#21046;&#20542;&#21521;&#24471;&#20998;&#12289;&#24378;&#21046;&#38543;&#26426;&#21270;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#31216;&#20026;&#31070;&#32463;&#31526;&#21495;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#22120;&#65288;NESTER&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#27835;&#30103;&#25928;&#26524;&#35780;&#20272;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;NESTER&#23558;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#25152;&#26377;&#35201;&#27714;&#38598;&#25104;&#21040;&#19968;&#20010;&#26694;&#26550;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25991;&#29486;&#20013;&#20351;&#29992;&#30340;&#24402;&#32435;&#20559;&#32622;&#30340;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;DSL&#65289;&#12290;&#25105;&#20204;&#36824;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;NESTER&#22312;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20840;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;NESTER&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzSDN&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#23548;&#33268;SDN&#31995;&#32479;&#22833;&#36133;&#30340;&#26377;&#25928;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#20934;&#30830;&#30340;&#25925;&#38556;&#35825;&#23548;&#27169;&#22411;&#20197;&#34920;&#24449;&#27492;&#31867;&#31995;&#32479;&#22833;&#36133;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2210.15469</link><description>&lt;p&gt;
&#23398;&#20064;SDN&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#27979;&#35797;&#22833;&#36133;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Failure-Inducing Models for Testing Software-Defined Networks. (arXiv:2210.15469v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzSDN&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29983;&#25104;&#23548;&#33268;SDN&#31995;&#32479;&#22833;&#36133;&#30340;&#26377;&#25928;&#27979;&#35797;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#20934;&#30830;&#30340;&#25925;&#38556;&#35825;&#23548;&#27169;&#22411;&#20197;&#34920;&#24449;&#27492;&#31867;&#31995;&#32479;&#22833;&#36133;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#23450;&#20041;&#32593;&#32476;&#65288;SDN&#65289;&#33021;&#22815;&#23454;&#29616;&#30001;&#20013;&#22830;&#21270;&#30340;&#36719;&#20214;&#25511;&#21046;&#22120;&#31649;&#29702;&#30340;&#28789;&#27963;&#21644;&#39640;&#25928;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#25511;&#21046;&#22120;&#21487;&#33021;&#20250;&#30772;&#22351;&#22522;&#20110;SDN&#30340;&#31995;&#32479;&#30340;&#24213;&#23618;&#36890;&#20449;&#32593;&#32476;&#65292;&#22240;&#27492;&#24517;&#39035;&#36827;&#34892;&#20180;&#32454;&#27979;&#35797;&#12290;&#24403;&#22522;&#20110;SDN&#30340;&#31995;&#32479;&#22833;&#36133;&#26102;&#65292;&#24037;&#31243;&#24072;&#38656;&#35201;&#31934;&#30830;&#22320;&#20102;&#35299;&#20854;&#21457;&#29983;&#26465;&#20214;&#20197;&#24212;&#23545;&#27492;&#31867;&#25925;&#38556;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FuzzSDN&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#23548;&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#65288;1&#65289;&#29983;&#25104;&#23548;&#33268;SDN&#31995;&#32479;&#22833;&#36133;&#30340;&#26377;&#25928;&#27979;&#35797;&#25968;&#25454;&#21644;&#65288;2&#65289;&#23398;&#20064;&#20934;&#30830;&#30340;&#25925;&#38556;&#35825;&#23548;&#27169;&#22411;&#65292;&#20197;&#34920;&#24449;&#27492;&#31867;&#31995;&#32479;&#22833;&#36133;&#30340;&#26465;&#20214;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;FuzzSDN&#26159;&#39318;&#27425;&#23581;&#35797;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;SDN&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#24320;&#28304;SDN&#25511;&#21046;&#22120;&#25511;&#21046;&#30340;&#31995;&#32479;&#26469;&#35780;&#20272;FuzzSDN&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;FuzzSDN&#19982;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;SDN&#27169;&#31946;&#27979;&#35797;&#26041;&#27861;&#21644;&#20004;&#20010;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software-defined networks (SDN) enable flexible and effective communication systems that are managed by centralized software controllers. However, such a controller can undermine the underlying communication network of an SDN-based system and thus must be carefully tested. When an SDN-based system fails, in order to address such a failure, engineers need to precisely understand the conditions under which it occurs. In this article, we introduce a machine learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating effective test data leading to failures in SDN-based systems and (2) learning accurate failure-inducing models that characterize conditions under which such system fails. To our knowledge, FuzzSDN is the first attempt to simultaneously address these two objectives for SDNs. We evaluate FuzzSDN by applying it to systems controlled by two open-source SDN controllers. Further, we compare FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#32773;&#20998;&#31163;&#31995;&#32479;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#31574;&#30053;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#24182;&#21487;&#36866;&#24212;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#12290;</title><link>http://arxiv.org/abs/2210.13690</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#22810;&#38454;&#27573;&#32858;&#31867;&#23454;&#26102;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#20154;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering. (arXiv:2210.13690v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#65292;&#21487;&#20197;&#35299;&#20915;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#32773;&#20998;&#31163;&#31995;&#32479;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#27492;&#31574;&#30053;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#22788;&#29702;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#65292;&#24182;&#21487;&#36866;&#24212;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35828;&#35805;&#32773;&#20998;&#31163;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#20110;&#25552;&#39640;&#20998;&#31163;&#32467;&#26524;&#30340;&#36136;&#37327;&#65292;&#20294;&#21516;&#26102;&#20063;&#36234;&#26469;&#36234;&#20851;&#27880;&#25552;&#39640;&#20998;&#31163;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#8212;&#8212;&#23545;&#20110;&#19981;&#21516;&#38271;&#24230;&#30340;&#36755;&#20837;&#20351;&#29992;&#19981;&#21516;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23884;&#20837;&#24335;&#28436;&#35762;&#20154;&#20998;&#31163;&#24212;&#29992;&#30340;&#22810;&#26041;&#38754;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#37319;&#29992;&#21518;&#22791;&#32858;&#31867;&#22120;&#26469;&#22788;&#29702;&#30701;&#24418;&#24335;&#36755;&#20837;&#65307;&#20027;&#32858;&#31867;&#22120;&#29992;&#20110;&#22788;&#29702;&#20013;&#31561;&#38271;&#24230;&#30340;&#36755;&#20837;&#65307;&#22312;&#32463;&#36807;&#20027;&#32858;&#31867;&#22120;&#20043;&#21069;&#65292;&#37319;&#29992;&#39044;&#22788;&#29702;&#32858;&#31867;&#22120;&#26469;&#21387;&#32553;&#38271;&#24418;&#24335;&#36755;&#20837;&#12290;&#20027;&#32858;&#31867;&#22120;&#21644;&#39044;&#22788;&#29702;&#32858;&#31867;&#22120;&#37117;&#21487;&#20197;&#37197;&#32622;&#20026;&#35745;&#31639;&#22797;&#26434;&#24230;&#19978;&#30028;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#12290;&#35813;&#22810;&#38454;&#27573;&#32858;&#31867;&#31574;&#30053;&#23545;&#20110;CPU&#12289;&#20869;&#23384;&#21644;&#30005;&#27744;&#39044;&#31639;&#32039;&#24352;&#30340;&#27969;&#24335;&#23884;&#20837;&#24335;&#28436;&#35762;&#32773;&#20998;&#31163;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent research advances in speaker diarization mostly focus on improving the quality of diarization results, there is also an increasing interest in improving the efficiency of diarization systems. In this paper, we demonstrate that a multi-stage clustering strategy that uses different clustering algorithms for input of different lengths can address multi-faceted challenges of on-device speaker diarization applications. Specifically, a fallback clusterer is used to handle short-form inputs; a main clusterer is used to handle medium-length inputs; and a pre-clusterer is used to compress long-form inputs before they are processed by the main clusterer. Both the main clusterer and the pre-clusterer can be configured with an upper bound of the computational complexity to adapt to devices with different resource constraints. This multi-stage clustering strategy is critical for streaming on-device speaker diarization systems, where the budgets of CPU, memory and battery are tight.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#28237;&#27969;&#20013;&#31890;&#23376;&#32858;&#38598;&#30340;&#31354;&#38388;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#20026;&#26089;&#26399;&#34892;&#26143;&#24418;&#25104;&#20013;&#23576;&#22467;&#31890;&#23376;&#30340;&#30896;&#25758;&#29983;&#38271;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2210.02339</link><description>&lt;p&gt;
&#28237;&#27969;&#20013;&#30340;&#31890;&#23376;&#32858;&#38598;&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#31354;&#38388;&#21644;&#32479;&#35745;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Particle clustering in turbulence: Prediction of spatial and statistical properties with deep learning. (arXiv:2210.02339v2 [astro-ph.EP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;&#20102;&#28237;&#27969;&#20013;&#31890;&#23376;&#32858;&#38598;&#30340;&#31354;&#38388;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#20026;&#26089;&#26399;&#34892;&#26143;&#24418;&#25104;&#20013;&#23576;&#22467;&#31890;&#23376;&#30340;&#30896;&#25758;&#29983;&#38271;&#25552;&#20379;&#20102;&#29702;&#35770;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#25311;&#19982;&#28237;&#27969;&#27969;&#20307;&#27668;&#21160;&#32806;&#21512;&#30340;&#31890;&#23376;&#32858;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;Athena++&#27969;&#20307;&#21160;&#21147;&#23398;&#20195;&#30721;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#27169;&#22359;&#65292;&#22312;&#21508;&#21521;&#21516;&#24615;&#24378;&#36843;&#22411;&#27700;&#21160;&#21147;&#28237;&#27969;&#30340;&#21608;&#26399;&#22495;&#20869;&#27169;&#25311;&#20102;&#22312;&#29233;&#26222;&#26031;&#22374;&#38459;&#21147;&#21306;&#30340;&#31890;&#23376;&#21160;&#21147;&#23398;&#12290;&#36825;&#19968;&#35774;&#32622;&#26159;&#19968;&#20010;&#29702;&#24819;&#21270;&#27169;&#22411;&#65292;&#19982;&#26089;&#26399;&#34892;&#26143;&#24418;&#25104;&#20013;&#24494;&#31859;&#21040;&#27627;&#31859;&#23610;&#23544;&#30340;&#23576;&#22467;&#31890;&#23376;&#30340;&#30896;&#25758;&#29983;&#38271;&#30456;&#20851;&#12290;&#25105;&#20204;&#20351;&#29992;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;U-Net&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22522;&#20110;&#30456;&#24212;&#27969;&#20307;&#22330;&#36755;&#20837;&#30340;&#31890;&#23376;&#23494;&#24230;&#21644;&#36895;&#24230;&#22330;&#30340;&#19977;&#32500;&#32593;&#26684;&#34920;&#31034;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#21306;&#25429;&#25417;&#21040;&#20102;&#32858;&#38598;&#31890;&#23376;&#30340;&#32420;&#32500;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#23494;&#24230;&#22330;&#65288;&#24452;&#21521;&#20998;&#24067;&#20989;&#25968;&#65289;&#21644;&#36895;&#24230;&#22330;&#65288;&#30456;&#23545;&#36895;&#24230;&#21644;&#30456;&#23545;&#24452;&#21521;&#36895;&#24230;&#65289;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the utility of deep learning for modeling the clustering of particles that are aerodynamically coupled to turbulent fluids. Using a Lagrangian particle module within the Athena++ hydrodynamics code, we simulate the dynamics of particles in the Epstein drag regime within a periodic domain of isotropic forced hydrodynamic turbulence. This setup is an idealized model relevant to the collisional growth of micron to mm-sized dust particles in early stage planet formation. The simulation data are used to train a U-Net deep learning model to predict gridded three-dimensional representations of the particle density and velocity fields, given as input the corresponding fluid fields. The trained model qualitatively captures the filamentary structure of clustered particles in a highly non-linear regime. We assess model fidelity by calculating metrics of the density field (the radial distribution function) and of the velocity field (the relative velocity and the relative radial velo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#20013;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#36827;&#34892;&#27604;&#36739;&#21457;&#29616;&#65292;&#21407;&#22987;&#30340;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2209.14624</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#24615;&#65311;&#19968;&#20010;&#20851;&#20110;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#20013;&#26159;&#21542;&#38656;&#35201;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#36827;&#34892;&#27604;&#36739;&#21457;&#29616;&#65292;&#21407;&#22987;&#30340;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#20462;&#21098;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#22240;&#20026;&#20154;&#20204;&#21457;&#29616;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23433;&#20840;&#22320;&#21024;&#38500;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22823;&#37327;&#26435;&#37325;&#12290;&#33258;&#37027;&#26102;&#20197;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20462;&#21098;&#26041;&#27861;&#65292;&#27599;&#19968;&#31181;&#37117;&#22768;&#31216;&#27604;&#21069;&#19968;&#31181;&#26356;&#22909;&#12290;&#20170;&#22825;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#37117;&#20381;&#36182;&#20110;&#20351;&#29992;&#37325;&#35201;&#24615;&#20998;&#25968;&#12289;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#33719;&#21462;&#21453;&#39304;&#25110;&#22522;&#20110;&#21551;&#21457;&#24335;&#20462;&#21098;&#35268;&#21017;&#31561;&#22797;&#26434;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#31181;&#24341;&#20837;&#22797;&#26434;&#24615;&#26159;&#21542;&#30495;&#30340;&#26377;&#24517;&#35201;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#20462;&#21098;&#32467;&#26524;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#19982;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21363;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;(Global MP)&#12290;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#26681;&#25454;&#26435;&#37325;&#30340;&#22823;&#23567;&#23545;&#20854;&#36827;&#34892;&#25490;&#24207;&#24182;&#20462;&#21098;&#26368;&#23567;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#22312;&#20854;&#21407;&#22987;&#24418;&#24335;&#20013;&#65292;&#23427;&#26159;&#26368;&#31616;&#21333;&#30340;&#20462;&#21098;&#25216;&#26415;&#20043;&#19968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21407;&#22987;&#30340;&#20840;&#23616;&#24133;&#24230;&#20462;&#21098;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#24182;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. In this work, we question whether this pattern of introducing complexity is really necessary to achieve better pruning results. We benchmark these SOTA techniques against a naive pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and ach
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;</title><link>http://arxiv.org/abs/2207.05631</link><description>&lt;p&gt;
DGPO: &#20351;&#29992;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#21457;&#29616;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DGPO&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#20219;&#21153;&#26102;&#21457;&#29616;&#22810;&#31181;&#31574;&#30053;&#65292;&#20174;&#32780;&#25552;&#39640;&#31574;&#30053;&#40065;&#26834;&#24615;&#21644;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;&#20048;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#37117;&#35797;&#22270;&#23547;&#25214;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#21333;&#20010;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#22810;&#31181;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#20363;&#22914;&#65292;&#20351;&#26234;&#33021;&#20307;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#26356;&#21152;&#26377;&#36259;&#65292;&#25110;&#32773;&#25552;&#39640;&#31574;&#30053;&#23545;&#24847;&#22806;&#24178;&#25200;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26679;&#21270;&#31574;&#30053;&#20248;&#21270;&#65288;DGPO&#65289;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#35299;&#20915;&#32473;&#23450;&#20219;&#21153;&#30340;&#22810;&#31181;&#31574;&#30053;&#12290;&#19982;&#29616;&#26377;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#36890;&#36807;&#22312;&#21333;&#27425;&#36816;&#34892;&#20013;&#35757;&#32451;&#20849;&#20139;&#31574;&#30053;&#32593;&#32476;&#23454;&#29616;&#27492;&#30446;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#22810;&#26679;&#24615;&#30446;&#26631;&#30340;&#20869;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#20132;&#26367;&#32422;&#26463;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#22806;&#22312;&#22870;&#21169;&#12290;&#25105;&#20204;&#23558;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#27010;&#29575;&#25512;&#26029;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#31574;&#30053;&#36845;&#20195;&#26469;&#26368;&#22823;&#21270;&#24471;&#21040;&#30340;&#19979;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#26377;&#25928;&#22320;&#21457;&#29616;&#22810;&#26679;&#21270;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324; Atari &#28216;&#25103;&#21644; Mujoco &#27169;&#25311;&#22120;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#31995;&#21015;&#24615;&#33021;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variet
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.15462</link><description>&lt;p&gt;
&#36890;&#36807;&#40723;&#21169;&#19968;&#33268;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26469;&#25913;&#36827;&#35270;&#35273; grounding
&lt;/p&gt;
&lt;p&gt;
Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.15462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; AMC &#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#35206;&#30422;&#26377;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65292;&#21363;&#32534;&#30721;&#21306;&#22495;&#12290;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#35270;&#35273; grounding &#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#21331;&#36234;&#65292;&#26377;&#26395;&#25104;&#20026;&#35270;&#35273; grounding &#39046;&#22495;&#30340;&#26032;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#32536;&#30340;&#25439;&#22833;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#19982;&#21306;&#22495;&#32423;&#27880;&#37322;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30446;&#26631;&#31216;&#20026; Attention Mask Consistency&#65288;AMC&#65289;&#65292;&#24182;&#35777;&#26126;&#23427;&#20135;&#29983;&#20102;&#27604;&#20381;&#36182;&#20110;&#21306;&#22495;&#32423;&#27880;&#37322;&#30340;&#27169;&#22411;&#26356;&#20248;&#36234;&#30340;&#35270;&#35273; grounding &#24615;&#33021;&#12290; AMC &#36890;&#36807;&#40723;&#21169;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#25513;&#30721;&#65292;&#22312;&#21253;&#21547;&#27492;&#31867;&#27880;&#37322;&#30340;&#22270;&#20687;&#20013;&#65292;&#25226;&#23427;&#20204;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#20027;&#35201;&#38598;&#20013;&#22312;&#27880;&#37322;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#20869;&#12290;&#29305;&#21035;&#22320;&#65292;&#19968;&#20010;&#22312;&#26631;&#20934;&#35270;&#35273;-&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#20043;&#19978;&#29992; AMC &#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312; Flickr30k &#35270;&#35273; grounding &#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;86.59%&#30340;&#26368;&#26032;&#32467;&#26524;&#65292;&#30456;&#27604;&#26368;&#20339;&#32467;&#26524;&#33719;&#24471;&#20102;5.48%&#30340;&#32477;&#23545;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#25351;&#20195;&#34920;&#36798;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a margin-based loss for vision-language model pretraining that encourages gradient-based explanations that are consistent with region-level annotations. We refer to this objective as Attention Mask Consistency (AMC) and demonstrate that it produces superior visual grounding performance compared to models that rely instead on region-level annotations for explicitly training an object detector such as Faster R-CNN. AMC works by encouraging gradient-based explanation masks that focus their attention scores mostly within annotated regions of interest for images that contain such annotations. Particularly, a model trained with AMC on top of standard vision-language modeling objectives obtains a state-of-the-art accuracy of 86.59% in the Flickr30k visual grounding benchmark, an absolute improvement of 5.48% when compared to the best previous model. Our approach also performs exceedingly well on established benchmarks for referring expression comprehension and offers the added bene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24358;&#29366;&#31232;&#30095;&#24615;&#65292;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2206.03482</link><description>&lt;p&gt;
&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#20013;&#30340;&#24358;&#29366;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chordal Sparsity for SDP-based Neural Network Verification. (arXiv:2206.03482v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24358;&#29366;&#31232;&#30095;&#24615;&#65292;&#26088;&#22312;&#25913;&#21892;&#29616;&#26377;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#35768;&#22810;&#26032;&#20852;&#25216;&#26415;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#65292;&#20294;&#39564;&#35777;&#20854;&#27491;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24050;&#30693;&#32593;&#32476;&#36755;&#20986;&#23545;&#20110;&#21363;&#20351;&#26159;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#20063;&#38750;&#24120;&#25935;&#24863;&#21644;&#33030;&#24369;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#19981;&#21487;&#39044;&#27979;&#21644;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#30340;&#39118;&#38505;&#12290;&#24555;&#36895;&#32780;&#20934;&#30830;&#22320;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#23545;&#20854;&#24191;&#27867;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#36817;&#24180;&#26469;&#24050;&#32463;&#24320;&#21457;&#20986;&#22810;&#31181;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20110;&#25913;&#36827;&#22522;&#20110;&#21322;&#23450;&#35268;&#21010;&#65288;SDP&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#25216;&#26415;&#12290;&#36825;&#20123;&#25216;&#26415;&#22312;&#20445;&#30041;&#20984;&#38382;&#39064;&#24418;&#24335;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#34920;&#36798;&#22797;&#26434;&#20960;&#20309;&#32422;&#26463;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#25193;&#23637;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36215;&#28857;&#26159;Fazlyab&#31561;&#20154;&#25552;&#20986;&#30340;DeepSDP&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20108;&#27425;&#32422;&#26463;&#23558;&#39564;&#35777;&#38382;&#39064;&#25277;&#35937;&#20026;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;SDP&#12290;&#28982;&#32780;&#65292;&#24403;&#32593;&#32476;&#35268;&#27169;&#22686;&#38271;&#26102;&#65292;&#35299;&#20915;&#36825;&#20010;SDP&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#19988;&#32791;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are central to many emerging technologies, but verifying their correctness remains a major challenge. It is known that network outputs can be sensitive and fragile to even small input perturbations, thereby increasing the risk of unpredictable and undesirable behavior. Fast and accurate verification of neural networks is therefore critical to their widespread adoption, and in recent years a variety of methods have been developed as a response to this problem. In this paper, we focus on improving semidefinite programming (SDP) based techniques for neural network verification. Such techniques offer the power of expressing complex geometric constraints while retaining a convex problem formulation, but in practice, scalability remains a major issue. Our starting point is the DeepSDP framework proposed by Fazlyab et al, which uses quadratic constraints to abstract the verification problem into a large-scale SDP. When the network size grows, however, solving this SDP quickly 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;&#65288;CRP&#65289;&#26041;&#27861;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#20102;&#8220;&#20309;&#22320;&#8221;&#21644;&#8220;&#20309;&#29289;&#8221;&#20004;&#20010;&#38382;&#39064;&#30340;&#35299;&#31572;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#22270;&#35889;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.03208</link><description>&lt;p&gt;
&#20174;&#24402;&#22240;&#22270;&#21040;&#21487;&#29702;&#35299;&#30340;&#20154;&#31867;&#35299;&#37322;&#65306;&#36890;&#36807;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;&#65288;CRP&#65289;&#26041;&#27861;&#65292;&#23558;&#23616;&#37096;&#21644;&#20840;&#23616;&#35266;&#28857;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#20010;&#21035;&#39044;&#27979;&#25552;&#20379;&#20102;&#8220;&#20309;&#22320;&#8221;&#21644;&#8220;&#20309;&#29289;&#8221;&#20004;&#20010;&#38382;&#39064;&#30340;&#35299;&#31572;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#22270;&#35889;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#26088;&#22312;&#20351;&#24403;&#20170;&#24378;&#22823;&#20294;&#19981;&#36879;&#26126;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#36879;&#26126;&#12290;&#32780;&#23616;&#37096;&#30340;XAI&#26041;&#27861;&#36890;&#36807;&#24402;&#22240;&#22270;&#35299;&#37322;&#20010;&#21035;&#39044;&#27979;&#65292;&#20174;&#32780;&#30830;&#23450;&#37325;&#35201;&#29305;&#24449;&#20986;&#29616;&#30340;&#20301;&#32622;&#65288;&#20294;&#19981;&#25552;&#20379;&#26377;&#20851;&#23427;&#20204;&#20195;&#34920;&#20160;&#20040;&#30340;&#20449;&#24687;&#65289;&#65292;&#32780;&#20840;&#23616;&#35299;&#37322;&#25216;&#26415;&#21017;&#21487;&#35270;&#21270;&#27169;&#22411;&#36890;&#24120;&#23398;&#20064;&#32534;&#30721;&#30340;&#27010;&#24565;&#12290;&#22240;&#27492;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#21482;&#25552;&#20379;&#20102;&#37096;&#20998;&#27934;&#23519;&#21147;&#65292;&#24182;&#23558;&#35299;&#37322;&#27169;&#22411;&#30340;&#36127;&#25285;&#30041;&#32473;&#29992;&#25143;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#27010;&#24565;&#30456;&#20851;&#20256;&#25773;&#65288;CRP&#65289;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#35266;&#28857;&#65292;&#20174;&#32780;&#33021;&#22815;&#22238;&#31572;&#20010;&#21035;&#39044;&#27979;&#30340;&#8220;&#20309;&#22320;&#8221;&#21644;&#8220;&#20309;&#29289;&#8221;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;CRP&#22914;&#20309;&#25552;&#20379;&#26356;&#20855;&#20154;&#31867;&#35299;&#37322;&#24615;&#30340;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#22270;&#35889;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of eXplainable Artificial Intelligence (XAI) aims to bring transparency to today's powerful but opaque deep learning models. While local XAI methods explain individual predictions in form of attribution maps, thereby identifying where important features occur (but not providing information about what they represent), global explanation techniques visualize what concepts a model has generally learned to encode. Both types of methods thus only provide partial insights and leave the burden of interpreting the model's reasoning to the user. In this work we introduce the Concept Relevance Propagation (CRP) approach, which combines the local and global perspectives and thus allows answering both the "where" and "what" questions for individual predictions. We demonstrate the capability of our method in various settings, showcasing that CRP leads to more human interpretable explanations and provides deep insights into the model's representation and reasoning through concept atlases, 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#29983;&#23384;&#24378;&#30423;&#38382;&#39064;&#65292;&#36825;&#26159;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#21464;&#31181;&#12290;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#29983;&#23384;&#36951;&#25022;&#65292;&#21516;&#26102;&#35201;&#27714;&#31639;&#27861;&#21152;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2206.03019</link><description>&lt;p&gt;
&#29983;&#23384;&#24378;&#30423;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Survival Bandit Problem. (arXiv:2206.03019v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.03019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#29983;&#23384;&#24378;&#30423;&#38382;&#39064;&#65292;&#36825;&#26159;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#19968;&#20010;&#26032;&#21464;&#31181;&#12290;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#29983;&#23384;&#36951;&#25022;&#65292;&#21516;&#26102;&#35201;&#27714;&#31639;&#27861;&#21152;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;(MAB)&#30340;&#19968;&#20010;&#26032;&#21464;&#31181;&#65292;&#31216;&#20026;&#29983;&#23384;&#24378;&#30423;&#38382;&#39064;(S-MAB)&#12290;&#34429;&#28982;&#22312;&#36825;&#20004;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#37117;&#26159;&#26368;&#22823;&#21270;&#25152;&#35859;&#30340;&#32047;&#31215;&#22870;&#21169;&#65292;&#20294;&#22312;&#36825;&#20010;&#26032;&#30340;&#21464;&#31181;&#20013;&#65292;&#22914;&#26524;&#32047;&#31215;&#22870;&#21169;&#20302;&#20110;&#39044;&#35774;&#30340;&#38408;&#20540;&#65292;&#31243;&#24207;&#23558;&#34987;&#20013;&#26029;&#12290;&#36825;&#20010;&#31616;&#21333;&#20294;&#26410;&#34987;&#25506;&#35752;&#30340;MAB&#25193;&#23637;&#28304;&#33258;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#12290;&#20363;&#22914;&#65292;&#24403;&#23545;&#33258;&#24895;&#24739;&#32773;&#36827;&#34892;&#20004;&#31181;&#33647;&#29289;&#30340;&#27979;&#35797;&#26102;&#65292;&#20154;&#20204;&#30340;&#20581;&#24247;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#65292;&#22914;&#26524;&#20986;&#29616;&#20005;&#37325;&#21103;&#20316;&#29992;&#25110;&#32773;&#30142;&#30149;&#32508;&#21512;&#30151;&#27809;&#26377;&#24471;&#21040;&#27835;&#30103;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20013;&#26029;&#23454;&#39564;&#12290;&#20174;&#29702;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;S-MAB&#26159;&#31532;&#19968;&#31181;&#21487;&#33021;&#20013;&#26029;&#25110;&#19981;&#20013;&#26029;&#30340;MAB&#21464;&#31181;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;S-MAB&#36827;&#34892;&#24418;&#24335;&#21270;&#65292;&#23558;&#20854;&#30446;&#26631;&#23450;&#20041;&#20026;&#25152;&#35859;&#30340;&#29983;&#23384;&#36951;&#25022;&#30340;&#26368;&#23567;&#21270;&#65292;&#33258;&#28982;&#25512;&#24191;&#20102;MAB&#30340;&#36951;&#25022;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21516;&#26102;&#26368;&#23567;&#21270;&#29983;&#23384;&#36951;&#25022;&#21644;&#21152;&#36895;&#25910;&#25947;&#30340;&#36866;&#29992;&#20110;S-MAB&#30340;&#31639;&#27861;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and study a new variant of the multi-armed bandit problem (MAB), called the survival bandit problem (S-MAB). While in both problems, the objective is to maximize the so-called cumulative reward, in this new variant, the procedure is interrupted if the cumulative reward falls below a preset threshold. This simple yet unexplored extension of the MAB follows from many practical applications. For example, when testing two medicines against each other on voluntary patients, people's health are at stake, and it is necessary to be able to interrupt experiments if serious side effects occur or if the disease syndromes are not dissipated by the treatment. From a theoretical perspective, the S-MAB is the first variant of the MAB where the procedure may or may not be interrupted. We start by formalizing the S-MAB and we define its objective as the minimization of the so-called survival regret, which naturally generalizes the regret of the MAB. Then, we show that the objective of the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.11677</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#21322;&#30417;&#30563;&#32858;&#31867;&#65306;&#36328;&#36234;&#20102;&#20449;&#24687;&#29702;&#35770;&#38376;&#27099;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11677
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#31232;&#30095;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#32858;&#31867;&#22312;&#31232;&#30095;&#22270;&#19978;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#22359;&#27169;&#22411;&#26159;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#32467;&#26500;&#25968;&#25454;&#32858;&#31867;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#22522;&#26412;&#38543;&#26426;&#22270;&#27169;&#22411;&#12290;&#25968;&#21313;&#24180;&#26469;&#23545;&#35813;&#38382;&#39064;&#30340;&#24191;&#27867;&#30740;&#31350;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;Kesten-Stigum&#38376;&#27099;&#22788;&#30340;&#30456;&#21464;&#29616;&#35937;&#29305;&#21035;&#26377;&#36259;&#65292;&#20174;&#25968;&#23398;&#21644;&#24212;&#29992;&#35282;&#24230;&#37117;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23427;&#34920;&#26126;&#65292;&#22914;&#26524;&#27169;&#22411;&#21442;&#25968;&#22312;&#26576;&#20010;&#38376;&#27099;&#20197;&#19979;&#65292;&#22522;&#20110;&#32593;&#32476;&#25299;&#25169;&#30340;&#20219;&#20309;&#20272;&#35745;&#22120;&#22312;&#31232;&#30095;&#22270;&#19978;&#37117;&#19981;&#33021;&#27604;&#38543;&#26426;&#29468;&#27979;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#31245;&#24494;&#25193;&#23637;&#35270;&#37326;&#21040;&#26222;&#36941;&#23384;&#22312;&#30340;&#21322;&#30417;&#30563;&#35774;&#32622;&#65292;&#36825;&#26679;&#30340;&#22522;&#26412;&#38480;&#21046;&#23558;&#23436;&#20840;&#28040;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#25581;&#31034;&#20986;&#20219;&#24847;&#19968;&#37096;&#20998;&#26631;&#35760;&#65292;&#21487;&#20197;&#22312;&#25972;&#20010;&#21442;&#25968;&#22495;&#20869;&#23545;&#26816;&#27979;&#38382;&#39064;&#36827;&#34892;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#32452;&#21512;&#30340;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;&#20248;&#21270;&#30340;&#65292;&#29992;&#20110;&#23558;&#26631;&#31614;&#20449;&#24687;&#19982;&#22270;&#32467;&#26500;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#38543;&#26426;&#22359;&#27169;&#22411;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#24102;&#26469;&#20102;&#20840;&#26032;&#30340;&#35270;&#35282;&#65292;&#26631;&#24535;&#30528;&#31232;&#30095;&#22270;&#32858;&#31867;&#39046;&#22495;&#30340;&#37325;&#22823;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic block model is a canonical random graph model for clustering and community detection on network-structured data. Decades of extensive study on the problem have established many profound results, among which the phase transition at the Kesten-Stigum threshold is particularly interesting both from a mathematical and an applied standpoint. It states that no estimator based on the network topology can perform substantially better than chance on sparse graphs if the model parameter is below certain threshold. Nevertheless, if we slightly extend the horizon to the ubiquitous semi-supervised setting, such a fundamental limitation will disappear completely. We prove that with arbitrary fraction of the labels revealed, the detection problem is feasible throughout the parameter domain. Moreover, we introduce two efficient algorithms, one combinatorial and one based on optimization, to integrate label information with graph structures. Our work brings a new perspective to stochasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#22270;&#25193;&#25955;&#21327;&#21516;&#36807;&#28388;&#65288;MGDCF&#65289;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#19982;&#20256;&#32479;&#30340;&#19968;&#23618;NRL&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#20026;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#25552;&#20379;&#20102;&#26032;&#30340;&#36317;&#31163;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2204.02338</link><description>&lt;p&gt;
MGDCF: &#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#22270;&#25193;&#25955;&#36827;&#34892;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#30340;&#36317;&#31163;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MGDCF: Distance Learning via Markov Graph Diffusion for Neural Collaborative Filtering. (arXiv:2204.02338v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.02338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#22270;&#25193;&#25955;&#21327;&#21516;&#36807;&#28388;&#65288;MGDCF&#65289;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#19982;&#20256;&#32479;&#30340;&#19968;&#23618;NRL&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#20026;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;&#25552;&#20379;&#20102;&#26032;&#30340;&#36317;&#31163;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34987;&#29992;&#20110;&#26500;&#24314;&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#26469;&#39044;&#27979;&#29992;&#25143;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#19982;&#20256;&#32479;&#30340;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#65288;NRL&#65289;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#20102;&#35299;&#36824;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#20132;&#25442;&#20004;&#31181;&#31867;&#22411;&#30340;&#36317;&#31163;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#23637;&#31034;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GNN&#30340;CF&#27169;&#22411;&#19982;&#20256;&#32479;&#30340;&#19968;&#23618;NRL&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#39532;&#23572;&#21487;&#22827;&#22270;&#25193;&#25955;&#21327;&#21516;&#36807;&#28388;&#65288;MGDCF&#65289;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;GNN&#35270;&#20026;&#19968;&#20010;&#19981;&#21487;&#35757;&#32451;&#30340;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#21487;&#20197;&#20026;&#19968;&#20010;&#22522;&#20110;&#23436;&#20840;&#36830;&#25509;&#23618;&#32534;&#30721;&#19978;&#19979;&#25991;&#29305;&#24449;&#30340;&#20256;&#32479;NRL&#27169;&#22411;&#26500;&#36896;&#39030;&#28857;&#30340;&#24120;&#25968;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#36825;&#31181;&#31616;&#21270;&#26377;&#21161;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have recently been utilized to build Collaborative Filtering (CF) models to predict user preferences based on historical user-item interactions. However, there is relatively little understanding of how GNN-based CF models relate to some traditional Network Representation Learning (NRL) approaches. In this paper, we show the equivalence between some state-of-the-art GNN-based CF models and a traditional 1-layer NRL model based on context encoding. Based on a Markov process that trades off two types of distances, we present Markov Graph Diffusion Collaborative Filtering (MGDCF) to generalize some state-of-the-art GNN-based CF models. Instead of considering the GNN as a trainable black box that propagates learnable user/item vertex embeddings, we treat GNNs as an untrainable Markov process that can construct constant context features of vertices for a traditional NRL model that encodes context features with a fully-connected layer. Such simplification can help
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21644;&#24358;&#31232;&#30095;&#24615;&#30340;Chordal-LipSDP&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#21322;&#23450;&#32422;&#26463;&#20998;&#35299;&#20026;&#22810;&#20010;&#36739;&#23567;&#30340;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2204.00846</link><description>&lt;p&gt;
&#22522;&#20110;&#21644;&#24358;&#31232;&#30095;&#24615;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;Lipschitz&#24120;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks. (arXiv:2204.00846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21644;&#24358;&#31232;&#30095;&#24615;&#30340;Chordal-LipSDP&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#21322;&#23450;&#32422;&#26463;&#20998;&#35299;&#20026;&#22810;&#20010;&#36739;&#23567;&#30340;&#32422;&#26463;&#65292;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#33021;&#22815;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#21487;&#20197;&#20445;&#35777;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#12289;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30001;&#20110;&#35745;&#31639;Lipschitz&#24120;&#25968;&#26159;NP&#38590;&#30340;&#65292;&#20272;&#35745;Lipschitz&#24120;&#25968;&#30340;&#25216;&#26415;&#24517;&#39035;&#22312;&#21487;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#26435;&#34913;&#12290;&#26412;&#25991;&#22312;&#20445;&#25345;&#38646;&#20934;&#30830;&#24230;&#25439;&#22833;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25512;&#21160;&#20102;&#21322;&#23450;&#35268;&#21010;&#25216;&#26415;LipSDP&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;LipSDP&#20855;&#26377;&#21644;&#24358;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#19968;&#31181;&#31216;&#20026;Chordal-LipSDP&#30340;&#21644;&#24358;&#31232;&#30095;&#34920;&#36848;&#12290;&#20854;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#23558;LipSDP&#30340;&#20027;&#35201;&#35745;&#31639;&#29942;&#39048;&#65292;&#21363;&#19968;&#20010;&#22823;&#22411;&#21322;&#23450;&#32422;&#26463;&#65292;&#20998;&#35299;&#20026;&#31561;&#25928;&#30340;&#22810;&#20010;&#36739;&#23567;&#21322;&#23450;&#32422;&#26463;&#65306;&#20351;&#24471;Chordal-LipSDP&#22312;&#32593;&#32476;&#28145;&#24230;&#22686;&#21152;&#26102;&#33021;&#22815;&#20248;&#20110;LipSDP&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#34920;&#36848;&#20351;&#29992;&#20102;&#19968;&#20010;&#21487;&#35843;&#30340;&#31232;&#30095;&#21442;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#26174;&#33879;&#35823;&#24046;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#32039;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lipschitz constants of neural networks allow for guarantees of robustness in image classification, safety in controller design, and generalizability beyond the training data. As calculating Lipschitz constants is NP-hard, techniques for estimating Lipschitz constants must navigate the trade-off between scalability and accuracy. In this work, we significantly push the scalability frontier of a semidefinite programming technique known as LipSDP while achieving zero accuracy loss. We first show that LipSDP has chordal sparsity, which allows us to derive a chordally sparse formulation that we call Chordal-LipSDP. The key benefit is that the main computational bottleneck of LipSDP, a large semidefinite constraint, is now decomposed into an equivalent collection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP particularly as the network depth grows. Moreover, our formulation uses a tunable sparsity parameter that enables one to gain tighter estimates without incurring a signifi
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#23450;&#20041;&#26680;&#20989;&#25968;&#65292;&#30456;&#36739;&#24050;&#30693;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#25351;&#25968;&#21152;&#36895;&#20248;&#21183;&#12290;&#30001;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;&#35757;&#32451;&#31639;&#27861;&#21463;&#21040;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23545;&#20598;&#38382;&#39064;&#21487;&#20197;&#22312;$O(M^{4.67}/\varepsilon^2)$&#20010;&#37327;&#23376;&#30005;&#36335;&#35780;&#20272;&#20013;&#35299;&#20915;&#65292;&#26680;&#21270;&#30340;&#21407;&#22987;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$&#27425;&#35780;&#20272;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2203.00031</link><description>&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The complexity of quantum support vector machines. (arXiv:2203.00031v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00031
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#21033;&#29992;&#37327;&#23376;&#30005;&#36335;&#23450;&#20041;&#26680;&#20989;&#25968;&#65292;&#30456;&#36739;&#24050;&#30693;&#30340;&#32463;&#20856;&#31639;&#27861;&#65292;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#25351;&#25968;&#21152;&#36895;&#20248;&#21183;&#12290;&#30001;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;&#35757;&#32451;&#31639;&#27861;&#21463;&#21040;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#23545;&#20598;&#38382;&#39064;&#21487;&#20197;&#22312;$O(M^{4.67}/\varepsilon^2)$&#20010;&#37327;&#23376;&#30005;&#36335;&#35780;&#20272;&#20013;&#35299;&#20915;&#65292;&#26680;&#21270;&#30340;&#21407;&#22987;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$&#27425;&#35780;&#20272;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#25903;&#25345;&#21521;&#37327;&#26426;&#20351;&#29992;&#37327;&#23376;&#30005;&#36335;&#26469;&#23450;&#20041;&#26680;&#20989;&#25968;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#19982;&#26576;&#20123;&#25968;&#25454;&#38598;&#30340;&#20219;&#20309;&#24050;&#30693;&#32463;&#20856;&#31639;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#35777;&#26126;&#30340;&#25351;&#25968;&#21152;&#36895;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#23545;&#24212;&#20110;&#36890;&#36807;&#21407;&#22987;&#24418;&#24335;&#25110;&#23545;&#20598;&#24418;&#24335;&#27714;&#35299;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#37327;&#23376;&#21147;&#23398;&#30340;&#27010;&#29575;&#24615;&#36136;&#65292;&#35757;&#32451;&#31639;&#27861;&#21463;&#21040;&#32479;&#35745;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#23545;&#20854;&#22797;&#26434;&#24615;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20598;&#38382;&#39064;&#21487;&#20197;&#22312;$O(M^{4.67}/\varepsilon^2)$&#20010;&#37327;&#23376;&#30005;&#36335;&#35780;&#20272;&#20013;&#35299;&#20915;&#65292;&#20854;&#20013;$M$&#34920;&#31034;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#65292;$\varepsilon$&#34920;&#31034;&#19982;&#31934;&#30830;&#26399;&#26395;&#20540;&#30340;&#29702;&#24819;&#32467;&#26524;&#30456;&#27604;&#30340;&#35299;&#30340;&#31934;&#24230;&#65292;&#35813;&#31934;&#24230;&#29702;&#35770;&#19978;&#21482;&#33021;&#33719;&#24471;&#12290;&#22312;&#32463;&#39564;&#21160;&#26426;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26680;&#21270;&#30340;&#21407;&#22987;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$&#27425;&#35780;&#20272;&#26469;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum support vector machines employ quantum circuits to define the kernel function. It has been shown that this approach offers a provable exponential speedup compared to any known classical algorithm for certain data sets. The training of such models corresponds to solving a convex optimization problem either via its primal or dual formulation. Due to the probabilistic nature of quantum mechanics, the training algorithms are affected by statistical uncertainty, which has a major impact on their complexity. We show that the dual problem can be solved in $O(M^{4.67}/\varepsilon^2)$ quantum circuit evaluations, where $M$ denotes the size of the data set and $\varepsilon$ the solution accuracy compared to the ideal result from exact expectation values, which is only obtainable in theory. We prove under an empirically motivated assumption that the kernelized primal problem can alternatively be solved in $O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$ evaluations by employing a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#22806;&#25512;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#65288;GEM-UOT&#65289;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.03618</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65306;&#26799;&#24230;&#26041;&#27861;&#65292;&#31232;&#30095;&#24615;&#21644;&#36924;&#36817;&#35823;&#24046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error. (arXiv:2202.03618v4 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#22806;&#25512;&#26041;&#27861;&#30340;&#26032;&#31639;&#27861;&#65288;GEM-UOT&#65289;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#36136;&#37327;&#30340;&#20004;&#20010;&#27979;&#24230;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#26368;&#20248;&#36755;&#36816;&#65288;UOT&#65289;&#65292;&#20854;&#20013;&#36890;&#36807;&#20855;&#26377;&#27491;&#21017;&#21270;&#22240;&#23376;&#964;&#30340;Kullback-Leibler&#25955;&#24230;&#25918;&#26494;&#26631;&#20934;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#30340;&#36793;&#38469;&#32422;&#26463;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#21482;&#20998;&#26512;&#20102;&#22522;&#20110;Sinkhorn&#30340;UOT&#27714;&#35299;&#22120;&#65292;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;O&#65288;&#964;log&#65288;n&#65289;/&#949;log&#65288;log&#65288;n&#65289;/&#949;&#65289;&#65289;&#65292;&#27599;&#27425;&#36845;&#20195;&#30340;&#25104;&#26412;&#20026;O&#65288;n^2&#65289;&#20197;&#36798;&#21040;&#25152;&#38656;&#30340;&#35823;&#24046;&#949;&#65292;&#20294;&#23427;&#20204;&#30340;&#23494;&#38598;&#36755;&#20986;&#36755;&#36816;&#35745;&#21010;&#20005;&#37325;&#38459;&#30861;&#20102;&#23454;&#38469;&#24212;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#34429;&#28982;&#22312;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#29992;&#20316;&#35745;&#31639;UOT&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#31232;&#30095;OT&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#23545;&#24212;&#29992;&#20110;UOT&#30340;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#27491;&#24335;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#22806;&#25512;&#26041;&#27861;&#65288;GEM-UOT&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#19968;&#20010;&#949;-&#36817;&#20284;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the Unbalanced Optimal Transport (UOT) between two measures of possibly different masses with at most $n$ components, where the marginal constraints of standard Optimal Transport (OT) are relaxed via Kullback-Leibler divergence with regularization factor $\tau$. Although only Sinkhorn-based UOT solvers have been analyzed in the literature with the iteration complexity of ${O}\big(\tfrac{\tau \log(n)}{\varepsilon} \log\big(\tfrac{\log(n)}{{\varepsilon}}\big)\big)$ and per-iteration cost of $O(n^2)$ for achieving the desired error $\varepsilon$, their positively dense output transportation plans strongly hinder the practicality. On the other hand, while being vastly used as heuristics for computing UOT in modern deep learning applications and having shown success in sparse OT problem, gradient methods applied to UOT have not been formally studied. In this paper, we propose a novel algorithm based on Gradient Extrapolation Method (GEM-UOT) to find an $\varepsilon$-approximate sol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2112.08440</link><description>&lt;p&gt;
&#27668;&#20505;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Climate-Invariant Machine Learning. (arXiv:2112.08440v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#39044;&#27979;&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65306;&#25105;&#20204;&#20351;&#29992;&#29289;&#29702;&#27169;&#22411;&#22312;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;&#30340;&#27668;&#20505;&#20013;&#23545;&#26368;&#36817;&#30340;&#36807;&#21435;&#36827;&#34892;&#22806;&#25512;&#12290;&#30446;&#21069;&#30340;&#27668;&#20505;&#27169;&#22411;&#38656;&#35201;&#23545;&#23567;&#20110;&#27169;&#22411;&#32593;&#26684;&#22823;&#23567;&#30340;&#23610;&#24230;&#19978;&#21457;&#29983;&#30340;&#36807;&#31243;&#36827;&#34892;&#34920;&#31034;&#65292;&#36825;&#20123;&#36807;&#31243;&#26159;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#35201;&#26469;&#28304;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#26377;&#26395;&#25913;&#21892;&#36825;&#31181;&#36807;&#31243;&#34920;&#31034;&#65292;&#20294;&#24448;&#24448;&#22312;&#26410;&#32463;&#35757;&#32451;&#30340;&#27668;&#20505;&#29615;&#22659;&#20013;&#22806;&#25512;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#29289;&#29702;&#21644;&#32479;&#35745;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#8212;&#8212;&#31216;&#20026;"&#27668;&#20505;&#19981;&#21464;"&#30340;&#26426;&#22120;&#23398;&#20064;&#8212;&#8212;&#23558;&#27668;&#20505;&#36807;&#31243;&#30340;&#30693;&#35782;&#32435;&#20837;ML&#31639;&#27861;&#20013;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22823;&#27668;&#27169;&#22411;&#20013;&#22312;&#24191;&#27867;&#30340;&#27668;&#20505;&#21644;&#22320;&#29702;&#26465;&#20214;&#19979;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#29289;&#29702;&#30693;&#35782;&#26126;&#30830;&#32435;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#22320;&#29699;&#31995;&#32479;&#36807;&#31243;&#27169;&#22411;&#20013;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#30340;&#19968;&#33268;&#24615;&#12289;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Projecting climate change is a generalization problem: we extrapolate the recent past using physical models across past, present, and future climates. Current climate models require representations of processes that occur at scales smaller than model grid size, which have been the main source of model projection uncertainty. Recent machine learning (ML) algorithms hold promise to improve such process representations, but tend to extrapolate poorly to climate regimes they were not trained on. To get the best of the physical and statistical worlds, we propose a new framework -- termed "climate-invariant" ML -- incorporating knowledge of climate processes into ML algorithms, and show that it can maintain high accuracy across a wide range of climate and geographic conditions in three distinct atmospheric models. Our results suggest that explicitly incorporating physical knowledge into data-driven models of Earth system processes can improve their consistency, data efficiency, and generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#25512;&#26029;&#29992;&#25143;&#20301;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2112.03452</link><description>&lt;p&gt;
&#32852;&#37030;&#20449;&#21495;&#22320;&#22270;&#20013;&#30340;&#20301;&#32622;&#27844;&#38706;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Location Leakage in Federated Signal Maps. (arXiv:2112.03452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#25512;&#26029;&#29992;&#25143;&#20301;&#32622;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of inferring user location through gradient leakage attacks in the federated learning framework, and proposes a method to protect location privacy.
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#22810;&#20010;&#31227;&#21160;&#35774;&#22791;&#25910;&#38598;&#30340;&#27979;&#37327;&#25968;&#25454;&#20013;&#39044;&#27979;&#34562;&#31389;&#32593;&#32476;&#24615;&#33021;&#65288;&#20449;&#21495;&#22320;&#22270;&#65289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20869;&#21046;&#23450;&#20102;&#38382;&#39064;&#65306;&#65288;i&#65289;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20351;&#29992;&#25143;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#35774;&#22791;&#19978;&#65307;&#65288;ii&#65289;&#27979;&#37327;&#25968;&#25454;&#26159;&#38543;&#30528;&#29992;&#25143;&#38543;&#26102;&#38388;&#31227;&#21160;&#32780;&#25910;&#38598;&#30340;&#65292;&#24182;&#20197;&#22312;&#32447;&#26041;&#24335;&#29992;&#20110;&#26412;&#22320;&#35757;&#32451;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#35802;&#23454;&#20294;&#22909;&#22855;&#30340;&#26381;&#21153;&#22120;&#65292;&#35266;&#23519;&#21442;&#19982;FL&#30340;&#30446;&#26631;&#29992;&#25143;&#30340;&#26356;&#26032;&#24182;&#20351;&#29992;&#26799;&#24230;&#27844;&#28431;&#65288;DLG&#65289;&#31867;&#22411;&#30340;&#25915;&#20987;&#25512;&#26029;&#20182;&#20204;&#30340;&#20301;&#32622;&#65292;&#35813;&#25915;&#20987;&#26368;&#21021;&#26159;&#20026;&#37325;&#26500;DNN&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#25968;&#25454;&#32780;&#24320;&#21457;&#30340;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20851;&#38190;&#35266;&#23519;&#65292;&#21363;DLG&#25915;&#20987;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#35774;&#32622;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#26412;&#22320;&#25968;&#25454;&#25209;&#27425;&#30340;&#24179;&#22343;&#20301;&#32622;&#65292;&#24182;&#22240;&#27492;&#21487;&#20197;&#29992;&#20110;&#22312;&#31895;&#30053;&#31890;&#24230;&#19978;&#37325;&#26500;&#30446;&#26631;&#29992;&#25143;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#26469;&#20445;&#25252;&#20301;&#32622;&#38544;&#31169;&#65292;&#22312;&#25105;&#20204;&#30340;s&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of predicting cellular network performance (signal maps) from measurements collected by several mobile devices. We formulate the problem within the online federated learning framework: (i) federated learning (FL) enables users to collaboratively train a model, while keeping their training data on their devices; (ii) measurements are collected as users move around over time and are used for local training in an online fashion. We consider an honest-but-curious server, who observes the updates from target users participating in FL and infers their location using a deep leakage from gradients (DLG) type of attack, originally developed to reconstruct training data of DNN image classifiers. We make the key observation that a DLG attack, applied to our setting, infers the average location of a batch of local data, and can thus be used to reconstruct the target users' trajectory at a coarse granularity. We build on this observation to protect location privacy, in our s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2103.00676</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#30740;&#23545;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26631;&#35760;&#20462;&#25913;&#23545;&#25239;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#24182;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#24182;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#25915;&#20987;&#32452;&#20214;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#26377;&#24456;&#22810;&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#23545;&#25239;&#25915;&#20987;&#12290;&#20854;&#20013;&#65292;&#32477;&#22823;&#22810;&#25968;&#25915;&#20987;&#36890;&#36807;&#20462;&#25913;&#21333;&#20010;&#25991;&#26723;&#26631;&#35760;&#26469;&#23454;&#29616;&#25104;&#21151;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#12290;&#27599;&#31181;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#37117;&#30001;&#19968;&#32452;&#29305;&#23450;&#30340;&#22522;&#26412;&#32452;&#20214;&#23450;&#20041;&#65292;&#20363;&#22914;&#23545;&#25915;&#20987;&#32773;&#30340;&#32422;&#26463;&#25110;&#29305;&#23450;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26631;&#35760;&#20462;&#25913;&#25915;&#20987;&#36827;&#34892;&#35843;&#26597;&#65292;&#24182;&#25552;&#21462;&#27599;&#31181;&#25915;&#20987;&#30340;&#32452;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19982;&#25915;&#20987;&#26080;&#20851;&#30340;&#26694;&#26550;&#26469;&#32452;&#32455;&#25105;&#20204;&#30340;&#35843;&#30740;&#65292;&#20174;&#32780;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#26377;&#25928;&#30340;&#20998;&#31867;&#65292;&#24182;&#26041;&#20415;&#36827;&#34892;&#32452;&#20214;&#27604;&#36739;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#25351;&#23548;&#26032;&#30340;&#30740;&#31350;&#20154;&#21592;&#36827;&#20837;&#36825;&#19968;&#39046;&#22495;&#65292;&#24182;&#25512;&#21160;&#23545;&#20110;&#20010;&#20307;&#25915;&#20987;&#32452;&#20214;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are now many adversarial attacks for natural language processing systems. Of these, a vast majority achieve success by modifying individual document tokens, which we call here a token-modification attack. Each token-modification attack is defined by a specific combination of fundamental components, such as a constraint on the adversary or a particular search algorithm. Motivated by this observation, we survey existing token-modification attacks and extract the components of each. We use an attack-independent framework to structure our survey which results in an effective categorisation of the field and an easy comparison of components. This survey aims to guide new researchers to this field and spark further research into individual attack components.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26494;&#24347;&#20248;&#21270;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#39118;&#38505;&#19982;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#20174;&#22797;&#26434;&#24230;&#20272;&#35745;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2004.05839</link><description>&lt;p&gt;
&#36890;&#36807;&#26494;&#24347;&#20248;&#21270;&#29702;&#35770;&#21450;&#20854;&#22312;&#25903;&#25345;&#21521;&#37327;&#26426;&#20013;&#30340;&#24212;&#29992;&#65292;&#23545;&#39118;&#38505;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. (arXiv:2004.05839v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.05839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26494;&#24347;&#20248;&#21270;&#29702;&#35770;&#65292;&#25506;&#35752;&#20102;&#39118;&#38505;&#19982;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25552;&#20986;&#20102;&#21487;&#20197;&#20174;&#22797;&#26434;&#24230;&#20272;&#35745;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#25903;&#25345;&#21521;&#37327;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#26494;&#24347;&#20248;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#35774;&#35745;&#30340;&#24191;&#27867;&#33539;&#20363;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;"&#39118;&#38505;"&#65288;&#26410;&#33021;&#28385;&#36275;&#26032;&#30340;&#12289;&#26679;&#26412;&#22806;&#32422;&#26463;&#30340;&#27010;&#29575;&#65289;&#21644;"&#22797;&#26434;&#24230;"&#65288;&#26681;&#25454;Garatti&#21644;Campi&#65288;2019&#65289;&#20013;&#20171;&#32461;&#30340;&#23450;&#20041;&#65289;&#20043;&#38388;&#30340;&#28145;&#23618;&#32852;&#31995;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#32852;&#31995;&#23545;&#24212;&#29992;&#26377;&#28145;&#36828;&#24433;&#21709;&#65292;&#22240;&#20026;&#23427;&#24847;&#21619;&#30528;&#21487;&#20197;&#20174;&#22797;&#26434;&#24230;&#20272;&#35745;&#39118;&#38505;&#65292;&#32780;&#22797;&#26434;&#24230;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#36827;&#34892;&#27979;&#37327;&#65292;&#19981;&#38656;&#35201;&#20102;&#35299;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26032;&#30340;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25193;&#22823;&#20102;Garatti&#21644;Campi&#65288;2019&#65289;&#30340;&#33539;&#22260;&#65292;&#20197;&#28085;&#30422;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32463;&#20856;&#30340;&#25903;&#25345;&#21521;&#37327;&#26041;&#27861;&#65292;&#21253;&#25324;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#22238;&#24402;&#65288;SVR&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#25968;&#25454;&#25551;&#36848;&#65288;SVDD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider optimization with relaxation, an ample paradigm to make data-driven designs. This approach was previously considered by the same authors of this work in Garatti and Campi (2019), a study that revealed a deep-seated connection between two concepts: risk (probability of not satisfying a new, out-of-sample, constraint) and complexity (according to a definition introduced in paper Garatti and Campi (2019)). This connection was shown to have profound implications in applications because it implied that the risk can be estimated from the complexity, a quantity that can be measured from the data without any knowledge of the data-generation mechanism. In the present work we establish new results. First, we expand the scope of Garatti and Campi (2019) so as to embrace a more general setup that covers various algorithms in machine learning. Then, we study classical support vector methods - including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD 
&lt;/p&gt;</description></item></channel></rss>