<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36870;&#38382;&#39064;&#30340;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#19968;&#31867;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#23454;&#29616;&#21487;&#20197;&#36798;&#21040;&#25910;&#25947;&#65292;&#24182;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#23545;&#35745;&#31639;&#26426;&#23618;&#26512;&#25104;&#20687;&#20013;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22120;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01052</link><description>&lt;p&gt;
&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#22312;&#36870;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#24615;&#65306;&#20020;&#30028;&#28857;&#21644;&#21407;&#22987;-&#23545;&#20598;&#20248;&#21270;&#30340;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Weakly Convex Regularisers for Inverse Problems: Convergence of Critical Points and Primal-Dual Optimisation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36870;&#38382;&#39064;&#30340;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#25910;&#25947;&#24615;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36890;&#36807;&#19968;&#31867;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#30340;&#23454;&#29616;&#21487;&#20197;&#36798;&#21040;&#25910;&#25947;&#65292;&#24182;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20013;&#23454;&#29616;&#20102;&#23545;&#35745;&#31639;&#26426;&#23618;&#26512;&#25104;&#20687;&#20013;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22120;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#27491;&#21017;&#21270;&#26159;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#26368;&#36817;&#26377;&#24456;&#22810;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35299;&#20915;&#36825;&#31181;&#27491;&#21017;&#21270;&#25910;&#25947;&#24615;&#30340;&#38382;&#39064;&#19978;&#65292;&#24456;&#23569;&#26377;&#20851;&#20110;&#20020;&#30028;&#28857;&#25910;&#25947;&#24615;&#30340;&#32467;&#26524;&#65292;&#32780;&#38750;&#20840;&#23616;&#26497;&#23567;&#20540;&#28857;&#30340;&#25910;&#25947;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20020;&#30028;&#28857;&#25910;&#25947;&#24615;&#30340;&#19968;&#33324;&#21270;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#26159;&#36890;&#36807;&#19968;&#31867;&#24369;&#20984;&#27491;&#21017;&#21270;&#22120;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#30456;&#20851;&#21464;&#20998;&#38382;&#39064;&#30456;&#20851;&#30340;&#21407;&#22987;-&#23545;&#20598;&#28151;&#21512;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#32473;&#23450;Kurdyka-Lojasiewicz&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;O(log(k)/k)&#30340;&#36951;&#20256;&#25910;&#25947;&#36895;&#24230;&#12290;&#26368;&#21518;&#65292;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#27491;&#21017;&#21270;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36755;&#20837;&#20026;&#24369;&#20984;&#31070;&#32463;&#32593;&#32476;&#65288;IWCNN&#65289;&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;IWCNN&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#23618;&#26512;&#25104;&#20687;&#20013;&#23398;&#20064;&#23545;&#25239;&#24615;&#27491;&#21017;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational regularisation is the primary method for solving inverse problems, and recently there has been considerable work leveraging deeply learned regularisation for enhanced performance. However, few results exist addressing the convergence of such regularisation, particularly within the context of critical points as opposed to global minima. In this paper, we present a generalised formulation of convergent regularisation in terms of critical points, and show that this is achieved by a class of weakly convex regularisers. We prove convergence of the primal-dual hybrid gradient method for the associated variational problem, and, given a Kurdyka-Lojasiewicz condition, an $\mathcal{O}(\log{k}/k)$ ergodic convergence rate. Finally, applying this theory to learned regularisation, we prove universal approximation for input weakly convex neural networks (IWCNN), and show empirically that IWCNNs can lead to improved performance of learned adversarial regularisers for computed tomography (
&lt;/p&gt;</description></item><item><title>JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.01318</link><description>&lt;p&gt;
JailbreakBench: &#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01318
&lt;/p&gt;
&lt;p&gt;
JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25915;&#20987;&#20250;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#36947;&#24503;&#25110;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#12290;&#35780;&#20272;&#36825;&#20123;&#25915;&#20987;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JailbreakBench&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#65292;&#21253;&#25324;&#20855;&#26377;100&#20010;&#29420;&#29305;&#34892;&#20026;&#30340;&#26032;&#36234;&#29425;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;JBB-Behaviors&#65289;&#12289;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25552;&#31034;&#65288;&#31216;&#20026;&#36234;&#29425;&#24037;&#20214;&#65289;&#21644;&#19968;&#20010;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01318v1 Announce Type: cross  Abstract: Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) a new jailbreaking dataset containing 100 unique behaviors, which we call JBB-Behaviors; (2) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (3) a standardized evaluation framework that i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#40065;&#26834;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;&#31639;&#27861;&#65288;RD-MC&#65289;&#65292;&#36890;&#36807;&#23558;&#26368;&#22823;&#19968;&#33268;&#24615;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#27714;&#35299;&#65292;&#20165;&#20351;&#29992;&#21333;&#32452;&#20272;&#35745;&#20174;&#32780;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#26356;&#21152;&#25239;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2403.18509</link><description>&lt;p&gt;
&#22312;&#22122;&#22768;&#38142;&#36335;&#19978;&#30340;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Distributed Maximum Consensus over Noisy Links
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#40065;&#26834;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;&#31639;&#27861;&#65288;RD-MC&#65289;&#65292;&#36890;&#36807;&#23558;&#26368;&#22823;&#19968;&#33268;&#24615;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#27714;&#35299;&#65292;&#20165;&#20351;&#29992;&#21333;&#32452;&#20272;&#35745;&#20174;&#32780;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#26356;&#21152;&#25239;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22122;&#22768;&#40065;&#26834;&#20998;&#24067;&#24335;&#26368;&#22823;&#19968;&#33268;&#24615;&#65288;RD-MC&#65289;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#22024;&#26434;&#36890;&#20449;&#38142;&#36335;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#20272;&#35745;&#26368;&#22823;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37325;&#26032;&#23450;&#20041;&#20102;&#26368;&#22823;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23558;&#20854;&#20316;&#20026;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#27714;&#35299;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#20132;&#26367;&#26041;&#21521;&#20056;&#27861;&#22120;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#19982;&#20381;&#36182;&#22810;&#32452;&#22122;&#22768;&#27745;&#26579;&#20272;&#35745;&#30340;&#29616;&#26377;&#31639;&#27861;&#19981;&#21516;&#65292;RD-MC&#37319;&#29992;&#21333;&#32452;&#20272;&#35745;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20943;&#36731;&#38142;&#36335;&#22122;&#22768;&#30340;&#24433;&#21709;&#24182;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#23558;&#31227;&#21160;&#24179;&#22343;&#24212;&#29992;&#20110;&#26412;&#22320;&#20272;&#35745;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#20223;&#30495;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RD-MC&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#22823;&#19968;&#33268;&#24615;&#31639;&#27861;&#22312;&#36890;&#20449;&#38142;&#36335;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18509v1 Announce Type: cross  Abstract: We introduce a distributed algorithm, termed noise-robust distributed maximum consensus (RD-MC), for estimating the maximum value within a multi-agent network in the presence of noisy communication links. Our approach entails redefining the maximum consensus problem as a distributed optimization problem, allowing a solution using the alternating direction method of multipliers. Unlike existing algorithms that rely on multiple sets of noise-corrupted estimates, RD-MC employs a single set, enhancing both robustness and efficiency. To further mitigate the effects of link noise and improve robustness, we apply moving averaging to the local estimates. Through extensive simulations, we demonstrate that RD-MC is significantly more robust to communication link noise compared to existing maximum-consensus algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.18252</link><description>&lt;p&gt;
&#36229;&#36234;&#23884;&#20837;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35270;&#35273;&#34920;&#26684;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30707;&#65292;&#20174;&#20855;&#26377;&#20154;&#31867;&#27880;&#37322;&#26631;&#31614;&#30340;&#30417;&#30563;&#23398;&#20064;&#21457;&#23637;&#21040;&#23545;&#40784;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#22914;CLIP&#23884;&#20837;&#65289;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Table&#65292;&#36825;&#26159;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#25552;&#20379;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#30340;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#65292;&#21253;&#25324;&#22330;&#26223;&#25551;&#36848;&#21644;&#28085;&#30422;&#31867;&#21035;&#12289;&#23646;&#24615;&#21644;&#23454;&#20363;&#32423;&#21035;&#30693;&#35782;&#30340;&#22810;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20174;GPT4V&#30340;&#23567;&#35268;&#27169;&#27880;&#37322;&#20013;&#29983;&#25104;&#35270;&#35273;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#23427;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#34920;&#26684;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15502</link><description>&lt;p&gt;
&#22312;&#32447;&#25991;&#23383;&#33258;&#21160;&#23436;&#25104;&#30340;&#39034;&#24207;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Sequential Decision-Making for Inline Text Autocomplete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15502
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#25913;&#36827;&#20102;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#65292;&#23558;&#35748;&#30693;&#36127;&#33655;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#30446;&#26631;&#65292;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#29616;&#20195;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#21151;&#33021;&#65292;&#24212;&#29992;&#20110;&#35832;&#22914;&#28040;&#24687;&#20256;&#36882;&#21644;&#30005;&#23376;&#37038;&#20214;&#25776;&#20889;&#31561;&#39046;&#22495;&#12290;&#36890;&#24120;&#65292;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#26159;&#20174;&#20855;&#26377;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38408;&#20540;&#24182;&#27809;&#26377;&#30452;&#25509;&#32771;&#34385;&#29992;&#25143;&#22240;&#26174;&#31034;&#24314;&#35758;&#32780;&#26045;&#21152;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#20363;&#22914;&#20174;&#36755;&#20837;&#20999;&#25442;&#21040;&#38405;&#35835;&#24314;&#35758;&#30340;&#19978;&#19979;&#25991;&#20197;&#21450;&#20915;&#23450;&#26159;&#21542;&#25509;&#21463;&#24314;&#35758;&#30340;&#26102;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#26469;&#25913;&#36827;&#25991;&#26412;&#36755;&#20837;&#31995;&#32479;&#20013;&#30340;&#22312;&#32447;&#33258;&#21160;&#23436;&#25104;&#24314;&#35758;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#38543;&#26102;&#38388;&#19982;&#30446;&#26631;&#29992;&#25143;&#30340;&#37325;&#22797;&#20132;&#20114;&#26469;&#23398;&#20064;&#24314;&#35758;&#31574;&#30053;&#12290;&#36825;&#31181;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#23558;&#35748;&#30693;&#36127;&#33655;&#22240;&#32032;&#32435;&#20837;&#35757;&#32451;&#33258;&#21160;&#23436;&#25104;&#27169;&#22411;&#30340;&#30446;&#26631;&#20013;&#65292;&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#36895;&#24230;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#29702;&#35770;&#26041;&#38754;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15502v1 Announce Type: new  Abstract: Autocomplete suggestions are fundamental to modern text entry systems, with applications in domains such as messaging and email composition. Typically, autocomplete suggestions are generated from a language model with a confidence threshold. However, this threshold does not directly take into account the cognitive load imposed on the user by surfacing suggestions, such as the effort to switch contexts from typing to reading the suggestion, and the time to decide whether to accept the suggestion. In this paper, we study the problem of improving inline autocomplete suggestions in text entry systems via a sequential decision-making formulation, and use reinforcement learning to learn suggestion policies through repeated interactions with a target user over time. This formulation allows us to factor cognitive load into the objective of training an autocomplete model, through a reward function based on text entry speed. We acquired theoretica
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#36229;&#22768;&#32447;&#24615;&#30452;&#25509;&#27169;&#22411;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20808;&#39564;&#30340;&#28151;&#21512;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#24449;&#36229;&#22768;&#22270;&#20687;&#25193;&#25955;&#37325;&#24314;&#38543;&#26426;&#24615;&#30340;&#32463;&#39564;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15316</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#26041;&#24046;&#30340;&#36229;&#22768;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Ultrasound Imaging based on the Variance of a Diffusion Restoration Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15316
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#36229;&#22768;&#32447;&#24615;&#30452;&#25509;&#27169;&#22411;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20808;&#39564;&#30340;&#28151;&#21512;&#37325;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;&#36827;&#34892;&#26080;&#30417;&#30563;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34920;&#24449;&#36229;&#22768;&#22270;&#20687;&#25193;&#25955;&#37325;&#24314;&#38543;&#26426;&#24615;&#30340;&#32463;&#39564;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20170;&#22825;&#36229;&#22768;&#25104;&#20687;&#22312;&#21307;&#23398;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#36229;&#22768;&#20449;&#22122;&#27604;&#20173;&#21463;&#22810;&#31181;&#22122;&#22768;&#21644;&#20266;&#24433;&#24433;&#21709;&#12290;&#32780;&#19988;&#65292;&#25552;&#39640;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#28041;&#21450;&#24179;&#34913;&#23545;&#27604;&#24230;&#12289;&#20998;&#36776;&#29575;&#21644;&#26001;&#28857;&#20445;&#30041;&#31561;&#24182;&#21457;&#22240;&#32032;&#12290;&#26368;&#36817;&#65292;&#22312;&#27169;&#22411;&#20026;&#22522;&#30784;&#21644;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#20013;&#65292;&#22312;&#35299;&#20915;&#36229;&#22768;&#22270;&#20687;&#37325;&#24314;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#37325;&#24314;&#26041;&#27861;&#65292;&#23558;&#36229;&#22768;&#32447;&#24615;&#30452;&#25509;&#27169;&#22411;&#19982;&#29983;&#25104;&#24335;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#20808;&#39564;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#21435;&#22122;&#25193;&#25955;&#24674;&#22797;&#27169;&#22411;(DDRM)&#30340;&#26080;&#30417;&#30563;&#24494;&#35843;&#12290;&#37492;&#20110;&#36229;&#22768;&#22266;&#26377;&#30340;&#20056;&#24615;&#22122;&#22768;&#29305;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#39564;&#27169;&#22411;&#65292;&#29992;&#20110;&#34920;&#24449;&#36229;&#22768;&#22270;&#20687;&#25193;&#25955;&#37325;&#24314;&#30340;&#38543;&#26426;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15316v1 Announce Type: cross  Abstract: Despite today's prevalence of ultrasound imaging in medicine, ultrasound signal-to-noise ratio is still affected by several sources of noise and artefacts. Moreover, enhancing ultrasound image quality involves balancing concurrent factors like contrast, resolution, and speckle preservation. Recently, there has been progress in both model-based and learning-based approaches addressing the problem of ultrasound image reconstruction. Bringing the best from both worlds, we propose a hybrid reconstruction method combining an ultrasound linear direct model with a learning-based prior coming from a generative Denoising Diffusion model. More specifically, we rely on the unsupervised fine-tuning of a pre-trained Denoising Diffusion Restoration Model (DDRM). Given the nature of multiplicative noise inherent to ultrasound, this paper proposes an empirical model to characterize the stochasticity of diffusion reconstruction of ultrasound images, an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25805;&#20316;&#22120;&#26694;&#26550;&#65292;&#23558;&#20154;&#22312;&#22238;&#36335;&#21407;&#21017;&#21644;&#25193;&#23637;&#29616;&#23454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30452;&#35266;&#30340;&#27807;&#36890;&#21644;&#32534;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14597</link><description>&lt;p&gt;
&#25193;&#23637;&#29616;&#23454;&#29992;&#20110;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#65306;&#19968;&#31181;&#20154;&#22312;&#22238;&#36335;&#20013;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extended Reality for Enhanced Human-Robot Collaboration: a Human-in-the-Loop Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#20027;&#30340;&#12289;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25805;&#20316;&#22120;&#26694;&#26550;&#65292;&#23558;&#20154;&#22312;&#22238;&#36335;&#21407;&#21017;&#21644;&#25193;&#23637;&#29616;&#23454;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20419;&#36827;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#30452;&#35266;&#30340;&#27807;&#36890;&#21644;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#23835;&#36215;&#20026;&#21046;&#36896;&#36807;&#31243;&#30340;&#39640;&#25928;&#29575;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#24448;&#24448;&#29306;&#29298;&#20102;&#21450;&#26102;&#21709;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24066;&#22330;&#38656;&#27714;&#21644;&#28385;&#36275;&#23450;&#21046;&#38656;&#27714;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#20154;&#26426;&#21327;&#20316;&#35797;&#22270;&#36890;&#36807;&#23558;&#26426;&#22120;&#30340;&#21147;&#37327;&#21644;&#31934;&#24230;&#19982;&#20154;&#31867;&#30340;&#26426;&#26234;&#21644;&#24863;&#30693;&#29702;&#35299;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#26412;&#25991;&#27010;&#24565;&#21270;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29616;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20154;&#22312;&#22238;&#36335;&#21407;&#21017;&#19982;&#25193;&#23637;&#29616;&#23454;&#65288;XR&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#20415;&#20110;&#20154;&#19982;&#26426;&#22120;&#20154;&#20043;&#38388;&#36827;&#34892;&#30452;&#35266;&#27807;&#36890;&#21644;&#32534;&#31243;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#27010;&#24565;&#26694;&#26550;&#39044;&#35265;&#21040;&#20102;&#20154;&#30452;&#25509;&#21442;&#19982;&#26426;&#22120;&#20154;&#23398;&#20064;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36866;&#24212;&#24615;&#21644;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#25903;&#25345;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#24378;&#35843;&#20102;&#23454;&#29616;&#36825;&#19968;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14597v1 Announce Type: cross  Abstract: The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the im
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim2Real&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#22686;&#24378;&#31574;&#30053;&#23454;&#29616;&#20102;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12354</link><description>&lt;p&gt;
&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;Sim2Real&#65306;&#21033;&#29992;&#22686;&#24378;&#35774;&#22791;&#20449;&#24687;&#25968;&#25454;&#27169;&#25311;&#30340;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented Device-Informed Data Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12354
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sim2Real&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#37325;&#24314;&#20809;&#35889;&#23398;&#20013;&#30340;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#38382;&#39064;&#65292;&#36890;&#36807;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#30340;&#23618;&#27425;&#21270;&#22686;&#24378;&#31574;&#30053;&#23454;&#29616;&#20102;&#25512;&#29702;&#36895;&#24230;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26694;&#26550;&#65292;&#21363;Sim2Real&#65292;&#29992;&#20110;&#37325;&#24314;&#20809;&#35889;&#20449;&#21495;&#65292;&#22312;&#26377;&#25928;&#25968;&#25454;&#37319;&#26679;&#21644;&#24555;&#36895;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#36827;&#34892;&#20102;&#37325;&#28857;&#30740;&#31350;&#12290;&#35813;&#24037;&#20316;&#32858;&#28966;&#20110;&#22312;&#20165;&#26377;&#35774;&#22791;&#20449;&#24687;&#27169;&#25311;&#25968;&#25454;&#21487;&#29992;&#20110;&#35757;&#32451;&#30340;&#26497;&#31471;&#24773;&#20917;&#19979;&#65292;&#37325;&#24314;&#30495;&#23454;&#19990;&#30028;&#20809;&#35889;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#31181;&#27169;&#25311;&#30340;&#25968;&#25454;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20998;&#23618;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#32531;&#35299;&#39046;&#22495;&#36716;&#31227;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20809;&#35889;&#20449;&#21495;&#37325;&#24314;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#20174;&#25105;&#20204;&#30340;&#20998;&#20809;&#20202;&#35774;&#22791;&#27979;&#37327;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;Sim2Real&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12354v1 Announce Type: new  Abstract: This work proposes a deep learning (DL)-based framework, namely Sim2Real, for spectral signal reconstruction in reconstructive spectroscopy, focusing on efficient data sampling and fast inference time. The work focuses on the challenge of reconstructing real-world spectral signals under the extreme setting where only device-informed simulated data are available for training. Such device-informed simulated data are much easier to collect than real-world data but exhibit large distribution shifts from their real-world counterparts. To leverage such simulated data effectively, a hierarchical data augmentation strategy is introduced to mitigate the adverse effects of this domain shift, and a corresponding neural network for the spectral signal reconstruction with our augmented data is designed. Experiments using a real dataset measured from our spectrometer device demonstrate that Sim2Real achieves significant speed-up during the inference w
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.12242</link><description>&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;
&lt;/p&gt;
&lt;p&gt;
Reference-based Metrics Disprove Themselves in Question Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12242
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#22312;&#38382;&#21477;&#29983;&#25104;&#20013;&#34987;&#25512;&#32763;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#22810;&#32500;&#26631;&#20934;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLEU&#21644;BERTScore&#31561;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#38382;&#21477;&#29983;&#25104;(QG)&#12290;&#26412;&#30740;&#31350;&#22312;SQuAD&#21644;HotpotQA&#31561;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#24182;&#19981;&#33021;&#20445;&#35777;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#26377;&#25928;&#24615;&#12290;&#22823;&#22810;&#25968;QG&#22522;&#20934;&#25968;&#25454;&#38598;&#21482;&#26377;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#65307;&#25105;&#20204;&#22797;&#21046;&#20102;&#27880;&#37322;&#36807;&#31243;&#24182;&#25910;&#38598;&#20102;&#21478;&#19968;&#20010;&#21442;&#32771;&#25991;&#29486;&#12290;&#39044;&#26399;&#22909;&#30340;&#25351;&#26631;&#24212;&#35813;&#23545;&#20154;&#24037;&#39564;&#35777;&#30340;&#38382;&#39064;&#30340;&#35780;&#20998;&#19981;&#20250;&#20302;&#20110;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#26032;&#25910;&#38598;&#30340;&#21442;&#32771;&#25991;&#29486;&#19978;&#65292;&#22522;&#20110;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#30340;&#32467;&#26524;&#21364;&#35777;&#26126;&#20102;&#36825;&#20123;&#25351;&#26631;&#26412;&#36523;&#26159;&#38169;&#35823;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#21442;&#32771;&#25991;&#29486;&#30340;&#25351;&#26631;&#65292;&#30001;&#22810;&#32500;&#26631;&#20934;&#32452;&#25104;&#65292;&#22914;&#33258;&#28982;&#24615;&#12289;&#21487;&#22238;&#31572;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#26631;&#20934;&#19981;&#21463;&#38480;&#20110;&#21333;&#20010;&#21442;&#32771;&#38382;&#39064;&#30340;&#21477;&#27861;&#25110;&#35821;&#20041;&#65292;&#35813;&#25351;&#26631;&#20063;&#19981;&#38656;&#35201;&#22810;&#26679;&#21270;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12242v1 Announce Type: cross  Abstract: Reference-based metrics such as BLEU and BERTScore are widely used to evaluate question generation (QG). In this study, on QG benchmarks such as SQuAD and HotpotQA, we find that using human-written references cannot guarantee the effectiveness of the reference-based metrics. Most QG benchmarks have only one reference; we replicated the annotation process and collect another reference. A good metric was expected to grade a human-validated question no worse than generated questions. However, the results of reference-based metrics on our newly collected reference disproved the metrics themselves. We propose a reference-free metric consisted of multi-dimensional criteria such as naturalness, answerability, and complexity, utilizing large language models. These criteria are not constrained to the syntactic or semantic of a single reference question, and the metric does not require a diverse set of references. Experiments reveal that our met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.11925</link><description>&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#20840;&#23616;&#26368;&#20248;&#24615;&#32780;&#26080;&#38656;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#65306;&#22522;&#20110;&#22810;&#32423;Actor-Critic&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Global Optimality without Mixing Time Oracles in Average-reward RL via Multi-level Actor-Critic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11925
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;Actor-Critic&#26694;&#26550;&#21644;&#22810;&#32423;&#33945;&#29305;&#21345;&#32599;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#26412;&#30740;&#31350;&#25104;&#21151;&#35299;&#20915;&#20102;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#20013;&#23545;&#28151;&#21512;&#26102;&#38388;&#39044;&#27979;&#30340;&#20381;&#36182;&#24615;&#65292;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;&#20110;&#28151;&#21512;&#26102;&#38388;&#30340;&#39044;&#27979;&#30340;oracle&#30693;&#35782;&#35201;&#27714;&#65292;&#21363;&#24230;&#37327;&#39532;&#23572;&#21487;&#22827;&#38142;&#22312;&#22266;&#23450;&#31574;&#30053;&#19979;&#36798;&#21040;&#20854;&#31283;&#24577;&#20998;&#24067;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#23545;&#20110;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#29699;&#25910;&#25947;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#32423;Actor-Critic&#65288;MAC&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#26799;&#24230;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#28151;&#21512;&#26102;&#38388;&#30693;&#35782;&#30340;&#20381;&#36182;&#24615;&#30340;&#26377;&#25928;&#20943;&#36731;&#65292;&#36825;&#26159;&#24179;&#22343;&#22870;&#21169;MDPs&#20840;&#23616;&#25910;&#25947;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#26368;&#20005;&#26684;&#30340;$\mathcal{O}$&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11925v1 Announce Type: new  Abstract: In the context of average-reward reinforcement learning, the requirement for oracle knowledge of the mixing time, a measure of the duration a Markov chain under a fixed policy needs to achieve its stationary distribution-poses a significant challenge for the global convergence of policy gradient methods. This requirement is particularly problematic due to the difficulty and expense of estimating mixing time in environments with large state spaces, leading to the necessity of impractically long trajectories for effective gradient estimation in practical applications. To address this limitation, we consider the Multi-level Actor-Critic (MAC) framework, which incorporates a Multi-level Monte Carlo (MLMC) gradient estimator. With our approach, we effectively alleviate the dependency on mixing time knowledge, a first for average-reward MDPs global convergence. Furthermore, our approach exhibits the tightest-available dependence of $\mathcal{O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08540</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#36807;&#24230;&#35757;&#32451;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#21487;&#38752;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language models scale reliably with over-training and on downstream tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#35268;&#24459;&#23545;&#20110;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#29992;&#30340;&#25351;&#23548;&#65292;&#20294;&#24403;&#21069;&#30340;&#32553;&#25918;&#30740;&#31350;&#19982;&#35821;&#35328;&#27169;&#22411;&#26368;&#32456;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36807;&#24230;&#35757;&#32451;&#21644;&#22522;&#20110;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#26041;&#38754;&#30340;&#36825;&#20004;&#20010;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08540v1 Announce Type: new  Abstract: Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., "Chinchilla optimal" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B para
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08335</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Sparsity Principle for Partially Observable Causal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#24863;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#22240;&#26524;&#21464;&#37327;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#27425;&#27979;&#37327;&#20165;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#29366;&#24577;&#23376;&#38598;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#19981;&#37197;&#23545;&#35266;&#23519;&#23398;&#20064;&#65292;&#20854;&#20013;&#23384;&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#35813;&#35774;&#32622;&#24314;&#31435;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65306;&#19968;&#20010;&#26159;&#20851;&#20110;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#23545;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20570;&#21442;&#25968;&#20551;&#35774;&#65292;&#21478;&#19968;&#20010;&#26159;&#23545;&#20855;&#26377;&#39640;&#26031;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08335v1 Announce Type: cross  Abstract: Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variab
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07718</link><description>&lt;p&gt;
WorkArena&#65306;Web&#20195;&#29702;&#22312;&#35299;&#20915;&#24120;&#35265;&#30693;&#35782;&#24037;&#20316;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
WorkArena: How Capable Are Web Agents at Solving Common Knowledge Work Tasks?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07718
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#22312;&#36890;&#36807;web&#27983;&#35272;&#22120;&#19982;&#36719;&#20214;&#20132;&#20114;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;WorkArena&#21644;BrowserGym&#20004;&#20010;&#24037;&#20855;&#65292;&#22312;29&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#19982;&#36719;&#20214;&#36890;&#36807;web&#27983;&#35272;&#22120;&#20132;&#20114;&#30340;&#24212;&#29992;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20851;&#27880;&#34913;&#37327;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#20219;&#21153;&#28085;&#30422;&#20102;&#21033;&#29992;&#20225;&#19994;&#36719;&#20214;&#31995;&#32479;&#30340;&#30693;&#35782;&#24037;&#20316;&#32773;&#30340;&#20856;&#22411;&#26085;&#24120;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WorkArena&#65292;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;ServiceNow&#24179;&#21488;&#30340;29&#20010;&#20219;&#21153;&#30340;&#36828;&#31243;&#20027;&#26426;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;BrowserGym&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#36825;&#20123;&#20195;&#29702;&#30340;&#29615;&#22659;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#34892;&#20026;&#21644;&#22810;&#27169;&#24577;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#26174;&#31034;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#20195;&#29702;&#22312;WorkArena&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#35201;&#23454;&#29616;&#23436;&#20840;&#20219;&#21153;&#33258;&#21160;&#21270;&#20173;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;LLMs&#20043;&#38388;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;&#25506;&#32034;&#21644;&#21457;&#23637;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07718v1 Announce Type: cross  Abstract: We study the use of large language model-based agents for interacting with software via web browsers. Unlike prior work, we focus on measuring the agents' ability to perform tasks that span the typical daily work of knowledge workers utilizing enterprise software systems. To this end, we propose WorkArena, a remote-hosted benchmark of 29 tasks based on the widely-used ServiceNow platform. We also introduce BrowserGym, an environment for the design and evaluation of such agents, offering a rich set of actions as well as multimodal observations. Our empirical evaluation reveals that while current agents show promise on WorkArena, there remains a considerable gap towards achieving full task automation. Notably, our analysis uncovers a significant performance disparity between open and closed-source LLMs, highlighting a critical area for future exploration and development in the field.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07183</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#30417;&#27979;AI&#20462;&#25913;&#30340;&#20869;&#23481;&#65306;AI&#20250;&#35758;&#21516;&#34892;&#35780;&#23457;&#20013;ChatGPT&#24433;&#21709;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#22823;&#24133;&#20462;&#25913;&#30340;&#25991;&#26412;&#27604;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;AI&#20250;&#35758;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#34987;LLMs&#22823;&#24133;&#20462;&#25913;&#65292;&#25581;&#31034;&#20102;&#29992;&#25143;&#34892;&#20026;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#22823;&#35821;&#26009;&#24211;&#20013;&#25991;&#26412;&#21487;&#33021;&#34987;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22823;&#24133;&#20462;&#25913;&#25110;&#29983;&#25104;&#30340;&#37096;&#20998;&#27604;&#20363;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#22823;&#20284;&#28982;&#27169;&#22411;&#21033;&#29992;&#19987;&#23478;&#25776;&#20889;&#21644;AI&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#65292;&#20934;&#30830;&#39640;&#25928;&#22320;&#26816;&#26597;&#35821;&#26009;&#24211;&#32423;&#21035;&#19978;&#30495;&#23454;&#19990;&#30028;LLM&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;AI&#20250;&#35758;&#19978;&#31185;&#23398;&#21516;&#34892;&#35780;&#23457;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#21457;&#29983;&#22312;ChatGPT&#21457;&#24067;&#20043;&#21518;&#65292;&#21253;&#25324;ICLR 2024&#12289;NeurIPS 2023&#12289;CoRL 2023&#21644;EMNLP 2023&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#20250;&#35758;&#25552;&#20132;&#30340;&#21516;&#34892;&#35780;&#23457;&#20013;&#65292;6.5%&#33267;16.9%&#30340;&#25991;&#26412;&#21487;&#33021;&#26159;&#30001;LLMs&#22823;&#24133;&#20462;&#25913;&#30340;&#65292;&#21363;&#36229;&#20986;&#25340;&#20889;&#26816;&#26597;&#25110;&#23567;&#24133;&#26356;&#26032;&#30340;&#33539;&#22260;&#12290;&#29983;&#25104;&#25991;&#26412;&#20986;&#29616;&#30340;&#24773;&#20917;&#20026;&#29992;&#25143;&#34892;&#20026;&#25552;&#20379;&#20102;&#35265;&#35299;&#65306;&#22312;&#25253;&#21578;&#20449;&#24515;&#36739;&#20302;&#12289;&#22312;&#25130;&#27490;&#26085;&#26399;&#21069;&#25552;&#20132;&#30340;&#35780;&#35770;&#20197;&#21450;&#20174;&#35780;&#35770;&#20844;&#21496;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07183v1 Announce Type: cross  Abstract: We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from review
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.04224</link><description>&lt;p&gt;
Aligners: &#35299;&#32806;LLMs&#21644;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligners: Decoupling LLMs and Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#23545;&#40784;&#22120;&#27169;&#22411;&#26469;&#35299;&#32806;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#23545;&#40784;&#65292;&#20197;&#20943;&#23569;&#23545;&#40784;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#23427;&#20204;&#22312;&#22823;&#22810;&#25968;&#24212;&#29992;&#20013;&#30340;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#40784;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#25104;&#26412;&#39640;&#26114;&#65292;&#24182;&#19988;&#38656;&#35201;&#20026;&#27599;&#20010;LLM&#21644;&#23545;&#40784;&#26631;&#20934;&#37325;&#22797;&#36827;&#34892;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#35757;&#32451;&#21487;&#20197;&#26681;&#25454;&#38656;&#35201;&#29992;&#20110;&#23545;&#40784;&#32473;&#23450;&#26631;&#20934;&#30340;&#20219;&#20309;LLM&#30340;&#23545;&#40784;&#27169;&#22411;&#26469;&#35299;&#32806;LLMs&#21644;&#23545;&#40784;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#23569;&#23545;&#24615;&#33021;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23545;&#40784;&#27169;&#22411;&#35757;&#32451;&#37197;&#26041;&#20165;&#20381;&#36182;&#20110;&#20351;&#29992;&#65288;&#25552;&#31034;&#30340;&#65289;LLM &#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#35843;&#25972;&#20197;&#36866;&#24212;&#21508;&#31181;&#23545;&#40784;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#8220;&#36947;&#24503;&#8221;&#23545;&#40784;&#22120;&#24182;&#22312;&#23454;&#39564;&#19978;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#26469;&#38416;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04224v1 Announce Type: cross  Abstract: Large Language Models (LLMs) need to be aligned with human expectations to ensure their safety and utility in most applications. Alignment is challenging, costly, and needs to be repeated for every LLM and alignment criterion. We propose to decouple LLMs and alignment by training aligner models that can be used to align any LLM for a given criteria on an as-needed basis, thus also reducing the potential negative impacts of alignment on performance. Our recipe for training the aligner models solely relies on synthetic data generated with a (prompted) LLM and can be easily adjusted for a variety of alignment criteria. We illustrate our method by training an "ethical" aligner and verify its efficacy empirically.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;(RATSF)&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;(RACA)&#21450;&#30693;&#35782;&#24211;&#35774;&#35745;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#27573;&#36827;&#34892;&#23458;&#26381;&#37327;&#39044;&#27979;&#65292;&#22312;&#38750;&#24179;&#31283;&#25968;&#25454;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.04180</link><description>&lt;p&gt;
RATSF&#65306;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26469;&#36171;&#33021;&#23458;&#26381;&#37327;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
RATSF: Empowering Customer Service Volume Management through Retrieval-Augmented Time-Series Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;(RATSF)&#65292;&#36890;&#36807;&#24341;&#20837;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;(RACA)&#21450;&#30693;&#35782;&#24211;&#35774;&#35745;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#25968;&#25454;&#27573;&#36827;&#34892;&#23458;&#26381;&#37327;&#39044;&#27979;&#65292;&#22312;&#38750;&#24179;&#31283;&#25968;&#25454;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#39640;&#25928;&#30340;&#23458;&#26381;&#31649;&#29702;&#31995;&#32479;&#21462;&#20915;&#20110;&#23545;&#26381;&#21153;&#37327;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#22312;&#36825;&#31181;&#25968;&#25454;&#38750;&#24179;&#31283;&#24615;&#26126;&#26174;&#30340;&#24773;&#20917;&#19979;&#65292;&#25104;&#21151;&#30340;&#39044;&#27979;&#20005;&#37325;&#20381;&#36182;&#20110;&#35782;&#21035;&#21644;&#21033;&#29992;&#31867;&#20284;&#30340;&#21382;&#21490;&#25968;&#25454;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#21608;&#26399;&#24615;&#27169;&#24335;&#12290;&#29616;&#26377;&#22522;&#20110;RNN&#25110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#36890;&#24120;&#22312;&#28789;&#27963;&#21644;&#26377;&#25928;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#36866;&#24212;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#31216;&#20026;RACA&#65292;&#23427;&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#26377;&#25928;&#21033;&#29992;&#20102;&#21382;&#21490;&#27573;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#21382;&#21490;&#24207;&#21015;&#26597;&#35810;&#34920;&#31034;&#26041;&#26696;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#24211;&#30340;&#35774;&#35745;&#12290;&#36825;&#20123;&#20851;&#38190;&#32452;&#20214;&#20849;&#21516;&#26500;&#25104;&#20102;&#25105;&#20204;&#30340;&#26816;&#32034;&#22686;&#24378;&#26102;&#24207;&#24207;&#21015;&#39044;&#27979;&#26694;&#26550;&#65288;RATSF&#65289;&#12290;RATSF&#19981;&#20165;&#22312;&#33778;&#40481;&#37202;&#24215;&#26381;&#21153;&#37327;&#39044;&#27979;&#29615;&#22659;&#20013;&#26174;&#33879;&#22686;&#24378;&#20102;&#24615;&#33021;&#65292;&#32780;&#19988;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04180v1 Announce Type: new  Abstract: An efficient customer service management system hinges on precise forecasting of service volume. In this scenario, where data non-stationarity is pronounced, successful forecasting heavily relies on identifying and leveraging similar historical data rather than merely summarizing periodic patterns. Existing models based on RNN or Transformer architectures often struggle with this flexible and effective utilization. To address this challenge, we propose an efficient and adaptable cross-attention module termed RACA, which effectively leverages historical segments in forecasting task, and we devised a precise representation scheme for querying historical sequences, coupled with the design of a knowledge repository. These critical components collectively form our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF not only significantly enhances performance in the context of Fliggy hotel service volume forecasting but,
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#26465;&#20214;&#19979;&#29305;&#24449;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#34394;&#20551;&#29305;&#24449;&#30340;&#24378;&#24230;&#20250;&#24433;&#21709;&#26680;&#24515;&#29305;&#24449;&#23398;&#20064;&#36895;&#24230;&#65292;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#19981;&#24635;&#26159;&#21487;&#20998;&#24320;&#65292;&#24182;&#19988;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;</title><link>https://arxiv.org/abs/2403.03375</link><description>&lt;p&gt;
&#22797;&#26434;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24773;&#20917;&#19979;&#29305;&#24449;&#23398;&#20064;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Complexity Matters: Dynamics of Feature Learning in the Presence of Spurious Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03375
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#26465;&#20214;&#19979;&#29305;&#24449;&#23398;&#20064;&#21160;&#24577;&#65292;&#21457;&#29616;&#20102;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#34394;&#20551;&#29305;&#24449;&#30340;&#24378;&#24230;&#20250;&#24433;&#21709;&#26680;&#24515;&#29305;&#24449;&#23398;&#20064;&#36895;&#24230;&#65292;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#19981;&#24635;&#26159;&#21487;&#20998;&#24320;&#65292;&#24182;&#19988;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#32463;&#24120;&#23558;&#34394;&#20551;&#29305;&#24449;&#23450;&#20041;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#20013;&#8220;&#26356;&#23481;&#26131;&#8221;&#23398;&#20064;&#30340;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#30456;&#23545;&#31616;&#21333;&#24615;&#30340;&#24433;&#21709;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#26368;&#32456;&#24615;&#33021;&#65292;&#32780;&#19981;&#26159;&#29305;&#24449;&#23398;&#20064;&#30340;&#23398;&#20064;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#22522;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20801;&#35768;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#30456;&#23545;&#22797;&#26434;&#24615;&#65288;&#19982;&#26680;&#24515;&#29305;&#24449;&#30456;&#27604;&#65289;&#21644;&#30456;&#20851;&#24615;&#24378;&#24230;&#65288;&#30456;&#23545;&#20110;&#26631;&#31614;&#65289;&#36827;&#34892;&#32454;&#33268;&#25511;&#21046;&#65292;&#20197;&#30740;&#31350;&#22312;&#34394;&#20551;&#30456;&#20851;&#24615;&#19979;&#29305;&#24449;&#23398;&#20064;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#35774;&#32622;&#25581;&#31034;&#20102;&#20960;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#65288;1&#65289;&#26356;&#24378;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25110;&#26356;&#31616;&#21333;&#30340;&#34394;&#20551;&#29305;&#24449;&#20250;&#20943;&#24930;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#36895;&#24230;&#65292;&#65288;2&#65289;&#34394;&#20551;&#29305;&#24449;&#21644;&#26680;&#24515;&#29305;&#24449;&#30340;&#23398;&#20064;&#38454;&#27573;&#24182;&#38750;&#24635;&#26159;&#21487;&#20197;&#34987;&#20998;&#24320;&#65292;&#65288;3&#65289;&#34394;&#20551;&#29305;&#24449;&#21363;&#20351;&#22312;&#19968;&#27573;&#26102;&#38388;&#20043;&#21518;&#20063;&#19981;&#20250;&#34987;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03375v1 Announce Type: new  Abstract: Existing research often posits spurious features as "easier" to learn than core features in neural network optimization, but the impact of their relative simplicity remains under-explored. Moreover they mainly focus on the end performance intead of the learning dynamics of feature learning. In this paper, we propose a theoretical framework and associated synthetic dataset grounded in boolean function analysis which allows for fine-grained control on the relative complexity (compared to core features) and correlation strength (with respect to the label) of spurious features to study the dynamics of feature learning under spurious correlation. Our setup uncovers several interesting phenomenon: (1) stronger spurious correlations or simpler spurious features slow down the rate of learning for the core features, (2) learning phases of spurious features and core features are not always separable, (3) spurious features are not forgotten even af
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21363;&#21487;&#23454;&#29616;&#22312;&#25152;&#26377;&#36755;&#20837;&#21644;&#35757;&#32451;&#26102;&#21051;&#37117;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#36825;&#31181;&#31561;&#21464;&#24615;&#22312;&#31163;&#24320;&#27969;&#24418;&#26102;&#21644;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#30340;&#20219;&#20309;&#26550;&#26500;&#37117;&#33021;&#20445;&#25345;&#12290;</title><link>https://arxiv.org/abs/2403.03103</link><description>&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#22411;&#31561;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Emergent Equivariance in Deep Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03103
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21363;&#21487;&#23454;&#29616;&#22312;&#25152;&#26377;&#36755;&#20837;&#21644;&#35757;&#32451;&#26102;&#21051;&#37117;&#20445;&#25345;&#31561;&#21464;&#24615;&#65292;&#36825;&#31181;&#31561;&#21464;&#24615;&#22312;&#31163;&#24320;&#27969;&#24418;&#26102;&#21644;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#30340;&#20219;&#20309;&#26550;&#26500;&#37117;&#33021;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#26159;&#26263;&#20013;&#31561;&#21464;&#30340;&#27169;&#22411;&#12290;&#26356;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#31616;&#21333;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#65292;&#28145;&#24230;&#38598;&#25104;&#27169;&#22411;&#22312;&#25152;&#26377;&#36755;&#20837;&#21644;&#25152;&#26377;&#35757;&#32451;&#26102;&#21051;&#37117;&#21464;&#24471;&#31561;&#21464;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#36825;&#31181;&#31561;&#21464;&#24615;&#22312;&#31163;&#24320;&#27969;&#24418;&#26102;&#21644;&#22312;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#19979;&#30340;&#20219;&#20309;&#26550;&#26500;&#37117;&#20445;&#25345;&#12290;&#36825;&#31181;&#31561;&#21464;&#24615;&#26159;&#26032;&#20852;&#30340;&#65292;&#22240;&#20026;&#21333;&#20010;&#38598;&#25104;&#25104;&#21592;&#30340;&#39044;&#27979;&#24182;&#38750;&#31561;&#21464;&#65292;&#20294;&#23427;&#20204;&#30340;&#38598;&#20307;&#39044;&#27979;&#26159;&#31561;&#21464;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#20999;&#32447;&#26680;&#29702;&#35770;&#25512;&#23548;&#20102;&#36825;&#19968;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#35814;&#32454;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03103v1 Announce Type: new  Abstract: We demonstrate that deep ensembles are secretly equivariant models. More precisely, we show that deep ensembles become equivariant for all inputs and at all training times by simply using data augmentation. Crucially, equivariance holds off-manifold and for any architecture in the infinite width limit. The equivariance is emergent in the sense that predictions of individual ensemble members are not equivariant but their collective prediction is. Neural tangent kernel theory is used to derive this result and we verify our theoretical insights using detailed numerical experiments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.02253</link><description>&lt;p&gt;
KnowPhish&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#20197;&#22686;&#24378;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#21253;&#21547;20k&#21697;&#29260;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21487;&#29992;&#20110;&#21152;&#24378;&#29616;&#26377;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#24050;&#32473;&#20010;&#20154;&#21644;&#20225;&#19994;&#36896;&#25104;&#20102;&#37325;&#22823;&#25439;&#22833;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#39640;&#25928;&#30340;&#33258;&#21160;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#26041;&#27861;&#12290;&#22522;&#20110;&#21442;&#32771;&#30340;&#32593;&#32476;&#38035;&#40060;&#26816;&#27979;&#22120;&#65288;RBPDs&#65289;&#24050;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#27604;&#36739;&#30446;&#26631;&#32593;&#39029;&#19978;&#30340;&#26631;&#24535;&#19982;&#24050;&#30693;&#26631;&#24535;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;RBPDs&#30340;&#20027;&#35201;&#23616;&#38480;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#25163;&#21160;&#26500;&#24314;&#30340;&#21697;&#29260;&#30693;&#35782;&#24211;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#37327;&#21697;&#29260;&#65292;&#23548;&#33268;&#30001;&#20110;&#30693;&#35782;&#24211;&#20013;&#21697;&#29260;&#35206;&#30422;&#19981;&#36275;&#32780;&#20986;&#29616;&#20551;&#38452;&#24615;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30693;&#35782;&#25910;&#38598;&#27969;&#27700;&#32447;&#65292;&#37319;&#29992;&#35813;&#27969;&#27700;&#32447;&#25105;&#20204;&#25910;&#38598;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#21697;&#29260;&#30693;&#35782;&#24211;KnowPhish&#65292;&#21253;&#21547;20k&#20010;&#21697;&#29260;&#21644;&#27599;&#20010;&#21697;&#29260;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;KnowPhish&#21487;&#20197;&#29992;&#26469;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#25552;&#21319;&#29616;&#26377;RBPDs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.01537</link><description>&lt;p&gt;
&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#29992;&#20110;&#20154;&#32676;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Mixed-Strategy Nash Equilibrium for Crowd Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#21644;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#20026;&#20154;&#32676;&#23548;&#33322;&#25552;&#20379;&#20102;&#23454;&#26102;&#19988;&#21487;&#25193;&#23637;&#30340;&#20915;&#31574;&#21046;&#23450;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#20154;&#32676;&#23548;&#33322;&#25214;&#21040;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#30340;&#38382;&#39064;&#12290;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#20026;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#35880;&#30340;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#39044;&#27979;&#20154;&#32676;&#20013;&#19981;&#30830;&#23450;&#20294;&#21512;&#20316;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36890;&#24120;&#22826;&#39640;&#65292;&#26080;&#27861;&#36827;&#34892;&#21487;&#25193;&#23637;&#21644;&#23454;&#26102;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#36845;&#20195;&#36125;&#21494;&#26031;&#26356;&#26032;&#26041;&#26696;&#25910;&#25947;&#20110;&#28151;&#21512;&#31574;&#30053;&#31038;&#20132;&#23548;&#33322;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20195;&#29702;&#31574;&#30053;&#21021;&#22987;&#21270;&#20026;&#20174;&#20154;&#31867;&#25968;&#25454;&#38598;&#23398;&#20064;&#30340;&#39640;&#26031;&#36807;&#31243;&#65292;&#26469;&#26500;&#24314;&#35813;&#28216;&#25103;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#31574;&#30053;&#32435;&#20160;&#22343;&#34913;&#27169;&#22411;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#37319;&#26679;&#30340;&#20154;&#32676;&#23548;&#33322;&#26694;&#26550;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#29616;&#26377;&#23548;&#33322;&#26041;&#27861;&#20013;&#65292;&#24182;&#21487;&#22312;&#31508;&#35760;&#26412;&#30005;&#33041; CPU &#19978;&#23454;&#26102;&#36816;&#34892;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#20154;&#31867;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#23454;&#36341;&#31038;&#21306;&#65292;&#20998;&#20139;&#26368;&#20339;&#23454;&#36341;&#21644;&#32463;&#39564;&#25945;&#35757;&#65292;&#20026;&#23558;&#25968;&#25454;&#31185;&#23398;&#32435;&#20837;&#22823;&#23398;&#29289;&#29702;&#35838;&#31243;&#30340;&#20851;&#38190;&#31574;&#30053;&#21644;&#25361;&#25112;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2403.00961</link><description>&lt;p&gt;
&#22823;&#23398;&#29289;&#29702;&#23398;&#20013;&#30340;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#65306;&#26469;&#33258;&#23454;&#36341;&#31038;&#21306;&#30340;&#32463;&#39564;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#23454;&#36341;&#31038;&#21306;&#65292;&#20998;&#20139;&#26368;&#20339;&#23454;&#36341;&#21644;&#32463;&#39564;&#25945;&#35757;&#65292;&#20026;&#23558;&#25968;&#25454;&#31185;&#23398;&#32435;&#20837;&#22823;&#23398;&#29289;&#29702;&#35838;&#31243;&#30340;&#20851;&#38190;&#31574;&#30053;&#21644;&#25361;&#25112;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#31181;&#25968;&#25454;&#38598;&#30340;&#26085;&#30410;&#20016;&#23500;&#65292;&#20174;&#23567;&#35268;&#27169;&#23454;&#39564;&#25968;&#25454;&#28857;&#21040;&#22823;&#22411;&#22797;&#26434;&#25968;&#25454;&#23384;&#20648;&#24211;&#21644;&#24378;&#22823;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#29289;&#29702;&#25945;&#32946;&#24037;&#20316;&#32773;&#36234;&#26469;&#36234;&#37325;&#35270;&#35013;&#22791;&#20182;&#20204;&#30340;&#23398;&#29983;&#26377;&#25928;&#22788;&#29702;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25945;&#32946;&#24037;&#20316;&#32773;&#21487;&#33021;&#32570;&#20047;&#25945;&#25480;&#36825;&#20123;&#25216;&#33021;&#25152;&#38656;&#30340;&#25968;&#25454;&#31185;&#23398;&#22521;&#35757;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#23454;&#36341;&#31038;&#21306;&#65288;DSECOP&#65289;&#65292;&#27719;&#38598;&#20102;&#19981;&#21516;&#38498;&#26657;&#21644;&#32972;&#26223;&#30340;&#30740;&#31350;&#29983;&#21644;&#29289;&#29702;&#25945;&#32946;&#24037;&#20316;&#32773;&#65292;&#20998;&#20139;&#25972;&#21512;&#25968;&#25454;&#31185;&#23398;&#21040;&#22823;&#23398;&#29289;&#29702;&#25945;&#32946;&#20013;&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#32463;&#39564;&#25945;&#35757;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20010;&#23454;&#36341;&#31038;&#21306;&#30340;&#35265;&#35299;&#21644;&#32463;&#39564;&#65292;&#31361;&#20986;&#20102;&#22312;&#23558;&#25968;&#25454;&#31185;&#23398;&#32435;&#20837;&#21021;&#32423;&#29289;&#29702;&#35838;&#31243;&#20013;&#30340;&#20851;&#38190;&#31574;&#30053;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#25351;&#23548;&#21644;ins
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00961v1 Announce Type: cross  Abstract: With the increasing availability of diverse datasets, ranging from small-scale experimental data points to large and complex data repositories and powerful data analysis tools, it is increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned in integrating data science into undergraduate physics education. In this article, we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00165</link><description>&lt;p&gt;
TELEClass: &#31246;&#21153;&#23398;&#20016;&#23500;&#21644;LLM&#22686;&#24378;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#26631;&#31614;Taxonomy&#20013;&#30340;&#19968;&#32452;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#65306;&#20165;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#30417;&#30563;&#26469;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;&#25552;&#31034;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#23618;&#35774;&#32622;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22823;&#32780;&#32467;&#26500;&#21270;&#30340;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#25928;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#30340;Taxonomy&#39592;&#26550;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20316;&#39069;&#22806;&#30340;&#31867;&#21035;&#25351;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2402.19379</link><description>&lt;p&gt;
&#30789;&#35895;&#20154;&#32676;&#30340;&#26234;&#24935;&#65306;LLM&#38598;&#25104;&#39044;&#27979;&#33021;&#21147;&#36798;&#21040;&#20154;&#32676;&#20934;&#30830;&#29575;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19379
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#19982;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LLM&#32676;&#20307;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#36341;&#20013;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;&#20381;&#36182;&#20110;&#8220;&#32676;&#20307;&#26234;&#24935;&#8221;&#25928;&#24212;&#65292;&#21363;&#36890;&#36807;&#32858;&#21512;&#19968;&#32676;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23545;&#26410;&#26469;&#20107;&#20214;&#30340;&#39044;&#27979;&#12290;&#36807;&#21435;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39044;&#27979;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20010;&#20307;&#39044;&#27979;&#32773;&#30340;&#21069;&#27839;LLMs&#34920;&#29616;&#19981;&#20339;&#65292;&#19982;&#20154;&#31867;&#32676;&#20307;&#39044;&#27979;&#27604;&#36187;&#30340;&#40644;&#37329;&#26631;&#20934;&#30456;&#27604;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30001;&#21313;&#20108;&#20010;LLMs&#32452;&#25104;&#30340;LLM&#38598;&#25104;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;31&#20010;&#20108;&#20803;&#38382;&#39064;&#30340;&#32858;&#21512;LLM&#39044;&#27979;&#19982;&#19968;&#20010;&#26469;&#33258;&#19977;&#20010;&#26376;&#39044;&#27979;&#27604;&#36187;&#30340;925&#21517;&#20154;&#31867;&#39044;&#27979;&#32773;&#30340;&#32676;&#20307;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#32676;&#20307;&#30340;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#26080;&#20449;&#24687;&#22522;&#20934;&#65292;&#24182;&#22312;&#32479;&#35745;&#19978;&#31561;&#25928;&#20110;&#20154;&#31867;&#32676;&#20307;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#19968;&#31181;&#39034;&#20174;&#25928;&#24212;&#65292;&#24179;&#22343;&#27169;&#22411;&#39044;&#27979;&#26126;&#26174;&#39640;&#20110;50%&#65292;&#23613;&#31649;&#20960;&#20046;&#26159;&#24179;&#31561;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19379v1 Announce Type: cross  Abstract: Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.19348</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22478;&#24066;&#35745;&#31639;&#20013;&#30340;&#36328;&#22495;&#25968;&#25454;&#34701;&#21512;&#65306;&#20998;&#31867;&#12289;&#36827;&#23637;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19348
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38024;&#23545;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#26041;&#27861;&#20998;&#20026;&#22235;&#22823;&#31867;&#21035;&#65292;&#24182;&#23545;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#27169;&#24577;&#22312;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22478;&#24066;&#30340;&#19981;&#26029;&#34028;&#21187;&#21457;&#23637;&#65292;&#22478;&#24066;&#35745;&#31639;&#20316;&#20026;&#19968;&#38376;&#20851;&#38190;&#23398;&#31185;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#26469;&#28304;&#65288;&#22914;&#22320;&#29702;&#12289;&#20132;&#36890;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#29615;&#22659;&#25968;&#25454;&#65289;&#21644;&#27169;&#24577;&#65288;&#22914;&#26102;&#31354;&#12289;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65289;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#21147;&#37327;&#65292;&#25104;&#20026;&#21487;&#25345;&#32493;&#21457;&#23637;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#65292;&#25105;&#20204;&#27491;&#22312;&#35265;&#35777;&#19968;&#31181;&#21033;&#29992;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20419;&#36827;&#26234;&#24935;&#22478;&#24066;&#20013;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#34701;&#21512;&#30340;&#36235;&#21183;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20221;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#19987;&#38376;&#20026;&#22478;&#24066;&#35745;&#31639;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#25454;&#34701;&#21512;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#35843;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;&#25968;&#25454;&#35270;&#35282;&#65292;&#20197;&#29702;&#35299;&#27599;&#31181;&#27169;&#24577;&#21644;&#25968;&#25454;&#26469;&#28304;&#30340;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#26041;&#27861;&#35770;&#20998;&#31867;&#20026;&#22235;&#22823;&#20027;&#35201;&#31867;&#21035;&#65306;&#22522;&#20110;&#29305;&#24449;&#12289;&#22522;&#20110;&#23545;&#40784;&#12289;&#22522;&#20110;&#23545;&#27604;&#21644;&#22522;&#20110;&#29983;&#25104;&#30340;&#34701;&#21512;&#26041;&#27861;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#22810;&#27169;&#24577;&#22478;&#24066;&#24212;&#29992;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19348v1 Announce Type: cross  Abstract: As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applicatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;</title><link>https://arxiv.org/abs/2402.17065</link><description>&lt;p&gt;
&#39535;&#26381;&#31867;&#21035;&#26465;&#20214;GAN&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#65306;&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#36827;&#34892;&#30693;&#35782;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17065
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36739;&#20302;&#20998;&#36776;&#29575;&#36827;&#34892;&#26080;&#26465;&#20214;&#35757;&#32451;&#65292;&#20801;&#35768;&#38271;&#23614;&#31867;&#21035;&#20174;&#20449;&#24687;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25913;&#21892;&#38271;&#23614;&#25968;&#25454;&#19979;&#31867;&#21035;&#26465;&#20214;GANs&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#20110;&#20351;&#29992;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#20174;&#38271;&#23614;&#35757;&#32451;&#20998;&#24067;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#20173;&#28982;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#23384;&#22312;&#19981;&#24179;&#34913;&#30340;&#22810;&#31867;&#21035;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;GANs&#20542;&#21521;&#20110;&#20559;&#29233;&#26679;&#26412;&#26356;&#22810;&#30340;&#31867;&#21035;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#29983;&#25104;&#20302;&#36136;&#37327;&#19988;&#26679;&#26412;&#19981;&#22815;&#22810;&#26679;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#36827;&#20351;&#29992;&#38271;&#23614;&#25968;&#25454;&#35757;&#32451;&#31867;&#21035;&#26465;&#20214;GANs&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#30693;&#35782;&#20849;&#20139;&#26041;&#27861;&#65292;&#20801;&#35768;&#23614;&#37096;&#31867;&#21035;&#20174;&#35757;&#32451;&#25968;&#25454;&#26356;&#20016;&#23500;&#30340;&#31867;&#21035;&#20013;&#20511;&#37492;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#31867;&#21035;&#26465;&#20214;GAN&#26550;&#26500;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#22120;&#30340;&#36739;&#20302;&#20998;&#36776;&#29575;&#23618;&#23436;&#20840;&#26080;&#26465;&#20214;&#22320;&#36827;&#34892;&#35757;&#32451;&#65292;&#21516;&#26102;&#23558;&#31867;&#21035;&#26465;&#20214;&#29983;&#25104;&#20445;&#30041;&#32473;&#36739;&#39640;&#20998;&#36776;&#29575;&#23618;&#12290;&#22312;&#22810;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17065v1 Announce Type: cross  Abstract: Despite the extensive research on training generative adversarial networks (GANs) with limited training data, learning to generate images from long-tailed training distributions remains fairly unexplored. In the presence of imbalanced multi-class training data, GANs tend to favor classes with more samples, leading to the generation of low-quality and less diverse samples in tail classes. In this study, we aim to improve the training of class-conditional GANs with long-tailed data. We propose a straightforward yet effective method for knowledge sharing, allowing tail classes to borrow from the rich information from classes with more abundant training data. More concretely, we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers. Experiments on seve
&lt;/p&gt;</description></item><item><title>NIFTy.re&#37325;&#26032;&#26500;&#24314;&#20102;NIFTy&#30340;&#24314;&#27169;&#21407;&#21017;&#21644;&#25512;&#26029;&#31574;&#30053;&#65292;&#36890;&#36807;&#22806;&#21253;&#32321;&#37325;&#24037;&#20316;&#32473;JAX&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#36895;&#24230;&#65292;&#25552;&#21319;&#20102;&#21487;&#32500;&#25252;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;JAX&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16683</link><description>&lt;p&gt;
&#37325;&#26032;&#26500;&#24819;&#25968;&#20540;&#20449;&#24687;&#22330;&#29702;&#35770;&#65288;NIFTy.re&#65289;&#65306;&#39640;&#26031;&#36807;&#31243;&#21644;&#21464;&#20998;&#25512;&#26029;&#24211;
&lt;/p&gt;
&lt;p&gt;
Re-Envisioning Numerical Information Field Theory (NIFTy.re): A Library for Gaussian Processes and Variational Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16683
&lt;/p&gt;
&lt;p&gt;
NIFTy.re&#37325;&#26032;&#26500;&#24314;&#20102;NIFTy&#30340;&#24314;&#27169;&#21407;&#21017;&#21644;&#25512;&#26029;&#31574;&#30053;&#65292;&#36890;&#36807;&#22806;&#21253;&#32321;&#37325;&#24037;&#20316;&#32473;JAX&#65292;&#21152;&#36895;&#20102;&#27169;&#22411;&#30340;&#36895;&#24230;&#65292;&#25552;&#21319;&#20102;&#21487;&#32500;&#25252;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;JAX&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#24314;&#27169;&#21407;&#21017;&#12289;&#25193;&#23637;&#25512;&#26029;&#31574;&#30053;&#65292;&#20197;&#21450;&#23558;&#22823;&#37096;&#20998;&#32321;&#37325;&#24037;&#20316;&#22806;&#21253;&#32473;JAX&#65292;&#37325;&#26032;&#21152;&#36895;&#32534;&#20889;&#22312;NIFTy&#20013;&#30340;&#27169;&#22411;&#65292;&#22880;&#23450;&#20102;&#26032;&#31867;&#22411;&#25512;&#29702;&#26426;&#21046;&#30340;&#22522;&#30784;&#65292;&#25552;&#39640;&#20102;&#21487;&#32500;&#25252;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;NIFTy&#19982;JAX&#26426;&#22120;&#23398;&#20064;&#29983;&#24577;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16683v1 Announce Type: cross  Abstract: Imaging is the process of transforming noisy, incomplete data into a space that humans can interpret. NIFTy is a Bayesian framework for imaging and has already successfully been applied to many fields in astrophysics. Previous design decisions held the performance and the development of methods in NIFTy back. We present a rewrite of NIFTy, coined NIFTy.re, which reworks the modeling principle, extends the inference strategies, and outsources much of the heavy lifting to JAX. The rewrite dramatically accelerates models written in NIFTy, lays the foundation for new types of inference machineries, improves maintainability, and enables interoperability between NIFTy and the JAX machine learning ecosystem.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.15607</link><description>&lt;p&gt;
&#35757;&#32451;&#38750;&#32447;&#24615;Transformer&#36827;&#34892;&#39640;&#25928;&#19978;&#19979;&#25991;&#23398;&#20064;&#65306;&#29702;&#35770;&#23398;&#20064;&#21644;&#27867;&#21270;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Training Nonlinear Transformers for Efficient In-Context Learning: A Theoretical Learning and Generalization Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#31616;&#21333;&#22320;&#22686;&#21152;&#26597;&#35810;&#19982;&#26469;&#33258;&#35813;&#20219;&#21153;&#30340;&#19968;&#20123;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#24494;&#35843;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#20998;&#26512;Transformers&#20013;&#38750;&#20984;&#35757;&#32451;&#38382;&#39064;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#35757;&#32451;Transformer&#20197;&#23454;&#29616;ICL&#21450;&#30456;&#24212;&#30340;ICL&#23481;&#37327;&#30340;&#26426;&#21046;&#22823;&#22810;&#19981;&#20026;&#20154;&#30693;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20855;&#26377;&#38750;&#32447;&#24615;&#33258;&#27880;&#24847;&#21147;&#21644;&#38750;&#32447;&#24615;MLP&#30340;Transformers&#30340;&#35757;&#32451;&#21160;&#24577;&#20197;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#30340;ICL&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#32452;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#36825;&#20123;&#20219;&#21153;&#23376;&#38598;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;Transformers&#65292;&#24182;&#37327;&#21270;&#21508;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15607v1 Announce Type: new  Abstract: Transformer-based large language models have displayed impressive in-context learning capabilities, where a pre-trained model can handle new tasks without fine-tuning by simply augmenting the query with some input-output examples from that task. Despite the empirical success, the mechanics of how to train a Transformer to achieve ICL and the corresponding ICL capacity is mostly elusive due to the technical challenges of analyzing the nonconvex training problems resulting from the nonlinear self-attention and nonlinear activation in Transformers. To the best of our knowledge, this paper provides the first theoretical analysis of the training dynamics of Transformers with nonlinear self-attention and nonlinear MLP, together with the ICL generalization capability of the resulting model. Focusing on a group of binary classification tasks, we train Transformers using data from a subset of these tasks and quantify the impact of various factors
&lt;/p&gt;</description></item><item><title>Chu-ko-nu&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21311;&#21517;&#35748;&#35777;&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#31192;&#23494;&#23494;&#38053;&#32452;&#20214;&#26469;&#31361;&#30772;&#20102;&#20849;&#20139;&#20256;&#36755;&#30340;&#27010;&#29575;P&#38480;&#21046;</title><link>https://arxiv.org/abs/2402.15111</link><description>&lt;p&gt;
Chu-ko-nu&#65306;&#19968;&#31181;&#21487;&#38752;&#12289;&#39640;&#25928;&#19988;&#25903;&#25345;&#21311;&#21517;&#35748;&#35777;&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#23454;&#29616;&#65292;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
Chu-ko-nu: A Reliable, Efficient, and Anonymously Authentication-Enabled Realization for Multi-Round Secure Aggregation in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15111
&lt;/p&gt;
&lt;p&gt;
Chu-ko-nu&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#21644;&#21311;&#21517;&#35748;&#35777;&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#65292;&#36890;&#36807;&#37325;&#26032;&#20998;&#37197;&#31192;&#23494;&#23494;&#38053;&#32452;&#20214;&#26469;&#31361;&#30772;&#20102;&#20849;&#20139;&#20256;&#36755;&#30340;&#27010;&#29575;P&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#32858;&#21512;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#33021;&#22815;&#36890;&#36807;&#26412;&#22320;&#26799;&#24230;&#26356;&#26032;&#23545;&#23458;&#25143;&#31471;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#27599;&#36718;&#24517;&#39035;&#25191;&#34892;&#26114;&#36149;&#30340;&#21047;&#26032;&#35774;&#32622;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#38656;&#35201;&#22312;&#19981;&#21516;&#36718;&#27425;&#24314;&#31435;&#26032;&#30340;&#29420;&#31435;&#20110;&#36755;&#20837;&#30340;&#23494;&#38053;&#12290;&#26368;&#26032;&#30740;&#31350;Flamingo&#65288;S&amp;P 2023&#65289;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#20139;&#20256;&#36755;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#23494;&#38053;&#65292;&#20197;&#25903;&#25345;&#26381;&#21153;&#22120;&#25345;&#32493;&#25191;&#34892;&#22810;&#36718;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#23427;&#25552;&#20986;&#30340;&#20849;&#20139;&#20256;&#36755;&#26426;&#21046;&#20165;&#33021;&#20197;P&#27010;&#29575;&#23454;&#29616;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21487;&#38752;&#19988;&#25903;&#25345;&#21311;&#21517;&#35748;&#35777;&#30340;&#21517;&#20026;Chu-ko-nu&#30340;&#22810;&#36718;&#23433;&#20840;&#32858;&#21512;&#26041;&#26696;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#20849;&#20139;&#20256;&#36755;&#26041;&#38754;&#65292;Chu-ko-nu&#36890;&#36807;&#34917;&#20805;&#31192;&#23494;&#23494;&#38053;&#32452;&#20214;&#30340;&#37325;&#26032;&#20998;&#37197;&#36807;&#31243;&#65292;&#31361;&#30772;&#20102;&#27010;&#29575;P&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15111v1 Announce Type: cross  Abstract: Secure aggregation enables federated learning (FL) to perform collaborative training of clients from local gradient updates without exposing raw data. However, existing secure aggregation schemes inevitably perform an expensive fresh setup per round because each client needs to establish fresh input-independent secrets over different rounds. The latest research, Flamingo (S&amp;P 2023), designed a share-transfer-based reusable secret key to support the server continuously performing multiple rounds of aggregation. Nevertheless, the share transfer mechanism it proposed can only be achieved with P probability, which has limited reliability. To tackle the aforementioned problems, we propose a more reliable and anonymously authenticated scheme called Chu-ko-nu for multi-round secure aggregation. Specifically, in terms of share transfer, Chu-ko-nu breaks the probability P barrier by supplementing a redistribution process of secret key component
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.14683</link><description>&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Visual Hallucinations of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14683
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#30340;&#35270;&#35273;&#24187;&#35273;&#23454;&#20363;&#26469;&#26816;&#39564;&#20854;&#24615;&#33021;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#23384;&#22312;&#24187;&#35273;&#38382;&#39064;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24187;&#35273;&#65288;VH&#65289;&#24847;&#21619;&#30528;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#23545;&#22270;&#20687;&#24819;&#35937;&#20986;&#38169;&#35823;&#30340;&#32454;&#33410;&#12290;&#29616;&#26377;&#30740;&#31350;&#21457;&#29616;VH&#23454;&#20363;&#20165;&#23384;&#22312;&#20110;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#65292;&#36825;&#23548;&#33268;&#20102;&#23545;MLLM&#22312;VH&#19979;&#30340;&#24615;&#33021;&#29702;&#35299;&#23384;&#22312;&#20559;&#24046;&#65292;&#21407;&#22240;&#22312;&#20110;&#36825;&#31867;VH&#23454;&#20363;&#30340;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;VHTest&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#29983;&#25104;&#22810;&#26679;&#30340;VH&#23454;&#20363;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;VHTest&#22312;&#29616;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;COCO&#65289;&#20013;&#25214;&#21040;&#19968;&#20123;&#21021;&#22987;&#30340;VH&#23454;&#20363;&#65292;&#20026;&#27599;&#20010;VH&#27169;&#24335;&#29983;&#25104;&#19968;&#20010;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;DALL-E-3&#65289;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;VH&#22270;&#20687;&#12290;&#25105;&#20204;&#21033;&#29992;VHTest&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;VH&#27169;&#24335;&#20013;1,200&#20010;VH&#23454;&#20363;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#65288;&#20363;&#22914;GPT-4V&#12289;LLaVA-1.5&#21644;MiniGPT-v2&#65289;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#22823;&#37096;&#20998;&#23454;&#20363;&#20135;&#29983;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#25968;&#25454;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14683v1 Announce Type: cross  Abstract: Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark data
&lt;/p&gt;</description></item><item><title>MLXP&#26159;&#19968;&#20010;&#31616;&#21333;&#36731;&#37327;&#30340;&#22522;&#20110;Python&#30340;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.13831</link><description>&lt;p&gt;
MLXP&#65306;&#19968;&#20010;&#29992;&#20110;&#22312;Python&#20013;&#36827;&#34892;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#23454;&#39564;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MLXP: A framework for conducting replicable Machine Learning eXperiments in Python
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13831
&lt;/p&gt;
&lt;p&gt;
MLXP&#26159;&#19968;&#20010;&#31616;&#21333;&#36731;&#37327;&#30340;&#22522;&#20110;Python&#30340;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#65292;&#26088;&#22312;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30740;&#31350;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#22240;&#20026;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#38750;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#24182;&#20381;&#36182;&#20110;&#20247;&#22810;&#36229;&#21442;&#25968;&#36873;&#25321;&#65292;&#22914;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#30830;&#20445;&#21487;&#37325;&#29616;&#21644;&#21487;&#22797;&#21046;&#30340;&#32467;&#26524;&#23545;&#20110;&#25512;&#36827;&#35813;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#36827;&#34892;&#31995;&#32479;&#21270;&#21644;&#32452;&#32455;&#33391;&#22909;&#30340;&#23454;&#39564;&#65292;&#20174;&#32780;&#24471;&#20986;&#31283;&#20581;&#30340;&#32467;&#35770;&#65292;&#21364;&#38656;&#35201;&#25237;&#20837;&#22823;&#37327;&#25216;&#26415;&#24037;&#20316;&#12290;&#20026;&#20102;&#35299;&#20915;&#20302;&#37319;&#32435;&#29575;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;MLXP&#65292;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#24320;&#28304;&#12289;&#31616;&#21333;&#12289;&#36731;&#37327;&#32423;&#23454;&#39564;&#31649;&#29702;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13831v1 Announce Type: new  Abstract: Replicability in machine learning (ML) research is increasingly concerning due to the utilization of complex non-deterministic algorithms and the dependence on numerous hyper-parameter choices, such as model architecture and training datasets. Ensuring reproducible and replicable results is crucial for advancing the field, yet often requires significant technical effort to conduct systematic and well-organized experiments that yield robust conclusions. Several tools have been developed to facilitate experiment management and enhance reproducibility; however, they often introduce complexity that hinders adoption within the research community, despite being well-handled in industrial settings. To address the challenge of low adoption, we propose MLXP, an open-source, simple, and lightweight experiment management tool based on Python, available at https://github.com/inria-thoth/mlxp . MLXP streamlines the experimental process with minimal p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12038</link><description>&lt;p&gt;
Self-AMPLIFY&#65306;&#36890;&#36807;&#33258;&#25105;&#20107;&#21518;&#35299;&#37322;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#65292;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Self-AMPLIFY&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24212;&#29992;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#22522;&#20110;&#21407;&#22240;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#30340;&#24615;&#33021;&#12290;Self-AMPLIFY&#26159;&#19968;&#20010;3&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#26679;&#26412;&#12289;&#29983;&#25104;&#29702;&#30001;&#21644;&#26500;&#24314;&#26368;&#32456;&#25552;&#31034;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#38656;&#35201;&#25512;&#29702;&#33021;&#21147;&#30340;SLMs&#21644;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;Self-AMPLIFY&#30340;&#24615;&#33021;&#65306;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;Self-AMPLIFY&#22312;&#19982;&#31454;&#20105;&#23545;&#25163;&#30456;&#27604;&#34920;&#29616;&#20986;&#33394;&#12290;Self-AMPLIFY&#26159;&#31532;&#19968;&#20010;&#23558;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#24212;&#29992;&#20110;SLMs&#30340;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#35299;&#37322;&#24182;&#25552;&#39640;&#23427;&#20204;&#33258;&#36523;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12038v1 Announce Type: new  Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a full
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#25512;&#26029;&#28216;&#25103;&#21442;&#25968;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26500;&#24314;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08902</link><description>&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#36125;&#21494;&#26031;&#21453;&#21521;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Auto-Encoding Bayesian Inverse Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#25512;&#26029;&#28216;&#25103;&#21442;&#25968;&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26469;&#26500;&#24314;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#20849;&#21516;&#29615;&#22659;&#20013;&#20114;&#21160;&#26102;&#65292;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#21160;&#20250;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#20915;&#31574;&#65292;&#32780;&#38750;&#21512;&#20316;&#21160;&#24577;&#28216;&#25103;&#33258;&#28982;&#22320;&#25429;&#25417;&#21040;&#20102;&#36825;&#31181;&#32806;&#21512;&#12290;&#28982;&#32780;&#65292;&#22312;&#20132;&#20114;&#24335;&#36816;&#21160;&#35268;&#21010;&#20013;&#65292;&#26234;&#33021;&#20307;&#36890;&#24120;&#27809;&#26377;&#23436;&#25972;&#30340;&#28216;&#25103;&#27169;&#22411;&#65292;&#20363;&#22914;&#30001;&#20110;&#20854;&#20182;&#29609;&#23478;&#30340;&#30446;&#26631;&#26159;&#26410;&#30693;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36870;&#21521;&#28216;&#25103;&#38382;&#39064;&#65292;&#20854;&#20013;&#28216;&#25103;&#30340;&#26576;&#20123;&#23646;&#24615;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#65292;&#24517;&#39035;&#26681;&#25454;&#35266;&#27979;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#12290;&#29616;&#26377;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#26041;&#27861;&#35299;&#20915;&#36870;&#21521;&#28216;&#25103;&#38382;&#39064;&#26102;&#20165;&#25552;&#20379;&#26410;&#30693;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#32780;&#19981;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#23450;&#37327;&#21270;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#21442;&#25968;&#20540;&#33021;&#35299;&#37322;&#35266;&#27979;&#34892;&#20026;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#37319;&#29992;&#36125;&#21494;&#26031;&#35266;&#28857;&#26500;&#24314;&#20102;&#28216;&#25103;&#21442;&#25968;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;&#20026;&#20102;&#20351;&#25512;&#26029;&#21487;&#34892;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20869;&#23884;&#21487;&#24494;&#20998;&#28216;&#25103;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08902v1 Announce Type: cross Abstract: When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#20869;&#31215;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#12290;&#24471;&#21040;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#20851;&#31995;&#20989;&#25968;&#36924;&#36817;&#25152;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08856</link><description>&lt;p&gt;
&#20851;&#20110;&#20851;&#31995;&#20989;&#25968;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Approximation of relation functions and attention mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08856
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#20869;&#31215;&#30340;&#36817;&#20284;&#24615;&#36136;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20316;&#20026;&#36890;&#29992;&#36924;&#36817;&#22120;&#30340;&#33021;&#21147;&#12290;&#24471;&#21040;&#20102;&#23545;&#31216;&#21644;&#38750;&#23545;&#31216;&#20851;&#31995;&#20989;&#25968;&#36924;&#36817;&#25152;&#38656;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#29305;&#24449;&#26144;&#23556;&#30340;&#20869;&#31215;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#34987;&#29992;&#20110;&#24314;&#27169;&#36755;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#31215;&#30340;&#36817;&#20284;&#24615;&#36136;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23618;&#24863;&#30693;&#26426;&#33258;&#36523;&#30340;&#20869;&#31215;&#26159;&#23545;&#31216;&#27491;&#23450;&#20851;&#31995;&#20989;&#25968;&#30340;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#23545;&#20110;&#38750;&#23545;&#31216;&#20851;&#31995;&#20989;&#25968;&#65292;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340;&#20869;&#31215;&#26159;&#19968;&#20010;&#36890;&#29992;&#36924;&#36817;&#22120;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#37117;&#24471;&#21040;&#20102;&#36798;&#21040;&#32473;&#23450;&#36924;&#36817;&#31934;&#24230;&#25152;&#38656;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#30340;&#30028;&#38480;&#12290;&#23545;&#31216;&#24773;&#20917;&#19979;&#65292;&#20989;&#25968;&#31867;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#26680;&#20989;&#25968;&#65292;&#32780;&#23545;&#31216;&#24773;&#20917;&#19979;&#20989;&#25968;&#31867;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#20013;&#30340;&#26680;&#20989;&#25968;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;&#36924;&#36817;&#32467;&#26524;&#34987;&#24212;&#29992;&#20110;&#20998;&#26512;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08856v1 Announce Type: new Abstract: Inner products of neural network feature maps arises in a wide variety of machine learning frameworks as a method of modeling relations between inputs. This work studies the approximation properties of inner products of neural networks. It is shown that the inner product of a multi-layer perceptron with itself is a universal approximator for symmetric positive-definite relation functions. In the case of asymmetric relation functions, it is shown that the inner product of two different multi-layer perceptrons is a universal approximator. In both cases, a bound is obtained on the number of neurons required to achieve a given accuracy of approximation. In the symmetric case, the function class can be identified with kernels of reproducing kernel Hilbert spaces, whereas in the asymmetric case the function class can be identified with kernels of reproducing kernel Banach spaces. Finally, these approximation results are applied to analyzing the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20445;&#35777;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#36827;&#34892;&#38750;&#26657;&#20934;&#39044;&#27979;&#65292;&#20294;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#20107;&#20214;&#38598;&#21512;&#19979;&#20445;&#25345;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.08753</link><description>&lt;p&gt;
&#38754;&#21521;&#25152;&#26377;&#19979;&#28216;&#20195;&#29702;&#30340;&#25442;&#20301;&#21518;&#24724;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Forecasting for Swap Regret for All Downstream Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20445;&#35777;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#30340;&#21516;&#26102;&#26174;&#33879;&#25552;&#39640;&#36895;&#24230;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#36890;&#36807;&#36827;&#34892;&#38750;&#26657;&#20934;&#39044;&#27979;&#65292;&#20294;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#20107;&#20214;&#38598;&#21512;&#19979;&#20445;&#25345;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#33021;&#22815;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#36827;&#34892;&#39044;&#27979;&#65292;&#20197;&#30830;&#20445;&#26368;&#20339;&#23545;&#31574;&#30340;&#19979;&#28216;&#20195;&#29702;&#22312;&#20219;&#20309;&#25928;&#29992;&#20989;&#25968;&#19979;&#37117;&#33021;&#20445;&#35777;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#12290;&#33258;&#20174;Foster&#21644;Vohra&#65288;1997&#65289;&#20197;&#26469;&#65292;&#24050;&#32463;&#30693;&#36947;&#26368;&#20339;&#23545;&#31574;&#20110;&#26657;&#20934;&#30340;&#39044;&#27979;&#27809;&#26377;&#25442;&#20301;&#21518;&#24724;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24050;&#30693;&#30340;&#22312;&#39034;&#24207;&#23545;&#25239;&#29615;&#22659;&#20013;&#20445;&#35777;&#26657;&#20934;&#39044;&#27979;&#30340;&#31639;&#27861;&#65292;&#20854;&#36895;&#24230;&#22312;&#39044;&#27979;&#31354;&#38388;&#32500;&#24230;&#22686;&#21152;&#26102;&#21576;&#25351;&#25968;&#32423;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36827;&#34892;&#38750;&#26657;&#20934;&#39044;&#27979;&#65292;&#20294;&#22312;&#31934;&#24515;&#36873;&#25321;&#30340;&#20107;&#20214;&#38598;&#21512;&#19979;&#20445;&#25345;&#26080;&#20559;&#30340;&#39044;&#27979;&#65292;&#25105;&#20204;&#33021;&#22815;&#20445;&#35777;&#20219;&#24847;&#19979;&#28216;&#20195;&#29702;&#32553;&#20943;&#25442;&#20301;&#21518;&#24724;&#30340;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#21516;&#26102;&#20445;&#25345;&#25105;&#20204;&#30340;&#39044;&#27979;&#20026;&#20219;&#20309;&#19979;&#28216;&#20195;&#29702;&#25552;&#20379;&#20445;&#35777;&#30340;&#21560;&#24341;&#21147;&#29305;&#24615;&#65292;&#32780;&#26080;&#38656;&#25105;&#20204;&#30340;&#39044;&#27979;&#31639;&#27861;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08753v1 Announce Type: cross Abstract: We study the problem of making predictions so that downstream agents who best respond to them will be guaranteed diminishing swap regret, no matter what their utility functions are. It has been known since Foster and Vohra (1997) that agents who best-respond to calibrated forecasts have no swap regret. Unfortunately, the best known algorithms for guaranteeing calibrated forecasts in sequential adversarial environments do so at rates that degrade exponentially with the dimension of the prediction space. In this work, we show that by making predictions that are not calibrated, but are unbiased subject to a carefully selected collection of events, we can guarantee arbitrary downstream agents diminishing swap regret at rates that substantially improve over the rates that result from calibrated forecasts -- while maintaining the appealing property that our forecasts give guarantees for any downstream agent, without our forecasting algorithm 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07025</link><description>&lt;p&gt;
&#22343;&#22330;&#26497;&#38480;&#19979;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Generalization Error of Graph Neural Networks in the Mean-field Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#21363;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65306;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#27867;&#21270;&#35823;&#24046;&#30340;&#29616;&#26377;&#30028;&#38480;&#32570;&#20047;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#26159;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#19978;&#30028;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20197;$O(1/n)$&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$n$&#26159;&#22270;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#36825;&#20123;&#19978;&#30028;&#20026;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#20174;&#32780;&#23545;&#25105;&#20204;&#30340;&#29702;&#35299;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#36973;&#36935;&#28151;&#28102;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24573;&#30053;&#28151;&#28102;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06434</link><description>&lt;p&gt;
&#30495;&#30456;&#22312;&#21738;&#37324;&#65311;&#22312;&#36830;&#32493;&#30340;&#19990;&#30028;&#20013;&#36973;&#36935;&#28151;&#28102;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Where is the Truth? The Risk of Getting Confounded in a Continual World
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06434
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#36973;&#36935;&#28151;&#28102;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20256;&#32479;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24573;&#30053;&#28151;&#28102;&#65292;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#19968;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#19968;&#20010;&#34394;&#20551;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#65292;&#32780;&#36825;&#31181;&#30456;&#20851;&#24615;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#25968;&#25454;&#65292;&#35813;&#25968;&#25454;&#38598;&#23601;&#26159;&#28151;&#28102;&#30340;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;&#65292;&#22312;&#19968;&#20010;&#36830;&#32493;&#23398;&#20064;&#30340;&#29615;&#22659;&#20013;&#65292;&#28151;&#28102;&#22240;&#32032;&#21487;&#33021;&#38543;&#30528;&#20219;&#21153;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#23548;&#33268;&#30340;&#25361;&#25112;&#36828;&#36828;&#36229;&#36807;&#36890;&#24120;&#32771;&#34385;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#19978;&#25512;&#23548;&#20102;&#36825;&#31181;&#28151;&#28102;&#22240;&#32032;&#23545;&#19968;&#32452;&#28151;&#28102;&#20219;&#21153;&#30340;&#26377;&#25928;&#32852;&#21512;&#35299;&#31354;&#38388;&#30340;&#24433;&#21709;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#39044;&#27979;&#65292;&#22312;&#35768;&#22810;&#36825;&#26679;&#30340;&#36830;&#32493;&#25968;&#25454;&#38598;&#20013;&#65292;&#24403;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#26102;&#65292;&#34394;&#20551;&#30456;&#20851;&#24615;&#24456;&#23481;&#26131;&#34987;&#24573;&#30053;&#65292;&#20294;&#26159;&#22312;&#39034;&#24207;&#32771;&#34385;&#20219;&#21153;&#26102;&#65292;&#36991;&#20813;&#28151;&#28102;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#36825;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#26631;&#20934;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26080;&#27861;&#24573;&#30053;&#28151;&#28102;&#65292;&#32780;&#21516;&#26102;&#23545;&#25152;&#26377;&#20219;&#21153;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#21017;&#26159;&#25104;&#21151;&#30340;&#12290;&#25105;&#20204;&#30340;&#36830;&#32493;&#28151;&#28102;&#25968;&#25454;&#38598;ConCon&#22522;&#20110;CLEVR&#22270;&#20687;&#65292;&#35777;&#26126;&#20102;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#22788;&#29702;&#28151;&#28102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A dataset is confounded if it is most easily solved via a spurious correlation which fails to generalize to new data. We will show that, in a continual learning setting where confounders may vary in time across tasks, the resulting challenge far exceeds the standard forgetting problem normally considered. In particular, we derive mathematically the effect of such confounders on the space of valid joint solutions to sets of confounded tasks. Interestingly, our theory predicts that for many such continual datasets, spurious correlations are easily ignored when the tasks are trained on jointly, but it is far harder to avoid confounding when they are considered sequentially. We construct such a dataset and demonstrate empirically that standard continual learning methods fail to ignore confounders, while training jointly on all tasks is successful. Our continually confounded dataset, ConCon, is based on CLEVR images and demonstrates the need for continual learning methods with more robust b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;&#65288;PDDS&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#36845;&#20195;&#31890;&#23376;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#23545;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#37319;&#26679;&#21644;&#35745;&#31639;&#35268;&#33539;&#21270;&#24120;&#25968;&#12290;&#19982;&#26631;&#20934;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;PDDS &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#28176;&#36817;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.06320</link><description>&lt;p&gt;
&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Particle Denoising Diffusion Sampler
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120;&#65288;PDDS&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#36845;&#20195;&#31890;&#23376;&#26041;&#26696;&#21644;&#26032;&#39062;&#30340;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#65292;&#23545;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#36827;&#34892;&#37319;&#26679;&#21644;&#35745;&#31639;&#35268;&#33539;&#21270;&#24120;&#25968;&#12290;&#19982;&#26631;&#20934;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;PDDS &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#28176;&#36817;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#25193;&#25955;&#23558;&#25968;&#25454;&#20998;&#24067;&#36716;&#21270;&#20026;&#39640;&#26031;&#20998;&#24067;&#12290;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#24471;&#20998;&#21305;&#37197;&#24605;&#24819;&#20272;&#35745;&#36825;&#31181;&#25193;&#25955;&#30340;&#26102;&#38388;&#21453;&#28436;&#26469;&#33719;&#24471;&#26469;&#33258;&#25968;&#25454;&#20998;&#24067;&#30340;&#36817;&#20284;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#37319;&#29992;&#31867;&#20284;&#30340;&#31574;&#30053;&#26469;&#20174;&#38750;&#24402;&#19968;&#21270;&#27010;&#29575;&#23494;&#24230;&#20013;&#37319;&#26679;&#24182;&#35745;&#31639;&#23427;&#20204;&#30340;&#35268;&#33539;&#21270;&#24120;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#37324;&#65292;&#26102;&#38388;&#21453;&#28436;&#25193;&#25955;&#26159;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26032;&#39062;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#21407;&#22987;&#36845;&#20195;&#31890;&#23376;&#26041;&#26696;&#26469;&#27169;&#25311;&#30340;&#12290;&#19982;&#26631;&#20934;&#30340;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#32467;&#26524;&#30340;&#31890;&#23376;&#21435;&#22122;&#25193;&#25955;&#37319;&#26679;&#22120; (PDDS) &#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#28176;&#36817;&#19968;&#33268;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#21644;&#39640;&#32500;&#37319;&#26679;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102; PDDS&#12290;
&lt;/p&gt;
&lt;p&gt;
Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;Transformer&#26102;&#65292;&#21024;&#38500;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#21518;&#65292;&#36755;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#27531;&#24046;&#36830;&#25509;&#23545;Transformer&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2402.05969</link><description>&lt;p&gt;
&#25171;&#30772;&#35757;&#32451;Transformer&#26102;&#30340;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Breaking Symmetry When Training Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05969
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#35757;&#32451;Transformer&#26102;&#65292;&#21024;&#38500;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#21518;&#65292;&#36755;&#20986;&#30340;&#39044;&#27979;&#32467;&#26524;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#36827;&#34892;&#32454;&#33268;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#27531;&#24046;&#36830;&#25509;&#23545;Transformer&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#22914;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#21644;&#22240;&#26524;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#26550;&#26500;&#23545;&#20110;&#36755;&#20837;&#31526;&#21495;1, 2, ..., n-1&#30340;&#25490;&#21015;&#26159;&#19981;&#21464;&#30340;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#20004;&#31181;&#26426;&#21046;&#37117;&#20250;&#34987;&#20351;&#29992;&#65292;&#20197;&#25171;&#30772;&#23545;&#36755;&#20837;&#31526;&#21495;&#30340;&#23545;&#31216;&#24615;&#12290;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;Transformer&#12290;&#36825;&#24517;&#39035;&#36890;&#36807;&#22240;&#26524;&#27880;&#24847;&#26426;&#21046;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#22240;&#26524;&#36830;&#25509;&#26426;&#21046;&#24517;&#39035;&#26159;&#20351;Transformer&#33021;&#22815;&#27169;&#25311;&#36755;&#20837;&#39034;&#24207;&#37325;&#35201;&#24615;&#30340;&#21407;&#22240;&#12290;Transformer&#30340;&#22402;&#30452;&#8220;&#20999;&#29255;&#8221;&#37117;&#34987;&#40723;&#21169;&#34920;&#31034;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#30456;&#21516;&#20301;&#32622;k&#12290;&#25105;&#20204;&#20551;&#35774;&#27531;&#24046;&#36830;&#25509;&#23545;&#20110;&#36825;&#31181;&#29616;&#35937;&#36215;&#21040;&#20102;&#36129;&#29486;&#65292;&#24182;&#25552;&#20379;&#20102;&#35777;&#25454;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
As we show in this paper, the prediction for output token $n+1$ of Transformer architectures without one of the mechanisms of positional encodings and causal attention is invariant to permutations of input tokens $1, 2, ..., n-1$. Usually, both mechanisms are employed and the symmetry with respect to the input tokens is broken. Recently, it has been shown that one can train Transformers without positional encodings. This must be enabled by the causal attention mechanism. In this paper, we elaborate on the argument that the causal connection mechanism must be responsible for the fact that Transformers are able to model input sequences where the order is important. Vertical "slices" of Transformers are all encouraged to represent the same location $k$ in the input sequence. We hypothesize that residual connections contribute to this phenomenon, and demonstrate evidence for this.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.05929</link><description>&lt;p&gt;
&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Interactive Agent Foundation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05929
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#26032;&#39062;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#33021;&#22815;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#29616;&#20986;&#36890;&#29992;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#19988;&#22312;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21457;&#23637;&#27491;&#22312;&#20174;&#21019;&#24314;&#38745;&#24577;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#36716;&#21464;&#20026;&#33021;&#22815;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#21160;&#24577;&#26234;&#33021;&#20307;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26234;&#33021;&#20307;&#22522;&#30784;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#35757;&#32451;&#36328;&#39046;&#22495;&#12289;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#30340; AI &#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#33539;&#24335;&#32479;&#19968;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21253;&#25324;&#35270;&#35273;&#36974;&#25377;&#33258;&#32534;&#30721;&#22120;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#19979;&#19968;&#27493;&#34892;&#21160;&#39044;&#27979;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#36866;&#24212;&#24615;&#24378;&#30340; AI &#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#29420;&#31435;&#39046;&#22495; - &#26426;&#22120;&#20154;&#12289;&#28216;&#25103; AI &#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#39046;&#22495;&#37117;&#23637;&#31034;&#20102;&#29983;&#25104;&#26377;&#24847;&#20041;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#20854;&#24191;&#27867;&#24615;&#65292;&#21033;&#29992;&#20102;&#21508;&#31181;&#25968;&#25454;&#28304;&#65292;&#22914;&#26426;&#22120;&#20154;&#24207;&#21015;&#12289;&#28216;&#25103;&#25968;&#25454;&#12289;&#22823;&#35268;&#27169;&#35270;&#39057;&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effectiv
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05926</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#38646;&#38454;&#32852;&#37030;&#35843;&#25972;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Zeroth-Order Federated Tuning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05926
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedMeZO&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#38646;&#38454;&#32852;&#37030;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#23454;&#29616;&#20869;&#23384;&#39640;&#25928;&#30340;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;FedMeZO&#19981;&#20165;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#20013;&#20855;&#26377;&#20851;&#38190;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#34701;&#21512;&#20026;&#38544;&#31169;&#20445;&#25252;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24102;&#26469;&#20102;&#26032;&#26102;&#20195;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;LLM&#25152;&#38656;&#30340;&#24378;&#22823;&#20869;&#23384;&#35201;&#27714;&#22312;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#26102;&#20250;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35774;&#22791;&#30340;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#32852;&#37030;&#29615;&#22659;&#20013;&#25506;&#32034;&#20102;&#20869;&#23384;&#39640;&#25928;&#30340;&#38646;&#38454;&#20248;&#21270;&#30340;&#20840;&#26032;&#25972;&#21512;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;FedMeZO&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#31532;&#19968;&#20010;&#22312;LLM&#32972;&#26223;&#19979;&#32771;&#23519;FedMeZO&#30340;&#29702;&#35770;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#28041;&#21450;&#21040;&#22823;&#21442;&#25968;&#31354;&#38388;&#23545;&#20248;&#21270;&#34892;&#20026;&#30340;&#24433;&#21709;&#12289;&#25910;&#25947;&#24615;&#30340;&#24314;&#31435;&#20197;&#21450;&#20026;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#31574;&#30053;&#30830;&#23450;&#20851;&#38190;&#21442;&#25968;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#35777;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#20010;&#29702;&#35770;&#65292;&#34920;&#26126;FedMeZO&#19981;&#20165;&#27604;&#20256;&#32479;&#30340;&#19968;&#38454;&#26041;&#27861;&#65288;&#22914;SGD&#65289;&#25910;&#25947;&#26356;&#24555;&#65292;&#32780;&#19988;&#26126;&#26174;...
&lt;/p&gt;
&lt;p&gt;
The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also signific
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#35757;&#32451;&#25439;&#22833;&#30340;&#39640;&#26041;&#24046;&#65292;&#20174;&#32780;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.05453</link><description>&lt;p&gt;
&#36890;&#36807;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#35757;&#32451;&#25439;&#22833;&#30340;&#39640;&#26041;&#24046;&#65292;&#20174;&#32780;&#38477;&#20302;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#20250;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#21363;&#25512;&#26029;&#26679;&#26412;&#26159;&#21542;&#22312;&#35757;&#32451;&#38598;&#20013;&#12290;&#29616;&#26377;&#24037;&#20316;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#26469;&#22686;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#25439;&#22833;&#26041;&#24046;&#65292;&#32531;&#35299;&#38544;&#31169;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#21521;&#30456;&#21453;&#26041;&#21521;&#20248;&#21270;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#21442;&#25968;&#22312;&#23616;&#37096;&#26368;&#23567;&#20540;&#38468;&#36817;&#25391;&#33633;&#65292;&#23548;&#33268;&#19981;&#31283;&#23450;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20984;&#20985;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#26799;&#24230;&#19979;&#38477;&#30340;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#35757;&#32451;&#25439;&#22833;&#20998;&#24067;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#29702;&#35770;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#20984;&#25439;&#22833;&#20989;&#25968;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20542;&#21521;&#20110;&#20943;&#23569;&#25439;&#22833;&#26041;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;CCL&#30340;&#32972;&#21518;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#20985;&#20989;&#25968;&#39033;&#20943;&#23567;&#25439;&#22833;&#20989;&#25968;&#30340;&#20984;&#24615;&#12290;&#20351;&#29992;CCL&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#26041;&#24046;&#25439;&#22833;&#65292;&#21152;&#24378;&#20102;&#23545;MIAs&#30340;&#38450;&#24481;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;CCL&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03741</link><description>&lt;p&gt;
SUB-PLAY&#65306;&#38024;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#23545;&#25239;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SUB-PLAY: Adversarial Policies against Partially Observed Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03741
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;&#25915;&#20987;&#32773;&#22312;&#22810;&#26234;&#33021;&#20307;&#31454;&#20105;&#29615;&#22659;&#20013;&#21363;&#20351;&#21463;&#38480;&#20110;&#21463;&#23475;&#32773;&#30340;&#37096;&#20998;&#35266;&#27979;&#20063;&#33021;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#20026;&#26080;&#20154;&#26426;&#30340;&#32676;&#20307;&#25511;&#21046;&#12289;&#26426;&#26800;&#33218;&#30340;&#21327;&#20316;&#25805;&#32437;&#20197;&#21450;&#22810;&#30446;&#26631;&#21253;&#22260;&#31561;&#24320;&#36767;&#20102;&#24191;&#38420;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#22312;MARL&#37096;&#32626;&#36807;&#31243;&#20013;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#23041;&#32961;&#38656;&#35201;&#26356;&#22810;&#20851;&#27880;&#21644;&#28145;&#20837;&#35843;&#26597;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36805;&#36895;&#21033;&#29992;&#21463;&#23475;&#32773;&#30340;&#28431;&#27934;&#29983;&#25104;&#23545;&#25239;&#31574;&#30053;&#65292;&#23548;&#33268;&#21463;&#23475;&#32773;&#22312;&#29305;&#23450;&#20219;&#21153;&#20013;&#22833;&#36133;&#12290;&#20363;&#22914;&#65292;&#23558;&#36229;&#20154;&#32423;&#21035;&#30340;&#22260;&#26827;AI&#30340;&#33719;&#32988;&#29575;&#38477;&#20302;&#21040;&#32422;20%&#12290;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20004;&#20154;&#31454;&#20105;&#29615;&#22659;&#65292;&#24182;&#20551;&#35774;&#25915;&#20987;&#32773;&#20855;&#26377;&#23436;&#25972;&#30340;&#20840;&#23616;&#29366;&#24577;&#35266;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in multi-agent reinforcement learning (MARL) have opened up vast application prospects, including swarm control of drones, collaborative manipulation by robotic arms, and multi-target encirclement. However, potential security threats during the MARL deployment need more attention and thorough investigation. Recent researches reveal that an attacker can rapidly exploit the victim's vulnerabilities and generate adversarial policies, leading to the victim's failure in specific tasks. For example, reducing the winning rate of a superhuman-level Go AI to around 20%. They predominantly focus on two-player competitive environments, assuming attackers possess complete global state observation.   In this study, we unveil, for the first time, the capability of attackers to generate adversarial policies even when restricted to partial observations of the victims in multi-agent competitive environments. Specifically, we propose a novel black-box attack (SUB-PLAY), which incorporate
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03496</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#21435;&#25481;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21527;&#65311;&#19968;&#20010;&#20108;&#38454;&#35282;&#24230;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03496
&lt;/p&gt;
&lt;p&gt;
&#31227;&#38500;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30340;&#24179;&#26041;&#26681;&#21487;&#20197;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#20943;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;transformers&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26799;&#24230;&#20248;&#21270;&#22120;&#22914;Adam(W)&#26159;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#32467;&#26500;&#65288;&#22914;transformers&#65289;&#30340;&#40664;&#35748;&#35757;&#32451;&#31639;&#27861;&#12290;&#23427;&#20204;&#30340;&#23545;&#35282;&#20808;&#39564;&#22522;&#20110;&#26799;&#24230;&#22806;&#31215;&#65292;&#36890;&#36807;&#24179;&#26041;&#26681;&#21152;&#20837;&#21040;&#21442;&#25968;&#26356;&#26032;&#20013;&#12290;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#34987;&#31216;&#20026;&#36817;&#20284;&#30340;&#20108;&#38454;&#26041;&#27861;&#65292;&#20294;&#24179;&#26041;&#26681;&#34920;&#31034;&#20102;&#19968;&#20010;&#26681;&#26412;&#24615;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21435;&#25481;&#24179;&#26041;&#26681;&#21518;&#33258;&#36866;&#24212;&#26041;&#27861;&#30340;&#34892;&#20026;&#22914;&#20309;&#21464;&#21270;&#65292;&#21363;&#21152;&#24378;&#23427;&#20204;&#30340;&#20108;&#38454;&#21160;&#26426;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21435;&#25481;&#24179;&#26041;&#26681;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#33021;&#22815;&#22312;&#21367;&#31215;&#32467;&#26500;&#19978;&#32553;&#23567;&#19982;SGD&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22312;transformers&#19978;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20108;&#38454;&#35282;&#24230;&#23545;&#20110;&#24320;&#21457;&#20855;&#26377;&#38750;&#23545;&#35282;&#20808;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#20063;&#20855;&#26377;&#23454;&#38469;&#22909;&#22788;&#12290;&#19982;&#20687;Shampoo&#36825;&#26679;&#22522;&#20110;&#24179;&#26041;&#26681;&#30340;&#23545;&#24212;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#20204;&#19981;&#38656;&#35201;&#25968;&#20540;&#19981;&#31283;&#23450;&#30340;&#30697;&#38453;&#24179;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.03485</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19982;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#30456;&#36935;&#65306;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attention Meets Post-hoc Interpretability: A Mathematical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22522;&#20110;transformer&#31561;&#26550;&#26500;&#65292;&#25104;&#20026;&#20102;&#25216;&#26415;&#38761;&#21629;&#30340;&#26680;&#24515;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38500;&#20102;&#24110;&#21161;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20043;&#22806;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#26412;&#36523;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34892;&#20026;&#30340;&#26377;&#24847;&#20041;&#27934;&#23519;&#12290;&#36825;&#20123;&#27934;&#23519;&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#35299;&#37322;&#65311;&#20851;&#20110;&#27492;&#20105;&#35770;&#19981;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#24182;&#20934;&#30830;&#23450;&#20301;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#23427;&#20204;&#25552;&#20379;&#20102;&#30456;&#24403;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#23613;&#31649;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.
&lt;/p&gt;</description></item><item><title>$\textsf{DynaBRO}$&#26159;&#19968;&#31181;&#21160;&#24577;&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#20999;&#25442;&#25308;&#21344;&#24237;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#19978;&#19982;&#38745;&#24577;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#28176;&#21464;&#20272;&#35745;&#25216;&#26415;&#12289;&#24378;&#40065;&#26834;&#24037;&#20316;&#26426;&#21046;&#26356;&#26032;&#30340;&#32858;&#21512;&#21644;&#25925;&#38556;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32463;&#21463;&#20303;$\mathcal{O}(\sqrt{T})$&#36718;&#25308;&#21344;&#24237;&#36523;&#20221;&#30340;&#25913;&#21464;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#30334;&#20998;&#27604;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.02951</link><description>&lt;p&gt;
&#21160;&#24577;&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#65306;&#36866;&#24212;&#20999;&#25442;&#25308;&#21344;&#24237;&#24037;&#20316;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Dynamic Byzantine-Robust Learning: Adapting to Switching Byzantine Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02951
&lt;/p&gt;
&lt;p&gt;
$\textsf{DynaBRO}$&#26159;&#19968;&#31181;&#21160;&#24577;&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36866;&#24212;&#20999;&#25442;&#25308;&#21344;&#24237;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#19988;&#22312;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#19978;&#19982;&#38745;&#24577;&#24773;&#20917;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#28176;&#21464;&#20272;&#35745;&#25216;&#26415;&#12289;&#24378;&#40065;&#26834;&#24037;&#20316;&#26426;&#21046;&#26356;&#26032;&#30340;&#32858;&#21512;&#21644;&#25925;&#38556;&#23433;&#20840;&#36807;&#28388;&#22120;&#30340;&#24341;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32463;&#21463;&#20303;$\mathcal{O}(\sqrt{T})$&#36718;&#25308;&#21344;&#24237;&#36523;&#20221;&#30340;&#25913;&#21464;&#12290;&#21478;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#30334;&#20998;&#27604;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25308;&#21344;&#24237;-&#24378;&#40065;&#26834;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#23481;&#38169;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#24050;&#32463;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25216;&#26415;&#32771;&#34385;&#30340;&#26159;&#38745;&#24577;&#24773;&#20917;&#65292;&#20854;&#20013;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25308;&#21344;&#24237;&#26426;&#22120;&#30340;&#36523;&#20221;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#20551;&#35774;&#19981;&#33021;&#25429;&#25417;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#24577;&#25308;&#21344;&#24237;&#34892;&#20026;&#65292;&#21487;&#33021;&#21253;&#25324;&#30701;&#26242;&#25925;&#38556;&#25110;&#26377;&#38024;&#23545;&#24615;&#30340;&#26102;&#38388;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;$\textsf{DynaBRO}$&#65292;&#23427;&#33021;&#22815;&#32463;&#21463;&#20303;$\mathcal{O}(\sqrt{T})$&#36718;&#25308;&#21344;&#24237;&#36523;&#20221;&#30340;&#25913;&#21464;&#65288;&#20854;&#20013;$T$&#26159;&#24635;&#35757;&#32451;&#36718;&#25968;&#65289;&#65292;&#21516;&#26102;&#19982;&#38745;&#24577;&#24773;&#20917;&#19979;&#30340;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#32423;&#33945;&#29305;&#21345;&#27931;&#65288;MLMC&#65289;&#28176;&#21464;&#20272;&#35745;&#25216;&#26415;&#19982;&#24037;&#20316;&#26426;&#21046;&#26356;&#26032;&#30340;&#24378;&#40065;&#26834;&#32858;&#21512;&#30456;&#32467;&#21512;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#25925;&#38556;&#23433;&#20840;&#36807;&#28388;&#22120;&#26469;&#38480;&#21046;&#21160;&#24577;&#25308;&#21344;&#24237;&#31574;&#30053;&#30340;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#30334;&#20998;&#27604;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Byzantine-robust learning has emerged as a prominent fault-tolerant distributed machine learning framework. However, most techniques consider the static setting, wherein the identity of Byzantine machines remains fixed during the learning process. This assumption does not capture real-world dynamic Byzantine behaviors, which may include transient malfunctions or targeted temporal attacks. Addressing this limitation, we propose $\textsf{DynaBRO}$ -- a new method capable of withstanding $\mathcal{O}(\sqrt{T})$ rounds of Byzantine identity alterations (where $T$ is the total number of training rounds), while matching the asymptotic convergence rate of the static setting. Our method combines a multi-level Monte Carlo (MLMC) gradient estimation technique with robust aggregation of worker updates and incorporates a fail-safe filter to limit bias from dynamic Byzantine strategies. Additionally, by leveraging an adaptive learning rate, our approach eliminates the need for knowing the percentag
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#24120;&#21463;&#38480;&#20110;&#22522;&#30784;&#20998;&#24067;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;CDiffuser&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02772</link><description>&lt;p&gt;
&#23545;&#27604;&#25193;&#25955;&#22120;&#65306;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35268;&#21010;&#39640;&#22238;&#25253;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Contrastive Diffuser: Planning Towards High Return States via Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02772
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#24120;&#21463;&#38480;&#20110;&#22522;&#30784;&#20998;&#24067;&#21644;&#26679;&#26412;&#22810;&#26679;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#30340;CDiffuser&#26041;&#27861;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25552;&#39640;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#38271;&#26399;&#35268;&#21010;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20960;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#25193;&#25955;&#30340;&#24314;&#27169;&#33021;&#21147;&#36827;&#34892;&#20219;&#24847;&#20998;&#24067;&#30340;&#35268;&#21010;&#12290;&#36825;&#20123;&#26041;&#27861;&#20026;&#35268;&#21010;&#29983;&#25104;&#20102;&#21518;&#32493;&#36712;&#36857;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#22522;&#30784;&#20998;&#24067;&#30340;&#38480;&#21046;&#65292;&#24182;&#24573;&#35270;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#19981;&#21516;&#29366;&#24577;&#20855;&#26377;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#23427;&#20204;&#20165;&#20165;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#65292;&#24182;&#29983;&#25104;&#19982;&#31163;&#32447;&#25968;&#25454;&#38598;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#36712;&#36857;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21040;&#36798;&#39640;&#22238;&#25253;&#29366;&#24577;&#30340;&#27010;&#29575;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#12290;&#21363;&#20351;&#37197;&#22791;&#20102;&#24341;&#23548;&#27169;&#22411;&#65292;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#21387;&#21046;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CDiffuser&#30340;&#26032;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36820;&#22238;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Applying diffusion models in reinforcement learning for long-term planning has gained much attention recently. Several diffusion-based methods have successfully leveraged the modeling capabilities of diffusion for arbitrary distributions. These methods generate subsequent trajectories for planning and have demonstrated significant improvement. However, these methods are limited by their plain base distributions and their overlooking of the diversity of samples, in which different states have different returns. They simply leverage diffusion to learn the distribution of offline dataset, generate the trajectories whose states share the same distribution with the offline dataset. As a result, the probability of these models reaching the high-return states is largely dependent on the dataset distribution. Even equipped with the guidance model, the performance is still suppressed. To address these limitations, in this paper, we propose a novel method called CDiffuser, which devises a return
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;Mobius&#21464;&#25442;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#38646;&#31995;&#25968;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#24674;&#22797;Mobius&#21464;&#25442;&#65292;&#24182;&#25581;&#31034;&#20102;&#32676;&#20307;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.02631</link><description>&lt;p&gt;
&#23398;&#20064;&#29702;&#35299;&#65306;&#36890;&#36807;Mobius&#21464;&#25442;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Understand: Identifying Interactions via the Mobius Transform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;Mobius&#21464;&#25442;&#35782;&#21035;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#21487;&#20197;&#22312;&#38750;&#38646;&#31995;&#25968;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#31934;&#30830;&#24674;&#22797;Mobius&#21464;&#25442;&#65292;&#24182;&#25581;&#31034;&#20102;&#32676;&#20307;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#25214;&#21040;&#25105;&#20204;&#23398;&#20064;&#30340;&#20989;&#25968;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#12290;Mobius&#21464;&#25442;&#26159;&#19968;&#20010;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#30340;&#31995;&#25968;&#23545;&#24212;&#20110;&#36755;&#20837;&#21464;&#37327;&#38598;&#21512;&#19978;&#30340;&#21807;&#19968;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;Mobius&#21464;&#25442;&#19982;Shapley&#20540;&#30340;&#27010;&#24565;&#23494;&#20999;&#30456;&#20851;&#65288;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#31561;&#20215;&#30340;&#65289;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21338;&#24328;&#35770;&#37325;&#35201;&#24615;&#27010;&#24565;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#22312;$n$&#20010;&#36755;&#20837;&#20043;&#38388;&#30340;&#25152;&#26377;$2^n$&#20010;&#21487;&#33021;&#20132;&#20114;&#20043;&#20013;&#65292;&#38750;&#38646;Mobius&#31995;&#25968;&#65288;&#21644;&#22240;&#27492;&#36755;&#20837;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65289;&#30340;&#27604;&#20363;&#23567;&#20110;&#38750;&#38646;&#31995;&#25968;&#24635;&#25968;&#30340;&#65288;&#20856;&#22411;&#65289;&#24773;&#20917;&#12290;&#24403;&#26377;$K = O(2^{n \delta})$&#20010;&#65292;&#20854;&#20013;$\delta \leq \frac{1}{3}$&#30340;&#38750;&#38646;&#31995;&#25968;&#20197;&#22343;&#21248;&#38543;&#26426;&#26041;&#24335;&#36873;&#25321;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;$O(Kn)$&#20010;&#26679;&#26412;&#21644;$O(Kn^2)$&#30340;&#26102;&#38388;&#20869;&#23436;&#20840;&#24674;&#22797;Mobius&#21464;&#25442;&#65292;&#24182;&#19988;&#38543;&#30528;$K \rightarrow \infty$&#65292;&#35823;&#24046;&#36235;&#20110;&#38646;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#32676;&#20307;&#29702;&#35770;&#21644;&#20449;&#24687;&#35770;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most fundamental problems in machine learning is finding interpretable representations of the functions we learn. The Mobius transform is a useful tool for this because its coefficients correspond to unique importance scores on sets of input variables. The Mobius Transform is strongly related (and in some cases equivalent) to the concept of Shapley value, which is a widely used game-theoretic notion of importance. This work focuses on the (typical) regime where the fraction of non-zero Mobius coefficients (and thus interactions between inputs) is small compared to the set of all $2^n$ possible interactions between $n$ inputs. When there are $K = O(2^{n \delta})$ with $\delta \leq \frac{1}{3}$ non-zero coefficients chosen uniformly at random, our algorithm exactly recovers the Mobius transform in $O(Kn)$ samples and $O(Kn^2)$ time with vanishing error as $K \rightarrow \infty$, the first non-adaptive algorithm to do so. We also uncover a surprising connection between group te
&lt;/p&gt;</description></item><item><title>Minusformer&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#23558;Transformer&#27169;&#22411;&#20013;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#25913;&#20026;&#20943;&#27861;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02332</link><description>&lt;p&gt;
Minusformer: &#36890;&#36807;&#28176;&#36827;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Minusformer: Improving Time Series Forecasting by Progressively Learning Residuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02332
&lt;/p&gt;
&lt;p&gt;
Minusformer&#36890;&#36807;&#36880;&#27493;&#23398;&#20064;&#27531;&#24046;&#26469;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#23427;&#23558;Transformer&#27169;&#22411;&#20013;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#25913;&#20026;&#20943;&#27861;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#26222;&#36941;&#23384;&#22312;&#30340;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#39044;&#27979;&#27169;&#22411;&#23481;&#26131;&#20005;&#37325;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#21435;&#20887;&#20313;&#30340;&#26041;&#27861;&#36880;&#27493;&#24674;&#22797;TS&#30340;&#20869;&#22312;&#20215;&#20540;&#20197;&#29992;&#20110;&#26410;&#26469;&#30340;&#26102;&#38388;&#27573;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20449;&#24687;&#32858;&#21512;&#26426;&#21046;&#20174;&#21152;&#27861;&#36716;&#21464;&#20026;&#20943;&#27861;&#26469;&#25913;&#36827;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#21407;&#27169;&#22411;&#30340;&#27599;&#20010;&#27169;&#22359;&#20013;&#21152;&#20837;&#19968;&#20010;&#36741;&#21161;&#36755;&#20986;&#20998;&#25903;&#65292;&#26500;&#24314;&#19968;&#26465;&#36890;&#24448;&#26368;&#32456;&#39044;&#27979;&#30340;&#39640;&#36895;&#20844;&#36335;&#12290;&#35813;&#20998;&#25903;&#20013;&#21518;&#32493;&#27169;&#22359;&#30340;&#36755;&#20986;&#23558;&#20943;&#21435;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#32467;&#26524;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36880;&#23618;&#23398;&#20064;&#30417;&#30563;&#20449;&#21495;&#30340;&#27531;&#24046;&#12290;&#36825;&#31181;&#35774;&#35745;&#20419;&#36827;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#27969;&#30340;&#36880;&#27493;&#23398;&#20064;&#39537;&#21160;&#38544;&#24335;&#20998;&#35299;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#36807;&#25311;&#21512;&#30340;&#38887;&#24615;&#12290;&#30001;&#20110;&#27169;&#22411;&#20013;&#30340;&#25152;&#26377;&#32858;&#21512;&#37117;&#26159;&#20943;&#21495;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;Minusformer&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we find that ubiquitous time series (TS) forecasting models are prone to severe overfitting. To cope with this problem, we embrace a de-redundancy approach to progressively reinstate the intrinsic values of TS for future intervals. Specifically, we renovate the vanilla Transformer by reorienting the information aggregation mechanism from addition to subtraction. Then, we incorporate an auxiliary output branch into each block of the original model to construct a highway leading to the ultimate prediction. The output of subsequent modules in this branch will subtract the previously learned results, enabling the model to learn the residuals of the supervision signal, layer by layer. This designing facilitates the learning-driven implicit progressive decomposition of the input and output streams, empowering the model with heightened versatility, interpretability, and resilience against overfitting. Since all aggregations in the model are minus signs, which is called Minusfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.01663</link><description>&lt;p&gt;
&#26432;&#25163;&#32423;&#24212;&#29992;&#65306;&#20302;&#36895;&#22823;&#35268;&#27169;AI&#27494;&#22120;
&lt;/p&gt;
&lt;p&gt;
Killer Apps: Low-Speed, Large-Scale AI Weapons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#65292;&#24378;&#35843;&#20102;&#22312;&#20449;&#24687;&#39046;&#22495;&#20869;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#30340;&#28508;&#21147;&#65292;&#20197;&#21450;&#20854;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#29305;&#21035;&#26159;&#30001;OpenAI&#12289;Meta&#21644;Anthropic&#31561;&#32452;&#32455;&#24320;&#21457;&#30340;&#23574;&#31471;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#32473;&#25112;&#20105;&#21644;&#23433;&#20840;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;&#30446;&#21069;&#20851;&#27880;&#30340;&#20027;&#35201;&#26159;AI&#22312;&#27494;&#22120;&#31995;&#32479;&#20013;&#30340;&#25972;&#21512;&#20197;&#21450;&#22312;&#21160;&#33021;&#20914;&#31361;&#20013;&#24555;&#36895;&#20915;&#31574;&#20013;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#21516;&#26679;&#37325;&#35201;&#20294;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#22312;&#20449;&#24687;&#39046;&#22495;&#20013;&#22522;&#20110;AI&#30340;&#24515;&#29702;&#25805;&#32437;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#20869;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#21487;&#33021;&#23545;&#20840;&#29699;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#31038;&#20250;&#36896;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;AI&#27494;&#22120;&#30340;&#27010;&#24565;&#12289;&#37096;&#32626;&#12289;&#26816;&#27979;&#21644;&#28508;&#22312;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accelerating advancements in Artificial Intelligence (AI) and Machine Learning (ML), highlighted by the development of cutting-edge Generative Pre-trained Transformer (GPT) models by organizations such as OpenAI, Meta, and Anthropic, present new challenges and opportunities in warfare and security. Much of the current focus is on AI's integration within weapons systems and its role in rapid decision-making in kinetic conflict. However, an equally important but often overlooked aspect is the potential of AI-based psychological manipulation at internet scales within the information domain. These capabilities could pose significant threats to individuals, organizations, and societies globally. This paper explores the concept of AI weapons, their deployment, detection, and potential countermeasures.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00236</link><description>&lt;p&gt;
&#20301;&#32622;&#32534;&#30721;&#26377;&#21161;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;
&lt;/p&gt;
&lt;p&gt;
Positional Encoding Helps Recurrent Neural Networks Handle a Large Vocabulary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#20173;&#28982;&#26377;&#25928;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#22823;&#35789;&#27719;&#37327;&#21644;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#26102;&#12290;&#36825;&#20026;&#20351;&#29992;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#30740;&#31350;&#32467;&#26524;&#20063;&#23545;&#31070;&#32463;&#20803;&#25391;&#33633;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#20301;&#32622;&#32534;&#30721;&#22312;&#21033;&#29992;&#21512;&#25104;&#22522;&#20934;&#27979;&#35797;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#20013;&#30340;&#24433;&#21709;&#12290;&#20301;&#32622;&#32534;&#30721;&#23558;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#28857;&#8220;&#26102;&#38388;&#25139;&#21270;&#8221;&#65292;&#24182;&#34917;&#20805;&#20102;Transformer&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#21147;&#65292;&#21518;&#32773;&#32570;&#20047;&#34920;&#31034;&#25968;&#25454;&#39034;&#24207;&#30340;&#20869;&#22312;&#26426;&#21046;&#12290;&#30456;&#21453;&#65292;RNN&#21487;&#20197;&#33258;&#24049;&#23545;&#25968;&#25454;&#28857;&#36827;&#34892;&#26102;&#38388;&#32534;&#30721;&#65292;&#20351;&#24471;&#23427;&#20204;&#23545;&#20301;&#32622;&#32534;&#30721;&#30340;&#20351;&#29992;&#20284;&#20046;&#26159;&#8220;&#20887;&#20313;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;RNN&#32467;&#21512;&#20351;&#29992;&#65292;&#20301;&#32622;&#32534;&#30721;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#24456;&#39640;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#22788;&#29702;&#20135;&#29983;&#22810;&#26679;&#35266;&#23519;&#32467;&#26524;&#30340;&#22823;&#35789;&#27719;&#37327;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28041;&#21450;&#36755;&#20837;&#39537;&#21160;&#21644;&#33258;&#20027;&#26102;&#38388;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;&#35745;&#31639;/&#27169;&#25311;&#32467;&#26524;&#30340;&#29983;&#29289;&#23398;&#24847;&#20041;&#65292;&#32771;&#34385;&#21040;&#20301;&#32622;&#32534;&#30721;&#30340;&#27491;&#24358;&#23454;&#29616;&#19982;&#31070;&#32463;&#20803;&#25391;&#33633;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study discusses the effects of positional encoding on recurrent neural networks (RNNs) utilizing synthetic benchmarks. Positional encoding "time-stamps" data points in time series and complements the capabilities of Transformer neural networks, which lack an inherent mechanism for representing the data order. By contrast, RNNs can encode the temporal information of data points on their own, rendering their use of positional encoding seemingly "redundant". Nonetheless, empirical investigations reveal the effectiveness of positional encoding even when coupled with RNNs, specifically for handling a large vocabulary that yields diverse observations. These findings pave the way for a new line of research on RNNs, concerning the combination of input-driven and autonomous time representation. Additionally, biological implications of the computational/simulational results are discussed, in the light of the affinity between the sinusoidal implementation of positional encoding and neural os
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.18070</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#26159;&#21542;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30456;&#21516;&#30340;&#35748;&#30693;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.18070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20154;&#31867;&#23398;&#20064;&#32773;&#30340;&#35748;&#30693;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#38454;&#27573;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35748;&#30693;&#27169;&#22411;&#24863;&#20852;&#36259;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#30340;&#65292;&#20102;&#35299;LLMs&#33021;&#22815;&#27169;&#25311;&#21738;&#20123;&#35748;&#30693;&#29305;&#24615;&#20197;&#21450;&#21738;&#20123;&#19981;&#33021;&#27169;&#25311;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#35299;&#20915;&#31639;&#26415;&#38382;&#39064;&#26102;&#19982;&#20799;&#31461;&#24050;&#30693;&#35748;&#30693;&#20559;&#35265;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35843;&#26597;&#23398;&#20064;&#31185;&#23398;&#25991;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#21487;&#20197;&#20998;&#20026;&#19977;&#20010;&#26126;&#30830;&#30340;&#27493;&#39588;&#65306;&#25991;&#26412;&#29702;&#35299;&#12289;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#21644;&#35299;&#20915;&#26041;&#26696;&#25191;&#34892;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#27493;&#39588;&#26500;&#24314;&#20102;&#27979;&#35797;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#22914;&#20309;&#24544;&#23454;&#22320;&#27169;&#25311;&#36825;&#20010;&#36807;&#31243;&#30340;&#21738;&#20123;&#37096;&#20998;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#20026;&#27599;&#20010;&#27979;&#35797;&#29983;&#25104;&#20102;&#19968;&#32452;&#26032;&#30340;&#21333;&#35789;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23545;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#35268;&#21010;&#20004;&#20010;&#35299;&#20915;&#36807;&#31243;&#30340;&#27493;&#39588;&#20013;&#65292;&#19981;&#35770;&#26159;&#21542;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#65292;&#37117;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which cognitive properties are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand which parts of this process can be faithfully modeled by current state-of-the-art LLMs. We generate a novel set of word problems for each of these tests, using a neuro-symbolic method that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#25913;&#21892;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.16327</link><description>&lt;p&gt;
PICL: &#29289;&#29702;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
PICL: Physics Informed Contrastive Learning for Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#25913;&#21892;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26367;&#20195;&#27169;&#22411;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#32780;&#19981;&#26159;&#20989;&#25968;&#26412;&#36523;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#24555;&#36895;&#20934;&#30830;&#22320;&#27714;&#35299;&#22797;&#26434;&#30340;PDE&#12290;&#23613;&#31649;&#22312;&#24191;&#27867;&#30340;&#20195;&#29702;&#24314;&#27169;&#20219;&#21153;&#20013;&#23545;&#31070;&#32463;&#31639;&#23376;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#26159;&#36880;&#20010;&#26041;&#31243;&#35780;&#20272;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#25511;&#21046;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25511;&#21046;&#26041;&#31243;&#31995;&#25968;&#29992;&#20110;&#34913;&#37327;&#31995;&#32479;&#20043;&#38388;&#30340;&#30495;&#23454;&#30456;&#20284;&#24615;&#12290;&#29289;&#29702;&#20449;&#24687;&#31995;&#32479;&#28436;&#21270;&#21644;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#36755;&#20986;&#30340;&#32467;&#21512;&#34987;&#38170;&#23450;&#21040;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#24182;&#29992;&#20110;&#25105;&#20204;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29289;&#29702;&#20449;&#24687;&#23545;&#27604;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neur
&lt;/p&gt;</description></item><item><title>EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;</title><link>https://arxiv.org/abs/2312.04916</link><description>&lt;p&gt;
EE-LLM: &#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#30340;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04916
&lt;/p&gt;
&lt;p&gt;
EE-LLM&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#19977;&#32500;&#24182;&#34892;&#24615;&#21644;&#22810;&#39033;&#31639;&#27861;&#21019;&#26032;&#12290;&#30740;&#31350;&#21457;&#29616;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#35745;&#31639;&#24320;&#38144;&#26497;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;EE-LLM&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22823;&#35268;&#27169;&#35757;&#32451;&#21644;&#25512;&#29702;&#26089;&#36864;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#21021;&#27493;&#35777;&#26126;&#20102;&#26089;&#36864;&#20986;&#22312;&#21152;&#36895;LLM&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;EE-LLM&#36890;&#36807;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#19977;&#32500;&#24182;&#34892;&#24615;&#26469;&#25512;&#21160;&#26089;&#36864;&#20986;LLM&#30340;&#35268;&#27169;&#21270;&#12290;&#22522;&#20110;Megatron-LM&#26500;&#24314;&#30340;EE-LLM&#23454;&#29616;&#20102;&#21508;&#31181;&#31639;&#27861;&#21019;&#26032;&#21644;&#24615;&#33021;&#20248;&#21270;&#65292;&#20197;&#36866;&#24212;&#26089;&#36864;&#20986;&#65292;&#21253;&#25324;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#27700;&#32447;&#24182;&#34892;&#24615;&#20419;&#36827;&#26089;&#36864;&#20986;&#35757;&#32451;&#30446;&#26631;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#21033;&#29992;&#21407;&#22987;&#27969;&#27700;&#32447;&#35843;&#24230;&#20013;&#30340;&#31354;&#38386;&#36164;&#28304;&#36827;&#34892;&#19982;&#26089;&#36864;&#20986;&#23618;&#30456;&#20851;&#30340;&#35745;&#31639;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#20004;&#31181;&#19982;KV&#32531;&#23384;&#20860;&#23481;&#30340;&#26089;&#36864;&#20986;&#25512;&#29702;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#22238;&#24402;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#24573;&#30053;&#30340;&#35745;&#31639;&#24320;&#38144;&#30456;&#27604;&#65292;EE-LLM&#22312;&#35757;&#32451;&#25928;&#29575;&#19978;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present EE-LLM, a framework for large-scale training and inference of early-exit large language models (LLMs). While recent works have shown preliminary evidence for the efficacy of early exiting in accelerating LLM inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs by supporting their training and inference with massive 3D parallelism. Built upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and performance optimizations tailored to early exiting, including a lightweight method that facilitates backpropagation for the early-exit training objective with pipeline parallelism, techniques of leveraging idle resources in the original pipeline schedule for computation related to early-exit layers, and two approaches of early-exit inference that are compatible with KV caching for autoregressive generation. Our analytical and empirical study shows that EE-LLM achieves great training efficiency with negligible computational overhead compared
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Pointer Q-Network (PQN)&#26041;&#27861;&#65292;&#23558;Ptr-Nets&#21644;Q-learning&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20854;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#20986;&#33394;&#22320;&#25429;&#33719;&#20102;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;OP&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20855;&#20307;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.02629</link><description>&lt;p&gt;
&#24102;&#26377;Q-Learning&#30340;&#25351;&#38024;&#32593;&#32476;&#29992;&#20110;OP&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pointer Networks with Q-Learning for OP Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02629
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Pointer Q-Network (PQN)&#26041;&#27861;&#65292;&#23558;Ptr-Nets&#21644;Q-learning&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#20854;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#20986;&#33394;&#22320;&#25429;&#33719;&#20102;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#20915;&#20102;OP&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20855;&#20307;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Orienteering Problem&#65288;OP&#65289;&#22312;&#32452;&#21512;&#20248;&#21270;&#65288;CO&#65289;&#20013;&#25552;&#20986;&#20102;&#29420;&#29305;&#25361;&#25112;&#65292;&#31361;&#26174;&#20102;&#20854;&#22312;&#29289;&#27969;&#12289;&#20132;&#20184;&#21644;&#36816;&#36755;&#35268;&#21010;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;OP&#30340;NP-hard&#24615;&#36136;&#65292;&#33719;&#24471;&#26368;&#20248;&#35299;&#26412;&#36136;&#19978;&#26159;&#22797;&#26434;&#30340;&#12290;&#23613;&#31649;&#25351;&#38024;&#32593;&#32476;&#65288;Ptr-Nets&#65289;&#22312;&#21508;&#31181;&#32452;&#21512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;OP&#19978;&#30340;&#34920;&#29616;&#20197;&#21450;&#38656;&#35201;&#19987;&#27880;&#20110;&#26410;&#26469;&#22238;&#25253;&#25110;&#25506;&#32034;&#30340;&#20219;&#21153;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#35748;&#35782;&#21040;&#23558;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#19982;&#24207;&#21015;-&#24207;&#21015;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#28508;&#33021;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#25351;&#38024;Q&#32593;&#32476;&#65288;PQN&#65289;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;Ptr-Nets&#21644;Q-learning&#65292;&#30001;&#20110;&#20854;&#20165;&#20855;&#25209;&#35780;&#32773;&#24615;&#36136;&#65292;&#23427;&#22312;&#25429;&#33719;&#23884;&#20837;&#22270;&#20013;&#30340;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#26159;&#26377;&#25928;&#24212;&#23545;OP&#25552;&#20986;&#30340;&#20855;&#20307;&#25361;&#25112;&#30340;&#22522;&#26412;&#35201;&#27714;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#26550;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02629v2 Announce Type: replace  Abstract: The Orienteering Problem (OP) presents a unique challenge in Combinatorial Optimization (CO), emphasized by its widespread use in logistics, delivery, and transportation planning. Given the NP-hard nature of OP, obtaining optimal solutions is inherently complex. While Pointer Networks (Ptr-Nets) have exhibited prowess in various combinatorial tasks, their performance in the context of OP, and duties requiring focus on future return or exploration, leaves room for improvement. Recognizing the potency combining Reinforcement Learning (RL) methods with sequence-to-sequence models, this research unveils the Pointer Q-Network (PQN). This method combines Ptr-Nets and Q-learning, which, thanks to its critic only nature, outstands in its capability of capturing relationships within an embedded graph, a fundamental requirement in order to effectively address the specific challenges presented by OP. We explore the architecture and functionalit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.16986</link><description>&lt;p&gt;
&#29992;&#20110;&#25104;&#26412;&#25928;&#30410;&#20248;&#21270;&#30340;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#22312;&#21457;&#23637;&#25588;&#21161;&#20998;&#37197;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Causal Machine Learning for Cost-Effective Allocation of Development Aid. (arXiv:2401.16986v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#22269;&#30340;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#25552;&#20379;&#20102;&#8220;&#26080;&#20154;&#34987;&#36951;&#24323;&#8221;&#30340;&#26356;&#32654;&#22909;&#26410;&#26469;&#34013;&#22270;&#65292;&#20026;&#20102;&#22312;2030&#24180;&#20043;&#21069;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;&#36139;&#31351;&#22269;&#23478;&#38656;&#35201;&#22823;&#37327;&#30340;&#21457;&#23637;&#25588;&#21161;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;&#25588;&#21161;&#20998;&#37197;&#30340;&#24322;&#36136;&#21270;&#27835;&#30103;&#25928;&#26524;&#65292;&#20197;&#25903;&#25345;&#26377;&#25928;&#30340;&#25588;&#21161;&#20998;&#37197;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#19968;&#20010;&#24179;&#34913;&#33258;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#23558;&#39640;&#32500;&#22269;&#23478;&#29305;&#24449;&#23884;&#20837;&#65292;&#21516;&#26102;&#35299;&#20915;&#27835;&#30103;&#36873;&#25321;&#20559;&#24046;&#38382;&#39064;&#65307;&#65288;ii&#65289;&#19968;&#20010;&#21453;&#20107;&#23454;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#35745;&#31639;&#22312;&#19981;&#21516;&#25588;&#21161;&#35268;&#27169;&#19979;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#65292;&#20197;&#35299;&#20915;&#23567;&#26679;&#26412;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#19968;&#20010;&#25512;&#26029;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#24322;&#36136;&#21270;&#30340;&#27835;&#30103;&#25928;&#26524;&#26354;&#32447;&#12290;&#25105;&#20204;&#20351;&#29992;105&#20010;&#22269;&#23478;&#25112;&#30053;&#24615;&#21457;&#23637;&#25588;&#21161;&#25968;&#25454;&#65288;&#24635;&#39069;&#36229;&#36807;52&#20159;&#32654;&#20803;&#65289;&#65292;&#20197;&#32467;&#26463;HIV/AIDS&#20026;&#30446;&#26631;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Sustainable Development Goals (SDGs) of the United Nations provide a blueprint of a better future by 'leaving no one behind', and, to achieve the SDGs by 2030, poor countries require immense volumes of development aid. In this paper, we develop a causal machine learning framework for predicting heterogeneous treatment effects of aid disbursements to inform effective aid allocation. Specifically, our framework comprises three components: (i) a balancing autoencoder that uses representation learning to embed high-dimensional country characteristics while addressing treatment selection bias; (ii) a counterfactual generator to compute counterfactual outcomes for varying aid volumes to address small sample-size settings; and (iii) an inference model that is used to predict heterogeneous treatment-response curves. We demonstrate the effectiveness of our framework using data with official development aid earmarked to end HIV/AIDS in 105 countries, amounting to more than USD 5.2 billion. F
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#25429;&#25417;&#21644;&#25512;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16421</link><description>&lt;p&gt;
&#20004;&#31181;&#30707;&#22836;&#20987;&#25171;&#19968;&#21482;&#40479;&#65306;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#20197;&#26356;&#22909;&#22320;&#25512;&#27979;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation. (arXiv:2401.16421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#65292;&#36890;&#36807;&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#35821;&#20041;&#20449;&#24687;&#30340;&#25429;&#25417;&#21644;&#25512;&#27979;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#35328;&#24207;&#21015;&#30340;&#20869;&#22312;&#20998;&#21106;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#32966;&#23618;&#20301;&#32622;&#32534;&#30721;&#65288;BiPE&#65289;&#12290;&#23545;&#20110;&#27599;&#20010;&#20301;&#32622;&#65292;&#25105;&#20204;&#30340;BiPE&#23558;&#20998;&#27573;&#20869;&#32534;&#30721;&#21644;&#20998;&#27573;&#38388;&#32534;&#30721;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#20998;&#27573;&#20869;&#32534;&#30721;&#29992;&#20110;&#35782;&#21035;&#27573;&#20869;&#20301;&#32622;&#65292;&#24182;&#36890;&#36807;&#32477;&#23545;&#20301;&#32622;&#32534;&#30721;&#24110;&#21161;&#27169;&#22411;&#25429;&#25417;&#20854;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20998;&#27573;&#38388;&#32534;&#30721;&#21017;&#29992;&#20110;&#25351;&#23450;&#27573;&#32034;&#24341;&#65292;&#24314;&#27169;&#27573;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#30456;&#23545;&#20301;&#32622;&#32534;&#30721;&#25552;&#39640;&#25512;&#27979;&#33021;&#21147;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#20301;&#32622;&#20449;&#24687;&#30340;&#35299;&#32806;&#20351;&#23398;&#20064;&#26356;&#21152;&#26377;&#25928;&#12290;&#32463;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;BiPE&#22312;&#19981;&#21516;&#25991;&#26412;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20248;&#36234;&#30340;&#38271;&#24230;&#25512;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.
&lt;/p&gt;</description></item><item><title>HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15207</link><description>&lt;p&gt;
HiFT:&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy. (arXiv:2401.15207v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15207
&lt;/p&gt;
&lt;p&gt;
HiFT&#26159;&#19968;&#31181;&#20998;&#23618;&#20840;&#21442;&#25968;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;GPU&#20869;&#23384;&#20351;&#29992;&#65292;&#24182;&#23454;&#29616;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#38271;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#38656;&#35201;&#21344;&#29992;&#22823;&#37327;GPU&#20869;&#23384;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#20197;&#33410;&#30465;GPU&#20869;&#23384;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#38750;&#38646;&#38454;&#20248;&#21270;&#22120;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#19978;&#26356;&#23481;&#26131;&#25910;&#25947;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29420;&#31435;&#20110;&#20248;&#21270;&#22120;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#24494;&#35843;&#31574;&#30053;HiFT&#65292;&#23427;&#20165;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#26356;&#26032;&#21442;&#25968;&#30340;&#23376;&#38598;&#12290; HiFT&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#30340;&#26799;&#24230;&#21644;&#20248;&#21270;&#22120;&#29366;&#24577;&#21442;&#25968;&#30340;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#20869;&#23384;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;HiFT&#23454;&#29616;&#20102;&#19982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#26631;&#20934;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#65288;2&#65289;HiFT&#25903;&#25345;&#21253;&#25324;&#22312;&#20869;&#30340;&#21508;&#31181;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Full-parameter fine-tuning has become the go-to choice for adapting language models (LMs) to downstream tasks due to its excellent performance. As LMs grow in size, fine-tuning the full parameters of LMs requires a prohibitively large amount of GPU memory. Existing approaches utilize zeroth-order optimizer to conserve GPU memory, which can potentially compromise the performance of LMs as non-zero order optimizers tend to converge more readily on most downstream tasks. In this paper, we propose a novel optimizer-independent end-to-end hierarchical fine-tuning strategy, HiFT, which only updates a subset of parameters at each training step. HiFT can significantly reduce the amount of gradients and optimizer state parameters residing in GPU memory at the same time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT achieves comparable performance to parameter-efficient fine-tuning and standard full parameter fine-tuning. (2) HiFT supports various optimizers including
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.13311</link><description>&lt;p&gt;
ConTextual: &#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35780;&#20272;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in Large Multimodal Models. (arXiv:2401.13311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;ConTextual&#65292;&#29992;&#20110;&#35780;&#20272;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#65292;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#28041;&#21450;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#22797;&#26434;&#20219;&#21153;&#65292;&#20363;&#22914;&#22312;&#20844;&#20849;&#22330;&#25152;&#23548;&#33322;&#22320;&#22270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ConTextual&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#19987;&#38376;&#35774;&#35745;&#30340;&#25351;&#20196;&#65292;&#29992;&#20110;&#35780;&#20272;LMMs&#22312;&#25191;&#34892;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;ConTextual&#24378;&#35843;&#20102;&#22810;&#26679;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#65288;&#20363;&#22914;&#26102;&#38388;&#38405;&#35835;&#12289;&#23548;&#33322;&#12289;&#36141;&#29289;&#31561;&#65289;&#65292;&#35201;&#27714;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;LMM&#65292;GPT-4V(ision)&#65292;&#19982;&#20154;&#31867;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;30.8%&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#25351;&#20986;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#25991;&#26412;&#23500;&#26377;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#34429;&#28982;GPT-4V&#22312;&#25277;&#35937;&#31867;&#21035;&#65288;&#22914;&#27169;&#22240;&#21644;&#24341;&#25991;&#35299;&#37322;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#25972;&#20307;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in AI have led to the development of large multimodal models (LMMs) capable of processing complex tasks involving joint reasoning over text and visual content in the image (e.g., navigating maps in public places). This paper introduces ConTextual, a novel benchmark comprising instructions designed explicitly to evaluate LMMs' ability to perform context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse real-world scenarios (e.g., time-reading, navigation, shopping and more) demanding a deeper understanding of the interactions between textual and visual elements. Our findings reveal a significant performance gap of 30.8% between the best-performing LMM, GPT-4V(ision), and human capabilities using human evaluation indicating substantial room for improvement in context-sensitive text-rich visual reasoning. Notably, while GPT-4V excelled in abstract categories like meme and quote interpretation, its overall performance still lagged behind humans. In add
&lt;/p&gt;</description></item><item><title>&#32842;&#22825;&#23453;&#30418;&#65288;Chatterbox&#65289;&#26159;&#38024;&#23545;LLM Chatbots&#20013;&#30340;&#20196;&#29260;&#27969;&#38382;&#39064;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#23618;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26032;&#29983;&#25104;&#30340;&#20196;&#29260;&#21644;&#24403;&#21069;&#26410;&#30830;&#35748;&#30340;&#20196;&#29260;&#25918;&#20837;&#19979;&#19968;&#20010;&#21457;&#36865;&#30340;&#25968;&#25454;&#21253;&#20013;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#20256;&#36755;&#21644;&#28210;&#26579;&#65292;&#36991;&#20813;&#20102;&#22312;&#19981;&#31283;&#23450;&#32593;&#32476;&#29615;&#22659;&#19979;&#30340;&#20572;&#39039;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2401.12961</link><description>&lt;p&gt;
&#32842;&#22825;&#23453;&#30418;&#65306;&#19981;&#31283;&#23450;&#32593;&#32476;&#19979;LLM Token Streaming&#30340;&#31283;&#20581;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network. (arXiv:2401.12961v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12961
&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;&#23453;&#30418;&#65288;Chatterbox&#65289;&#26159;&#38024;&#23545;LLM Chatbots&#20013;&#30340;&#20196;&#29260;&#27969;&#38382;&#39064;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#23618;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#26032;&#29983;&#25104;&#30340;&#20196;&#29260;&#21644;&#24403;&#21069;&#26410;&#30830;&#35748;&#30340;&#20196;&#29260;&#25918;&#20837;&#19979;&#19968;&#20010;&#21457;&#36865;&#30340;&#25968;&#25454;&#21253;&#20013;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#20256;&#36755;&#21644;&#28210;&#26579;&#65292;&#36991;&#20813;&#20102;&#22312;&#19981;&#31283;&#23450;&#32593;&#32476;&#29615;&#22659;&#19979;&#30340;&#20572;&#39039;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#26102;&#28210;&#26579;&#29983;&#25104;&#30340;&#20196;&#29260;&#65292;LLM&#26381;&#21153;&#22120;&#36880;&#20010;&#29983;&#25104;&#21709;&#24212;&#20196;&#29260;&#65292;&#24182;&#36890;&#36807;&#32593;&#32476;&#23558;&#27599;&#20010;&#29983;&#25104;&#30340;&#20196;&#29260;&#65288;&#25110;&#23569;&#37327;&#20196;&#29260;&#32452;&#65289;&#27969;&#24335;&#20256;&#36755;&#21040;&#29992;&#25143;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;LLM&#20196;&#29260;&#27969;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#31283;&#23450;&#30340;&#32593;&#32476;&#26465;&#20214;&#19979;&#65292;LLM&#20196;&#29260;&#20256;&#36755;&#20307;&#39564;&#21487;&#33021;&#20250;&#21463;&#21040;&#26497;&#22823;&#30340;&#20572;&#39039;&#24433;&#21709;&#65292;&#22240;&#20026;&#19968;&#27425;&#25968;&#25454;&#21253;&#20002;&#22833;&#21487;&#33021;&#20250;&#38459;&#22622;&#21518;&#32493;&#25968;&#25454;&#21253;&#20013;&#21253;&#21547;&#30340;&#20196;&#29260;&#30340;&#28210;&#26579;&#65292;&#21363;&#20351;&#23427;&#20204;&#25353;&#26102;&#21040;&#36798;&#12290;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#37327;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;ChatGPT&#65292;Claude&#21644;Bard&#65289;&#22312;&#19981;&#31283;&#23450;&#32593;&#32476;&#26465;&#20214;&#19979;&#37117;&#20250;&#36973;&#21463;&#20572;&#39039;&#38382;&#39064;&#30340;&#22686;&#21152;&#12290;&#38024;&#23545;LLM Chatbots&#20013;&#20986;&#29616;&#30340;&#36825;&#20010;&#26032;&#20852;&#30340;&#20196;&#29260;&#27969;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#36755;&#23618;&#26041;&#26696;&#65292;&#31216;&#20026;&#32842;&#22825;&#23453;&#30418;&#65288;Chatterbox&#65289;&#65292;&#23427;&#23558;&#26032;&#29983;&#25104;&#30340;&#20196;&#29260;&#21644;&#24403;&#21069;&#26410;&#30830;&#35748;&#30340;&#20196;&#29260;&#25918;&#20837;&#19979;&#19968;&#20010;&#21457;&#36865;&#30340;&#25968;&#25454;&#21253;&#20013;&#12290;&#36825;&#26679;&#65292;&#27599;&#20010;&#25968;&#25454;&#21253;&#37117;&#21253;&#21547;&#19968;&#20123;&#26032;&#30340;&#20196;&#29260;&#65292;&#24182;&#19988;&#22312;&#25509;&#25910;&#21040;&#26102;&#21487;&#20197;&#29420;&#31435;&#36827;&#34892;&#28210;&#26579;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20572;&#39039;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
To render each generated token in real time, the LLM server generates response tokens one by one and streams each generated token (or group of a few tokens) through the network to the user right after it is generated, which we refer to as LLM token streaming. However, under unstable network conditions, the LLM token streaming experience could suffer greatly from stalls since one packet loss could block the rendering of tokens contained in subsequent packets even if they arrive on time. With a real-world measurement study, we show that current applications including ChatGPT, Claude, and Bard all suffer from increased stall under unstable network.  For this emerging token streaming problem in LLM Chatbots, we propose a novel transport layer scheme, called Chatterbox, which puts new generated tokens as well as currently unacknowledged tokens in the next outgoing packet. This ensures that each packet contains some new tokens and can be independently rendered when received, thus avoiding af
&lt;/p&gt;</description></item><item><title>AFS-BM&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.11250</link><description>&lt;p&gt;
AFS-BM:&#36890;&#36807;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#20108;&#20540;&#23631;&#34109;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
AFS-BM: Enhancing Model Performance through Adaptive Feature Selection with Binary Masking. (arXiv:2401.11250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11250
&lt;/p&gt;
&lt;p&gt;
AFS-BM&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#23454;&#29616;&#20102;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#29305;&#24449;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#36825;&#26159;&#35813;&#39046;&#22495;&#20013;&#26368;&#20851;&#38190;&#30340;&#20027;&#39064;&#20043;&#19968;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#12289;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#12289;&#22788;&#29702;&#30456;&#20851;&#29305;&#24449;&#12289;&#36866;&#24212;&#21487;&#21464;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;&#33258;&#36866;&#24212;&#29305;&#24449;&#36873;&#25321;&#21644;&#20108;&#20540;&#23631;&#34109;&#8221;(AFS-BM)&#12290;AFS-BM&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26469;&#21516;&#26102;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#27169;&#22411;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21644;&#20108;&#20540;&#23631;&#34109;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25345;&#32493;&#35843;&#25972;&#29305;&#24449;&#38598;&#21644;&#27169;&#22411;&#21442;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#20943;&#23569;&#20102;&#35745;&#31639;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;AFS-BM&#19982;&#24050;&#26377;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of feature selection in general machine learning (ML) context, which is one of the most critical subjects in the field. Although, there exist many feature selection methods, however, these methods face challenges such as scalability, managing high-dimensional data, dealing with correlated features, adapting to variable feature importance, and integrating domain knowledge. To this end, we introduce the ``Adaptive Feature Selection with Binary Masking" (AFS-BM) which remedies these problems. AFS-BM achieves this by joint optimization for simultaneous feature selection and model training. In particular, we do the joint optimization and binary masking to continuously adapt the set of features and model parameters during the training process. This approach leads to significant improvements in model accuracy and a reduction in computational requirements. We provide an extensive set of experiments where we compare AFS-BM with the established feature selection methods usin
&lt;/p&gt;</description></item><item><title>Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2401.10774</link><description>&lt;p&gt;
Medusa: &#22810;&#35299;&#30721;&#22836;&#30340;&#31616;&#27905;LLM&#25512;&#29702;&#21152;&#36895;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. (arXiv:2401.10774v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10774
&lt;/p&gt;
&lt;p&gt;
Medusa&#26159;&#19968;&#20010;&#33021;&#22815;&#25552;&#21319;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#31616;&#27905;&#26694;&#26550;&#65292;&#36890;&#36807;&#22686;&#21152;&#22810;&#20010;&#35299;&#30721;&#22836;&#20197;&#23454;&#29616;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#65292;&#24182;&#36890;&#36807;&#26641;&#29366;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#24182;&#34892;&#22788;&#29702;&#26469;&#20943;&#23569;&#35299;&#30721;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#36890;&#24120;&#21463;&#38480;&#20110;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#24182;&#34892;&#24615;&#32570;&#22833;&#65292;&#20351;&#24471;&#22823;&#22810;&#25968;&#25805;&#20316;&#21463;&#38480;&#20110;&#21152;&#36895;&#22120;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#33719;&#24471;&#21644;&#32500;&#25252;&#29420;&#31435;&#30340;&#33609;&#31295;&#27169;&#22411;&#25152;&#28041;&#21450;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#30340;&#23454;&#26045;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#26469;&#22686;&#24378;LLM&#25512;&#29702;&#65292;&#20197;&#24182;&#34892;&#39044;&#27979;&#22810;&#20010;&#21518;&#32493;&#26631;&#35760;&#12290;Medusa&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#21516;&#26102;&#26500;&#36896;&#22810;&#20010;&#20505;&#36873;&#24310;&#32493;&#24182;&#36827;&#34892;&#39564;&#35777;&#12290;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#65292;Medusa&#22312;&#21333;&#27493;&#24310;&#36831;&#26041;&#38754;&#20165;&#24341;&#20837;&#20102;&#26368;&#23567;&#30340;&#24320;&#38144;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#20102;&#25152;&#38656;&#30340;&#35299;&#30721;&#27493;&#39588;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inference process in Large Language Models (LLMs) is often limited due to the absence of parallelism in the auto-regressive decoding process, resulting in most operations being restricted by the memory bandwidth of accelerators. While methods such as speculative decoding have been suggested to address this issue, their implementation is impeded by the challenges associated with acquiring and maintaining a separate draft model. In this paper, we present Medusa, an efficient method that augments LLM inference by adding extra decoding heads to predict multiple subsequent tokens in parallel. Using a tree-based attention mechanism, Medusa constructs multiple candidate continuations and verifies them simultaneously in each decoding step. By leveraging parallel processing, Medusa introduces only minimal overhead in terms of single-step latency while substantially reducing the number of decoding steps required.  We present two levels of fine-tuning procedures for Medusa to meet the needs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10155</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A novel hybrid time-varying graph neural network for traffic flow forecasting. (arXiv:2401.10155v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#39044;&#23450;&#20041;&#22270;&#21644;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#20934;&#30830;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#30830;&#20445;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#39640;&#25928;&#36816;&#34892;&#30340;&#22522;&#30784;&#12290;&#22312;&#29616;&#26377;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26041;&#27861;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#22270;&#26469;&#25551;&#36848;&#22478;&#24066;&#36947;&#36335;&#32593;&#32476;&#20013;&#19981;&#21516;&#20132;&#36890;&#33410;&#28857;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#39044;&#23450;&#20041;&#22270;&#25551;&#36848;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20808;&#21069;&#30340;&#30693;&#35782;&#21644;&#22270;&#29983;&#25104;&#26041;&#27861;&#12290;&#23613;&#31649;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#30340;&#26102;&#21464;&#22270;&#33021;&#37096;&#20998;&#20811;&#26381;&#39044;&#23450;&#20041;&#22270;&#30340;&#32570;&#28857;&#65292;&#20294;&#29616;&#26377;&#33258;&#36866;&#24212;&#22270;&#30340;&#23398;&#20064;&#33021;&#21147;&#26377;&#38480;&#12290;&#20363;&#22914;&#65292;&#26102;&#21464;&#22270;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#28151;&#21512;&#26102;&#21464;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HTVGNN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time and accurate traffic flow prediction is the foundation for ensuring the efficient operation of intelligent transportation systems.In existing traffic flow prediction methods based on graph neural networks (GNNs), pre-defined graphs were usually used to describe the spatial correlations of different traffic nodes in urban road networks. However, the ability of pre-defined graphs used to describe spatial correlation was limited by prior knowledge and graph generation methods. Although time-varying graphs based on data-driven learning can partially overcome the drawbacks of pre-defined graphs, the learning ability of existing adaptive graphs was limited. For example, time-varying graphs cannot adequately capture the inherent spatial correlations in traffic flow data.In order to solve these problems, we have proposed a hybrid time-varying graph neural network (HTVGNN) for traffic flow prediction.
&lt;/p&gt;</description></item><item><title>MADA&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;MADA&#22312;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19979;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#12290;&#25554;&#20540;&#20248;&#21270;&#22120;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08893</link><description>&lt;p&gt;
MADA: &#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#30340;&#20803;&#36866;&#24212;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
MADA: Meta-Adaptive Optimizers through hyper-gradient Descent. (arXiv:2401.08893v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08893
&lt;/p&gt;
&lt;p&gt;
MADA&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#36229;&#26799;&#24230;&#19979;&#38477;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;MADA&#22312;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19979;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;&#20854;&#20182;&#20248;&#21270;&#22120;&#12290;&#25554;&#20540;&#20248;&#21270;&#22120;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Adam&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#12290;&#36825;&#20123;&#20248;&#21270;&#22120;&#36890;&#24120;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21487;&#33021;&#26080;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;Adam&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20803;&#36866;&#24212;&#20248;&#21270;&#22120;(MADA)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#20248;&#21270;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#27010;&#25324;&#22810;&#31181;&#24050;&#30693;&#30340;&#20248;&#21270;&#22120;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#23398;&#20064;&#26368;&#36866;&#21512;&#30340;&#20248;&#21270;&#22120;&#12290;MADA&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#20248;&#21270;&#22120;&#30340;&#31354;&#38388;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#20351;&#29992;&#36229;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#25628;&#32034;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;MADA&#23545;&#20110;&#27425;&#20248;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#26159;&#31283;&#20581;&#30340;&#65292;&#24182;&#19988;&#22312;&#40664;&#35748;&#36229;&#21442;&#25968;&#19979;&#24120;&#24120;&#20248;&#20110;Adam&#12289;Lion&#21644;Adan&#65292;&#29978;&#33267;&#22312;&#20248;&#21270;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;AVGrad&#65292;&#23427;&#26159;AMSGrad&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#22312;&#20854;&#20013;&#23558;&#26368;&#22823;&#25805;&#20316;&#31526;&#26367;&#25442;&#20026;&#24179;&#22343;&#25805;&#20316;&#31526;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#22312;MADA&#20013;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25910;&#25947;&#20998;&#26512;&#65292;&#20197;&#34920;&#26126;&#20248;&#21270;&#22120;&#30340;&#25554;&#20540;&#65288;&#29305;&#21035;&#26159;AVGrad&#21644;Adam&#65289;&#21487;&#20197;&#25913;&#36827;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since Adam was introduced, several novel adaptive optimizers for deep learning have been proposed. These optimizers typically excel in some tasks but may not outperform Adam uniformly across all tasks. In this work, we introduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can generalize several known optimizers and dynamically learn the most suitable one during training. The key idea in MADA is to parameterize the space of optimizers and search through it using hyper-gradient descent. Numerical results suggest that MADA is robust against sub-optimally tuned hyper-parameters, and outperforms Adam, Lion, and Adan with their default hyper-parameters, often even with optimized hyper-parameters. We also propose AVGrad, a variant of AMSGrad where the maximum operator is replaced with averaging, and observe that it performs better within MADA. Finally, we provide a convergence analysis to show that interpolation of optimizers (specifically, AVGrad and Adam) can improve
&lt;/p&gt;</description></item><item><title>MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08743</link><description>&lt;p&gt;
MMToM-QA: &#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08743
&lt;/p&gt;
&lt;p&gt;
MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#20204;&#30340;&#24515;&#26234;&#26159;&#24320;&#21457;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#31038;&#20132;&#26234;&#33021;&#30340;&#26426;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20284;&#20046;&#23637;&#29616;&#20986;&#26576;&#20123;&#24515;&#26234;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#26234;&#29702;&#35770;&#22522;&#20934;&#20351;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;-&#25110;&#32773;&#35270;&#39057;&#25110;&#32773;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24515;&#26234;&#29702;&#35770;&#19981;&#20165;&#20165;&#26159;&#35270;&#39057;&#25110;&#25991;&#26412;&#29702;&#35299;&#12290;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;&#20174;&#20219;&#20309;&#21487;&#29992;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#34920;&#31034;&#65288;&#20363;&#22914;&#30446;&#26631;&#65292;&#20449;&#24565;&#65292;&#35745;&#21010;&#65289;&#28789;&#27963;&#22320;&#25512;&#29702;&#21478;&#19968;&#20010;&#20154;&#30340;&#24515;&#26234;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#21253;&#25324;&#35270;&#35273;&#32447;&#32034;&#65292;&#35821;&#35328;&#21465;&#20107;&#25110;&#20004;&#32773;&#20860;&#26377;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#65288;MMToM-QA&#65289;&#22522;&#20934;&#12290;MMToM-QA&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#20851;&#20110;&#19968;&#20010;&#20154;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#30340;&#19981;&#21516;&#31181;&#31867;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;BIP-ALM&#65288;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.06757</link><description>&lt;p&gt;
&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#12289;&#25968;&#25454;&#38598;&#21644;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;ARCANE&#65292;&#24182;&#20171;&#32461;&#20102;&#30456;&#24212;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;PedSynth&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#28145;&#24230;&#27169;&#22411;PedGNN&#65292;&#29992;&#20110;&#23454;&#26102;C/NC&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20154;&#24847;&#22270;&#39044;&#27979;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#29305;&#21035;&#26159;&#65292;&#20102;&#35299;&#34892;&#20154;&#26159;&#21542;&#23558;&#27178;&#31359;&#22312;&#33258;&#20027;&#36710;&#36742;&#21069;&#26041;&#23545;&#20110;&#25191;&#34892;&#23433;&#20840;&#21644;&#33298;&#36866;&#30340;&#25805;&#25511;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#24207;&#21015;&#22270;&#20687;&#20013;&#20934;&#30830;&#19988;&#24555;&#36895;&#22320;&#39044;&#27979;&#27492;&#31867;&#24847;&#22270;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#23548;&#33268;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#20010;&#22240;&#32032;&#26159;&#32570;&#20047;&#20855;&#26377;&#22810;&#26679;&#21270;&#27178;&#31359;&#21644;&#38750;&#27178;&#31359;&#65288;C/NC&#65289;&#22330;&#26223;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;ARCANE&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#33258;&#21160;&#22320;&#29983;&#25104;&#21253;&#21547;C/NC&#35270;&#39057;&#21098;&#36753;&#26679;&#26412;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20316;&#20026;&#19968;&#20010;&#31034;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;ARCANE&#29983;&#25104;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;PedSynth&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;PedSynth&#22914;&#20309;&#34917;&#20805;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#65292;&#22914;JAAD&#21644;PIE&#65292;&#20174;&#32780;&#20026;C/NC&#39044;&#27979;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#32771;&#34385;&#21040;C/NC&#39044;&#27979;&#27169;&#22411;&#30340;&#36710;&#36733;&#37096;&#32626;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PedGNN&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#23427;&#36895;&#24230;&#24555;&#19988;&#20869;&#23384;&#21344;&#29992;&#38750;&#24120;&#20302;&#12290;PedGNN&#22522;&#20110;GNN-G&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06699</link><description>&lt;p&gt;
&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#20248;&#21270;&#30340;&#38381;&#21512;&#35299;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#21512;&#35299;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#26469;&#20248;&#21270;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#21644;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#19982;&#29616;&#26377;&#30340;&#22522;&#20110;&#21453;&#21521;&#20256;&#25773;&#21644;&#38142;&#24335;&#35268;&#21017;&#26799;&#24230;&#20248;&#21270;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#20108;&#20056;&#27861;&#25552;&#20379;&#20102;&#38381;&#21512;&#24418;&#24335;&#30340;&#26435;&#37325;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#26159;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65292;&#26032;&#26041;&#27861;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#19968;&#32452;&#26435;&#37325;&#65292;&#22312;&#21333;&#27425;&#36845;&#20195;&#20013;&#20197;&#21453;&#21521;&#20256;&#25773;&#30340;&#26041;&#24335;&#20248;&#21270;&#26435;&#37325;&#12290;&#22312;&#36755;&#20837;&#21040;&#36755;&#20986;&#26144;&#23556;&#19981;&#21487;&#36870;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#20998;&#31867;&#38382;&#39064;&#65289;&#65292;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#36731;&#26494;&#22320;&#22312;&#20960;&#27425;&#36845;&#20195;&#20013;&#33719;&#24471;&#26368;&#32456;&#35299;&#12290;&#19982;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#20248;&#21183;&#26159;&#36825;&#20123;&#35745;&#31639;&#65288;&#23545;&#20110;&#27599;&#20010;&#31070;&#32463;&#20803;&#23618;&#30340;&#25152;&#26377;&#31070;&#32463;&#20803;&#65289;&#26159;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri
&lt;/p&gt;</description></item><item><title>StockFormer&#26159;&#19968;&#31181;&#22522;&#20110;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#65292;&#20351;&#29992;TopKDropout&#26041;&#27861;&#26469;&#25552;&#39640;&#32929;&#31080;&#36873;&#21462;&#33021;&#21147;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#21313;&#20010;&#34892;&#19994;&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;62.39%&#30340;&#24066;&#22330;&#36235;&#21183;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20851;&#38190;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#12290;&#22312;&#22238;&#27979;&#20013;&#65292;StockFormer&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#32047;&#35745;&#25910;&#30410;&#29575;&#20026;13.19%&#12290;</title><link>http://arxiv.org/abs/2401.06139</link><description>&lt;p&gt;
StockFormer: &#19968;&#31181;&#22522;&#20110;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks. (arXiv:2401.06139v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06139
&lt;/p&gt;
&lt;p&gt;
StockFormer&#26159;&#19968;&#31181;&#22522;&#20110;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#65292;&#20351;&#29992;TopKDropout&#26041;&#27861;&#26469;&#25552;&#39640;&#32929;&#31080;&#36873;&#21462;&#33021;&#21147;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#20854;&#39044;&#27979;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#21313;&#20010;&#34892;&#19994;&#27169;&#22411;&#65292;&#36798;&#21040;&#20102;62.39%&#30340;&#24066;&#22330;&#36235;&#21183;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#20851;&#38190;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#19978;&#30340;&#20248;&#31168;&#34920;&#29616;&#12290;&#22312;&#22238;&#27979;&#20013;&#65292;StockFormer&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#32047;&#35745;&#25910;&#30410;&#29575;&#20026;13.19%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#30340;&#24066;&#22330;&#37325;&#26032;&#26657;&#20934;&#21644;&#25237;&#36164;&#32773;&#20048;&#35266;&#24773;&#32490;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#32654;&#22269;&#32929;&#24066;&#27491;&#22312;&#32463;&#21382;&#22797;&#33487;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#19968;&#20123;&#20808;&#36827;&#30340;&#24037;&#20855;&#26469;&#20445;&#25252;&#21644;&#22686;&#38271;&#25237;&#36164;&#32452;&#21512;&#12290;&#38024;&#23545;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"Stockformer"&#30340;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;&#25670;&#21160;&#20132;&#26131;&#65292;&#24182;&#37319;&#29992;&#20102;TopKDropout&#26041;&#27861;&#26469;&#22686;&#24378;&#32929;&#31080;&#36873;&#21462;&#33021;&#21147;&#12290;&#36890;&#36807;&#25972;&#21512;STL&#20998;&#35299;&#21644;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;Stockformer&#21033;&#29992;&#26631;&#26222;500&#30340;&#22797;&#26434;&#25968;&#25454;&#26469;&#25552;&#21319;&#32929;&#31080;&#25910;&#30410;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23558;&#25968;&#25454;&#20998;&#20026;&#35757;&#32451;&#21644;&#39564;&#35777;&#38598;&#65288;2021&#24180;1&#26376;&#33267;2023&#24180;1&#26376;&#65289;&#20197;&#21450;&#27979;&#35797;&#38598;&#65288;2023&#24180;2&#26376;&#33267;6&#26376;&#65289;&#12290;&#22312;&#27979;&#35797;&#26399;&#38388;&#65292;Stockformer&#30340;&#39044;&#27979;&#32467;&#26524;&#20248;&#20110;&#20854;&#20182;&#21313;&#20010;&#34892;&#19994;&#27169;&#22411;&#65292;&#22312;&#20851;&#38190;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#65288;MAE&#65292;RMSE&#65292;MAPE&#65289;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#31934;&#24230;&#65292;&#26816;&#27979;&#24066;&#22330;&#36235;&#21183;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;62.39%&#12290;&#22312;&#25105;&#20204;&#30340;&#22238;&#27979;&#20013;&#65292;Stockformer&#30340;&#25670;&#21160;&#20132;&#26131;&#31574;&#30053;&#32047;&#35745;&#25910;&#30410;&#29575;&#20026;13.19%&#65292;&#24180;&#21270;&#25910;&#30410;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce "Stockformer," a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&amp;P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.03160</link><description>&lt;p&gt;
&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65306;&#22686;&#24378;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#23454;&#29616;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#30340;&#23433;&#20840;&#39640;&#25928;&#33258;&#21160;&#39550;&#39542;&#12290;&#35813;&#26041;&#27861;&#23558;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#65292;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#25506;&#32034;&#65292;&#21516;&#26102;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#20197;&#36991;&#20813;&#20107;&#25925;&#65292;&#24182;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30830;&#20445;AVs&#30340;&#23433;&#20840;&#24615;&#21644;&#20132;&#36890;&#27969;&#25928;&#29575;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21457;&#23637;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#20154;&#26426;&#21327;&#20316;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;HAIM-DRL&#65289;&#26694;&#26550;&#65292;&#20197;&#22312;&#28151;&#21512;&#20132;&#36890;&#32534;&#38431;&#20013;&#23454;&#29616;&#23433;&#20840;&#39640;&#25928;&#30340;&#33258;&#21160;&#39550;&#39542;&#12290;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#26377;&#25928;&#22320;&#23558;&#20154;&#31867;&#26234;&#33021;&#27880;&#20837;&#21040;AI&#20013;&#65292;&#31216;&#20026;&#20154;&#20316;&#20026;AI&#23548;&#24072;&#65288;HAIM&#65289;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20154;&#31867;&#19987;&#23478;&#20316;&#20026;&#23548;&#24072;&#20026;AI&#20195;&#29702;&#25552;&#20379;&#24110;&#21161;&#12290;&#22312;&#20801;&#35768;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#30340;&#21516;&#26102;&#65292;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#22312;&#21361;&#38505;&#24773;&#20917;&#19979;&#25509;&#31649;&#25511;&#21046;&#65292;&#24182;&#23637;&#31034;&#27491;&#30830;&#30340;&#34892;&#21160;&#20197;&#36991;&#20813;&#28508;&#22312;&#20107;&#25925;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21487;&#20197;&#25351;&#23548;&#20195;&#29702;&#20943;&#23567;&#20132;&#36890;&#27969;&#24178;&#25200;&#65292;&#20174;&#32780;&#20248;&#21270;&#20132;&#36890;&#27969;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
&lt;/p&gt;</description></item><item><title>t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.02576</link><description>&lt;p&gt;
t-DGR: &#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual Learning in Decision Making. (arXiv:2401.02576v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02576
&lt;/p&gt;
&lt;p&gt;
t-DGR&#26159;&#19968;&#31181;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#22522;&#20110;&#36712;&#36857;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#24050;&#32463;&#25104;&#20026;&#20915;&#31574;&#21046;&#23450;&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#20174;&#20197;&#21069;&#36935;&#21040;&#30340;&#20219;&#21153;&#29983;&#25104;&#36712;&#36857;&#26469;&#22686;&#21152;&#24403;&#21069;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#29983;&#25104;&#22238;&#25918;&#26041;&#27861;&#20381;&#36182;&#20110;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#30340;&#36712;&#36857;&#20013;&#20250;&#20986;&#29616;&#32047;&#31215;&#35823;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#19988;&#38750;&#33258;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#26681;&#25454;&#36712;&#36857;&#26102;&#38388;&#27493;&#29983;&#25104;&#20219;&#21153;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#36830;&#32493;&#19990;&#30028;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24179;&#22343;&#25104;&#21151;&#29575;&#25351;&#26631;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/WilliamYue37/t-DGR&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative replay has emerged as a promising approach for continual learning in decision-making tasks. This approach addresses the problem of catastrophic forgetting by leveraging the generation of trajectories from previously encountered tasks to augment the current dataset. However, existing deep generative replay methods for continual learning rely on autoregressive models, which suffer from compounding errors in the generated trajectories. In this paper, we propose a simple, scalable, and non-autoregressive method for continual learning in decision-making tasks using a generative model that generates task samples conditioned on the trajectory timestep. We evaluate our method on Continual World benchmarks and find that our approach achieves state-of-the-art performance on the average success rate metric among continual learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.01335</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#21487;&#20197;&#23558;&#20854;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models. (arXiv:2401.01335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#33258;&#25105;&#23545;&#24328;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#20248;&#21270;&#27169;&#22411;&#31574;&#30053;&#65292;&#20174;&#32780;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#36716;&#21270;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#32454;&#35843;&#65288;SFT&#65289;&#21033;&#29992;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#21147;&#37327;&#23545;&#20110;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#19981;&#38656;&#35201;&#33719;&#21462;&#39069;&#22806;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#25104;&#20026;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#35843;&#65288;SPIN&#65289;&#30340;&#26032;&#30340;&#32454;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20174;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#32454;&#35843;&#30340;&#27169;&#22411;&#24320;&#22987;&#12290;SPIN&#30340;&#26680;&#24515;&#26159;&#33258;&#25105;&#23545;&#24369;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#21046;&#65292;&#20854;&#20013;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19982;&#33258;&#36523;&#30340;&#23454;&#20363;&#23545;&#24328;&#26469;&#25552;&#21319;&#33258;&#24049;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24369;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#29983;&#25104;&#33258;&#24049;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#20248;&#21270;&#33258;&#36523;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#33258;&#25105;&#29983;&#25104;&#30340;&#22238;&#24212;&#19982;&#26469;&#33258;&#20154;&#31867;&#26631;&#27880;&#25968;&#25454;&#30340;&#22238;&#24212;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23558;&#24369;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20026;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#20805;&#20998;&#21457;&#25496;&#20154;&#31867;&#26631;&#27880;&#31034;&#33539;&#25968;&#25454;&#22312;SFT&#20013;&#30340;&#28508;&#21147;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#35757;&#32451;&#30446;&#26631;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#26159;&#21487;&#20197;&#36798;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of human-annotated data through Supervised Fine-Tuning (SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we delve into the prospect of growing a strong LLM out of a weak one without the need for acquiring additional human-annotated data. We propose a new fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism, where the LLM refines its capability by playing against instances of itself. More specifically, the LLM generates its own training data from its previous iterations, refining its policy by discerning these self-generated responses from those obtained from human-annotated data. Our method progressively elevates the LLM from a nascent model to a formidable one, unlocking the full potential of human-annotated demonstration data for SFT. Theoretically, we prove that the global optimum to the training objective function of our method is achiev
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#40065;&#26834;&#30340;&#39640;&#26031;&#36807;&#31243;&#22343;&#21248;&#35823;&#24046;&#30028;&#38480;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#22312;&#32447;&#20248;&#21270;&#20013;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.07281</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#22810;&#20219;&#21153;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Safe Multi-Task Bayesian Optimization. (arXiv:2312.07281v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07281
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#40065;&#26834;&#30340;&#39640;&#26031;&#36807;&#31243;&#22343;&#21248;&#35823;&#24046;&#30028;&#38480;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#20197;&#35299;&#20915;&#23433;&#20840;&#22312;&#32447;&#20248;&#21270;&#20013;&#36229;&#21442;&#25968;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#25104;&#20026;&#23433;&#20840;&#22312;&#32447;&#31995;&#32479;&#20248;&#21270;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22240;&#20854;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#22122;&#22768;&#20581;&#22766;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21152;&#24555;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#20943;&#23569;&#30340;&#29289;&#29702;&#27169;&#22411;&#32435;&#20837;&#20248;&#21270;&#36807;&#31243;&#20013;&#20197;&#21152;&#36895;&#36807;&#31243;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#23545;&#23454;&#38469;&#31995;&#32479;&#30340;&#36817;&#20284;&#65292;&#24182;&#19988;&#20174;&#20013;&#36827;&#34892;&#37319;&#26679;&#35201;&#20415;&#23452;&#24471;&#22810;&#12290;&#27169;&#22411;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30001;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#34920;&#31034;&#65292;&#24182;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23398;&#20064;&#12290;&#23433;&#20840;&#24615;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31561;&#22312;&#32447;&#20248;&#21270;&#26041;&#27861;&#30340;&#37325;&#35201;&#26631;&#20934;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#24050;&#32463;&#35299;&#20915;&#20102;&#27492;&#38382;&#39064;&#65292;&#24182;&#22312;&#24050;&#30693;&#36229;&#21442;&#25968;&#30340;&#20551;&#35774;&#19979;&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#36825;&#26159;&#19981;&#36866;&#29992;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#40065;&#26834;&#39640;&#26031;&#36807;&#31243;&#22343;&#21248;&#35823;&#24046;&#30028;&#38480;&#65292;&#20197;&#28385;&#36275;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#28041;&#21450;&#20174;&#36229;&#21442;&#25968;&#21518;&#39564;&#20998;&#24067;&#35745;&#31639;&#32622;&#20449;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has become a powerful tool for safe online optimization of systems, due to its high sample efficiency and noise robustness. For further speed-up reduced physical models of the system can be incorporated into the optimization to accelerate the process, since the models are able to offer an approximation of the actual system, and sampling from them is significantly cheaper. The similarity between model and reality is represented by additional hyperparameters and learned within the optimization process. Safety is an important criteria for online optimization methods like Bayesian optimization, which has been addressed by recent literature, which provide safety guarantees under the assumption of known hyperparameters. However, in practice this is not applicable. Therefore, we extend the robust Gaussian process uniform error bounds to meet the multi-task setting, which involves the calculation of a confidence region from the hyperparameter posterior distribution utiliz
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2311.00226</link><description>&lt;p&gt;
Transformers&#26159;&#26080;&#32447;&#36890;&#20449;&#20013;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00226
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;Transformers&#21487;&#20197;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#21463;&#21040;&#36825;&#20010;&#23646;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20272;&#35745;&#65292;&#29992;&#20110;&#20272;&#35745;&#20174;&#25509;&#25910;&#21040;&#30340;&#31526;&#21495;&#20013;&#30340;&#20256;&#36755;&#31526;&#21495;&#30340;&#32463;&#20856;&#36890;&#20449;&#38382;&#39064;&#12290;&#36890;&#20449;&#20449;&#36947;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#23558;&#20256;&#36755;&#31526;&#21495;&#26144;&#23556;&#21040;&#25509;&#25910;&#31526;&#21495;&#30340;&#22122;&#22768;&#20989;&#25968;&#65292;&#36825;&#20010;&#20989;&#25968;&#21487;&#20197;&#30001;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#34920;&#31034;&#65292;&#20854;&#32479;&#35745;&#25968;&#25454;&#20381;&#36182;&#20110;&#19968;&#20010;&#65288;&#20063;&#26159;&#26410;&#30693;&#30340;&#65289;&#28508;&#22312;&#19978;&#19979;&#25991;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65292;&#21482;&#26159;&#35797;&#22270;&#20351;&#29992;&#24050;&#30693;&#30340;&#20256;&#36755;&#20449;&#21495;&#36827;&#34892;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#28982;&#21518;&#29992;&#20110;&#20272;&#35745;&#36830;&#32493;&#30340;&#26410;&#30693;&#20256;&#36755;&#31526;&#21495;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#32852;&#31995;&#65292;&#21363;Transformers&#22312;&#23569;&#37327;&#25552;&#31034;&#19979;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#19978;&#19979;&#25991;&#24207;&#21015;&#23436;&#25104;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#38544;&#24335;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#65292;&#22312;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#36827;&#34892;&#20102;&#20855;&#20307;&#30340;&#20272;&#35745;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#25269;&#25239;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#31561;&#20851;&#38190;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15330</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#24322;&#26500;&#28151;&#21512;&#27169;&#22411;&#30340;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Federated Learning: A Federated Gradient EM Algorithm for Heterogeneous Mixture Models with Robustness against Adversarial Attacks. (arXiv:2310.15330v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#65292;&#22312;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#23545;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#36827;&#34892;&#20102;&#20855;&#20307;&#30340;&#20272;&#35745;&#35823;&#24046;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#20855;&#26377;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#25269;&#25239;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#31561;&#20851;&#38190;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#26080;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#39046;&#22495;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#24322;&#26500;&#28151;&#21512;&#27604;&#20363;&#30340;&#28151;&#21512;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#26222;&#36890;&#28151;&#21512;&#27169;&#22411;&#30340;&#20840;&#38754;&#26377;&#38480;&#26679;&#26412;&#29702;&#35770;&#65292;&#28982;&#21518;&#23558;&#36825;&#19968;&#36890;&#29992;&#29702;&#35770;&#24212;&#29992;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#21644;&#28151;&#21512;&#22238;&#24402;&#65288;MoRs&#65289;&#20197;&#25551;&#36848;&#27169;&#22411;&#21442;&#25968;&#21644;&#28151;&#21512;&#27604;&#20363;&#30340;&#26174;&#24335;&#20272;&#35745;&#35823;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#32852;&#37030;&#26799;&#24230;EM&#31639;&#27861;&#20855;&#26377;&#20197;&#19979;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#36866;&#24212;&#26410;&#30693;&#20219;&#21153;&#30456;&#20284;&#24615;&#12289;&#23545;&#23569;&#37096;&#20998;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#25915;&#20987;&#20855;&#26377;&#24377;&#24615;&#12289;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#38544;&#31169;&#20197;&#21450;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While supervised federated learning approaches have enjoyed significant success, the domain of unsupervised federated learning remains relatively underexplored. In this paper, we introduce a novel federated gradient EM algorithm designed for the unsupervised learning of mixture models with heterogeneous mixture proportions across tasks. We begin with a comprehensive finite-sample theory that holds for general mixture models, then apply this general theory on Gaussian Mixture Models (GMMs) and Mixture of Regressions (MoRs) to characterize the explicit estimation error of model parameters and mixture proportions. Our proposed federated gradient EM algorithm demonstrates several key advantages: adaptability to unknown task similarity, resilience against adversarial attacks on a small fraction of data sources, protection of local data privacy, and computational and communication efficiency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#31574;&#30053;&#35780;&#20272;&#65292;&#22312;&#22312;&#32447;&#25512;&#26029;&#20013;&#33719;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.14286</link><description>&lt;p&gt;
&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#30340;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite-Sample Analysis of the Temporal Difference Learning. (arXiv:2310.14286v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#31574;&#30053;&#35780;&#20272;&#65292;&#22312;&#22312;&#32447;&#25512;&#26029;&#20013;&#33719;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#32447;&#31574;&#30053;&#35780;&#20272;&#20013;&#20351;&#29992;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#26102;&#38388;&#24046;&#24322;(TD)&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36890;&#29992;&#19988;&#19982;&#23454;&#20363;&#26080;&#20851;&#30340;&#27493;&#38271;&#21644;Polyak-Ruppert&#23614;&#24179;&#22343;&#65292;&#21487;&#20197;&#33719;&#24471;&#25509;&#36817;&#26368;&#20248;&#30340;&#26041;&#24046;&#21644;&#20559;&#24046;&#39033;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#25216;&#24039;&#22522;&#20110;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#30340;&#31934;&#30830;&#35823;&#24046;&#30028;&#38480;&#20197;&#21450;TD&#31867;&#22411;&#36882;&#24402;&#20135;&#29983;&#30340;&#38543;&#26426;&#30697;&#38453;&#20056;&#31215;&#30340;&#26032;&#31283;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the problem of obtaining sharp bounds for the performance of temporal difference (TD) methods with linear functional approximation for policy evaluation in discounted Markov Decision Processes. We show that a simple algorithm with a universal and instance-independent step size together with Polyak-Ruppert tail averaging is sufficient to obtain near-optimal variance and bias terms. We also provide the respective sample complexity bounds. Our proof technique is based on refined error bounds for linear stochastic approximation together with the novel stability result for the product of random matrices that arise from the TD-type recurrence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#30340;&#26041;&#27861;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;&#24369;&#35821;&#20041;&#20808;&#39564;&#21644;&#36328;&#23610;&#24230;&#30456;&#20851;&#20808;&#39564;&#30693;&#35782;&#36880;&#27493;&#25913;&#36827;&#20998;&#21106;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#20998;&#21106;&#24615;&#33021;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.13574</link><description>&lt;p&gt;
&#22522;&#20110;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#30340;&#27867;&#21270;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Progressive Dual Priori Network for Generalized Breast Tumor Segmentation. (arXiv:2310.13574v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#30340;&#26041;&#27861;&#29992;&#20110;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;&#24369;&#35821;&#20041;&#20808;&#39564;&#21644;&#36328;&#23610;&#24230;&#30456;&#20851;&#20808;&#39564;&#30693;&#35782;&#36880;&#27493;&#25913;&#36827;&#20998;&#21106;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#20998;&#21106;&#24615;&#33021;&#19978;&#37117;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#20083;&#33146;&#32959;&#30244;&#20998;&#21106;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25913;&#36827;&#23545;&#23567;&#23610;&#23544;&#12289;&#20302;&#23545;&#27604;&#24230;&#21644;&#19981;&#35268;&#21017;&#24418;&#29366;&#20083;&#33146;&#32959;&#30244;&#30340;&#20998;&#21106;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36827;&#21452;&#20808;&#39564;&#32593;&#32476;&#65288;PDPNet&#65289;&#65292;&#29992;&#20110;&#20174;&#19981;&#21516;&#26426;&#26500;&#37319;&#38598;&#30340;&#21160;&#24577;&#22686;&#24378;&#30913;&#20849;&#25391;&#22270;&#20687;&#65288;DCE-MRI&#65289;&#20013;&#20998;&#21106;&#20083;&#33146;&#32959;&#30244;&#12290;PDPNet&#39318;&#20808;&#36890;&#36807;&#31895;&#20998;&#21106;&#23450;&#20301;&#27169;&#22359;&#35009;&#21098;&#32959;&#30244;&#21306;&#22495;&#65292;&#28982;&#21518;&#20351;&#29992;&#24369;&#35821;&#20041;&#20808;&#39564;&#21644;&#36328;&#23610;&#24230;&#30456;&#20851;&#20808;&#39564;&#30693;&#35782;&#36880;&#27493;&#25913;&#36827;&#20083;&#33146;&#32959;&#30244;&#25513;&#33180;&#12290;&#20026;&#20102;&#39564;&#35777;PDPNet&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#22312;&#22810;&#20013;&#24515;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20122;&#20248;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;PDPNet&#30340;DSC&#12289;SEN&#12289;KAPPA&#21644;HD95&#20998;&#21035;&#25552;&#39640;&#20102;3.63&#65285;&#12289;8.19&#65285;&#12289;5.52&#65285;&#21644;3.66&#65285;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23450;&#20301;&#27169;&#22359;&#21487;&#20197;&#20943;&#23569;&#27491;&#24120;&#32452;&#32455;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
To promote the generalization ability of breast tumor segmentation models, as well as to improve the segmentation performance for breast tumors with smaller size, low-contrast amd irregular shape, we propose a progressive dual priori network (PDPNet) to segment breast tumors from dynamic enhanced magnetic resonance images (DCE-MRI) acquired at different sites. The PDPNet first cropped tumor regions with a coarse-segmentation based localization module, then the breast tumor mask was progressively refined by using the weak semantic priori and cross-scale correlation prior knowledge. To validate the effectiveness of PDPNet, we compared it with several state-of-the-art methods on multi-center datasets. The results showed that, comparing against the suboptimal method, the DSC, SEN, KAPPA and HD95 of PDPNet were improved 3.63\%, 8.19\%, 5.52\%, and 3.66\% respectively. In addition, through ablations, we demonstrated that the proposed localization module can decrease the influence of normal t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.09518</link><description>&lt;p&gt;
&#20154;&#31867;&#35838;&#31243;&#25351;&#23548;&#19979;&#30340;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning with Human Curriculum. (arXiv:2310.09518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#30340;&#20027;&#27969;&#33539;&#24335;&#26159;&#38543;&#26426;&#27927;&#29260;&#35757;&#32451;&#26368;&#22823;&#22810;&#26679;&#21270;&#25351;&#20196;-&#21709;&#24212;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#20013;&#24212;&#29992;&#32467;&#26500;&#21270;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#19982;&#20197;&#24448;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#25351;&#20196;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#24230;&#32467;&#26500;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#27169;&#25311;&#20102;&#20154;&#31867;&#25945;&#32946;&#30340;&#28176;&#36827;&#24615;&#21644;&#26377;&#32452;&#32455;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25968;&#25454;&#38598;&#19982;&#25945;&#32946;&#26694;&#26550;&#23545;&#40784;&#26469;&#31574;&#21010;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#21253;&#25324;&#20027;&#39064;&#21644;&#35748;&#30693;&#20005;&#35880;&#31243;&#24230;&#31561;&#20803;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#20174;&#20013;&#23398;&#21040;&#30740;&#31350;&#29983;&#38454;&#27573;&#30340;&#20840;&#38754;&#32454;&#31890;&#24230;&#20027;&#39064;&#65292;&#27599;&#20010;&#20027;&#39064;&#37117;&#26377;&#21508;&#31181;&#38382;&#39064;&#65292;&#20197;&#21033;&#29992;&#24067;&#40065;&#22982;&#30340;&#35748;&#30693;&#20998;&#32423;&#27861;&#25552;&#39640;&#27010;&#24565;&#28145;&#24230;&#65292;&#35813;&#20998;&#32423;&#27861;&#29992;&#20110;&#21306;&#20998;&#27599;&#20010;&#27010;&#24565;&#30340;&#19981;&#21516;&#20154;&#31867;&#35748;&#30693;&#27700;&#24179;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35748;&#30693;&#23398;&#20064;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#38543;&#26426;&#21270;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm for instruction tuning is the random-shuffled training of maximally diverse instruction-response pairs. This paper explores the potential benefits of applying a structured cognitive learning approach to instruction tuning in contemporary large language models like ChatGPT and GPT-4. Unlike the previous conventional randomized instruction dataset, we propose a highly structured synthetic dataset that mimics the progressive and organized nature of human education. We curate our dataset by aligning it with educational frameworks, incorporating meta information including its topic and cognitive rigor level for each sample. Our dataset covers comprehensive fine-grained topics spanning diverse educational stages (from middle school to graduate school) with various questions for each topic to enhance conceptual depth using Bloom's taxonomy-a classification framework distinguishing various levels of human cognition for each concept. The results demonstrate that this cogni
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.08795</link><description>&lt;p&gt;
&#36890;&#36807;&#36861;&#36394;&#20559;&#35265;&#24433;&#21709;&#26469;&#20943;&#36731;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias for Question Answering Models by Tracking Bias Influence. (arXiv:2310.08795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#65292;&#27979;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#12290;&#21516;&#26102;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#27169;&#22411;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#65292;&#32780;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#23588;&#20854;&#26377;&#23475;&#65292;&#22240;&#20026;&#36755;&#20986;&#30340;&#31572;&#26696;&#21487;&#33021;&#30452;&#25509;&#34987;&#26368;&#32456;&#29992;&#25143;&#20351;&#29992;&#12290;&#24050;&#32463;&#26377;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;QA&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20294;&#26159;&#23545;&#20110;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#20173;&#22788;&#20110;&#25506;&#32034;&#38454;&#27573;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BMBI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22810;&#36873;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#22522;&#20110;&#19968;&#20010;&#30452;&#35273;&#65292;&#21363;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#20174;&#19968;&#20010;&#26377;&#20559;&#35265;&#30340;&#20363;&#23376;&#20013;&#23398;&#21040;&#20102;&#19996;&#35199;&#65292;&#23427;&#21487;&#33021;&#26356;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#25105;&#20204;&#36890;&#36807;&#35266;&#23519;&#19968;&#20010;&#26597;&#35810;&#23454;&#20363;&#23545;&#21478;&#19968;&#20010;&#23454;&#20363;&#30340;&#24433;&#21709;&#26469;&#34913;&#37327;&#26597;&#35810;&#23454;&#20363;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;&#22914;&#26524;&#21463;&#21040;&#24433;&#21709;&#30340;&#23454;&#20363;&#26356;&#20559;&#35265;&#65292;&#25105;&#20204;&#35748;&#20026;&#26597;&#35810;&#23454;&#20363;&#26159;&#26377;&#20559;&#35265;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;&#26816;&#27979;&#21040;&#30340;&#20559;&#35265;&#31243;&#24230;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#65292;&#24418;&#25104;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#38500;&#20102;&#21407;&#26469;&#30340;QA&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#20840;&#38754;&#32780;&#25935;&#24863;&#30340;&#26041;&#24335;&#37327;&#21270;&#20559;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20943;&#36731;QA&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models of various NLP tasks have been shown to exhibit stereotypes, and the bias in the question answering (QA) models is especially harmful as the output answers might be directly consumed by the end users. There have been datasets to evaluate bias in QA models, while bias mitigation technique for the QA models is still under-explored. In this work, we propose BMBI, an approach to mitigate the bias of multiple-choice QA models. Based on the intuition that a model would lean to be more biased if it learns from a biased example, we measure the bias level of a query instance by observing its influence on another instance. If the influenced instance is more biased, we derive that the query instance is biased. We then use the bias level detected as an optimization objective to form a multi-task learning setting in addition to the original QA task. We further introduce a new bias evaluation metric to quantify bias in a comprehensive and sensitive way. We show that our method could be applie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2310.07891</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#19968;&#27425;&#26799;&#24230;&#19979;&#38477;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. (arXiv:2310.07891v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07891
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#19981;&#21516;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#23398;&#20064;&#21040;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#32452;&#20214;&#65292;&#32780;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#21017;&#30001;&#36825;&#20123;&#29305;&#24449;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25104;&#21151;&#30340;&#22522;&#26412;&#21407;&#22240;&#20043;&#19968;&#12290;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#24050;&#32463;&#20005;&#26684;&#35777;&#26126;&#65292;&#22312;&#20004;&#23618;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#31532;&#19968;&#23618;&#36827;&#34892;&#19968;&#27493;&#26799;&#24230;&#19979;&#38477;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#23618;&#36827;&#34892;&#23725;&#22238;&#24402;&#21487;&#20197;&#23548;&#33268;&#29305;&#24449;&#23398;&#20064;&#65307;&#29305;&#24449;&#30697;&#38453;&#30340;&#35889;&#20013;&#20250;&#20986;&#29616;&#20998;&#31163;&#30340;&#19968;&#32500;&#32452;&#20214;&#65292;&#31216;&#20026;&#8220;spike&#8221;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#22266;&#23450;&#26799;&#24230;&#19979;&#38477;&#27493;&#38271;&#26102;&#65292;&#36825;&#20010;&#8220;spike&#8221;&#20165;&#25552;&#20379;&#20102;&#30446;&#26631;&#20989;&#25968;&#30340;&#32447;&#24615;&#32452;&#20214;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#23398;&#20064;&#38750;&#32447;&#24615;&#32452;&#20214;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#23398;&#20064;&#29575;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#38271;&#26102;&#65292;&#36825;&#26679;&#30340;&#35757;&#32451;&#23454;&#38469;&#19978;&#24341;&#20837;&#20102;&#22810;&#20010;&#19968;&#32500;&#32452;&#20214;&#65292;&#27599;&#20010;&#32452;&#20214;&#23545;&#24212;&#19968;&#20010;&#29305;&#23450;&#30340;&#22810;&#39033;&#24335;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26356;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26497;&#38480;&#22823;&#32500;&#24230;&#21644;&#22823;&#26679;&#26412;&#35757;&#32451;&#21644;&#27979;&#35797;&#35823;&#24046;&#23436;&#20840;&#30001;&#36825;&#20123;&#8220;spike&#8221;&#25152;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature learning is thought to be one of the fundamental reasons for the success of deep neural networks. It is rigorously known that in two-layer fully-connected neural networks under certain conditions, one step of gradient descent on the first layer followed by ridge regression on the second layer can lead to feature learning; characterized by the appearance of a separated rank-one component -- spike -- in the spectrum of the feature matrix. However, with a constant gradient descent step size, this spike only carries information from the linear component of the target function and therefore learning non-linear components is impossible. We show that with a learning rate that grows with the sample size, such training in fact introduces multiple rank-one components, each corresponding to a specific polynomial feature. We further prove that the limiting large-dimensional and large sample training and test errors of the updated neural networks are fully characterized by these spikes. By 
&lt;/p&gt;</description></item><item><title>ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2310.07446</link><description>&lt;p&gt;
ProbTS&#65306;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32479;&#19968;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
ProbTS: A Unified Toolkit to Probe Deep Time-series Forecasting. (arXiv:2310.07446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07446
&lt;/p&gt;
&lt;p&gt;
ProbTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#21327;&#21516;&#21644;&#27604;&#36739;&#23450;&#21046;&#31070;&#32463;&#26550;&#26500;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#20248;&#21183;&#21644;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#36825;&#20010;&#39046;&#22495;&#20998;&#21270;&#25104;&#20102;&#20004;&#20010;&#26174;&#33879;&#30340;&#20998;&#25903;&#65306;&#19968;&#20010;&#19987;&#27880;&#20110;&#20026;&#26102;&#38388;&#24207;&#21015;&#23450;&#21046;&#29305;&#23450;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#21478;&#19968;&#20010;&#21033;&#29992;&#20808;&#36827;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#20998;&#25903;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#25968;&#25454;&#24773;&#26223;&#12289;&#26041;&#27861;&#35770;&#28966;&#28857;&#21644;&#35299;&#30721;&#26041;&#26696;&#19978;&#30340;&#24046;&#24322;&#25552;&#20986;&#20102;&#28145;&#20837;&#32780;&#26410;&#34987;&#25506;&#32034;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#40511;&#27807;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ProbTS&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#21327;&#21516;&#21644;&#27604;&#36739;&#36825;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#25903;&#12290;ProbTS&#20855;&#22791;&#32479;&#19968;&#30340;&#25968;&#25454;&#27169;&#22359;&#12289;&#27169;&#22359;&#21270;&#30340;&#27169;&#22411;&#27169;&#22359;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#22120;&#27169;&#22359;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#37325;&#26032;&#23457;&#35270;&#21644;&#22522;&#20934;&#27979;&#35797;&#36825;&#20004;&#20010;&#20998;&#25903;&#30340;&#39046;&#20808;&#26041;&#27861;&#12290;&#36890;&#36807;ProbTS&#30340;&#23457;&#26597;&#65292;&#31361;&#26174;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#29305;&#28857;&#12289;&#30456;&#23545;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20197;&#21450;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-series forecasting serves as a linchpin in a myriad of applications, spanning various domains. With the growth of deep learning, this arena has bifurcated into two salient branches: one focuses on crafting specific neural architectures tailored for time series, and the other harnesses advanced deep generative models for probabilistic forecasting. While both branches have made significant progress, their differences across data scenarios, methodological focuses, and decoding schemes pose profound, yet unexplored, research questions. To bridge this knowledge chasm, we introduce ProbTS, a pioneering toolkit developed to synergize and compare these two distinct branches. Endowed with a unified data module, a modularized model module, and a comprehensive evaluator module, ProbTS allows us to revisit and benchmark leading methods from both branches. The scrutiny with ProbTS highlights their distinct characteristics, relative strengths and weaknesses, and areas that need further explorat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2310.01651</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#30340;&#25490;&#21015;&#27450;&#39575;&#65288;&#35270;&#35273;&#21644;&#65289;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations. (arXiv:2310.01651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#23427;&#20204;&#23545;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#65292;&#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#38752;&#24615;&#20998;&#26512;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#21644;&#26368;&#26032;&#30340;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25351;&#31034;&#36981;&#24490;&#33021;&#21147;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#32780;&#36805;&#36895;&#22312;&#23454;&#36341;&#20013;&#37096;&#32626;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#65292;&#21363;&#20180;&#32454;&#20998;&#26512;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#20415;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#24212;&#29992;&#31243;&#24207;&#20013;&#26159;&#21542;&#36275;&#22815;&#21487;&#20449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#27969;&#34892;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#29305;&#23450;&#28431;&#27934;&#65292;&#21363;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#65288;MCQA&#65289;&#20013;&#30340;&#25490;&#21015;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#27969;&#34892;&#27169;&#22411;&#26131;&#21463;&#22810;&#39033;&#36873;&#25321;&#25552;&#31034;&#20013;&#31572;&#26696;&#38598;&#30340;&#23545;&#25239;&#24615;&#25490;&#21015;&#25915;&#20987;&#65292;&#36825;&#20196;&#20154;&#24778;&#35766;&#65292;&#22240;&#20026;&#27169;&#22411;&#29702;&#24819;&#19978;&#24212;&#35813;&#21644;&#20154;&#31867;&#19968;&#26679;&#23545;&#25552;&#31034;&#25490;&#21015;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#36825;&#20123;&#28431;&#27934;&#22312;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#19979;&#25345;&#32493;&#23384;&#22312;&#65292;&#24182;&#23384;&#22312;&#20110;&#26368;&#26032;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language and vision-language models are rapidly being deployed in practice thanks to their impressive capabilities in instruction following, in-context learning, and so on. This raises an urgent need to carefully analyse their robustness so that stakeholders can understand if and when such models are trustworthy enough to be relied upon in any given application. In this paper, we highlight a specific vulnerability in popular models, namely permutation sensitivity in multiple-choice question answering (MCQA). Specifically, we show empirically that popular models are vulnerable to adversarial permutation in answer sets for multiple-choice prompting, which is surprising as models should ideally be as invariant to prompt permutation as humans are. These vulnerabilities persist across various model sizes, and exist in very recent language and vision-language models. Code is available at \url{https://github.com/ys-zong/FoolyourVLLMs}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#27963;&#24615;&#29289;&#36136;&#31995;&#32479;&#29109;&#20135;&#29983;&#36895;&#29575;&#21644;&#27010;&#29575;&#27969;&#22823;&#23567;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#20013;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#30452;&#25509;&#35745;&#31639;&#36825;&#20123;&#29289;&#29702;&#37327;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.12991</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27963;&#24615;&#29289;&#36136;&#20013;&#30340;&#27010;&#29575;&#27969;&#21644;&#29109;&#20135;&#29983;&#36895;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep learning probability flows and entropy production rates in active matter. (arXiv:2309.12991v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20272;&#35745;&#27963;&#24615;&#29289;&#36136;&#31995;&#32479;&#29109;&#20135;&#29983;&#36895;&#29575;&#21644;&#27010;&#29575;&#27969;&#22823;&#23567;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35745;&#31639;&#26041;&#27861;&#20013;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#30340;&#38382;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#30452;&#25509;&#35745;&#31639;&#36825;&#20123;&#29289;&#29702;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#29289;&#36136;&#31995;&#32479;&#65292;&#20174;&#33258;&#39537;&#33014;&#20307;&#21040;&#36816;&#21160;&#30340;&#32454;&#33740;&#65292;&#20854;&#29305;&#28857;&#26159;&#23558;&#33258;&#30001;&#33021;&#36716;&#21270;&#20026;&#24494;&#35266;&#23610;&#24230;&#30340;&#26377;&#25928;&#24037;&#20316;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#28041;&#21450;&#21040;&#36229;&#20986;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#33539;&#30068;&#30340;&#29289;&#29702;&#23398;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#38750;&#24179;&#34913;&#24577;&#36136;&#24615;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29109;&#20135;&#29983;&#36895;&#29575;&#21644;&#31283;&#24577;&#27010;&#29575;&#27969;&#30340;&#22823;&#23567;&#25552;&#20379;&#20102;&#37327;&#21270;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#27979;&#37327;&#26102;&#38388;&#21453;&#28436;&#23545;&#31216;&#24615;&#30340;&#30772;&#32570;&#21644;&#38750;&#24179;&#34913;&#36755;&#36816;&#24378;&#24230;&#26469;&#29702;&#35299;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#39640;&#25928;&#35745;&#31639;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#31995;&#32479;&#30340;&#26410;&#30693;&#21644;&#39640;&#32500;&#27010;&#29575;&#23494;&#24230;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#23637;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#20272;&#35745;&#36825;&#31181;&#23494;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20998;&#25968;&#20197;&#21450;&#24494;&#35266;&#36816;&#21160;&#26041;&#31243;&#21487;&#20197;&#30452;&#25509;&#33719;&#21462;&#29109;&#20135;&#29983;&#36895;&#29575;&#65292;&#20854;&#27010;&#29575;&#27969;&#22823;&#23567;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active matter systems, from self-propelled colloids to motile bacteria, are characterized by the conversion of free energy into useful work at the microscopic scale. These systems generically involve physics beyond the reach of equilibrium statistical mechanics, and a persistent challenge has been to understand the nature of their nonequilibrium states. The entropy production rate and the magnitude of the steady-state probability current provide quantitative ways to do so by measuring the breakdown of time-reversal symmetry and the strength of nonequilibrium transport of measure. Yet, their efficient computation has remained elusive, as they depend on the system's unknown and high-dimensional probability density. Here, building upon recent advances in generative modeling, we develop a deep learning framework that estimates the score of this density. We show that the score, together with the microscopic equations of motion, gives direct access to the entropy production rate, the probabi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#65292;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.10083</link><description>&lt;p&gt;
&#19981;&#21464;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Invariant Probabilistic Prediction. (arXiv:2309.10083v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10083
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#65292;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#36890;&#36807;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#21644;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#65292;&#21487;&#20197;&#22312;&#29305;&#23450;&#27169;&#22411;&#20013;&#23454;&#29616;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#20998;&#24067;&#21464;&#21270;&#19979;&#34920;&#29616;&#31283;&#20581;&#30340;&#32479;&#35745;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#34429;&#28982;&#22823;&#37096;&#20998;&#30456;&#20851;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25439;&#22833;&#30340;&#28857;&#39044;&#27979;&#19978;&#65292;&#20294;&#26412;&#25991;&#23558;&#28966;&#28857;&#36716;&#21521;&#20102;&#27010;&#29575;&#39044;&#27979;&#65292;&#26088;&#22312;&#20840;&#38754;&#37327;&#21270;&#32473;&#23450;&#21327;&#21464;&#37327;&#30340;&#32467;&#26524;&#21464;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27010;&#29575;&#39044;&#27979;&#22312;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#19979;&#30340;&#19981;&#21464;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20219;&#24847;&#20998;&#24067;&#20559;&#31227;&#19968;&#33324;&#19981;&#20855;&#26377;&#19981;&#21464;&#21644;&#31283;&#20581;&#30340;&#27010;&#29575;&#39044;&#27979;&#65292;&#19982;&#28857;&#39044;&#27979;&#30340;&#24773;&#20917;&#30456;&#21453;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;&#35780;&#20272;&#25351;&#26631;&#24182;&#38480;&#21046;&#20998;&#24067;&#20559;&#31227;&#31867;&#21035;&#65292;&#20197;&#23454;&#29616;&#21407;&#22411;&#39640;&#26031;&#24322;&#26041;&#24046;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19981;&#21464;&#24615;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20135;&#29983;&#19981;&#21464;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21516;&#36136;&#31181;&#32676;&#30340;&#38543;&#26426;&#23454;&#39564;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#20195;&#29702;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23558;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#30340;&#26500;&#24314;&#36827;&#34892;&#20102;&#24402;&#32422;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#20102;&#38477;&#22122;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.07893</link><description>&lt;p&gt;
&#20174;&#36807;&#21435;&#30340;&#23454;&#39564;&#20013;&#36873;&#25321;&#20195;&#29702;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Choosing a Proxy Metric from Past Experiments. (arXiv:2309.07893v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21516;&#36136;&#31181;&#32676;&#30340;&#38543;&#26426;&#23454;&#39564;&#20013;&#36873;&#25321;&#26368;&#20248;&#30340;&#20195;&#29702;&#25351;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#23558;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#30340;&#26500;&#24314;&#36827;&#34892;&#20102;&#24402;&#32422;&#65292;&#24182;&#23545;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#20102;&#38477;&#22122;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#38543;&#26426;&#23454;&#39564;&#20013;&#65292;&#24448;&#24448;&#24456;&#38590;&#25110;&#19981;&#21487;&#34892;&#22320;&#27979;&#37327;&#38271;&#26399;&#25351;&#26631;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#20027;&#35201;&#32467;&#26524;&#65289;&#12290;&#36825;&#20123;&#38271;&#26399;&#25351;&#26631;&#24448;&#24448;&#21453;&#24212;&#21464;&#21270;&#36739;&#24930;&#65292;&#19988;&#22122;&#22768;&#36739;&#22823;&#65292;&#20351;&#24471;&#22312;&#30701;&#26399;&#23454;&#39564;&#20013;&#38590;&#20197;&#20934;&#30830;&#20272;&#35745;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26367;&#20195;&#26041;&#27861;&#26159;&#27979;&#37327;&#20960;&#20010;&#30701;&#26399;&#20195;&#29702;&#25351;&#26631;&#65292;&#24076;&#26395;&#23427;&#20204;&#33021;&#22815;&#32039;&#23494;&#36861;&#36394;&#38271;&#26399;&#25351;&#26631;&#65292;&#20174;&#32780;&#22312;&#36817;&#26399;&#26377;&#25928;&#22320;&#25351;&#23548;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26694;&#26550;&#65292;&#29992;&#20110;&#23450;&#20041;&#21644;&#26500;&#24314;&#19968;&#20010;&#36866;&#29992;&#20110;&#21516;&#36136;&#31181;&#32676;&#38543;&#26426;&#23454;&#39564;&#30340;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#23558;&#32473;&#23450;&#23454;&#39564;&#20013;&#26368;&#20248;&#20195;&#29702;&#25351;&#26631;&#30340;&#26500;&#24314;&#24402;&#32422;&#20026;&#19968;&#20010;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21462;&#20915;&#20110;&#32771;&#34385;&#20013;&#23454;&#39564;&#30340;&#30495;&#23454;&#28508;&#22312;&#27835;&#30103;&#25928;&#26524;&#21644;&#22122;&#22768;&#27700;&#24179;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#38271;&#26399;&#25351;&#26631;&#21644;&#19968;&#32452;&#20195;&#29702;&#30340;&#35266;&#23519;&#21040;&#30340;&#27835;&#30103;&#25928;&#26524;&#36827;&#34892;&#38477;&#22122;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric -- so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#20102;&#31995;&#32479;&#35782;&#21035;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#38750;&#28176;&#36817;&#26041;&#27861;&#12289;&#24378;&#35843;&#20102;&#35206;&#30422;&#25216;&#26415;&#12289;Hanson-Wright&#19981;&#31561;&#24335;&#21644;&#33258;&#26631;&#20934;&#21270;&#39532;&#19969;&#26684;&#23572;&#27861;&#31561;&#24037;&#20855;&#30340;&#24212;&#29992;&#12289;&#24182;&#32473;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#31616;&#21270;&#35777;&#26126;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#21442;&#25968;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12289;&#26368;&#21518;&#20171;&#32461;&#20102;&#23558;&#36825;&#20123;&#24605;&#24819;&#25193;&#23637;&#21040;&#26576;&#20123;&#38750;&#32447;&#24615;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.03873</link><description>&lt;p&gt;
&#31995;&#32479;&#35782;&#21035;&#30340;&#38750;&#28176;&#36817;&#29702;&#35770;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Non-Asymptotic Theory of System Identification. (arXiv:2309.03873v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#20102;&#31995;&#32479;&#35782;&#21035;&#20013;&#26368;&#36817;&#21457;&#23637;&#30340;&#38750;&#28176;&#36817;&#26041;&#27861;&#12289;&#24378;&#35843;&#20102;&#35206;&#30422;&#25216;&#26415;&#12289;Hanson-Wright&#19981;&#31561;&#24335;&#21644;&#33258;&#26631;&#20934;&#21270;&#39532;&#19969;&#26684;&#23572;&#27861;&#31561;&#24037;&#20855;&#30340;&#24212;&#29992;&#12289;&#24182;&#32473;&#20986;&#20102;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#31616;&#21270;&#35777;&#26126;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#21442;&#25968;&#35782;&#21035;&#20013;&#30340;&#24615;&#33021;&#12289;&#26368;&#21518;&#20171;&#32461;&#20102;&#23558;&#36825;&#20123;&#24605;&#24819;&#25193;&#23637;&#21040;&#26576;&#20123;&#38750;&#32447;&#24615;&#35782;&#21035;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25945;&#31243;&#20171;&#32461;&#26368;&#36817;&#21457;&#23637;&#30340;&#38750;&#28176;&#36817;&#26041;&#27861;&#22312;&#20027;&#35201;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#29702;&#35770;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24378;&#35843;&#19968;&#20123;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#29305;&#21035;&#26377;&#29992;&#30340;&#24037;&#20855;&#65292;&#22914;&#35206;&#30422;&#25216;&#26415;&#12289;Hanson-Wright&#19981;&#31561;&#24335;&#21644;&#33258;&#26631;&#20934;&#21270;&#39532;&#19969;&#26684;&#23572;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#24037;&#20855;&#26469;&#32473;&#20986;&#19968;&#20123;&#22522;&#20110;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#30340;&#31616;&#21270;&#35777;&#26126;&#65292;&#29992;&#20110;&#35782;&#21035;&#33258;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#21442;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#22914;&#20309;&#23558;&#25152;&#21576;&#29616;&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#26576;&#20123;&#38750;&#32447;&#24615;&#35782;&#21035;&#38382;&#39064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This tutorial serves as an introduction to recently developed non-asymptotic methods in the theory of -- mainly linear -- system identification. We emphasize tools we deem particularly useful for a range of problems in this domain, such as the covering technique, the Hanson-Wright Inequality and the method of self-normalized martingales. We then employ these tools to give streamlined proofs of the performance of various least-squares based estimators for identifying the parameters in autoregressive models. We conclude by sketching out how the ideas presented herein can be extended to certain nonlinear identification problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.00079</link><description>&lt;p&gt;
&#20851;&#20110;Adam&#30340;&#38544;&#24335;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#23384;&#22312;&#38544;&#24335;&#35268;&#33539;&#21270;&#20316;&#29992;&#65292;&#20854;&#21462;&#20915;&#20110;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#23545;&#27867;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#21518;&#21521;&#35823;&#24046;&#20998;&#26512;&#34987;&#29992;&#26469;&#25214;&#21040;&#36817;&#20284;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#12290;&#21457;&#29616;&#26377;&#38480;&#27493;&#38271;&#20250;&#38544;&#24335;&#22320;&#35268;&#33539;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#20986;&#29616;&#22312;ODE&#20013;&#30340;&#39033;&#20250;&#24809;&#32602;&#25439;&#22833;&#26799;&#24230;&#30340;&#20108;&#33539;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RMSProp&#21644;Adam&#20013;&#26159;&#21542;&#23384;&#22312;&#31867;&#20284;&#30340;&#38544;&#24335;&#35268;&#33539;&#21270;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#21644;&#35757;&#32451;&#38454;&#27573;&#65292;&#20294;&#28041;&#21450;&#30340;&#8220;&#33539;&#25968;&#8221;&#19981;&#21516;&#65306;&#23545;&#24212;&#30340;ODE&#39033;&#35201;&#20040;&#24809;&#32602;&#65288;&#25200;&#21160;&#30340;&#65289;&#25439;&#22833;&#26799;&#24230;&#30340;&#19968;&#33539;&#25968;&#65292;&#35201;&#20040;&#30456;&#21453;&#22320;&#38459;&#27490;&#20854;&#20943;&#23567;&#65288;&#21518;&#19968;&#31181;&#24773;&#20917;&#26159;&#20856;&#22411;&#30340;&#65289;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#20123;&#35777;&#26126;&#20107;&#23454;&#22914;&#20309;&#24433;&#21709;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
&lt;/p&gt;</description></item><item><title>ULDP-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#29992;&#20110;&#36328;&#36793;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#23494;&#30721;&#23398;&#26500;&#20214;&#22686;&#24378;&#20102;&#20854;&#25928;&#29992;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12210</link><description>&lt;p&gt;
ULDP-FL:&#20855;&#26377;&#36328;&#36793;&#30028;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ULDP-FL: Federated Learning with Across Silo User-Level Differential Privacy. (arXiv:2308.12210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12210
&lt;/p&gt;
&lt;p&gt;
ULDP-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#35774;&#35745;&#29992;&#20110;&#36328;&#36793;&#30028;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#12290;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;&#24046;&#20998;&#38544;&#31169;&#65292;&#24182;&#36890;&#36807;&#23494;&#30721;&#23398;&#26500;&#20214;&#22686;&#24378;&#20102;&#20854;&#25928;&#29992;&#12290;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;&#65288;DP-FL&#65289;&#20316;&#20026;&#19968;&#31181;&#30830;&#20445;&#24418;&#24335;&#38544;&#31169;&#30340;&#21327;&#21516;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;DP-FL&#26041;&#27861;&#30830;&#20445;&#22312;&#27599;&#20010;&#36793;&#30028;&#20869;&#20197;&#35760;&#24405;&#32423;&#21035;&#30340;DP&#36827;&#34892;&#36328;&#36793;&#30028;FL&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#21487;&#33021;&#24310;&#20280;&#21040;&#22810;&#20010;&#36793;&#30028;&#65292;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26399;&#26395;&#29992;&#25143;&#32423;DP&#20445;&#35777;&#20173;&#28982;&#26410;&#30693;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ULDP-FL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;FL&#26694;&#26550;&#65292;&#26088;&#22312;&#22312;&#21333;&#20010;&#29992;&#25143;&#30340;&#25968;&#25454;&#21487;&#33021;&#23646;&#20110;&#22810;&#20010;&#36793;&#30028;&#30340;&#36328;&#36793;&#30028;FL&#20013;&#20445;&#35777;&#29992;&#25143;&#32423;DP&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#27599;&#20010;&#29992;&#25143;&#30340;&#21152;&#26435;&#21098;&#35009;&#30452;&#25509;&#30830;&#20445;&#29992;&#25143;&#32423;DP&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#32452;&#38544;&#31169;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#23494;&#30721;&#23398;&#26500;&#20214;&#26469;&#22686;&#24378;&#31639;&#27861;&#30340;&#25928;&#29992;&#24182;&#23637;&#31034;&#20102;&#20854;&#31169;&#23494;&#23454;&#29616;&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38544;&#31169;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Federated Learning (DP-FL) has garnered attention as a collaborative machine learning approach that ensures formal privacy. Most DP-FL approaches ensure DP at the record-level within each silo for cross-silo FL. However, a single user's data may extend across multiple silos, and the desired user-level DP guarantee for such a setting remains unknown. In this study, we present ULDP-FL, a novel FL framework designed to guarantee user-level DP in cross-silo FL where a single user's data may belong to multiple silos. Our proposed algorithm directly ensures user-level DP through per-user weighted clipping, departing from group-privacy approaches. We provide a theoretical analysis of the algorithm's privacy and utility. Additionally, we enhance the algorithm's utility and showcase its private implementation using cryptographic building blocks. Empirical experiments on real-world datasets show substantial improvements in our methods in privacy-utility trade-offs under us
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.11804</link><description>&lt;p&gt;
&#36825;&#19981;&#26159;&#19968;&#20010;&#33529;&#26524;&#65306;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11804
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#20013;&#30340;&#23545;&#25239;&#24187;&#35273;&#38382;&#39064;&#12290;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#36755;&#20837;&#30340;&#20219;&#24847;&#27169;&#24577;&#65292;&#20351;&#20854;&#23884;&#20837;&#19982;&#20854;&#20182;&#27169;&#24577;&#30340;&#20219;&#24847;&#36755;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#23454;&#29616;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#30340;&#23545;&#40784;&#12290;&#35813;&#38382;&#39064;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#65292;&#23545;&#29983;&#25104;&#21644;&#20998;&#31867;&#20219;&#21153;&#20250;&#20135;&#29983;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#23558;&#22270;&#20687;&#12289;&#22768;&#38899;&#12289;&#25991;&#26412;&#12289;&#35270;&#39057;&#31561;&#26144;&#23556;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23545;&#40784;&#19981;&#21516;&#27169;&#24577;&#30340;&#34920;&#31034;&#65288;&#20363;&#22914;&#23558;&#19968;&#24352;&#29399;&#30340;&#22270;&#20687;&#19982;&#19968;&#31181;&#21483;&#22768;&#30456;&#20851;&#32852;&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#21487;&#20197;&#21463;&#21040;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#23545;&#25239;&#24187;&#35273;&#8221;&#30340;&#25915;&#20987;&#12290;&#32473;&#23450;&#20219;&#24847;&#27169;&#24577;&#30340;&#36755;&#20837;&#65292;&#23545;&#25163;&#21487;&#20197;&#25200;&#21160;&#23427;&#65292;&#20351;&#20854;&#23884;&#20837;&#25509;&#36817;&#20110;&#21478;&#19968;&#27169;&#24577;&#20013;&#20219;&#24847;&#23545;&#25163;&#36873;&#25321;&#30340;&#36755;&#20837;&#30340;&#23884;&#20837;&#12290;&#24187;&#35273;&#20351;&#23545;&#25163;&#33021;&#22815;&#23558;&#20219;&#24847;&#22270;&#20687;&#19982;&#20219;&#24847;&#25991;&#26412;&#12289;&#20219;&#24847;&#25991;&#26412;&#19982;&#20219;&#24847;&#22768;&#38899;&#31561;&#36827;&#34892;&#23545;&#40784;&#12290;&#23545;&#25239;&#24187;&#35273;&#21033;&#29992;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#25509;&#36817;&#24615;&#65292;&#22240;&#27492;&#19982;&#19979;&#28216;&#20219;&#21153;&#26080;&#20851;&#12290;&#20351;&#29992;ImageBind&#23884;&#20837;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#27809;&#26377;&#20855;&#20307;&#19979;&#28216;&#20219;&#21153;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23545;&#40784;&#30340;&#36755;&#20837;&#22914;&#20309;&#35823;&#23548;&#22270;&#20687;&#29983;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#38646;&#26679;&#20363;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20855;&#26377;&#26368;&#36817;&#37051;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#19979;&#25910;&#25947;&#36895;&#24230;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01490</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#36817;&#37051;&#30340;&#26497;&#23567;&#26497;&#22823;&#26368;&#20248;$Q$&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#20855;&#26377;&#26368;&#36817;&#37051;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#19979;&#25910;&#25947;&#36895;&#24230;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
$Q$&#23398;&#20064;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20998;&#26512;&#26377;&#38480;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;$Q$&#23398;&#20064;&#12290;&#22914;&#26524;&#29366;&#24577;&#31354;&#38388;&#26159;&#36830;&#32493;&#30340;&#65292;&#37027;&#20040;&#21407;&#22987;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#23601;&#26080;&#27861;&#30452;&#25509;&#20351;&#29992;&#12290;(Shah and Xie, 2018) &#25552;&#20986;&#20102;&#21407;&#22987;$Q$&#23398;&#20064;&#26041;&#27861;&#30340;&#20462;&#25913;&#29256;&#65292;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#20272;&#35745;$Q$&#20540;&#12290;&#36825;&#31181;&#20462;&#25913;&#20351;&#24471;$Q$&#23398;&#20064;&#36866;&#29992;&#20110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#35813;&#35770;&#25991;&#25351;&#20986;&#20272;&#35745;$Q$&#20989;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#20026;$\tilde{O}(T^{-1/(d+3)})$&#65292;&#27604;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;$\tilde{\Omega}(T^{-1/(d+2)})$&#24930;&#65292;&#35828;&#26126;&#35813;&#26041;&#27861;&#25928;&#29575;&#19981;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;$Q$&#23398;&#20064;&#26041;&#27861;&#65292;&#26469;&#24357;&#21512;(Shah and Xie, 2018)&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#36317;&#65292;&#20854;&#20013;&#19968;&#31181;&#26159;&#31163;&#32447;&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22312;&#32447;&#30340;&#12290;&#23613;&#31649;&#25105;&#20204;&#20173;&#28982;&#20351;&#29992;&#26368;&#36817;&#37051;&#26041;&#27861;&#26469;&#20272;&#35745;$Q$&#20989;&#25968;&#65292;&#20294;&#31639;&#27861;&#19982;(Shah and Xie, 2018)&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
$Q$ learning is a popular model free reinforcement learning method. Most of existing works focus on analyzing $Q$ learning for finite state and action spaces. If the state space is continuous, then the original $Q$ learning method can not be directly used. A modification of the original $Q$ learning method was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest neighbors. Such modification makes $Q$ learning suitable for continuous state space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$ function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not efficient. This paper proposes two new $Q$ learning methods to bridge the gap of convergence rates in (Shah and Xie, 2018), with one of them being offline, while the other is online. Despite that we still use nearest neighbor approach to estimate $Q$ function, the algorithms are crucially different from (Sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.06092</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#23450;&#37327;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#20248;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#39640;&#26031;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#24067;&#65292;&#20854;&#20013;&#38544;&#34255;&#23618;&#23485;&#24230;&#19982;&#22823;&#24120;&#25968; $n$ &#25104;&#27604;&#20363;&#12290;&#22312;&#38750;&#32447;&#24615;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#22312;&#22823;&#20294;&#26377;&#38480;&#30340; $n$ &#21644;&#20219;&#24847;&#22266;&#23450;&#32593;&#32476;&#28145;&#24230;&#19979;&#25104;&#31435;&#30340;&#27491;&#24577;&#36924;&#36817;&#30340;&#23450;&#37327;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#26377;&#38480;&#32500;&#20998;&#24067;&#36824;&#26159;&#25972;&#20010;&#36807;&#31243;&#65292;&#38543;&#26426;&#20840;&#36830;&#25509;&#32593;&#32476;&#65288;&#21450;&#20854;&#23548;&#25968;&#65289;&#19982;&#30456;&#24212;&#30340;&#26080;&#38480;&#23485;&#39640;&#26031;&#36807;&#31243;&#20043;&#38388;&#30340;&#36317;&#31163;&#37117;&#20250;&#25353;&#29031; $n^{-\gamma}$ &#32553;&#25918;&#65292;&#20854;&#20013; $\gamma&gt;0$&#65292;&#25351;&#25968;&#21462;&#20915;&#20110;&#29992;&#20110;&#24230;&#37327;&#24046;&#24322;&#30340;&#24230;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#22312;&#32593;&#32476;&#23485;&#24230;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#27604;&#25991;&#29486;&#20013;&#20197;&#21069;&#25552;&#20379;&#30340;&#20219;&#20309;&#30028;&#38480;&#37117;&#35201;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma&gt;0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05213</link><description>&lt;p&gt;
&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#21442;&#25968;&#20998;&#24067;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20197;&#25193;&#22823;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20248;&#21270;&#38382;&#39064;&#37117;&#21253;&#21547;&#38656;&#35201;&#22312;&#35299;&#20915;&#20043;&#21069;&#36827;&#34892;&#39044;&#27979;&#30340;&#26410;&#30693;&#21442;&#25968;&#12290;&#20026;&#20102;&#35757;&#32451;&#28041;&#21450;&#30340;&#39044;&#27979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#65292;&#36890;&#24120;&#37319;&#29992;&#30340;&#26041;&#27861;&#26159;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#12290;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;&#24335;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#30452;&#25509;&#26368;&#23567;&#21270;&#20219;&#21153;&#25439;&#22833;&#26469;&#35757;&#32451;ML&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;DFL&#26041;&#27861;&#21463;&#21040;&#23427;&#20204;&#23545;&#20248;&#21270;&#38382;&#39064;&#32467;&#26500;&#30340;&#20551;&#35774;&#65288;&#20363;&#22914;&#65292;&#38382;&#39064;&#26159;&#32447;&#24615;&#30340;&#65289;&#20197;&#21450;&#21482;&#33021;&#39044;&#27979;&#20986;&#29616;&#22312;&#30446;&#26631;&#20989;&#25968;&#20013;&#30340;&#21442;&#25968;&#30340;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30456;&#21453;&#22320;&#39044;&#27979;&#21442;&#25968;&#30340;&#20998;&#24067;&#65292;&#24182;&#37319;&#29992;&#35780;&#20998;&#20989;&#25968;&#26799;&#24230;&#20272;&#35745;&#65288;SFGE&#65289;&#26469;&#35745;&#31639;&#20915;&#31574;&#28966;&#28857;&#27169;&#22411;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#25193;&#22823;DFL&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
Many real-world optimization problems contain unknown parameters that must be predicted prior to solving. To train the predictive machine learning (ML) models involved, the commonly adopted approach focuses on maximizing predictive accuracy. However, this approach does not always lead to the minimization of the downstream task loss. Decision-focused learning (DFL) is a recently proposed paradigm whose goal is to train the ML model by directly minimizing the task loss. However, state-of-the-art DFL methods are limited by the assumptions they make about the structure of the optimization problem (e.g., that the problem is linear) and by the fact that can only predict parameters that appear in the objective function. In this work, we address these limitations by instead predicting \textit{distributions} over parameters and adopting score function gradient estimation (SFGE) to compute decision-focused updates to the predictive model, thereby widening the applicability of DFL. Our experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#40065;&#26834;&#24615;&#25945;&#24072;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.16170</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#25945;&#24072;&#23545;&#25239;&#33976;&#39311;&#20943;&#36731;&#20934;&#30830;&#24615;&#19982;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#40065;&#26834;&#24615;&#25945;&#24072;&#26469;&#25913;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#19968;&#31181;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#25915;&#20987;&#40065;&#26834;&#24615;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#34429;&#28982;&#26377;&#25928;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#24615;&#33021;&#21364;&#26377;&#25152;&#19979;&#38477;&#65292;&#36825;&#24847;&#21619;&#30528;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26435;&#34913;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#25552;&#39640;&#40065;&#26834;&#24615;&#30340;&#31454;&#20105;&#24615;&#24615;&#33021;&#65292;&#20294;&#24182;&#27809;&#26377;&#26174;&#33879;&#25913;&#21892;&#23545;&#24178;&#20928;&#26679;&#26412;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#20943;&#36731;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#22810;&#25945;&#24072;&#23545;&#25239;&#40065;&#26834;&#24615;&#33976;&#39311;&#65288;MTARD&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#24378;&#22823;&#30340;&#24178;&#20928;&#26679;&#26412;&#25945;&#24072;&#21644;&#24378;&#22823;&#30340;&#40065;&#26834;&#26679;&#26412;&#25945;&#24072;&#26469;&#25351;&#23548;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#25945;&#24072;&#26174;&#31034;&#30456;&#20284;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#29109;&#30340;&#24179;&#34913;&#31639;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a practical approach for improving the robustness of deep neural networks against adversarial attacks. Although bringing reliable robustness, the performance toward clean examples is negatively affected after adversarial training, which means a trade-off exists between accuracy and robustness. Recently, some studies have tried to use knowledge distillation methods in adversarial training, achieving competitive performance in improving the robustness but the accuracy for clean samples is still limited. In this paper, to mitigate the accuracy-robustness trade-off, we introduce the Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's adversarial training process by applying a strong clean teacher and a strong robust teacher to handle the clean examples and adversarial examples, respectively. During the optimization process, to ensure that different teachers show similar knowledge scales, we design the Entropy-Based Balance algorithm to adj
&lt;/p&gt;</description></item><item><title>&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.08141</link><description>&lt;p&gt;
ArtWhisperer&#65306;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#33402;&#26415;&#21019;&#20316;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in Artistic Creations. (arXiv:2306.08141v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08141
&lt;/p&gt;
&lt;p&gt;
&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#65292;&#30740;&#31350;&#32773;&#21019;&#24314;&#20102;ArtWhisperer&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#20154;&#20204;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#26469;&#29983;&#25104;&#21644;&#30446;&#26631;&#22270;&#20687;&#31867;&#20284;&#30340;&#22270;&#20687;&#65292;&#24182;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20132;&#20114;&#35760;&#24405;&#12290;&#22312;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#29992;&#25143;&#22914;&#20309;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#20204;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#27169;&#22411;&#29983;&#25104;&#25152;&#38656;&#30340;&#30446;&#26631;&#22270;&#20687;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;ArtWhisperer&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#32447;&#28216;&#25103;&#65292;&#29992;&#25143;&#20250;&#24471;&#21040;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#38656;&#35201;&#21453;&#22797;&#23581;&#35797;&#19981;&#21516;&#30340;&#25552;&#31034;&#35789;&#65292;&#20197;&#20415;&#29983;&#25104;&#31867;&#20284;&#30446;&#26631;&#22270;&#20687;&#30340;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#20010;&#28216;&#25103;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;50,000&#22810;&#20010;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20132;&#20114;&#30340;&#35760;&#24405;&#65307;&#27599;&#20010;&#20132;&#20114;&#37117;&#23545;&#24212;&#30528;&#29992;&#25143;&#21019;&#24314;&#30340;&#19968;&#20010;&#25552;&#31034;&#35789;&#21644;&#30456;&#24212;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#22823;&#22810;&#25968;&#35760;&#24405;&#37117;&#26159;&#37325;&#22797;&#30340;&#20132;&#20114;&#65292;&#29992;&#25143;&#36890;&#36807;&#21453;&#22797;&#23581;&#35797;&#25214;&#21040;&#26368;&#20339;&#30340;&#25552;&#31034;&#35789;&#20197;&#29983;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#20351;&#24471;&#36825;&#20010;&#25968;&#25454;&#38598;&#25104;&#20026;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#29420;&#29305;&#36830;&#32493;&#25968;&#25454;&#38598;&#12290;&#22312;&#23545;&#36825;&#20010;&#25968;&#25454;&#38598;&#30340;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#25552;&#31034;&#35789;&#20132;&#20114;&#21644;&#29992;&#25143;&#31574;&#30053;&#30340;&#29305;&#24449;&#12290;&#20154;&#20204;&#25552;&#20132;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#25552;&#31034;&#35789;&#65292;&#24182;&#33021;&#22815;&#21457;&#29616;&#29983;&#25104;&#21508;&#31181;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As generative AI becomes more prevalent, it is important to study how human users interact with such models. In this work, we investigate how people use text-to-image models to generate desired target images. To study this interaction, we created ArtWhisperer, an online game where users are given a target image and are tasked with iteratively finding a prompt that creates a similar-looking image as the target. Through this game, we recorded over 50,000 human-AI interactions; each interaction corresponds to one text prompt created by a user and the corresponding generated image. The majority of these are repeated interactions where a user iterates to find the best prompt for their target image, making this a unique sequential dataset for studying human-AI collaborations. In an initial analysis of this dataset, we identify several characteristics of prompt interactions and user strategies. People submit diverse prompts and are able to discover a variety of text descriptions that generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;SGD&#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#38408;&#20540;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;&#20854;&#19982;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#21576;&#21333;&#35843;&#38750;&#38477;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21487;&#33021;&#20250;&#24433;&#21709;SGD&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07850</link><description>&lt;p&gt;
SGD&#30340;&#31934;&#30830;&#24179;&#22343;&#20108;&#27425;&#32447;&#24615;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exact Mean Square Linear Stability Analysis for SGD. (arXiv:2306.07850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;SGD&#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#38408;&#20540;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;&#20854;&#19982;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#21576;&#21333;&#35843;&#38750;&#38477;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21487;&#33021;&#20250;&#24433;&#21709;SGD&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#20248;&#21270;&#26041;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#26497;&#23567;&#20540;&#28857;&#38468;&#36817;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;GD&#65289;&#65292;&#31283;&#23450;&#30340;&#25910;&#25947;&#20165;&#21487;&#33021;&#21457;&#29983;&#22312;&#36275;&#22815;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#22788;&#65292;&#24182;&#19988;&#24050;&#32463;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#33391;&#22909;&#24615;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;GD&#30340;&#31283;&#23450;&#24615;&#38408;&#20540;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23578;&#26410;&#25512;&#23548;&#20986;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#31934;&#30830;&#38408;&#20540;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#23553;&#38381;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#27493;&#38271;$\eta$&#30340;&#26174;&#24335;&#26465;&#20214;&#65292;&#26082;&#26159;SGD&#22312;&#22343;&#26041;&#24847;&#20041;&#19979;&#31283;&#23450;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25209;&#37327;&#22823;&#23567;$B$&#30340;&#31934;&#30830;&#20316;&#29992;&#65292;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38408;&#20540;&#26159;&#25209;&#37327;&#22823;&#23567;&#30340;&#21333;&#35843;&#38750;&#38477;&#20989;&#25968;&#65292;&#36825;&#24847;&#21619;&#30528;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21482;&#20250;&#38477;&#20302;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.05567</link><description>&lt;p&gt;
&#26234;&#33021;&#20998;&#26512;&#65292;&#22312;&#29289;&#32852;&#32593;&#26694;&#26550;&#19979;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#65306;&#22797;&#26434;&#32593;&#32476;&#21644;&#31995;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems. (arXiv:2306.05567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26080;&#32447;&#20256;&#24863;&#31995;&#32479;&#26469;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#20010;&#29289;&#32852;&#32593;&#26550;&#26500;&#21644;&#26694;&#26550;&#30340;&#32452;&#20214;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26234;&#33021;&#20998;&#26512;&#65292;&#19981;&#20165;&#25910;&#38598;&#21644;&#23384;&#20648;&#20449;&#24687;&#65292;&#32780;&#19988;&#36824;&#26159;&#20854;&#20182;&#20225;&#19994;&#24320;&#21457;&#24212;&#29992;&#30340;&#24179;&#21488;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;&#33021;&#28304;&#36164;&#28304;&#30340;&#28040;&#32791;&#21644;&#38656;&#27714;&#22686;&#21152;&#23548;&#33268;&#20102;&#33410;&#33021;&#19982;&#20248;&#21270;&#31649;&#29702;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings are increasingly using Internet of Things (IoT)-based wireless sensing systems to reduce their energy consumption and environmental impact. As a result of their compact size and ability to sense, measure, and compute all electrical properties, Internet of Things devices have become increasingly important in our society. A major contribution of this study is the development of a comprehensive IoT-based framework for smart city energy management, incorporating multiple components of IoT architecture and framework. An IoT framework for intelligent energy management applications that employ intelligent analysis is an essential system component that collects and stores information. Additionally, it serves as a platform for the development of applications by other companies. Furthermore, we have studied intelligent energy management solutions based on intelligent mechanisms. The depletion of energy resources and the increase in energy demand have led to an increase in energy 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2306.04817</link><description>&lt;p&gt;
SiBBlInGS: &#20351;&#29992;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#27169;&#22359;&#25512;&#29702;&#30340;&#24314;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#29366;&#24577;&#30340;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#24046;&#24322;&#12290;&#23427;&#21487;&#20197;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#32500;&#26102;&#38388;&#24207;&#21015;&#26469;&#35828;&#65292;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#27169;&#22359;&#26159;&#21457;&#29616;&#22797;&#26434;&#31995;&#32479;&#20013;&#26377;&#20215;&#20540;&#35265;&#35299;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30456;&#20284;&#24615;&#39537;&#21160;&#30340;&#27169;&#22359;&#25512;&#29702;&#26694;&#26550;(SiBBlInGS)&#65292;&#29992;&#20110;&#21457;&#29616;&#27169;&#22359;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#25968;&#25454;&#20013;&#30340;&#29366;&#24577;&#38388;&#21644;&#29366;&#24577;&#20869;&#20851;&#31995;&#65292;&#33021;&#22815;&#25552;&#21462;&#38750;&#27491;&#20132;&#32452;&#20214;&#65292;&#24182;&#20801;&#35768;&#29366;&#24577;&#20043;&#38388;&#30340;&#20250;&#35805;&#35745;&#25968;&#21644;&#25345;&#32493;&#26102;&#38388;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;SiBBlInGS&#36824;&#20801;&#35768;&#36328;&#29366;&#24577;&#21464;&#21270;&#27169;&#22359;&#32467;&#26500;&#21644;&#27599;&#27425;&#35797;&#39564;&#30340;&#26102;&#38388;&#21464;&#24322;&#65292;&#24182;&#21487;&#35782;&#21035;&#29305;&#23450;&#29366;&#24577;&#19982;&#29366;&#24577;&#38750;&#29305;&#23450;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#34394;&#25311;&#20581;&#24247;&#20013;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#21644;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03833</link><description>&lt;p&gt;
&#34394;&#25311;&#20581;&#24247;&#20013;&#30340;&#24739;&#32773;&#39044;&#27979;&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#22270;&#35889;&#21644;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic Knowledge Graph and Text Mining Approach. (arXiv:2306.03833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#34394;&#25311;&#20581;&#24247;&#20013;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#21644;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#20043;&#38388;&#30340;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#20581;&#24247;&#34987;&#35465;&#20026;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#20013;&#30340;&#25913;&#21464;&#24615;&#21147;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#36864;&#20986;&#38382;&#39064;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#20581;&#24247;&#32467;&#26524;&#65292;&#22686;&#21152;&#20581;&#24247;&#12289;&#31038;&#20250;&#21644;&#32463;&#27982;&#25104;&#26412;&#12290;&#21450;&#26102;&#39044;&#27979;&#24739;&#32773;&#30340;&#36864;&#20986;&#20351;&#32929;&#19996;&#33021;&#22815;&#37319;&#21462;&#31215;&#26497;&#30340;&#27493;&#39588;&#65292;&#35299;&#20915;&#24739;&#32773;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#25552;&#39640;&#20445;&#30041;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20449;&#24687;&#19981;&#23545;&#31216;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21160;&#24577;&#30693;&#35782;&#39537;&#21160;&#36864;&#20986;&#39044;&#27979;&#65288;MDKDP&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#22312;&#32447;&#21644;&#31163;&#32447;&#21307;&#30103;&#20445;&#20581;&#20132;&#20184;&#31995;&#32479;&#30340;&#21307;&#29983;&#24739;&#32773;&#23545;&#35805;&#12289;&#21508;&#20010;&#32929;&#19996;&#30340;&#21160;&#24577;&#21644;&#22797;&#26434;&#32593;&#32476;&#20013;&#23398;&#20064;&#38544;&#24335;&#21644;&#26174;&#24335;&#30693;&#35782;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#20013;&#22269;&#26368;&#22823;&#30340;&#34394;&#25311;&#20581;&#24247;&#24179;&#21488;&#20043;&#19968;&#21512;&#20316;&#26469;&#35780;&#20272;MDKDP&#12290;MDKDP&#25552;&#39640;&#20102;&#36864;&#20986;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual health has been acclaimed as a transformative force in healthcare delivery. Yet, its dropout issue is critical that leads to poor health outcomes, increased health, societal, and economic costs. Timely prediction of patient dropout enables stakeholders to take proactive steps to address patients' concerns, potentially improving retention rates. In virtual health, the information asymmetries inherent in its delivery format, between different stakeholders, and across different healthcare delivery systems hinder the performance of existing predictive methods. To resolve those information asymmetries, we propose a Multimodal Dynamic Knowledge-driven Dropout Prediction (MDKDP) framework that learns implicit and explicit knowledge from doctor-patient dialogues and the dynamic and complex networks of various stakeholders in both online and offline healthcare delivery systems. We evaluate MDKDP by partnering with one of the largest virtual health platforms in China. MDKDP improves the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.15742</link><description>&lt;p&gt;
&#26102;&#38388;&#21464;&#21270;&#22788;&#29702;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Generative Models for Time-Varying Treatments. (arXiv:2305.15742v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26102;&#38388;&#21464;&#37327;&#22788;&#29702;&#24773;&#20917;&#19979;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25512;&#26029;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#65292;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#26159;&#27979;&#35797;&#26032;&#30103;&#27861;&#30340;&#24120;&#29992;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24179;&#22343;&#25928;&#24212;&#20250;&#25513;&#30422;&#21453;&#20107;&#23454;&#20998;&#24067;&#20013;&#37325;&#35201;&#30340;&#20010;&#20307;&#29305;&#24449;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#23433;&#20840;&#12289;&#20844;&#24179;&#21644;&#36947;&#24503;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#26102;&#38388;&#35774;&#32622;&#20013;&#26356;&#21152;&#20005;&#37325;&#65292;&#22240;&#20026;&#22788;&#29702;&#26159;&#26102;&#24207;&#30340;&#21644;&#26102;&#21464;&#30340;&#65292;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#20135;&#29983;&#20102;&#38169;&#32508;&#22797;&#26434;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26465;&#20214;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#25429;&#33719;&#25972;&#20010;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#20801;&#35768;&#23545;&#21453;&#20107;&#23454;&#20998;&#24067;&#30340;&#26576;&#20123;&#32479;&#35745;&#37327;&#36827;&#34892;&#26377;&#25928;&#25512;&#26029;&#12290;&#36825;&#20351;&#24471;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23588;&#20854;&#36866;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#21644;&#20844;&#20849;&#25919;&#31574;&#21046;&#23450;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#36890;&#36807;&#36793;&#38469;&#32467;&#26500;&#27169;&#22411;&#35880;&#24910;&#22320;&#35299;&#20915;&#20102;&#35266;&#23519;&#25968;&#25454;&#21644;&#30446;&#26631;&#21453;&#20107;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating average causal effects is a common practice to test new treatments. However, the average effect ''masks'' important individual characteristics in the counterfactual distribution, which may lead to safety, fairness, and ethical concerns. This issue is exacerbated in the temporal setting, where the treatment is sequential and time-varying, leading to an intricate influence on the counterfactual distribution. In this paper, we propose a novel conditional generative modeling approach to capture the whole counterfactual distribution, allowing efficient inference on certain statistics of the counterfactual distribution. This makes the proposed approach particularly suitable for healthcare and public policy making. Our generative modeling approach carefully tackles the distribution mismatch in the observed data and the targeted counterfactual distribution via a marginal structural model. Our method outperforms state-of-the-art baselines on both synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.02012</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#35780;&#36848;&#65306;SHAP &#21644; LIME
&lt;/p&gt;
&lt;p&gt;
Commentary on explainable artificial intelligence methods: SHAP and LIME. (arXiv:2305.02012v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35780;&#35770;&#23545;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861; SHAP &#21644; LIME &#36827;&#34892;&#20102;&#35780;&#36848;&#21644;&#27604;&#36739;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#19988;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#21457;&#23637;&#20986;&#26469;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#36716;&#21270;&#20026;&#26356;&#26131;&#29702;&#35299;&#30340;&#24418;&#24335;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#20256;&#36798;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#26088;&#22312;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26356;&#36879;&#26126;&#65292;&#24182;&#22686;&#21152;&#26368;&#32456;&#29992;&#25143;&#23545;&#20854;&#36755;&#20986;&#30340;&#20449;&#20219;&#12290; SHapley Additive exPlanations&#65288;SHAP&#65289;&#21644;Local Interpretable Model Agnostic Explanation&#65288;LIME&#65289;&#26159;&#20004;&#31181;&#22312;&#34920;&#26684;&#25968;&#25454;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;XAI&#26041;&#27861;&#12290;&#22312;&#36825;&#31687;&#35780;&#35770;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#26159;&#22914;&#20309;&#29983;&#25104;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#23427;&#20204;&#36755;&#20986;&#30340;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.
&lt;/p&gt;</description></item><item><title>FedML-HE&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#21516;&#24577;&#21152;&#23494;&#30340;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24615;&#21152;&#23494;&#25935;&#24863;&#21442;&#25968;&#26469;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#20379;&#21487;&#23450;&#21046;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2303.10837</link><description>&lt;p&gt;
FedML-HE:&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#21516;&#24577;&#21152;&#23494;&#30340;&#38544;&#31169;&#20445;&#25252;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FedML-HE: An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System. (arXiv:2303.10837v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10837
&lt;/p&gt;
&lt;p&gt;
FedML-HE&#26159;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#21516;&#24577;&#21152;&#23494;&#30340;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#36873;&#25321;&#24615;&#21152;&#23494;&#25935;&#24863;&#21442;&#25968;&#26469;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#25552;&#20379;&#21487;&#23450;&#21046;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#36890;&#36807;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#32780;&#19981;&#26159;&#26412;&#22320;&#25968;&#25454;&#65292;&#22312;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#38382;&#39064;&#20135;&#29983;&#20102;&#65292;&#22240;&#20026;&#26381;&#21153;&#31471;&#19978;&#32858;&#21512;&#30340;&#26412;&#22320;&#27169;&#22411;&#21487;&#33021;&#36890;&#36807;&#36870;&#21521;&#25915;&#20987;&#25581;&#31034;&#25935;&#24863;&#20010;&#20154;&#20449;&#24687;&#12290;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22914;&#21516;&#24577;&#21152;&#23494;&#65288;HE&#65289;&#65292;&#22240;&#27492;&#25104;&#20026;FL&#35757;&#32451;&#30340;&#24517;&#35201;&#25163;&#27573;&#12290;&#23613;&#31649;HE&#20855;&#26377;&#38544;&#31169;&#20248;&#21183;&#65292;&#20294;&#20854;&#24212;&#29992;&#21463;&#21040;&#19981;&#23454;&#38469;&#30340;&#24320;&#38144;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#23545;&#22522;&#30784;&#27169;&#22411;&#32780;&#35328;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedML-HE&#65292;&#31532;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;HE&#23433;&#20840;&#27169;&#22411;&#32858;&#21512;&#30340;&#23454;&#29992;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#12290;FedML-HE&#25552;&#20986;&#20102;&#36873;&#25321;&#24615;&#21152;&#23494;&#25935;&#24863;&#21442;&#25968;&#65292;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#65292;&#21516;&#26102;&#25552;&#20379;&#21487;&#23450;&#21046;&#30340;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#20248;&#21270;&#30340;&#31995;&#32479;&#26174;&#31034;&#20986;&#30456;&#24403;&#22823;&#30340;&#24320;&#38144;&#38477;&#20302;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;ResNet-50&#20943;&#23569;&#20102;&#32422;10&#20493;&#65292;&#32780;B&#27169;&#22411;&#20943;&#23569;&#20102;&#32422;40&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, the first practical federated learning system with efficient HE-based secure model aggregation. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., ~10x reduction for ResNet-50, and up to ~40x reduction for B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.09780</link><description>&lt;p&gt;
Mpox-AISM&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36229;&#32423;&#30417;&#27979;&#20197;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Mpox-AISM: AI-Mediated Super Monitoring for Forestalling Monkeypox Spread. (arXiv:2303.09780v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#36229;&#32423;&#30417;&#27979;&#30340;&#36828;&#31243;&#23454;&#29616;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#30340;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#20197;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20026;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#39640;&#28789;&#25935;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#30149;&#30151;&#20998;&#31867;&#65292;&#21516;&#26102;&#25104;&#26412;&#20302;&#12289;&#26131;&#29992;&#24615;&#39640;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21450;&#26102;&#12289;&#20415;&#25463;&#21644;&#20934;&#30830;&#35786;&#26029;&#26089;&#26399;&#24739;&#32773;&#26159;&#36943;&#21046;&#29492;&#30168;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#12289;&#23454;&#26102;&#30340;&#22312;&#32447;&#21487;&#35270;&#21270;&#31574;&#30053;&#65292;&#31216;&#20026;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65292;&#29992;&#20110;&#26500;&#24314;&#20302;&#25104;&#26412;&#12289;&#26041;&#20415;&#12289;&#21450;&#26102;&#21644;&#26080;&#19987;&#19994;&#30693;&#35782;&#30340;&#29492;&#30168;&#26089;&#26399;&#35786;&#26029;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#32452;&#35013;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20171;&#23548;&#30340;&#8220;&#36229;&#32423;&#30417;&#27979;&#8221;&#65288;Mpox-AISM&#65289;&#65292;&#26681;&#25454;&#25968;&#25454;&#38598;&#29305;&#24449;&#21644;&#29492;&#30168;&#28436;&#21464;&#36235;&#21183;&#20197;&#21450;&#19982;&#39640;&#30456;&#20284;&#24230;&#30340;&#20854;&#20182;&#19971;&#31181;&#30382;&#32932;&#30149;&#30340;&#19987;&#19994;&#20998;&#31867;&#65292;&#22240;&#27492;&#36825;&#20123;&#21151;&#33021;&#19982;&#21512;&#29702;&#30340;&#31243;&#24207;&#30028;&#38754;&#21644;&#38408;&#20540;&#35774;&#32622;&#30830;&#20445;&#20102;&#20854;&#28789;&#25935;&#24230;&#36229;&#36807;95.9&#65285;&#65292;&#29305;&#24322;&#24230;&#20960;&#20046;&#36798;&#21040;100&#65285;&#12290;&#22240;&#27492;&#65292;&#22312;&#20114;&#32852;&#32593;&#21644;&#36890;&#35759;&#32456;&#31471;&#30340;&#20113;&#26381;&#21153;&#30340;&#24110;&#21161;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#28508;&#22312;&#22320;&#29992;&#20110;&#23454;&#26102;&#26816;&#27979;&#29492;&#30168;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge on forestalling monkeypox (Mpox) spread is the timely, convenient and accurate diagnosis for earlystage infected individuals. Here, we propose a remote and realtime online visualization strategy, called "Super Monitoring" to construct a low cost, convenient, timely and unspecialized diagnosis of early-stage Mpox. Such AI-mediated "Super Monitoring" (Mpox-AISM) invokes a framework assembled by deep learning, data augmentation and self-supervised learning, as well as professionally classifies four subtypes according to dataset characteristics and evolution trend of Mpox and seven other types of dermatopathya with high similarity, hence these features together with reasonable program interface and threshold setting ensure that its Recall (Sensitivity) was beyond 95.9% and the specificity was almost 100%. As a result, with the help of cloud service on Internet and communication terminal, this strategy can be potentially utilized for the real-time detection of earlystage Mpox 
&lt;/p&gt;</description></item></channel></rss>