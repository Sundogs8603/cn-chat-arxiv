<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#32473;&#23450;&#35780;&#20272;&#29615;&#22659;&#19979;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.08913</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Statistical Turing Test for Generative Models. (arXiv:2309.08913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#35745;&#22270;&#28789;&#27979;&#35797;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#22312;&#32473;&#23450;&#35780;&#20272;&#29615;&#22659;&#19979;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35813;&#26694;&#26550;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25991;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#35273;&#31561;&#39046;&#22495;&#30340;&#20869;&#23481;&#29983;&#25104;&#33021;&#21147;&#30340;&#20986;&#29616;&#20652;&#29983;&#20102;&#29992;&#20110;&#21306;&#20998;&#20869;&#23481;&#26469;&#28304;&#20110;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#20998;&#31867;&#22120;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#24037;&#20316;&#30340;&#38544;&#21547;&#20551;&#35774;&#26159;&#20154;&#31867;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;&#26426;&#22120;&#30340;&#29983;&#25104;&#33021;&#21147;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32479;&#35745;&#27169;&#24335;&#35782;&#21035;&#35821;&#35328;&#20013;&#37327;&#21270;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#20998;&#24067;&#24046;&#24322;&#30340;&#26694;&#26550;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#22312;&#26694;&#26550;&#20013;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#21521;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20998;&#26512;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of human-like abilities of AI systems for content generation in domains such as text, audio, and vision has prompted the development of classifiers to determine whether content originated from a human or a machine. Implicit in these efforts is an assumption that the generation properties of a human are different from that of the machine. In this work, we provide a framework in the language of statistical pattern recognition that quantifies the difference between the distributions of human and machine-generated content conditioned on an evaluation context. We describe current methods in the context of the framework and demonstrate how to use the framework to evaluate the progression of generative models towards human-like capabilities, among many axes of analysis.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#27599;&#36718;&#25237;&#24433;&#30340;&#25968;&#37327;&#26469;&#20248;&#21270;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08911</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Methods for Non-stationary Online Learning. (arXiv:2309.08911v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08911
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#27599;&#36718;&#25237;&#24433;&#30340;&#25968;&#37327;&#26469;&#20248;&#21270;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#31283;&#22312;&#32447;&#23398;&#20064;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#65292;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#34987;&#25552;&#20986;&#20316;&#20026;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#20004;&#20010;&#21407;&#21017;&#24615;&#24615;&#33021;&#24230;&#37327;&#12290;&#20026;&#20102;&#20248;&#21270;&#23427;&#20204;&#65292;&#36890;&#24120;&#37319;&#29992;&#20004;&#23618;&#22312;&#32447;&#38598;&#25104;&#65292;&#30001;&#20110;&#38750;&#24179;&#31283;&#24615;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#20854;&#20013;&#32500;&#25252;&#19968;&#32452;&#22522;&#23398;&#20064;&#22120;&#65292;&#24182;&#37319;&#29992;&#20803;&#31639;&#27861;&#22312;&#36816;&#34892;&#36807;&#31243;&#20013;&#36319;&#36394;&#26368;&#20339;&#23398;&#20064;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20004;&#23618;&#32467;&#26500;&#24341;&#21457;&#20102;&#20851;&#20110;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#25285;&#24551; -&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21516;&#26102;&#32500;&#25252;$\mathcal{O}(\log T)$&#20010;&#22522;&#23398;&#20064;&#22120;&#65292;&#23545;&#20110;&#19968;&#20010;$T$&#36718;&#22312;&#32447;&#28216;&#25103;&#65292;&#22240;&#27492;&#27599;&#36718;&#25191;&#34892;&#22810;&#27425;&#25237;&#24433;&#21040;&#21487;&#34892;&#22495;&#19978;&#65292;&#24403;&#22495;&#24456;&#22797;&#26434;&#26102;&#65292;&#36825;&#25104;&#20026;&#35745;&#31639;&#29942;&#39048;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20248;&#21270;&#21160;&#24577;&#36951;&#25022;&#21644;&#33258;&#36866;&#24212;&#36951;&#25022;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#23558;&#27599;&#36718;&#30340;&#25237;&#24433;&#27425;&#25968;&#20174;$\mathcal{O}(\log T)$&#38477;&#20302;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Non-stationary online learning has drawn much attention in recent years. In particular, dynamic regret and adaptive regret are proposed as two principled performance measures for online convex optimization in non-stationary environments. To optimize them, a two-layer online ensemble is usually deployed due to the inherent uncertainty of the non-stationarity, in which a group of base-learners are maintained and a meta-algorithm is employed to track the best one on the fly. However, the two-layer structure raises the concern about the computational complexity -- those methods typically maintain $\mathcal{O}(\log T)$ base-learners simultaneously for a $T$-round online game and thus perform multiple projections onto the feasible domain per round, which becomes the computational bottleneck when the domain is complicated. In this paper, we present efficient methods for optimizing dynamic regret and adaptive regret, which reduce the number of projections per round from $\mathcal{O}(\log T)$ t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#25968;&#25454;&#25439;&#22351;&#19979;&#40065;&#26834;&#22320;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#21644;&#31232;&#30095;&#31934;&#24230;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.08884</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#25968;&#25454;&#25439;&#22351;&#19979;&#30340;&#40065;&#26834;&#22312;&#32447;&#21327;&#26041;&#24046;&#21644;&#31232;&#30095;&#31934;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Online Covariance and Sparse Precision Estimation Under Arbitrary Data Corruption. (arXiv:2309.08884v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08884
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#25968;&#25454;&#25439;&#22351;&#19979;&#40065;&#26834;&#22320;&#22312;&#32447;&#20272;&#35745;&#21327;&#26041;&#24046;&#21644;&#31232;&#30095;&#31934;&#24230;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#22270;&#27169;&#22411;&#34987;&#24191;&#27867;&#29992;&#20110;&#34920;&#31034;&#23454;&#20307;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#25439;&#22351;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20462;&#25913;&#21518;&#30340;&#20462;&#21098;&#20869;&#31215;&#31639;&#27861;&#65292;&#22312;&#20219;&#24847;&#21644;&#23545;&#25239;&#24615;&#25968;&#25454;&#25915;&#20987;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#22312;&#32447;&#22330;&#26223;&#20013;&#40065;&#26834;&#22320;&#20272;&#35745;&#21327;&#26041;&#24046;&#12290;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#65292;&#27491;&#24120;&#22320;&#29420;&#31435;&#21516;&#20998;&#24067;&#20110;&#22810;&#20803;&#39640;&#26031;&#20998;&#24067;&#30340;&#25968;&#25454;&#28857;&#21040;&#26469;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#23450;&#27604;&#20363;&#30340;&#25968;&#25454;&#28857;&#21487;&#33021;&#24050;&#34987;&#20219;&#24847;&#30772;&#22351;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#23613;&#31649;&#23384;&#22312;&#25968;&#25454;&#25439;&#22351;&#65292;&#21487;&#20197;&#20272;&#35745;&#31232;&#30095;&#36870;&#21327;&#26041;&#24046;&#65288;&#21363;&#31934;&#24230;&#65289;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#31639;&#27861;&#19979;&#20272;&#35745;&#31934;&#24230;&#30697;&#38453;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian graphical models are widely used to represent correlations among entities but remain vulnerable to data corruption. In this work, we introduce a modified trimmed-inner-product algorithm to robustly estimate the covariance in an online scenario even in the presence of arbitrary and adversarial data attacks. At each time step, data points, drawn nominally independently and identically from a multivariate Gaussian distribution, arrive. However, a certain fraction of these points may have been arbitrarily corrupted. We propose an online algorithm to estimate the sparse inverse covariance (i.e., precision) matrix despite this corruption. We provide the error-bound and convergence properties of the estimates to the true precision matrix under our algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;H-infinity&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23436;&#20840;&#26080;&#27169;&#22411;&#65292;&#19988;&#19981;&#38656;&#35201;&#21021;&#22987;&#31283;&#23450;&#31574;&#30053;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#38381;&#24335;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.08880</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#26102;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;H-infinity&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;&#65306;&#33258;&#20027;&#31227;&#21160;&#38656;&#27714;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems. (arXiv:2309.08880v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;H-infinity&#25511;&#21046;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#23436;&#20840;&#26080;&#27169;&#22411;&#65292;&#19988;&#19981;&#38656;&#35201;&#21021;&#22987;&#31283;&#23450;&#31574;&#30053;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31867;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#35774;&#35745;&#36866;&#24212;&#24615;&#26368;&#20248;&#25511;&#21046;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#26080;&#20851;&#12289;&#23454;&#26102;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;Q-learning&#31639;&#27861;&#26469;&#35299;&#20915;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#31995;&#32479;&#30340;H-infinity&#25511;&#21046;&#38382;&#39064;&#12290;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;&#25991;&#29486;&#20013;&#30340;O(q^3)&#38477;&#20302;&#21040;&#20102;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;O(q^2)&#65292;&#20854;&#20013;q&#26159;&#29366;&#24577;&#21464;&#37327;&#12289;&#25511;&#21046;&#36755;&#20837;&#21644;&#24178;&#25200;&#30340;&#22823;&#23567;&#20043;&#21644;&#30340;&#20108;&#27425;&#39033;&#12290;&#36890;&#36807;&#22312;&#32447;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#33258;&#36866;&#24212;&#26368;&#20248;&#25511;&#21046;&#22120;&#65292;&#24182;&#23398;&#20064;&#20102;&#21160;&#20316;&#21644;&#35780;&#35770;&#32593;&#32476;&#30340;&#21442;&#25968;&#65292;&#19981;&#38656;&#35201;&#23545;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#20102;&#35299;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#23436;&#20840;&#26080;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20165;&#22312;&#31532;&#19968;&#27425;&#36845;&#20195;&#20013;&#38656;&#35201;&#36275;&#22815;&#30340;&#25200;&#21160;&#22122;&#22768;&#65292;&#32780;&#19981;&#24433;&#21709;&#25552;&#20986;&#30340;&#31639;&#27861;&#12290;&#26080;&#38656;&#21021;&#22987;&#31283;&#23450;&#31574;&#30053;&#65292;&#31639;&#27861;&#25910;&#25947;&#21040;&#38381;&#24335;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\mathcal{O}(\underline{q}^3)$ in the literature to $\mathcal{O}(\underline{q}^2)$ in the proposed algorithm, where $\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form 
&lt;/p&gt;</description></item><item><title>PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08872</link><description>&lt;p&gt;
PDFTriage: &#23545;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#36827;&#34892;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08872
&lt;/p&gt;
&lt;p&gt;
PDFTriage&#26159;&#19968;&#31181;&#22788;&#29702;&#38271;&#31687;&#32467;&#26500;&#21270;&#25991;&#26723;&#38382;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32467;&#26500;&#25110;&#20869;&#23481;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38382;&#31572;&#20013;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#38382;&#31572;&#26102;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#25991;&#26723;&#26080;&#27861;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#23558;&#20854;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20687;PDF&#12289;&#32593;&#39029;&#21644;&#28436;&#31034;&#25991;&#31295;&#36825;&#26679;&#30340;&#25991;&#26723;&#26159;&#26377;&#32467;&#26500;&#30340;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#39029;&#30721;&#12289;&#34920;&#26684;&#12289;&#31456;&#33410;&#31561;&#12290;&#23558;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25991;&#26723;&#34920;&#31034;&#20026;&#32431;&#25991;&#26412;&#19982;&#29992;&#25143;&#23545;&#36825;&#20123;&#20855;&#26377;&#20016;&#23500;&#32467;&#26500;&#30340;&#25991;&#26723;&#30340;&#35748;&#30693;&#27169;&#22411;&#19981;&#31526;&#12290;&#24403;&#31995;&#32479;&#38656;&#35201;&#20174;&#25991;&#26723;&#20013;&#26597;&#35810;&#19978;&#19979;&#25991;&#26102;&#65292;&#36825;&#31181;&#19981;&#31526;&#20250;&#26174;&#29616;&#20986;&#26469;&#65292;&#29978;&#33267;&#31616;&#21333;&#30340;&#38382;&#39064;&#20063;&#21487;&#33021;&#20351;&#38382;&#31572;&#31995;&#32479;&#20986;&#38169;&#12290;&#20026;&#20102;&#24357;&#21512;&#22788;&#29702;&#32467;&#26500;&#21270;&#25991;&#26723;&#20013;&#30340;&#22522;&#26412;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PDFTriage&#30340;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#32467;&#26500;&#25110;&#20869;&#23481;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PDFTriage&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;LRBench++&#26469;&#24110;&#21161;&#30740;&#31350;&#32773;&#35780;&#20272;&#23398;&#20064;&#29575;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.08859</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#23398;&#20064;&#29575;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Rethinking Learning Rate Tuning in the Era of Large Language Models. (arXiv:2309.08859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20986;&#20102;LRBench++&#26469;&#24110;&#21161;&#30740;&#31350;&#32773;&#35780;&#20272;&#23398;&#20064;&#29575;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#20154;&#31867;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#21151;&#12290;&#37492;&#20110;LLM&#35757;&#32451;&#30340;&#26114;&#36149;&#36153;&#29992;&#65292;&#21033;&#29992;&#24494;&#35843;&#26469;&#36866;&#24212;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#24050;&#25104;&#20026;&#20027;&#27969;&#31574;&#30053;&#12290;&#23398;&#20064;&#29575;&#26159;LLM&#24494;&#35843;&#20013;&#26368;&#37325;&#35201;&#30340;&#36229;&#21442;&#25968;&#20043;&#19968;&#65292;&#30452;&#25509;&#24433;&#21709;&#24494;&#35843;&#25928;&#29575;&#21644;&#24494;&#35843;&#21518;&#30340;LLM&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#23398;&#20064;&#29575;&#31574;&#30053;&#20027;&#35201;&#38024;&#23545;&#20256;&#32479;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#35757;&#32451;&#32780;&#35774;&#35745;&#65292;&#21487;&#33021;&#22312;LLM&#24494;&#35843;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#30740;&#31350;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#20570;&#20986;&#20102;&#19977;&#20010;&#21407;&#21019;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#30340;&#23398;&#20064;&#29575;&#31574;&#30053;&#65292;&#20998;&#26512;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LRBench++&#26469;&#35780;&#20272;&#23398;&#20064;&#29575;&#31574;&#30053;&#24182;&#20419;&#36827;&#23398;&#20064;&#29575;&#35843;&#25972;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) represent the recent success of deep learning in achieving remarkable human-like predictive performance. It has become a mainstream strategy to leverage fine-tuning to adapt LLMs for various real-world applications due to the prohibitive expenses associated with LLM training. The learning rate is one of the most important hyperparameters in LLM fine-tuning with direct impacts on both fine-tuning efficiency and fine-tuned LLM quality. Existing learning rate policies are primarily designed for training traditional deep neural networks (DNNs), which may not work well for LLM fine-tuning. We reassess the research challenges and opportunities of learning rate tuning in the coming era of Large Language Models. This paper makes three original contributions. First, we revisit existing learning rate policies to analyze the critical challenges of learning rate tuning in the era of LLMs. Second, we present LRBench++ to benchmark learning rate policies and facilitate l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2309.08835</link><description>&lt;p&gt;
&#36890;&#36807;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#65292;&#26234;&#33021;&#26426;&#22120;&#22312;&#26080;&#32467;&#26500;&#29615;&#22659;&#20013;&#24037;&#20316;
&lt;/p&gt;
&lt;p&gt;
Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24046;&#20998;&#31070;&#32463;&#35745;&#31639;&#30340;&#26234;&#33021;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#24212;&#30340;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#24182;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#24212;&#29992;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24076;&#26395;&#26234;&#33021;&#26426;&#22120;&#33021;&#22815;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#39640;&#25928;&#22320;&#24037;&#20316;&#65292;&#38656;&#35201;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20934;&#30830;&#22320;&#29702;&#35299;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#26080;&#32467;&#26500;&#20449;&#24687;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35760;&#24518;&#38459;&#24615;&#31070;&#32463;&#35745;&#31639;&#30340;&#24863;&#30693;&#20449;&#21495;&#24046;&#20998;&#22788;&#29702;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#29615;&#22659;&#20449;&#24687;&#30340;&#20027;&#35201;&#29305;&#24449;&#24182;&#24212;&#29992;&#30456;&#20851;&#32534;&#30721;&#21050;&#28608;&#21040;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#33719;&#24471;&#20102;&#22788;&#29702;&#26080;&#32467;&#26500;&#29615;&#22659;&#20449;&#24687;&#30340;&#31867;&#20154;&#33021;&#21147;&#65292;&#22914;&#26426;&#26800;&#21050;&#28608;&#30340;&#25918;&#22823;&#65288;&gt;720%&#65289;&#21644;&#36866;&#24212;&#65288;&lt;50%&#65289;&#12290;&#35813;&#26041;&#27861;&#36824;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#22312;&#26234;&#33021;&#26426;&#22120;&#30340;&#20004;&#20010;&#20856;&#22411;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65306;&#29289;&#20307;&#25235;&#21462;&#21644;&#33258;&#21160;&#39550;&#39542;&#12290;&#22312;&#29289;&#20307;&#25235;&#21462;&#26041;&#38754;&#65292;&#36890;&#36807;&#22312;1&#27627;&#31186;&#20869;&#20351;&#29992;&#21333;&#20010;&#35760;&#24518;&#38459;&#24615;&#22120;&#20214;&#23398;&#20064;&#26410;&#30693;&#29289;&#20307;&#29305;&#24449;&#65288;&#20363;&#22914;&#23574;&#38160;&#30340;&#35282;&#21644;&#20809;&#28369;&#30340;&#34920;&#38754;&#65289;&#65292;&#19968;&#20010;&#26426;&#22120;&#25163;&#23454;&#29616;&#20102;&#23433;&#20840;&#31283;&#23450;&#30340;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (&gt;720%) and adaptation (&lt;50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#20808;&#39564;&#20559;&#31227;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#20107;&#21518;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#19978;&#36827;&#34892;&#32553;&#25918;&#35843;&#25972;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21608;&#22260;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2309.08825</link><description>&lt;p&gt;
&#20808;&#39564;&#20559;&#31227;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#20107;&#21518;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08825
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#20808;&#39564;&#20559;&#31227;&#19979;&#30340;&#20998;&#24067;&#40065;&#26834;&#20107;&#21518;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#19978;&#36827;&#34892;&#32553;&#25918;&#35843;&#25972;&#65292;&#20197;&#26368;&#23567;&#21270;&#30446;&#26631;&#20998;&#24067;&#21608;&#22260;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27979;&#35797;&#20998;&#24067;&#20559;&#31163;&#35757;&#32451;&#20998;&#24067;&#26102;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#38477;&#20302;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#27169;&#22411;&#20197;&#24212;&#23545;&#30001;&#31867;&#20808;&#39564;&#25110;&#32452;&#20808;&#39564;&#20998;&#24067;&#21464;&#21270;&#24341;&#36215;&#30340;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;&#23384;&#22312;&#20559;&#26012;&#30340;&#35757;&#32451;&#20808;&#39564;&#24448;&#24448;&#20250;&#23548;&#33268;&#27169;&#22411;&#23545;&#22122;&#22768;&#29305;&#24449;&#36807;&#25311;&#21512;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20248;&#21270;&#26368;&#24046;&#25110;&#24179;&#22343;&#24615;&#33021;&#65292;&#32780;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#20986;&#20110;&#23545;&#27169;&#22411;&#40065;&#26834;&#24615;&#36136;&#26356;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26497;&#20854;&#36731;&#37327;&#32423;&#30340;&#20107;&#21518;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#32553;&#25918;&#35843;&#25972;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#36873;&#25321;&#30340;&#30446;&#26631;&#20998;&#24067;&#21608;&#22260;&#30340;&#20998;&#24067;&#40065;&#26834;&#25439;&#22833;&#12290;&#36825;&#20123;&#35843;&#25972;&#36890;&#36807;&#22312;&#39564;&#35777;&#38598;&#19978;&#27714;&#35299;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#26469;&#35745;&#31639;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#38388;&#24212;&#29992;&#20110;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization obje
&lt;/p&gt;</description></item><item><title>SHAPNN&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08799</link><description>&lt;p&gt;
SHAPNN: Shapley Value&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#22411;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
SHAPNN: Shapley Value Regularized Tabular Neural Network. (arXiv:2309.08799v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08799
&lt;/p&gt;
&lt;p&gt;
SHAPNN&#26159;&#19968;&#31181;&#20351;&#29992;Shapley&#20540;&#27491;&#21017;&#21270;&#30340;&#34920;&#26684;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#24182;&#25913;&#21892;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SHAPNN&#65292;&#19968;&#31181;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#28145;&#24230;&#34920;&#26684;&#25968;&#25454;&#24314;&#27169;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Shapley&#20540;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#26631;&#20934;&#30340;&#21453;&#21521;&#20256;&#25773;&#20248;&#21270;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#23454;&#26102;&#20272;&#35745;&#30340;Shapley&#20540;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#65292;&#21253;&#25324;&#33021;&#22815;&#23545;&#25968;&#25454;&#23454;&#20363;&#21644;&#25968;&#25454;&#38598;&#25552;&#20379;&#26377;&#25928;&#30340;&#35299;&#37322;&#32780;&#19981;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#24102;&#26377;&#35299;&#37322;&#30340;&#39044;&#27979;&#20316;&#20026;&#19968;&#31181;&#27491;&#21017;&#21270;&#22120;&#65292;&#25913;&#21892;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#27491;&#21017;&#21270;&#30340;&#39044;&#27979;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36830;&#32493;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#23637;&#31034;&#20102;SHAPNN&#22312;AUROC&#12289;&#36879;&#26126;&#24615;&#20197;&#21450;&#23545;&#27969;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SHAPNN, a novel deep tabular data modeling architecture designed for supervised learning. Our approach leverages Shapley values, a well-established technique for explaining black-box models. Our neural network is trained using standard backward propagation optimization methods, and is regularized with realtime estimated Shapley values. Our method offers several advantages, including the ability to provide valid explanations with no computational overhead for data instances and datasets. Additionally, prediction with explanation serves as a regularizer, which improves the model's performance. Moreover, the regularized prediction enhances the model's capability for continual learning. We evaluate our method on various publicly available datasets and compare it with state-of-the-art deep neural network models, demonstrating the superior performance of SHAPNN in terms of AUROC, transparency, as well as robustness to streaming data.
&lt;/p&gt;</description></item><item><title>Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2309.08793</link><description>&lt;p&gt;
Fin-Fact:&#19968;&#31181;&#38754;&#21521;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08793
&lt;/p&gt;
&lt;p&gt;
Fin-Fact&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#37329;&#34701;&#20107;&#23454;&#26680;&#26597;&#21644;&#35299;&#37322;&#29983;&#25104;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#19987;&#19994;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#20197;&#21450;&#22810;&#27169;&#24577;&#20449;&#24687;&#28304;&#26469;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#25171;&#20987;&#37329;&#34701;&#39046;&#22495;&#30340;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#24314;&#31435;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#20107;&#23454;&#26680;&#26597;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fin-Fact&#65292;&#19968;&#31181;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#22810;&#27169;&#24577;&#20107;&#23454;&#26680;&#26597;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#21253;&#25324;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#30340;&#27880;&#37322;&#21644;&#35777;&#25454;&#65292;&#25552;&#20379;&#19987;&#19994;&#30693;&#35782;&#21644;&#21487;&#20449;&#24230;&#12290;&#30001;&#20110;&#20854;&#22810;&#27169;&#24577;&#24615;&#36136;&#28085;&#30422;&#20102;&#25991;&#26412;&#21644;&#35270;&#35273;&#20869;&#23481;&#65292;Fin-Fact&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#28304;&#65292;&#20197;&#22686;&#24378;&#20107;&#23454;&#24615;&#20998;&#26512;&#12290;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#37329;&#34701;&#39046;&#22495;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#65292;&#20419;&#36827;&#36879;&#26126;&#24230;&#65292;&#24182;&#22312;&#36130;&#21153;&#25253;&#21578;&#21644;&#26032;&#38395;&#20256;&#25773;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#36890;&#36807;&#25552;&#20379;&#26377;&#28145;&#24230;&#30340;&#35299;&#37322;&#65292;Fin-Fact&#20351;&#29992;&#25143;&#65292;&#21253;&#25324;&#39046;&#22495;&#19987;&#23478;&#21644;&#32456;&#31471;&#29992;&#25143;&#65292;&#33021;&#22815;&#29702;&#35299;&#20107;&#23454;&#26680;&#26597;&#20915;&#31574;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#39564;&#35777;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#20419;&#36827;&#23545;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#30340;&#20449;&#20219;&#12290;Fin-Fact&#25968;&#25454;&#38598;&#20197;&#21450;&#25105;&#20204;&#30340;&#23454;&#39564;&#20195;&#30721;&#21487;&#22312;https://github.com/IIT-DM/Fin-Fact/ &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
&lt;/p&gt;</description></item><item><title>&#19968;&#20010;&#21517;&#20026;BioinspiredLLM&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#25991;&#29486;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#20027;&#21160;&#20132;&#20114;&#22320;&#22238;&#24518;&#21644;&#35780;&#20272;&#29983;&#29289;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#25552;&#20986;&#26032;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24182;&#20026;&#29983;&#29289;&#26448;&#26009;&#35774;&#35745;&#25552;&#20379;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.08788</link><description>&lt;p&gt;
BioinspiredLLM: &#29983;&#29289;&#21644;&#29983;&#29289;&#21463;&#21551;&#21457;&#26448;&#26009;&#21147;&#23398;&#30340;&#23545;&#35805;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials. (arXiv:2309.08788v1 [cond-mat.mtrl-sci])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08788
&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21517;&#20026;BioinspiredLLM&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22823;&#37327;&#30340;&#25991;&#29486;&#36827;&#34892;&#24494;&#35843;&#65292;&#33021;&#22815;&#20027;&#21160;&#20132;&#20114;&#22320;&#22238;&#24518;&#21644;&#35780;&#20272;&#29983;&#29289;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#25552;&#20986;&#26032;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24182;&#20026;&#29983;&#29289;&#26448;&#26009;&#35774;&#35745;&#25552;&#20379;&#21512;&#29702;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26448;&#26009;&#21644;&#29983;&#29289;&#21463;&#21551;&#21457;&#26448;&#26009;&#31185;&#23398;&#30340;&#30740;&#31350;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#21457;&#23637;&#65307;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24456;&#23569;&#26377;&#31995;&#32479;&#22320;&#23558;&#36825;&#20123;&#30693;&#35782;&#36716;&#21270;&#20026;&#24037;&#31243;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#21152;&#24555;&#21457;&#29616;&#24182;&#24341;&#23548;&#27934;&#23519;&#65292;&#25253;&#36947;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#33258;&#22238;&#24402;&#36716;&#25442;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;BioinspiredLLM&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#19968;&#21315;&#22810;&#31687;&#32463;&#36807;&#21516;&#34892;&#35780;&#23457;&#30340;&#32467;&#26500;&#29983;&#29289;&#23398;&#21644;&#29983;&#29289;&#21463;&#21551;&#21457;&#26448;&#26009;&#39046;&#22495;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#21487;&#20197;&#34987;&#25552;&#31034;&#20027;&#21160;&#21644;&#20132;&#20114;&#22320;&#22238;&#24518;&#20449;&#24687;&#65292;&#21327;&#21161;&#30740;&#31350;&#20219;&#21153;&#65292;&#24182;&#20316;&#20026;&#21019;&#36896;&#21147;&#30340;&#24341;&#25806;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#31034;&#20363;&#35777;&#26126;&#20102;&#23427;&#19981;&#20165;&#33021;&#22815;&#22312;&#26597;&#35810;&#26102;&#20934;&#30830;&#22238;&#24518;&#26377;&#20851;&#29983;&#29289;&#26448;&#26009;&#30340;&#20449;&#24687;&#65292;&#36824;&#33021;&#22815;&#25552;&#20986;&#29983;&#29289;&#26448;&#26009;&#38382;&#39064;&#21644;&#31572;&#26696;&#26469;&#35780;&#20272;&#33258;&#24049;&#30340;&#24615;&#33021;&#12290;BioinspiredLLM&#36824;&#34987;&#35777;&#26126;&#33021;&#22815;&#23545;&#29983;&#29289;&#26448;&#26009;&#35774;&#35745;&#25552;&#20986;&#21512;&#29702;&#30340;&#20551;&#35774;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#37027;&#20123;&#20174;&#26410;&#26126;&#30830;&#30740;&#31350;&#36807;&#30340;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of biological materials and bio-inspired materials science is well established; however, surprisingly little knowledge has been systematically translated to engineering solutions. To accelerate discovery and guide insights, an open-source autoregressive transformer large language model, BioinspiredLLM, is reported. The model was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to actively and interactively recall information, assist with research tasks, and function as an engine for creativity. The model has proven by example that it is not only able to accurately recall information about biological materials when queried but also formulate biomaterials questions and answers that can evaluate its own performance. BioinspiredLLM also has been shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#31867;&#22411;&#26631;&#31614;&#36827;&#34892;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#22411;&#39057;&#35889;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#25429;&#25417;&#26631;&#39064;&#20013;&#30340;&#24494;&#22937;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;LLMs&#22312;&#22686;&#24378;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.08787</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;: &#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;LLMs&#36827;&#34892;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata. (arXiv:2309.08787v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#31867;&#22411;&#26631;&#31614;&#36827;&#34892;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#31867;&#22411;&#39057;&#35889;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#25429;&#25417;&#26631;&#39064;&#20013;&#30340;&#24494;&#22937;&#31867;&#22411;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35752;&#35770;&#20102;LLMs&#22312;&#22686;&#24378;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#23481;&#20803;&#25968;&#25454;&#22312;&#30005;&#24433;&#25512;&#33616;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#23427;&#25552;&#20379;&#20102;&#26377;&#20851;&#30005;&#24433;&#30340;&#21508;&#20010;&#26041;&#38754;&#65288;&#22914;&#31867;&#22411;&#12289;&#28436;&#21592;&#12289;&#21095;&#24773;&#27010;&#35201;&#12289;&#31080;&#25151;&#25688;&#35201;&#31561;&#65289;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#12290;&#20998;&#26512;&#20803;&#25968;&#25454;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#29992;&#25143;&#20559;&#22909;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#21644;&#35299;&#20915;&#39033;&#30446;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#37325;&#28857;&#20851;&#27880;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#20803;&#25968;&#25454;&#8212;&#8212;&#8220;&#31867;&#22411;&#8221;&#26631;&#31614;&#12290;&#30005;&#24433;&#25110;&#30005;&#35270;&#21095;&#30340;&#31867;&#22411;&#26631;&#31614;&#26377;&#21161;&#20110;&#23558;&#19968;&#31995;&#21015;&#26631;&#39064;&#20998;&#31867;&#20026;&#19981;&#21516;&#20027;&#39064;&#65292;&#24182;&#30456;&#24212;&#22320;&#35774;&#32622;&#35266;&#20247;&#26399;&#26395;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#31867;&#22411;&#26631;&#31614;&#20449;&#24687;&#25152;&#28041;&#21450;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#31867;&#22411;&#39057;&#35889;&#8221;&#30340;&#26032;&#26041;&#27861;&#26469;&#30740;&#31350;&#31867;&#22411;&#20449;&#24687;&#12290;&#31867;&#22411;&#39057;&#35889;&#26377;&#21161;&#20110;&#25429;&#25417;&#26631;&#39064;&#20013;&#30340;&#21508;&#31181;&#24494;&#22937;&#31867;&#22411;&#65292;&#25105;&#20204;&#30340;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#35777;&#23454;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;LLMs&#22312;&#22686;&#24378;&#20869;&#23481;&#20803;&#25968;&#25454;&#20998;&#26512;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Content metadata plays a very important role in movie recommender systems as it provides valuable information about various aspects of a movie such as genre, cast, plot synopsis, box office summary, etc. Analyzing the metadata can help understand the user preferences to generate personalized recommendations and item cold starting. In this talk, we will focus on one particular type of metadata - \textit{genre} labels. Genre labels associated with a movie or a TV series help categorize a collection of titles into different themes and correspondingly setting up the audience expectation. We present some of the challenges associated with using genre label information and propose a new way of examining the genre information that we call as the \textit{Genre Spectrum}. The Genre Spectrum helps capture the various nuanced genres in a title and our offline and online experiments corroborate the effectiveness of the approach. Furthermore, we also talk about applications of LLMs in augmenting con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08776</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;Projected Task-Specific Layers
&lt;/p&gt;
&lt;p&gt;
Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;Projected Task-Specific Layers (PTSL)&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#26469;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25512;&#24191;&#21644;&#24178;&#25200;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#23478;&#24237;&#21644;&#24037;&#20316;&#22330;&#25152;&#30340;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#35268;&#27169;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#25512;&#24191;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#24182;&#20943;&#36731;&#36127;&#38754;&#20219;&#21153;&#24178;&#25200;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25104;&#21151;&#22320;&#22312;&#20219;&#21153;&#20043;&#38388;&#20849;&#20139;&#20449;&#24687;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#23558;&#21462;&#20915;&#20110;&#23545;&#20219;&#21153;&#24213;&#23618;&#32467;&#26500;&#30340;&#26377;&#25928;&#25429;&#25417;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#21363;Projected Task-Specific Layers&#65288;PTSL&#65289;&#65292;&#23427;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#23618;&#65292;&#36890;&#36807;&#31264;&#23494;&#30340;&#20219;&#21153;&#29305;&#23450;&#30340;&#20462;&#27491;&#26469;&#26356;&#22909;&#22320;&#34920;&#36798;&#20849;&#20139;&#21644;&#21487;&#21464;&#30340;&#20219;&#21153;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Meta-World&#30340;MT10&#21644;MT50&#22522;&#20934;&#20013;&#65288;&#21253;&#25324;Sawyer&#26426;&#22120;&#20154;&#33218;&#19978;&#30340;10&#20010;&#21644;50&#20010;&#30446;&#26631;&#26465;&#20214;&#20219;&#21153;&#65289;&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#35843;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38899;&#20048;&#21644;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08773</link><description>&lt;p&gt;
&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#21487;&#25511;&#24615;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#34920;&#31034;&#30456;&#20284;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhance audio generation controllability through representation similarity regularization. (arXiv:2309.08773v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08773
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24378;&#35843;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#26469;&#22686;&#24378;&#38899;&#39057;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38899;&#20048;&#21644;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#24378;&#35843;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#22686;&#24378;&#23545;&#38899;&#39057;&#29983;&#25104;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#22312;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#38899;&#39057;&#29983;&#25104;&#20013;&#65292;&#27169;&#22411;&#21033;&#29992;&#26469;&#33258;&#25991;&#26412;&#21644;&#38899;&#39057;&#26631;&#35760;&#34920;&#31034;&#30340;&#36755;&#20837;&#26469;&#39044;&#27979;&#21518;&#32493;&#30340;&#38899;&#39057;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37197;&#32622;&#32570;&#20047;&#26126;&#30830;&#30340;&#27491;&#21017;&#21270;&#26469;&#30830;&#20445;&#25152;&#36873;&#25321;&#30340;&#25991;&#26412;&#34920;&#31034;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#28041;&#21450;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#25972;&#21512;&#65292;&#29305;&#21035;&#26159;&#22312;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#38454;&#27573;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25991;&#26412;&#26465;&#20214;&#34987;&#25490;&#38500;&#22312;&#36328;&#27880;&#24847;&#21147;&#20043;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#30446;&#30340;&#26159;&#26368;&#23567;&#21270;&#19982;&#21516;&#19968;&#35757;&#32451;&#25209;&#27425;&#20013;&#20854;&#20182;&#26679;&#26412;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#30456;&#20284;&#24615;&#24046;&#24322;&#12290;&#22312;&#38899;&#20048;&#21644;&#38899;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#21270;&#23398;&#19987;&#21033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21253;&#21547;10&#19975;&#20010;&#20998;&#23376;&#21450;&#20854;&#21151;&#33021;&#26631;&#31614;&#30340;&#21270;&#23398;&#21151;&#33021;&#65288;CheF&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#21151;&#33021;&#26631;&#31614;&#32463;&#36807;&#39564;&#35777;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.08765</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#19987;&#21033;&#23637;&#31034;&#20102;&#21151;&#33021;&#26631;&#31614;&#19982;&#21270;&#23398;&#32467;&#26500;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures. (arXiv:2309.08765v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08765
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25366;&#25496;&#21270;&#23398;&#19987;&#21033;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21253;&#21547;10&#19975;&#20010;&#20998;&#23376;&#21450;&#20854;&#21151;&#33021;&#26631;&#31614;&#30340;&#21270;&#23398;&#21151;&#33021;&#65288;CheF&#65289;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#21151;&#33021;&#26631;&#31614;&#32463;&#36807;&#39564;&#35777;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32467;&#26500;&#39044;&#27979;&#21270;&#23398;&#21151;&#33021;&#26159;&#21270;&#23398;&#31185;&#23398;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#65292;&#20174;&#21457;&#29616;&#21644;&#37325;&#29992;&#26032;&#22411;&#33647;&#29289;&#21040;&#21019;&#36896;&#26032;&#26448;&#26009;&#12290;&#26368;&#36817;&#65292;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24320;&#36767;&#20102;&#28085;&#30422;&#35768;&#22810;&#19981;&#21516;&#21270;&#23398;&#21151;&#33021;&#30340;&#36890;&#29992;&#39044;&#27979;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#21270;&#23398;&#19987;&#21033;&#30340;&#25361;&#25112;&#65292;&#20197;&#25972;&#21512;&#21644;&#21033;&#29992;&#36825;&#20123;&#36164;&#28304;&#25152;&#25429;&#25417;&#30340;&#20851;&#20110;&#21270;&#23398;&#21151;&#33021;&#30340;&#20449;&#24687;&#12290;&#21270;&#23398;&#19987;&#21033;&#21253;&#21547;&#22823;&#37327;&#20851;&#20110;&#21270;&#23398;&#21151;&#33021;&#30340;&#30693;&#35782;&#65292;&#20294;&#30001;&#20110;&#25552;&#21462;&#39640;&#36136;&#37327;&#21151;&#33021;&#26631;&#31614;&#30340;&#19981;&#21487;&#34892;&#24615;&#65292;&#23427;&#20204;&#20316;&#20026;&#25968;&#25454;&#38598;&#30340;&#26377;&#29992;&#24615;&#21382;&#26469;&#34987;&#24573;&#35270;&#12290;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;ChatGPT&#36741;&#21161;&#19987;&#21033;&#25688;&#35201;&#21644;&#35789;&#23884;&#20837;&#26631;&#31614;&#28165;&#29702;&#27969;&#31243;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#21270;&#23398;&#21151;&#33021;&#65288;CheF&#65289;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;10&#19975;&#20010;&#20998;&#23376;&#21450;&#20854;&#19987;&#21033;&#34893;&#29983;&#30340;&#21151;&#33021;&#26631;&#31614;&#12290;&#36825;&#20123;&#21151;&#33021;&#26631;&#31614;&#32463;&#36807;&#39564;&#35777;&#26159;&#39640;&#36136;&#37327;&#30340;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
Predicting chemical function from structure is a major goal of the chemical sciences, from the discovery and repurposing of novel drugs to the creation of new materials. Recently, new machine learning algorithms are opening up the possibility of general predictive models spanning many different chemical functions. Here, we consider the challenge of applying large language models to chemical patents in order to consolidate and leverage the information about chemical functionality captured by these resources. Chemical patents contain vast knowledge on chemical function, but their usefulness as a dataset has historically been neglected due to the impracticality of extracting high-quality functional labels. Using a scalable ChatGPT-assisted patent summarization and word-embedding label cleaning pipeline, we derive a Chemical Function (CheF) dataset, containing 100K molecules and their patent-derived functional labels. The functional labels were validated to be of high quality, allowing us 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24490;&#29615;&#25968;&#25454;&#32858;&#31867;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26497;&#22352;&#26631;&#37325;&#24314;&#21644;&#25968;&#23398;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#37325;&#26500;&#25968;&#25454;&#38598;&#20013;&#20934;&#30830;&#25214;&#20986;&#32858;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.08757</link><description>&lt;p&gt;
&#20351;&#29992;&#26497;&#22352;&#26631;&#37325;&#24314;&#30340;&#24490;&#29615;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Circular Clustering with Polar Coordinate Reconstruction. (arXiv:2309.08757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24490;&#29615;&#25968;&#25454;&#32858;&#31867;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#26497;&#22352;&#26631;&#37325;&#24314;&#21644;&#25968;&#23398;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#37325;&#26500;&#25968;&#25454;&#38598;&#20013;&#20934;&#30830;&#25214;&#20986;&#32858;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22312;&#29983;&#29289;&#31995;&#32479;&#20013;&#21457;&#29616;&#30340;&#24490;&#29615;&#25968;&#25454;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#20854;&#36827;&#34892;&#29305;&#24449;&#21270;&#24863;&#20852;&#36259;&#12290;&#36825;&#20123;&#25968;&#25454;&#33539;&#22260;&#24191;&#27867;&#65292;&#22810;&#31181;&#22810;&#26679;&#65292;&#20174;&#31070;&#32463;&#35760;&#24405;&#20013;&#30340;&#20449;&#21495;&#30456;&#20301;&#21040;&#22278;&#24418;&#22522;&#22240;&#32452;&#20013;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#12290;&#20256;&#32479;&#30340;&#32858;&#31867;&#31639;&#27861;&#24448;&#24448;&#30001;&#20110;&#20854;&#23545;&#21608;&#26399;&#24615;&#32452;&#25104;&#37096;&#20998;&#30340;&#24046;&#24322;&#26377;&#38480;&#65292;&#32780;&#26174;&#24471;&#19981;&#22815;&#36866;&#29992;&#12290;&#30446;&#21069;&#22312;&#26497;&#22352;&#26631;&#31995;&#32479;&#20013;&#24037;&#20316;&#30340;&#32858;&#31867;&#26041;&#26696;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20165;&#19987;&#27880;&#20110;&#35282;&#24230;&#25110;&#32570;&#20047;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;&#65292;&#21033;&#29992;&#25237;&#24433;&#21040;&#26609;&#29366;&#22352;&#26631;&#31995;&#32479;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#26497;&#22352;&#26631;&#31995;&#32479;&#20013;&#30340;&#23545;&#35937;&#12290;&#21033;&#29992;&#24490;&#29615;&#25968;&#25454;&#30340;&#25968;&#23398;&#23646;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#37325;&#24314;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#21482;&#35201;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#21608;&#26399;&#37325;&#22797;&#65292;&#24635;&#33021;&#25214;&#21040;&#27491;&#30830;&#30340;&#32858;&#31867;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#35843;&#24615;&#65292;&#24182;&#21487;&#20197;&#34701;&#20837;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a growing interest in characterizing circular data found in biological systems. Such data are wide ranging and varied, from signal phase in neural recordings to nucleotide sequences in round genomes. Traditional clustering algorithms are often inadequate due to their limited ability to distinguish differences in the periodic component. Current clustering schemes that work in a polar coordinate system have limitations, such as being only angle-focused or lacking generality. To overcome these limitations, we propose a new analysis framework that utilizes projections onto a cylindrical coordinate system to better represent objects in a polar coordinate system. Using the mathematical properties of circular data, we show our approach always finds the correct clustering result within the reconstructed dataset, given sufficient periodic repetitions of the data. Our approach is generally applicable and adaptable and can be incorporated into most state-of-the-art clustering algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#30340;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#65292;&#20197;&#21450;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08751</link><description>&lt;p&gt;
&#22810;&#26679;&#30340;&#31070;&#32463;&#38899;&#39057;&#23884;&#20837; - &#24674;&#22797;&#29305;&#24449;&#65281;
&lt;/p&gt;
&lt;p&gt;
Diverse Neural Audio Embeddings -- Bringing Features back !. (arXiv:2309.08751v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#30340;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#65292;&#20197;&#21450;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;&#24320;&#22987;&#27969;&#34892;&#12290;&#36825;&#31181;&#36716;&#21464;&#23548;&#33268;&#20102;&#31070;&#32463;&#26550;&#26500;&#22312;&#27809;&#26377;&#39046;&#22495;&#29305;&#23450;&#20559;&#35265;/&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#26681;&#25454;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65288;&#22312;&#26412;&#20363;&#20013;&#26159;&#39046;&#22495;&#29305;&#23450;&#30340;&#65289;&#23398;&#20064;&#38899;&#39057;&#23884;&#20837;&#12290;&#23545;&#20110;&#28041;&#21450;&#25968;&#30334;&#31181;&#22768;&#38899;&#20998;&#31867;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23398;&#20064;&#20998;&#21035;&#38024;&#23545;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#31561;&#22810;&#26679;&#30340;&#38899;&#39057;&#23646;&#24615;&#24314;&#31435;&#31283;&#20581;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#36890;&#36807;&#31471;&#21040;&#31471;&#26550;&#26500;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25163;&#24037;&#21046;&#20316;&#30340;&#23884;&#20837;&#65292;&#20363;&#22914;&#22522;&#20110;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#23884;&#20837;&#65292;&#34429;&#28982;&#21333;&#29420;&#20351;&#29992;&#26102;&#26080;&#27861;&#20987;&#36133;&#23436;&#20840;&#31471;&#21040;&#31471;&#30340;&#34920;&#31034;&#65292;&#20294;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#31471;&#21040;&#31471;&#23884;&#20837;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#22312;&#31471;&#21040;&#31471;&#27169;&#22411;&#20013;&#24341;&#20837;&#19968;&#20123;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#36947;&#36335;&#65292;&#24182;&#36229;&#36234;&#20165;&#35757;&#32451;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08748</link><description>&lt;p&gt;
Wasserstein&#20998;&#24067;&#20445;&#35777;&#30340;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#22312;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08748
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;Wasserstein&#36317;&#31163;&#32780;&#19981;&#26159;KL&#25955;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#20048;&#38431;&#20013;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#26368;&#22351;&#24773;&#20917;&#19979;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#19982;&#29615;&#22659;&#30452;&#25509;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#25910;&#38598;&#30340;&#29615;&#22659;&#36890;&#24120;&#19982;&#23398;&#20064;&#30340;&#31574;&#30053;&#24212;&#29992;&#30340;&#29615;&#22659;&#19981;&#21516;&#12290;&#20026;&#20102;&#22312;&#23398;&#20064;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#32771;&#34385;&#19981;&#21516;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#26032;&#22411;&#20998;&#24067;&#20445;&#35777;&#20248;&#21270;(DRO)&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#20551;&#35774;&#26032;&#29615;&#22659;&#30340;&#20998;&#24067;&#20301;&#20110;&#19981;&#30830;&#23450;&#38598;&#21512;&#20869;&#26102;&#65292;&#35745;&#31639;&#31574;&#30053;&#20540;&#30340;&#26368;&#22351;&#24773;&#20917;&#19979;&#30028;&#12290;&#20856;&#22411;&#22320;&#65292;&#36825;&#20010;&#19981;&#30830;&#23450;&#38598;&#21512;&#26159;&#22522;&#20110;&#20174;&#26085;&#24535;&#25968;&#25454;&#38598;&#20013;&#35745;&#31639;&#30340;&#32463;&#39564;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;KL&#19981;&#30830;&#23450;&#38598;&#21512;&#26080;&#27861;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#25903;&#25345;&#30340;&#20998;&#24067;&#65292;&#20063;&#32570;&#20047;&#23545;&#20998;&#24067;&#25903;&#25345;&#30340;&#20960;&#20309;&#24863;&#30693;&#12290;&#32467;&#26524;&#65292;KL&#26041;&#27861;&#22312;&#35299;&#20915;&#23454;&#38469;&#29615;&#22659;&#19981;&#21305;&#37197;&#21644;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#26368;&#22351;&#24773;&#20917;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#30340;&#26032;&#22411;DRO&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;AlbNER&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#24433;&#21709;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#26377;&#30528;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#36164;&#28304;&#21644;&#32467;&#26524;&#20026;&#26410;&#26469;&#23454;&#39564;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2309.08741</link><description>&lt;p&gt;
AlbNER&#65306;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AlbNER: A Corpus for Named Entity Recognition in Albanian. (arXiv:2309.08741v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#23572;&#24052;&#23612;&#20122;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;AlbNER&#65292;&#35813;&#35821;&#26009;&#24211;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#24433;&#21709;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#26377;&#30528;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#36164;&#28304;&#21644;&#32467;&#26524;&#20026;&#26410;&#26469;&#23454;&#39564;&#25552;&#20379;&#20102;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#38463;&#23572;&#24052;&#23612;&#20122;&#31561;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65292;&#22914;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#23384;&#22312;&#30528;&#26631;&#27880;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#20005;&#37325;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AlbNER&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#38463;&#23572;&#24052;&#23612;&#20122;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#20013;&#25910;&#38598;&#30340;900&#20010;&#24102;&#26377;&#26631;&#35760;&#21629;&#21517;&#23454;&#20307;&#30340;&#21477;&#23376;&#30340;&#35821;&#26009;&#24211;&#12290;&#29992;&#20351;&#29992;AlbNER&#25968;&#25454;&#36827;&#34892;&#32454;&#35843;&#21644;&#27979;&#35797;&#30340;BERT&#21644;RoBERTa&#21464;&#20307;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#22823;&#23567;&#23545;NER&#24615;&#33021;&#30340;&#24433;&#21709;&#24456;&#23567;&#65292;&#32780;&#35821;&#35328;&#36801;&#31227;&#30340;&#24433;&#21709;&#24456;&#22823;&#12290;AlbNER&#35821;&#26009;&#24211;&#21644;&#36825;&#20123;&#32467;&#26524;&#24212;&#20316;&#20026;&#26410;&#26469;&#23454;&#39564;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scarcity of resources such as annotated text corpora for under-resourced languages like Albanian is a serious impediment in computational linguistics and natural language processing research. This paper presents AlbNER, a corpus of 900 sentences with labeled named entities, collected from Albanian Wikipedia articles. Preliminary results with BERT and RoBERTa variants fine-tuned and tested with AlbNER data indicate that model size has slight impact on NER performance, whereas language transfer has a significant one. AlbNER corpus and these obtained results should serve as baselines for future experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Testing with Concept Activation Vectors (TCAV)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26893;&#29289;&#30142;&#30149;&#20998;&#31867;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08739</link><description>&lt;p&gt;
&#26893;&#29289;&#30142;&#30149;&#20998;&#31867;&#30340;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Concept explainability for plant diseases classification. (arXiv:2309.08739v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Testing with Concept Activation Vectors (TCAV)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26893;&#29289;&#30142;&#30149;&#20998;&#31867;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#29289;&#30142;&#30149;&#23545;&#39135;&#21697;&#23433;&#20840;&#21644;&#20892;&#19994;&#21487;&#25345;&#32493;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#23041;&#32961;&#12290;&#24555;&#36895;&#21644;&#26089;&#26399;&#35782;&#21035;&#36825;&#20123;&#30142;&#30149;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#36825;&#25512;&#21160;&#20102;&#20960;&#39033;&#30740;&#31350;&#20381;&#36182;&#20110;&#20840;&#29699;&#25968;&#23383;&#21270;&#30340;&#22686;&#38271;&#21644;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20107;&#23454;&#19978;&#65292;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26893;&#29289;&#30142;&#30149;&#20998;&#31867;&#24050;&#32463;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#12289;&#36879;&#26126;&#24615;&#20197;&#21450;&#19982;&#20154;&#24037;&#19987;&#23478;&#23545;&#29031;&#30456;&#27604;&#32570;&#20047;&#35299;&#37322;&#24615;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#26041;&#27861;&#23578;&#26410;&#20840;&#29699;&#37319;&#29992;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#32593;&#32476;&#36755;&#20986;&#19982;&#36755;&#20837;&#20687;&#32032;&#30340;&#25200;&#21160;&#30456;&#20851;&#32852;&#65292;&#20197;&#25552;&#20379;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#27934;&#23519;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20154;&#31867;&#29992;&#25143;&#26469;&#35828;&#24182;&#19981;&#23481;&#26131;&#29702;&#35299;&#21644;&#30452;&#35266;&#65292;&#24182;&#19988;&#21463;&#21040;&#20559;&#35265;&#30340;&#23041;&#32961;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Testing with Concept Activation Vectors (TCAV)&#30340;&#26041;&#27861;&#65292;&#26469;&#25913;&#21464;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plant diseases remain a considerable threat to food security and agricultural sustainability. Rapid and early identification of these diseases has become a significant concern motivating several studies to rely on the increasing global digitalization and the recent advances in computer vision based on deep learning. In fact, plant disease classification based on deep convolutional neural networks has shown impressive performance. However, these methods have yet to be adopted globally due to concerns regarding their robustness, transparency, and the lack of explainability compared with their human experts counterparts. Methods such as saliency-based approaches associating the network output to perturbations of the input pixels have been proposed to give insights into these algorithms. Still, they are not easily comprehensible and not intuitive for human users and are threatened by bias. In this work, we deploy a method called Testing with Concept Activation Vectors (TCAV) that shifts th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#26080;&#32447;&#30005;&#24863;&#30693;&#26041;&#27861;&#30340;&#21069;&#21521;&#30896;&#25758;&#35686;&#21578;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20998;&#26512;&#27874;&#24418;&#20449;&#21495;&#19978;&#30340;&#22810;&#26222;&#21202;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#21069;&#26041;&#26469;&#36710;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.08737</link><description>&lt;p&gt;
&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#20998;&#25955;&#24335;&#26080;&#32447;&#30005;&#24863;&#30693;&#30340;&#21069;&#21521;&#30896;&#25758;&#35686;&#21578;&#31995;&#32479;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Experimental Assessment of a Forward-Collision Warning System Fusing Deep Learning and Decentralized Radio Sensing. (arXiv:2309.08737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08737
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#26080;&#32447;&#30005;&#24863;&#30693;&#26041;&#27861;&#30340;&#21069;&#21521;&#30896;&#25758;&#35686;&#21578;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20998;&#26512;&#27874;&#24418;&#20449;&#21495;&#19978;&#30340;&#22810;&#26222;&#21202;&#29305;&#24449;&#65292;&#23454;&#29616;&#23545;&#21069;&#26041;&#26469;&#36710;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#35813;&#31995;&#32479;&#22312;&#39640;&#36895;&#20844;&#36335;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25955;&#24335;&#26080;&#32447;&#30005;&#24863;&#30693;&#26041;&#27861;&#30340;&#33258;&#21160;&#21069;&#21521;&#30896;&#25758;&#35686;&#21578;&#31995;&#32479;&#30340;&#24605;&#24819;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25509;&#25910;&#27169;&#24335;&#19979;&#30340;&#19968;&#36742;&#36710;&#20351;&#29992;&#31532;&#20108;&#36742;&#36710;&#21457;&#23556;&#30340;&#36830;&#32493;&#27874;&#24418;&#20316;&#20026;&#25506;&#27979;&#20449;&#21495;&#65292;&#20197;&#26816;&#27979;&#21069;&#26041;&#26469;&#36710;&#24182;&#25552;&#37266;&#39550;&#39542;&#21592;&#21487;&#33021;&#20250;&#21457;&#29983;&#30896;&#25758;&#12290;&#36825;&#31181;&#36830;&#32493;&#27874;&#24418;&#21487;&#20197;&#23481;&#26131;&#22320;&#20316;&#20026;&#23548;&#39057;&#20449;&#21495;&#23884;&#20837;&#21040;&#24403;&#21069;&#22810;&#36733;&#27874;&#36710;&#36733;&#36890;&#20449;&#31995;&#32479;&#30340;&#25968;&#25454;&#24103;&#20013;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#20998;&#26512;&#24555;&#36895;&#25509;&#36817;&#30340;&#36710;&#36742;&#25152;&#21360;&#21051;&#22312;&#36830;&#32493;&#27874;&#24418;&#25506;&#27979;&#20449;&#21495;&#19978;&#30340;&#22810;&#26222;&#21202;&#29305;&#24449;&#26469;&#23454;&#29616;&#23545;&#21069;&#26041;&#26469;&#36710;&#30340;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#19968;&#26465;&#21452;&#36710;&#36947;&#39640;&#36895;&#20844;&#36335;&#19978;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#29616;&#22330;&#35797;&#39564;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#23545;&#36825;&#31181;&#20998;&#25955;&#24335;&#36830;&#32493;&#27874;&#24418;&#26080;&#32447;&#30005;&#24863;&#30693;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#27979;&#24615;&#33021;&#35780;&#20272;&#65306;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This paper presents the idea of an automatic forward-collision warning system based on a decentralized radio sensing (RS) approach. In this framework, a vehicle in receiving mode employs a continuous waveform (CW) transmitted by a second vehicle as a probe signal to detect oncoming vehicles and warn the driver of a potential forward collision. Such a CW can easily be incorporated as a pilot signal within the data frame of current multicarrier vehicular communication systems. Detection of oncoming vehicles is performed by a deep learning (DL) module that analyzes the features of the Doppler signature imprinted on the CW probe signal by a rapidly approaching vehicle. This decentralized CW RS approach was assessed experimentally using data collected by a series of field trials conducted in a two-lanes high-speed highway. Detection performance was evaluated for two different DL models: a long short-term memory network and a convolutional neural network. The obtained results demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08731</link><description>&lt;p&gt;
&#25351;&#24341;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#25913;&#36827;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights. (arXiv:2309.08731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#12290;&#34429;&#28982;&#30446;&#21069;&#23450;&#20301;&#30340;&#25216;&#26415;&#27700;&#24179;&#26159;&#23558;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#26159;&#38647;&#36798;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23545;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#20855;&#26377;&#26356;&#24378;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#65292;&#21516;&#26102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#20445;&#25345;&#24615;&#33021;&#65292;&#23558;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38647;&#36798;&#27979;&#37327;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#20266;&#24433;&#65292;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#19968;&#30452;&#38590;&#20197;&#36798;&#21040;&#19982;&#28608;&#20809;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#24037;&#20316;&#22312;&#22522;&#20110;ICP&#30340;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#31995;&#32479;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26681;&#25454;&#39640;&#23618;&#27425;&#30340;&#25195;&#25551;&#20449;&#24687;&#23545;&#38647;&#36798;&#28857;&#36827;&#34892;&#21152;&#26435;&#12290;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20998;&#26512;&#26041;&#27861;&#19982;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#20943;&#23567;&#20102;&#38647;&#36798;&#23450;&#20301;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. Although the state of the art for localization is matching lidar data to lidar maps, radar has been considered as a promising alternative, as it is potentially more resilient against adverse weather such as precipitation and heavy fog. To make use of existing high-quality lidar maps, while maintaining performance in adverse weather, matching radar data to lidar maps is of interest. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on an ICP-based radar-lidar localization system by including a learned preprocessing step that weights radar points based on high-level scan information. Combining a proven analytical approach with a learned weight reduces localization errors in rad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#21152;&#36895;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#32858;&#31867;&#36136;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08710</link><description>&lt;p&gt;
&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Clustered Multi-Agent Linear Bandits. (arXiv:2309.08710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#21152;&#36895;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#32858;&#31867;&#36136;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#26469;&#21152;&#36895;&#25972;&#20307;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#36129;&#29486;&#20013;&#65292;&#32593;&#32476;&#25511;&#21046;&#22120;&#36127;&#36131;&#20272;&#35745;&#32593;&#32476;&#30340;&#22522;&#26412;&#38598;&#32676;&#32467;&#26500;&#24182;&#20248;&#21270;&#21516;&#19968;&#32452;&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#32463;&#39564;&#20998;&#20139;&#12290;&#25105;&#20204;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#21644;&#32858;&#31867;&#36136;&#37327;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#19982;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#25105;&#20204;&#30340;&#31639;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;&#30495;&#23454;&#30340;&#22522;&#26412;&#38598;&#32676;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address in this paper a particular instance of the multi-agent linear stochastic bandit problem, called clustered multi-agent linear bandits. In this setting, we propose a novel algorithm leveraging an efficient collaboration between the agents in order to accelerate the overall optimization problem. In this contribution, a network controller is responsible for estimating the underlying cluster structure of the network and optimizing the experiences sharing among agents within the same groups. We provide a theoretical analysis for both the regret minimization problem and the clustering quality. Through empirical evaluation against state-of-the-art algorithms on both synthetic and real data, we demonstrate the effectiveness of our approach: our algorithm significantly improves regret minimization while managing to recover the true underlying cluster partitioning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#21453;&#39304;&#30340;&#23433;&#20840;&#26368;&#20248;&#33218;&#35782;&#21035;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#32467;&#26500;&#26469;&#20445;&#35777;&#22312;&#27599;&#19968;&#36718;&#20013;&#19981;&#36829;&#21453;&#38454;&#27573;&#24615;&#23433;&#20840;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38388;&#38553;&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08709</link><description>&lt;p&gt;
&#32447;&#24615;&#26368;&#20248;&#33218;&#35782;&#21035;&#20013;&#30340;&#23433;&#20840;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
Price of Safety in Linear Best Arm Identification. (arXiv:2309.08709v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#32447;&#24615;&#21453;&#39304;&#30340;&#23433;&#20840;&#26368;&#20248;&#33218;&#35782;&#21035;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#32467;&#26500;&#26469;&#20445;&#35777;&#22312;&#27599;&#19968;&#36718;&#20013;&#19981;&#36829;&#21453;&#38454;&#27573;&#24615;&#23433;&#20840;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38388;&#38553;&#30340;&#31639;&#27861;&#26469;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#32447;&#24615;&#21453;&#39304;&#30340;&#23433;&#20840;&#26368;&#20248;&#33218;&#35782;&#21035;&#26694;&#26550;&#65292;&#20854;&#20013;&#20195;&#29702;&#21463;&#21040;&#19968;&#20123;&#38454;&#27573;&#24615;&#23433;&#20840;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#35813;&#38480;&#21046;&#32447;&#24615;&#22320;&#20381;&#36182;&#20110;&#26410;&#30693;&#30340;&#21442;&#25968;&#21521;&#37327;&#12290;&#20195;&#29702;&#24517;&#39035;&#20197;&#20445;&#23432;&#30340;&#26041;&#24335;&#37319;&#21462;&#34892;&#21160;&#65292;&#20197;&#30830;&#20445;&#22312;&#27599;&#19968;&#36718;&#20013;&#19981;&#20250;&#39640;&#27010;&#29575;&#36829;&#21453;&#23433;&#20840;&#32422;&#26463;&#12290;&#24050;&#32463;&#30740;&#31350;&#20102;&#21033;&#29992;&#32447;&#24615;&#32467;&#26500;&#26469;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#30740;&#31350;&#22312;&#26368;&#20248;&#33218;&#35782;&#21035;&#20013;&#24212;&#29992;&#35813;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38388;&#38553;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30830;&#20445;&#38454;&#27573;&#24615;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#26377;&#24847;&#20041;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#30001;&#20110;&#39069;&#22806;&#30340;&#23433;&#20840;&#32422;&#26463;&#23548;&#33268;&#30340;&#24378;&#21046;&#25506;&#32034;&#38454;&#27573;&#65292;&#25105;&#20204;&#22312;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#20184;&#20986;&#20102;&#39069;&#22806;&#30340;&#20195;&#20215;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#35828;&#26126;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#31639;&#27861;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the safe best-arm identification framework with linear feedback, where the agent is subject to some stage-wise safety constraint that linearly depends on an unknown parameter vector. The agent must take actions in a conservative way so as to ensure that the safety constraint is not violated with high probability at each round. Ways of leveraging the linear structure for ensuring safety has been studied for regret minimization, but not for best-arm identification to the best our knowledge. We propose a gap-based algorithm that achieves meaningful sample complexity while ensuring the stage-wise safety. We show that we pay an extra term in the sample complexity due to the forced exploration phase incurred by the additional safety constraint. Experimental illustrations are provided to justify the design of our algorithm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#20998;&#24067;&#40065;&#26834;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#38887;&#24615;&#65292;&#24182;&#20445;&#25345;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20272;&#35745;&#26465;&#20214;&#39118;&#38505;&#26469;&#34913;&#37327;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2309.08700</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#21487;&#24494;&#20984;&#35268;&#21010;&#30340;&#26465;&#20214;&#39118;&#38505;&#30340;Wasserstein&#20998;&#24067;&#40065;&#26834;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Distributionally Robust Control Barrier Function using Conditional Value-at-Risk with Differentiable Convex Programming. (arXiv:2309.08700v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#20998;&#24067;&#40065;&#26834;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#38887;&#24615;&#65292;&#24182;&#20445;&#25345;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20272;&#35745;&#26465;&#20214;&#39118;&#38505;&#26469;&#34913;&#37327;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#23433;&#20840;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBFs&#65289;&#22312;&#35774;&#35745;&#23433;&#20840;&#25511;&#21046;&#22120;&#20013;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20197;&#22312;&#23454;&#38469;&#30340;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#21608;&#22260;&#29615;&#22659;&#30340;&#24863;&#30693;&#36890;&#24120;&#21463;&#21040;&#38543;&#26426;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21487;&#33021;&#36827;&#19968;&#27493;&#20559;&#31163;&#21517;&#20041;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#24067;&#40065;&#26834;CBF&#65288;DR-CBF&#65289;&#65292;&#20197;&#23454;&#29616;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#38887;&#24615;&#65292;&#24182;&#20445;&#25345;CBF&#30340;&#20248;&#21183;&#65292;&#22914;&#35745;&#31639;&#25928;&#29575;&#21644;&#27491;&#21521;&#19981;&#21464;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#21333;&#23618;&#20984;&#37325;&#32452;&#65292;&#29992;&#20110;&#20272;&#35745;&#20998;&#24067;&#20559;&#31227;&#19979;&#29992;Wasserstein&#24230;&#37327;&#34913;&#37327;&#30340;&#23433;&#20840;&#32422;&#26463;&#30340;&#26465;&#20214;&#39118;&#38505;&#65288;CVaR&#65289;&#65292;&#36825;&#26412;&#36136;&#19978;&#26159;&#19977;&#23618;&#35268;&#21010;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#26500;&#24314;&#25511;&#21046;&#23631;&#38556;&#26465;&#20214;&#20197;&#24378;&#21046;&#23454;&#29616;CVaR&#30340;&#27491;&#21521;&#19981;&#21464;&#24615;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#21487;&#24494;&#20984;&#35268;&#21010;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;CVaR&#20272;&#35745;&#30340;&#20248;&#21270;&#23618;&#20869;&#30340;&#24494;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Control Barrier functions (CBFs) have attracted extensive attention for designing safe controllers for their deployment in real-world safety-critical systems. However, the perception of the surrounding environment is often subject to stochasticity and further distributional shift from the nominal one. In this paper, we present distributional robust CBF (DR-CBF) to achieve resilience under distributional shift while keeping the advantages of CBF, such as computational efficacy and forward invariance.  To achieve this goal, we first propose a single-level convex reformulation to estimate the conditional value at risk (CVaR) of the safety constraints under distributional shift measured by a Wasserstein metric, which is by nature tri-level programming. Moreover, to construct a control barrier condition to enforce the forward invariance of the CVaR, the technique of differentiable convex programming is applied to enable differentiation through the optimization layer of CVaR estimation. We a
&lt;/p&gt;</description></item><item><title>SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2309.08698</link><description>&lt;p&gt;
&#26080;&#38656;&#25554;&#20540;&#30340;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Modelling Irregularly Sampled Time Series Without Imputation. (arXiv:2309.08698v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08698
&lt;/p&gt;
&lt;p&gt;
SLAN&#26159;&#19968;&#31181;&#26080;&#38656;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#24314;&#27169;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65292;&#21033;&#29992;&#21160;&#24577;&#36866;&#24212;&#30340;LSTM&#26550;&#26500;&#26469;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#65288;ISTS&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#32570;&#22833;&#20540;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#23558;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36716;&#25442;&#20026;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#26469;&#22788;&#29702;ISTS&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20551;&#35774;&#23384;&#22312;&#28508;&#22312;&#30340;&#32570;&#22833;&#26426;&#21046;&#65292;&#23548;&#33268;&#20102;&#19981;&#24076;&#26395;&#30340;&#20559;&#24046;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SLAN&#65288;Switch LSTM Aggregate Network&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19968;&#32452;LSTM&#23545;ISTS&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#26080;&#38656;&#25554;&#20540;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#28508;&#22312;&#36807;&#31243;&#30340;&#20551;&#35774;&#12290;&#23427;&#26681;&#25454;&#27979;&#37327;&#20256;&#24863;&#22120;&#21160;&#24577;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#20854;&#26550;&#26500;&#12290;SLAN&#21033;&#29992;&#19981;&#35268;&#21017;&#24615;&#20449;&#24687;&#26126;&#30830;&#25429;&#25417;&#27599;&#20010;&#20256;&#24863;&#22120;&#30340;&#23616;&#37096;&#25688;&#35201;&#65292;&#24182;&#22312;&#25972;&#20010;&#35266;&#27979;&#26399;&#38388;&#32500;&#25345;&#19968;&#20010;&#20840;&#23616;&#25688;&#35201;&#29366;&#24577;&#12290;&#25105;&#20204;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;SLAN&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;MIMIC-III&#12289;Physionet 2012&#21644;Physionet 2019&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/Rohit102497/SLAN&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modelling irregularly-sampled time series (ISTS) is challenging because of missing values. Most existing methods focus on handling ISTS by converting irregularly sampled data into regularly sampled data via imputation. These models assume an underlying missing mechanism leading to unwanted bias and sub-optimal performance. We present SLAN (Switch LSTM Aggregate Network), which utilizes a pack of LSTMs to model ISTS without imputation, eliminating the assumption of any underlying process. It dynamically adapts its architecture on the fly based on the measured sensors. SLAN exploits the irregularity information to capture each sensor's local summary explicitly and maintains a global summary state throughout the observational period. We demonstrate the efficacy of SLAN on publicly available datasets, namely, MIMIC-III, Physionet 2012 and Physionet 2019. The code is available at https://github.com/Rohit102497/SLAN.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#24448;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#26032;&#30340;&#27861;&#24237;&#21028;&#20915;&#26631;&#27880;&#25968;&#25454;&#29992;&#20110;&#25913;&#36827;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;86.7&#65285;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#12290;</title><link>http://arxiv.org/abs/2309.08695</link><description>&lt;p&gt;
&#35299;&#20915;&#27861;&#24459;&#26415;&#35821;&#65306;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#22810;&#35821;&#35328;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents. (arXiv:2309.08695v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#35821;&#35328;&#25506;&#32034;&#65292;&#35299;&#20915;&#20102;&#27861;&#24459;&#25991;&#20214;&#20013;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20197;&#24448;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#26032;&#30340;&#27861;&#24237;&#21028;&#20915;&#26631;&#27880;&#25968;&#25454;&#29992;&#20110;&#25913;&#36827;&#35299;&#26512;&#25928;&#26524;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;86.7&#65285;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21477;&#23376;&#20013;&#35299;&#26512;&#21542;&#23450;&#30340;&#33539;&#22260;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#32570;&#20047;&#32463;&#36807;&#27880;&#37322;&#30340;&#39046;&#22495;&#20869;&#21542;&#23450;&#35821;&#26009;&#24211;&#32473;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#35821;&#35328;&#27861;&#24459;&#25968;&#25454;&#19978;&#30340;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#26102;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#39044;&#20808;&#26410;&#20351;&#29992;&#27861;&#24459;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#20165;&#22312;&#25991;&#23398;&#25991;&#26412;&#21644;&#21307;&#23398;&#25968;&#25454;&#31561;&#39046;&#22495;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#19982;&#20043;&#21069;&#30340;&#36328;&#39046;&#22495;&#23454;&#39564;&#20013;&#35760;&#24405;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#22871;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#30340;&#26631;&#27880;&#27861;&#38498;&#21028;&#20915;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#25913;&#36827;&#38646;&#25668;&#21462;&#21644;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#21542;&#23450;&#33539;&#22260;&#35299;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#38646;&#25668;&#21462;&#36328;&#35821;&#35328;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26631;&#35760;&#32423;F1&#20998;&#36798;&#21040;&#20102;86.7&#65285;&#65292;&#20854;&#20013;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#27861;&#24459;&#25968;&#25454;&#38598;&#30340;&#20004;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#22312;&#31532;&#19977;&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resolving the scope of a negation within a sentence is a challenging NLP task. The complexity of legal texts and the lack of annotated in-domain negation corpora pose challenges for state-of-the-art (SotA) models when performing negation scope resolution on multilingual legal data. Our experiments demonstrate that models pre-trained without legal data underperform in the task of negation scope resolution. Our experiments, using language models exclusively fine-tuned on domains like literary texts and medical data, yield inferior results compared to the outcomes documented in prior cross-domain experiments. We release a new set of annotated court decisions in German, French, and Italian and use it to improve negation scope resolution in both zero-shot and multilingual settings. We achieve token-level F1-scores of up to 86.7% in our zero-shot cross-lingual experiments, where the models are trained on two languages of our legal datasets and evaluated on the third. Our multilingual experim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#35780;&#20272;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#23545;&#25928;&#29992;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25968;&#25454;&#31649;&#29702;&#21592;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#30340;&#38544;&#31169;&#21442;&#25968;&#20540;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.08678</link><description>&lt;p&gt;
&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#35780;&#20272;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#23545;&#25928;&#29992;&#25439;&#22833;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions. (arXiv:2309.08678v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#35780;&#20272;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#23545;&#25928;&#29992;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25968;&#25454;&#31649;&#29702;&#21592;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#30340;&#38544;&#31169;&#21442;&#25968;&#20540;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#22823;&#37327;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#27491;&#30830;&#35774;&#32622;&#24046;&#20998;&#38544;&#31169;&#20013;&#30340;&#38544;&#31169;&#21442;&#25968;&#26159;&#33258;&#20174;2006&#24180;&#39318;&#27425;&#25552;&#20986;&#24046;&#20998;&#38544;&#31169;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24433;&#21709;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25581;&#31034;&#22312;&#22522;&#20110;&#38543;&#26426;&#21709;&#24212;&#30340;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#35774;&#32622;&#19979;&#65292;&#29305;&#23450;&#38544;&#31169;&#21442;&#25968;&#20540;&#23558;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#27979;&#35797;&#25439;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20801;&#35768;&#25968;&#25454;&#31649;&#29702;&#21592;&#36873;&#25321;&#19982;&#20854;&#20801;&#35768;&#30340;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#26368;&#31526;&#21512;&#30340;&#38544;&#31169;&#21442;&#25968;&#20540;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#32321;&#37325;&#30340;&#35745;&#31639;&#65292;&#22914;&#22823;&#37327;&#27169;&#22411;&#37325;&#35757;&#32451;&#21644;&#25968;&#25454;&#31169;&#26377;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#31181;&#24120;&#35265;&#30340;&#38543;&#26426;&#21270;&#22330;&#26223;&#65292;&#20363;&#22914;&#23545;&#29305;&#24449;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#65292;&#23545;&#26631;&#31614;&#36827;&#34892;&#38543;&#26426;&#21709;&#24212;&#65292;&#20197;&#21450;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;&#31867;&#21035;&#30340;&#26631;&#31614;&#22122;&#22768;&#26657;&#27491;&#26041;&#27861;&#26469;&#25269;&#28040;&#38543;&#26426;&#21270;&#36896;&#25104;&#30340;&#22122;&#22768;&#30340;&#26356;&#22797;&#26434;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35814;&#32454;&#35752;&#35770;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21253;&#25324;&#32463;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to properly set the privacy parameter in differential privacy (DP) has been an open question in DP research since it was first proposed in 2006. In this work, we demonstrate the ability of influence functions to offer insight into how a specific privacy parameter value will affect a model's test loss in the randomized response-based local DP setting. Our proposed method allows a data curator to select the privacy parameter best aligned with their allowed privacy-utility trade-off without requiring heavy computation such as extensive model retraining and data privatization. We consider multiple common randomization scenarios, such as performing randomized response over the features, and/or over the labels, as well as the more complex case of applying a class-dependent label noise correction method to offset the noise incurred by randomization. Further, we provide a detailed discussion over the computational complexity of our proposed approach inclusive of an empirical analysis. Thro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#37329;&#34701;&#30456;&#20851;&#30697;&#38453;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#24433;&#21709;&#25237;&#36164;&#32452;&#21512;&#22810;&#20803;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.08652</link><description>&lt;p&gt;
&#29992;&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks. (arXiv:2309.08652v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#37329;&#34701;&#30456;&#20851;&#30697;&#38453;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#24433;&#21709;&#25237;&#36164;&#32452;&#21512;&#22810;&#20803;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#37329;&#34701;&#30456;&#20851;&#30697;&#38453;&#65292;&#37327;&#21270;&#20449;&#36151;&#32452;&#21512;&#20215;&#20540;&#39118;&#38505;&#65288;VaR&#65289;&#23545;&#36164;&#20135;&#30456;&#20851;&#24615;&#30340;&#25935;&#24863;&#24615;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#28436;&#31034;&#20102;&#29983;&#25104;&#33021;&#25429;&#25417;&#21040;&#36164;&#20135;&#25910;&#30410;&#30340;&#32463;&#39564;&#30456;&#20851;&#30697;&#38453;&#20013;&#35266;&#23519;&#21040;&#30340;&#22522;&#26412;&#29305;&#24449;&#30340;&#21487;&#20449;&#30456;&#20851;&#30697;&#38453;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#32780;&#19981;&#26159;GANs&#65292;&#20197;&#23454;&#29616;&#26356;&#21487;&#35299;&#37322;&#30340;&#28508;&#22312;&#31354;&#38388;&#34920;&#31034;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;VAE&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25104;&#20026;&#25429;&#25417;&#24433;&#21709;&#25237;&#36164;&#32452;&#21512;&#22810;&#20803;&#21270;&#30340;&#20851;&#38190;&#22240;&#32032;&#30340;&#26377;&#29992;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#19982;&#20449;&#36151;&#32452;&#21512;&#25935;&#24863;&#24615;&#21644;&#36164;&#20135;&#30456;&#20851;&#24615;&#21464;&#21270;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this research, we propose a novel approach for the quantification of credit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the use of synthetic financial correlation matrices generated with deep learning models. In previous work Generative Adversarial Networks (GANs) were employed to demonstrate the generation of plausible correlation matrices, that capture the essential characteristics observed in empirical correlation matrices estimated on asset returns. Instead of GANs, we employ Variational Autoencoders (VAE) to achieve a more interpretable latent space representation. Through our analysis, we reveal that the VAE latent space can be a useful tool to capture the crucial factors impacting portfolio diversification, particularly in relation to credit portfolio sensitivity to asset correlations changes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.08646</link><description>&lt;p&gt;
&#36890;&#36807;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#35299;&#20915;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08646
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20849;&#32447;&#32422;&#26463;&#27880;&#24847;&#21147;&#65288;CoCA&#65289;&#32467;&#26500;&#65292;&#35299;&#20915;Transformer&#27169;&#22411;&#20013;&#30340;&#22836;&#30171;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20986;&#33394;&#30340;&#22806;&#25512;&#24615;&#33021;&#21644;&#25552;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#25512;&#26029;&#24615;&#33021;&#30340;&#22806;&#25512;&#21464;&#24471;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;Transformer&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#34987;&#20043;&#21069;&#24573;&#35270;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;&#23548;&#33268;&#20102;&#26368;&#25509;&#36817;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#28151;&#20081;&#65292;&#36825;&#20123;&#26631;&#35760;&#25658;&#24102;&#20102;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#21457;&#29616;&#31216;&#20026;&#8220;Transformer&#30340;&#22836;&#30171;&#38382;&#39064;&#8221;&#12290;&#20026;&#20102;&#20174;&#26681;&#26412;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#32467;&#26500;&#65292;&#21629;&#21517;&#20026;Collinear Constrained Attention&#65288;CoCA&#65289;&#12290;&#36825;&#20010;&#32467;&#26500;&#21487;&#20197;&#26080;&#32541;&#22320;&#19982;&#29616;&#26377;&#30340;&#25512;&#26029;&#12289;&#25554;&#20540;&#26041;&#27861;&#21644;&#20854;&#20182;&#38024;&#23545;&#20256;&#32479;Transformer&#27169;&#22411;&#35774;&#35745;&#30340;&#20248;&#21270;&#31574;&#30053;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#22806;&#25512;&#24615;&#33021;&#65292;&#21363;&#20351;&#26159;16&#21040;24&#20493;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#32780;&#19988;&#27809;&#26377;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#12290;&#25105;&#20204;&#36824;&#22686;&#24378;&#20102;CoCA&#30340;&#35745;&#31639;&#21644;&#31354;&#38388;&#25928;&#29575;&#65292;&#20197;&#30830;&#20445;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;...
&lt;/p&gt;
&lt;p&gt;
As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#33021;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#21644;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#24494;&#35843;&#26469;&#35299;&#20915;&#25968;&#25454;&#27874;&#21160;&#12289;&#27169;&#22411;&#24046;&#24322;&#21644;&#29615;&#22659;&#25200;&#21160;&#31561;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.08642</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#34394;&#25311;&#30005;&#21147;&#21378;&#20013;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#33021;&#28304;&#35843;&#24230;&#30340;&#38543;&#26426;&#22312;&#32447;&#39044;&#27979;&#19982;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Stochastic Online Forecast-and-Optimize Framework for Real-Time Energy Dispatch in Virtual Power Plants under Uncertainty. (arXiv:2309.08642v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#24212;&#19981;&#30830;&#23450;&#24615;&#30340;&#23454;&#26102;&#33021;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#21644;&#38543;&#26426;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#27169;&#22411;&#24494;&#35843;&#26469;&#35299;&#20915;&#25968;&#25454;&#27874;&#21160;&#12289;&#27169;&#22411;&#24046;&#24322;&#21644;&#29615;&#22659;&#25200;&#21160;&#31561;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#32858;&#21512;&#20998;&#24067;&#24335;&#33021;&#28304;&#36164;&#28304;&#26174;&#33879;&#22686;&#21152;&#20102;&#19981;&#30830;&#23450;&#24615;&#65292;&#29305;&#21035;&#26159;&#30001;&#21487;&#20877;&#29983;&#33021;&#28304;&#20135;&#29983;&#30340;&#27874;&#21160;&#25152;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#39537;&#20351;&#20102;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#24191;&#27867;&#21033;&#29992;&#20808;&#36827;&#30340;&#39044;&#27979;&#25511;&#21046;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#38271;&#26399;&#32463;&#27982;&#21644;&#20943;&#30899;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#26102;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#33021;&#28304;&#35843;&#24230;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#30001;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#32452;&#25104;&#65306;(i) &#19968;&#20010;&#28151;&#21512;&#30340;&#39044;&#27979;&#21644;&#20248;&#21270;&#39034;&#24207;&#20219;&#21153;&#65292;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#21644;&#38543;&#26426;&#20248;&#21270;&#36827;&#34892;&#25972;&#21512;&#65292;&#36825;&#20004;&#20010;&#38454;&#27573;&#36890;&#36807;&#22810;&#20010;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#36827;&#34892;&#36830;&#25509;&#65307;(ii) &#19968;&#31181;&#39640;&#25928;&#30340;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#21516;&#26102;&#28041;&#21450;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#22312;&#32447;&#24494;&#35843;&#38454;&#27573;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#36805;&#36895;&#36866;&#24212;&#23454;&#26102;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#38024;&#23545;&#25968;&#25454;&#28418;&#31227;&#12289;&#27169;&#22411;&#24046;&#24322;&#21644;&#29615;&#22659;&#25200;&#21160;&#24341;&#36215;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aggregating distributed energy resources in power systems significantly increases uncertainties, in particular caused by the fluctuation of renewable energy generation. This issue has driven the necessity of widely exploiting advanced predictive control techniques under uncertainty to ensure long-term economics and decarbonization. In this paper, we propose a real-time uncertainty-aware energy dispatch framework, which is composed of two key elements: (i) A hybrid forecast-and-optimize sequential task, integrating deep learning-based forecasting and stochastic optimization, where these two stages are connected by the uncertainty estimation at multiple temporal resolutions; (ii) An efficient online data augmentation scheme, jointly involving model pre-training and online fine-tuning stages. In this way, the proposed framework is capable to rapidly adapt to the real-time data distribution, as well as to target on uncertainties caused by data drift, model discrepancy and environment pertu
&lt;/p&gt;</description></item><item><title>FedFNN&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#21152;&#36895;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#26410;&#25277;&#26679;&#29992;&#25143;&#30340;&#26435;&#37325;&#26356;&#26032;&#65292;&#20351;&#29992;&#24050;&#25277;&#26679;&#38598;&#30340;&#26356;&#26032;&#65292;FedFNN&#23454;&#29616;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;5&#20493;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08635</link><description>&lt;p&gt;
FedFNN: &#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#36890;&#36807;&#26356;&#26032;&#39044;&#27979;&#23454;&#29616;&#26356;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
FedFNN: Faster Training Convergence Through Update Predictions in Federated Recommender Systems. (arXiv:2309.08635v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08635
&lt;/p&gt;
&lt;p&gt;
FedFNN&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#25512;&#33616;&#31995;&#32479;&#20013;&#21152;&#36895;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#26410;&#25277;&#26679;&#29992;&#25143;&#30340;&#26435;&#37325;&#26356;&#26032;&#65292;&#20351;&#29992;&#24050;&#25277;&#26679;&#38598;&#30340;&#26356;&#26032;&#65292;FedFNN&#23454;&#29616;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#24555;5&#20493;&#30340;&#35757;&#32451;&#36895;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#25104;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#22312;&#32447;&#20010;&#24615;&#21270;&#30340;&#21516;&#26102;&#30830;&#20445;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#23558;&#31169;&#26377;&#25968;&#25454;&#21457;&#36865;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#19981;&#21516;&#65292;FL&#23558;&#35745;&#31639;&#20998;&#25955;&#65306;&#35774;&#22791;&#22312;&#26412;&#22320;&#35757;&#32451;&#24182;&#19982;&#20840;&#23616;&#26381;&#21153;&#22120;&#20849;&#20139;&#26356;&#26032;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20027;&#35201;&#25361;&#25112;&#26159;&#23454;&#29616;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;&#27169;&#22411;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#25512;&#33616;&#31995;&#32479;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#24310;&#36831;&#21487;&#33021;&#20250;&#25439;&#23475;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FedFNN&#65292;&#19968;&#31181;&#21152;&#36895;&#20998;&#25955;&#24335;&#27169;&#22411;&#35757;&#32451;&#30340;&#31639;&#27861;&#12290;&#22312;FL&#20013;&#65292;&#27599;&#20010;&#35757;&#32451;&#21608;&#26399;&#20165;&#28041;&#21450;&#29992;&#25143;&#23376;&#38598;&#12290;FedFNN&#21033;&#29992;&#30417;&#30563;&#23398;&#20064;&#20174;&#26410;&#25277;&#26679;&#30340;&#29992;&#25143;&#20013;&#39044;&#27979;&#26435;&#37325;&#26356;&#26032;&#65292;&#20351;&#29992;&#26469;&#33258;&#25277;&#26679;&#38598;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65306;1. FedFNN&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;&#39046;&#20808;&#26041;&#27861;&#24555;5&#20493;&#65292;&#20445;&#25345;&#25110;&#25552;&#39640;&#20934;&#30830;&#24615;&#65307;2. &#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#19982;&#23458;&#25143;&#31471;&#38598;&#32676;&#30340;&#21464;&#21270;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a key approach for distributed machine learning, enhancing online personalization while ensuring user data privacy. Instead of sending private data to a central server as in traditional approaches, FL decentralizes computations: devices train locally and share updates with a global server. A primary challenge in this setting is achieving fast and accurate model training - vital for recommendation systems where delays can compromise user engagement. This paper introduces FedFNN, an algorithm that accelerates decentralized model training. In FL, only a subset of users are involved in each training epoch. FedFNN employs supervised learning to predict weight updates from unsampled users, using updates from the sampled set. Our evaluations, using real and synthetic data, show: 1. FedFNN achieves training speeds 5x faster than leading methods, maintaining or improving accuracy; 2. the algorithm's performance is consistent regardless of client cluster va
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#39640;&#32500;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#38382;&#39064;&#65292;&#36890;&#36807;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#27169;&#22411;&#25429;&#25417;&#21327;&#21464;&#37327;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#20860;&#23481;&#22810;&#31181;&#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#21644;&#23450;&#20215;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#34892;&#30340;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.08634</link><description>&lt;p&gt;
&#21452;&#39640;&#32500;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65306;&#29992;&#20110;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing. (arXiv:2309.08634v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#39640;&#32500;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#38382;&#39064;&#65292;&#36890;&#36807;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#27169;&#22411;&#25429;&#25417;&#21327;&#21464;&#37327;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26041;&#27861;&#20860;&#23481;&#22810;&#31181;&#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#21644;&#23450;&#20215;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#34892;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21806;&#19994;&#21153;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#26159;&#22914;&#20309;&#36873;&#25321;&#35201;&#21521;&#28040;&#36153;&#32773;&#23637;&#31034;&#30340;&#20135;&#21697;&#65288;&#32452;&#21512;&#38382;&#39064;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#23450;&#20215;&#20135;&#21697;&#65288;&#23450;&#20215;&#38382;&#39064;&#65289;&#20197;&#26368;&#22823;&#21270;&#25910;&#20837;&#25110;&#21033;&#28070;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#24378;&#21270;&#23398;&#20064;&#30340;&#32852;&#21512;&#32452;&#21512;-&#23450;&#20215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21516;&#26102;&#32771;&#34385;&#20102;&#32452;&#21512;&#21644;&#23450;&#20215;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21452;&#39640;&#32500;&#30340;&#65292;&#21363;&#19978;&#19979;&#25991;&#21521;&#37327;&#21644;&#34892;&#20026;&#37117;&#20801;&#35768;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#21462;&#20540;&#12290;&#20026;&#20102;&#20811;&#26381;&#32500;&#24230;&#28798;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#65288;&#36817;&#20284;&#65289;&#20302;&#31209;&#34920;&#31034;&#30697;&#38453;&#26469;&#25429;&#25417;&#21327;&#21464;&#37327;&#21644;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#31867;&#26159;&#30456;&#24403;&#34920;&#36798;&#21147;&#30340;&#65292;&#21516;&#26102;&#36890;&#36807;&#28508;&#22312;&#22240;&#32032;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#21253;&#25324;&#19981;&#21516;&#32467;&#26500;&#21270;&#30340;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#21644;&#23450;&#20215;&#27169;&#22411;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#21487;&#34892;&#30340;&#27969;&#31243;&#65292;&#23558;&#25506;&#32034;/&#21033;&#29992;&#21327;&#35758;&#19982;&#39640;&#25928;&#30340;&#20302;&#31209;&#30697;&#38453;&#20272;&#35745;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key challenges in running a retail business include how to select products to present to consumers (the assortment problem), and how to price products (the pricing problem) to maximize revenue or profit. Instead of considering these problems in isolation, we propose a joint approach to assortment-pricing based on contextual bandits. Our model is doubly high-dimensional, in that both context vectors and actions are allowed to take values in high-dimensional spaces. In order to circumvent the curse of dimensionality, we propose a simple yet flexible model that captures the interactions between covariates and actions via a (near) low-rank representation matrix. The resulting class of models is reasonably expressive while remaining interpretable through latent factors, and includes various structured linear bandit and pricing models as particular cases. We propose a computationally tractable procedure that combines an exploration/exploitation protocol with an efficient low-rank matrix esti
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08631</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#24515;&#29702;&#20542;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08631
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#25968;&#23383;&#36275;&#36857;&#25512;&#26029;&#20182;&#20204;&#30340;&#24515;&#29702;&#20542;&#21521;&#65292;&#20855;&#20307;&#34920;&#29616;&#20026;&#20174;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#26029;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25512;&#26029;&#24471;&#20998;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#23558;&#25104;&#20026;&#20010;&#24615;&#21270;&#25216;&#26415;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#29702;&#35299;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#22266;&#26377;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;ChatGPT&#30340;LLMs&#20174;&#20010;&#20154;&#25968;&#23383;&#36275;&#36857;&#20013;&#25512;&#26029;&#20010;&#20154;&#24515;&#29702;&#20542;&#21521;&#30340;&#28508;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#20174;&#29992;&#25143;&#30340;Facebook&#29366;&#24577;&#26356;&#26032;&#20013;&#25512;&#23548;&#20986;&#20116;&#22823;&#20154;&#26684;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;LLM&#25512;&#26029;&#19982;&#33258;&#25105;&#25253;&#21578;&#24471;&#20998;&#20043;&#38388;&#30340;&#24179;&#22343;&#30456;&#20851;&#24615;&#20026;r = 0.29&#65288;&#33539;&#22260;&#20026;[0.22, 0.33]&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#24615;&#21035;&#21644;&#24180;&#40836;&#26041;&#38754;&#23384;&#22312;&#20010;&#24615;&#25512;&#26029;&#30340;&#20559;&#35265;&#65306;&#23545;&#20110;&#20960;&#20010;&#29305;&#36136;&#65292;&#25512;&#26029;&#24471;&#20998;&#22312;&#22899;&#24615;&#21644;&#24180;&#36731;&#20154;&#20013;&#30340;&#35823;&#24046;&#36739;&#23567;&#65292;&#36825;&#34920;&#26126;&#21487;&#33021;&#23384;&#22312;&#26469;&#33258;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#25110;&#22312;&#32447;&#33258;&#25105;&#21576;&#29616;&#30340;&#24046;&#24322;&#30340;&#31995;&#32479;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08630</link><description>&lt;p&gt;
PCN&#65306;&#19968;&#31181;&#21033;&#29992;&#26032;&#39062;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21943;&#27880;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21943;&#27880;&#26631;&#35760;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#31890;&#23376;&#30896;&#25758;&#20135;&#29983;&#30340;&#38181;&#29366;&#21943;&#27880;&#65292;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#21457;&#23556;&#31890;&#23376;&#12290;&#21943;&#27880;&#26631;&#35760;&#30340;&#36827;&#23637;&#20026;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#26032;&#29289;&#29702;&#25628;&#32034;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#22797;&#26434;&#30896;&#25758;&#25968;&#25454;&#20013;&#23547;&#25214;&#38544;&#34255;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#21943;&#27880;&#34920;&#31034;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#30340;&#26041;&#27861;&#22810;&#31181;&#22810;&#26679;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#21521;&#27169;&#22411;&#38544;&#34255;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#32534;&#30721;&#26368;&#22810;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#34920;&#31034;&#20013;&#26368;&#22909;&#22320;&#23398;&#20064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Particle Chebyshev Network&#65288;PCN&#65289;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#12290;ChebConv&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;GNN&#20013;&#30340;&#19968;&#31181;&#26377;&#25928;&#26367;&#20195;&#20256;&#32479;&#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#21943;&#27880;&#26631;&#35760;&#20013;&#36824;&#27809;&#26377;&#34987;&#25506;&#32034;&#36807;&#12290;PCN&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08628</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#20445;&#25252;&#25513;&#30721;&#30340;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25506;&#32034;&#20102;&#26367;&#25442;&#26631;&#35782;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#28151;&#28102;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36866;&#24212;&#23545;&#20110;&#22788;&#29702;&#20195;&#29702;&#35757;&#32451;&#25968;&#25454;&#21644;&#23454;&#38469;&#29992;&#25143;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#36827;&#34892;&#36866;&#24212;&#65292;&#29992;&#25143;&#30340;&#25991;&#26412;&#25968;&#25454;&#36890;&#24120;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#25110;&#26412;&#22320;&#35774;&#22791;&#19978;&#65292;&#19979;&#28216;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#39046;&#22495;&#20869;&#30340;&#25968;&#25454;&#36827;&#34892;&#30452;&#25509;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24341;&#36215;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#21521;&#23545;&#25163;&#27844;&#38706;&#29992;&#25143;&#20449;&#24687;&#30340;&#39069;&#22806;&#39118;&#38505;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#36890;&#29992;&#26631;&#35760;&#26367;&#25442;&#25991;&#26412;&#20013;&#30340;&#26631;&#35782;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#24314;&#35758;&#26367;&#25442;&#25513;&#30721;&#26631;&#35760;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#19979;&#28216;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#35780;&#20272;&#20854;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28151;&#28102;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;DTMs&#21644;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.08627</link><description>&lt;p&gt;
&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Dynamic Topic Models. (arXiv:2309.08627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08627
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#65292;&#24182;&#32467;&#21512;&#20102;&#27169;&#22411;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#23454;&#29992;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;DTMs&#21644;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#20027;&#39064;&#27169;&#22411;(DTMs)&#22312;&#35780;&#20272;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36827;&#23637;&#26041;&#38754;&#32570;&#20047;&#23450;&#37327;&#25351;&#26631;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DTMs&#35780;&#20272;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;&#27599;&#20010;&#20027;&#39064;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#36136;&#37327;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32467;&#21512;&#20027;&#39064;&#36136;&#37327;&#21644;&#27169;&#22411;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#24050;&#26377;DTMs&#30340;&#25968;&#25454;&#26469;&#35777;&#26126;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#33391;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#35782;&#21035;&#21464;&#21270;&#30340;&#20027;&#39064;&#12289;&#35780;&#20272;&#19981;&#21516;&#30340;DTMs&#20197;&#21450;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#38795;&#22443;&#20256;&#24863;&#22120;&#33719;&#24471;&#30340;&#24179;&#34913;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#27969;&#31243;&#31649;&#36947;&#26469;&#36741;&#21161;&#37492;&#21035;&#26089;&#26399;&#24739;&#26377;Lewy&#20307;&#30196;&#21574;&#30340;&#24739;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#20197;78.0%&#30340;&#20934;&#30830;&#29575;&#23558;MCI-LB&#19982;&#20854;&#20182;&#32452;&#21035;&#21306;&#20998;&#24320;&#65292;&#27604;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#21644;&#20020;&#24202;&#31070;&#32463;&#24515;&#29702;&#27979;&#37327;&#30340;&#21442;&#32771;&#27169;&#22411;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;6.8%&#12290;</title><link>http://arxiv.org/abs/2309.08623</link><description>&lt;p&gt;
&#28304;&#20110;&#38795;&#22443;&#20256;&#24863;&#22120;&#30340;&#24179;&#34913;&#27979;&#37327;&#33021;&#21306;&#20998;&#26089;&#26399;&#24739;&#26377;Lewy&#20307;&#30196;&#21574;&#30340;&#24739;&#32773;
&lt;/p&gt;
&lt;p&gt;
Balance Measures Derived from Insole Sensor Differentiate Prodromal Dementia with Lewy Bodies. (arXiv:2309.08623v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#38795;&#22443;&#20256;&#24863;&#22120;&#33719;&#24471;&#30340;&#24179;&#34913;&#27979;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#27969;&#31243;&#31649;&#36947;&#26469;&#36741;&#21161;&#37492;&#21035;&#26089;&#26399;&#24739;&#26377;Lewy&#20307;&#30196;&#21574;&#30340;&#24739;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#20197;78.0%&#30340;&#20934;&#30830;&#29575;&#23558;MCI-LB&#19982;&#20854;&#20182;&#32452;&#21035;&#21306;&#20998;&#24320;&#65292;&#27604;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#21644;&#20020;&#24202;&#31070;&#32463;&#24515;&#29702;&#27979;&#37327;&#30340;&#21442;&#32771;&#27169;&#22411;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;6.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lewy&#20307;&#30196;&#21574;&#26159;&#31532;&#20108;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30196;&#21574;&#31867;&#22411;&#65292;&#26089;&#26399;&#35782;&#21035;&#65292;&#21363;&#30001;Lewy&#20307;&#24341;&#36215;&#30340;&#36731;&#24230;&#35748;&#30693;&#21151;&#33021;&#38556;&#30861;&#65288;MCI-LB&#65289;&#65292;&#23545;&#20110;&#25552;&#20379;&#36866;&#24403;&#30340;&#25252;&#29702;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20020;&#24202;&#34920;&#29616;&#22810;&#26679;&#24615;&#20197;&#21450;&#19982;&#20854;&#20182;&#30149;&#30151;&#65288;&#22914;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24341;&#36215;&#30340;&#36731;&#24230;&#35748;&#30693;&#21151;&#33021;&#38556;&#30861;&#65289;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;MCI-LB&#32463;&#24120;&#34987;&#20302;&#20272;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#27969;&#31243;&#31649;&#36947;&#65292;&#36890;&#36807;&#21033;&#29992;&#38795;&#22443;&#20256;&#24863;&#22120;&#22312;30&#31186;&#31449;&#31435;&#20219;&#21153;&#26399;&#38388;&#33719;&#21462;&#30340;&#24179;&#34913;&#27979;&#37327;&#26469;&#24110;&#21161;&#35782;&#21035;MCI-LB&#12290;98&#21517;&#21442;&#19982;&#32773;&#65288;14&#21517;MCI-LB&#12289;38&#21517;MCI-AD&#12289;46&#21517;&#35748;&#30693;&#27491;&#24120;&#20154;&#65289;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#33021;&#22815;&#20197;&#39640;&#36798;78.0%&#30340;&#20934;&#30830;&#29575;&#65288;AUC: 0.681&#65289;&#23558;MCI-LB&#19982;&#20854;&#20182;&#32452;&#21035;&#36827;&#34892;&#21306;&#20998;&#65292;&#36825;&#27604;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#21644;&#20020;&#24202;&#31070;&#32463;&#24515;&#29702;&#27979;&#37327;&#30340;&#21442;&#32771;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;6.8%&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#21487;&#33021;&#20026;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dementia with Lewy bodies is the second most common type of neurodegenerative dementia, and identification at the prodromal stage$-$i.e., mild cognitive impairment due to Lewy bodies (MCI-LB)$-$is important for providing appropriate care. However, MCI-LB is often underrecognized because of its diversity in clinical manifestations and similarities with other conditions such as mild cognitive impairment due to Alzheimer's disease (MCI-AD). In this study, we propose a machine learning-based automatic pipeline that helps identify MCI-LB by exploiting balance measures acquired with an insole sensor during a 30-s standing task. An experiment with 98 participants (14 MCI-LB, 38 MCI-AD, 46 cognitively normal) showed that the resultant models could discriminate MCI-LB from the other groups with up to 78.0% accuracy (AUC: 0.681), which was 6.8% better than the accuracy of a reference model based on demographic and clinical neuropsychological measures. Our findings may open up a new approach for 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#22797;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#20013;&#20301;&#25968;&#36941;&#21382;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#36924;&#36817;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26102;&#26356;&#24555;&#19988;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.08620</link><description>&lt;p&gt;
&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#24046;&#20943;&#23567;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction of Resampling for Sequential Monte Carlo. (arXiv:2309.08620v1 [stat.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#26696;&#65292;&#36890;&#36807;&#24341;&#20837;&#37325;&#22797;&#30830;&#23450;&#24615;&#21306;&#22495;&#21644;&#20013;&#20301;&#25968;&#36941;&#21382;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#22312;&#36924;&#36817;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26102;&#26356;&#24555;&#19988;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#37319;&#26679;&#26041;&#26696;&#20026;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#26356;&#39640;&#26435;&#37325;&#30340;&#31890;&#23376;&#26469;&#34920;&#31034;&#30446;&#26631;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#26435;&#37325;&#20998;&#24067;&#30340;&#26041;&#24046;&#36234;&#23567;&#65292;&#26377;&#25928;&#31890;&#23376;&#30340;&#38598;&#20013;&#31243;&#24230;&#36234;&#39640;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#23601;&#36234;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#22797;&#30830;&#23450;&#24615;&#21306;&#22495;&#19982;&#20013;&#20301;&#25968;&#36941;&#21382;&#24615;&#30340;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#24182;&#22312;&#19982;&#20854;&#20182;&#37325;&#37319;&#26679;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#26041;&#24046;&#12290;&#22312;&#30830;&#23450;&#24615;&#21306;&#22495;&#30340;&#22823;&#23567;$M\ll N$&#65288;&#31890;&#23376;&#25968;&#37327;&#30340;&#22823;&#23567;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#32771;&#34385;&#21040;&#23454;&#38469;&#30340;&#31890;&#23376;&#25968;&#37327;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#29366;&#24577;&#35201;&#26356;&#24555;&#65292;&#36825;&#19968;&#28857;&#36890;&#36807;&#22312;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#30340;&#29702;&#35770;&#25512;&#23548;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
A resampling scheme provides a way to switch low-weight particles for sequential Monte Carlo with higher-weight particles representing the objective distribution. The less the variance of the weight distribution is, the more concentrated the effective particles are, and the quicker and more accurate it is to approximate the hidden Markov model, especially for the nonlinear case. We propose a repetitive deterministic domain with median ergodicity for resampling and have achieved the lowest variances compared to the other resampling methods. As the size of the deterministic domain $M\ll N$ (the size of population), given a feasible size of particles, our algorithm is faster than the state of the art, which is verified by theoretical deduction and experiments of a hidden Markov model in both the linear and non-linear cases.
&lt;/p&gt;</description></item><item><title>Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.08617</link><description>&lt;p&gt;
Drifter: &#22823;&#35268;&#27169;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#20197;&#25552;&#39640;&#25968;&#25454;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;
Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08617
&lt;/p&gt;
&lt;p&gt;
Drifter&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#31995;&#32479;&#65292;&#36890;&#36807;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#23454;&#26102;&#20998;&#26512;&#12289;&#26816;&#27979;&#21644;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#38382;&#39064;&#65292;&#20351;&#24471;&#23454;&#26102;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29983;&#20135;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#22312;&#22823;&#35268;&#27169;&#12289;&#21160;&#24577;&#27969;&#20013;&#32500;&#25252;&#25968;&#25454;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Drifter&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#20013;&#22312;&#32447;&#29305;&#24449;&#30417;&#25511;&#21644;&#39564;&#35777;&#30340;&#39640;&#25928;&#19988;&#36731;&#37327;&#32423;&#30340;&#31995;&#32479;&#12290;Drifter&#36890;&#36807;&#25552;&#20379;&#25935;&#25463;&#12289;&#21709;&#24212;&#21644;&#36866;&#24212;&#24615;&#30340;&#25968;&#25454;&#36136;&#37327;&#30417;&#25511;&#65292;&#33021;&#22815;&#23454;&#26102;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12289;&#28418;&#31227;&#26816;&#27979;&#20197;&#21450;&#23545;&#26377;&#38382;&#39064;&#30340;&#29983;&#20135;&#20107;&#20214;&#30340;&#27934;&#23519;&#12290;Drifter&#38598;&#25104;&#20102;&#26368;&#20808;&#36827;&#30340;&#31232;&#30095;&#25968;&#25454;&#22312;&#32447;&#29305;&#24449;&#25490;&#21517;&#21644;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#36164;&#28304;&#25928;&#29575;&#24615;&#65292;&#27599;&#20998;&#38047;&#22788;&#29702;&#25968;&#30334;&#19975;&#20010;&#23454;&#20363;&#20165;&#38656;&#35201;&#20004;&#20010;&#32447;&#31243;&#21644;&#23569;&#20110;&#19968;GB&#30340;RAM&#12290;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;Drifter&#22312;&#35686;&#25253;&#21644;&#32531;&#35299;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23454;&#26102;&#23454;&#20917;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.08613</link><description>&lt;p&gt;
&#39044;&#27979;&#30142;&#30149;&#24182;&#21457;&#30151;&#20013;&#30340;&#22810;&#27169;&#24577;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#21033;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#36827;&#34892;&#30142;&#30149;&#24182;&#21457;&#30151;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#20351;&#29992;&#20102;NCF&#21644;DHF&#20004;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#24182;&#21033;&#29992;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;NCF&#27169;&#22411;&#22312;&#20934;&#30830;&#29575;&#21644;&#21629;&#20013;&#29575;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21327;&#21516;&#36807;&#28388;&#25512;&#33616;&#31995;&#32479;&#24050;&#32463;&#22312;&#20854;&#20182;&#39046;&#22495;&#30340;&#25512;&#33616;&#20013;&#24471;&#21040;&#26222;&#36941;&#24212;&#29992;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#36824;&#24456;&#26377;&#38480;&#12290;&#38500;&#20102;&#24314;&#27169;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20027;&#39064;-&#30142;&#30149;&#30721;&#20132;&#20114;&#12290;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#21327;&#21516;&#36807;&#28388;(NCF)&#21644;&#28145;&#24230;&#28151;&#21512;&#36807;&#28388;(DHF)&#36825;&#20004;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#30142;&#30149;&#35786;&#26029;&#20013;&#36827;&#34892;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#24212;&#29992;&#65292;&#22522;&#20110;&#24050;&#30693;&#30340;&#36807;&#21435;&#24739;&#32773;&#24182;&#21457;&#30151;&#26469;&#36827;&#34892;&#39044;&#27979;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#21253;&#21547;MIMIC-III&#25968;&#25454;&#24211;&#20013;&#30340;&#25152;&#26377;&#20027;&#39064;-&#30142;&#30149;&#30721;&#23545;&#65292;&#21478;&#19968;&#20010;&#21253;&#21547;&#21457;&#29983;&#26368;&#24120;&#35265;&#30340;50&#31181;&#30142;&#30149;&#12290;&#20934;&#30830;&#29575;&#21644;Hit Ratio@10&#34987;&#29992;&#20316;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#21457;&#29616;&#21033;&#29992;&#20943;&#23569;&#30340;&#8220;top 50&#8221; ICD-9&#30721;&#25968;&#25454;&#38598;&#30340;NCF&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#20302;(&#20934;&#30830;&#29575;&#32422;&#20026;80%&#21644;Hit Ratio@10&#20026;...
&lt;/p&gt;
&lt;p&gt;
While deep-learning based recommender systems utilizing collaborative filtering have been commonly used for recommendation in other domains, their application in the medical domain have been limited. In addition to modeling user-item interactions, we show that deep-learning based recommender systems can be used to model subject-disease code interactions. Two novel applications of deep learning-based recommender systems using Neural Collaborative Filtering (NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based on known past patient comorbidities. Two datasets, one incorporating all subject-disease code pairs present in the MIMIC-III database, and the other incorporating the top 50 most commonly occurring diseases, were used for prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate model performance. The performance of the NCF model making use of the reduced "top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit ratio@10 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#27861;&#65292;&#22312;&#24494;&#35843;&#27169;&#22411;&#21518;&#29983;&#25104;&#34701;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#34920;&#29616;&#26356;&#22909;&#65288;&#27604;&#26368;&#20339;&#21333;&#20010;&#27169;&#22411;&#25552;&#39640;&#20102;3.5%&#65289;&#65292;&#21516;&#26102;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#20063;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08610</link><description>&lt;p&gt;
&#36827;&#34892;&#24343;&#20848;&#32943;&#26031;&#22374;&#27861;&#25110;&#22914;&#20309;&#36890;&#36807;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#26469;&#25552;&#21319;&#23545;&#20998;&#24067;&#20043;&#22806;&#24615;&#33021;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Do the Frankenstein, or how to achieve better out-of-distribution performance with manifold mixing model soup. (arXiv:2309.08610v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08610
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#27861;&#65292;&#22312;&#24494;&#35843;&#27169;&#22411;&#21518;&#29983;&#25104;&#34701;&#21512;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#34920;&#29616;&#26356;&#22909;&#65288;&#27604;&#26368;&#20339;&#21333;&#20010;&#27169;&#22411;&#25552;&#39640;&#20102;3.5%&#65289;&#65292;&#21516;&#26102;&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#20063;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#29992;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#35774;&#32622;&#65292;&#24182;&#36873;&#25321;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#29575;&#26368;&#39640;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#25442;&#26102;&#34920;&#29616;&#19981;&#20339;&#65292;&#20363;&#22914;&#24403;&#27169;&#22411;&#25509;&#25910;&#36755;&#20837;&#30340;&#26159;&#22270;&#24418;&#21270;&#30340;&#29289;&#20307;&#33609;&#22270;&#32780;&#19981;&#26159;&#29031;&#29255;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#27169;&#22411;&#31354;&#38388;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#26368;&#20339;&#26041;&#24335;&#28151;&#21512;&#22810;&#20010;&#24494;&#35843;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#27969;&#24418;&#26469;&#29983;&#25104;&#19968;&#20010;&#34701;&#21512;&#27169;&#22411;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#22312;&#23545;&#22270;&#20687;&#20998;&#31867;&#36827;&#34892;&#24494;&#35843;&#21518;&#65292;&#34701;&#21512;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#24615;&#33021;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#26368;&#20339;&#30340;&#21333;&#20010;&#27169;&#22411;&#65288;&#27604;&#26368;&#20339;&#21333;&#20010;&#27169;&#22411;&#25552;&#39640;&#20102;3.5%&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#22312;&#36827;&#34892;&#24494;&#35843;&#30340;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard recipe applied in transfer learning is to finetune a pretrained model on the task-specific dataset with different hyperparameter settings and pick the model with the highest accuracy on the validation dataset. Unfortunately, this leads to models which do not perform well under distribution shifts, e.g. when the model is given graphical sketches of the object as input instead of photos. In order to address this, we propose the manifold mixing model soup, an algorithm which mixes together the latent space manifolds of multiple finetuned models in an optimal way in order to generate a fused model. We show that the fused model gives significantly better out-of-distribution performance (+3.5 % compared to best individual model) when finetuning a CLIP model for image classification. In addition, it provides also better accuracy on the original dataset where the finetuning has been done.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35777;&#26126;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#22478;&#24066;&#21464;&#21270;&#30417;&#27979;&#38382;&#39064;&#30340;&#21487;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21644;&#20809;&#23398;&#22810;&#20809;&#35889;&#35266;&#27979;&#25968;&#25454;&#65292;&#25104;&#21151;&#30417;&#27979;&#20102;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#22312;&#20420;&#20044;&#20914;&#31361;&#24320;&#22987;&#38454;&#27573;&#30340;&#30456;&#20851;&#22478;&#24066;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.08607</link><description>&lt;p&gt;
&#22312;2022/23&#24180;&#30417;&#27979;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#30340;&#22478;&#24066;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Monitoring Urban Changes in Mariupol/Ukraine in 2022/23. (arXiv:2309.08607v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35777;&#26126;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#35299;&#20915;&#22478;&#24066;&#21464;&#21270;&#30417;&#27979;&#38382;&#39064;&#30340;&#21487;&#34892;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21644;&#20809;&#23398;&#22810;&#20809;&#35889;&#35266;&#27979;&#25968;&#25454;&#65292;&#25104;&#21151;&#30417;&#27979;&#20102;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#22312;&#20420;&#20044;&#20914;&#31361;&#24320;&#22987;&#38454;&#27573;&#30340;&#30456;&#20851;&#22478;&#24066;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#30417;&#27979;&#22478;&#24066;&#21464;&#21270;&#30340;&#33021;&#21147;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#32463;&#27982;&#21033;&#30410;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#23637;&#31034;&#22312;&#35757;&#32451;&#25110;&#36801;&#31227;&#39046;&#22495;&#20043;&#22806;&#30340;&#26102;&#38388;&#23610;&#24230;&#12290;&#26412;&#30740;&#31350;&#22312;&#29616;&#26377;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26159;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20173;&#28982;&#21487;&#20197;&#23545;&#20197;&#21518;&#30340;&#24180;&#20221;&#36827;&#34892;&#22478;&#24066;&#21464;&#21270;&#30417;&#27979;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#23545;&#20844;&#20849;&#21644;&#20813;&#36153;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#35775;&#38382;&#26377;&#38480;&#30340;&#24773;&#20917;&#26469;&#25351;&#23548;&#36801;&#31227;&#12290;&#20026;&#20102;&#25552;&#20379;&#39640;&#26102;&#31354;&#20998;&#36776;&#29575;&#65292;&#25105;&#20204;&#30340;&#30417;&#27979;&#26041;&#27861;&#30340;&#26680;&#24515;&#25968;&#25454;&#21253;&#25324;&#26469;&#33258;Sentinel 1&#65288;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65289;&#21644;Sentinel 2&#65288;&#20809;&#23398;&#22810;&#20809;&#35889;&#65289;&#30340;&#22810;&#27169;&#24577;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#21644;&#20809;&#23398;&#22810;&#20809;&#35889;&#35266;&#27979;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#23454;&#38469;&#24212;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#30417;&#27979;&#20044;&#20811;&#20848;&#39532;&#37324;&#20044;&#27874;&#23572;&#24066;&#19982;&#20420;&#20044;&#20914;&#31361;&#24320;&#22987;&#26102;&#30340;&#30456;&#20851;&#22478;&#24066;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to constantly monitor urban changes is of large socio-economic interest. Previous works have already shown approaches in this field with the use of Deep Neural Networks (DNNs) and transfer learning. However, they fell short in demonstrating temporal scale outside of either the training or transfer domain.  This work builds on existing research and proves that transfer learning with the use of historic data is a feasible solution, which still allows the urban change monitoring of later years. We considered a case with limited access to public and free Very High Resolution (VHR) imagery to guide the transfer. To provide a high temporal resolution, the core data of our monitoring method comprised multi-modal Synthetic Aperture Radar (SAR) and optical multispectral observations from Sentinel 1 and Sentinel 2, respectively.  We chose a practical application of our methods for monitoring urban-related changes in the city of Mariupol in Ukraine during the beginning of the Russo-Uk
&lt;/p&gt;</description></item><item><title>CRYPTO-MINE&#26159;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#20013;&#26126;&#25991;&#21644;&#23494;&#25991;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20998;&#26512;&#23494;&#30721;&#31995;&#32479;&#30340;&#35745;&#31639;&#23433;&#20840;&#24615;&#21644;&#20449;&#24687;&#27844;&#38706;&#19982;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.08019</link><description>&lt;p&gt;
CRYPTO-MINE: &#36890;&#36807;&#20114;&#20449;&#24687;&#31070;&#32463;&#20272;&#35745;&#36827;&#34892;&#23494;&#30721;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation. (arXiv:2309.08019v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08019
&lt;/p&gt;
&lt;p&gt;
CRYPTO-MINE&#26159;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20114;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#20013;&#26126;&#25991;&#21644;&#23494;&#25991;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#20998;&#26512;&#23494;&#30721;&#31995;&#32479;&#30340;&#35745;&#31639;&#23433;&#20840;&#24615;&#21644;&#20449;&#24687;&#27844;&#38706;&#19982;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20316;&#20026;&#35780;&#20272;&#23494;&#30721;&#31995;&#32479;&#25928;&#29575;&#30340;&#25351;&#26631;&#20855;&#26377;&#24191;&#27867;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20272;&#35745;&#26410;&#30693;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20351;&#24471;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#20114;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20114;&#20449;&#24687;&#20272;&#35745;&#22312;&#23494;&#30721;&#23398;&#39046;&#22495;&#30340;&#26032;&#24212;&#29992;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#31181;&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26126;&#25991;&#25915;&#20987;&#20013;&#26126;&#25991;&#21644;&#23494;&#25991;&#20043;&#38388;&#30340;&#20272;&#35745;&#20114;&#20449;&#24687;&#12290;&#22914;&#26524;&#26377;&#30340;&#35805;&#65292;&#21152;&#23494;&#20013;&#30340;&#27844;&#38706;&#20449;&#24687;&#21487;&#33021;&#20250;&#34987;&#23545;&#25163;&#21033;&#29992;&#26469;&#30772;&#22351;&#23494;&#30721;&#31995;&#32479;&#30340;&#35745;&#31639;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#20010;&#21152;&#23494;&#26041;&#26696;&#21644;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#26469;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#23545;&#25552;&#20379;&#20010;&#20307;&#20445;&#23494;&#24615;&#30340;&#22522;&#20110;&#32593;&#32476;&#32534;&#30721;&#30340;&#23494;&#30721;&#31995;&#32479;&#30340;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#20449;&#24687;&#27844;&#38706;&#21644;&#36755;&#20837;&#20998;&#24067;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Mutual Information (MI) as a measure to evaluate the efficiency of cryptosystems has an extensive history. However, estimating MI between unknown random variables in a high-dimensional space is challenging. Recent advances in machine learning have enabled progress in estimating MI using neural networks. This work presents a novel application of MI estimation in the field of cryptography. We propose applying this methodology directly to estimate the MI between plaintext and ciphertext in a chosen plaintext attack. The leaked information, if any, from the encryption could potentially be exploited by adversaries to compromise the computational security of the cryptosystem. We evaluate the efficiency of our approach by empirically analyzing multiple encryption schemes and baseline approaches. Furthermore, we extend the analysis to novel network coding-based cryptosystems that provide individual secrecy and study the relationship between information leakage and input distribution
&lt;/p&gt;</description></item><item><title>Voxtlm&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.07937</link><description>&lt;p&gt;
Voxtlm: &#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#29992;&#20110;&#21512;&#24182;&#35821;&#38899;&#35782;&#21035;/&#21512;&#25104;&#21644;&#35821;&#38899;/&#25991;&#26412;&#34917;&#20805;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks. (arXiv:2309.07937v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07937
&lt;/p&gt;
&lt;p&gt;
Voxtlm&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#21482;&#35299;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21482;&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;VoxtLM&#65292;&#33021;&#22815;&#25191;&#34892;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#38899;&#35782;&#21035;&#12289;&#35821;&#38899;&#21512;&#25104;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#35821;&#38899;&#24310;&#32493;&#12290;VoxtLM&#23558;&#25991;&#26412;&#35789;&#27719;&#19982;&#33258;&#30417;&#30563;&#35821;&#38899;&#29305;&#24449;&#20013;&#30340;&#31163;&#25955;&#35821;&#38899;&#20196;&#29260;&#36827;&#34892;&#25972;&#21512;&#65292;&#24182;&#20351;&#29992;&#29305;&#27530;&#20196;&#29260;&#23454;&#29616;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#19982;&#21333;&#20219;&#21153;&#27169;&#22411;&#30456;&#27604;&#65292;VoxtLM&#22312;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#35821;&#38899;&#21487;&#29702;&#35299;&#24615;&#20174;28.9&#25552;&#39640;&#21040;5.6&#65292;&#23458;&#35266;&#36136;&#37327;&#20174;2.68&#25552;&#39640;&#21040;3.90&#12290;VoxtLM&#36824;&#25913;&#21892;&#20102;&#35821;&#38899;&#29983;&#25104;&#21644;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;VoxtLM&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#25552;&#20379;&#35757;&#32451;&#33050;&#26412;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20197;&#23454;&#29616;&#23436;&#20840;&#21487;&#22797;&#29616;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. VoxtLM is trained with publicly available data and training recipes and model checkpoints will be open-sourced to make fully reproducible work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#33539;&#24335;&#12289;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#21644;&#30456;&#20851;&#36866;&#37197;&#22120;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;</title><link>http://arxiv.org/abs/2309.07929</link><description>&lt;p&gt;
&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;
&lt;/p&gt;
&lt;p&gt;
Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22768;&#38899;&#25552;&#31034;&#36827;&#34892;&#20998;&#21106;&#30340;&#27867;&#21270;&#38899;&#39057;-&#35270;&#35273;&#28304;&#23450;&#20301;&#22120;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#36890;&#36807;&#24341;&#20837;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#33539;&#24335;&#12289;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#21644;&#30456;&#20851;&#36866;&#37197;&#22120;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20174;&#26410;&#21516;&#26102;&#30475;&#21040;&#29289;&#20307;&#21644;&#21548;&#21040;&#20854;&#22768;&#38899;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#20934;&#30830;&#22320;&#20174;&#36755;&#20837;&#38899;&#39057;&#20013;&#23450;&#20301;&#20854;&#35270;&#35273;&#20301;&#32622;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#38899;&#39057;-&#35270;&#35273;&#23450;&#20301;&#21644;&#20998;&#21106;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32534;&#30721;&#22120;&#25552;&#31034;&#35299;&#30721;&#22120;&#30340;&#33539;&#24335;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#32534;&#30721;&#22120;&#34701;&#21512;&#35299;&#30721;&#22120;&#33539;&#24335;&#20174;&#34701;&#21512;&#38899;&#39057;-&#35270;&#35273;&#29305;&#24449;&#20013;&#35299;&#30721;&#23450;&#20301;&#20449;&#24687;&#65292;&#25105;&#20204;&#26088;&#22312;&#20511;&#21161;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20016;&#23500;&#30693;&#35782;&#26469;&#26356;&#22909;&#22320;&#36866;&#24212;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#22659;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#38899;&#39057;&#25552;&#31034;&#65288;SAP&#65289;&#26469;&#24110;&#21161;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#20851;&#27880;&#26377;&#22768;&#23545;&#35937;&#65292;&#21516;&#26102;&#20063;&#40723;&#21169;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#32553;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#30456;&#20851;&#36866;&#37197;&#22120;&#65288;ColA&#65289;&#26469;&#20445;&#25345;&#26368;&#23567;&#30340;&#35757;&#32451;&#24037;&#20316;&#37327;&#24182;&#32500;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.07812</link><description>&lt;p&gt;
&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30284;&#30151;&#20020;&#24202;&#35797;&#39564;&#20013;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#65292;&#36890;&#36807;&#24212;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#33258;&#21160;&#20998;&#31867;&#25490;&#38500;&#26631;&#20934;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#19987;&#38376;&#20026;&#20020;&#24202;&#35797;&#39564;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35797;&#39564;&#36164;&#26684;&#26631;&#20934;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#38472;&#36848;&#65292;&#22240;&#27492;&#33258;&#21160;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#35797;&#39564;&#36164;&#26684;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#19968;&#20010;&#28508;&#22312;&#26041;&#27861;&#26159;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#24120;&#35265;&#31867;&#22411;&#30340;&#36164;&#26684;&#26631;&#20934;&#36827;&#34892;&#22788;&#29702;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#30284;&#30151;&#35797;&#39564;&#20013;&#30340;&#19971;&#20010;&#24120;&#35265;&#25490;&#38500;&#26631;&#20934;&#65306;&#20808;&#21069;&#24694;&#24615;&#32959;&#30244;&#12289;&#20154;&#31867;&#20813;&#30123;&#32570;&#38519;&#30149;&#27602;&#12289;&#20057;&#32925;&#30149;&#27602;&#12289;&#19993;&#32925;&#30149;&#27602;&#12289;&#31934;&#31070;&#30142;&#30149;&#12289;&#33647;&#29289;/&#29289;&#36136;&#28389;&#29992;&#21644;&#33258;&#36523;&#20813;&#30123;&#30142;&#30149;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;764&#20010;&#24102;&#26377;&#36825;&#20123;&#25490;&#38500;&#26631;&#20934;&#27880;&#37322;&#30340;&#19977;&#26399;&#30284;&#30151;&#35797;&#39564;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#24120;&#35265;&#30340;transformer&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#30340;&#20020;&#24202;&#35797;&#39564;BERT&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#21160;&#20998;&#31867;&#24120;&#35265;&#30340;&#25490;&#38500;&#26631;&#20934;&#26159;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#20020;&#24202;&#35797;&#39564;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#35813;&#27169;&#22411;&#22312;&#25152;&#26377;&#26631;&#20934;&#20013;&#34920;&#29616;&#20986;&#26368;&#39640;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07383</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#36817;&#20284;&#30340;&#26576;&#20123;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#20540;&#20989;&#25968;&#36817;&#20284;&#22312;&#29305;&#23450;&#26412;&#22320;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#36816;&#29992;&#31639;&#23376;&#26041;&#31243;&#36827;&#34892;&#31163;&#32447;&#36817;&#20284;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#32467;&#26524;&#25913;&#36827;&#21644;&#32454;&#21270;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#19968;&#32452;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;$H(\Omega)$&#20013;&#20986;&#29616;&#30340;&#19968;&#20123;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#29305;&#23450;&#31867;&#30340;&#26412;&#22320;&#31354;&#38388;&#20013;&#26500;&#24314;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31163;&#32447;&#36817;&#20284;&#30340;&#31639;&#23376;&#26041;&#31243;&#30340;&#24378;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#20010;&#31639;&#23376;&#26041;&#31243;&#20986;&#29616;&#22312;&#31574;&#30053;&#36845;&#20195;&#20013;&#12290;&#21033;&#29992;&#26377;&#38480;&#32500;&#36817;&#20284;&#31354;&#38388;$H_N$&#22312;&#26412;&#22320;&#31354;&#38388;$H(\Omega)$&#20013;&#30340;&#21151;&#29575;&#20989;&#25968;$\Pwr_{H,N}$&#65292;&#24471;&#21040;&#20102;&#20540;&#20989;&#25968;&#36817;&#20284;&#35823;&#24046;&#30340;&#26174;&#24335;&#19978;&#30028;&#12290;&#36825;&#20123;&#19978;&#30028;&#20855;&#26377;&#20960;&#20309;&#24615;&#36136;&#65292;&#24182;&#23545;&#20540;&#20989;&#25968;&#36817;&#20284;&#30340;&#25910;&#25947;&#24615;&#26377;&#20102;&#19968;&#20123;&#25913;&#36827;&#21644;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23884;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26410;&#30693;&#39046;&#22495;&#30340;&#26816;&#27979;&#65292;&#24182;&#22522;&#20110;&#36882;&#24402;&#39044;&#27979;&#26500;&#24314;&#20102;&#22312;&#32447;&#30340;&#30417;&#35270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#39046;&#22495;&#20449;&#24687;&#30340;&#20869;&#26680;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#22238;&#24402;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.06655</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#39046;&#22495;&#20449;&#24687;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23454;&#29616;&#26410;&#30693;&#39046;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models. (arXiv:2309.06655v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23884;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26410;&#30693;&#39046;&#22495;&#30340;&#26816;&#27979;&#65292;&#24182;&#22522;&#20110;&#36882;&#24402;&#39044;&#27979;&#26500;&#24314;&#20102;&#22312;&#32447;&#30340;&#30417;&#35270;&#22120;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#24102;&#26377;&#39046;&#22495;&#20449;&#24687;&#30340;&#20869;&#26680;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#22238;&#24402;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#26410;&#30693;&#24773;&#26223;&#20013;&#23433;&#20840;&#23548;&#33322;&#65292;&#20934;&#30830;&#22320;&#22312;&#32447;&#26816;&#27979;&#35757;&#32451;&#38598;&#20043;&#22806;&#30340;&#24773;&#20917;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#39640;&#26031;&#36807;&#31243;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;GPSSM&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#30830;&#23450;&#30340;&#35266;&#27979;&#19982;&#27010;&#29575;&#39044;&#27979;&#36827;&#34892;&#27604;&#36739;&#26469;&#21306;&#20998;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#21306;&#20998;&#35757;&#32451;&#38598;&#20869;&#22806;&#30340;&#35266;&#27979;&#21462;&#20915;&#20110;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20027;&#35201;&#21463;&#21040;GPSSM&#20869;&#26680;&#25152;&#33021;&#34920;&#31034;&#30340;&#20989;&#25968;&#31867;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#29616;&#26377;&#39046;&#22495;&#30693;&#35782;&#23884;&#20837;&#20869;&#26680;&#20013;&#65292;&#24182;&#22522;&#20110;&#36882;&#24402;&#22320;&#39044;&#27979;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#32447;&#36816;&#34892;&#26102;&#30340;&#26410;&#30693;&#39046;&#22495;&#30417;&#35270;&#22120;&#12290;&#39046;&#22495;&#30693;&#35782;&#20551;&#35774;&#20197;&#22312;&#27169;&#25311;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#25110;&#20351;&#29992;&#21517;&#20041;&#27169;&#22411;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#24102;&#26377;&#39046;&#22495;&#20449;&#24687;&#30340;&#20869;&#26680;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22238;&#24402;&#36136;&#37327;&#65292;&#19982;&#26631;&#20934;&#20869;&#26680;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard ker
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2309.06584</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction. (arXiv:2309.06584v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06584
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#26469;&#39044;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#30340;&#39118;&#38505;&#12290;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#19981;&#20165;&#33021;&#21457;&#29616;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#36824;&#33021;&#25581;&#31034;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#36890;&#36807;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21644;&#20854;&#23545;&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#35813;&#26041;&#27861;&#33021;&#25552;&#20379;&#20840;&#38754;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#30456;&#20851;&#30196;&#21574;&#30151;&#65288;ADRD&#65289;&#22312;&#32654;&#22269;&#26159;&#31532;&#20845;&#22823;&#27515;&#20129;&#21407;&#22240;&#65292;&#20934;&#30830;&#30340;ADRD&#39118;&#38505;&#39044;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;ADRD&#39118;&#38505;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#22270;&#20687;&#20998;&#26512;&#65292;&#32780;&#24182;&#38750;&#25152;&#26377;&#24739;&#32773;&#22312;ADRD&#35786;&#26029;&#21069;&#37117;&#25509;&#21463;&#21307;&#23398;&#24433;&#20687;&#26816;&#26597;&#12290;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#32034;&#36180;&#25968;&#25454;&#30456;&#32467;&#21512;&#21487;&#20197;&#25581;&#31034;&#39069;&#22806;&#30340;&#39118;&#38505;&#22240;&#32032;&#24182;&#21457;&#29616;&#19981;&#21516;&#21307;&#23398;&#20195;&#30721;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21644;&#32034;&#36180;&#25968;&#25454;&#36827;&#34892;ADRD&#39118;&#38505;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#32972;&#21518;&#32570;&#20047;&#21487;&#35299;&#37322;&#21407;&#22240;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#20851;&#31995;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;ADRD&#39118;&#38505;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#30830;&#20445;&#20840;&#38754;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#27491;&#21017;&#21270;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;VGNN&#65289;&#26469;&#20272;&#35745;ADRD&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#24773;&#26223;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#20351;&#29992;&#20102;&#38543;&#26426;&#26862;&#26519;&#21644;&#36731;&#26799;&#24230;...
&lt;/p&gt;
&lt;p&gt;
Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.  We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.05833</link><description>&lt;p&gt;
PACE: &#20351;&#29992;GPT-4&#36827;&#34892;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#20013;&#30340;&#25552;&#31034;&#21644;&#22686;&#21152;&#20197;&#36827;&#34892;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;IT&#34892;&#19994;&#21521;&#22522;&#20110;&#20113;&#30340;&#24179;&#21488;&#30340;&#36716;&#21464;&#24378;&#35843;&#20102;&#20113;&#20107;&#20214;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#26381;&#21153;&#30340;&#21487;&#38752;&#24615;&#21644;&#32500;&#25252;&#23458;&#25143;&#20449;&#20219;&#12290;&#26680;&#24515;&#38382;&#39064;&#26159;&#26377;&#25928;&#30830;&#23450;&#26681;&#26412;&#21407;&#22240;&#65292;&#30001;&#20110;&#24403;&#20195;&#20113;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#29992;&#20110;&#26681;&#26412;&#21407;&#22240;&#35782;&#21035;&#30340;&#22522;&#20110;AI&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20173;&#21463;&#21040;&#20854;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#22686;&#24378;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#24037;&#20855;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#27169;&#22411;&#26681;&#25454;&#21382;&#21490;&#20107;&#20214;&#25968;&#25454;&#35780;&#20272;&#33258;&#36523;&#30340;&#32622;&#20449;&#24230;&#65292;&#32771;&#34385;&#20854;&#23545;&#35777;&#25454;&#30340;&#35780;&#20272;&#24378;&#24230;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#23457;&#26680;&#30001;&#39044;&#27979;&#22120;&#29983;&#25104;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#20248;&#21270;&#27493;&#39588;&#23558;&#36825;&#20123;&#35780;&#20272;&#32467;&#21512;&#36215;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04160</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#26469;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21407;&#22411;&#24739;&#32773;&#34920;&#31034;&#21644;&#29305;&#24449;&#32570;&#22833;&#24863;&#30693;&#26657;&#20934;&#30340;&#38388;&#25509;&#25554;&#34917;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#26469;&#25552;&#39640;&#39044;&#27979;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#32463;&#24120;&#21576;&#29616;&#31232;&#30095;&#29305;&#24449;&#65292;&#32473;&#39044;&#27979;&#24314;&#27169;&#24102;&#26469;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#30452;&#25509;&#25554;&#34917;&#26041;&#27861;&#65288;&#22914;&#30697;&#38453;&#25554;&#34917;&#26041;&#27861;&#65289;&#20381;&#36182;&#20110;&#21442;&#32771;&#31867;&#20284;&#34892;&#25110;&#21015;&#26469;&#23436;&#25104;&#21407;&#22987;&#32570;&#22833;&#25968;&#25454;&#65292;&#19981;&#21306;&#20998;&#25554;&#34917;&#21644;&#23454;&#38469;&#20540;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19982;&#39044;&#27979;&#30446;&#26631;&#26080;&#20851;&#30340;&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#25439;&#23475;&#19979;&#28216;&#24615;&#33021;&#30340;&#25928;&#26524;&#12290;&#34429;&#28982;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#22312;&#30452;&#25509;&#25554;&#34917;&#21518;&#37325;&#26032;&#26657;&#20934;&#25110;&#22686;&#24378;EHR&#23884;&#20837;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38169;&#35823;&#22320;&#20248;&#20808;&#32771;&#34385;&#25554;&#34917;&#29305;&#24449;&#12290;&#36825;&#31181;&#20248;&#20808;&#38169;&#35823;&#21487;&#33021;&#20250;&#32473;&#27169;&#22411;&#24341;&#20837;&#20559;&#35265;&#25110;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#37319;&#29992;&#38388;&#25509;&#25554;&#34917;&#65292;&#21033;&#29992;&#31867;&#20284;&#24739;&#32773;&#30340;&#21407;&#22411;&#34920;&#31034;&#33719;&#21462;&#26356;&#23494;&#38598;&#30340;&#23884;&#20837;&#12290;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#26102;&#36890;&#24120;&#23558;&#32570;&#22833;&#29305;&#24449;&#19982;&#23384;&#22312;&#29305;&#24449;&#30456;&#21516;&#30340;&#38480;&#21046;&#26102;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#39640;&#25928;&#35299;&#20915;&#21453;&#21521;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#37319;&#26679;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.02040</link><description>&lt;p&gt;
&#25193;&#25955;&#29983;&#25104;&#21453;&#21521;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Diffusion Generative Inverse Design. (arXiv:2309.02040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#39640;&#25928;&#35299;&#20915;&#21453;&#21521;&#35774;&#35745;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#37319;&#26679;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35774;&#35745;&#26159;&#25351;&#36890;&#36807;&#20248;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#36755;&#20837;&#26469;&#23454;&#29616;&#30446;&#26631;&#32467;&#26524;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24037;&#31243;&#38382;&#39064;&#65292;&#30446;&#26631;&#20989;&#25968;&#37319;&#29992;&#27169;&#25311;&#22120;&#30340;&#24418;&#24335;&#65292;&#39044;&#27979;&#31995;&#32479;&#29366;&#24577;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#65292;&#35774;&#35745;&#25361;&#25112;&#26159;&#20248;&#21270;&#23548;&#33268;&#30446;&#26631;&#32467;&#26524;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#26368;&#36817;&#65292;&#23398;&#20064;&#27169;&#25311;&#30340;&#21457;&#23637;&#34920;&#26126;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#29992;&#20110;&#20934;&#30830;&#12289;&#39640;&#25928;&#12289;&#21487;&#24494;&#20998;&#22320;&#20272;&#35745;&#27169;&#25311;&#22120;&#21160;&#24577;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#26799;&#24230;&#25110;&#22522;&#20110;&#37319;&#26679;&#30340;&#20248;&#21270;&#31243;&#24207;&#30340;&#39640;&#36136;&#37327;&#35774;&#35745;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#20174;&#22836;&#24320;&#22987;&#20248;&#21270;&#35774;&#35745;&#38656;&#35201;&#35768;&#22810;&#26114;&#36149;&#30340;&#27169;&#22411;&#26597;&#35810;&#65292;&#24182;&#19988;&#36825;&#20123;&#31243;&#24207;&#22312;&#38750;&#20984;&#25110;&#39640;&#32500;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#22522;&#26412;&#22833;&#36133;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#65288;DDMs&#65289;&#26469;&#39640;&#25928;&#22320;&#35299;&#20915;&#21453;&#21521;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31890;&#23376;&#37319;&#26679;&#31639;&#27861;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems. In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving
&lt;/p&gt;</description></item><item><title>TML&#36719;&#20214;&#21253;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#19968;&#22871;&#20840;&#38754;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;R&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#28909;&#24102;&#20984;&#24615;&#30456;&#20851;&#30340;&#22522;&#26412;&#35745;&#31639;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#28909;&#24102;&#24230;&#37327;&#36827;&#34892;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.01082</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#28909;&#24102;&#20960;&#20309;&#24037;&#20855;&#65306;TML&#36719;&#20214;&#21253;
&lt;/p&gt;
&lt;p&gt;
Tropical Geometric Tools for Machine Learning: the TML package. (arXiv:2309.01082v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01082
&lt;/p&gt;
&lt;p&gt;
TML&#36719;&#20214;&#21253;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#19968;&#22871;&#20840;&#38754;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;R&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#28909;&#24102;&#20984;&#24615;&#30456;&#20851;&#30340;&#22522;&#26412;&#35745;&#31639;&#21644;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#28909;&#24102;&#24230;&#37327;&#36827;&#34892;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28909;&#24102;&#20960;&#20309;&#23398;&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#35768;&#22810;&#30452;&#25509;&#24212;&#29992;&#20110;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#30340;&#24037;&#20855;&#12290;TML&#36719;&#20214;&#21253;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#19968;&#22871;&#20840;&#38754;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#30340;R&#36719;&#20214;&#21253;&#65292;&#29992;&#20110;&#22788;&#29702;&#19982;&#28909;&#24102;&#20984;&#24615;&#30456;&#20851;&#30340;&#22522;&#26412;&#35745;&#31639;&#12289;&#28909;&#24102;&#20984;&#38598;&#30340;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#28909;&#24102;&#24230;&#37327;&#21644;&#28909;&#24102;&#25237;&#24433;&#29615;&#19978;&#30340;max-plus&#20195;&#25968;&#36827;&#34892;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;&#20027;&#35201;&#30340;&#65292;TML&#36719;&#20214;&#21253;&#20351;&#29992;Hit and Run Markov chain Monte Carlo&#37319;&#26679;&#22120;&#19982;&#28909;&#24102;&#24230;&#37327;&#20316;&#20026;&#32479;&#35745;&#25512;&#26029;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#38500;&#20102;&#22522;&#26412;&#35745;&#31639;&#21644;&#28909;&#24102;HAR&#37319;&#26679;&#22120;&#30340;&#21508;&#31181;&#24212;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20851;&#27880;TML&#36719;&#20214;&#21253;&#20013;&#21253;&#21547;&#30340;&#20960;&#31181;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21253;&#25324;&#28909;&#24102;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#28909;&#24102;&#36923;&#36753;&#22238;&#24402;&#21644;&#28909;&#24102;&#26680;&#23494;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, developments in tropical geometry have provided a number of uses directly applicable to problems in statistical learning. The TML package is the first R package which contains a comprehensive set of tools and methods used for basic computations related to tropical convexity, visualization of tropically convex sets, as well as supervised and unsupervised learning models using the tropical metric under the max-plus algebra over the tropical projective torus. Primarily, the TML package employs a Hit and Run Markov chain Monte Carlo sampler in conjunction with the tropical metric as its main tool for statistical inference. In addition to basic computation and various applications of the tropical HAR sampler, we also focus on several supervised and unsupervised methods incorporated in the TML package including tropical principal component analysis, tropical logistic regression and tropical kernel density estimation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.01029</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#37322;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#21450;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#19981;&#26126;&#30830;&#65292;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#20026;&#19979;&#28216;&#24212;&#29992;&#24102;&#26469;&#20102;&#19981;&#24517;&#35201;&#30340;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#38416;&#26126;&#23427;&#20204;&#30340;&#34892;&#20026;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#27010;&#36848;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;LLMs&#30340;&#35757;&#32451;&#33539;&#24335;&#23558;&#25216;&#26415;&#36827;&#34892;&#20998;&#31867;&#65306;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#21644;&#25552;&#31034;&#33539;&#24335;&#12290;&#23545;&#20110;&#27599;&#20010;&#33539;&#24335;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29983;&#25104;&#20010;&#20307;&#39044;&#27979;&#30340;&#23616;&#37096;&#35299;&#37322;&#21644;&#25972;&#20307;&#27169;&#22411;&#30693;&#35782;&#30340;&#20840;&#23616;&#35299;&#37322;&#30340;&#30446;&#26631;&#21644;&#20027;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35780;&#20272;&#29983;&#25104;&#35299;&#37322;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#35299;&#37322;&#26469;&#35843;&#35797;&#27169;&#22411;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.16215</link><description>&lt;p&gt;
&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#30772;&#22351;&#29616;&#26377;&#26631;&#20934;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20002;&#22833;&#29575;&#35270;&#39057;&#21387;&#32553;&#36890;&#24120;&#29992;&#20110;&#20256;&#36755;&#21644;&#23384;&#20648;&#35270;&#39057;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#36827;&#38454;&#65288;&#31070;&#32463;&#65289;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#32479;&#19968;&#35270;&#39057;&#32534;&#30721;&#22120;&#65288;&#22914;H.264&#25110;H.265&#65289;&#20173;&#28982;&#26159;&#20107;&#23454;&#19978;&#30340;&#26631;&#20934;&#12290;&#22312;&#38754;&#23545;&#21160;&#24577;&#32593;&#32476;&#24102;&#23485;&#26465;&#20214;&#30340;&#35270;&#39057;&#20256;&#36755;&#20013;&#65292;&#35270;&#39057;&#32534;&#30721;&#22120;&#38656;&#35201;&#36866;&#24212;&#38750;&#24120;&#19981;&#21516;&#30340;&#21387;&#32553;&#24378;&#24230;&#12290;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#22686;&#24378;&#32534;&#35299;&#30721;&#22120;&#30340;&#21387;&#32553;&#33021;&#21147;&#65292;&#20197;&#28385;&#36275;&#24102;&#23485;&#38480;&#21046;&#24182;&#23613;&#37327;&#20943;&#23569;&#35270;&#39057;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#35270;&#39057;&#32534;&#30721;&#22120;&#21450;&#20854;&#36895;&#29575;&#25511;&#21046;&#27169;&#22359;&#26159;&#20026;&#20102;&#26368;&#23567;&#21270;&#20154;&#31867;&#36136;&#37327;&#35780;&#20272;&#32780;&#24320;&#21457;&#30340;&#65292;&#21364;&#27809;&#26377;&#32771;&#34385;&#20445;&#25252;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#23398;&#20064;&#30340;&#28145;&#24230;&#35270;&#39057;&#32534;&#30721;&#25511;&#21046;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#24102;&#23485;&#38480;&#21046;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#65292;&#24182;&#19981;&#30772;&#22351;&#29616;&#26377;&#30340;&#26631;&#20934;&#21270;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#24120;&#35265;&#30340;&#35270;&#35273;&#20219;&#21153;&#65288;&#35821;&#20041;&#20998;&#21106;...
&lt;/p&gt;
&lt;p&gt;
Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
&lt;/p&gt;</description></item><item><title>LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.15930</link><description>&lt;p&gt;
LLaSM: &#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15930
&lt;/p&gt;
&lt;p&gt;
LLaSM&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#36890;&#36807;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;&#19978;&#65292;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#26469;&#36981;&#24490;&#35270;&#35273;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#35821;&#38899;&#20063;&#26159;&#20154;&#31867;&#19982;&#19990;&#30028;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#19968;&#20010;&#36890;&#29992;&#30340;&#21161;&#25163;&#26469;&#35828;&#65292;&#33021;&#22815;&#36981;&#24490;&#22810;&#27169;&#24577;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#38899;&#27169;&#22411;&#65288;LLaSM&#65289;&#12290;LLaSM&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#38899;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#36328;&#27169;&#24577;&#23545;&#35805;&#33021;&#21147;&#65292;&#33021;&#22815;&#36981;&#24490;&#35821;&#38899;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLaSM&#23637;&#31034;&#20102;&#19968;&#31181;&#26356;&#26041;&#20415;&#33258;&#28982;&#30340;&#20154;&#26426;&#20132;&#20114;&#26041;&#24335;&#12290;&#20026;&#20102;&#25903;&#25345;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#38899;&#25351;&#20196;&#25968;&#25454;&#38598;LLaSM-Audio-Instructions&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/LinkSoul-AI/LLaSM&#21644;ht&#19978;&#26597;&#30475;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.15321</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Epsilon Scaling&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#8220;&#26333;&#20809;&#20559;&#24046;&#8221;&#38382;&#39064;&#65292;&#21363;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#65292;&#32570;&#20047;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#39318;&#20808;&#23545;&#37319;&#26679;&#20998;&#24067;&#36827;&#34892;&#20998;&#26512;&#24314;&#27169;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;&#39044;&#27979;&#35823;&#24046;&#24402;&#22240;&#20026;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#30340;&#26681;&#26412;&#21407;&#22240;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#38500;&#20102;&#38416;&#26126;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#20813;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;Epsilon Scaling&#65292;&#20197;&#20943;&#36731;&#26333;&#20809;&#20559;&#24046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Epsilon Scaling&#36890;&#36807;&#32553;&#23567;&#32593;&#32476;&#36755;&#20986;&#65288;Epsilon&#65289;&#26126;&#30830;&#22320;&#23558;&#37319;&#26679;&#36712;&#36857;&#31227;&#36817;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21040;&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37319;&#26679;&#20043;&#38388;&#30340;&#36755;&#20837;&#19981;&#21305;&#37197;&#12290;&#22312;&#21508;&#31181;&#25193;&#25955;&#26694;&#26550;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
&lt;/p&gt;</description></item><item><title>3D-MuPPET&#26159;&#19968;&#20010;&#29992;&#20110;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#21482;&#40509;&#23376;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#23454;&#26102;&#25512;&#27979;2D&#20851;&#38190;&#28857;&#24182;&#23558;&#20854;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#21305;&#37197;&#21644;2D&#36319;&#36394;&#22120;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22810;&#21482;&#40509;&#23376;&#25968;&#25454;&#65292;&#31616;&#21270;&#39046;&#22495;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2308.15316</link><description>&lt;p&gt;
3D-MuPPET: 3D&#22810;&#40509;&#23039;&#24577;&#20272;&#35745;&#19982;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking. (arXiv:2308.15316v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15316
&lt;/p&gt;
&lt;p&gt;
3D-MuPPET&#26159;&#19968;&#20010;&#29992;&#20110;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#21482;&#40509;&#23376;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#23454;&#26102;&#25512;&#27979;2D&#20851;&#38190;&#28857;&#24182;&#23558;&#20854;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#21305;&#37197;&#21644;2D&#36319;&#36394;&#22120;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#20855;&#26377;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26694;&#26550;&#36824;&#33021;&#22312;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22810;&#21482;&#40509;&#23376;&#25968;&#25454;&#65292;&#31616;&#21270;&#39046;&#22495;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#21160;&#29289;&#23039;&#21183;&#36319;&#36394;&#30340;&#26080;&#26631;&#35760;&#26041;&#27861;&#24050;&#26377;&#25152;&#21457;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#29992;&#20110;&#36861;&#36394;&#22823;&#35268;&#27169;&#21160;&#29289;&#32676;&#20307;&#30340;&#19977;&#32500;&#26694;&#26550;&#21644;&#22522;&#20934;&#12290;&#20026;&#20102;&#24357;&#34917;&#25991;&#29486;&#20013;&#30340;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;3D-MuPPET&#65292;&#19968;&#20010;&#20351;&#29992;&#22810;&#35270;&#35282;&#23454;&#26102;&#20272;&#35745;&#21644;&#36319;&#36394;&#22810;&#36798;10&#21482;&#40509;&#23376;&#30340;&#19977;&#32500;&#23039;&#21183;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#23039;&#21183;&#20272;&#35745;&#22120;&#65292;&#29992;&#20110;&#25512;&#27979;&#22810;&#21482;&#40509;&#23376;&#30340;2D&#20851;&#38190;&#28857;&#21644;&#36793;&#30028;&#26694;&#65292;&#28982;&#21518;&#23558;&#20851;&#38190;&#28857;&#19977;&#35282;&#21270;&#21040;3D&#31354;&#38388;&#12290;&#23545;&#20110;&#21305;&#37197;&#23545;&#24212;&#20851;&#31995;&#65292;&#25105;&#20204;&#39318;&#20808;&#21160;&#24577;&#22320;&#23558;2D&#26816;&#27979;&#32467;&#26524;&#19982;&#31532;&#19968;&#24103;&#20013;&#30340;&#20840;&#23616;&#36523;&#20221;&#36827;&#34892;&#21305;&#37197;&#65292;&#28982;&#21518;&#20351;&#29992;2D&#36319;&#36394;&#22120;&#22312;&#21518;&#32493;&#24103;&#20013;&#32500;&#25345;&#23545;&#24212;&#20851;&#31995;&#12290;&#25105;&#20204;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;3D&#23039;&#21183;&#20272;&#35745;&#22120;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#27491;&#30830;&#20851;&#38190;&#28857;&#30334;&#20998;&#27604;&#65288;PCK&#65289;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#20363;&#65292;&#21363;&#25105;&#20204;&#20351;&#29992;&#21333;&#21482;&#40509;&#23376;&#30340;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#24182;&#22312;&#21253;&#21547;&#22810;&#21482;&#40509;&#23376;&#30340;&#25968;&#25454;&#19978;&#24471;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#23545;&#26032;&#22330;&#26223;&#30340;&#39046;&#22495;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markerless methods for animal posture tracking have been developing recently, but frameworks and benchmarks for tracking large animal groups in 3D are still lacking. To overcome this gap in the literature, we present 3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at interactive speed using multiple-views. We train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D. For correspondence matching, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain correspondences accross views in subsequent frames. We achieve comparable accuracy to a state of the art 3D pose estimator for Root Mean Square Error (RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel use case where our model trained with data of single pigeons provides comparable results on data containing multiple pigeons. This can simplify the domain shift to new sp
&lt;/p&gt;</description></item><item><title>&#24555;&#36895;&#21069;&#39304;&#32593;&#32476;&#26159;&#19968;&#31181;&#23545;&#20110;&#21069;&#39304;&#32593;&#32476;&#30340;&#25913;&#36827;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#27604;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#65292;&#23427;&#20204;&#21487;&#20197;&#20165;&#20351;&#29992;1%&#30340;&#23618;&#31070;&#32463;&#20803;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;94.2%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.14711</link><description>&lt;p&gt;
&#24555;&#36895;&#21069;&#39304;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fast Feedforward Networks. (arXiv:2308.14711v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14711
&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21069;&#39304;&#32593;&#32476;&#26159;&#19968;&#31181;&#23545;&#20110;&#21069;&#39304;&#32593;&#32476;&#30340;&#25913;&#36827;&#26550;&#26500;&#65292;&#33021;&#22815;&#20197;&#26356;&#24555;&#30340;&#36895;&#24230;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#27604;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#65292;&#23427;&#20204;&#21487;&#20197;&#20165;&#20351;&#29992;1%&#30340;&#23618;&#31070;&#32463;&#20803;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;94.2%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#24555;&#36895;&#21069;&#39304;(FFF)&#26550;&#26500;&#65292;&#25171;&#30772;&#20102;&#23618;&#22823;&#23567;&#19982;&#25512;&#29702;&#25104;&#26412;&#20043;&#38388;&#30340;&#32447;&#24615;&#20851;&#31995;&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#20110;&#21069;&#39304;&#32593;&#32476;&#30340;&#23545;&#25968;&#26102;&#38388;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;FFF&#27604;&#21069;&#39304;&#32593;&#32476;&#24555;&#39640;&#36798;220&#20493;&#65292;&#27604;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#24555;&#39640;&#36798;6&#20493;&#65292;&#24182;&#19988;&#30001;&#20110;&#26080;&#22122;&#22768;&#26465;&#20214;&#25191;&#34892;&#32780;&#34920;&#29616;&#20986;&#27604;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26356;&#22909;&#30340;&#35757;&#32451;&#24615;&#33021;&#12290;&#23558;FFF&#25512;&#21040;&#26497;&#38480;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#35270;&#35273;&#36716;&#25442;&#22120;&#20013;&#65292;&#23427;&#20204;&#21487;&#20165;&#20351;&#29992;1%&#30340;&#23618;&#31070;&#32463;&#20803;&#36827;&#34892;&#25512;&#29702;&#65292;&#21516;&#26102;&#20445;&#25345;94.2%&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We break the linear link between the layer size and its inference cost by introducing the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11446</link><description>&lt;p&gt;
&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#26377;&#21161;&#20110;&#21307;&#30103;&#25968;&#25454;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#21644;&#20998;&#26512;&#21307;&#30103;&#25968;&#25454;&#20013;&#30340;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#20174;&#32780;&#36229;&#36234;&#20256;&#32479;&#21333;&#19968;&#27169;&#22411;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#35782;&#21035;&#20986;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#36807;&#31243;&#36890;&#24120;&#20197;&#36873;&#25321;&#26368;&#22823;&#21270;&#26576;&#20010;&#24615;&#33021;&#25351;&#26631;&#30340;&#21333;&#19968;&#27169;&#22411;&#20316;&#20026;&#26368;&#32456;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#23545;&#31245;&#24494;&#24046;&#19968;&#20123;&#30340;&#27169;&#22411;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#34987;&#24573;&#35270;&#12290;&#23588;&#20854;&#22312;&#21307;&#30103;&#21644;&#20581;&#24247;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#65292;&#36824;&#21253;&#25324;&#20135;&#29983;&#26377;&#20215;&#20540;&#30340;&#27934;&#23519;&#65292;&#20165;&#20165;&#20381;&#36182;&#24615;&#33021;&#25351;&#26631;&#21487;&#33021;&#20250;&#23548;&#33268;&#35823;&#23548;&#25110;&#19981;&#23436;&#25972;&#30340;&#32467;&#35770;&#12290;&#24403;&#22788;&#29702;&#19968;&#32452;&#24615;&#33021;&#25509;&#36817;&#26368;&#20248;&#30340;&#27169;&#22411;&#38598;&#21512;&#26102;&#65292;&#21363;&#25152;&#35859;&#30340;"&#25289;&#33298;&#33945;&#38598;&#21512;"&#65292;&#36825;&#20010;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#12290;&#36825;&#26679;&#30340;&#38598;&#21512;&#21487;&#33021;&#21253;&#21547;&#25551;&#36848;&#25968;&#25454;&#30340;&#19981;&#21516;&#26041;&#24335;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36807;&#31243;&#26469;&#25506;&#32034;&#25289;&#33298;&#33945;&#38598;&#21512;&#27169;&#22411;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#24314;&#27169;&#26041;&#27861;&#12290;&#26680;&#24515;&#26159;&#36890;&#36807;&#24341;&#20837;&#30340;"&#25289;&#33298;&#33945;&#26816;&#27979;"&#31639;&#27861;&#26469;&#35782;&#21035;&#25289;&#33298;&#33945;&#38598;&#21512;&#20013;&#26368;&#19981;&#21516;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11127</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#22810;&#24378;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#29616;&#26377;&#30340;&#34920;&#36798;&#33021;&#21147;&#24230;&#37327;&#26631;&#20934;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#21033;&#29992;&#22270;&#20013;&#30340;&#29992;&#25143;-&#29289;&#21697;&#21327;&#20316;&#36807;&#28388;&#20449;&#21495;&#36827;&#34892;&#25512;&#33616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#22312;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#27169;&#22411;&#20013;&#30340;&#32463;&#39564;&#26377;&#25928;&#24615;&#65292;&#20294;&#23545;&#20110;&#23427;&#20204;&#30340;&#33021;&#21147;&#30340;&#29702;&#35770;&#34920;&#36848;&#38750;&#24120;&#31232;&#23569;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;GNNs&#30340;&#19968;&#33324;&#34920;&#36798;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#28040;&#24687;&#20256;&#36882;GNNs&#33267;&#22810;&#19982;Weisfeiler-Lehman&#27979;&#35797;&#19968;&#26679;&#24378;&#22823;&#65292;&#24182;&#19988;&#19982;&#38543;&#26426;&#33410;&#28857;&#21021;&#22987;&#21270;&#30456;&#32467;&#21512;&#30340;GNNs&#26159;&#36890;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#8220;&#34920;&#36798;&#33021;&#21147;&#8221;&#27010;&#24565;&#20173;&#28982;&#23450;&#20041;&#27169;&#31946;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#37319;&#29992;&#22270;&#21516;&#26500;&#27979;&#35797;&#20316;&#20026;&#34920;&#36798;&#33021;&#21147;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20294;&#36825;&#31181;&#22270;&#32423;&#20219;&#21153;&#21487;&#33021;&#19981;&#33021;&#26377;&#25928;&#35780;&#20272;&#27169;&#22411;&#22312;&#25512;&#33616;&#20013;&#21306;&#20998;&#19981;&#21516;&#25509;&#36817;&#31243;&#24230;&#33410;&#28857;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;GNNs&#22312;&#25512;&#33616;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07843</link><description>&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) &#35813;&#35770;&#25991;&#26631;&#39064;&#24050;&#32763;&#35793;&#65306;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31216;&#20026;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#19978;&#19979;&#25991;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#19982;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#65292;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#24178;&#39044;&#25514;&#26045;&#12290;&#35813;&#31639;&#27861;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#65292;&#24182;&#36890;&#36807;&#27169;&#25311;&#23637;&#31034;&#20102;&#33391;&#22909;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#21307;&#30103;&#26088;&#22312;&#36890;&#36807;&#22312;&#20010;&#20154;&#26085;&#24120;&#29983;&#27963;&#20013;&#25552;&#20379;&#24178;&#39044;&#26469;&#25552;&#39640;&#20581;&#24247;&#32467;&#26524;&#12290;&#29031;&#39038;&#20276;&#20387;&#21644;&#31038;&#20250;&#25903;&#25345;&#32593;&#32476;&#30340;&#21442;&#19982;&#32463;&#24120;&#22312;&#24110;&#21161;&#20010;&#20154;&#31649;&#29702;&#32321;&#37325;&#30340;&#21307;&#30103;&#26465;&#20214;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20026;&#31227;&#21160;&#21307;&#30103;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#35774;&#35745;&#38024;&#23545;&#20108;&#20803;&#20851;&#31995;&#8212;&#8212;&#30446;&#26631;&#20154;&#21644;&#20854;&#29031;&#39038;&#20276;&#20387;&#20043;&#38388;&#20851;&#31995;&#8212;&#8212;&#20197;&#25552;&#39640;&#31038;&#20250;&#25903;&#25345;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;Dyadic RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#22240;&#32032;&#21644;&#30446;&#26631;&#20154;&#21450;&#20854;&#29031;&#39038;&#20276;&#20387;&#30340;&#36807;&#21435;&#21453;&#39304;&#20010;&#24615;&#21270;&#24178;&#39044;&#25514;&#26045;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#22810;&#32452;&#24178;&#39044;&#25514;&#26045;&#24433;&#21709;&#30528;&#20108;&#20803;&#20851;&#31995;&#22312;&#22810;&#20010;&#26102;&#38388;&#38388;&#38548;&#20869;&#12290;&#24320;&#21457;&#30340;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#26159;&#36125;&#21494;&#26031;&#21644;&#23618;&#27425;&#30340;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#38382;&#39064;&#35774;&#23450;&#65292;&#24320;&#21457;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#24182;&#30830;&#23450;&#20102;&#36951;&#25022;&#36793;&#30028;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
&lt;/p&gt;</description></item><item><title>RusKey&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38190;&#20540;&#23384;&#20648;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#32447;&#32534;&#25490;LSM&#26641;&#32467;&#26500;&#26469;&#23454;&#29616;&#22312;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#19979;&#30340;&#31283;&#20581;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;FLSM&#26641;&#35774;&#35745;&#26469;&#39640;&#25928;&#36716;&#25442;&#19981;&#21516;&#21387;&#23454;&#31574;&#30053;&#65292;&#26080;&#38656;&#20808;&#39564;&#24037;&#20316;&#36127;&#36733;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.07013</link><description>&lt;p&gt;
&#23398;&#20064;&#20248;&#21270;LSM&#26641;&#65306;&#38754;&#21521;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38190;&#20540;&#23384;&#20648;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads. (arXiv:2308.07013v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07013
&lt;/p&gt;
&lt;p&gt;
RusKey&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#38190;&#20540;&#23384;&#20648;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#32447;&#32534;&#25490;LSM&#26641;&#32467;&#26500;&#26469;&#23454;&#29616;&#22312;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#19979;&#30340;&#31283;&#20581;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;FLSM&#26641;&#35774;&#35745;&#26469;&#39640;&#25928;&#36716;&#25442;&#19981;&#21516;&#21387;&#23454;&#31574;&#30053;&#65292;&#26080;&#38656;&#20808;&#39564;&#24037;&#20316;&#36127;&#36733;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LSM&#26641;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38190;&#20540;&#23384;&#20648;&#30340;&#23384;&#20648;&#21518;&#31471;&#12290;&#28982;&#32780;&#65292;&#22312;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#19979;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#30340;&#38382;&#39064;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#25110;&#35780;&#20272;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RusKey&#65292;&#19968;&#20010;&#20855;&#26377;&#20197;&#19979;&#26032;&#29305;&#24615;&#30340;&#38190;&#20540;&#23384;&#20648;&#65306;&#65288;1&#65289;RusKey&#26159;&#31532;&#19968;&#20010;&#23581;&#35797;&#22312;&#32447;&#32534;&#25490;LSM&#26641;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#22312;&#21160;&#24577;&#24037;&#20316;&#36127;&#36733;&#24773;&#20917;&#19979;&#30340;&#31283;&#20581;&#24615;&#33021;&#65307;&#65288;2&#65289;RusKey&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26469;&#24341;&#23548;LSM&#26641;&#36716;&#25442;&#30340;&#30740;&#31350;&#65307;&#65288;3&#65289;RusKey&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;LSM&#26641;&#35774;&#35745;&#65292;&#31216;&#20026;FLSM&#26641;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#30340;&#21387;&#23454;&#31574;&#30053;&#20043;&#38388;&#23454;&#29616;&#39640;&#25928;&#36716;&#25442;&#65292;&#36825;&#26159;&#21160;&#24577;&#38190;&#20540;&#23384;&#20648;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#26032;&#35774;&#35745;&#30340;&#20248;&#36234;&#24615;&#65307;&#65288;4&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;RusKey&#19981;&#38656;&#35201;&#20808;&#39564;&#30340;&#24037;&#20316;&#36127;&#36733;&#30693;&#35782;&#36827;&#34892;&#31995;&#32479;&#35843;&#25972;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;RusKey&#22312;&#22810;&#26679;&#30340;&#24037;&#20316;&#36127;&#36733;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#31283;&#20581;&#24615;&#65292;&#26368;&#39640;&#21487;&#36798;4&#20493;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#22120;&#30697;&#38453;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#29305;&#24449;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#21333;&#21464;&#37327;&#37327;&#21270;&#26469;&#35782;&#21035;&#21464;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24863;&#20852;&#36259;&#20301;&#32622;&#12290;</title><link>http://arxiv.org/abs/2308.06213</link><description>&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#22120;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Change Point Detection With Conceptors. (arXiv:2308.06213v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06213
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#24565;&#22120;&#30697;&#38453;&#36827;&#34892;&#21464;&#28857;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#29305;&#24449;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#21333;&#21464;&#37327;&#37327;&#21270;&#26469;&#35782;&#21035;&#21464;&#28857;&#12290;&#35813;&#26041;&#27861;&#22312;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24863;&#20852;&#36259;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#21464;&#28857;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21457;&#29983;&#21464;&#21270;&#30340;&#28857;&#12290;&#23545;&#20110;&#21333;&#21464;&#37327;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65292;&#36825;&#20010;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#20102;&#36739;&#22909;&#30340;&#30740;&#31350;&#65292;&#20294;&#26159;&#38543;&#30528;&#32500;&#24230;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#22686;&#21152;&#65292;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#38024;&#23545;&#33267;&#22810;&#19968;&#20010;&#21464;&#28857;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27010;&#24565;&#22120;&#30697;&#38453;&#26469;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20013;&#25351;&#23450;&#35757;&#32451;&#31383;&#21475;&#30340;&#29305;&#24449;&#21160;&#24577;&#12290;&#30456;&#20851;&#30340;&#38543;&#26426;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#25968;&#25454;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#19988;&#36890;&#36807;&#35745;&#31639;&#29305;&#24449;&#21270;&#19982;&#20195;&#34920;&#24615;&#27010;&#24565;&#22120;&#30697;&#38453;&#25152;&#24352;&#25104;&#31354;&#38388;&#20043;&#38388;&#30340;&#36317;&#31163;&#30340;&#21333;&#21464;&#37327;&#37327;&#21270;&#26469;&#35782;&#21035;&#21464;&#28857;&#12290;&#36825;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#31034;&#21487;&#33021;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24863;&#20852;&#36259;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#30495;&#23454;&#21464;&#28857;&#30340;&#19968;&#33268;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#31227;&#21160;&#22359;&#33258;&#21161;&#27861;&#20135;&#29983;&#32479;&#35745;&#37327;&#30340;&#20998;&#20301;&#25968;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#26465;&#20214;&#21644;&#26080;&#26465;&#20214;&#30340;&#21464;&#28857;&#26816;&#27979;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#65292;&#32467;&#26524;&#34920;&#26126;PINNs&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#22788;&#29702;&#21453;&#21521;&#38382;&#39064;&#26041;&#38754;&#27604;&#20256;&#32479;CFD&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.04501</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#21387;&#32553;&#26426;&#21494;&#26629;&#27969;&#22330;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Investigation of compressor cascade flow based on physics-informed neural networks. (arXiv:2308.04501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#65292;&#32467;&#26524;&#34920;&#26126;PINNs&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#22788;&#29702;&#21453;&#21521;&#38382;&#39064;&#26041;&#38754;&#27604;&#20256;&#32479;CFD&#26041;&#27861;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#20351;&#29992;&#26032;&#20852;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#26469;&#39044;&#27979;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#20108;&#32500;&#38382;&#39064;&#19978;&#36827;&#34892;&#28436;&#31034;&#65292;&#21516;&#26102;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#20013;&#30340;Navier-Stokes&#26041;&#31243;&#12290;&#22312;&#27491;&#21521;&#38382;&#39064;&#20013;&#65292;PINNs&#33021;&#22815;&#26377;&#25928;&#39044;&#27979;&#21387;&#32553;&#26426;&#30340;&#27969;&#22330;&#12290;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30456;&#27604;&#65292;PINNs&#27169;&#22411;&#34701;&#21512;&#20102;&#30456;&#20851;&#37327;&#20043;&#38388;&#30340;&#29289;&#29702;&#20851;&#31995;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#31934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#27809;&#26377;&#37096;&#20998;&#36793;&#30028;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;PINNs&#22312;&#22788;&#29702;&#21453;&#21521;&#38382;&#39064;&#26102;&#26174;&#31034;&#20986;&#26174;&#33879;&#20248;&#21183;&#12290;PINNs&#20165;&#22522;&#20110;&#37096;&#20998;&#36895;&#24230;&#21521;&#37327;&#21644;&#22721;&#21387;&#20449;&#24687;&#25104;&#21151;&#37325;&#26500;&#20102;&#21387;&#32553;&#26426;&#21494;&#26629;&#30340;&#27969;&#22330;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#35777;&#25454;&#65292;&#35777;&#26126;PINNs&#20026;&#28065;&#36718;&#26426;&#26800;&#35774;&#35745;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21462;&#20195;&#20102;&#24403;&#21069;&#20027;&#23548;&#30340;CFD&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we utilize the emerging Physics Informed Neural Networks (PINNs) approach for the first time to predict the flow field of a compressor cascade. The approach is demonstrated on a two-dimensional problem, incorporating Navier-Stokes equations in both the forward and inverse problems. In the forward problem, PINNs effectively predict the flow field of the compressor. The key advantage over Deep Neural Networks (DNNs) is that the PINNs model incorporates a physical relationship between the relevant quantities, resulting in more precise predictions. PINNs show obvious advantages over the traditional CFD approaches when dealing with inverse problems in the absence of partial boundary conditions. PINNs successfully reconstruct the flow field of the compressor cascade solely based on partial velocity vectors and wall pressure information. This research provides compelling evidence that PINNs offer turbomachinery designers a promising alternative to the current dominant CFD metho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;</title><link>http://arxiv.org/abs/2308.03887</link><description>&lt;p&gt;
&#29992;&#19968;&#31181;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach. (arXiv:2308.03887v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35270;&#39057;&#26174;&#24494;&#38236;&#35760;&#24405;&#20934;&#30830;&#36319;&#36394;&#27963;&#32454;&#32990;&#20173;&#28982;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#26041;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#20960;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#24212;&#29992;&#23581;&#35797;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#25972;&#21512;&#21040;&#35813;&#20219;&#21153;&#20013;&#65292;&#20294;&#22823;&#37096;&#20998;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#23884;&#20837;&#20854;&#26550;&#26500;&#25110;&#20854;&#20182;&#21069;&#25552;&#26465;&#20214;&#20013;&#30340;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24191;&#20041;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#32454;&#32990;&#21487;&#20197;&#26681;&#25454;&#20854;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#30340;&#20551;&#35774;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#36830;&#32493;&#24103;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39069;&#22806;&#20248;&#28857;&#26159;&#32454;&#32990;&#30340;&#36816;&#21160;&#27169;&#24335;&#21487;&#20197;&#23436;&#20840;&#30001;&#39044;&#27979;&#22120;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#35270;&#39057;&#24103;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated thro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#23558;&#28304;&#20934;&#30830;&#29575;&#32435;&#20837;&#24230;&#37327;&#20013;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;MLP&#20998;&#31867;&#22120;&#36827;&#34892;&#25913;&#36827;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#21407;&#26377;&#24230;&#37327;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#23558;&#20854;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.00287</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35780;&#20272;&#24230;&#37327;&#29992;&#20110;&#23454;&#36341;&#21644;&#33258;&#21160;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation. (arXiv:2308.00287v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00287
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#35780;&#20272;&#24230;&#37327;&#12290;&#36890;&#36807;&#23558;&#28304;&#20934;&#30830;&#29575;&#32435;&#20837;&#24230;&#37327;&#20013;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;MLP&#20998;&#31867;&#22120;&#36827;&#34892;&#25913;&#36827;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#21407;&#26377;&#24230;&#37327;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#23558;&#20854;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26041;&#27861;&#21487;&#20197;&#22312;&#27809;&#26377;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#26631;&#35760;&#30340;&#30446;&#26631;&#39564;&#35777;&#38598;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#20248;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#19968;&#31181;&#33021;&#22815;&#35780;&#20272;&#36716;&#31227;&#27169;&#22411;&#36136;&#37327;&#30340;&#35780;&#20272;&#24230;&#37327;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#30446;&#26631;&#39564;&#35777;&#26631;&#31614;&#12290;&#25105;&#20204;&#39318;&#20808;&#22522;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#20114;&#20449;&#24687;&#30340;&#24230;&#37327;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#20010;&#24230;&#37327;&#30340;&#19977;&#20010;&#26222;&#36941;&#38382;&#39064;&#65306;1&#65289;&#23427;&#27809;&#26377;&#32771;&#34385;&#28304;&#32467;&#26500;&#65307;2&#65289;&#23427;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#65307;3&#65289;&#23427;&#26080;&#27861;&#26816;&#27979;&#21040;&#30001;&#20110;&#28304;&#21644;&#30446;&#26631;&#29305;&#24449;&#36807;&#24230;&#23545;&#40784;&#23548;&#33268;&#30340;&#36127;&#36801;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#21069;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#28304;&#20934;&#30830;&#29575;&#32435;&#20837;&#24230;&#37327;&#20013;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#29420;&#31435;&#30340;&#26032;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#36827;&#20102;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#22686;&#24378;&#30340;&#24230;&#37327;&#19982;&#25968;&#25454;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#24471;&#21040;&#19968;&#20010;n
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.14439</link><description>&lt;p&gt;
&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968;&#30340;&#35299;&#26512;&#31215;&#20998;&#65292;&#20855;&#26377;&#35745;&#31639;&#31934;&#30830;&#31215;&#20998;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#65292;&#32780;&#19988;&#36824;&#20171;&#32461;&#20102;&#23558;&#23398;&#20064;&#20989;&#25968;&#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#20197;&#21450;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23398;&#20064;&#20989;&#25968;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#31215;&#20998;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#65292;&#20294;&#26159;&#36825;&#31181;&#31215;&#20998;&#36890;&#24120;&#26159;&#36890;&#36807;&#25968;&#20540;&#26041;&#27861;&#26469;&#35745;&#31639;&#30340;&#65292;&#22240;&#20026;&#35299;&#26512;&#35745;&#31639;&#31215;&#20998;&#36807;&#31243;&#22797;&#26434;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#36825;&#26679;&#30340;&#23398;&#20064;&#20989;&#25968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#20989;&#25968; $f$ &#35299;&#26512;&#31215;&#20998;&#30340;&#26041;&#27861;&#12290;&#36825;&#20801;&#35768;&#31934;&#30830;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#30340;&#31215;&#20998;&#65292;&#24182;&#19988;&#36890;&#36807;&#23558;&#32422;&#26463;&#30452;&#25509;&#24212;&#29992;&#20110;&#31215;&#20998;&#26469;&#23545;&#32422;&#26463;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558; $f$ &#32422;&#26463;&#20026;&#27491;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#35768;&#22810;&#24212;&#29992;&#65288;&#20363;&#22914;&#27010;&#29575;&#20998;&#24067;&#12289;&#36317;&#31163;&#24230;&#37327;&#31561;&#65289;&#25152;&#24517;&#38656;&#30340;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20960;&#20010;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#30340;&#22266;&#23450;&#31215;&#20998;&#31070;&#32463;&#32593;&#32476;&#65288;FINN&#65289;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13885</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#39640;&#25928;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#65292;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#20998;&#26512;&#20272;&#35745;&#22120;&#12290;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#36755;&#20837;&#25968;&#25454;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22122;&#22768;&#65288;&#36890;&#24120;&#26159;&#38543;&#26426;&#30340;&#65289;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#26469;&#25429;&#25417;&#65292;&#21363;&#22312;&#36755;&#20837;&#21608;&#22260;&#30340;&#23616;&#37096;&#21306;&#22495;&#20869;&#27169;&#22411;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#37319;&#26679;&#30340;&#35745;&#31639;&#23616;&#37096;&#40065;&#26834;&#24615;&#30340;&#26420;&#32032;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#32780;&#35328;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23616;&#37096;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#21644;&#22810;&#20803;&#27491;&#24577;CDF&#24320;&#21457;&#20102;&#39318;&#20010;&#20998;&#26512;&#20272;&#35745;&#22120;&#65292;&#20197;&#39640;&#25928;&#35745;&#31639;&#22810;&#31867;&#21035;&#21028;&#21035;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#20272;&#35745;&#22120;&#30340;&#25512;&#23548;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#40065;&#26834;&#24615;&#19982;&#38543;&#26426;&#24179;&#28369;&#21644;softmax&#27010;&#29575;&#31561;&#27010;&#24565;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#36825;&#20123;&#20272;&#35745;&#22120;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#35745;&#31639;&#20102;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23616;&#37096;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RED CoMETS&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23427;&#22312;&#22810;&#21464;&#37327;&#35774;&#32622;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;'HandMovementDirection'&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#25253;&#21578;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13679</link><description>&lt;p&gt;
RED CoMETS: &#19968;&#31181;&#29992;&#20110;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
RED CoMETS: An ensemble classifier for symbolically represented multivariate time series. (arXiv:2307.13679v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RED CoMETS&#30340;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#31526;&#21495;&#21270;&#34920;&#31034;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;&#23427;&#22312;&#22810;&#21464;&#37327;&#35774;&#32622;&#20013;&#23637;&#29616;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;'HandMovementDirection'&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#25253;&#21578;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#22312;&#37329;&#34701;&#12289;&#21307;&#30103;&#12289;&#24037;&#31243;&#31561;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20998;&#31867;&#22797;&#26434;&#24615;&#26469;&#33258;&#20110;&#20854;&#39640;&#32500;&#24230;&#12289;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#38271;&#24230;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RED CoMETS&#65288;Random Enhanced Co-eye for Multivariate Time Series&#65289;&#30340;&#26032;&#22411;&#38598;&#25104;&#20998;&#31867;&#22120;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;RED CoMETS&#22522;&#20110;Co-eye&#30340;&#25104;&#21151;&#65292;&#24182;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#22788;&#29702;&#22810;&#21464;&#37327;&#25968;&#25454;&#12290;&#20351;&#29992;UCR&#26723;&#26696;&#20013;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;RED CoMETS&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#22810;&#21464;&#37327;&#35774;&#32622;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;&#23427;&#26174;&#31034;&#20986;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#22312;&#25991;&#29486;&#20013;&#23545;&#20110;'HandMovementDirection'&#25968;&#25454;&#38598;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#25253;&#21578;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#22320;&#25913;&#36827;&#20102;&#20256;&#32479;&#30340;Co-eye&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method signific
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28388;&#27874;&#21644;&#27169;&#24335;&#35782;&#21035;&#24378;&#21270;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#34987;&#29992;&#20110;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#32500;&#24230;&#20351;&#24471;&#20854;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;POD&#31561;&#38477;&#32500;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODEs)&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#19982;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.17181</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#26080;&#30417;&#30563;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29992;&#20110;&#25991;&#26412;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#29983;&#25104;&#36830;&#32493;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#30340;&#26041;&#27861;&#65288;TESGAN&#65289;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;GAN&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#21462;&#20195;&#31163;&#25955;&#30340;&#26631;&#35760;&#65292;&#20351;&#24471;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#21512;&#25104;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#30340;&#31454;&#20105;&#26469;&#21019;&#24314;&#36924;&#30495;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;GAN&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#30340;&#38480;&#21046;&#12290;&#22240;&#20026;&#33258;&#28982;&#35821;&#35328;&#30001;&#31163;&#25955;&#30340;&#26631;&#35760;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#26356;&#26032;&#26799;&#24230;&#26102;&#36935;&#21040;&#22256;&#38590;&#65307;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;-GAN&#30740;&#31350;&#20351;&#29992;&#22870;&#21169;&#31995;&#32479;&#20197;&#38543;&#26426;&#26631;&#35760;&#20026;&#22522;&#30784;&#29983;&#25104;&#21477;&#23376;&#12290;&#22240;&#27492;&#65292;&#20808;&#21069;&#30740;&#31350;&#20013;&#30340;&#29983;&#25104;&#22120;&#22312;&#23545;&#25239;&#35757;&#32451;&#20043;&#21069;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23548;&#33268;&#21512;&#25104;&#30340;&#21477;&#23376;&#37325;&#22797;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31867;&#20284;&#21407;&#22987;GAN&#30340;&#26694;&#26550;&#26469;&#21512;&#25104;&#21477;&#23376;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;TESGAN&#65289;&#65292;&#23427;&#29983;&#25104;&#36830;&#32493;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#26469;&#35299;&#20915;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;</title><link>http://arxiv.org/abs/2306.10359</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#39537;&#21160;Foley&#38899;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#23545;&#31995;&#32479;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#25913;&#36827;&#20102;&#29983;&#25104;&#30340;&#27874;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Foley&#38899;&#25928;&#29983;&#25104;&#26088;&#22312;&#20026;&#22810;&#23186;&#20307;&#20869;&#23481;&#29983;&#25104;&#32972;&#26223;&#38899;&#25928;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#22823;&#37327;&#26377;&#26631;&#31614;&#30340;&#24320;&#21457;&#38598;&#20316;&#20026;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;&#25968;&#23383;&#25110;one-hot&#21521;&#37327;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;Foley&#38899;&#25928;&#29983;&#25104;&#31995;&#32479;&#65292;&#21487;&#36827;&#34892;&#25991;&#26412;&#26465;&#20214;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#39318;&#20808;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#27604;&#35821;&#35328;-&#38899;&#39057;&#37197;&#23545;&#65288;CLAP&#65289;&#25216;&#26415;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#26469;&#23545;&#35813;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#23884;&#20837;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#20043;&#21518;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#23618;&#26469;&#25913;&#21892;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#20505;&#36873;&#38899;&#39057;&#29255;&#27573;&#24182;&#36873;&#25321;&#26368;&#20339;&#29255;&#27573;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#29983;&#25104;&#30340;&#27874;&#24418;&#65292;&#26368;&#20339;&#29255;&#27573;&#26159;&#26681;&#25454;&#23884;&#20837;&#20043;&#38388;&#30456;&#20284;&#24615;&#24471;&#20998;&#30830;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;FairCFS&#65292;&#36890;&#36807;&#26500;&#24314;&#23616;&#37096;&#22240;&#26524;&#22270;&#35782;&#21035;&#31867;&#21644;&#25935;&#24863;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#65292;&#38459;&#27490;&#25935;&#24863;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#32463;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#65292;FairCFS&#22312;&#29305;&#24449;&#36873;&#25321;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#20843;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10336</link><description>&lt;p&gt;
&#20844;&#24179;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Fair Causal Feature Selection. (arXiv:2306.10336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10336
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;FairCFS&#65292;&#36890;&#36807;&#26500;&#24314;&#23616;&#37096;&#22240;&#26524;&#22270;&#35782;&#21035;&#31867;&#21644;&#25935;&#24863;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#65292;&#38459;&#27490;&#25935;&#24863;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#20174;&#32780;&#23454;&#29616;&#20844;&#24179;&#30340;&#29305;&#24449;&#36873;&#25321;&#12290;&#32463;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#65292;FairCFS&#22312;&#29305;&#24449;&#36873;&#25321;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;&#20843;&#31181;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#20998;&#31867;&#20915;&#31574;&#20219;&#21153;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#24341;&#36215;&#20102;&#30740;&#31350;&#32773;&#30340;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#22312;&#25552;&#20379;&#29305;&#24449;&#21644;&#25935;&#24863;&#23646;&#24615;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#20840;&#38754;&#35299;&#37322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#21487;&#33021;&#24433;&#21709;&#20844;&#24179;&#29305;&#24449;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#22240;&#26524;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#31216;&#20026;FairCFS&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FairCFS&#26500;&#24314;&#20102;&#19968;&#20010;&#23616;&#37096;&#22240;&#26524;&#22270;&#65292;&#35782;&#21035;&#20102;&#31867;&#21644;&#25935;&#24863;&#21464;&#37327;&#30340;&#39532;&#23572;&#21487;&#22827;&#27631;&#20197;&#38459;&#27490;&#25935;&#24863;&#20449;&#24687;&#30340;&#20256;&#36755;&#65292;&#20197;&#36873;&#25321;&#20844;&#24179;&#22240;&#26524;&#29305;&#24449;&#12290;&#23545;&#19971;&#20010;&#20844;&#20849;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#65292;FairCFS&#22312;&#19982;&#20843;&#31181;&#26368;&#20808;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21576;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair feature selection for classification decision tasks has recently garnered significant attention from researchers. However, existing fair feature selection algorithms fall short of providing a full explanation of the causal relationship between features and sensitive attributes, potentially impacting the accuracy of fair feature identification. To address this issue, we propose a Fair Causal Feature Selection algorithm, called FairCFS. Specifically, FairCFS constructs a localized causal graph that identifies the Markov blankets of class and sensitive variables, to block the transmission of sensitive information for selecting fair causal features. Extensive experiments on seven public real-world datasets validate that FairCFS has comparable accuracy compared to eight state-of-the-art feature selection algorithms, while presenting more superior fairness.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09273</link><description>&lt;p&gt;
&#20320;&#30340;&#25151;&#38388;&#19981;&#26159;&#31169;&#23494;&#30340;&#65306;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning. (arXiv:2306.09273v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#21457;&#23637;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#35813;&#25216;&#26415;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20114;&#21160;&#12290;&#30001;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#38544;&#31169;&#38382;&#39064;&#22312;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#20010;&#20154;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25915;&#20987;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36873;&#25321;&#20351;&#29992;&#26799;&#24230;&#36827;&#34892;&#25915;&#20987;&#26159;&#22240;&#20026;&#24120;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#20165;&#21033;&#29992;&#22522;&#20110;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#32780;&#19981;&#23384;&#20648;&#25110;&#20256;&#36755;&#29992;&#25143;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or trans
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#26469;&#25552;&#39640;&#23548;&#33322;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.06766</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#36890;&#36807;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning. (arXiv:2306.06766v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#30340;&#38646;&#26679;&#26412;&#26080;&#32447;&#23460;&#20869;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26679;&#26412;&#39640;&#25928;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#26469;&#25552;&#39640;&#23548;&#33322;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#23460;&#20869;&#26426;&#22120;&#20154;&#23548;&#33322;&#21033;&#29992;&#26080;&#32447;&#20449;&#21495;&#30340;&#19981;&#26029;&#20851;&#27880;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#24378;&#21270;&#23398;&#20064;&#65288;PIRL&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#30456;&#23545;&#20110;&#22522;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#20256;&#32479;&#26041;&#27861;&#65292;&#22522;&#20110;&#23556;&#39057;&#20256;&#25773;&#30340;&#26041;&#27861;&#30452;&#35266;&#19988;&#33021;&#22815;&#36866;&#24212;&#31616;&#21333;&#30340;&#22330;&#26223;&#65292;&#20294;&#22312;&#22797;&#26434;&#29615;&#22659;&#19979;&#38590;&#20197;&#23548;&#33322;&#12290;&#32780;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35745;&#31639;&#26426;&#25216;&#26415;&#26469;&#25506;&#32034;&#25972;&#20010;&#29366;&#24577;&#31354;&#38388;&#65292;&#22312;&#38754;&#23545;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#26102;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#32780;&#19988;&#24471;&#21040;&#30340;&#31574;&#30053;&#22312;&#26410;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#26410;&#35265;&#36807;&#30340;&#26032;&#22330;&#26223;&#20013;&#26377;&#25928;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL), powered by advanced computing machinery, can explore the entire state space, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and {zero-shot} generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.14704</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#30340;&#22312;&#32447;&#33258;&#36866;&#24212;&#27969;&#37327;&#23454;&#39564;&#30340;&#23454;&#29992;&#25209;&#27425;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#65292;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#65292;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21152;&#36895;&#22312;&#32447;&#27979;&#35797;&#65292;&#22810;&#33218;&#36172;&#21338;&#31639;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#22320;&#25910;&#38598;&#25968;&#25454;&#32780;&#34987;&#20316;&#20026;&#22266;&#23450;&#26102;&#38388;A/B&#27979;&#35797;&#30340;&#37325;&#35201;&#34917;&#20805;&#26041;&#24335;&#19981;&#26029;&#25552;&#39640;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#33258;&#36866;&#24212;&#25910;&#38598;&#25968;&#25454;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#32479;&#35745;&#25512;&#26029;&#30340;&#30740;&#31350;&#65292;&#25512;&#23548;&#21644;&#35780;&#20272;&#20102;&#22235;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#25209;&#27425;&#36172;&#21338;&#31639;&#27861;&#65288;NB-TS&#65292;WB-TS&#65292;NB-TTTS&#65292;WB-TTTS&#65289;&#65292;&#23427;&#20204;&#26159;&#20004;&#31181;&#21152;&#26435;&#25209;&#27425;&#65288;Naive Batch&#21644;Weighted Batch&#65289;&#21644;&#20004;&#31181;&#36125;&#21494;&#26031;&#25277;&#26679;&#31574;&#30053;&#65288;Thompson Sampling&#21644;Top-Two Thompson Sampling&#65289;&#30340;&#32452;&#21512;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#30830;&#23450;&#27969;&#37327;&#20998;&#37197;&#12290;&#26412;&#25991;&#25552;&#20379;&#30340;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#32479;&#35745;&#22870;&#21169;&#24230;&#37327;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#22312;&#23454;&#36341;&#20013;&#24471;&#20197;&#24212;&#29992;&#65292;&#32780;&#26412;&#25991;&#25552;&#20986;&#30340;&#20854;&#20013;&#19968;&#20010;&#32452;&#21512;WB-TTTS&#20284;&#20046;&#26159;&#26368;&#26032;&#35752;&#35770;&#30340;&#12290;&#23545;&#36825;&#22235;&#31181;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#31639;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#27979;&#35797;&#26041;&#27861;&#30340;&#21487;&#20449;&#24230;&#12289;&#25935;&#24863;&#24615;&#21644;&#21518;&#24724;&#24230;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#32771;&#34385;&#20102;&#25209;&#27425;&#20869;&#22870;&#21169;&#24230;&#37327;&#30340;&#26041;&#24046;&#20197;&#21450;&#25209;&#27425;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#36172;&#21338;&#31639;&#27861;&#65288;&#20363;&#22914;UCB1&#65292;TS&#21644;Exp3&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#22522;&#20110;&#25209;&#27425;&#30340;&#36125;&#21494;&#26031;&#31639;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13617</link><description>&lt;p&gt;
SPEECH: &#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13617
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SPEECH&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#33021;&#37327;&#24314;&#27169;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20013;&#24515;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#28041;&#21450;&#39044;&#27979;&#20107;&#20214;&#30340;&#32467;&#26500;&#21270;&#36755;&#20986;&#12290;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24773;&#20917;&#19979;&#65292;&#20107;&#20214;&#32467;&#26500;&#37117;&#20855;&#26377;&#22797;&#26434;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#34920;&#31034;&#36825;&#20123;&#22797;&#26434;&#30340;&#20107;&#20214;&#32467;&#26500;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#20107;&#20214;&#20013;&#24515;&#36229;&#29699;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979; (SPEECH)&#12290; SPEECH &#20351;&#29992;&#22522;&#20110;&#33021;&#37327;&#30340;&#24314;&#27169;&#26469;&#27169;&#25311;&#20107;&#20214;&#32467;&#26500;&#32452;&#20214;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36229;&#29699;&#26469;&#34920;&#31034;&#20107;&#20214;&#31867;&#21035;&#12290;&#22312;&#20004;&#20010;&#32479;&#19968;&#26631;&#27880;&#30340;&#20107;&#20214;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;SPEECH&#22312;&#20107;&#20214;&#26816;&#27979;&#21644;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#20013;&#21344;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;X-Ray Microbeam&#25968;&#25454;&#38598;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;&#22810;&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65292;&#36827;&#32780;&#25913;&#36827;&#20102;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10775</link><description>&lt;p&gt;
&#37319;&#29992;X&#23556;&#32447;&#24494;&#26463;&#25968;&#25454;&#20960;&#20309;&#21464;&#25442;&#22686;&#24378;&#35821;&#38899;&#21457;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset. (arXiv:2305.10775v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;X-Ray Microbeam&#25968;&#25454;&#38598;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;&#22810;&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65292;&#36827;&#32780;&#25913;&#36827;&#20102;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20998;&#26512;&#35821;&#38899;&#21457;&#38899;&#23545;&#20110;&#35821;&#38899;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22768;&#38376;&#30340;X-Y&#22352;&#26631;&#20005;&#37325;&#20381;&#36182;&#20110;&#21457;&#35328;&#32773;&#30340;&#35299;&#21078;&#32467;&#26500;&#21644;&#39063;&#31890;&#20301;&#32622;&#30340;&#21487;&#21464;&#24615;&#65292;&#29616;&#26377;&#30340;X&#23556;&#32447;&#24494;&#26463;&#25968;&#25454;&#38598;&#65288;XRMB&#65289;&#20013;&#30340;&#35299;&#21078;&#26631;&#24535;&#29289;&#26144;&#23556;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#21457;&#38899;&#36947;&#30340;&#25972;&#20010;&#35299;&#21078;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#65292;&#25913;&#36827;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21464;&#25442;&#23558;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;6&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65306;&#21767;&#32541;&#24352;&#24230;&#65288;LA&#65289;&#12289;&#21767;&#37096;&#31361;&#20986;&#65288;LP&#65289;&#12289;&#33292;&#20307;&#25910;&#32553;&#20301;&#32622;&#65288;TTCL&#65289;&#12289;&#24230;&#25968;&#65288;TBCD&#65289;&#12289;&#33292;&#23574;&#25910;&#32553;&#20301;&#32622;&#65288;TTCL&#65289;&#21644;&#24230;&#25968;&#65288;TTCD&#65289;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#36129;&#29486;&#26159;&#23558;&#33133;&#26495;&#36861;&#36394;&#24310;&#20280;&#21040;&#25512;&#27979;&#30340;&#21693;&#21897;&#21069;&#32447;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#33292;&#20307;&#25910;&#32553;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and the variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue Body Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL) and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;Dir-GNN&#65292;&#24182;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10498</link><description>&lt;p&gt;
&#36793;&#26041;&#21521;&#24615;&#25552;&#39640;&#20102;&#24322;&#36136;&#22270;&#19978;&#30340;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Edge Directionality Improves Learning on Heterophilic Graphs. (arXiv:2305.10498v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;Dir-GNN&#65292;&#24182;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#25104;&#20026;&#24314;&#27169;&#20851;&#31995;&#25968;&#25454;&#30340;&#20107;&#23454;&#26631;&#20934;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#26159;&#26377;&#21521;&#30340;&#65292;&#20294;&#20170;&#22825;&#22823;&#22810;&#25968;GNN&#27169;&#22411;&#37117;&#36890;&#36807;&#20351;&#22270;&#25104;&#20026;&#26080;&#21521;&#22270;&#26469;&#23436;&#20840;&#24573;&#30053;&#36825;&#20123;&#20449;&#24687;&#12290;&#36825;&#26679;&#20570;&#30340;&#21407;&#22240;&#26159;&#21382;&#21490;&#24615;&#30340;&#65306;1&#65289;&#35768;&#22810;&#26089;&#26399;&#30340;&#35889;GNN&#21464;&#20307;&#26126;&#30830;&#35201;&#27714;&#22270;&#26159;&#26080;&#21521;&#30340;&#65292;2&#65289;&#20851;&#20110;&#21516;&#31867;&#22270;&#30340;&#31532;&#19968;&#25209;&#22522;&#20934;&#27979;&#35797;&#24182;&#26410;&#21457;&#29616;&#20351;&#29992;&#26041;&#21521;&#24615;&#26377;&#26126;&#26174;&#30340;&#22686;&#30410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24322;&#31867;&#35774;&#32622;&#20013;&#65292;&#23558;&#22270;&#24418;&#35270;&#20026;&#26377;&#21521;&#22270;&#21487;&#20197;&#22686;&#21152;&#22270;&#30340;&#20869;&#22312;&#21516;&#36136;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#20174;&#27491;&#30830;&#20351;&#29992;&#26041;&#21521;&#24615;&#20449;&#24687;&#20013;&#21487;&#33021;&#24471;&#21040;&#30340;&#22909;&#22788;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Directed Graph Neural Network&#65288;Dir-GNN&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#38754;&#21521;&#26377;&#21521;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#36890;&#29992;&#26694;&#26550;&#12290;Dir-GNN&#21487;&#20197;&#29992;&#20110;&#25193;&#23637;&#20219;&#20309;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#20197;&#36890;&#36807;&#23545;&#27599;&#20010;&#33410;&#28857;&#25191;&#34892;&#21333;&#29420;&#30340;&#36827;&#20986;&#28040;&#24687;&#32858;&#21512;&#26469;&#32771;&#34385;&#36793;&#26041;&#21521;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#26377;&#21521;&#24341;&#29992;&#22270;&#19978;&#35780;&#20272;&#20102;Dir-GNN&#65292;&#24182;&#35777;&#26126;&#23427;&#22312;&#39044;&#27979;&#32570;&#22833;&#30340;&#24341;&#29992;&#38142;&#25509;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26080;&#21521;GNN&#21644;&#19968;&#20123;&#26377;&#21521;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26041;&#21521;&#24615;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;Dir-GNN&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outg
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2305.09820</link><description>&lt;p&gt;
&#26426;&#22120;&#21046;&#36896;&#30340;&#23186;&#20307;&#65306;&#30417;&#27979;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#19978;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#30340;&#21160;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09820
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#31456;&#22312;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#26222;&#21450;&#31243;&#24230;&#65292;&#21457;&#29616;&#34394;&#20551;&#26032;&#38395;&#32593;&#31449;&#19978;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26085;&#30410;&#27969;&#34892;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#38395;&#32593;&#31449;&#24320;&#22987;&#21033;&#29992;&#23427;&#20204;&#29983;&#25104;&#25991;&#31456;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#33021;&#22312;&#22768;&#35465;&#33391;&#22909;&#30340;&#32593;&#31449;&#19978;&#20135;&#29983;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#25991;&#31456;&#65292;&#32780;&#19988;&#19981;&#33391;&#26032;&#38395;&#32593;&#31449;&#20063;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#25209;&#37327;&#29983;&#20135;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#24320;&#22987;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39318;&#20010;&#22823;&#35268;&#27169;&#30740;&#31350;&#21512;&#25104;&#25991;&#31456;&#22312;&#32447;&#26032;&#38395;&#23186;&#20307;&#20013;&#26222;&#21450;&#29575;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;DeBERTa&#30340;&#21512;&#25104;&#26032;&#38395;&#26816;&#27979;&#22120;&#65292;&#24182;&#23545;3074&#20010;&#34394;&#20551;&#26032;&#38395;&#21644;&#20027;&#27969;&#26032;&#38395;&#32593;&#31449;&#30340;&#36229;&#36807;1291&#19975;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;2022&#24180;1&#26376;1&#26085;&#33267;2023&#24180;4&#26376;1&#26085;&#26399;&#38388;&#65292;&#21512;&#25104;&#26032;&#38395;&#25991;&#31456;&#30340;&#30456;&#23545;&#25968;&#37327;&#22312;&#20027;&#27969;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;79.4&#65285;&#65292;&#32780;&#22312;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#22686;&#21152;&#20102;342&#65285;&#12290;&#20998;&#26512;ChatGPT&#21457;&#24067;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20013;&#26029;&#26102;&#38388;&#24207;&#21015;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#23427;&#30340;&#21457;&#24067;&#23548;&#33268;&#21512;&#25104;&#25991;&#31456;&#30340;&#20351;&#29992;&#26174;&#33879;&#22686;&#21152;&#65292;&#20294;&#34394;&#20551;&#20449;&#24687;&#32593;&#31449;&#19978;&#30340;&#21512;&#25104;&#25991;&#31456;&#20351;&#29992;&#36895;&#24230;&#27604;&#20027;&#27969;&#32593;&#31449;&#19978;&#30340;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#26377;&#38480;&#34920;&#36798;&#27861;" (FEX) &#30340;&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#25968;&#25454;&#20013;PDE&#35299;&#30340;&#23548;&#25968;&#65292;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;FEX&#22312;&#22810;&#31181;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#21253;&#25324;&#26102;&#21464;&#30340;PDE&#38382;&#39064;&#21644;&#20855;&#26377;&#26102;&#21464;&#31995;&#25968;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.08342</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#29289;&#29702;&#23450;&#24459;&#30340;&#26377;&#38480;&#34920;&#36798;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finite Expression Methods for Discovering Physical Laws from Data. (arXiv:2305.08342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"&#26377;&#38480;&#34920;&#36798;&#27861;" (FEX) &#30340;&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#25968;&#25454;&#20013;PDE&#35299;&#30340;&#23548;&#25968;&#65292;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65292;FEX&#22312;&#22810;&#31181;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#21253;&#25324;&#26102;&#21464;&#30340;PDE&#38382;&#39064;&#21644;&#20855;&#26377;&#26102;&#21464;&#31995;&#25968;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#26159;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#25551;&#36848;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#23558;&#20171;&#32461;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#31526;&#21495;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;"&#26377;&#38480;&#34920;&#36798;&#27861;" (FEX)&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#25968;&#25454;&#20013;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#30340;&#23548;&#25968;&#65292;&#21033;&#29992;FEX&#22312;&#21253;&#21547;&#26377;&#38480;&#38598;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#21457;&#29616;&#25511;&#21046;&#26041;&#31243;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;PDE-Net, SINDy, GP &#21644; SPL&#65289;&#65292;&#25105;&#20204;&#30340;FEX&#22312;&#22810;&#31181;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#21253;&#25324;&#26102;&#21464;&#30340;PDE&#38382;&#39064;&#21644;&#20855;&#26377;&#26102;&#21464;&#31995;&#25968;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#32467;&#26524;&#31361;&#26174;&#20102;FEX&#30340;&#28789;&#27963;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinear dynamics is a pervasive phenomenon observed in scientific and engineering disciplines. However, the task of deriving analytical expressions to describe nonlinear dynamics from limited data remains challenging. In this paper, we shall present a novel deep symbolic learning method called the "finite expression method" (FEX) to discover governing equations within a function space containing a finite set of analytic expressions, based on observed dynamic data. The key concept is to employ FEX to generate analytical expressions of the governing equations by learning the derivatives of partial differential equation (PDE) solutions through convolutions. Our numerical results demonstrate that our FEX surpasses other existing methods (such as PDE-Net, SINDy, GP, and SPL) in terms of numerical performance across a range of problems, including time-dependent PDE problems and nonlinear dynamical systems with time-varying coefficients. Moreover, the results highlight FEX's flexibility and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.14104</link><description>&lt;p&gt;
&#20174;&#24369;&#25991;&#26412;&#30417;&#30563;&#20013;&#23398;&#20064;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#33258;&#30001;&#25991;&#26412;&#30340;&#24418;&#24335;&#26469;&#28789;&#27963;&#24314;&#27169;&#20154;&#38469;&#20114;&#21160;&#12290;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#29992;&#20110;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#39044;&#27979;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#65292;&#24182;&#22312;&#27492;&#20219;&#21153;&#19978;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20114;&#21160;&#26159;&#22810;&#26679;&#19988;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23558;&#23427;&#20204;&#35270;&#20026;&#20998;&#31867;&#65292;&#24573;&#30053;&#20102;&#21487;&#33021;&#30340;&#20114;&#21160;&#30340;&#37325;&#23614;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#20154;&#38469;&#20114;&#21160;&#30340;&#33539;&#24335;&#65292;&#23558;&#20854;&#20316;&#20026;&#33258;&#30001;&#25991;&#26412;&#20174;&#21333;&#19968;&#30340;&#38745;&#24577;&#22270;&#20687;&#20013;&#23398;&#20064;&#65292;&#20174;&#32780;&#20801;&#35768;&#23545;&#24773;&#20917;&#21644;&#20154;&#38469;&#20851;&#31995;&#30340;&#26080;&#38480;&#31354;&#38388;&#36827;&#34892;&#28789;&#27963;&#24314;&#27169;&#12290;&#20026;&#20102;&#20811;&#26381;&#32570;&#20047;&#29305;&#23450;&#20110;&#27492;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#24212;&#29992;&#20110;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#21512;&#25104;&#23383;&#24149;&#25968;&#25454;&#65292;&#20197;&#27492;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#30340;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#19968;&#31181;&#23383;&#24149;&#27169;&#22411;&#65292;&#33021;&#26377;&#25928;&#29702;&#35299;&#22270;&#20687;&#20013;&#30340;&#20154;&#38469;&#20114;&#21160;&#65292;&#36890;&#36807;&#34913;&#37327;&#25105;&#20204;&#39044;&#27979;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#36136;&#37327;&#19982;&#20107;&#23454;&#30340;&#22522;&#30784;&#24615;&#30340;&#21508;&#31181;&#25351;&#26631;&#26469;&#34913;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;SOTA&#30340;&#22270;&#20687;&#23383;&#24149;&#21644;&#24773;&#22659;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#20844;&#24320;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14094</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#33539;&#30068;&#22522;&#30784;&#65306;&#19968;&#31181;&#32479;&#19968;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#24418;&#24335;&#20307;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#20307;&#31995;&#65292;&#20026;&#39046;&#22495;&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#25552;&#20379;&#20102;&#28165;&#26224;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#36981;&#24490;&#25152;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#22238;&#31572;&#19982;AI&#27169;&#22411;&#37096;&#32626;&#30456;&#20851;&#30340;&#20262;&#29702;&#21644;&#27861;&#24459;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#39046;&#22495;&#29305;&#23450;&#35780;&#35770;&#24378;&#35843;&#38656;&#35201;&#19968;&#20010;&#25968;&#23398;&#22522;&#30784;&#26469;&#23450;&#20041;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#27010;&#24565;&#65292;&#21363;&#20351;&#8220;&#35299;&#37322;&#8221;&#36825;&#20010;&#26415;&#35821;&#36824;&#32570;&#20047;&#31934;&#30830;&#23450;&#20041;&#12290;&#36825;&#20123;&#35780;&#35770;&#36824;&#20027;&#24352;&#24314;&#31435;&#19968;&#20010;&#20581;&#20840;&#32780;&#32479;&#19968;&#30340;&#21487;&#35299;&#37322;AI&#24418;&#24335;&#20307;&#31995;&#65292;&#20197;&#36991;&#20813;&#20986;&#29616;&#19981;&#33391;&#25552;&#20986;&#38382;&#39064;&#65292;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#27983;&#35272;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#30693;&#35782;&#20307;&#31995;&#12290;&#25454;&#20316;&#32773;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#26159;&#22635;&#34917;&#35813;&#31354;&#30333;&#30340;&#39318;&#27425;&#23581;&#35797;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;&#21487;&#35299;&#37322;AI&#30340;&#32479;&#19968;&#29702;&#35770;&#12290;&#37319;&#29992;&#33539;&#30068;&#29702;&#35770;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#21453;&#39304;&#21333;&#35843;&#33539;&#30068;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;AI&#20013;&#25152;&#26377;&#37325;&#35201;&#26415;&#35821;&#30340;&#24418;&#24335;&#23450;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36981;&#24490;&#25552;&#20986;&#32467;&#26500;&#30340;&#39046;&#22495;&#20998;&#31867;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#24341;&#20837;&#30340;&#29702;&#35770;&#26469;&#23545;&#24403;&#21069;&#30740;&#31350;&#30340;&#25152;&#26377;&#20027;&#35201;XAI&#31995;&#32479;&#31867;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12751</link><description>&lt;p&gt;
&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#25913;&#36827;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Grad-Align+&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;NA&#26041;&#27861;&#32570;&#20047;&#39069;&#22806;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23545;&#40784;&#65288;NA&#65289;&#26159;&#36890;&#36807;&#32473;&#23450;&#32593;&#32476;&#30340;&#25299;&#25169;&#21644;/&#25110;&#29305;&#24449;&#20449;&#24687;&#26469;&#21457;&#29616;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;NA&#26041;&#27861;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#24182;&#19981;&#24635;&#26159;&#26377;&#39069;&#22806;&#20449;&#24687;&#65292;&#22914;&#20808;&#21069;&#30340;&#38170;&#28857;&#38142;&#25509;&#21644;/&#25110;&#33410;&#28857;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23454;&#38469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Grad-Align+&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;NA&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#26368;&#36817;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;NA&#26041;&#27861;Grad-Align&#20043;&#19978;&#65292;Grad-Align+&#20165;&#36880;&#27493;&#21457;&#29616;&#37096;&#20998;&#33410;&#28857;&#23545;&#65292;&#30452;&#21040;&#25214;&#21040;&#25152;&#26377;&#33410;&#28857;&#23545;&#12290;&#22312;&#35774;&#35745;Grad-Align+&#26102;&#65292;&#25105;&#20204;&#32771;&#34385;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#33410;&#28857;&#29305;&#24449;&#26469;&#25191;&#34892;NA&#20219;&#21153;&#65292;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22686;&#24378;&#30340;&#33410;&#28857;&#29305;&#24449;&#26469;&#35774;&#35745;NA&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;Grad-Align+&#65306;&#22522;&#20110;&#20013;&#24515;&#24615;&#30340;&#33410;&#28857;&#29305;&#24449;&#22686;&#24378;&#65288;CNFA&#65289;&#12289;&#22270;&#20999;&#29255;&#29983;&#25104;&#21644;&#20248;&#21270;&#33410;&#28857;&#23884;&#20837;&#29305;&#24449;&#65288;ONIFE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Transformer&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#32431;&#24230;&#20316;&#20026;&#25351;&#26631;&#65292;&#37319;&#29992;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32500;&#25345;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;Cross-attention&#26426;&#21046;&#25552;&#39640;&#32858;&#31867;&#30340;&#32431;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.07408</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#32858;&#31867;&#31639;&#27861;:&#19968;&#31181;&#26032;&#30340;Transformer&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fairness in Visual Clustering: A Novel Transformer Clustering Approach. (arXiv:2304.07408v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07408
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Transformer&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32858;&#31867;&#32431;&#24230;&#20316;&#20026;&#25351;&#26631;&#65292;&#37319;&#29992;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32500;&#25345;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#24341;&#20837;Cross-attention&#26426;&#21046;&#25552;&#39640;&#32858;&#31867;&#30340;&#32431;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#32858;&#31867;&#30340;&#24773;&#26223;&#19979;&#65292;&#20026;&#20102;&#20943;&#23569;&#20154;&#32676;&#20559;&#24046;&#32780;&#22686;&#21152;&#28145;&#24230;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#20174;&#32858;&#31867;&#32431;&#24230;&#30340;&#35282;&#24230;&#35780;&#20272;&#20102;&#28145;&#24230;&#32858;&#31867;&#27169;&#22411;&#20013;&#30340;&#20154;&#21475;&#20559;&#24046;&#65292;&#32858;&#31867;&#32431;&#24230;&#26159;&#25351;&#32858;&#31867;&#20013;&#27491;&#26679;&#26412;&#19982;&#23427;&#20204;&#30340;&#30456;&#20851;&#31243;&#24230;&#30340;&#27604;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#40723;&#21169;&#25152;&#26377;&#32858;&#31867;&#30340;&#32431;&#24230;&#19968;&#33268;&#24615;&#20197;&#32500;&#25345;&#23398;&#20064;&#21040;&#30340;&#32858;&#31867;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;, &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Cross-attention&#26426;&#21046;&#65292;&#29992;&#20110;&#27979;&#37327;&#22810;&#20010;&#32858;&#31867;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21152;&#24378;&#36828;&#36317;&#31163;&#30340;&#27491;&#26679;&#26412;&#65292;&#25552;&#39640;&#32858;&#31867;&#30340;&#32431;&#24230;&#12290;&#22312;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#22810;&#31181;&#23646;&#24615;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07163</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;Bandit&#26041;&#27861;&#30340;&#26174;&#24335;&#22609;&#24418;&#22806;&#37096;&#24314;&#35758;&#31639;&#27861;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22522;&#20110;Bandit&#26041;&#27861;&#23558;&#22806;&#37096;&#24314;&#35758;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22806;&#37096;&#25110;&#19987;&#23478;&#30340;&#24314;&#35758;&#34701;&#20837;&#21040;&#23398;&#20064;&#24403;&#20013;&#12290;&#26412;&#25991;&#23558;&#23558;&#23558;&#27492;&#38382;&#39064;&#34920;&#36848;&#20026;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#31216;&#20026;&#22609;&#24418;&#36172;&#21338;&#26426;&#65288;shaping-bandits&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#22609;&#24418;&#31639;&#27861;&#65306;UCB-PIES&#65288;UPIES&#65289;&#65292; Racing-PIES&#65288;RPIES&#65289;&#21644;Lazy PIES&#65288;LPIES&#65289;&#12290;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;LQR&#21644;Atari&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#19977;&#31181;&#31639;&#27861;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#12289;&#23398;&#20064;&#36895;&#24230;&#21644;&#24418;&#29366;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#27169;&#22359;&#21270;&#30340; R6 &#25509;&#21475;&#65292;&#29992;&#20110;&#20855;&#20307;&#23454;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#29616;&#19977;&#31181;&#26041;&#27861;&#24182;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#32467;&#21512;&#30495;&#23454;&#29992;&#20363;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#24471;&#20986;&#26377;&#20851;&#22914;&#20309;&#26356;&#25913;&#21333;&#20010;&#35266;&#27979;&#20540;&#30340;&#29305;&#24449;&#20540;&#20197;&#33719;&#24471;&#25152;&#38656;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2304.06569</link><description>&lt;p&gt;
counterfactuals: &#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#30340; R &#21253;
&lt;/p&gt;
&lt;p&gt;
counterfactuals: An R Package for Counterfactual Explanation Methods. (arXiv:2304.06569v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06569
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#19988;&#27169;&#22359;&#21270;&#30340; R6 &#25509;&#21475;&#65292;&#29992;&#20110;&#20855;&#20307;&#23454;&#29616;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#29616;&#19977;&#31181;&#26041;&#27861;&#24182;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#24773;&#22659;&#20013;&#65292;&#32467;&#21512;&#30495;&#23454;&#29992;&#20363;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#20934;&#30830;&#22320;&#24471;&#20986;&#26377;&#20851;&#22914;&#20309;&#26356;&#25913;&#21333;&#20010;&#35266;&#27979;&#20540;&#30340;&#29305;&#24449;&#20540;&#20197;&#33719;&#24471;&#25152;&#38656;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#26356;&#25913;&#21333;&#20010;&#35266;&#27979;&#20540;&#30340;&#29305;&#24449;&#20540;&#20197;&#33719;&#24471;&#25152;&#38656;&#39044;&#27979;&#30340;&#20449;&#24687;&#12290;&#23613;&#31649;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#20855;&#26377;&#24191;&#27867;&#21464;&#21270;&#30340;&#25509;&#21475;&#21644;&#35201;&#27714;&#30340;&#23454;&#29616;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461; counterfactuals R &#21253;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110; R6 &#30340;&#27169;&#22359;&#21270;&#21644;&#32479;&#19968;&#30340;&#25509;&#21475;&#65292;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#29616;&#20102;&#19977;&#31181;&#29616;&#26377;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#36873;&#30340;&#26041;&#27861;&#23398;&#25193;&#23637;&#65292;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#22330;&#26223;&#24182;&#20351;&#20854;&#26356;&#20855;&#21487;&#27604;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#29992;&#20363;&#35299;&#37322;&#20102;&#21253;&#30340;&#32467;&#26500;&#21644;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20854;&#20182;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#38598;&#25104;&#21040;&#21253;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#21508;&#31181;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#27604;&#36739;&#20102;&#23454;&#26045;&#30340;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#36136;&#37327;&#21644;&#36816;&#34892;&#26102;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanation methods provide information on how feature values of individual observations must be changed to obtain a desired prediction. Despite the increasing amount of proposed methods in research, only a few implementations exist whose interfaces and requirements vary widely. In this work, we introduce the counterfactuals R package, which provides a modular and unified R6-based interface for counterfactual explanation methods. We implemented three existing counterfactual explanation methods and propose some optional methodological extensions to generalize these methods to different scenarios and to make them more comparable. We explain the structure and workflow of the package using real use cases and show how to integrate additional counterfactual explanation methods into the package. In addition, we compared the implemented methods for a variety of models and datasets with regard to the quality of their counterfactual explanations and their runtime behavior.
&lt;/p&gt;</description></item><item><title>&#32654;&#22269;&#20844;&#31435;&#23398;&#26657;&#24341;&#20837;&#30340;&#39044;&#27979;&#31639;&#27861;&#65288;EWS&#65289;&#26410;&#33021;&#25552;&#39640;&#27605;&#19994;&#29575;&#65292;EWS&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#29615;&#22659;&#22240;&#32032;&#24433;&#21709;&#26356;&#22823;&#12290;</title><link>http://arxiv.org/abs/2304.06205</link><description>&lt;p&gt;
&#23041;&#26031;&#24247;&#26143;&#20844;&#31435;&#23398;&#26657;&#31038;&#20132;&#39044;&#27979;&#30340;&#22256;&#38590;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Difficult Lessons on Social Prediction from Wisconsin Public Schools. (arXiv:2304.06205v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06205
&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20844;&#31435;&#23398;&#26657;&#24341;&#20837;&#30340;&#39044;&#27979;&#31639;&#27861;&#65288;EWS&#65289;&#26410;&#33021;&#25552;&#39640;&#27605;&#19994;&#29575;&#65292;EWS&#20934;&#30830;&#24615;&#39640;&#65292;&#20294;&#29615;&#22659;&#22240;&#32032;&#24433;&#21709;&#26356;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20844;&#31435;&#23398;&#26657;&#36817;&#26399;&#24341;&#20837;&#20102;&#39044;&#27979;&#31639;&#27861;&#65288;EWS&#65289;&#26469;&#25552;&#39640;&#27605;&#19994;&#29575;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#36807;&#39044;&#27979;&#21738;&#20123;&#23398;&#29983;&#21487;&#33021;&#36864;&#23398;&#65292;&#24110;&#21161;&#23545;&#20010;&#20307;&#23398;&#29983;&#36827;&#34892;&#24178;&#39044;&#12290;&#34429;&#28982;&#25237;&#36164;&#24040;&#22823;&#65292;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23545;EWS&#26377;&#25928;&#24615;&#30340;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#23041;&#26031;&#24247;&#26143;&#20840;&#21306;&#30340;&#36817;&#21313;&#24180;&#25968;&#25454;&#65292;&#39318;&#27425;&#23545;EWS&#23545;&#27605;&#19994;&#29575;&#30340;&#38271;&#26399;&#24433;&#21709;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#39044;&#27979;&#31995;&#32479;&#25152;&#20570;&#30340;&#39118;&#38505;&#35780;&#20272;&#38750;&#24120;&#20934;&#30830;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26469;&#33258;&#36793;&#32536;&#21270;&#32972;&#26223;&#30340;&#23398;&#29983;&#12290;&#23613;&#31649;&#35813;&#31995;&#32479;&#20934;&#30830;&#24615;&#39640;&#24182;&#19988;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#23427;&#20250;&#23548;&#33268;&#27605;&#19994;&#29575;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#32479;&#35745;&#27169;&#24335;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#35265;&#35299;&#23384;&#22312;&#65292;&#21363;&#29615;&#22659;&#22240;&#32032;&#65292;&#20363;&#22914;&#23398;&#29983;&#25152;&#22312;&#23398;&#26657;&#25110;&#31038;&#21306;&#30340;&#36136;&#37327;&#65292;&#28153;&#27809;&#20102;EWS&#23545;&#27605;&#19994;&#29575;&#21487;&#33021;&#20135;&#29983;&#30340;&#20219;&#20309;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early warning systems (EWS) are prediction algorithms that have recently taken a central role in efforts to improve graduation rates in public schools across the US. These systems assist in targeting interventions at individual students by predicting which students are at risk of dropping out. Despite significant investments and adoption, there remain significant gaps in our understanding of the efficacy of EWS. In this work, we draw on nearly a decade's worth of data from a system used throughout Wisconsin to provide the first large-scale evaluation of the long-term impact of EWS on graduation outcomes.  We present evidence that risk assessments made by the prediction system are highly accurate, including for students from marginalized backgrounds. Despite the system's accuracy and widespread use, we find no evidence that it has led to improved graduation rates. We surface a robust statistical pattern that can explain why these seemingly contradictory insights hold. Namely, environmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#36719;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21253;&#25324;ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24418;&#24335;&#39564;&#35777;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06049</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21040;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#30340;&#31934;&#30830;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#33258;&#21160;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Exact and Cost-Effective Automated Transformation of Neural Network Controllers to Decision Tree Controllers. (arXiv:2304.06049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#36719;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21253;&#25324;ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24418;&#24335;&#39564;&#35777;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#25511;&#21046;&#22120;&#22312;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#21644;&#24847;&#22806;&#34892;&#20026;&#21644;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#30340;&#39118;&#38505;&#23545;&#20110;&#22312;&#20855;&#26377;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#24378;&#20445;&#35777;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#37096;&#32626;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#23558;&#22522;&#20110;NN&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#30340;&#36719;&#20915;&#31574;&#26641;&#65288;SDT&#65289;&#25511;&#21046;&#22120;&#21450;&#20854;&#23545;&#21487;&#39564;&#35777;&#24615;&#30340;&#24433;&#21709;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;argmax&#25805;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#30830;&#20294;&#33410;&#30465;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33258;&#21160;&#21024;&#38500;&#22810;&#20313;&#30340;&#20998;&#25903;&#12290;&#25105;&#20204;&#20351;&#29992;OpenAI Gym&#29615;&#22659;&#30340;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SDT&#36716;&#25442;&#21487;&#20197;&#20351;&#24418;&#24335;&#39564;&#35777;&#21463;&#30410;&#65292;&#26174;&#31034;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, neural network (NN)-based controllers have demonstrated remarkable efficacy in a variety of decision-making tasks. However, their black-box nature and the risk of unexpected behaviors and surprising results pose a challenge to their deployment in real-world systems with strong guarantees of correctness and safety. We address these limitations by investigating the transformation of NN-based controllers into equivalent soft decision tree (SDT)-based controllers and its impact on verifiability. Differently from previous approaches, we focus on discrete-output NN controllers including rectified linear unit (ReLU) activation functions as well as argmax operations. We then devise an exact but cost-effective transformation algorithm, in that it can automatically prune redundant branches. We evaluate our approach using two benchmarks from the OpenAI Gym environment. Our results indicate that the SDT transformation can benefit formal verification, showing runtime improveme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.04066</link><description>&lt;p&gt;
&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#30340;&#23433;&#20840;&#31283;&#23450;&#25511;&#21046;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#35270;&#39057;&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#23637;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#26102;&#65292;&#30830;&#20445;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#30340;&#23450;&#20041;&#65292;&#28982;&#21518;&#23558;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#65288;CBF&#65289;&#21644;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLF&#65289;&#26041;&#27861;&#19982;Actor-Critic&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#38556;-&#26446;&#20122;&#26222;&#35834;&#22827;Actor-Critic&#65288;BLAC&#65289;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#20445;&#25345;&#31995;&#32479;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#22522;&#20110;&#26469;&#33258;&#37325;&#25918;&#32531;&#20914;&#21306;&#37319;&#26679;&#30340;&#25968;&#25454;&#26500;&#24314;&#20102;CBF&#23433;&#20840;&#32422;&#26463;&#21644;CLF&#31283;&#23450;&#32422;&#26463;&#65292;&#24182;&#20351;&#29992;&#22686;&#24191;&#25289;&#26684;&#26391;&#26085;&#26041;&#27861;&#26469;&#26356;&#26032;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22791;&#29992;&#25511;&#21046;&#22120;&#65292;&#20197;&#38450;RL&#25511;&#21046;&#22120;&#26080;&#27861;&#25552;&#20379;&#31283;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01483</link><description>&lt;p&gt;
&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20998;&#22359;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#38477;&#20302;&#37096;&#32626;&#38376;&#27099;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;GPT-3&#12289;ChatGPT&#21644;GPT-4&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#30340;&#24040;&#22823;&#35745;&#31639;&#36164;&#28304;&#21644;&#23384;&#20648;&#24320;&#38144;&#20173;&#28982;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BCT&#30340;&#20998;&#22359;&#21387;&#32553;&#26694;&#26550;&#65292;&#21487;&#20197;&#23545;&#25972;&#20010;Transformer&#27169;&#22411;&#36827;&#34892;&#26356;&#32454;&#31890;&#24230;&#30340;&#21387;&#32553;&#65292;&#21253;&#25324;&#23884;&#20837;&#12289;&#30697;&#38453;&#20056;&#27861;&#12289;GELU&#12289;Softmax&#12289;&#23618;&#35268;&#33539;&#21270;&#20197;&#21450;&#25152;&#26377;&#20013;&#38388;&#32467;&#26524;&#12290;&#25105;&#20204;&#23545;&#19968;&#20010;&#39640;&#25928;&#27169;&#22411;&#20351;&#29992;BCT&#36827;&#34892;&#20102;&#21387;&#32553;&#24182;&#22312;&#22810;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;BCT&#21482;&#20250;&#24102;&#26469;&#23569;&#20110;0.90%&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PIPN&#65292;&#23427;&#33021;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#65292;&#26377;&#26395;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.13634</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;PointNet&#65306;&#23427;&#33021;&#21516;&#26102;&#35299;&#20915;&#22810;&#23569;&#19981;&#35268;&#21017;&#20960;&#20309;&#20307;&#30340;&#21453;&#38382;&#39064;&#65311;&#20197;&#32447;&#24377;&#24615;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity. (arXiv:2303.13634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;PIPN&#65292;&#23427;&#33021;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#65292;&#26377;&#26395;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35268;&#30340;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#21033;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#39044;&#27979;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#21482;&#38480;&#20110;&#21333;&#19968;&#30340;&#22495;&#12290;&#30456;&#21453;&#65292;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#39318;&#20808;&#22312;&#24050;&#30693;&#35299;&#65288;&#21363;&#26631;&#35760;&#25968;&#25454;&#65289;&#30340;&#20960;&#21315;&#20010;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#39044;&#27979;&#22312;&#19968;&#20123;&#26410;&#30693;&#22495;&#19978;&#30340;&#35299;&#12290;&#29289;&#29702;&#23398;&#25351;&#23548;&#30340;PointNet&#65288;PIPN&#65289;&#20027;&#35201;&#26088;&#22312;&#22635;&#34917;PINN&#65288;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65289;&#21644;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PIPN&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#25152;&#38656;&#20559;&#24494;&#20998;&#26041;&#31243;&#22312;&#25968;&#30334;&#20010;&#22495;&#19978;&#30340;&#35299;&#65292;&#32780;&#21482;&#20351;&#29992;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20010;&#26694;&#26550;&#26377;&#21161;&#20110;&#22312;&#24037;&#19994;&#30028;&#36827;&#34892;&#24555;&#36895;&#30340;&#20960;&#20309;&#35774;&#35745;&#65292;&#23588;&#20854;&#24403;&#21482;&#26377;&#31232;&#30095;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#26102;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;PIPN&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#24179;&#38754;&#24212;&#21147;&#38382;&#39064;&#22312;500&#22810;&#20010;&#19981;&#21516;&#20960;&#20309;&#20307;&#19978;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pio
&lt;/p&gt;</description></item><item><title>&#22806;&#31185;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#38754;&#20020;&#20960;&#20309;&#22495;&#36716;&#25442;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26368;&#26032;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#22312;&#20960;&#20309;&#20998;&#24067;&#36716;&#25442;&#25968;&#25454;&#19978;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#8220;&#22120;&#23448;&#31227;&#26893;&#8221;&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#20102;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10972</link><description>&lt;p&gt;
&#22806;&#31185;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#22312;&#20960;&#20309;&#22495;&#36716;&#25442;&#19979;
&lt;/p&gt;
&lt;p&gt;
Semantic segmentation of surgical hyperspectral images under geometric domain shifts. (arXiv:2303.10972v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10972
&lt;/p&gt;
&lt;p&gt;
&#22806;&#31185;&#39640;&#20809;&#35889;&#22270;&#20687;&#30340;&#35821;&#20041;&#20998;&#21106;&#38754;&#20020;&#20960;&#20309;&#22495;&#36716;&#25442;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#26368;&#26032;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#22312;&#20960;&#20309;&#20998;&#24067;&#36716;&#25442;&#25968;&#25454;&#19978;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#8220;&#22120;&#23448;&#31227;&#26893;&#8221;&#22686;&#24378;&#25216;&#26415;&#35299;&#20915;&#20102;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#30340;&#26415;&#20013;&#22270;&#20687;&#25968;&#25454;&#30340;&#35821;&#20041;&#20998;&#21106;&#21487;&#20197;&#20026;&#33258;&#21160;&#25163;&#26415;&#22330;&#26223;&#29702;&#35299;&#21644;&#33258;&#20027;&#26426;&#22120;&#20154;&#25163;&#26415;&#38138;&#24179;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25163;&#26415;&#31243;&#24207;&#30340;&#21464;&#21270;&#25110;&#25805;&#20316;&#37096;&#20301;&#30340;&#36974;&#25377;&#65292;&#20960;&#20309;&#22495;&#36716;&#25442;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24320;&#25918;&#25163;&#26415;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#22312;&#35813;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#20027;&#39064;&#12290;&#20026;&#20102;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#65288;1&#65289;&#22312;&#20960;&#20309;&#22495;&#30340;&#20998;&#24067;&#36716;&#25442;&#25968;&#25454;&#23384;&#22312;&#19979;&#65292;&#25552;&#20986;&#20102;&#23545;&#26368;&#26032;&#30340;&#35821;&#20041;&#20998;&#21106;&#32593;&#32476;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#65288;2&#65289;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#8220;&#22120;&#23448;&#31227;&#26893;&#8221;&#30340;&#19987;&#29992;&#22686;&#24378;&#25216;&#26415;&#26469;&#35299;&#20915;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#26159;&#25105;&#20204;&#20174;&#19968;&#33324;&#35745;&#31639;&#26426;&#35270;&#35273;&#31038;&#21306;&#20013;&#25913;&#32534;&#30340;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;&#26469;&#33258;33&#21482;&#29482;&#30340;600&#20010;RGB&#21644;&#39640;&#20809;&#35889;&#25104;&#20687;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#20845;&#20010;&#19981;&#21516;&#20998;&#24067;&#36716;&#25442;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#39564;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SOA&#22120;&#23448;&#20998;&#21106;&#32593;&#32476;&#22312;&#20960;&#20309;&#20998;&#24067;&#36716;&#25442;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#36739;&#22823;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#35821;&#20041;&#19978;&#27880;&#37322;&#20102;19&#20010;&#31867;&#21035;&#30340;&#39640;&#20809;&#35889;&#25104;&#20687;&#65288;HSI&#65289;&#31435;&#26041;&#20307;&#30340;&#22120;&#23448;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust semantic segmentation of intraoperative image data could pave the way for automatic surgical scene understanding and autonomous robotic surgery. Geometric domain shifts, however, although common in real-world open surgeries due to variations in surgical procedures or situs occlusions, remain a topic largely unaddressed in the field. To address this gap in the literature, we (1) present the first analysis of state-of-the-art (SOA) semantic segmentation networks in the presence of geometric out-of-distribution (OOD) data, and (2) address generalizability with a dedicated augmentation technique termed "Organ Transplantation" that we adapted from the general computer vision community. According to a comprehensive validation on six different OOD data sets comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs semantically annotated with 19 classes, we demonstrate a large performance drop of SOA organ segmentation networks applied to geometric OOD data. Surprisingly, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.07925</link><description>&lt;p&gt;
&#36890;&#36807; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#26696;&#20363;&#65292;&#29702;&#35299;&#26102;&#38388;&#34920;&#26684;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#27169;&#22411;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37319;&#29992; Numerai &#25968;&#25454;&#31185;&#23398;&#31454;&#36187;&#30340;&#25968;&#25454;&#65292;&#25506;&#31350;&#20102;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#65307;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22312;&#36890;&#29992;&#24615;&#12289;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#19978;&#20248;&#20110;&#19968;&#20123;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#21033;&#29992;&#20174; Numerai &#25968;&#25454;&#31454;&#36187;&#21019;&#24314;&#30340;&#29305;&#24449;&#30446;&#26631;&#20132;&#21449;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#30340;&#24615;&#33021;&#19982;&#39044;&#27979;&#20250;&#25910;&#25947;&#21040;&#21487;&#30001;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#21051;&#30011;&#30340;&#30456;&#21516;&#24179;&#34913;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19981;&#21516;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#21464;&#25442;&#65292;&#38543;&#21518;&#37319;&#29992;&#23725;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#19982;&#19968;&#20123;&#24120;&#29992;&#30340;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914; LSTM &#21644; transformer&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#65288;&#22312;&#19981;&#21516;&#30340;&#38543;&#26426;&#31181;&#23376;&#19979;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#26041;&#24046;&#65292;&#19988;&#23545;&#26550;&#26500;&#30340;&#36873;&#25321;&#19981;&#22826;&#25935;&#24863;&#65289;&#65292;&#24182;&#19988;&#26356;&#26377;&#25928;&#29575;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#21478;&#19968;&#20010;&#20248;&#21183;&#22312;&#20110;&#27169;&#22411;&#30340;&#31616;&#21333;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#24517;&#35201;&#20351;&#29992;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07900</link><description>&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#30340;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#20013;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23427;&#20204;&#29983;&#25104;&#20174;&#23398;&#20064;&#22270;&#20687;&#20998;&#24067;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#26679;&#26412;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#26041;&#27861;&#26368;&#21021;&#26159;&#21463;&#28418;&#31227;-&#25193;&#25955;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#20294;&#22312;&#36817;&#26399;&#30340;&#23454;&#36341;&#23548;&#21521;&#30340;&#20986;&#29256;&#29289;&#20013;&#65292;&#36825;&#20123;&#36215;&#28304;&#24471;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#23610;&#24230;&#31354;&#38388;&#30740;&#31350;&#30340;&#35282;&#24230;&#30740;&#31350;&#27010;&#29575;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#22312;&#28436;&#21270;&#27010;&#29575;&#20998;&#24067;&#19978;&#28385;&#36275;&#24191;&#20041;&#23610;&#24230;&#31354;&#38388;&#29305;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#19990;&#30028;&#20013;&#28418;&#31227;-&#25193;&#25955;&#29289;&#29702;&#26680;&#24515;&#27010;&#24565;&#35299;&#37322;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;&#27010;&#29575;&#25193;&#25955;&#19982;&#28183;&#36879;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
&lt;/p&gt;</description></item><item><title>RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.07622</link><description>&lt;p&gt;
RE-MOVE&#65306;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#21160;&#24577;&#29615;&#22659;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07622
&lt;/p&gt;
&lt;p&gt;
RE-MOVE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#21453;&#39304;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#35774;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#36866;&#24212;&#23454;&#26102;&#29615;&#22659;&#21464;&#21270;&#65292;&#24182;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#32463;&#24120;&#26080;&#27861;&#22312;&#23454;&#26102;&#37096;&#32626;&#26399;&#38388;&#36866;&#24212;&#29615;&#22659;&#30340;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RE-MOVE&#65288;&#35831;&#27714;&#24110;&#21161;&#24182;&#31227;&#21160;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#26469;&#35843;&#25972;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#20197;&#36866;&#24212;&#29615;&#22659;&#30340;&#23454;&#26102;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#20915;&#23450;&#20309;&#26102;&#35831;&#27714;&#21453;&#39304;&#24182;&#22914;&#20309;&#23558;&#21453;&#39304;&#32435;&#20837;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#20013;&#12290;RE-MOVE&#21033;&#29992;&#20808;&#39564;&#19981;&#30830;&#23450;&#24615;&#26469;&#30830;&#23450;&#35831;&#27714;&#20154;&#31867;&#21453;&#39304;&#30340;&#26368;&#20339;&#26102;&#38388;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#21453;&#39304;&#36827;&#34892;&#23454;&#26102;&#36866;&#24212;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#21512;&#25104;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;&#27979;&#35797;&#26102;&#38388;&#21160;&#24577;&#23548;&#33322;&#22330;&#26223;&#20013;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24182;&#36866;&#24212;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.02731</link><description>&lt;p&gt;
&#23548;&#33322;&#30340;&#20013;&#23618;&#34920;&#31034;&#8212;&#8212;&#34394;&#25311;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#22320;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#25351;&#24341;&#32473;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#23548;&#33322;&#20449;&#24687;&#26159;&#22810;&#27169;&#24577;&#30340;&#26102;&#20505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#20197;&#35270;&#35273;&#26041;&#24335;&#21576;&#29616;&#38750;&#35270;&#35273;&#25351;&#20196;&#20449;&#21495;&#12290;&#36825;&#20123;&#35270;&#35273;&#25351;&#24341;&#20197;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#21472;&#21152;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#65292;&#20316;&#20026;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#34394;&#25311;&#23548;&#33322;&#22312;&#22810;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#28151;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#34394;&#25311;&#23548;&#33322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23558;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#25351;&#20196;&#36716;&#25442;&#20026;&#29992;&#20110;&#30495;&#23454;&#29615;&#22659;&#23454;&#39564;&#30340;&#30452;&#35266;&#35270;&#35273;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#34394;&#25311;&#23548;&#33322;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#35299;&#20915;&#20102;&#26631;&#31614;&#37325;&#21472;&#21644;&#21487;&#35835;&#24615;&#38382;&#39064;&#65292;&#19982;&#29616;&#26377;&#30340;&#25163;&#24037;&#35774;&#35745;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.01388</link><description>&lt;p&gt;
&#24378;&#21270;&#26631;&#31614;: &#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28857;&#29305;&#24449;&#26631;&#31614;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement. (arXiv:2303.01388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#35299;&#20915;&#20102;&#26631;&#31614;&#37325;&#21472;&#21644;&#21487;&#35835;&#24615;&#38382;&#39064;&#65292;&#19982;&#29616;&#26377;&#30340;&#25163;&#24037;&#35774;&#35745;&#31639;&#27861;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#37329;&#34701;&#31561;&#12290;&#26412;&#25991;&#23558;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#26631;&#31614;&#25918;&#32622;&#65292;&#36825;&#26159;&#25968;&#25454;&#21487;&#35270;&#21270;&#20013;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20339;&#20301;&#32622;&#20197;&#36991;&#20813;&#37325;&#21472;&#24182;&#30830;&#20445;&#21487;&#35835;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28857;&#29305;&#24449;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(MADRL)&#26469;&#23398;&#20064;&#26631;&#31614;&#25918;&#32622;&#31574;&#30053;&#65292;&#36825;&#26159;&#19982;&#29616;&#26377;&#25163;&#24037;&#35774;&#35745;&#30340;&#31639;&#27861;&#30456;&#23545;&#24212;&#30340;&#31532;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26631;&#31614;&#25918;&#32622;&#26041;&#27861;&#12290;&#20026;&#20102;&#26041;&#20415;&#24378;&#21270;&#23398;&#20064;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#20854;&#20013;&#20195;&#29702;&#20316;&#20026;&#26631;&#31614;&#30340;&#20195;&#29702;&#65292;&#36825;&#20123;&#26631;&#31614;&#26159;&#19968;&#31181;&#22686;&#24378;&#21487;&#35270;&#21270;&#30340;&#30701;&#25991;&#26412;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#24471;&#21040;&#30340;&#31574;&#30053;&#26126;&#26174;&#20248;&#20110;&#26410;&#32463;&#35757;&#32451;&#30340;&#20195;&#29702;&#30340;&#38543;&#26426;&#31574;&#30053;&#20197;&#21450;&#30001;&#20154;&#31867;&#35774;&#35745;&#30340;&#27604;&#36739;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this paper, we are introducing Reinforcement Learning (RL) to label placement, a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label placement strategy, which is the first machine-learning-driven labeling method in contrast to existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and compared methods designed by human 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#23398;&#20064;&#35268;&#21010;&#21644;&#25511;&#21046;&#31574;&#30053;&#26469;&#35299;&#20915;&#24102;&#26377;&#22797;&#26434;&#36923;&#36753;&#32422;&#26463;&#30340;&#39640;&#32500;&#24230;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#26469;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#38271;&#26399;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.01346</link><description>&lt;p&gt;
&#21463;&#21487;&#24494;&#20998;&#36923;&#36753;&#32422;&#26463;&#30340;&#20849;&#23398;&#20064;&#35268;&#21010;&#19982;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications. (arXiv:2303.01346v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20849;&#23398;&#20064;&#35268;&#21010;&#21644;&#25511;&#21046;&#31574;&#30053;&#26469;&#35299;&#20915;&#24102;&#26377;&#22797;&#26434;&#36923;&#36753;&#32422;&#26463;&#30340;&#39640;&#32500;&#24230;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#30456;&#27604;&#29616;&#26377;&#31639;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#26469;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#38271;&#26399;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#36335;&#24452;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#32508;&#21512;&#35268;&#21010;&#19982;&#25511;&#21046;&#31574;&#30053;&#26159;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#22797;&#26434;&#30340;&#36923;&#36753;&#32422;&#26463;&#21644;&#39640;&#32500;&#24230;&#30340;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#20351;&#20854;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20849;&#23398;&#20064;&#35268;&#21010;&#21644;&#25511;&#21046;&#31574;&#30053;&#26469;&#35299;&#20915;&#24102;&#26377;&#22797;&#26434;&#36923;&#36753;&#32422;&#26463;&#30340;&#39640;&#32500;&#24230;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#35757;&#32451;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#20174;&#22320;&#22270;&#22270;&#20687;&#20013;&#25552;&#21462;&#22797;&#26434;&#35268;&#33539;&#24182;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#19981;&#21516;&#22320;&#22270;&#24067;&#23616;&#30340;&#38271;&#26399;&#26426;&#22120;&#20154;&#36816;&#21160;&#36335;&#24452;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24230;&#25511;&#21046;&#21644;&#36991;&#20813;&#27425;&#20248;&#31574;&#30053;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#25311;&#39640;&#32500;&#26426;&#22120;&#20154;&#23548;&#33322;&#20219;&#21153;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesizing planning and control policies in robotics is a fundamental task, further complicated by factors such as complex logic specifications and high-dimensional robot dynamics. This paper presents a novel reinforcement learning approach to solving high-dimensional robot navigation tasks with complex logic specifications by co-learning planning and control policies. Notably, this approach significantly reduces the sample complexity in training, allowing us to train high-quality policies with much fewer samples compared to existing reinforcement learning algorithms. In addition, our methodology streamlines complex specification extraction from map images and enables the efficient generation of long-horizon robot motion paths across different map layouts. Moreover, our approach also demonstrates capabilities for high-dimensional control and avoiding suboptimal policies via policy alignment. The efficacy of our approach is demonstrated through experiments involving simulated high-dim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#27169;&#25311;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#65292;&#20197;&#21450;&#27169;&#25311;&#26234;&#33021;&#20307;&#25968;&#37327;&#36234;&#22810;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#25928;&#26524;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2302.13423</link><description>&lt;p&gt;
&#27169;&#25311;&#19982;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#22312;&#25805;&#32437;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach. (arXiv:2302.13423v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#20013;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#27169;&#25311;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#65292;&#20197;&#21450;&#27169;&#25311;&#26234;&#33021;&#20307;&#25968;&#37327;&#36234;&#22810;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#25928;&#26524;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#26159;&#26426;&#22120;&#20154;&#25805;&#32437;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#26082;&#19981;&#39640;&#25928;&#65288;&#21363;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#36739;&#24930;&#65289;&#65292;&#20063;&#19981;&#26377;&#25928;&#65288;&#21363;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#25968;&#25454;&#36739;&#23569;&#65289;&#12290;&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#26102;&#38388;&#21644;&#30828;&#20214;&#39044;&#31639;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#30340;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#30340;&#27169;&#25311;&#19982;&#23454;&#38469;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861; (CSAR)&#65292;&#29992;&#20110;&#25805;&#32437;&#22120;&#20154;&#30340;&#25361;&#36873;&#21644;&#25918;&#32622;&#20219;&#21153;&#65292;&#35813;&#31639;&#27861;&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#19990;&#30028;&#20013;&#37117;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20010;&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#22120;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#35757;&#32451;&#26234;&#33021;&#20307;&#26469;&#33719;&#24471;&#27169;&#25311;&#21644;&#23454;&#38469;&#19990;&#30028;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#65288;1&#65289;&#22312;&#27169;&#25311;&#20013;&#30340;&#26368;&#20339;&#31574;&#30053;&#24182;&#19981;&#26159;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#30340;&#26368;&#20339;&#31574;&#30053;&#12290;&#65288;2&#65289;&#27169;&#25311;&#26234;&#33021;&#20307;&#36234;&#22810;&#65292;&#27169;&#25311;&#19982;&#23454;&#38469;&#35757;&#32451;&#25928;&#26524;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sim-and-real training is a promising alternative to sim-to-real training for robot manipulations. However, the current sim-and-real training is neither efficient, i.e., slow convergence to the optimal policy, nor effective, i.e., sizeable real-world robot data. Given limited time and hardware budgets, the performance of sim-and-real training is not satisfactory. In this paper, we propose a Consensus-based Sim-And-Real deep reinforcement learning algorithm (CSAR) for manipulator pick-and-place tasks, which shows comparable performance in both sim-and-real worlds. In this algorithm, we train the agents in simulators and the real world to get the optimal policies for both sim-and-real worlds. We found two interesting phenomenons: (1) Best policy in simulation is not the best for sim-and-real training. (2) The more simulation agents, the better sim-and-real training. The experimental video is available at: https://youtu.be/mcHJtNIsTEQ.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07729</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#37117;&#20197;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#21069;&#35328;&#65292;&#20197;&#24635;&#32467;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#20142;&#28857;&#19981;&#20165;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#35770;&#25991;&#30340;&#36129;&#29486;&#65292;&#36824;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#22686;&#21152;&#20102;&#25991;&#31456;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#30740;&#31350;&#35770;&#25991;&#30340;&#29305;&#23450;&#27573;&#33853;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#26500;&#24314;&#30740;&#31350;&#20142;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#35206;&#30422;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#23618;&#30340;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65292;&#23558;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20026;SciBERT&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;CSPubSum&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36824;&#25552;&#20986;&#20102;MixSub&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#26032;&#30340;&#36328;&#23398;&#31185;&#35770;&#25991;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;CSPubSum&#21644;MixSub&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#30456;&#20851;&#21464;&#20307;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;CSPubSum&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#35770;&#25991;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
&lt;/p&gt;</description></item><item><title>&#36807;&#24230;&#20351;&#29992;&#40657;&#21283;&#23376;&#27169;&#22411;&#20250;&#23548;&#33268;&#31616;&#21333;&#20219;&#21153;&#30340;&#28010;&#36153;&#65292;&#36879;&#26126;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.02804</link><description>&lt;p&gt;
&#19981;&#35201;&#20877;&#36807;&#24230;&#20351;&#29992;&#40657;&#21283;&#23376;&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#20219;&#21153;&#65292;&#36716;&#32780;&#20351;&#29992;&#36879;&#26126;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stop overkilling simple tasks with black-box models and use transparent models instead. (arXiv:2302.02804v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02804
&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#20351;&#29992;&#40657;&#21283;&#23376;&#27169;&#22411;&#20250;&#23548;&#33268;&#31616;&#21333;&#20219;&#21153;&#30340;&#28010;&#36153;&#65292;&#36879;&#26126;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#21516;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#33021;&#22815;&#33258;&#20027;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#36825;&#20801;&#35768;&#36339;&#36807;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#23481;&#26131;&#20986;&#38169;&#21644;&#28902;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#28145;&#24230;&#23398;&#20064;&#31574;&#30053;&#22312;&#31934;&#24230;&#26041;&#38754;&#36890;&#24120;&#20248;&#20110;&#20256;&#32479;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the employment of deep learning methods has led to several significant breakthroughs in artificial intelligence. Different from traditional machine learning models, deep learning-based approaches are able to extract features autonomously from raw data. This allows for bypassing the feature engineering process, which is generally considered to be both error-prone and tedious. Moreover, deep learning strategies often outperform traditional models in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25216;&#33021;&#30340;&#20998;&#23618;&#39550;&#39542;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#20351;&#29992;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#39640;&#23618;&#21160;&#20316;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#21512;&#24182;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#26356;&#39640;&#24615;&#33021;&#30340;&#39550;&#39542;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.02179</link><description>&lt;p&gt;
&#39640;&#25928;&#24320;&#21457;&#39550;&#39542;&#31574;&#30053;&#65306;&#22522;&#20110;&#25216;&#33021;&#30340;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach. (arXiv:2302.02179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#25216;&#33021;&#30340;&#20998;&#23618;&#39550;&#39542;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#20351;&#29992;&#36816;&#21160;&#21407;&#35821;&#20316;&#20026;&#39640;&#23618;&#21160;&#20316;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#65292;&#24182;&#19988;&#22312;&#21512;&#24182;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#26356;&#39640;&#24615;&#33021;&#30340;&#39550;&#39542;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#39550;&#39542;&#27773;&#36710;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#23618;&#27425;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#20154;&#31867;&#39550;&#39542;&#21592;&#21487;&#20197;&#36731;&#26494;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#22240;&#27492;&#19968;&#30452;&#22312;&#21162;&#21147;&#27169;&#25311;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20316;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#25110;&#21019;&#24314;&#30495;&#23454;&#24863;&#27169;&#25311;&#22120;&#30340;&#28789;&#24863;&#12290;&#24378;&#21270;&#23398;&#20064;&#26159;&#24314;&#27169;&#39550;&#39542;&#31574;&#30053;&#30340;&#24120;&#29992;&#24037;&#20855;&#65292;&#20294;&#20256;&#32479;&#30340;&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;"&#22522;&#20110;&#25216;&#33021;"&#30340;&#20998;&#23618;&#39550;&#39542;&#31574;&#30053;&#65292;&#20854;&#20013;&#36816;&#21160;&#21407;&#35821;&#65288;&#21363;&#25216;&#33021;&#65289;&#34987;&#35774;&#35745;&#24182;&#29992;&#20316;&#39640;&#23618;&#21160;&#20316;&#12290;&#36825;&#20943;&#23569;&#20102;&#38656;&#35201;&#20855;&#26377;&#19981;&#21516;&#34892;&#20026;&#30340;&#22810;&#20010;&#27169;&#22411;&#30340;&#24212;&#29992;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#21512;&#24182;&#22330;&#26223;&#30340;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#22312;&#36739;&#23569;&#35757;&#32451;&#27425;&#25968;&#19979;&#23454;&#29616;&#26356;&#39640;&#24615;&#33021;&#30340;&#39550;&#39542;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Driving in dense traffic with human and autonomous drivers is a challenging task that requires high-level planning and reasoning. Human drivers can achieve this task comfortably, and there has been many efforts to model human driver strategies. These strategies can be used as inspirations for developing autonomous driving algorithms or to create high-fidelity simulators. Reinforcement learning is a common tool to model driver policies, but conventional training of these models can be computationally expensive and time-consuming. To address this issue, in this paper, we propose ``skill-based" hierarchical driving strategies, where motion primitives, i.e. skills, are designed and used as high-level actions. This reduces the training time for applications that require multiple models with varying behavior. Simulation results in a merging scenario demonstrate that the proposed approach yields driver models that achieve higher performance with less training compared to baseline reinforcemen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;3D-MIM&#65292;&#29992;&#20110;&#39044;&#27979;&#36229;&#26032;&#26143;&#29190;&#28856;&#21518;&#30340;&#22771;&#23618;&#25193;&#24352;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#26816;&#27979;&#24182;&#39044;&#27979;&#36229;&#26032;&#26143;&#24433;&#21709;&#31890;&#23376;&#25152;&#22312;&#30340;&#22771;&#23618;&#24418;&#29366;&#65292;&#35299;&#20915;&#20102;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;&#20013;&#36229;&#26032;&#26143;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.00026</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#36229;&#26032;&#26143;&#22771;&#23618;&#25193;&#24352;&#30340;3D&#26102;&#31354;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
3D-Spatiotemporal Forecasting the Expansion of Supernova Shells Using Deep Learning toward High-Resolution Galaxy Simulations. (arXiv:2302.00026v2 [astro-ph.GA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;3D-MIM&#65292;&#29992;&#20110;&#39044;&#27979;&#36229;&#26032;&#26143;&#29190;&#28856;&#21518;&#30340;&#22771;&#23618;&#25193;&#24352;&#65292;&#36890;&#36807;&#22312;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#26816;&#27979;&#24182;&#39044;&#27979;&#36229;&#26032;&#26143;&#24433;&#21709;&#31890;&#23376;&#25152;&#22312;&#30340;&#22771;&#23618;&#24418;&#29366;&#65292;&#35299;&#20915;&#20102;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;&#20013;&#36229;&#26032;&#26143;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#26032;&#26143;&#22312;&#26143;&#31995;&#24418;&#25104;&#21644;&#28436;&#21270;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#26143;&#31995;&#27169;&#25311;&#26102;&#65292;&#36229;&#26032;&#26143;&#30340;&#30701;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#25104;&#20026;&#20005;&#37325;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;Hamiltonian&#20998;&#35010;&#26041;&#27861;&#65292;&#21363;&#23558;&#38656;&#35201;&#30701;&#31215;&#20998;&#26102;&#38388;&#27493;&#38271;&#30340;&#21306;&#22495;&#19982;&#25972;&#20010;&#31995;&#32479;&#20998;&#24320;&#31215;&#20998;&#12290;&#20026;&#20102;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#27169;&#25311;&#20013;&#21463;&#36229;&#26032;&#26143;&#24433;&#21709;&#30340;&#31890;&#23376;&#65292;&#25105;&#20204;&#38656;&#35201;&#22312;&#38543;&#21518;&#30340;&#20840;&#23616;&#27493;&#39588;&#20013;&#25552;&#21069;&#26816;&#27979;&#21040;&#36825;&#20123;&#31890;&#23376;&#25152;&#22312;&#30340;&#22771;&#23618;&#30340;&#24418;&#29366;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;3D-MIM&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#36229;&#26032;&#26143;&#29190;&#28856;&#21518;&#30340;&#22771;&#23618;&#25193;&#24352;&#12290;&#36890;&#36807;&#23545;&#24102;&#26377;&#31890;&#23376;&#36136;&#37327;$m_{\rm gas}$ = 1 M$_\odot$&#30340;&#28237;&#27969;&#20113;&#27169;&#25311;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#22320;&#20877;&#29616;&#20986;&#21508;&#21521;&#24322;&#24615;&#30340;&#22771;&#23618;&#24418;&#29366;&#65292;&#20854;&#20013;&#23494;&#24230;&#36880;&#28176;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supernova (SN) plays an important role in galaxy formation and evolution. In high-resolution galaxy simulations using massively parallel computing, short integration timesteps for SNe are serious bottlenecks. This is an urgent issue that needs to be resolved for future higher-resolution galaxy simulations. One possible solution would be to use the Hamiltonian splitting method, in which regions requiring short timesteps are integrated separately from the entire system. To apply this method to the particles affected by SNe in a smoothed-particle hydrodynamics simulation, we need to detect the shape of the shell on and within which such SN-affected particles reside during the subsequent global step in advance. In this paper, we develop a deep learning model, 3D-MIM, to predict a shell expansion after a SN explosion. Trained on turbulent cloud simulations with particle mass $m_{\rm gas}$~=~1 M$_\odot$, the model accurately reproduces the anisotropic shell shape, where densities decrease by
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;BGLMs&#19978;&#23454;&#29616;&#30340;&#26080;&#38656;&#22270;&#39592;&#26550;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#30340;&#28176;&#36827;&#36951;&#25022;&#29575;$O(\sqrt{T}\ln T)$&#12290;</title><link>http://arxiv.org/abs/2301.13392</link><description>&lt;p&gt;
&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Combinatorial Causal Bandits without Graph Skeleton. (arXiv:2301.13392v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21487;&#22312;BGLMs&#19978;&#23454;&#29616;&#30340;&#26080;&#38656;&#22270;&#39592;&#26550;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#36798;&#21040;&#20102;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#30340;&#28176;&#36827;&#36951;&#25022;&#29575;$O(\sqrt{T}\ln T)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#22312;&#27599;&#19968;&#36718;&#36873;&#25321;&#19968;&#32452;&#21464;&#37327;&#36827;&#34892;&#24178;&#39044;&#65292;&#25910;&#38598;&#35266;&#27979;&#21464;&#37327;&#30340;&#21453;&#39304;&#20197;&#26368;&#23567;&#21270;&#26399;&#26395;&#36951;&#25022;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;&#20108;&#20540;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;BGLMs&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#37117;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#26469;&#26500;&#24314;&#22240;&#26524;&#20851;&#31995;&#22270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20108;&#20540;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#21644;BGLMs&#19978;&#19981;&#32771;&#34385;&#22270;&#39592;&#26550;&#30340;&#32452;&#21512;&#22240;&#26524;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19968;&#33324;&#22240;&#26524;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#32047;&#31215;&#36951;&#25022;&#30340;&#25351;&#25968;&#19979;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#38656;&#22270;&#39592;&#26550;&#26469;&#23454;&#29616;BGLMs&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#34920;&#26126;&#23427;&#20173;&#28982;&#36798;&#21040;$O(\sqrt{T}\ln T)$&#30340;&#26399;&#26395;&#36951;&#25022;&#12290;&#36825;&#20010;&#28176;&#36827;&#30340;&#36951;&#25022;&#29575;&#19982;&#20381;&#36182;&#20110;&#22270;&#32467;&#26500;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In combinatorial causal bandits (CCB), the learning agent chooses a subset of variables in each round to intervene and collects feedback from the observed variables to minimize expected regret or sample complexity. Previous works study this problem in both general causal models and binary generalized linear models (BGLMs). However, all of them require prior knowledge of causal graph structure. This paper studies the CCB problem without the graph structure on binary general causal models and BGLMs. We first provide an exponential lower bound of cumulative regrets for the CCB problem on general causal models. To overcome the exponentially large space of parameters, we then consider the CCB problem on BGLMs. We design a regret minimization algorithm for BGLMs even without the graph skeleton and show that it still achieves $O(\sqrt{T}\ln T)$ expected regret. This asymptotic regret is the same as the state-of-art algorithms relying on the graph structure. Moreover, we sacrifice the regret t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;&#20256;&#32479;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24037;&#31243;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.13331</link><description>&lt;p&gt;
&#31070;&#32463;&#25805;&#20316;&#21592;&#65306;&#25968;&#25454;&#26159;&#21542;&#36275;&#20197;&#27169;&#25311;&#19990;&#30028;&#65311;&#23545;&#29289;&#29702;&#21551;&#31034;&#26426;&#22120;&#23398;&#20064;&#24433;&#21709;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning. (arXiv:2301.13331v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19982;&#20256;&#32479;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#20197;&#35299;&#20915;&#24037;&#31243;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#24182;&#25351;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#24120;&#20351;&#29992;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#25968;&#20540;&#36817;&#20284;&#26469;&#26500;&#24314;&#35299;&#20915;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#38382;&#39064;&#30340;&#26041;&#26696;&#65292;&#36825;&#20123;&#38382;&#39064;&#28041;&#21450;&#21040;&#22810;&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#27604;&#22914;&#28909;&#20256;&#23548;&#25110;&#22768;&#38899;&#20256;&#25773;&#12289;&#27969;&#20307;&#27969;&#21160;&#12289;&#24377;&#24615;&#12289;&#38745;&#30005;&#23398;&#12289;&#30005;&#21160;&#21147;&#23398;&#31561;&#12290;&#34429;&#28982;&#36825;&#22312;&#35299;&#20915;&#35768;&#22810;&#22797;&#26434;&#29616;&#35937;&#26041;&#38754;&#21457;&#25381;&#20102;&#20316;&#29992;&#65292;&#20294;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#24120;&#35268;&#26041;&#27861;&#22914;&#26377;&#38480;&#20803;&#27861;&#65288;FEM&#65289;&#21644;&#26377;&#38480;&#24046;&#20998;&#27861;&#65288;FDM&#65289;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#19988;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#12289;&#30456;&#23545;&#20934;&#30830;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#20855;&#26377;&#31163;&#25955;&#19981;&#21464;&#24615;&#21644;&#20998;&#36776;&#29575;&#19981;&#21464;&#24615;&#31561;&#20248;&#21183;&#12290;&#26412;&#25991;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22914;&#20309;&#19982;&#20256;&#32479;&#25216;&#26415;&#30456;&#36741;&#30456;&#25104;&#65292;&#35299;&#20915;&#24037;&#31243;&#21644;&#29289;&#29702;&#38382;&#39064;&#65292;&#21516;&#26102;&#25351;&#20986;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20123;&#20027;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#21435;&#38500;&#25110;&#32553;&#30701;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36339;&#36291;&#36830;&#25509;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#30828;&#20214;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30828;&#20214;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2301.07247</link><description>&lt;p&gt;
Tailor&#65306;&#20026;&#36164;&#28304;&#25928;&#29575;&#25512;&#26029;&#25913;&#21464;&#36339;&#36291;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Tailor: Altering Skip Connections for Resource-Efficient Inference. (arXiv:2301.07247v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#21435;&#38500;&#25110;&#32553;&#30701;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36339;&#36291;&#36830;&#25509;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#30828;&#20214;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30828;&#20214;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#24102;&#23485;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20351;&#29992;&#36339;&#36291;&#36830;&#25509;&#26469;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36339;&#36291;&#36830;&#25509;&#22312;&#30828;&#20214;&#19978;&#24456;&#26114;&#36149;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#32531;&#20914;&#21306;&#65292;&#24182;&#22686;&#21152;&#20102;&#29255;&#19978;&#21644;&#29255;&#22806;&#23384;&#20648;&#22120;&#30340;&#21033;&#29992;&#21644;&#24102;&#23485;&#38656;&#27714;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#30828;&#20214;-&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#26041;&#27861;&#26469;&#20248;&#21270;&#30828;&#20214;&#19978;&#30340;&#36339;&#36291;&#36830;&#25509;&#12290;&#20316;&#32773;&#35748;&#20026;&#65292;&#23613;&#31649;&#32593;&#32476;&#30340;&#36339;&#36291;&#36830;&#25509;&#23545;&#32593;&#32476;&#30340;&#23398;&#20064;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#21435;&#38500;&#25110;&#32553;&#30701;&#36339;&#36291;&#36830;&#25509;&#65292;&#20197;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#30828;&#20214;&#23454;&#29616;&#12290;&#20316;&#32773;&#24341;&#20837;&#20102;Tailor&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#30721;&#35774;&#35745;&#24037;&#20855;&#65292;&#20854;&#30828;&#20214;&#24863;&#30693;&#30340;&#35757;&#32451;&#31639;&#27861;&#36880;&#28176;&#21435;&#38500;&#25110;&#32553;&#30701;&#19968;&#20010;&#23436;&#20840;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#30340;&#36339;&#36291;&#36830;&#25509;&#65292;&#20174;&#32780;&#38477;&#20302;&#23427;&#20204;&#30340;&#30828;&#20214;&#20195;&#20215;&#12290;Tailor&#22312;&#29255;&#19978;&#25968;&#25454;&#27969;&#24335;&#20307;&#31995;&#32467;&#26500;&#20013;&#25552;&#39640;&#20102;34&#65285;&#30340;BRAM&#21033;&#29992;&#29575;&#65292;13&#65285;&#30340;FF&#21033;&#29992;&#29575;&#21644;16&#65285;&#30340;LUT&#21033;&#29992;&#29575;&#12290;Tailor&#25552;&#39640;&#20102;30&#65285;&#30340;&#24615;&#33021;&#65292;&#24182;&#38477;&#20302;&#20102;45&#65285;&#30340;&#20869;&#23384;&#24102;&#23485;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks use skip connections to improve training convergence. However, these skip connections are costly in hardware, requiring extra buffers and increasing on- and off-chip memory utilization and bandwidth requirements. In this paper, we show that skip connections can be optimized for hardware when tackled with a hardware-software codesign approach. We argue that while a network's skip connections are needed for the network to learn, they can later be removed or shortened to provide a more hardware efficient implementation with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose hardware-aware training algorithm gradually removes or shortens a fully trained network's skip connections to lower their hardware cost. Tailor improves resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs for on-chip, dataflow-style architectures. Tailor increases performance by 30% and reduces memory bandwidth by 45% for a 2D processing element array arc
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#36235;&#21183;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20171;&#32461;&#20102;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#12289;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#22270;&#20687;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.05712</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#32508;&#36848;: &#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends. (arXiv:2301.05712v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#21644;&#36235;&#21183;&#30340;&#35282;&#24230;&#27010;&#36848;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22810;&#32500;&#35270;&#35282;&#12290;&#23427;&#20171;&#32461;&#20102;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#12289;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#20197;&#21450;&#22312;&#22270;&#20687;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#26679;&#26412;&#26469;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21644;&#26631;&#35760;&#36807;&#22810;&#30340;&#26679;&#26412;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;&#20316;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#23376;&#38598;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26088;&#22312;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#12290;SSL&#26368;&#36817;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#19988;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#30456;&#20851;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#23569;&#26377;&#32508;&#36848;&#30740;&#31350;&#26469;&#35299;&#37322;&#19981;&#21516;&#30340;SSL&#21464;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#28436;&#21464;&#12290;&#26412;&#25991;&#20174;&#31639;&#27861;&#12289;&#24212;&#29992;&#12289;&#19977;&#20010;&#20027;&#35201;&#36235;&#21183;&#21644;&#24453;&#35299;&#38382;&#39064;&#30340;&#35270;&#35282;&#32508;&#36848;&#20102;&#21508;&#31181;SSL&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#22823;&#22810;&#25968;SSL&#31639;&#27861;&#30340;&#21160;&#26426;&#65292;&#24182;&#27604;&#36739;&#20102;&#23427;&#20204;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#27010;&#36848;&#20102;SSL&#22312;&#22270;&#20687;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#39046;&#22495;&#20013;&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep supervised learning algorithms generally require large numbers of labeled examples to achieve satisfactory performance. However, collecting and labeling too many examples can be costly and time-consuming. As a subset of unsupervised learning, self-supervised learning (SSL) aims to learn useful features from unlabeled examples without any human-annotated labels. SSL has recently attracted much attention and many related algorithms have been developed. However, there are few comprehensive studies that explain the connections and evolution of different SSL variants. In this paper, we provide a review of various SSL methods from the perspectives of algorithms, applications, three main trends, and open questions. First, the motivations of most SSL algorithms are introduced in detail, and their commonalities and differences are compared. Second, typical applications of SSL in domains such as image processing and computer vision (CV), as well as natural language processing (NLP), are dis
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;EquIN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#25968;&#25454;&#19978;&#20855;&#26377;&#19968;&#33324;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#32771;&#34385;&#31283;&#23450;&#23376;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2301.05231</link><description>&lt;p&gt;
&#22312;&#31283;&#23450;&#23376;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Representation Learning in the Presence of Stabilizers. (arXiv:2301.05231v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05231
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;EquIN&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#25968;&#25454;&#19978;&#20855;&#26377;&#19968;&#33324;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;&#32771;&#34385;&#31283;&#23450;&#23376;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#21462;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31561;&#21464;&#21516;&#26500;&#32593;&#32476;&#65288;EquIN&#65289;-&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#19982;&#25968;&#25454;&#19978;&#30340;&#19968;&#33324;&#32676;&#20316;&#29992;&#31561;&#21464;&#30340;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#31561;&#21464;&#34920;&#31034;&#23398;&#20064;&#26041;&#24335;&#19981;&#21516;&#65292;EquIN&#36866;&#29992;&#20110;&#38750;&#33258;&#30001;&#32676;&#20316;&#29992;&#65292;&#21363;&#36890;&#36807;&#38750;&#24179;&#20961;&#23545;&#31216;&#24615;&#31283;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;EquIN&#22312;&#32676;&#35770;&#20013;&#30340;&#36712;&#36947;&#31283;&#23450;&#23376;&#23450;&#29702;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#36827;&#34892;&#12290;&#36825;&#20445;&#35777;&#20102;&#29702;&#24819;&#30340;&#23398;&#20064;&#22120;&#20165;&#36890;&#36807;&#31561;&#21464;&#24615;&#35757;&#32451;&#26102;&#25512;&#26029;&#20986;&#21516;&#26500;&#34920;&#31034;&#65292;&#24182;&#23436;&#20840;&#25552;&#21462;&#20102;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#26059;&#36716;&#23545;&#31216;&#24615;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#32771;&#34385;&#31283;&#23450;&#23376;&#21487;&#20197;&#25552;&#39640;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Equivariant Isomorphic Networks (EquIN) -- a method for learning representations that are equivariant with respect to general group actions over data. Differently from existing equivariant representation learners, EquIN is suitable for group actions that are not free, i.e., that stabilize data via nontrivial symmetries. EquIN is theoretically grounded in the orbit-stabilizer theorem from group theory. This guarantees that an ideal learner infers isomorphic representations while trained on equivariance alone and thus fully extracts the geometric structure of data. We provide an empirical investigation on image datasets with rotational symmetries and show that taking stabilizers into account improves the quality of the representations.
&lt;/p&gt;</description></item><item><title>&#35299;&#20915;&#22870;&#21169;&#20551;&#35774;&#65292;&#26126;&#30830;&#25351;&#26126;&#20551;&#35774;&#25104;&#31435;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#38544;&#21547;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2212.10420</link><description>&lt;p&gt;
&#35299;&#20915;&#22870;&#21169;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Settling the Reward Hypothesis. (arXiv:2212.10420v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10420
&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#22870;&#21169;&#20551;&#35774;&#65292;&#26126;&#30830;&#25351;&#26126;&#20551;&#35774;&#25104;&#31435;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#38544;&#21547;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#20551;&#35774;&#35748;&#20026;&#65292;&#8220;&#25105;&#20204;&#25152;&#35828;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#37117;&#21487;&#20197;&#24819;&#35937;&#20026;&#26368;&#22823;&#21270;&#25509;&#25910;&#21040;&#30340;&#26631;&#37327;&#20449;&#21495;&#65288;&#22870;&#21169;&#65289;&#30340;&#32047;&#31215;&#24635;&#21644;&#30340;&#39044;&#26399;&#20540;&#12290;&#8221;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23436;&#20840;&#35299;&#20915;&#36825;&#20010;&#20551;&#35774;&#12290;&#36825;&#23558;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#32943;&#23450;&#25110;&#21542;&#23450;&#65292;&#32780;&#26159;&#23436;&#20840;&#25351;&#26126;&#20551;&#35774;&#25104;&#31435;&#30340;&#30446;&#26631;&#21644;&#30446;&#30340;&#30340;&#38544;&#21547;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reward hypothesis posits that, "all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)." We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;</title><link>http://arxiv.org/abs/2212.09597</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#32508;&#21512;&#35843;&#26597;&#12290;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#31995;&#32479;&#36164;&#28304;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#20316;&#20026;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#21307;&#30103;&#35786;&#26029;&#12289;&#35848;&#21028;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#21518;&#31471;&#25903;&#25345;&#12290;&#26412;&#25991;&#23545;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#36827;&#34892;&#25512;&#29702;&#30340;&#21069;&#27839;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30740;&#31350;&#25104;&#26524;&#30340;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#25552;&#20379;&#20102;&#31995;&#32479;&#36164;&#28304;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26032;&#20852;&#25512;&#29702;&#33021;&#21147;&#20986;&#29616;&#30340;&#28508;&#22312;&#21407;&#22240;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#36164;&#28304;&#21487;&#22312; https://github.com/zjunlp/Prompt4ReasoningPapers &#19978;&#33719;&#21462;&#65288;&#23450;&#26399;&#26356;&#26032;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#25903;&#25345;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;OTA-FL&#23454;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#31354;&#20013;&#35745;&#31639;&#20943;&#36731;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#39564;&#35777;&#20102;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.06482</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#25193;&#23637;&#30340;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#36827;&#34892;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;&#65288;OTA-FL&#65289;
&lt;/p&gt;
&lt;p&gt;
Over-The-Air Federated Learning Over Scalable Cell-free Massive MIMO. (arXiv:2212.06482v3 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#25903;&#25345;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#38469;&#30340;OTA-FL&#23454;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#31354;&#20013;&#35745;&#31639;&#20943;&#36731;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#39564;&#35777;&#20102;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#25216;&#26415;&#65292;&#39044;&#35745;&#23558;&#19982;&#20256;&#32479;&#34562;&#31389;&#31995;&#32479;&#30456;&#27604;&#65292;&#25552;&#20379;&#22343;&#21248;&#35206;&#30422;&#21644;&#39640;&#39057;&#35889;&#25928;&#29575;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#22914;&#20309;&#25903;&#25345;&#32852;&#37030;&#36793;&#32536;&#23398;&#20064;&#12290;&#21033;&#29992;&#26080;&#32447;&#22810;&#22336;&#20449;&#36947;&#30340;&#21487;&#21152;&#24615;&#29305;&#24615;&#65292;&#36890;&#36807;&#31354;&#20013;&#35745;&#31639;&#30340;&#26041;&#24335;&#65292;&#23458;&#25143;&#31471;&#21516;&#26102;&#23558;&#20182;&#20204;&#30340;&#26412;&#22320;&#26356;&#26032;&#21457;&#36865;&#21040;&#30456;&#21516;&#30340;&#36890;&#20449;&#36164;&#28304;&#19978;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;&#31354;&#20013;&#32852;&#37030;&#23398;&#20064;&#65288;OTA-FL&#65289;&#65292;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#36731;&#22312;&#26080;&#32447;&#32593;&#32476;&#19978;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#32771;&#34385;&#21040;&#20449;&#36947;&#30456;&#20851;&#24615;&#21644;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#20165;&#26377;&#30340;&#19981;&#23436;&#20840;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#19978;&#23454;&#29616;OTA-FL&#30340;&#23454;&#38469;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#30740;&#31350;&#20102;&#35813;&#23454;&#29616;&#30340;&#25910;&#25947;&#24615;&#65292;&#39564;&#35777;&#20102;&#26080;&#34562;&#31389;&#22823;&#35268;&#27169;MIMO&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cell-free massive MIMO is emerging as a promising technology for future wireless communication systems, which is expected to offer uniform coverage and high spectral efficiency compared to classical cellular systems. We study in this paper how cell-free massive MIMO can support federated edge learning. Taking advantage of the additive nature of the wireless multiple access channel, over-the-air computation is exploited, where the clients send their local updates simultaneously over the same communication resource. This approach, known as over-the-air federated learning (OTA-FL), is proven to alleviate the communication overhead of federated learning over wireless networks. Considering channel correlation and only imperfect channel state information available at the central server, we propose a practical implementation of OTA-FL over cell-free massive MIMO. The convergence of the proposed implementation is studied analytically and experimentally, confirming the benefits of cell-free mas
&lt;/p&gt;</description></item><item><title>FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.01197</link><description>&lt;p&gt;
FedALA: &#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01197
&lt;/p&gt;
&lt;p&gt;
FedALA&#26159;&#19968;&#31181;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#26469;&#35299;&#20915;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#36229;&#36807;&#20102;11&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#36825;&#20250;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated learning with Adaptive Local Aggregation&#65288;FedALA&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#25429;&#25417;&#20840;&#23616;&#27169;&#22411;&#23545;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;FedALA&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#33258;&#36866;&#24212;&#23616;&#37096;&#32858;&#21512;&#65288;ALA&#65289;&#27169;&#22359;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#23616;&#37096;&#30446;&#26631;&#33258;&#36866;&#24212;&#32858;&#21512;&#19979;&#36733;&#30340;&#20840;&#23616;&#27169;&#22411;&#21644;&#26412;&#22320;&#27169;&#22411;&#20197;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#21021;&#22987;&#21270;&#26412;&#22320;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;FedALA&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20351;&#29992;&#20102;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;FedALA&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#27604;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26368;&#22810;3.27%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;ALA&#27169;&#22359;&#24212;&#29992;&#20110;&#20854;&#20182;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#22312;&#27979;&#35797;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#22810;24.19%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19981;&#21487;&#20449;&#25512;&#33616;&#32773;&#30340;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#25289;&#20280;&#26426;&#21046;&#21644;&#37325;&#26032;&#32553;&#25918;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2212.00306</link><description>&lt;p&gt;
&#20855;&#26377;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#30340;&#20998;&#25955;&#30697;&#38453;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Decentralized Matrix Factorization with Heterogeneous Differential Privacy. (arXiv:2212.00306v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19981;&#21487;&#20449;&#25512;&#33616;&#32773;&#30340;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65292;&#36890;&#36807;&#20462;&#25913;&#25289;&#20280;&#26426;&#21046;&#21644;&#37325;&#26032;&#32553;&#25918;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30697;&#38453;&#20998;&#35299;&#20381;&#36182;&#20110;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#38598;&#20013;&#25910;&#38598;&#26469;&#36827;&#34892;&#25512;&#33616;&#65292;&#36825;&#21487;&#33021;&#20250;&#22686;&#21152;&#38544;&#31169;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#29305;&#21035;&#26159;&#22312;&#25512;&#33616;&#32773;&#19981;&#21487;&#20449;&#30340;&#24773;&#20917;&#19979;&#12290;&#29616;&#26377;&#30340;&#24046;&#20998;&#38544;&#31169;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#25512;&#33616;&#32773;&#26159;&#21487;&#20449;&#30340;&#65292;&#35201;&#20040;&#21482;&#33021;&#20026;&#25152;&#26377;&#29992;&#25143;&#21644;&#29289;&#21697;&#25552;&#20379;&#32479;&#19968;&#32423;&#21035;&#30340;&#38544;&#31169;&#20445;&#25252;&#65292;&#24403;&#25512;&#33616;&#32773;&#19981;&#21487;&#20449;&#26102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#19981;&#21487;&#20449;&#25512;&#33616;&#32773;&#30340;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#30697;&#38453;&#20998;&#35299;&#31639;&#27861;&#65288;&#31216;&#20026;HDPMF&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#19981;&#21487;&#20449;&#25512;&#33616;&#32773;&#22330;&#26223;&#19979;&#23454;&#29616;&#20998;&#24067;&#24335;&#30697;&#38453;&#20998;&#35299;&#30340;&#24322;&#26500;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#29992;&#25913;&#36827;&#30340;&#25289;&#20280;&#26426;&#21046;&#21644;&#21019;&#26032;&#30340;&#37325;&#26032;&#32553;&#25918;&#26041;&#26696;&#65292;&#22312;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#36866;&#24403;&#20998;&#37197;&#38544;&#31169;&#39044;&#31639;&#65292;&#25105;&#20204;&#21487;&#20197;&#25429;&#25417;&#21040;&#29992;&#25143;&#20869;&#30340;&#21516;&#36136;&#38544;&#31169;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional matrix factorization relies on centralized collection of users' data for recommendation, which might introduce an increased risk of privacy leakage especially when the recommender is untrusted. Existing differentially private matrix factorization methods either assume the recommender is trusted, or can only provide a uniform level of privacy protection for all users and items with untrusted recommender. In this paper, we propose a novel Heterogeneous Differentially Private Matrix Factorization algorithm (denoted as HDPMF) for untrusted recommender. To the best of our knowledge, we are the first to achieve heterogeneous differential privacy for decentralized matrix factorization in untrusted recommender scenario. Specifically, our framework uses modified stretching mechanism with an innovative rescaling scheme to achieve better trade off between privacy and accuracy. Meanwhile, by allocating privacy budget properly, we can capture homogeneous privacy preference within a use
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;(PAT)&#65292;&#23427;&#37319;&#29992;&#20102;&#26032;&#30340;&#22278;&#29615;&#26680;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16961</link><description>&lt;p&gt;
&#24102;&#26377;&#22278;&#29615;&#26680;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pattern Attention Transformer with Doughnut Kernel. (arXiv:2211.16961v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#24335;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;(PAT)&#65292;&#23427;&#37319;&#29992;&#20102;&#26032;&#30340;&#22278;&#29615;&#26680;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;&#20998;&#31867;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#21363;Pattern Attention Transformer&#65288;PAT&#65289;&#65292;&#35813;&#20307;&#31995;&#32467;&#26500;&#30001;&#26032;&#30340;&#22278;&#29615;&#26680;&#32452;&#25104;&#12290;&#19982;NLP&#39046;&#22495;&#30340;&#26631;&#35760;&#19981;&#21516;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;Transformer&#35299;&#20915;&#20102;&#22788;&#29702;&#22270;&#20687;&#20013;&#20687;&#32032;&#39640;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#12290;&#22312;ViT&#20013;&#65292;&#22270;&#20687;&#34987;&#20999;&#25104;&#26041;&#24418;&#30340;&#34917;&#19969;&#12290;&#20316;&#20026;ViT&#30340;&#21518;&#32493;&#65292;Swin Transformer&#25552;&#20986;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#31227;&#20301;&#27493;&#39588;&#20197;&#20943;&#23569;&#22266;&#23450;&#36793;&#30028;&#30340;&#23384;&#22312;&#65292;&#36825;&#20063;&#23548;&#33268;&#8220;&#20004;&#20010;&#36830;&#25509;&#30340;Swin Transformer&#22359;&#8221;&#25104;&#20026;&#27169;&#22411;&#30340;&#26368;&#23567;&#21333;&#20301;&#12290;&#32487;&#25215;&#20102;&#34917;&#19969;/&#31383;&#21475;&#30340;&#24819;&#27861;&#65292;&#25105;&#20204;&#30340;&#22278;&#29615;&#26680;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#34917;&#19969;&#30340;&#35774;&#35745;&#12290;&#23427;&#29992;&#20256;&#24863;&#22120;&#21644;&#26356;&#26032;&#20004;&#31181;&#21306;&#22495;&#20195;&#26367;&#20102;&#32447;&#22411;&#36793;&#30028;&#65292;&#36825;&#26159;&#22522;&#20110;&#33258;&#25105;&#20851;&#27880;&#30340;&#29702;&#35299;&#65288;&#31216;&#20026;QKVA&#32593;&#26684;&#65289;&#12290;&#22278;&#29615;&#26680;&#36824;&#24102;&#26469;&#20102;&#19968;&#20010;&#20851;&#20110;&#26680;&#24418;&#29366;&#30340;&#26032;&#35805;&#39064;&#65292;&#36229;&#36234;&#20102;&#26041;&#24418;&#12290;&#20026;&#20102;&#39564;&#35777;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#30340;&#24615;&#33021;&#65292;PAT&#34987;&#35774;&#35745;&#20026;&#30001;&#23450;&#26399;&#20843;&#36793;&#24418;&#24418;&#29366;&#30340;Transformer&#22359;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this paper a new architecture, the Pattern Attention Transformer (PAT), that is composed of the new doughnut kernel. Compared with tokens in the NLP field, Transformer in computer vision has the problem of handling the high resolution of pixels in images. In ViT, an image is cut into square-shaped patches. As the follow-up of ViT, Swin Transformer proposes an additional step of shifting to decrease the existence of fixed boundaries, which also incurs 'two connected Swin Transformer blocks' as the minimum unit of the model. Inheriting the patch/window idea, our doughnut kernel enhances the design of patches further. It replaces the line-cut boundaries with two types of areas: sensor and updating, which is based on the comprehension of self-attention (named QKVA grid). The doughnut kernel also brings a new topic about the shape of kernels beyond square. To verify its performance on image classification, PAT is designed with Transformer blocks of regular octagon shape doughn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#26356;&#20934;&#30830;&#22320;&#12289;&#20197;&#26356;&#39640;&#25928;&#29575;&#22320;&#35782;&#21035;&#20302;&#33021;&#21560;&#38468;&#29289;-&#34920;&#38754;&#37197;&#32622;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26435;&#34913;&#65292;&#20854;&#20013;&#19968;&#20010;&#24179;&#34913;&#36873;&#39033;&#22312;&#35745;&#31639;&#20013;&#21457;&#29616;&#26368;&#20302;&#33021;&#37327;&#32467;&#26500;&#30340;&#20934;&#30830;&#29575;&#20026;87.36&#65285;&#65292;&#21516;&#26102;&#35745;&#31639;&#36895;&#24230;&#25552;&#39640;&#20102;2000&#20493;&#12290;</title><link>http://arxiv.org/abs/2211.16486</link><description>&lt;p&gt;
AdsorbML&#65306;&#20351;&#29992;&#21487;&#25512;&#24191;&#30340;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#25552;&#39640;&#21560;&#38468;&#33021;&#35745;&#31639;&#25928;&#29575;&#30340;&#19968;&#22823;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
AdsorbML: A Leap in Efficiency for Adsorption Energy Calculations using Generalizable Machine Learning Potentials. (arXiv:2211.16486v3 [cond-mat.mtrl-sci] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#26356;&#20934;&#30830;&#22320;&#12289;&#20197;&#26356;&#39640;&#25928;&#29575;&#22320;&#35782;&#21035;&#20302;&#33021;&#21560;&#38468;&#29289;-&#34920;&#38754;&#37197;&#32622;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26435;&#34913;&#65292;&#20854;&#20013;&#19968;&#20010;&#24179;&#34913;&#36873;&#39033;&#22312;&#35745;&#31639;&#20013;&#21457;&#29616;&#26368;&#20302;&#33021;&#37327;&#32467;&#26500;&#30340;&#20934;&#30830;&#29575;&#20026;87.36&#65285;&#65292;&#21516;&#26102;&#35745;&#31639;&#36895;&#24230;&#25552;&#39640;&#20102;2000&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20652;&#21270;&#35745;&#31639;&#20013;&#65292;&#35745;&#31639;&#26041;&#27861;&#22312;&#35774;&#35745;&#21508;&#31181;&#24212;&#29992;&#30340;&#20652;&#21270;&#21058;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#35768;&#22810;&#35745;&#31639;&#26041;&#27861;&#30340;&#24120;&#35265;&#20219;&#21153;&#26159;&#20934;&#30830;&#35745;&#31639;&#24863;&#20852;&#36259;&#30340;&#21560;&#38468;&#29289;&#21644;&#20652;&#21270;&#21058;&#34920;&#38754;&#30340;&#21560;&#38468;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#35782;&#21035;&#20302;&#33021;&#21560;&#38468;&#29289;-&#34920;&#38754;&#26500;&#22411;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#30740;&#31350;&#20154;&#21592;&#30340;&#30452;&#35273;&#12290;&#38543;&#30528;&#39640;&#36890;&#37327;&#31579;&#36873;&#30340;&#38656;&#27714;&#22686;&#21152;&#65292;&#20165;&#38752;&#21551;&#21457;&#24335;&#21644;&#30452;&#35273;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#26426;&#22120;&#23398;&#20064;&#21183;&#20989;&#25968;&#21487;&#20197;&#26356;&#20934;&#30830;&#12289;&#39640;&#25928;&#22320;&#35782;&#21035;&#20302;&#33021;&#21560;&#38468;&#29289;-&#34920;&#38754;&#37197;&#32622;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#19968;&#31995;&#21015;&#26435;&#34913;&#65292;&#20854;&#20013;&#19968;&#20010;&#24179;&#34913;&#36873;&#39033;&#22312;&#35745;&#31639;&#20013;&#21457;&#29616;&#26368;&#20302;&#33021;&#37327;&#32467;&#26500;&#30340;&#20934;&#30830;&#29575;&#20026;87.36&#65285;&#65292;&#21516;&#26102;&#35745;&#31639;&#36895;&#24230;&#25552;&#39640;&#20102;2000&#20493;&#12290;&#20026;&#20102;&#26631;&#20934;&#21270;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#20652;&#21270;&#21058;&#23494;&#38598;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computational catalysis is playing an increasingly significant role in the design of catalysts across a wide range of applications. A common task for many computational methods is the need to accurately compute the adsorption energy for an adsorbate and a catalyst surface of interest. Traditionally, the identification of low energy adsorbate-surface configurations relies on heuristic methods and researcher intuition. As the desire to perform high-throughput screening increases, it becomes challenging to use heuristics and intuition alone. In this paper, we demonstrate machine learning potentials can be leveraged to identify low energy adsorbate-surface configurations more accurately and efficiently. Our algorithm provides a spectrum of trade-offs between accuracy and efficiency, with one balanced option finding the lowest energy configuration 87.36% of the time, while achieving a 2000x speedup in computation. To standardize benchmarking, we introduce the Open Catalyst Dense dataset con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2211.05793</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#20248;&#21270;&#21644;&#37327;&#23376;&#36866;&#29992;&#24615;&#30340;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#23427;&#23558;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#65292;&#36755;&#20986;&#29289;&#29702;&#29305;&#24615;&#65292;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#37327;&#23376;&#31995;&#32479;&#65292;&#32780;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#65292;&#20854;&#37327;&#23376;&#29305;&#24615;&#24102;&#26469;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36153;&#31859;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#65292;&#20854;&#29289;&#29702;&#29305;&#24615;&#65288;&#20363;&#22914;&#23616;&#37096;&#24577;&#23494;&#24230;&#25110;&#26465;&#20214;&#30005;&#23548;&#65289;&#22312;&#36755;&#20837;&#20316;&#20026;&#21021;&#22987;&#23618;&#21518;&#20316;&#20026;&#36755;&#20986;&#12290;&#19982;&#21453;&#21521;&#20256;&#25773;&#31867;&#20284;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#39640;&#25928;&#20248;&#21270;&#26041;&#27861;&#65292;&#20351;FNN&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#12290;FNN&#20063;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#23376;&#31995;&#32479;&#65292;&#21253;&#25324;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#30828;&#31995;&#32479;&#65292;&#24182;&#22312;&#26080;&#39044;&#22788;&#29702;&#25110;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#21407;&#20301;&#20998;&#26512;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21518;&#65292;FNN&#31934;&#30830;&#22320;&#30830;&#23450;&#25299;&#25169;&#30456;&#21644;&#32039;&#20945;&#30005;&#33655;&#24207;&#12290;&#23427;&#20204;&#30340;&#37327;&#23376;&#29305;&#24615;&#20063;&#24102;&#26469;&#20102;&#21508;&#31181;&#20248;&#21183;&#65306;&#37327;&#23376;&#30456;&#20851;&#24615;&#20351;&#32593;&#32476;&#36830;&#25509;&#26356;&#21152;&#36890;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#28040;&#22833;&#30340;&#26799;&#24230;&#38382;&#39064;&#65292;&#37327;&#23376;&#32416;&#32544;&#21017;&#20026;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#25171;&#24320;&#20102;&#26032;&#30340;&#36884;&#24452;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#27169;&#22411;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#34892;&#30340;&#32676;&#20307;&#20844;&#24179;&#24230;&#37327;&#26159;&#20851;&#20110;&#27169;&#22411;&#21442;&#25968;&#28857;&#20540;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#12290;&#38750;&#28176;&#36817;&#30028;&#38480;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#31169;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#36234;&#26469;&#36234;&#25509;&#36817;&#20110;&#38750;&#31169;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#65292;&#20063;&#31361;&#26174;&#20102;&#27169;&#22411;&#30340;&#32622;&#20449;&#36793;&#30028;&#23545;&#24046;&#20998;&#38544;&#31169;&#30340;&#19981;&#23545;&#31561;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.16242</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#23545;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#26377;&#38480;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy has Bounded Impact on Fairness in Classification. (arXiv:2210.16242v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#35777;&#26126;&#20102;&#22312;&#32473;&#23450;&#27169;&#22411;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#34892;&#30340;&#32676;&#20307;&#20844;&#24179;&#24230;&#37327;&#26159;&#20851;&#20110;&#27169;&#22411;&#21442;&#25968;&#28857;&#20540;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#12290;&#38750;&#28176;&#36817;&#30028;&#38480;&#34920;&#26126;&#65292;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#31169;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#36234;&#26469;&#36234;&#25509;&#36817;&#20110;&#38750;&#31169;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#65292;&#20063;&#31361;&#26174;&#20102;&#27169;&#22411;&#30340;&#32622;&#20449;&#36793;&#30028;&#23545;&#24046;&#20998;&#38544;&#31169;&#30340;&#19981;&#23545;&#31561;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#23545;&#20998;&#31867;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#32473;&#23450;&#27169;&#22411;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#27969;&#34892;&#30340;&#32676;&#20307;&#20844;&#24179;&#24230;&#37327;&#26159;&#20851;&#20110;&#27169;&#22411;&#21442;&#25968;&#28857;&#20540;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#30340;&#12290;&#36825;&#20010;&#32467;&#26524;&#26159;&#19968;&#20010;&#26356;&#19968;&#33324;&#30340;&#20851;&#20110;&#22312;&#20219;&#24847;&#20107;&#20214;&#26465;&#20214;&#19979;&#65288;&#27604;&#22914;&#23646;&#20110;&#25935;&#24863;&#32676;&#20307;&#65289;&#20934;&#30830;&#24615;&#30340;&#22768;&#26126;&#30340;&#32467;&#26524;&#65292;&#21487;&#33021;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#21033;&#26222;&#24076;&#33576;&#24615;&#36136;&#35777;&#26126;&#20102;&#19968;&#20010;&#38750;&#28176;&#36817;&#30028;&#38480;&#65292;&#23427;&#34920;&#26126;&#38543;&#30528;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#31169;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#36234;&#26469;&#36234;&#25509;&#36817;&#20110;&#38750;&#31169;&#26377;&#27169;&#22411;&#30340;&#20844;&#24179;&#24230;&#12290;&#36825;&#20010;&#30028;&#38480;&#36824;&#31361;&#26174;&#20102;&#27169;&#22411;&#30340;&#32622;&#20449;&#36793;&#30028;&#23545;&#24046;&#20998;&#38544;&#31169;&#30340;&#19981;&#23545;&#31561;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We theoretically study the impact of differential privacy on fairness in classification. We prove that, given a class of models, popular group fairness measures are pointwise Lipschitz-continuous with respect to the parameters of the model. This result is a consequence of a more general statement on accuracy conditioned on an arbitrary event (such as membership to a sensitive group), which may be of independent interest. We use this Lipschitz property to prove a non-asymptotic bound showing that, as the number of samples increases, the fairness level of private models gets closer to the one of their non-private counterparts. This bound also highlights the importance of the confidence margin of a model on the disparate impact of differential privacy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2210.12714</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#26041;&#27861;&#20998;&#31867;&#21644;&#20248;&#21155;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#65288;KGC&#65289;&#26159;&#25351;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#26694;&#26550;&#26500;&#24314;&#28789;&#27963;&#19988;&#21487;&#36866;&#29992;&#20110;&#24191;&#27867;&#20219;&#21153;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#39046;&#22495;&#20013;&#36817;&#26399;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#23545;&#19981;&#21516;&#30340;&#29983;&#25104;&#30446;&#26631;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#35282;&#24230;&#20998;&#21035;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#26377;&#28508;&#21147;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#30340;&#35814;&#32454;&#12289;&#23436;&#25972;&#30340;&#20998;&#31867;&#20307;&#31995;&#65307;&#65288;2&#65289;&#25105;&#20204;&#23545;&#29983;&#25104;&#24335;KGC&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65307;&#65288;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#21487;&#20197;&#21457;&#23637;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;</title><link>http://arxiv.org/abs/2210.10678</link><description>&lt;p&gt;
&#23454;&#29616;&#30495;&#23454;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;: &#38024;&#23545;&#20855;&#26377;&#23454;&#35777;&#22522;&#20934;&#30740;&#31350;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#21253;&#25324;&#20351;&#29992;&#25552;&#31034;&#26041;&#27861;&#12289;&#24179;&#34913;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;&#36890;&#36807;&#23545;8&#20010;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#27604;&#36739;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#26377;&#30410;&#20110;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#26159;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#38024;&#23545;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26500;&#24314;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#22522;&#20110;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19977;&#31181;&#26041;&#26696;&#26469;&#35780;&#20272;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#24615;&#33021;&#65306;(i) &#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#65307; (ii) &#22810;&#26679;&#21270;&#30340;&#24179;&#34913;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#38382;&#39064;&#65307; (iii) &#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;&#33258;&#35757;&#32451;&#26469;&#29983;&#25104;&#26356;&#22810;&#39046;&#22495;&#20869;&#26631;&#35760;&#25968;&#25454;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;8&#20010;&#20851;&#31995;&#25277;&#21462;(RE) &#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#39046;&#22495;&#21644;&#19978;&#19979;&#25991;&#65292;&#24182;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#26696;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65306;(i) &#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#35843;&#25972;&#22312;&#20302;&#36164;&#28304;&#20851;&#31995;&#25277;&#21462;&#20013;&#26159;&#26377;&#30410;&#30340;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#21462;&#36328;&#21477;&#23376;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#19977;&#20803;&#32452;&#26041;&#38754;&#65307; (ii) &#24179;&#34913;&#26041;&#27861;&#24182;&#19981;&#24635;&#26159;&#26377;&#21161;&#20110;&#38271;&#23614;&#20998;&#24067;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#20934;&#30830;&#22320;&#25512;&#23548;&#20986;&#26448;&#26009;&#30340;&#24102;&#32467;&#26500;&#21442;&#25968;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#23618;&#30707;&#22696;&#28911;&#30340;&#31359;&#36879;&#22330;&#30005;&#23481;&#27979;&#37327;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06310</link><description>&lt;p&gt;
&#20174;&#24577;&#23494;&#24230;&#20013;&#25552;&#21462;&#24102;&#32467;&#26500;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65306;&#20197;&#19977;&#23618;&#30707;&#22696;&#28911;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep learning extraction of band structure parameters from density of states: a case study on trilayer graphene. (arXiv:2210.06310v2 [cond-mat.mes-hall] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#20934;&#30830;&#22320;&#25512;&#23548;&#20986;&#26448;&#26009;&#30340;&#24102;&#32467;&#26500;&#21442;&#25968;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#23618;&#30707;&#22696;&#28911;&#30340;&#31359;&#36879;&#22330;&#30005;&#23481;&#27979;&#37327;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#32500;&#26448;&#26009;&#30340;&#21457;&#23637;&#23548;&#33268;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#12289;&#39640;&#36136;&#37327;&#30340;&#21270;&#21512;&#29289;&#65292;&#20854;&#22797;&#26434;&#24615;&#36880;&#28176;&#22686;&#21152;&#12290;&#19968;&#20010;&#20840;&#38754;&#30340;&#23450;&#37327;&#29702;&#35770;&#30340;&#20851;&#38190;&#35201;&#27714;&#26159;&#20934;&#30830;&#30830;&#23450;&#36825;&#20123;&#26448;&#26009;&#30340;&#24102;&#32467;&#26500;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#24102;&#32467;&#26500;&#21644;&#23454;&#39564;&#25506;&#27979;&#26041;&#24335;&#30340;&#38388;&#25509;&#24615;&#36136;&#65292;&#36825;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#25512;&#23548;&#24102;&#32467;&#26500;&#21442;&#25968;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#19977;&#23618;&#30707;&#22696;&#28911;&#30340;&#31359;&#36879;&#22330;&#30005;&#23481;&#27979;&#37327;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#25928;&#25506;&#27979;&#20854;&#23494;&#24230;&#24577;&#30340;&#26041;&#24335;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#28145;&#24230;&#32593;&#32476;&#22312;&#32039;&#26463;&#32538;&#21442;&#25968;&#20989;&#25968;&#20851;&#31995;&#30340;&#31359;&#36879;&#22330;&#30005;&#23481;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#20934;&#30830;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21033;&#29992;&#35757;&#32451;&#32593;&#32476;&#30340;&#24555;&#36895;&#12289;&#20934;&#30830;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#30452;&#25509;&#20174;&#23454;&#39564;&#25968;&#25454;&#20013;&#33258;&#21160;&#30830;&#23450;&#32039;&#26463;&#32538;&#21442;&#25968;&#65292;&#25552;&#21462;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of two-dimensional materials has resulted in a diverse range of novel, high-quality compounds with increasing complexity. A key requirement for a comprehensive quantitative theory is the accurate determination of these materials' band structure parameters. However, this task is challenging due to the intricate band structures and the indirect nature of experimental probes. In this work, we introduce a general framework to derive band structure parameters from experimental data using deep neural networks. We applied our method to the penetration field capacitance measurement of trilayer graphene, an effective probe of its density of states. First, we demonstrate that a trained deep network gives accurate predictions for the penetration field capacitance as a function of tight-binding parameters. Next, we use the fast and accurate predictions from the trained network to automatically determine tight-binding parameters directly from experimental data, with extracted parame
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;CMA-MAE&#21464;&#20307;&#65292;&#21033;&#29992;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#25552;&#39640;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#65292;&#30456;&#36739;&#20110;ES&#22522;&#32447;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#36798;&#21040;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.02622</link><description>&lt;p&gt;
&#36890;&#36807;&#32553;&#25918;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#27169;&#25311;&#26469;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#39640;&#32500;&#24230;&#25511;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing. (arXiv:2210.02622v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;CMA-MAE&#21464;&#20307;&#65292;&#21033;&#29992;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#25552;&#39640;&#20102;&#20854;&#21487;&#25193;&#23637;&#24615;&#65292;&#30456;&#36739;&#20110;ES&#22522;&#32447;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#36798;&#21040;&#25110;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#25311;&#20013;&#39044;&#20808;&#35757;&#32451;&#22810;&#26679;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#36866;&#24212;&#36816;&#21160;&#20219;&#21153;&#20013;&#30340;&#25439;&#20260;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#22810;&#26679;&#24615;&#39640;&#19988;&#24615;&#33021;&#20248;&#24322;&#30340;&#25511;&#21046;&#22120;&#38656;&#35201;&#26114;&#36149;&#30340;&#32593;&#32476;&#35757;&#32451;&#21644;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#30456;&#27604;&#32780;&#35328;&#65292;&#22522;&#20110;&#36827;&#21270;&#31574;&#30053;&#65288;ES&#65289;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#8220;&#21327;&#26041;&#24046;&#30697;&#38453;&#33258;&#36866;&#24212;MAP&#27169;&#25311;&#8221;&#27809;&#26377;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#22312;&#26631;&#20934;&#30340;QD&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;CMA-MAE&#26080;&#27861;&#25193;&#23637;&#21040;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#21033;&#29992;ES&#20013;&#30340;&#39640;&#25928;&#36817;&#20284;&#26041;&#27861;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;CMA-MAE&#21464;&#20307;&#65292;&#36825;&#20123;&#21464;&#20307;&#33021;&#22815;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#21464;&#20307;&#22312;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;ES&#22522;&#32447;&#65292;&#21516;&#26102;&#21487;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36136;&#37327;&#22810;&#26679;&#24615;&#31639;&#27861;&#23218;&#32654;&#25110;&#36229;&#36234;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training a diverse set of neural network controllers in simulation has enabled robots to adapt online to damage in robot locomotion tasks. However, finding diverse, high-performing controllers requires expensive network training and extensive tuning of a large number of hyperparameters. On the other hand, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), an evolution strategies (ES)-based quality diversity algorithm, does not have these limitations and has achieved state-of-the-art performance on standard QD benchmarks. However, CMA-MAE cannot scale to modern neural network controllers due to its quadratic complexity. We leverage efficient approximation methods in ES to propose three new CMA-MAE variants that scale to high dimensions. Our experiments show that the variants outperform ES-based baselines in benchmark robotic locomotion tasks, while being comparable with or exceeding state-of-the-art deep reinforcement learning-based quality diversity algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#35299;&#20915;&#23398;&#20064;&#20219;&#21153;&#26102;&#33258;&#21160;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#22686;&#24378;&#34920;&#36848;&#20026;&#22522;&#20110;&#19981;&#21464;&#24615;&#32422;&#26463;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#35299;&#20915;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20248;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2209.15031</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21464;&#24615;&#32422;&#26463;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Automatic Data Augmentation via Invariance-Constrained Learning. (arXiv:2209.15031v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#35299;&#20915;&#23398;&#20064;&#20219;&#21153;&#26102;&#33258;&#21160;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#22686;&#24378;&#34920;&#36848;&#20026;&#22522;&#20110;&#19981;&#21464;&#24615;&#32422;&#26463;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#37319;&#26679;&#26469;&#35299;&#20915;&#65292;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20248;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#23545;&#31216;&#24615;&#25110;&#21464;&#25442;&#19981;&#21464;&#24615;&#65289;&#65292;&#32463;&#24120;&#34987;&#21033;&#29992;&#26469;&#25913;&#21892;&#23398;&#20064;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#23646;&#24615;&#23884;&#20837;&#21040;&#27169;&#22411;&#25110;&#23398;&#20064;&#31639;&#27861;&#20013;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#35745;&#31639;&#23494;&#38598;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25968;&#25454;&#22686;&#24378;&#36890;&#36807;&#23545;&#36755;&#20837;&#25968;&#25454;&#24212;&#29992;&#22810;&#20010;&#21464;&#25442;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24341;&#20837;&#36825;&#20123;&#23545;&#31216;&#24615;&#12290;&#23613;&#31649;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#24212;&#29992;&#21738;&#20123;&#21464;&#25442;&#65292;&#20309;&#26102;&#24212;&#29992;&#20197;&#21450;&#22810;&#20037;&#24212;&#29992;&#30340;&#36873;&#25321;&#12290;&#20107;&#23454;&#19978;&#65292;&#32463;&#39564;&#21644;&#29702;&#35770;&#35777;&#25454;&#37117;&#34920;&#26126;&#65292;&#19981;&#21152;&#36873;&#25321;&#22320;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#24341;&#20837;&#20559;&#35265;&#65292;&#36229;&#36807;&#20854;&#22909;&#22788;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#35299;&#20915;&#23398;&#20064;&#20219;&#21153;&#26102;&#33258;&#21160;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#23427;&#23558;&#25968;&#25454;&#22686;&#24378;&#34920;&#36848;&#20026;&#19968;&#20010;&#22522;&#20110;&#19981;&#21464;&#24615;&#32422;&#26463;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#37319;&#26679;&#26469;&#35299;&#20915;&#23427;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#23454;&#29992;&#31639;&#27861;&#65292;&#19981;&#20165;&#33021;&#22815;&#33258;&#21160;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65292;&#32780;&#19988;&#33021;&#22815;&#20248;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Underlying data structures, such as symmetries or invariances to transformations, are often exploited to improve the solution of learning tasks. However, embedding these properties in models or learning algorithms can be challenging and computationally intensive. Data augmentation, on the other hand, induces these symmetries during training by applying multiple transformations to the input data. Despite its ubiquity, its effectiveness depends on the choices of which transformations to apply, when to do so, and how often. In fact, there is both empirical and theoretical evidence that the indiscriminate use of data augmentation can introduce biases that outweigh its benefits. This work tackles these issues by automatically adapting the data augmentation while solving the learning task. To do so, it formulates data augmentation as an invariance-constrained learning problem and leverages Monte Carlo Markov Chain (MCMC) sampling to solve it. The result is a practical algorithm that not only
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#30636;&#21464;&#20809;&#26354;&#32447;&#36924;&#36817;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#31181;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#26102;&#38388;&#27493;&#38271;&#35268;&#21017;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2209.07542</link><description>&lt;p&gt;
&#23545;&#20110;&#30636;&#21464;&#20809;&#26354;&#32447;&#36924;&#36817;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#24615;&#36136;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding of the properties of neural network approaches for transient light curve approximations. (arXiv:2209.07542v2 [astro-ph.IM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07542
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#30636;&#21464;&#20809;&#26354;&#32447;&#36924;&#36817;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#31181;&#36924;&#36817;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#26102;&#38388;&#27493;&#38271;&#35268;&#21017;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26102;&#22495;&#20809;&#24230;&#27979;&#37327;&#35843;&#26597;&#25910;&#38598;&#20102;&#22823;&#37327;&#21508;&#31181;&#22825;&#20307;&#29289;&#20307;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#22823;&#35268;&#27169;&#35843;&#26597;&#21363;&#23558;&#21040;&#26469;&#65292;&#23558;&#20250;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#23427;&#20204;&#24615;&#36136;&#30340;&#20449;&#24687;&#12290;&#23545;&#20110;&#36229;&#26032;&#26143;&#31561;&#30636;&#21464;&#22825;&#20307;&#26469;&#35828;&#65292;&#20809;&#35889;&#36861;&#36394;&#26159;&#29305;&#21035;&#20851;&#38190;&#30340;&#65292;&#20294;&#22823;&#37096;&#20998;&#36825;&#20123;&#22825;&#20307;&#36824;&#26410;&#32463;&#36807;&#36825;&#26679;&#30340;&#30740;&#31350;&#12290;} {&#27969;&#37327;&#26102;&#38388;&#24207;&#21015;&#34987;&#31215;&#26497;&#29992;&#20316;&#20809;&#24230;&#20998;&#31867;&#21644;&#34920;&#24449;&#30340;&#24265;&#20215;&#26367;&#20195;&#26041;&#26696;&#65292;&#20363;&#22914;&#23792;&#20540;&#26631;&#35782;&#21644;&#20142;&#24230;&#34928;&#20943;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#26159;&#22810;&#32500;&#30340;&#19988;&#37319;&#26679;&#19981;&#35268;&#21017;&#65292;&#24182;&#19988;&#21253;&#21547;&#24322;&#24120;&#20540;&#65292;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#31995;&#32479;&#24615;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#20110;&#36924;&#36817;&#32463;&#36807;&#26102;&#38388;&#21644;&#27874;&#38271;&#35266;&#27979;&#30340;&#20809;&#26354;&#32447;&#30340;&#26368;&#20339;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#30446;&#30340;&#26159;&#29983;&#25104;&#27599;&#20010;&#36890;&#36947;&#19979;&#20855;&#26377;&#35268;&#21017;&#26102;&#38388;&#27493;&#38271;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern-day time-domain photometric surveys collect a lot of observations of various astronomical objects and the coming era of large-scale surveys will provide even more information on their properties. Spectroscopic follow-ups are especially crucial for transients such as supernovae and most of these objects have not been subject to such studies. }{Flux time series are actively used as an affordable alternative for photometric classification and characterization, for instance, peak identifications and luminosity decline estimations. However, the collected time series are multidimensional and irregularly sampled, while also containing outliers and without any well-defined systematic uncertainties. This paper presents a search for the best-performing methods to approximate the observed light curves over time and wavelength for the purpose of generating time series with regular time steps in each passband.}{We examined several light curve approximation methods based on neural networks su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.06261</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#32034;&#39537;&#21160;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#25511;&#21046;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#36716;&#31227;&#30340;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#35813;&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30001;&#22362;&#30828;&#30340;&#26438;&#21644;&#26580;&#36719;&#30340;&#32518;&#32499;&#32452;&#25104;&#65292;&#20855;&#26377;&#39640;&#24378;&#24230;&#37325;&#37327;&#27604;&#21644;&#26174;&#33879;&#30340;&#21464;&#24418;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#38750;&#32467;&#26500;&#21270;&#30340;&#22320;&#24418;&#20013;&#33322;&#34892;&#24182;&#22312;&#20005;&#23803;&#30340;&#25758;&#20987;&#20013;&#23384;&#27963;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32500;&#24230;&#39640;&#12289;&#21160;&#21147;&#22797;&#26434;&#19988;&#32806;&#21512;&#32467;&#26500;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25511;&#21046;&#12290;&#22522;&#20110;&#29289;&#29702;&#30340;&#20223;&#30495;&#26159;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#31574;&#30053;&#30340;&#26377;&#21069;&#36884;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#23545;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#36827;&#34892;&#24314;&#27169;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21516;iable&#29289;&#29702;&#24341;&#25806;&#30340;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#30340;&#30495;&#23454;&#19990;&#30028;&#21040;&#20223;&#30495;&#19990;&#30028;&#30340;&#36716;&#31227;&#31574;&#30053;(R2S2R)&#12290;&#35813;&#31574;&#30053;&#22522;&#20110;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#65292;&#36890;&#36807;&#23545;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#21253;&#25324;&#29289;&#29702;&#23646;&#24615;&#30340;&#31163;&#32447;&#27979;&#37327;&#65292;&#22914;&#36136;&#37327;&#21644;&#20960;&#20309;&#20307;&#30340;&#21508;&#31181;&#26426;&#22120;&#20154;&#37096;&#20214;&#65292;&#20197;&#21450;&#20351;&#29992;&#38543;&#26426;&#25511;&#21046;&#31574;&#30053;&#30340;&#36712;&#36857;&#35266;&#23519;&#12290;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#65292;&#29289;&#29702;&#24341;&#25806;&#21487;&#20197;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#65292;&#20197;&#20943;&#23569;&#23454;&#21040;&#34394;&#20043;&#38388;&#30340;&#24046;&#36317;&#24182;&#20135;&#29983;&#20934;&#30830;&#30340;&#20223;&#30495;&#12290;&#36825;&#31181;R2S2R&#31574;&#30053;&#22312;&#32034;&#39537;&#21160;&#24352;&#21147;&#32467;&#26500;&#26426;&#22120;&#20154;&#19978;&#24471;&#21040;&#20102;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#21487;&#24494;&#29289;&#29702;&#24341;&#25806;&#24320;&#21457;&#21487;&#20197;&#36716;&#31227;&#21040;&#23454;&#38469;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
&lt;/p&gt;</description></item><item><title>TabPFN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#20869;&#23436;&#25104;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20998;&#31867;&#30340;Transformer&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#23427;&#20351;&#29992;&#20808;&#39564;&#36866;&#24212;&#32593;&#32476;&#65288;PFN&#65289;&#36924;&#36817;&#22522;&#20110;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20808;&#39564;&#34701;&#21512;&#20102;&#22240;&#26524;&#25512;&#29702;&#30340;&#24605;&#24819;&#12290;</title><link>http://arxiv.org/abs/2207.01848</link><description>&lt;p&gt;
TabPFN&#65306;&#22312;&#19968;&#31186;&#20869;&#35299;&#20915;&#23567;&#22411;&#34920;&#26684;&#20998;&#31867;&#38382;&#39064;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. (arXiv:2207.01848v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01848
&lt;/p&gt;
&lt;p&gt;
TabPFN&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#20869;&#23436;&#25104;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20998;&#31867;&#30340;Transformer&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#23427;&#20351;&#29992;&#20808;&#39564;&#36866;&#24212;&#32593;&#32476;&#65288;PFN&#65289;&#36924;&#36817;&#22522;&#20110;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#20808;&#39564;&#34701;&#21512;&#20102;&#22240;&#26524;&#25512;&#29702;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TabPFN&#65292;&#19968;&#31181;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#65292;&#21487;&#20197;&#22312;&#19981;&#21040;&#19968;&#31186;&#38047;&#30340;&#26102;&#38388;&#20869;&#23436;&#25104;&#23567;&#22411;&#34920;&#26684;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#20998;&#31867;&#65292;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#26041;&#27861;&#30340;&#26368;&#26032;&#29366;&#24577;&#19979;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;TabPFN&#23436;&#20840;&#21253;&#21547;&#22312;&#25105;&#20204;&#32593;&#32476;&#30340;&#26435;&#37325;&#20013;&#65292;&#25509;&#21463;&#35757;&#32451;&#21644;&#27979;&#35797;&#26679;&#26412;&#20316;&#20026;&#35774;&#32622;&#20540;&#36755;&#20837;&#65292;&#24182;&#22312;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#20013;&#20026;&#25972;&#20010;&#27979;&#35797;&#38598;&#25552;&#20379;&#39044;&#27979;&#12290;TabPFN&#26159;&#19968;&#31181;&#20808;&#39564;&#36866;&#24212;&#32593;&#32476;&#65288;PFN&#65289;&#65292;&#21482;&#38656;&#35201;&#32447;&#19979;&#35757;&#32451;&#19968;&#27425;&#65292;&#21363;&#21487;&#36924;&#36817;&#22522;&#20110;&#25105;&#20204;&#30340;&#20808;&#39564;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#36825;&#20010;&#20808;&#39564;&#34701;&#21512;&#20102;&#22240;&#26524;&#25512;&#29702;&#30340;&#24605;&#24819;&#65306;&#23427;&#21253;&#25324;&#19968;&#20010;&#22823;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#31354;&#38388;&#65292;&#20559;&#22909;&#20110;&#31616;&#21333;&#32467;&#26500;&#12290;&#22312;OpenML-CC18&#22871;&#20214;&#30340;18&#20010;&#21253;&#21547;&#26368;&#22810;1000&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#12289;&#26368;&#22810;100&#20010;&#32431;&#25968;&#20540;&#29305;&#24449;&#19988;&#26080;&#32570;&#22833;&#20540;&#12289;&#26368;&#22810;10&#20010;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#25552;&#21319;&#26641;&#65292;&#19982;&#22797;&#26434;&#30340;&#26368;&#26032;AutoM&#26041;&#27861;&#34920;&#29616;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoM
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#26041;&#21644;&#26494;&#24347;&#26041;&#27861;&#22312;&#20449;&#24687;&#35770;&#21644;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#31639;$f$-divergences&#30340;&#20984;&#26494;&#24347;&#31639;&#27861;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#20174;&#38750;&#23616;&#37096;&#21327;&#26041;&#24046;&#30697;&#38453;&#35745;&#31639;&#36825;&#20123;divergences&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#22810;&#20010;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2206.13285</link><description>&lt;p&gt;
&#20449;&#24687;&#35770;&#21644;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#22522;&#20110;&#24179;&#26041;&#21644;&#26494;&#24347;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Squares Relaxations for Information Theory and Variational Inference. (arXiv:2206.13285v3 [cs.IT] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.13285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24179;&#26041;&#21644;&#26494;&#24347;&#26041;&#27861;&#22312;&#20449;&#24687;&#35770;&#21644;&#21464;&#20998;&#25512;&#26029;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#31639;$f$-divergences&#30340;&#20984;&#26494;&#24347;&#31639;&#27861;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#20174;&#38750;&#23616;&#37096;&#21327;&#26041;&#24046;&#30697;&#38453;&#35745;&#31639;&#36825;&#20123;divergences&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#23545;&#20110;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#22810;&#20010;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#39321;&#20892;&#30456;&#23545;&#29109;&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;$f$-divergences&#12290;&#36825;&#20123;divergences&#36890;&#24120;&#19982;&#19977;&#20010;&#32463;&#20856;&#30340;&#30456;&#20851;&#35745;&#31639;&#38382;&#39064;&#30456;&#20851;&#32852;&#65306;&#65288;a&#65289;&#20174;&#30697;&#20272;&#35745;&#65292;&#65288;b&#65289;&#35745;&#31639;&#24402;&#19968;&#21270;&#31215;&#20998;&#65292;&#20197;&#21450;&#65288;c&#65289;&#27010;&#29575;&#27169;&#22411;&#30340;&#21464;&#20998;&#25512;&#26029;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#36807;&#20984;&#23545;&#20598;&#24615;&#30456;&#20114;&#20851;&#32852;&#65292;&#23545;&#20110;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#65292;&#37117;&#26377;&#35768;&#22810;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#33021;&#22815;&#20445;&#25345;&#21407;&#22987;&#38382;&#39064;&#29305;&#24615;&#65288;&#22914;&#28508;&#22312;&#20984;&#24615;&#25110;&#21333;&#35843;&#24615;&#65289;&#30340;&#35745;&#31639;&#19978;&#21487;&#34892;&#30340;&#36817;&#20284;&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20174;&#19982;&#32473;&#23450;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#30340;&#38750;&#23616;&#37096;&#21327;&#26041;&#24046;&#30697;&#38453;&#35745;&#31639;&#36825;&#20123;divergences&#30340;&#19968;&#31995;&#21015;&#20984;&#26494;&#24347;&#24320;&#22987;&#65306;&#20174;&#36890;&#24120;&#19981;&#26131;&#22788;&#29702;&#30340;&#26368;&#20248;&#19979;&#30028;&#24320;&#22987;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#22522;&#20110;&#8220;&#24179;&#26041;&#21644;&#8221;&#30340;&#26494;&#24347;&#65292;&#23427;&#29616;&#22312;&#20316;&#20026;&#21322;&#23450;&#35268;&#21010;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider extensions of the Shannon relative entropy, referred to as $f$-divergences.Three classical related computational problems are typically associated with these divergences: (a) estimation from moments, (b) computing normalizing integrals, and (c) variational inference in probabilistic models. These problems are related to one another through convex duality, and for all them, there are many applications throughout data science, and we aim for computationally tractable approximation algorithms that preserve properties of the original problem such as potential convexity or monotonicity. In order to achieve this, we derive a sequence of convex relaxations for computing these divergences from non-centered covariance matrices associated with a given feature vector: starting from the typically non-tractable optimal lower-bound, we consider an additional relaxation based on ``sums-of-squares'', which is is now computable in polynomial time as a semidefinite program. We also provide c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2206.10397</link><description>&lt;p&gt;
&#40065;&#26834;&#39134;&#34892;&#25511;&#21046;&#30340;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#36890;&#36807;&#25512;&#23548;&#20986;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23558;&#35813;&#20272;&#35745;&#22120;&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#35745;&#21644;&#24212;&#23545;&#24178;&#25200;&#23545;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#25511;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#36890;&#24120;&#38656;&#35201;&#23545;&#29305;&#23450;&#39134;&#34892;&#22330;&#26223;&#36827;&#34892;&#22823;&#37327;&#35843;&#25972;&#65292;&#25110;&#32773;&#32463;&#36807;&#24191;&#27867;&#30340;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#35757;&#32451;&#65292;&#25165;&#33021;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#31227;&#21160;&#35270;&#30028;&#20272;&#35745;&#22120;&#65288;NeuroMHE&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#35843;&#25972;&#30001;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20851;&#38190;&#21442;&#25968;&#65292;&#24182;&#36866;&#24212;&#19981;&#21516;&#30340;&#39134;&#34892;&#22330;&#26223;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#19982;&#21152;&#26435;&#30697;&#38453;&#30456;&#20851;&#30340;MHE&#20272;&#35745;&#30340;&#35299;&#26512;&#26799;&#24230;&#65292;&#23454;&#29616;&#20102;&#23558;MHE&#20316;&#20026;&#21487;&#23398;&#20064;&#23618;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20197;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#30340;&#26080;&#32541;&#34701;&#21512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#36882;&#24402;&#24418;&#24335;&#30340;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#39640;&#25928;&#22320;&#35745;&#31639;&#20986;&#26799;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#20197;&#20174;&#22235;&#26059;&#32764;&#36712;&#36857;&#36319;&#36394;&#35823;&#24046;&#20013;&#30452;&#25509;&#35757;&#32451;NeuroMHE&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#24178;&#25200;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#29992;&#20110;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20102;&#23450;&#20041;OOD&#27010;&#24565;&#21644;&#25552;&#20379;&#24378;&#26377;&#21147;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#19982;&#20043;&#21069;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#27979;&#35797;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;OOD&#23454;&#20363;&#20013;&#34920;&#29616;&#26356;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2206.09522</link><description>&lt;p&gt;
&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#29992;&#20110;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multiple Testing Framework for Out-of-Distribution Detection. (arXiv:2206.09522v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#37325;&#26816;&#39564;&#26694;&#26550;&#29992;&#20110;&#31163;&#32676;&#20998;&#24067;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#20102;&#23450;&#20041;OOD&#27010;&#24565;&#21644;&#25552;&#20379;&#24378;&#26377;&#21147;&#20445;&#35777;&#30340;&#26041;&#27861;&#65292;&#19982;&#20043;&#21069;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#27979;&#35797;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;OOD&#23454;&#20363;&#20013;&#34920;&#29616;&#26356;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#31163;&#32676;&#20998;&#24067;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#25512;&#29702;&#26102;&#26816;&#27979;&#23398;&#20064;&#31639;&#27861;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#20449;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#20123;OOD&#26816;&#27979;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#24418;&#24335;&#21270;&#30340;&#26694;&#26550;&#26469;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;OOD&#27010;&#24565;&#30340;&#23450;&#20041;&#65292;&#21253;&#25324;&#36755;&#20837;&#20998;&#24067;&#21644;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#20026;&#26500;&#24314;&#24378;&#22823;&#30340;OOD&#26816;&#27979;&#27979;&#35797;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#21551;&#21457;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#31526;&#21512;&#24615;p&#20540;&#31995;&#32479;&#22320;&#32467;&#21512;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#19981;&#21516;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#23558;&#20837;&#32676;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#20026;OOD&#30340;&#27010;&#29575;&#25552;&#20379;&#20102;&#24378;&#26377;&#21147;&#30340;&#20445;&#35777;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#24037;&#20316;&#20013;&#25552;&#20986;&#30340;&#22522;&#20110;&#38408;&#20540;&#30340;&#27979;&#35797;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;OOD&#23454;&#20363;&#20013;&#30340;&#34920;&#29616;&#24182;&#19981;&#19968;&#33268;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;m&#20010;&#19981;&#21516;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of Out-of-Distribution (OOD) detection, that is, detecting whether a learning algorithm's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the learning algorithm, which provides insights for the construction of powerful tests for OOD detection. We propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the learning algorithm using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different types of OOD instances. In contrast, our proposed method that combines m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#32771;&#23519;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;GRAPHRETRIEVAL&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#26696;&#65292;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#26377;&#29992;&#20449;&#24687;&#25552;&#20379;&#20102;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2206.00362</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Retrieval-enhanced Graph Neural Networks. (arXiv:2206.00362v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00362
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#23519;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#25968;&#25454;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#31216;&#20026;GRAPHRETRIEVAL&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#26696;&#65292;&#20026;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#26377;&#29992;&#20449;&#24687;&#25552;&#20379;&#20102;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#22823;&#22810;&#25968;GNNs&#20381;&#36182;&#20110;&#36882;&#24402;&#30340;&#37051;&#23621;&#32858;&#21512;&#26041;&#26696;&#65292;&#31216;&#20026;&#28040;&#24687;&#20256;&#36882;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#38480;&#20110;&#19968;&#38454;Weisfeiler-Lehman&#27979;&#35797;&#65288;1-WL&#65289;&#12290;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26159;&#26126;&#30830;&#22320;&#26816;&#32034;&#29992;&#20110;&#22686;&#24378;GNN&#27169;&#22411;&#30340;&#19968;&#20123;&#24050;&#27880;&#37322;&#30340;&#31034;&#20363;&#12290;&#34429;&#28982;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#24403;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#38598;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;GNNs&#30340;&#26377;&#25928;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20986;&#20110;&#36825;&#20010;&#30446;&#30340;&#65292;&#25105;&#20204;&#24819;&#25506;&#32034;&#26816;&#32034;&#24605;&#24819;&#22914;&#20309;&#24110;&#21161;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#26377;&#29992;&#20449;&#24687;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31216;&#20026;GRAPHRETRIEVAL&#30340;&#26816;&#32034;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#23545;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#36873;&#25321;&#26159;&#19981;&#21487;&#30693;&#30340;&#12290;&#22312;GRAPHRETRIEVAL&#20013;&#65292;&#23545;&#20110;&#27599;&#20010;&#36755;&#20837;&#22270;&#65292;&#20174;&#29616;&#26377;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#20986;&#30456;&#20284;&#30340;&#22270;&#20197;&#21450;&#23427;&#20204;&#30340;&#30495;&#23454;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neighborhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first-order Weisfeiler-Lehman test (1-WL). An effective approach to this challenge is to explicitly retrieve some annotated examples used to enhance GNN models. While retrieval-enhanced models have been proved to be effective in many language and vision domains, it remains an open question how effective retrieval-enhanced GNNs are when applied to graph datasets. Motivated by this, we want to explore how the retrieval idea can help augment the useful information learned in the graph neural networks, and we design a retrieval-enhanced scheme called GRAPHRETRIEVAL, which is agnostic to the choice of graph neural network models. In GRAPHRETRIEVAL, for each input graph, similar graphs together with their ground-true labels are retrieved from an existing database. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DeepCluE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#26725;&#25509;&#20102;&#28145;&#24230;&#32858;&#31867;&#21644;&#38598;&#25104;&#32858;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22270;&#20687;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.00359</link><description>&lt;p&gt;
DeepCluE&#65306;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#23618;&#38598;&#25104;&#22686;&#24378;&#22270;&#20687;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep Neural Networks. (arXiv:2206.00359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DeepCluE&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#26725;&#25509;&#20102;&#28145;&#24230;&#32858;&#31867;&#21644;&#38598;&#25104;&#32858;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#22270;&#20687;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32858;&#31867;&#36817;&#24180;&#26469;&#25104;&#20026;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#12290;&#23613;&#31649;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20197;&#21069;&#30340;&#28145;&#24230;&#32858;&#31867;&#24037;&#20316;&#22823;&#22810;&#21482;&#21033;&#29992;&#21333;&#23618;&#34920;&#31034;&#26500;&#24314;&#25110;&#23398;&#20064;&#26368;&#32456;&#30340;&#32858;&#31867;&#65292;&#20363;&#22914;&#22312;&#26368;&#21518;&#19968;&#20010;&#20840;&#36830;&#25509;&#23618;&#19978;&#25191;&#34892;K-means&#32858;&#31867;&#25110;&#23558;&#19968;&#20123;&#32858;&#31867;&#25439;&#22833;&#19982;&#29305;&#23450;&#23618;&#30456;&#20851;&#32852;&#65292;&#24573;&#35270;&#20102;&#32852;&#21512;&#21033;&#29992;&#22810;&#23618;&#34920;&#31034;&#20197;&#22686;&#24378;&#28145;&#24230;&#32858;&#31867;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38598;&#25104;&#65288;DeepCluE&#65289;&#30340;&#28145;&#24230;&#32858;&#31867;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#23618;&#30340;&#21147;&#37327;&#26469;&#24357;&#21512;&#28145;&#24230;&#32858;&#31867;&#21644;&#38598;&#25104;&#32858;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20855;&#26377;&#26435;&#37325;&#20849;&#20139;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39592;&#24178;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#23454;&#20363;&#32423;&#23545;&#27604;&#23398;&#20064;&#65288;&#36890;&#36807;&#23454;&#20363;&#25237;&#24433;&#20202;&#65289;&#21644;&#31751;&#32423;&#23545;&#27604;&#23398;&#20064;&#65288;&#36890;&#36807;&#31751;&#25237;&#24433;&#20202;&#65289;&#36827;&#34892;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Deep clustering has recently emerged as a promising technique for complex data clustering. Despite the considerable progress, previous deep clustering works mostly build or learn the final clustering by only utilizing a single layer of representation, e.g., by performing the K-means clustering on the last fully-connected layer or by associating some clustering loss to a specific layer, which neglect the possibilities of jointly leveraging multi-layer representations for enhancing the deep clustering performance. In view of this, this paper presents a Deep Clustering via Ensembles (DeepCluE) approach, which bridges the gap between deep clustering and ensemble clustering by harnessing the power of multiple layers in deep neural networks. In particular, we utilize a weight-sharing convolutional neural network as the backbone, which is trained with both the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector) i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20844;&#24179;&#20998;&#31867;&#20013;&#23384;&#22312;&#30340;&#25112;&#30053;&#25805;&#32437;&#24046;&#24322;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2205.10842</link><description>&lt;p&gt;
&#35299;&#20915;&#20844;&#24179;&#20998;&#31867;&#20013;&#25112;&#30053;&#25805;&#32437;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Addressing Strategic Manipulation Disparities in Fair Classification. (arXiv:2205.10842v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20844;&#24179;&#20998;&#31867;&#20013;&#23384;&#22312;&#30340;&#25112;&#30053;&#25805;&#32437;&#24046;&#24322;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#29615;&#22659;&#20013;&#65292;&#22914;&#36151;&#27454;&#30003;&#35831;&#35780;&#20272;&#25110;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#23457;&#26597;&#65292;&#20010;&#20307;&#36890;&#36807;&#25112;&#30053;&#24615;&#22320;&#26356;&#26032;&#20854;&#29305;&#24449;&#26469;&#22686;&#21152;&#20854;&#33719;&#24471;&#29305;&#23450;&#65288;&#31215;&#26497;&#65289;&#20915;&#31574;&#30340;&#21487;&#33021;&#24615;&#65288;&#20197;&#19968;&#23450;&#30340;&#25104;&#26412;&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#24067;&#25110;&#25903;&#20184;&#19981;&#21516;&#30340;&#26356;&#26032;&#25104;&#26412;&#26102;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26469;&#33258;&#23569;&#25968;&#32676;&#20307;&#30340;&#20010;&#20307;&#20184;&#20986;&#26356;&#39640;&#30340;&#25104;&#26412;&#26469;&#26356;&#26032;&#20854;&#29305;&#24449;&#12290;&#20844;&#24179;&#20998;&#31867;&#26088;&#22312;&#36890;&#36807;&#38480;&#21046;&#20998;&#31867;&#22120;&#28385;&#36275;&#32479;&#35745;&#20844;&#24179;&#24615;&#23646;&#24615;&#26469;&#35299;&#20915;&#27492;&#31867;&#20998;&#31867;&#22120;&#24615;&#33021;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#24182;&#19981;&#33021;&#30830;&#20445;&#21463;&#32422;&#26463;&#30340;&#20998;&#31867;&#22120;&#20943;&#23569;&#25112;&#30053;&#25805;&#32437;&#25104;&#26412;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#25112;&#30053;&#29615;&#22659;&#20013;&#30340;&#36825;&#31181;&#20559;&#24046;&#24182;&#20026;&#25112;&#30053;&#25805;&#32437;&#25552;&#20379;&#24179;&#31561;&#26426;&#20250;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#32422;&#26463;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#26500;&#24314;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world classification settings, such as loan application evaluation or content moderation on online platforms, individuals respond to classifier predictions by strategically updating their features to increase their likelihood of receiving a particular (positive) decision (at a certain cost). Yet, when different demographic groups have different feature distributions or pay different update costs, prior work has shown that individuals from minority groups often pay a higher cost to update their features. Fair classification aims to address such classifier performance disparities by constraining the classifiers to satisfy statistical fairness properties. However, we show that standard fairness constraints do not guarantee that the constrained classifier reduces the disparity in strategic manipulation cost. To address such biases in strategic settings and provide equal opportunities for strategic manipulation, we propose a constrained optimization framework that constructs classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2205.07877</link><description>&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#27169;&#22411;&#37327;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20302;&#20301;&#23485;&#23384;&#20648;&#20840;&#31934;&#24230;&#20540;&#20197;&#23454;&#29616;&#33410;&#32422;&#20869;&#23384;&#21644;&#25805;&#20316;&#25104;&#26412;&#30340;&#21387;&#32553;&#26041;&#27861;&#12290;&#25991;&#31456;&#20998;&#31867;&#20171;&#32461;&#20102;&#21508;&#31181;&#37327;&#21270;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36824;&#22238;&#39038;&#20102;&#27169;&#22411;&#37327;&#21270;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#20248;&#32570;&#28857;&#21644;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#38656;&#35201;&#22823;&#37327;&#30340;&#21442;&#25968;&#23384;&#20648;&#21644;&#36816;&#31639;&#20250;&#24102;&#26469;&#30828;&#20214;&#25104;&#26412;&#30340;&#22686;&#21152;&#21644;&#25361;&#25112;&#12290;&#23545;&#27492;&#65292;&#25552;&#20986;&#20102;&#21387;&#32553;&#26041;&#27861;&#20197;&#35774;&#35745;&#39640;&#25928;&#30340;&#21152;&#36895;&#22120;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#27861;&#26159;&#25226;&#20840;&#31934;&#24230;&#30340;&#20540;&#23384;&#20648;&#22312;&#20302;&#20301;&#23485;&#20013;&#65292;&#36825;&#23601;&#21487;&#20197;&#33410;&#32422;&#20869;&#23384;&#21516;&#26102;&#29992;&#20302;&#25104;&#26412;&#30340;&#31616;&#21333;&#36816;&#31639;&#20195;&#26367;&#21407;&#26412;&#30340;&#25805;&#20316;&#12290;&#30001;&#20110;&#27169;&#22411;&#37327;&#21270;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#35774;&#35745;&#39640;&#25928;&#30828;&#20214;&#30340;&#24433;&#21709;&#65292;&#26368;&#36817;&#20960;&#24180;&#25552;&#20986;&#20102;&#35768;&#22810;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#37327;&#21270;&#26041;&#27861;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#34892;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#20171;&#32461;&#20102;&#37327;&#21270;&#27010;&#24565;&#24182;&#20174;&#19981;&#21516;&#35282;&#24230;&#20998;&#31867;&#26041;&#27861;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#27604;&#20363;&#22240;&#23376;&#21305;&#37197;&#25968;&#25454;&#33539;&#22260;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#26041;&#27861;&#36991;&#20813;&#31934;&#24230;&#25439;&#22833;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#36817;&#24180;&#26469;&#23545;&#27169;&#22411;&#37327;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#24378;&#35843;&#20854;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26550;&#26500;&#36866;&#29992;&#20110;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#32423;&#34701;&#21512;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2205.02357</link><description>&lt;p&gt;
&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#29992;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26550;&#26500;&#36866;&#29992;&#20110;&#22810;&#26679;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#32423;&#34701;&#21512;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#65288;MKG&#65289;&#22312;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;MKG&#32452;&#32455;&#20102;&#35270;&#35273;-&#25991;&#26412;&#20107;&#23454;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;MKG&#37117;&#19981;&#23436;&#25972;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#22810;&#27169;&#24577;&#23454;&#20307;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#38142;&#25509;&#39044;&#27979;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;Transformer&#19982;&#22810;&#32423;&#34701;&#21512;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#28151;&#21512;Transformer&#26550;&#26500;&#21644;&#32479;&#19968;&#30340;&#36755;&#20837;-&#36755;&#20986;&#26469;&#23436;&#25104;&#22810;&#26679;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#32423;&#34701;&#21512;&#65292;&#36890;&#36807;&#31895;&#31890;&#24230;&#21069;&#32512;&#24341;&#23548;&#20132;&#20114;&#21644;&#32454;&#31890;&#24230;&#30456;&#20851;&#24863;&#30693;&#23558;&#35270;&#35273;&#21644;&#25991;&#26412;&#34920;&#31034;&#38598;&#25104;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2203.01077</link><description>&lt;p&gt;
&#35774;&#22791;&#19978;&#23398;&#20064;&#65306;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35757;&#32451;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.01077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#38024;&#23545;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#29615;&#22659;&#22240;&#32032;&#23545;&#20934;&#30830;&#24615;&#36896;&#25104;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35299;&#20915;&#12290;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#20854;&#20934;&#30830;&#24615;&#32463;&#24120;&#21463;&#21040;&#21508;&#31181;&#29615;&#22659;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#22914;&#22122;&#22768;&#12289;&#20256;&#24863;&#22120;&#30340;&#20301;&#32622;/&#26657;&#20934;&#21644;&#26102;&#38388;&#30456;&#20851;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#22791;&#19978;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20107;&#23454;&#19978;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#26377;&#24456;&#22823;&#21306;&#21035;&#65292;&#32780;&#26159;&#19987;&#20026;&#20302;&#31471;&#36793;&#32536;&#35774;&#22791;&#37327;&#36523;&#23450;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20854;&#31639;&#27861;&#21644;&#22312;&#30001;&#26641;&#33683;&#27966;Pico&#21644;&#20302;&#21151;&#32791;&#26080;&#32447;&#27169;&#22359;&#32452;&#25104;&#30340;&#26080;&#32447;&#20256;&#24863;&#22120;&#33410;&#28857;&#19978;&#30340;&#23454;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#26059;&#36716;&#26426;&#22120;&#30340;&#25391;&#21160;&#27169;&#24335;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#35774;&#22791;&#19978;&#23398;&#20064;&#30340;&#37325;&#26032;&#35757;&#32451;&#22312;&#22024;&#26434;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#24322;&#24120;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#33410;&#32422;&#20102;&#20302;&#21151;&#32791;&#35774;&#22791;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
&lt;/p&gt;</description></item><item><title>L4KDE&#26159;&#19968;&#31181;&#29992;&#20110;&#36816;&#21160;&#35268;&#21010;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#36807;&#28193;&#25104;&#26412;&#65292;&#35299;&#20915;&#20102;&#26641;&#25193;&#23637;&#20013;&#36873;&#25321;&#20302;&#25104;&#26412;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2203.00975</link><description>&lt;p&gt;
L4KDE&#65306;&#23398;&#20064;&#29992;&#20110;&#36816;&#21160;&#35268;&#21010;&#30340;&#21160;&#24577;&#26641;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
L4KDE: Learning for KinoDynamic Tree Expansion. (arXiv:2203.00975v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00975
&lt;/p&gt;
&lt;p&gt;
L4KDE&#26159;&#19968;&#31181;&#29992;&#20110;&#36816;&#21160;&#35268;&#21010;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#20043;&#38388;&#30340;&#36807;&#28193;&#25104;&#26412;&#65292;&#35299;&#20915;&#20102;&#26641;&#25193;&#23637;&#20013;&#36873;&#25321;&#20302;&#25104;&#26412;&#33410;&#28857;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36816;&#21160;&#35268;&#21010;&#30340;&#23398;&#20064;&#21160;&#24577;&#26641;&#25193;&#23637;&#65288;L4KDE&#65289;&#26041;&#27861;&#12290;&#22522;&#20110;&#26641;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#22914;&#24555;&#36895;&#38543;&#26426;&#26641;&#65288;RRT&#65289;&#65292;&#26159;&#22312;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#36816;&#21160;&#35268;&#21010;&#20013;&#23547;&#25214;&#20840;&#23616;&#26368;&#20248;&#36335;&#24452;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290; &#26641;&#30340;&#25193;&#23637;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#26680;&#24515;&#27493;&#39588;&#65292;&#21363;&#23558;&#26032;&#33410;&#28857;&#28155;&#21152;&#21040;&#19981;&#26029;&#25193;&#23637;&#30340;&#26641;&#20013;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26641;&#30340;&#36816;&#21160;&#35268;&#21010;&#30340;&#21160;&#21147;&#23398;&#21464;&#20307;&#65292;&#20854;&#20013;&#25105;&#20204;&#24050;&#30693;&#31995;&#32479;&#21160;&#21147;&#23398;&#21644;&#36816;&#21160;&#32422;&#26463;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#20248;&#21270;&#20197;&#25214;&#21040;&#20302;&#25104;&#26412;&#36807;&#28193;&#21040;&#37319;&#26679;&#22352;&#26631;&#30340;&#33410;&#28857;&#65292;&#32780;&#26159;&#20351;&#29992;&#20687;&#22352;&#26631;&#20043;&#38388;&#30340;&#27431;&#20960;&#37324;&#24503;&#36317;&#31163;&#36825;&#26679;&#30340;&#25351;&#26631;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#33410;&#28857;&#36830;&#25509;&#21040;&#25628;&#32034;&#26641;&#30340;&#21551;&#21457;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;L4KDE&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;L4KDE&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#39044;&#27979;&#26597;&#35810;&#29366;&#24577;&#20043;&#38388;&#30340;&#36807;&#28193;&#25104;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#39640;&#25928;&#22320;&#36873;&#25321;&#33410;&#28857;&#19982;&#26679;&#26412;&#22352;&#26631;&#30456;&#36830;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the Learning for KinoDynamic Tree Expansion (L4KDE) method for kinodynamic planning. Tree-based planning approaches, such as rapidly exploring random tree (RRT), are the dominant approach to finding globally optimal plans in continuous state-space motion planning. Central to these approaches is tree-expansion, the procedure in which new nodes are added into an ever-expanding tree. We study the kinodynamic variants of tree-based planning, where we have known system dynamics and kinematic constraints. In the interest of quickly selecting nodes to connect newly sampled coordinates, existing methods typically cannot optimise to find nodes that have low cost to transition to sampled coordinates. Instead, they use metrics like Euclidean distance between coordinates as a heuristic for selecting candidate nodes to connect to the search tree. We propose L4KDE to address this issue. L4KDE uses a neural network to predict transition costs between queried states, which can be efficientl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26469;&#26368;&#22823;&#21270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;AUC&#65288;pAUC&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20934;&#30830;&#21644;&#24179;&#28369;&#30340;pAUC&#20272;&#35745;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.00176</link><description>&lt;p&gt;
&#24403;AUC&#36935;&#19978;DRO&#65306;&#22522;&#20110;&#38750;&#20984;&#25910;&#25947;&#20445;&#35777;&#30340;&#28145;&#24230;&#23398;&#20064;&#23616;&#37096;AUC&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee. (arXiv:2203.00176v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.00176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26469;&#26368;&#22823;&#21270;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#23616;&#37096;AUC&#65288;pAUC&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20934;&#30830;&#21644;&#24179;&#28369;&#30340;pAUC&#20272;&#35745;&#37327;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#31181;&#31995;&#32479;&#19988;&#39640;&#25928;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#19968;&#27425;&#24615;&#21644;&#20108;&#27425;&#24615;&#23616;&#37096;AUC&#65288;pAUC&#65289;&#26368;&#22823;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;DRO&#65289;&#26469;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#27491;&#25968;&#25454;&#23450;&#20041;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;pAUC&#26367;&#20195;&#30446;&#26631;&#30340;&#26032;&#20844;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;DRO&#30340;&#24418;&#24335;&#65292;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#39118;&#38505;&#20540;&#65288;CVaR&#65289;&#65292;&#20135;&#29983;&#38750;&#24179;&#28369;&#20294;&#20934;&#30830;&#30340;pAUC&#20272;&#35745;&#37327;&#65307;&#21478;&#19968;&#31181;&#22522;&#20110;KL&#25955;&#24230;&#27491;&#21017;&#21270;&#30340;DRO&#65292;&#20135;&#29983;&#19981;&#20934;&#30830;&#20294;&#24179;&#28369;&#65288;&#36719;&#65289;&#30340;pAUC&#20272;&#35745;&#37327;&#12290;&#23545;&#20110;&#19968;&#27425;&#24615;&#21644;&#20108;&#27425;&#24615;pAUC&#26368;&#22823;&#21270;&#65292;&#25105;&#20204;&#20998;&#21035;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#23545;&#20110;&#20248;&#21270;&#21508;&#33258;&#30340;&#20004;&#31181;&#24418;&#24335;&#30340;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;pAUC&#26368;&#22823;&#21270;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose systematic and efficient gradient-based methods for both one-way and two-way partial AUC (pAUC) maximization that are applicable to deep learning. We propose new formulations of pAUC surrogate objectives by using the distributionally robust optimization (DRO) to define the loss for each individual positive data. We consider two formulations of DRO, one of which is based on conditional-value-at-risk (CVaR) that yields a non-smooth but exact estimator for pAUC, and another one is based on a KL divergence regularized DRO that yields an inexact but smooth (soft) estimator for pAUC. For both one-way and two-way pAUC maximization, we propose two algorithms and prove their convergence for optimizing their two formulations, respectively. Experiments demonstrate the effectiveness of the proposed algorithms for pAUC maximization for deep learning on various datasets.
&lt;/p&gt;</description></item><item><title>&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20016;&#23500;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2202.12040</link><description>&lt;p&gt;
&#33258;&#20027;&#35757;&#32451;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Self-Training: A Survey. (arXiv:2202.12040v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12040
&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#26080;&#38656;&#39069;&#22806;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#65292;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20998;&#25968;&#20316;&#20026;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#20016;&#23500;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#31639;&#27861;&#26088;&#22312;&#20174;&#23569;&#37327;&#26377;&#26631;&#31614;&#35266;&#27979;&#21644;&#22823;&#37327;&#26080;&#26631;&#31614;&#35266;&#27979;&#20013;&#23398;&#20064;&#39044;&#27979;&#20989;&#25968;&#12290;&#30001;&#20110;&#36825;&#20010;&#26694;&#26550;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26159;&#30456;&#20851;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#37117;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#29616;&#26377;&#30340;&#25216;&#26415;&#20013;&#65292;&#33258;&#20027;&#35757;&#32451;&#26041;&#27861;&#22312;&#36817;&#24180;&#26469;&#26080;&#30097;&#24341;&#36215;&#20102;&#26356;&#22823;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#22312;&#20302;&#23494;&#24230;&#21306;&#22495;&#25214;&#21040;&#20915;&#31574;&#36793;&#30028;&#65292;&#32780;&#19981;&#23545;&#25968;&#25454;&#20998;&#24067;&#20316;&#20986;&#39069;&#22806;&#30340;&#20551;&#35774;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#26080;&#31526;&#21495;&#36755;&#20986;&#20998;&#25968;&#25110;&#20854;&#36793;&#30028;&#20316;&#20026;&#32622;&#20449;&#24230;&#30340;&#25351;&#26631;&#12290;&#33258;&#20027;&#35757;&#32451;&#31639;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#26159;&#36890;&#36807;&#32473;&#20855;&#26377;&#22823;&#20110;&#26576;&#20010;&#38408;&#20540;&#30340;&#36793;&#30028;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#26679;&#26412;&#20998;&#37197;&#20266;&#26631;&#31614;&#65292;&#36845;&#20195;&#22320;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20266;&#26631;&#35760;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#26377;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19982;&#26377;&#26631;&#31614;&#35757;&#32451;&#38598;&#19968;&#36215;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#24182;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.05250</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#40065;&#26834;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive and Robust Multi-Task Learning. (arXiv:2202.05250v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#24182;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35299;&#20915;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#24182;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#33258;&#21160;&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#22788;&#29702;&#23427;&#20204;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#24322;&#24120;&#20219;&#21153;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#30340;&#21151;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the multi-task learning problem that aims to simultaneously analyze multiple datasets collected from different sources and learn one model for each of them. We propose a family of adaptive methods that automatically utilize possible similarities among those tasks while carefully handling their differences. We derive sharp statistical guarantees for the methods and prove their robustness against outlier tasks. Numerical experiments on synthetic and real datasets demonstrate the efficacy of our new methods.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2202.03466</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#36981;&#24490;&#22870;&#21169;&#30340;&#23376;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03466
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#22870;&#21169;&#21152;&#25104;&#30340;&#23376;&#20219;&#21153;&#26469;&#21457;&#29616;&#36873;&#39033;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#24573;&#30053;&#21407;&#22987;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#36828;&#22823;&#30446;&#26631;&#65292;&#24378;&#21270;&#23398;&#20064;&#24517;&#39035;&#21253;&#25324;&#23545;&#25277;&#35937;&#29366;&#24577;&#21644;&#26102;&#38388;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#35268;&#21010;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#29366;&#24577;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26102;&#38388;&#25277;&#35937;&#21364;&#24456;&#23569;&#34987;&#20351;&#29992;&#65292;&#23613;&#31649;&#22522;&#20110;&#36873;&#39033;&#26694;&#26550;&#24050;&#32463;&#24191;&#27867;&#21457;&#23637;&#20102;&#29702;&#35770;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#21487;&#33021;&#30340;&#36873;&#39033;&#31354;&#38388;&#24456;&#22823;&#65292;&#20197;&#21069;&#25552;&#20986;&#30340;&#36873;&#39033;&#21457;&#29616;&#26041;&#27861;&#27809;&#26377;&#32771;&#34385;&#21040;&#36873;&#39033;&#27169;&#22411;&#22312;&#35268;&#21010;&#20013;&#30340;&#20351;&#29992;&#26041;&#24335;&#12290;&#36890;&#24120;&#36890;&#36807;&#25552;&#20986;&#23376;&#20219;&#21153;&#65288;&#20363;&#22914;&#36798;&#21040;&#29942;&#39048;&#29366;&#24577;&#25110;&#26368;&#22823;&#21270;&#38500;&#22870;&#21169;&#22806;&#30340;&#24863;&#30693;&#20449;&#21495;&#30340;&#32047;&#31215;&#21644;&#65289;&#26469;&#21457;&#29616;&#36873;&#39033;&#12290;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#20197;&#29983;&#25104;&#19968;&#20010;&#36873;&#39033;&#65292;&#28982;&#21518;&#23398;&#20064;&#36873;&#39033;&#30340;&#27169;&#22411;&#24182;&#20351;&#20854;&#21487;&#29992;&#20110;&#35268;&#21010;&#36807;&#31243;&#12290;&#22312;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#23376;&#20219;&#21153;&#24573;&#30053;&#20102;&#21407;&#22987;&#38382;&#39064;&#19978;&#30340;&#22870;&#21169;&#65292;&#32780;&#25105;&#20204;&#25552;&#20986;&#30340;&#23376;&#20219;&#21153;&#20351;&#29992;&#21407;&#22987;&#22870;&#21169;&#21152;&#19978;&#22522;&#20110;&#26576;&#20010;&#29305;&#24449;&#30340;&#22870;&#21169;&#21152;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
&lt;/p&gt;</description></item><item><title>DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2201.03335</link><description>&lt;p&gt;
DeepKE: &#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#29992;&#20110;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03335
&lt;/p&gt;
&lt;p&gt;
DeepKE&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;&#65292;&#25903;&#25345;&#22797;&#26434;&#30340;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#21487;&#29992;&#20110;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26469;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;&#21644;&#21487;&#25193;&#23637;&#30340;&#30693;&#35782;&#25552;&#21462;&#24037;&#20855;&#21253;DeepKE&#65292;&#25903;&#25345;&#30693;&#35782;&#24211;&#26500;&#24314;&#20013;&#30340;&#22797;&#26434;&#20302;&#36164;&#28304;&#12289;&#25991;&#26723;&#32423;&#21644;&#22810;&#27169;&#24577;&#22330;&#26223;&#12290;DeepKE&#23454;&#29616;&#20102;&#21508;&#31181;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23646;&#24615;&#25552;&#21462;&#12290;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;DeepKE&#20801;&#35768;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#23450;&#21046;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DeepKE&#19981;&#20165;&#20026;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#25552;&#20379;&#21508;&#31181;&#21151;&#33021;&#27169;&#22359;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#36824;&#36890;&#36807;&#19968;&#33268;&#30340;&#26694;&#26550;&#32452;&#32455;&#25152;&#26377;&#32452;&#20214;&#65292;&#20197;&#20445;&#25345;&#36275;&#22815;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/zjunlp/DeepKE&#21457;&#24067;&#20102;&#28304;&#20195;&#30721;&#65292;&#24182;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;&#21021;&#23398;&#32773;&#30340;Google Colab&#25945;&#31243;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;http URL&#19978;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#32447;&#31995;&#32479;&#65292;&#29992;&#20110;&#23454;&#26102;&#25552;&#21462;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#28436;&#31034;&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in this http URL for real-time extraction of various tasks, and a demo video
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#23398;&#31038;&#21306;&#20013;&#30340;Reddit&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;COVID-19&#30123;&#24773;&#23545;&#20154;&#20204;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;RoBERTa&#21644;GAT&#30340;&#24773;&#32490;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2112.04351</link><description>&lt;p&gt;
&#24773;&#32490;&#20998;&#26512;&#21644;&#26032;&#20896;&#30123;&#24773;&#23545;&#22823;&#23398;&#31038;&#21306;&#20013;Reddit&#25968;&#25454;&#30340;&#24433;&#21709;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.04351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#22823;&#23398;&#31038;&#21306;&#20013;&#30340;Reddit&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;COVID-19&#30123;&#24773;&#23545;&#20154;&#20204;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;RoBERTa&#21644;GAT&#30340;&#24773;&#32490;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;COVID-19&#30123;&#24773;&#20197;&#21508;&#31181;&#26041;&#24335;&#24433;&#21709;&#20102;&#25105;&#20204;&#30340;&#31038;&#20250;&#21644;&#20154;&#31867;&#31119;&#31049;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#35843;&#26597;&#20102;&#30123;&#24773;&#22914;&#20309;&#19982;&#30123;&#24773;&#21069;&#26399;&#30456;&#27604;&#24433;&#21709;&#20102;&#20154;&#20204;&#30340;&#24773;&#32490;&#21644;&#24515;&#29702;&#29366;&#24577;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#25910;&#38598;&#20102;&#19982;&#20843;&#25152;&#22823;&#23398;&#30456;&#20851;&#30340;Reddit&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;2019&#24180;&#65288;&#30123;&#24773;&#21069;&#65289;&#21644;2020&#24180;&#65288;&#30123;&#24773;&#26399;&#38388;&#65289;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;RoBERTa&#26041;&#27861;&#23398;&#20064;Reddit&#28040;&#24687;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#21516;&#26102;&#21033;&#29992;&#21457;&#24067;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#20449;&#24687;&#35757;&#32451;&#20102;&#19968;&#20010;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#36827;&#34892;&#24773;&#32490;&#20998;&#31867;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;RoBERTa&#21644;GAT&#30340;&#39044;&#27979;&#27010;&#29575;&#36827;&#34892;&#27169;&#22411;&#22534;&#21472;&#65292;&#24471;&#20986;&#24773;&#32490;&#26368;&#32456;&#20998;&#31867;&#32467;&#26524;&#12290;&#36890;&#36807;&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#39044;&#27979;&#30340;&#24773;&#32490;&#26631;&#31614;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#20272;&#35745;&#20102;&#30123;&#24773;&#21644;&#24773;&#32490;&#20043;&#38388;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: The COVID-19 pandemic has affected our society and human well-being in various ways. In this study, we investigate how the pandemic has influenced people's emotions and psychological states compared to a pre-pandemic period using real-world data from social media.  Method: We collected Reddit social media data from 2019 (pre-pandemic) and 2020 (pandemic) from the subreddits communities associated with eight universities. We applied the pre-trained Robustly Optimized BERT pre-training approach (RoBERTa) to learn text embedding from the Reddit messages, and leveraged the relational information among posted messages to train a graph attention network (GAT) for sentiment classification. Finally, we applied model stacking to combine the prediction probabilities from RoBERTa and GAT to yield the final classification on sentiment. With the model-predicted sentiment labels on the collected data, we used a generalized linear mixed-effects model to estimate the effects of pandemic an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#21644;&#36880;&#36890;&#36947;&#30340;&#28176;&#36827;&#24335;&#36864;&#20986;&#26041;&#27861;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#35813;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#23618;&#32423;&#29305;&#23450;&#36864;&#20986;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#33021;&#22815;&#26356;&#20805;&#20998;&#22320;&#32531;&#35299;&#28304;&#39046;&#22495;&#19978;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2112.03676</link><description>&lt;p&gt;
PLACE&#36864;&#20986;&#65306;&#19968;&#31181;&#36880;&#23618;&#21644;&#36880;&#36890;&#36947;&#30340;&#28176;&#36827;&#24335;&#36864;&#20986;&#26041;&#27861;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization. (arXiv:2112.03676v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.03676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#21644;&#36880;&#36890;&#36947;&#30340;&#28176;&#36827;&#24335;&#36864;&#20986;&#26041;&#27861;&#29992;&#20110;&#39046;&#22495;&#27867;&#21270;&#65292;&#35813;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#23618;&#32423;&#29305;&#23450;&#36864;&#20986;&#26041;&#27861;&#20855;&#26377;&#26356;&#24378;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#33021;&#22815;&#26356;&#20805;&#20998;&#22320;&#32531;&#35299;&#28304;&#39046;&#22495;&#19978;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#65288;DG&#65289;&#26088;&#22312;&#20174;&#22810;&#20010;&#35266;&#27979;&#21040;&#30340;&#28304;&#39046;&#22495;&#20013;&#23398;&#20064;&#20986;&#19968;&#20010;&#27867;&#21270;&#33021;&#21147;&#33391;&#22909;&#30340;&#36890;&#29992;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#20219;&#24847;&#26410;&#35266;&#27979;&#21040;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;DG&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#65292;&#30001;&#20110;&#28304;&#39046;&#22495;&#21644;&#30446;&#26631;&#39046;&#22495;&#20043;&#38388;&#30340;&#39046;&#22495;&#24046;&#24322;&#65292;&#27169;&#22411;&#24517;&#28982;&#38754;&#20020;&#20005;&#37325;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#22522;&#20110;&#36864;&#20986;&#30340;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#36890;&#36807;&#20002;&#24323;&#20013;&#38388;&#23618;&#30340;&#37096;&#20998;&#34920;&#31034;&#26469;&#25269;&#25239;&#36807;&#25311;&#21512;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#21482;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#23618;&#32423;&#19978;&#25191;&#34892;&#36864;&#20986;&#25805;&#20316;&#65292;&#23548;&#33268;&#23545;&#27169;&#22411;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#19981;&#36275;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#22810;&#20010;&#23618;&#32423;&#19978;&#24212;&#29992;&#36864;&#20986;&#21487;&#20197;&#20135;&#29983;&#26356;&#24378;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#30456;&#27604;&#20197;&#21069;&#30340;&#23618;&#32423;&#29305;&#23450;&#36864;&#20986;&#26041;&#27861;&#65292;&#36825;&#21487;&#20197;&#26356;&#20805;&#20998;&#22320;&#32531;&#35299;&#28304;&#39046;&#22495;&#19978;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#36880;&#23618;&#21644;&#36880;&#36890;&#36947;&#36864;&#20986;&#26041;&#27861;&#29992;&#20110;DG&#65292;&#35813;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#23618;&#65292;&#28982;&#21518;&#25191;&#34892;&#36864;&#20986;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21015;&#29983;&#25104;&#27861;&#26469;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#24067;&#23572;&#35268;&#21017;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20860;&#39038;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#30340;&#24179;&#34913;&#26041;&#38754;&#20248;&#20110;&#24120;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#20197;&#28385;&#36275;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#12290;&#20351;&#29992;&#36817;&#20284;&#21015;&#29983;&#25104;&#31639;&#27861;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2111.08466</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#24067;&#23572;&#35268;&#21017;&#38598;&#21512;&#29983;&#25104;&#27861;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Fair Boolean Rule Sets via Column Generation. (arXiv:2111.08466v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.08466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21015;&#29983;&#25104;&#27861;&#26469;&#29983;&#25104;&#21487;&#35299;&#37322;&#21644;&#20844;&#24179;&#30340;&#24067;&#23572;&#35268;&#21017;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20860;&#39038;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#30340;&#24179;&#34913;&#26041;&#38754;&#20248;&#20110;&#24120;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#25366;&#25496;&#26041;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#20197;&#28385;&#36275;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#12290;&#20351;&#29992;&#36817;&#20284;&#21015;&#29983;&#25104;&#31639;&#27861;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#23558;&#24067;&#23572;&#35268;&#21017;&#23398;&#20064;&#34920;&#31034;&#20026;&#26512;&#21462;&#33539;&#24335;&#65288;DNF&#65292;&#19982;&#20915;&#31574;&#35268;&#21017;&#38598;&#30456;&#31561;&#65289;&#30340;&#21487;&#35299;&#37322;&#30340;&#20998;&#31867;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#27169;&#22411;&#65292;&#20197;&#26368;&#20248;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#35268;&#21017;&#31616;&#21333;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#20102;&#20844;&#24179;&#24615;&#35774;&#32622;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#65292;&#21253;&#25324;&#23545;&#20004;&#31181;&#20998;&#31867;&#20844;&#24179;&#24615;&#24230;&#37327;&#30340;&#26174;&#24335;&#32422;&#26463;&#65306;&#26426;&#20250;&#24179;&#31561;&#21644;&#22343;&#34913;&#20960;&#29575;&#12290;&#37319;&#29992;&#21015;&#29983;&#25104;&#27861;&#65288;CG&#65289;&#39640;&#25928;&#25628;&#32034;&#22823;&#37327;&#21487;&#33021;&#35268;&#21017;&#65292;&#32780;&#26080;&#38656;&#21551;&#21457;&#24335;&#35268;&#21017;&#25366;&#25496;&#12290;&#20026;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#21270;&#30340;&#36817;&#20284;CG&#31639;&#27861;&#12290;&#19982;&#19977;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#26367;&#20195;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;16&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;CG&#31639;&#27861;&#22312;8&#20010;&#25968;&#25454;&#38598;&#19978;&#20860;&#39038;&#20102;&#20934;&#30830;&#24615;&#21644;&#31616;&#21333;&#24615;&#30340;&#24179;&#34913;&#12290;&#24403;&#20197;&#20934;&#30830;&#24615;&#20026;&#26368;&#22823;&#21270;&#30446;&#26631;&#26102;&#65292;CG&#31639;&#27861;&#19982;&#19987;&#20026;&#27492;&#30446;&#30340;&#35774;&#35745;&#30340;&#35268;&#21017;&#23398;&#20064;&#22120;&#20855;&#26377;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#65292;&#26377;&#26102;&#33021;&#25214;&#21040;&#26356;&#31616;&#21333;&#20294;&#20934;&#30830;&#24615;&#19981;&#20943;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rule mining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;EPO Search&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#21046;&#23450;&#65288;MCDM&#65289;&#20013;&#20999;&#27604;&#38634;&#22827;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#25910;&#25947;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.00597</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#21046;&#23450;&#30340;&#20934;&#30830;&#24085;&#32047;&#25176;&#26368;&#20248;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exact Pareto Optimal Search for Multi-Task Learning and Multi-Criteria Decision-Making. (arXiv:2108.00597v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.00597
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#26041;&#27861;&#65292;EPO Search&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#21046;&#23450;&#65288;MCDM&#65289;&#20013;&#20999;&#27604;&#38634;&#22827;&#26631;&#37327;&#21270;&#26041;&#27861;&#22312;&#25910;&#25947;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#22810;&#20010;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#21644;&#30446;&#26631;&#29305;&#23450;&#26435;&#37325;&#65292;&#20999;&#27604;&#38634;&#22827;&#26631;&#37327;&#21270;&#65288;CS&#65289;&#26159;&#19968;&#31181;&#33719;&#24471;&#20934;&#30830;&#24085;&#32047;&#25176;&#26368;&#20248;&#65288;EPO&#65289;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#26435;&#37325;&#30340;&#20498;&#25968;&#23450;&#20041;&#30340;&#23556;&#32447;&#19982;&#24085;&#32047;&#25176;&#21069;&#27839;&#65288;PF&#65289;&#30456;&#20132;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20351;&#29992;CS&#20844;&#24335;&#23547;&#25214;EPO&#35299;&#30340;&#19968;&#38454;&#20248;&#21270;&#22120;&#20250;&#36935;&#21040;&#38663;&#33633;&#21644;&#20572;&#28382;&#31561;&#23454;&#38469;&#38382;&#39064;&#65292;&#24433;&#21709;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#24403;&#20351;&#29992;PO&#35299;&#21021;&#22987;&#21270;&#26102;&#65292;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#23436;&#20840;&#20301;&#20110;PF&#19978;&#30340;&#21463;&#25511;&#36712;&#36857;&#12290;&#36825;&#20123;&#32570;&#28857;&#23548;&#33268;&#21033;&#29992;CS&#36827;&#34892;&#22522;&#20110;&#38750;&#20984;&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#21644;&#22810;&#26631;&#20934;&#20915;&#31574;&#21046;&#23450;&#65288;MCDM&#65289;&#26041;&#27861;&#30340;&#24314;&#27169;&#38480;&#21046;&#21644;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;MOO&#26041;&#27861;&#65292;EPO&#25628;&#32034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;EPO&#25628;&#32034;&#25910;&#25947;&#21040;&#19968;&#20010;EPO&#35299;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#35828;&#26126;&#20102;&#20854;&#35745;&#31639;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given multiple non-convex objective functions and objective-specific weights, Chebyshev scalarization (CS) is a well-known approach to obtain an Exact Pareto Optimal (EPO), i.e., a solution on the Pareto front (PF) that intersects the ray defined by the inverse of the weights. First-order optimizers that use the CS formulation to find EPO solutions encounter practical problems of oscillations and stagnation that affect convergence. Moreover, when initialized with a PO solution, they do not guarantee a controlled trajectory that lies completely on the PF. These shortcomings lead to modeling limitations and computational inefficiency in multi-task learning (MTL) and multi-criteria decision-making (MCDM) methods that utilize CS for their underlying non-convex multi-objective optimization (MOO). To address these shortcomings, we design a new MOO method, EPO Search. We prove that EPO Search converges to an EPO solution and empirically illustrate its computational efficiency and robustness t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#25903;&#25345;&#38598;&#20013;&#25512;&#26029;&#30446;&#26631;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#23454;&#20363;&#32423;&#30340;&#27169;&#31946;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.08112</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#26469;&#23545;&#20803;&#23398;&#20064;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
Contextualizing Meta-Learning via Learning to Decompose. (arXiv:2106.08112v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.08112
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#65292;&#29992;&#20110;&#20174;&#25903;&#25345;&#38598;&#20013;&#25512;&#26029;&#30446;&#26631;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#25429;&#25417;&#21040;&#23454;&#20363;&#32423;&#30340;&#27169;&#31946;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#22522;&#20110;&#25903;&#25345;&#38598;&#26500;&#24314;&#30446;&#26631;&#27169;&#22411;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;&#20803;&#23398;&#20064;&#23884;&#20837;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#23558;&#23454;&#20363;&#25289;&#36817;&#21040;&#21516;&#31867;&#37051;&#23621;&#26469;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#30446;&#26631;&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#21333;&#20010;&#23454;&#20363;&#21487;&#20197;&#20174;&#22810;&#20010;&#28508;&#22312;&#23646;&#24615;&#36827;&#34892;&#27880;&#37322;&#65292;&#20351;&#24471;&#22312;&#25903;&#25345;&#38598;&#20869;&#37096;&#25110;&#36328;&#25903;&#25345;&#38598;&#30340;&#30475;&#20284;&#30456;&#20284;&#30340;&#23454;&#20363;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#21644;&#19982;&#20854;&#20182;&#23454;&#20363;&#30340;&#22810;&#26679;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20174;&#25903;&#25345;&#38598;&#25512;&#26029;&#30446;&#26631;&#27169;&#22411;&#30340;&#32479;&#19968;&#20803;&#23398;&#20064;&#31574;&#30053;&#26080;&#27861;&#25429;&#25417;&#21040;&#23454;&#20363;&#32423;&#30340;&#27169;&#31946;&#30456;&#20284;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#35299;&#32806;&#32593;&#32476;&#65288;LeadNet&#65289;&#65292;&#20197;&#22312;&#25903;&#25345;&#38598;&#20013;&#21033;&#29992;&#20855;&#26377;&#19968;&#31181;&#25110;&#28151;&#21512;&#28508;&#22312;&#23646;&#24615;&#30340;&#23454;&#20363;&#30340;&#19978;&#19979;&#25991;&#23545;&#20803;&#23398;&#20064;&#30340;&#8220;&#25903;&#25345;&#21040;&#30446;&#26631;&#8221;&#31574;&#30053;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#23454;&#20363;&#20043;&#38388;&#30340;&#27604;&#36739;&#20851;&#31995;&#22312;&#22810;&#20010;&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#35299;&#32806;&#12290;LeadNet&#23398;&#20064;&#33258;&#21160;&#21270;&#22320;...&#65288;&#24453;&#23436;&#25972;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
Meta-learning has emerged as an efficient approach for constructing target models based on support sets. For example, the meta-learned embeddings enable the construction of target nearest-neighbor classifiers for specific tasks by pulling instances closer to their same-class neighbors. However, a single instance can be annotated from various latent attributes, making visually similar instances inside or across support sets have different labels and diverse relationships with others. Consequently, a uniform meta-learned strategy for inferring the target model from the support set fails to capture the instance-wise ambiguous similarity. To this end, we propose Learning to Decompose Network (LeadNet) to contextualize the meta-learned ``support-to-target'' strategy, leveraging the context of instances with one or mixed latent attributes in a support set. In particular, the comparison relationship between instances is decomposed w.r.t. multiple embedding spaces. LeadNet learns to automatica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20449;&#24687;&#32858;&#21512;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#23548;&#33268;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2105.08511</link><description>&lt;p&gt;
&#38754;&#21521;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Constrained Domain Generalization for Medical Image Classification. (arXiv:2105.08511v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.08511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20449;&#24687;&#32858;&#21512;&#36807;&#31243;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#38382;&#39064;&#23548;&#33268;&#30340;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#24433;&#20687;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38598;&#26377;&#38480;&#21644;&#23545;&#24739;&#32773;&#38544;&#31169;&#20445;&#25252;&#30340;&#20005;&#26684;&#27861;&#24459;&#21644;&#20262;&#29702;&#35201;&#27714;&#65292;&#30001;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21307;&#23398;&#24433;&#20687;&#20998;&#31867;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#24456;&#22823;&#38459;&#30861;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#38544;&#31169;&#20445;&#25252;&#32422;&#26463;&#22495;&#27867;&#21270;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#26465;&#20214;&#19979;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#25913;&#21892;&#38598;&#20013;&#24335;&#26381;&#21153;&#22120;&#31471;&#30340;&#20449;&#24687;&#32858;&#21512;&#36807;&#31243;&#65292;&#26399;&#26395;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#27867;&#21270;&#21040;&#8220;&#26410;&#35265;&#36807;&#8221;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the "unseen" bu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#26080;&#32500;&#24230;&#24230;&#37327;&#65292;&#29992;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#32463;&#39564;&#27979;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#35299;&#20915;&#20102;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2104.12036</link><description>&lt;p&gt;
&#19968;&#31867;&#26080;&#32500;&#24230;&#24230;&#37327;&#29992;&#20110;&#32463;&#39564;&#27979;&#24230;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Class of Dimension-free Metrics for the Convergence of Empirical Measures. (arXiv:2104.12036v4 [math.PR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31867;&#26080;&#32500;&#24230;&#24230;&#37327;&#65292;&#29992;&#20110;&#39640;&#32500;&#24773;&#20917;&#19979;&#32463;&#39564;&#27979;&#24230;&#30340;&#25910;&#25947;&#24615;&#65292;&#35299;&#20915;&#20102;&#32500;&#24230;&#28798;&#38590;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#39640;&#32500;&#24773;&#20917;&#19979;&#32463;&#39564;&#27979;&#24230;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#24230;&#37327;&#31867;&#65292;&#35777;&#26126;&#22312;&#36825;&#31181;&#24230;&#37327;&#19979;&#65292;&#25910;&#25947;&#24615;&#19981;&#21463;&#32500;&#24230;&#28798;&#38590;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#29305;&#24615;&#23545;&#20110;&#39640;&#32500;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#32463;&#20856;&#24230;&#37327;&#65288;&#22914;Wasserstein&#24230;&#37327;&#65289;&#24418;&#25104;&#23545;&#27604;&#12290;&#25152;&#25552;&#20986;&#30340;&#24230;&#37327;&#23646;&#20110;&#31215;&#20998;&#27010;&#29575;&#24230;&#37327;&#30340;&#33539;&#30068;&#65292;&#25105;&#20204;&#25351;&#23450;&#20102;&#27979;&#35797;&#20989;&#25968;&#31354;&#38388;&#30340;&#20934;&#21017;&#65292;&#20197;&#30830;&#20445;&#19981;&#21463;&#32500;&#24230;&#28798;&#38590;&#24433;&#21709;&#12290;&#25152;&#36873;&#27979;&#35797;&#20989;&#25968;&#31354;&#38388;&#30340;&#20363;&#23376;&#21253;&#25324;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#12289;Barron&#31354;&#38388;&#21644;&#27969;&#35825;&#23548;&#20989;&#25968;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#25552;&#20986;&#24230;&#37327;&#30340;&#19977;&#20010;&#24212;&#29992;&#65306;1. &#22312;&#38543;&#26426;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#32463;&#39564;&#27979;&#24230;&#25910;&#25947;&#24615;&#65307;2. n&#31890;&#23376;&#31995;&#32479;&#25910;&#25947;&#20110;McKean-Vlasov&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65307;3. &#26500;&#36896;&#40784;&#27425;n-player&#30340;&#949;-Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the convergence of empirical measures in high dimensions. We propose a new class of probability metrics and show that under such metrics, the convergence is free of the curse of dimensionality (CoD). Such a feature is critical for high-dimensional analysis and stands in contrast to classical metrics ({\it e.g.}, the Wasserstein metric). The proposed metrics fall into the category of integral probability metrics, for which we specify criteria of test function spaces to guarantee the property of being free of CoD. Examples of the selected test function spaces include the reproducing kernel Hilbert spaces, Barron space, and flow-induced function spaces. Three applications of the proposed metrics are presented: 1. The convergence of empirical measure in the case of random variables; 2. The convergence of $n$-particle system to the solution to McKean-Vlasov stochastic differential equation; 3. The construction of an $\varepsilon$-Nash equilibrium for a homogeneous $n$-pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.07650</link><description>&lt;p&gt;
KnowPrompt&#65306;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowPrompt&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#65292;&#24182;&#36890;&#36807;&#21327;&#21516;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#25552;&#39640;&#20102;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25552;&#31034;&#35843;&#25972;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#23558;&#25991;&#26412;&#29255;&#27573;&#65288;&#21363;&#27169;&#26495;&#65289;&#25554;&#20837;&#36755;&#20837;&#65292;&#24182;&#23558;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20851;&#31995;&#25277;&#21462;&#65292;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#33719;&#21462;&#21512;&#36866;&#30340;&#26631;&#31614;&#35789;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#12290;&#27492;&#22806;&#65292;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#20016;&#23500;&#30340;&#35821;&#20041;&#21644;&#20808;&#39564;&#30693;&#35782;&#65292;&#19981;&#23481;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#30528;&#30524;&#20110;&#23558;&#20851;&#31995;&#26631;&#31614;&#20043;&#38388;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#20851;&#31995;&#25277;&#21462;&#30340;&#25552;&#31034;&#35843;&#25972;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21327;&#21516;&#20248;&#21270;&#30340;&#30693;&#35782;&#24863;&#30693;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65288;KnowPrompt&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#34394;&#25311;&#31867;&#22411;&#35789;&#21644;&#31572;&#26696;&#35789;&#23558;&#20851;&#31995;&#26631;&#31614;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#34701;&#20837;&#21040;&#25552;&#31034;&#26500;&#24314;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#21270;&#32422;&#26463;&#21327;&#21516;&#20248;&#21270;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#35299;&#31163;&#30028;&#65292;&#30456;&#27604;&#29616;&#26377;&#26694;&#26550;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#26377;&#26174;&#33879;&#30340;&#23454;&#29992;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2102.08649</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#29992;&#20110;PAC-Bayesian&#30028;&#30340;&#23454;&#29992;&#35299;&#35835;
&lt;/p&gt;
&lt;p&gt;
A General Framework for the Practical Disintegration of PAC-Bayesian Bounds. (arXiv:2102.08649v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.08649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#35299;&#31163;&#30028;&#65292;&#30456;&#27604;&#29616;&#26377;&#26694;&#26550;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#26377;&#26174;&#33879;&#30340;&#23454;&#29992;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian&#30028;&#22312;&#30740;&#31350;&#38543;&#26426;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26102;&#24050;&#34987;&#35777;&#26126;&#32039;&#20945;&#32780;&#19988;&#26377;&#20449;&#24687;&#37327;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#19968;&#20123;&#30830;&#23450;&#24615;&#27169;&#22411;&#23478;&#26063;&#65288;&#22914;&#31070;&#32463;&#32593;&#32476;&#65289;&#26102;&#65292;&#23427;&#20204;&#38656;&#35201;&#26494;&#24347;&#19988;&#26114;&#36149;&#30340;&#21435;&#38543;&#26426;&#21270;&#27493;&#39588;&#12290;&#20316;&#20026;&#26367;&#20195;&#27493;&#39588;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;PAC-Bayesian&#27867;&#21270;&#30028;&#65292;&#36825;&#20123;&#30028;&#29420;&#20855;&#21019;&#26032;&#24615;&#65292;&#33021;&#22815;&#25552;&#20379;&#35299;&#31163;&#30028;&#65292;&#21363;&#23427;&#20204;&#33021;&#22815;&#23545;&#19968;&#20010;&#21333;&#19968;&#20551;&#35774;&#25552;&#20379;&#20445;&#35777;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#24179;&#22343;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30028;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#29992;&#20110;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#31070;&#32463;&#32593;&#32476;&#19978;&#23637;&#31034;&#20102;&#36825;&#31181;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26694;&#26550;&#30456;&#27604;&#30340;&#26174;&#33879;&#23454;&#29992;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
PAC-Bayesian bounds are known to be tight and informative when studying the generalization ability of randomized classifiers. However, they require a loose and costly derandomization step when applied to some families of deterministic models such as neural networks. As an alternative to this step, we introduce new PAC-Bayesian generalization bounds that have the originality to provide disintegrated bounds, i.e., they give guarantees over one single hypothesis instead of the usual averaged analysis. Our bounds are easily optimizable and can be used to design learning algorithms. We illustrate this behavior on neural networks, and we show a significant practical improvement over the state-of-the-art framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25042;&#24816;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#20854;&#22312;&#20999;&#25442;&#27425;&#25968;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21576;&#29616;&#20986;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2102.03803</link><description>&lt;p&gt;
&#25042;&#24816;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;: &#20999;&#25442;&#39044;&#31639;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lazy OCO: Online Convex Optimization on a Switching Budget. (arXiv:2102.03803v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.03803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25042;&#24816;&#22411;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#20854;&#22312;&#20999;&#25442;&#27425;&#25968;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#30340;&#36951;&#25022;&#19978;&#30028;&#65292;&#24182;&#19988;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#21576;&#29616;&#20986;&#39640;&#25928;&#30340;&#35745;&#31639;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#20984;&#20248;&#21270;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#29609;&#23478;&#22312;T&#36718;&#20013;&#30340;&#26399;&#26395;&#20999;&#25442;&#20915;&#31574;&#19981;&#36229;&#36807;S&#27425;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35299;&#20915;&#20102;&#31163;&#25955;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#31867;&#20284;&#38382;&#39064;&#65292;&#26368;&#36817;&#20063;&#22312;&#36830;&#32493;&#35774;&#32622;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#23545;&#25163;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#22312;&#26222;&#36941;&#23384;&#22312;&#30340;&#26080;&#30693;&#35774;&#32622;&#20013;&#25552;&#20986;&#35745;&#31639;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#20026;&#19968;&#33324;&#20984;&#25439;&#22833;&#24314;&#31435;&#20102;O(T/S)&#30340;&#36951;&#25022;&#19978;&#30028;&#20197;&#21450;&#24378;&#20984;&#25439;&#22833;&#30340;&#36817;&#20284;O(T/S^2)&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38543;&#26426;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#25439;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#22312;&#19968;&#33324;&#21644;&#24378;&#20984;&#35774;&#32622;&#20013;&#36951;&#25022;&#20165;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#20056;&#27861;log T&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;log T&#27425;&#20999;&#25442;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34917;&#20805;&#20102;&#19982;&#25105;&#20204;&#32771;&#34385;&#30340;&#19968;&#20123;&#24773;&#20917;&#30456;&#21305;&#37197;&#30340;&#19979;&#30028;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variant of online convex optimization where the player is permitted to switch decisions at most $S$ times in expectation throughout $T$ rounds. Similar problems have been addressed in prior work for the discrete decision set setting, and more recently in the continuous setting but only with an adaptive adversary. In this work, we aim to fill the gap and present computationally efficient algorithms in the more prevalent oblivious setting, establishing a regret bound of $O(T/S)$ for general convex losses and $\widetilde O(T/S^2)$ for strongly convex losses. In addition, for stochastic i.i.d.~losses, we present a simple algorithm that performs $\log T$ switches with only a multiplicative $\log T$ factor overhead in its regret in both the general and strongly convex settings. Finally, we complement our algorithms with lower bounds that match our upper bounds in some of the cases we consider.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2101.11992</link><description>&lt;p&gt;
&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#20197;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Acting in Delayed Environments with Non-Stationary Markov Policies. (arXiv:2101.11992v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.11992
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#24310;&#36831;&#29615;&#22659;&#20013;&#65292;&#23398;&#20064;&#21644;&#35268;&#21010;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#38750;&#22266;&#23450;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#30340;&#38750;&#22266;&#23450;Q-learning&#39118;&#26684;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20551;&#35774;&#22312;&#36873;&#25321;&#21160;&#20316;&#21518;&#31435;&#21363;&#25191;&#34892;&#65292;&#20294;&#36825;&#31181;&#20551;&#35774;&#24120;&#24120;&#19981;&#20999;&#23454;&#38469;&#65292;&#20250;&#22312;&#26426;&#22120;&#20154;&#25805;&#32437;&#12289;&#20113;&#35745;&#31639;&#21644;&#37329;&#34701;&#31561;&#24212;&#29992;&#20013;&#23548;&#33268;&#28798;&#38590;&#24615;&#25925;&#38556;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23398;&#20064;&#21644;&#35745;&#21010;&#30340;MDP&#26694;&#26550;&#65292;&#20854;&#20013;&#20915;&#31574;&#32773;&#36873;&#25321;&#30340;&#21160;&#20316;&#38656;&#35201;&#24310;&#36831;$m$&#27493;&#25165;&#33021;&#25191;&#34892;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#24310;&#36831;&#25191;&#34892;&#30340;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#36275;&#20197;&#23454;&#29616;&#26368;&#22823;&#22870;&#21169;&#65292;&#20294;&#38656;&#35201;&#26159;&#38750;&#22266;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22266;&#23450;&#30340;&#39532;&#23572;&#21487;&#22827;&#31574;&#30053;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#22266;&#23450;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;Q-learning&#39118;&#26684;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#24310;&#36831;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#22312;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#26041;&#24046;&#39118;&#38505;&#20934;&#21017;&#30740;&#31350;&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#24046;&#32422;&#26463;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2012.14098</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#26041;&#24046;&#32422;&#26463;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#33021;&#22815;&#25214;&#21040;&#20840;&#23616;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy. (arXiv:2012.14098v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.14098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#22312;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#26041;&#24046;&#39118;&#38505;&#20934;&#21017;&#30740;&#31350;&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#24046;&#32422;&#26463;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#24037;&#20316;&#20165;&#20851;&#27880;&#26368;&#22823;&#21270;&#24635;&#22238;&#25253;&#30340;&#26399;&#26395;&#20540;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#20854;&#22266;&#26377;&#30340;&#38543;&#26426;&#24615;&#12290;&#36825;&#31181;&#38543;&#26426;&#24615;&#20063;&#34987;&#31216;&#20026;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19982;&#39118;&#38505;&#30340;&#27010;&#24565;&#23494;&#20999;&#30456;&#20851;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#22312;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#19979;&#65292;&#36890;&#36807;&#26041;&#24046;&#39118;&#38505;&#20934;&#21017;&#30740;&#31350;&#39118;&#38505;&#25935;&#24863;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#20010;&#26041;&#24046;&#32422;&#26463;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#31574;&#30053;&#65292;&#26368;&#22823;&#21270;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#30340;&#26399;&#26395;&#20540;&#65292;&#24182;&#19988;&#20351;&#24471;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#30340;&#26041;&#24046;&#19978;&#30028;&#19981;&#36229;&#36807;&#26576;&#20010;&#38408;&#20540;&#12290;&#21033;&#29992;Lagrange&#21644;Fenchel&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#38797;&#28857;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#21644;&#39640;&#25928;&#26356;&#26032;&#31574;&#30053;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep reinforcement learning has achieved tremendous successes in various applications, most existing works only focus on maximizing the expected value of total return and thus ignore its inherent stochasticity. Such stochasticity is also known as the aleatoric uncertainty and is closely related to the notion of risk. In this work, we make the first attempt to study risk-sensitive deep reinforcement learning under the average reward setting with the variance risk criteria. In particular, we focus on a variance-constrained policy optimization problem where the goal is to find a policy that maximizes the expected value of the long-run average reward, subject to a constraint that the long-run variance of the average reward is upper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we transform the original problem into an unconstrained saddle-point policy optimization problem, and propose an actor-critic algorithm that iteratively and efficiently updates the policy,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#25915;&#20987;&#20998;&#31867;&#21644;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#25915;&#20987;&#30340;&#21407;&#22240;&#21644;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2007.07646</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#38544;&#31169;&#25915;&#20987;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Privacy Attacks in Machine Learning. (arXiv:2007.07646v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.07646
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#25915;&#20987;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#25552;&#20986;&#20102;&#25915;&#20987;&#20998;&#31867;&#21644;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#20998;&#26512;&#20102;&#19981;&#21516;&#25915;&#20987;&#30340;&#21407;&#22240;&#21644;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#30740;&#31350;&#20854;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#24433;&#21709;&#21464;&#24471;&#26356;&#21152;&#32039;&#36843;&#12290;&#23613;&#31649;&#22312;&#38544;&#31169;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#31283;&#27493;&#22686;&#38271;&#65292;&#20294;&#19982;&#23433;&#20840;&#26041;&#38754;&#30456;&#27604;&#65292;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#38544;&#31169;&#26041;&#38754;&#30740;&#31350;&#30340;&#20851;&#27880;&#24230;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#26159;&#23545;&#36807;&#21435;&#19971;&#24180;&#21457;&#34920;&#30340;&#19982;&#26426;&#22120;&#23398;&#20064;&#38544;&#31169;&#25915;&#20987;&#30456;&#20851;&#30340;40&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25915;&#20987;&#20998;&#31867;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#23041;&#32961;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#23545;&#25163;&#30693;&#35782;&#21644;&#21463;&#25915;&#20987;&#36164;&#20135;&#23545;&#19981;&#21516;&#25915;&#20987;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#35752;&#65292;&#28982;&#21518;&#23545;&#19981;&#21516;&#30340;&#25915;&#20987;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#26368;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#20998;&#26512;&#36807;&#31243;&#20013;&#21457;&#29616;&#30340;&#24320;&#25918;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 40 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22686;&#21152;&#27744;&#21270;&#23618;&#21644;&#37325;&#24314;&#23618;&#26469;&#25913;&#36827;&#33014;&#22218;&#32593;&#32476;&#65288;CN&#65289;&#30340;&#35774;&#35745;&#65292;&#20197;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#23545;&#27604;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;CN&#22312;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;DL&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/1903.07497</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20808;&#36827;&#33014;&#22218;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Advanced Capsule Networks via Context Awareness. (arXiv:1903.07497v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1903.07497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22686;&#21152;&#27744;&#21270;&#23618;&#21644;&#37325;&#24314;&#23618;&#26469;&#25913;&#36827;&#33014;&#22218;&#32593;&#32476;&#65288;CN&#65289;&#30340;&#35774;&#35745;&#65292;&#20197;&#36866;&#24212;&#20855;&#26377;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#23545;&#27604;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;CN&#22312;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#20102;&#19982;DL&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;&#65288;CN&#65289;&#20026;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31038;&#21306;&#25552;&#20379;&#20102;&#26032;&#30340;&#26550;&#26500;&#12290;&#23613;&#31649;&#23427;&#30340;&#26377;&#25928;&#24615;&#24050;&#32463;&#22312;MNIST&#21644;smallNORB&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#20294;&#26159;&#23545;&#20110;&#20855;&#26377;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#32593;&#32476;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;CN&#65288;&#21521;&#37327;&#29256;&#26412;&#65289;&#30340;&#35774;&#35745;&#65292;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#27744;&#21270;&#23618;&#26469;&#36807;&#28388;&#22270;&#20687;&#32972;&#26223;&#65292;&#24182;&#22686;&#21152;&#20102;&#26356;&#22810;&#30340;&#37325;&#24314;&#23618;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;CN&#21644;DL&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;&#22312;DL&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;&#22312;&#24378;&#22823;&#30340;&#35745;&#31639;&#26426;&#19978;&#20351;&#29992;Inception V3&#21644;DenseNet V201&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;NASNet&#12289;MobileNet V1&#21644;MobileNet V2&#26469;&#36866;&#29992;&#20110;&#23567;&#22411;&#21644;&#23884;&#20837;&#24335;&#35774;&#22791;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#30340;&#25163;&#25351;&#25340;&#20889;&#23383;&#27597;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CN&#19982;DL&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#22823;&#22823;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#38142;&#25509;&#20197;&#36827;&#34892;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capsule Networks (CN) offer new architectures for Deep Learning (DL) community. Though its effectiveness has been demonstrated in MNIST and smallNORB datasets, the networks still face challenges in other datasets for images with distinct contexts. In this research, we improve the design of CN (Vector version) namely we expand more Pooling layers to filter image backgrounds and increase Reconstruction layers to make better image restoration. Additionally, we perform experiments to compare accuracy and speed of CN versus DL models. In DL models, we utilize Inception V3 and DenseNet V201 for powerful computers besides NASNet, MobileNet V1 and MobileNet V2 for small and embedded devices. We evaluate our models on a fingerspelling alphabet dataset from American Sign Language (ASL). The results show that CNs perform comparably to DL models while dramatically reducing training time. We also make a demonstration and give a link for the purpose of illustration.
&lt;/p&gt;</description></item></channel></rss>