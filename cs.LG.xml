<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#30340;&#21464;&#25442;&#20998;&#24067;&#65292;&#24182;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25512;&#24191;&#19981;&#21464;&#24615;&#30340;&#31867;&#21035;&#38388;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23039;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2309.16672</link><description>&lt;p&gt;
&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning to Transform for Generalizable Instance-wise Invariance. (arXiv:2309.16672v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16672
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#36716;&#25442;&#20197;&#23454;&#29616;&#36890;&#29992;&#30340;&#23454;&#20363;&#19981;&#21464;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#30340;&#21464;&#25442;&#20998;&#24067;&#65292;&#24182;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#24179;&#22343;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#23454;&#20363;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20174;&#32780;&#25512;&#24191;&#19981;&#21464;&#24615;&#30340;&#31867;&#21035;&#38388;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#23039;&#21183;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#26356;&#24191;&#27867;&#30340;&#21464;&#25442;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#26500;&#24314;&#23545;&#33258;&#28982;&#25968;&#25454;&#20013;&#30340;&#31354;&#38388;&#21464;&#25442;&#20855;&#26377;&#24378;&#40065;&#26834;&#24615;&#30340;&#31995;&#32479;&#12290;&#20256;&#32479;&#19978;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25110;&#23558;&#19981;&#21464;&#24615;&#30828;&#32534;&#30721;&#21040;&#26550;&#26500;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#28982;&#32780;&#65292;&#36807;&#22810;&#25110;&#36807;&#23569;&#30340;&#19981;&#21464;&#24615;&#37117;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#65292;&#27491;&#30830;&#30340;&#19981;&#21464;&#24615;&#31243;&#24230;&#22312;&#20808;&#39564;&#20013;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#23454;&#20363;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#24212;&#35813;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#36866;&#24403;&#30340;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#27979;&#35797;&#26102;&#25512;&#26029;&#12290;&#25105;&#20204;&#23558;&#19981;&#21464;&#24615;&#35270;&#20026;&#19968;&#20010;&#39044;&#27979;&#38382;&#39064;&#12290;&#32473;&#23450;&#20219;&#20309;&#22270;&#20687;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24402;&#19968;&#21270;&#27969;&#26469;&#39044;&#27979;&#21464;&#25442;&#30340;&#20998;&#24067;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;&#39044;&#27979;&#36827;&#34892;&#24179;&#22343;&#12290;&#30001;&#20110;&#36825;&#20010;&#20998;&#24067;&#20165;&#21462;&#20915;&#20110;&#23454;&#20363;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#20998;&#31867;&#20043;&#21069;&#23545;&#23454;&#20363;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#22312;&#31867;&#21035;&#20043;&#38388;&#25512;&#24191;&#19981;&#21464;&#24615;&#12290;&#21516;&#26679;&#30340;&#20998;&#24067;&#20063;&#21487;&#20197;&#29992;&#20110;&#36866;&#24212;&#36229;&#20986;&#20998;&#24067;&#30340;&#23039;&#21183;&#12290;&#36825;&#20010;&#24402;&#19968;&#21270;&#27969;&#26159;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#23398;&#20064;&#27604;Augerino&#21644;InstaAug&#26356;&#22810;&#33539;&#22260;&#30340;&#21464;&#25442;&#12290;&#24403;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#26102;&#65292;&#25105;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
Computer vision research has long aimed to build systems that are robust to spatial transformations found in natural data. Traditionally, this is done using data augmentation or hard-coding invariances into the architecture. However, too much or too little invariance can hurt, and the correct amount is unknown a priori and dependent on the instance. Ideally, the appropriate invariance would be learned from data and inferred at test-time.  We treat invariance as a prediction problem. Given any image, we use a normalizing flow to predict a distribution over transformations and average the predictions over them. Since this distribution only depends on the instance, we can align instances before classifying them and generalize invariance across classes. The same distribution can also be used to adapt to out-of-distribution poses. This normalizing flow is trained end-to-end and can learn a much larger range of transformations than Augerino and InstaAug. When used as data augmentation, our m
&lt;/p&gt;</description></item><item><title>RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.16668</link><description>&lt;p&gt;
RealFill&#65306;&#21442;&#32771;&#39537;&#21160;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RealFill: Reference-Driven Generation for Authentic Image Completion. (arXiv:2309.16668v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16668
&lt;/p&gt;
&lt;p&gt;
RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#65292;&#33021;&#22815;&#20197;&#30495;&#23454;&#12289;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#30340;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#22270;&#20687;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#33021;&#22815;&#22312;&#26410;&#30693;&#21306;&#22495;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#22270;&#20687;&#20869;&#23481;&#30340;&#22806;&#25299;&#21644;&#20462;&#22635;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#20869;&#23481;&#26159;&#19981;&#30495;&#23454;&#30340;&#65292;&#22240;&#20026;&#27169;&#22411;&#32570;&#20047;&#20851;&#20110;&#30495;&#23454;&#22330;&#26223;&#30340;&#36275;&#22815;&#32972;&#26223;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30495;&#23454;&#22270;&#20687;&#20462;&#22797;&#29983;&#25104;&#26041;&#27861;RealFill&#65292;&#23427;&#36890;&#36807;&#22635;&#20805;&#22270;&#20687;&#20013;&#32570;&#22833;&#21306;&#22495;&#20351;&#20854;&#20869;&#23481;&#30495;&#27491;&#24212;&#22312;&#30340;&#20869;&#23481;&#12290;RealFill&#26159;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#29983;&#25104;&#20462;&#22635;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#20960;&#24352;&#30446;&#26631;&#22330;&#26223;&#30340;&#21442;&#32771;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#12290;&#36825;&#20123;&#21442;&#32771;&#22270;&#20687;&#19981;&#38656;&#35201;&#19982;&#30446;&#26631;&#22270;&#20687;&#23545;&#40784;&#65292;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#12289;&#20809;&#29031;&#26465;&#20214;&#12289;&#25668;&#20687;&#26426;&#20809;&#22280;&#25110;&#22270;&#20687;&#39118;&#26684;&#25293;&#25668;&#12290;&#20010;&#24615;&#21270;&#21518;&#65292;RealFill&#33021;&#22815;&#20197;&#35270;&#35273;&#19978;&#24341;&#20154;&#27880;&#30446;&#30340;&#20869;&#23481;&#23436;&#25104;&#30446;&#26631;&#22270;&#20687;&#65292;&#24182;&#19988;&#24544;&#23454;&#20110;&#21407;&#22987;&#22330;&#26223;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#38754;&#19988;&#20855;&#25361;&#25112;&#24615;&#30340;&#22270;&#20687;&#20462;&#22797;&#22522;&#20934;&#19978;&#23545;RealFill&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions, but the content these models hallucinate is necessarily inauthentic, since the models lack sufficient context about the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging
&lt;/p&gt;</description></item><item><title>HyperPPO&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#23567;&#31574;&#30053;&#12290;&#23427;&#21033;&#29992;&#22270;&#29366;&#36229;&#32593;&#32476;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#33719;&#24471;&#24615;&#33021;&#20248;&#31168;&#30340;&#31574;&#30053;&#65292;&#24182;&#33021;&#22815;&#28385;&#36275;&#29992;&#25143;&#30340;&#35745;&#31639;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2309.16663</link><description>&lt;p&gt;
HyperPPO:&#19968;&#31181;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#23547;&#25214;&#23567;&#31574;&#30053;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperPPO: A scalable method for finding small policies for robotic control. (arXiv:2309.16663v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16663
&lt;/p&gt;
&lt;p&gt;
HyperPPO&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#23567;&#31574;&#30053;&#12290;&#23427;&#21033;&#29992;&#22270;&#29366;&#36229;&#32593;&#32476;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26435;&#37325;&#65292;&#21487;&#20197;&#33719;&#24471;&#24615;&#33021;&#20248;&#31168;&#30340;&#31574;&#30053;&#65292;&#24182;&#33021;&#22815;&#28385;&#36275;&#29992;&#25143;&#30340;&#35745;&#31639;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35760;&#24518;&#26377;&#38480;&#30340;&#39640;&#24615;&#33021;&#26426;&#22120;&#20154;&#30340;&#31070;&#32463;&#25511;&#21046;&#65292;&#38656;&#35201;&#20855;&#26377;&#36739;&#23569;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#23547;&#25214;&#36825;&#20123;&#36739;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21487;&#33021;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;HyperPPO&#65292;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#19978;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#22270;&#29366;&#36229;&#32593;&#32476;&#21516;&#26102;&#20272;&#35745;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20272;&#35745;&#30340;&#32593;&#32476;&#26435;&#37325;&#35201;&#36828;&#23567;&#20110;&#24120;&#29992;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20294;&#21364;&#33021;&#32534;&#30721;&#39640;&#24615;&#33021;&#31574;&#30053;&#12290;&#25105;&#20204;&#21516;&#26102;&#33719;&#24471;&#22810;&#20010;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#65292;&#24182;&#20445;&#25345;&#37319;&#26679;&#25928;&#29575;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#36873;&#25321;&#36866;&#21512;&#20854;&#35745;&#31639;&#32422;&#26463;&#26465;&#20214;&#30340;&#32593;&#32476;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;-&#26356;&#22810;&#30340;&#35757;&#32451;&#36164;&#28304;&#20250;&#20135;&#29983;&#26356;&#24555;&#25910;&#25947;&#21040;&#26356;&#39640;&#24615;&#33021;&#26550;&#26500;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#30001;HyperPPO&#20272;&#35745;&#30340;&#31070;&#32463;&#31574;&#30053;&#33021;&#22815;&#20998;&#25955;&#25511;&#21046;Crazyflie2.1&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models with fewer parameters are necessary for the neural control of memory-limited, performant robots. Finding these smaller neural network architectures can be time-consuming. We propose HyperPPO, an on-policy reinforcement learning algorithm that utilizes graph hypernetworks to estimate the weights of multiple neural architectures simultaneously. Our method estimates weights for networks that are much smaller than those in common-use networks yet encode highly performant policies. We obtain multiple trained policies at the same time while maintaining sample efficiency and provide the user the choice of picking a network architecture that satisfies their computational constraints. We show that our method scales well - more training resources produce faster convergence to higher-performing architectures. We demonstrate that the neural policies estimated by HyperPPO are capable of decentralized control of a Crazyflie2.1 quadrotor. Website: https://sites.google.com/usc.edu/hyperppo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#31350;&#20102;&#22899;&#24615;&#22823;&#33041;&#22312;&#26376;&#32463;&#26399;&#38388;&#30340;3D&#24418;&#29366;&#21464;&#21270;&#65292;&#36890;&#36807;&#22320;&#29702;&#22238;&#24402;&#26041;&#27861;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#20379;&#31934;&#30830;&#24230;&#19982;&#36895;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26681;&#25454;&#21512;&#25104;&#25968;&#25454;&#27979;&#35797;&#32467;&#26524;&#65292;&#21487;&#20197;&#22312;&#29306;&#29298;&#23569;&#37096;&#20998;&#31934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26174;&#33879;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.16662</link><description>&lt;p&gt;
&#39318;&#27425;&#30740;&#31350;&#22899;&#24615;&#22823;&#33041;&#22312;&#26376;&#32463;&#26399;&#38388;&#30340;3D&#24418;&#29366;&#21464;&#21270;&#30340;&#22320;&#29702;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Geodesic Regression Characterizes 3D Shape Changes in the Female Brain During Menstruation. (arXiv:2309.16662v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#25506;&#31350;&#20102;&#22899;&#24615;&#22823;&#33041;&#22312;&#26376;&#32463;&#26399;&#38388;&#30340;3D&#24418;&#29366;&#21464;&#21270;&#65292;&#36890;&#36807;&#22320;&#29702;&#22238;&#24402;&#26041;&#27861;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#20379;&#31934;&#30830;&#24230;&#19982;&#36895;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26681;&#25454;&#21512;&#25104;&#25968;&#25454;&#27979;&#35797;&#32467;&#26524;&#65292;&#21487;&#20197;&#22312;&#29306;&#29298;&#23569;&#37096;&#20998;&#31934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26174;&#33879;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22899;&#24615;&#22312;&#32477;&#32463;&#21518;&#26356;&#23481;&#26131;&#24739;&#19978;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#20854;&#20182;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#65292;&#28982;&#32780;&#65292;&#23558;&#22899;&#24615;&#22823;&#33041;&#20581;&#24247;&#19982;&#24615;&#28608;&#32032;&#27874;&#21160;&#32852;&#31995;&#36215;&#26469;&#30340;&#30740;&#31350;&#36824;&#26377;&#38480;&#12290;&#36890;&#36807;&#24320;&#21457;&#24037;&#20855;&#26469;&#37327;&#21270;&#22823;&#33041;&#22312;&#24615;&#28608;&#32032;&#27874;&#21160;&#26399;&#38388;&#21457;&#29983;&#30340;3D&#24418;&#29366;&#21464;&#21270;&#65292;&#25105;&#20204;&#35797;&#22270;&#35843;&#26597;&#36825;&#31181;&#32852;&#31995;&#12290;&#22320;&#29702;&#22238;&#24402;&#22312;3D&#31163;&#25955;&#34920;&#38754;&#31354;&#38388;&#19978;&#25552;&#20379;&#20102;&#21051;&#30011;&#22823;&#33041;&#24418;&#29366;&#28436;&#21464;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#35745;&#31639;&#22797;&#26434;&#24230;&#36807;&#39640;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21152;&#36895;3D&#31163;&#25955;&#34920;&#38754;&#24418;&#29366;&#22320;&#29702;&#22238;&#24402;&#30340;&#36817;&#20284;&#26041;&#26696;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20309;&#26102;&#20351;&#29992;&#27599;&#20010;&#36817;&#20284;&#26041;&#27861;&#30340;&#32463;&#39564;&#35268;&#21017;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#23545;&#36825;&#20123;&#36817;&#20284;&#26041;&#27861;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#37327;&#21270;&#20102;&#36895;&#24230;&#21644;&#31934;&#30830;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#35777;&#26126;&#20174;&#20174;&#20013;&#33719;&#24471;&#38750;&#24120;&#26174;&#33879;&#30340;&#21152;&#36895;&#65292;&#24182;&#21482;&#31245;&#24494;&#29306;&#29298;&#19968;&#20123;&#31934;&#30830;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#22823;&#33041;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Women are at higher risk of Alzheimer's and other neurological diseases after menopause, and yet research connecting female brain health to sex hormone fluctuations is limited. We seek to investigate this connection by developing tools that quantify 3D shape changes that occur in the brain during sex hormone fluctuations. Geodesic regression on the space of 3D discrete surfaces offers a principled way to characterize the evolution of a brain's shape. However, in its current form, this approach is too computationally expensive for practical use. In this paper, we propose approximation schemes that accelerate geodesic regression on shape spaces of 3D discrete surfaces. We also provide rules of thumb for when each approximation can be used. We test our approach on synthetic data to quantify the speed-accuracy trade-off of these approximations and show that practitioners can expect very significant speed-up while only sacrificing little accuracy. Finally, we apply the method to real brain 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#28287;&#30137;&#20998;&#21106;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;&#35270;&#35273;&#27169;&#22411;SegGPT&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#22270;&#20687;&#36827;&#34892;&#28287;&#30137;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.16656</link><description>&lt;p&gt;
&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#28287;&#30137;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visual In-Context Learning for Few-Shot Eczema Segmentation. (arXiv:2309.16656v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16656
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#28287;&#30137;&#20998;&#21106;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#29992;&#35270;&#35273;&#27169;&#22411;SegGPT&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#22270;&#20687;&#36827;&#34892;&#28287;&#30137;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#23383;&#30456;&#26426;&#22270;&#20687;&#20013;&#33258;&#21160;&#35786;&#26029;&#28287;&#30137;&#23545;&#20110;&#24320;&#21457;&#20801;&#35768;&#24739;&#32773;&#33258;&#25105;&#30417;&#27979;&#24674;&#22797;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#26159;&#20174;&#36825;&#20123;&#22270;&#20687;&#20013;&#20998;&#21106;&#28287;&#30137;&#21306;&#22495;&#12290;&#24403;&#21069;&#30340;&#28287;&#30137;&#20998;&#21106;&#26041;&#27861;&#20381;&#36182;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#22914;&#22522;&#20110;&#21367;&#31215;&#65288;CNN&#65289;&#30340;U-Net&#25110;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;Swin U-Net&#12290;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#32780;&#36825;&#24456;&#38590;&#33719;&#24471;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#32972;&#26223;&#19979;&#30340;&#23569;&#26679;&#26412;&#28287;&#30137;&#20998;&#21106;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#20165;&#29992;&#23569;&#37327;&#31034;&#20363;&#36827;&#34892;&#28287;&#30137;&#20998;&#21106;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#28287;&#30137;&#20998;&#21106;&#30340;&#35270;&#35273;&#32972;&#26223;&#19979;&#23398;&#20064;&#31574;&#30053;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;SegGPT&#30340;&#36890;&#29992;&#35270;&#35273;&#27169;&#22411;&#12290;&#22312;&#25317;&#26377;&#27880;&#37322;&#28287;&#30137;&#22270;&#20687;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SegGPT&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;2&#20010;&#20195;&#34920;&#24615;&#31034;&#20363;&#22270;&#20687;&#30340;&#24615;&#33021;&#26356;&#22909;&#65288;mIoU&#65306;36.69&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated diagnosis of eczema from digital camera images is crucial for developing applications that allow patients to self-monitor their recovery. An important component of this is the segmentation of eczema region from such images. Current methods for eczema segmentation rely on deep neural networks such as convolutional (CNN)-based U-Net or transformer-based Swin U-Net. While effective, these methods require high volume of annotated data, which can be difficult to obtain. Here, we investigate the capabilities of visual in-context learning that can perform few-shot eczema segmentation with just a handful of examples and without any need for retraining models. Specifically, we propose a strategy for applying in-context learning for eczema segmentation with a generalist vision model called SegGPT. When benchmarked on a dataset of annotated eczema images, we show that SegGPT with just 2 representative example images from the training dataset performs better (mIoU: 36.69) than a CNN U-Ne
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#39564;&#35777;&#21644;&#37325;&#26032;&#23454;&#29616;&#20316;&#32773;&#25552;&#20986;&#30340;&#29983;&#29289;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;P-NET&#30340;&#26041;&#27861;&#65292;&#37327;&#21270;&#20102;&#20351;&#29992;Reactome&#29983;&#29289;&#36890;&#36335;&#36827;&#34892;&#32593;&#32476;&#31232;&#30095;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#20182;&#31070;&#32463;&#26550;&#26500;&#21644;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.16645</link><description>&lt;p&gt;
&#21487;&#37325;&#22797;&#24615;&#25253;&#21578;&#65306;&#22810;&#26679;&#30340;&#29983;&#29289;&#20449;&#24687;&#21270;&#31070;&#32463;&#32467;&#26500;&#23545;&#21069;&#21015;&#33146;&#30284;&#20998;&#23618;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Reusability report: Prostate cancer stratification with diverse biologically-informed neural architectures. (arXiv:2309.16645v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16645
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#39564;&#35777;&#21644;&#37325;&#26032;&#23454;&#29616;&#20316;&#32773;&#25552;&#20986;&#30340;&#29983;&#29289;&#20449;&#24687;&#21270;&#31070;&#32463;&#32593;&#32476;P-NET&#30340;&#26041;&#27861;&#65292;&#37327;&#21270;&#20102;&#20351;&#29992;Reactome&#29983;&#29289;&#36890;&#36335;&#36827;&#34892;&#32593;&#32476;&#31232;&#30095;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#20182;&#31070;&#32463;&#26550;&#26500;&#21644;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#20102;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;Elmarakeby&#31561;&#20154;&#30340;&#30740;&#31350;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#29983;&#29289;&#20449;&#24687;&#21270;&#12289;&#31232;&#30095;&#36830;&#25509;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;P-NET&#65289;&#26469;&#27169;&#25311;&#21069;&#21015;&#33146;&#30284;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#20195;&#30721;&#21644;&#25105;&#20204;&#33258;&#24049;&#20351;&#29992;&#26356;&#29616;&#20195;&#21270;&#30340;&#24211;&#37325;&#26032;&#23454;&#29616;&#30340;&#26041;&#27861;&#26469;&#39564;&#35777;Elmarakeby&#31561;&#20154;&#30740;&#31350;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#36890;&#36807;Reactome&#29983;&#29289;&#36890;&#36335;&#36827;&#34892;&#32593;&#32476;&#31232;&#30095;&#21270;&#30340;&#36129;&#29486;&#65292;&#24182;&#30830;&#35748;&#20854;&#23545;P-NET&#30340;&#21331;&#36234;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#29983;&#29289;&#20449;&#24687;&#32435;&#20837;&#32593;&#32476;&#30340;&#20854;&#20182;&#31070;&#32463;&#26550;&#26500;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#23581;&#35797;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#20020;&#24202;&#39044;&#27979;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#19981;&#21516;&#32467;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20010;&#20307;&#24739;&#32773;&#36827;&#34892;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In, Elmarakeby et al., "Biologically informed deep neural network for prostate cancer discovery", a feedforward neural network with biologically informed, sparse connections (P-NET) was presented to model the state of prostate cancer. We verified the reproducibility of the study conducted by Elmarakeby et al., using both their original codebase, and our own re-implementation using more up-to-date libraries. We quantified the contribution of network sparsification by Reactome biological pathways, and confirmed its importance to P-NET's superior performance. Furthermore, we explored alternative neural architectures and approaches to incorporating biological information into the networks. We experimented with three types of graph neural networks on the same training data, and investigated the clinical prediction agreement between different models. Our analyses demonstrated that deep neural networks with distinct architectures make incorrect predictions for individual patient that are pers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#31639;&#27861;&#35748;&#35777;&#32473;&#23450;&#31574;&#30053;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>http://arxiv.org/abs/2309.16631</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064; - &#35748;&#35777;&#32622;&#20449;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Robust Offline Reinforcement Learning -- Certify the Confidence Interval. (arXiv:2309.16631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#24179;&#28369;&#31639;&#27861;&#35748;&#35777;&#32473;&#23450;&#31574;&#30053;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#39640;&#25928;&#24615;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;RL&#65292;&#22312;&#30740;&#31350;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25915;&#20987;&#26041;&#24335;&#21464;&#24471;&#25104;&#29087;&#65292;RL&#30340;&#23433;&#20840;&#24615;&#25104;&#20026;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25269;&#24481;&#27492;&#31867;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#12289;&#25968;&#25454;&#36807;&#28388;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;&#32463;&#39564;&#31639;&#27861;&#21644;&#23454;&#39564;&#65292;&#32570;&#20047;&#23545;&#31639;&#27861;&#40065;&#26834;&#24615;&#30340;&#20005;&#26684;&#29702;&#35770;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#32473;&#23450;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#30340;&#25928;&#29575;&#21487;&#20197;&#35777;&#26126;&#21644;&#36827;&#34892;&#65292;&#19982;&#27809;&#26377;&#38543;&#26426;&#24179;&#28369;&#30340;&#31639;&#27861;&#19968;&#26679;&#39640;&#25928;&#12290;&#19981;&#21516;&#29615;&#22659;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, reinforcement learning (RL), especially deep RL, has received more and more attention in the research area. However, the security of RL has been an obvious problem due to the attack manners becoming mature. In order to defend against such adversarial attacks, several practical approaches are developed, such as adversarial training, data filtering, etc. However, these methods are mostly based on empirical algorithms and experiments, without rigorous theoretical analysis of the robustness of the algorithms. In this paper, we develop an algorithm to certify the robustness of a given policy offline with random smoothing, which could be proven and conducted as efficiently as ones without random smoothing. Experiments on different environments confirm the correctness of our algorithm.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;LAD&#27169;&#22411;&#19981;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#20272;&#35745;&#20102;LAD&#27169;&#22411;&#30340;VC&#32500;&#24230;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16630</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;LAD&#36827;&#34892;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
On Learning with LAD. (arXiv:2309.16630v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16630
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;LAD&#27169;&#22411;&#19981;&#23384;&#22312;&#36807;&#25311;&#21512;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#24182;&#20272;&#35745;&#20102;LAD&#27169;&#22411;&#30340;VC&#32500;&#24230;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30340;&#36923;&#36753;&#20998;&#26512;&#65288;LAD&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#20855;&#26377;&#26512;&#21462;&#33539;&#24335;&#65288;DNF&#65289;&#34920;&#31034;&#30340;&#24067;&#23572;&#20989;&#25968;&#30340;&#25216;&#26415;&#65292;&#23427;&#20135;&#29983;&#30340;&#20004;&#31867;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;LAD&#31639;&#27861;&#37319;&#29992;&#20102;&#20248;&#21270;&#25216;&#26415;&#65292;&#20294;&#24471;&#21040;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#25110;&#20108;&#36827;&#21046;&#35268;&#21017;&#19981;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;LAD&#27169;&#22411;&#19981;&#23384;&#22312;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20272;&#35745;&#20102;&#30001;&#23569;&#37327;&#31435;&#26041;&#25968;&#21333;&#39033;&#24335;&#32452;&#25104;&#30340;DNF&#20551;&#35774;&#38598;&#30340;Vapnik-Chervonenkis&#32500;&#24230;&#65288;VC&#32500;&#24230;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The logical analysis of data, LAD, is a technique that yields two-class classifiers based on Boolean functions having disjunctive normal form (DNF) representation. Although LAD algorithms employ optimization techniques, the resulting binary classifiers or binary rules do not lead to overfitting. We propose a theoretical justification for the absence of overfitting by estimating the Vapnik-Chervonenkis dimension (VC dimension) for LAD models where hypothesis sets consist of DNFs with a small number of cubic monomials. We illustrate and confirm our observations empirically.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2309.16620</link><description>&lt;p&gt;
&#27531;&#24046;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#36229;&#21442;&#25968;&#36716;&#31227;&#65306;&#21160;&#24577;&#21644;&#32553;&#25918;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16620
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#30340;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#25104;&#26412;&#19981;&#26029;&#19978;&#21319;&#65292;&#20419;&#20351;&#20174;&#19994;&#32773;&#23547;&#25214;&#20351;&#29992;&#36739;&#23567;&#32593;&#32476;&#30340;&#20195;&#29702;&#26041;&#27861;&#36827;&#34892;&#35843;&#25972;&#12290;&#20854;&#20013;&#19968;&#20010;&#24314;&#35758;&#20351;&#29992;$\mu$P&#21442;&#25968;&#21270;&#32593;&#32476;&#65292;&#20854;&#20013;&#23567;&#23485;&#24230;&#32593;&#32476;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#21040;&#20219;&#24847;&#23485;&#24230;&#30340;&#32593;&#32476;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#36229;&#21442;&#25968;&#19981;&#20250;&#22312;&#19981;&#21516;&#28145;&#24230;&#20043;&#38388;&#36716;&#31227;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;$1/\sqrt{\text{depth}}$&#30340;&#27531;&#24046;&#20998;&#25903;&#23610;&#24230;&#21644;$\mu$P&#21442;&#25968;&#21270;&#30340;&#27531;&#24046;&#32593;&#32476;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36825;&#31181;&#21442;&#25968;&#21270;&#35757;&#32451;&#30340;&#27531;&#24046;&#32467;&#26500;&#65292;&#21253;&#25324;&#21367;&#31215;ResNet&#21644;Vision Transformer&#65292;&#22312;CIFAR-10&#21644;ImageNet&#19978;&#23637;&#31034;&#20102;&#36328;&#23485;&#24230;&#21644;&#28145;&#24230;&#30340;&#26368;&#20339;&#36229;&#21442;&#25968;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32463;&#39564;&#21457;&#29616;&#24471;&#21040;&#20102;&#29702;&#35770;&#30340;&#25903;&#25345;&#21644;&#21160;&#26426;&#12290;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#22343;&#22330;&#29702;&#35770;&#65288;DMFT&#65289;&#25551;&#36848;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
The cost of hyperparameter tuning in deep learning has been rising with model sizes, prompting practitioners to find new tuning methods using a proxy of smaller networks. One such proposal uses $\mu$P parameterized networks, where the optimal hyperparameters for small width networks transfer to networks with arbitrarily large width. However, in this scheme, hyperparameters do not transfer across depths. As a remedy, we study residual networks with a residual branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P parameterization. We provide experiments demonstrating that residual architectures including convolutional ResNets and Vision Transformers trained with this parameterization exhibit transfer of optimal hyperparameters across width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings are supported and motivated by theory. Using recent developments in the dynamical mean field theory (DMFT) description of neural network learning dynamics, we show
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#30340;&#22270;&#30340;&#25193;&#23637;Gromov-Wasserstein&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#36317;&#31163;&#21644;&#37325;&#24515;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16604</link><description>&lt;p&gt;
&#21033;&#29992;&#34701;&#21512;&#32593;&#32476;Gromov-Wasserstein&#36317;&#31163;&#20013;&#30340;&#36793;&#29305;&#24449;&#23545;&#22270;&#36827;&#34892;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Exploiting Edge Features in Graphs with Fused Network Gromov-Wasserstein Distance. (arXiv:2309.16604v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#30340;&#22270;&#30340;&#25193;&#23637;Gromov-Wasserstein&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#36317;&#31163;&#21644;&#37325;&#24515;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#30340;&#25104;&#23545;&#27604;&#36739;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#32858;&#31867;&#12289;&#22522;&#20110;&#26680;&#30340;&#20998;&#31867;/&#22238;&#24402;&#20197;&#21450;&#26368;&#36817;&#30340;&#30417;&#30563;&#22270;&#39044;&#27979;&#12290;&#22270;&#20043;&#38388;&#30340;&#36317;&#31163;&#36890;&#24120;&#20381;&#36182;&#20110;&#36825;&#20123;&#32467;&#26500;&#21270;&#23545;&#35937;&#30340;&#20449;&#24687;&#34920;&#36798;&#65292;&#22914;&#23376;&#32467;&#26500;&#21253;&#25110;&#20854;&#20182;&#22270;&#23884;&#20837;&#12290;&#19968;&#31181;&#26368;&#36817;&#27969;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#22270;&#34920;&#31034;&#20026;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#65292;&#36825;&#26679;&#21487;&#20197;&#25104;&#21151;&#22320;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#65292;&#23427;&#25552;&#20379;&#20102;&#26377;&#24847;&#20041;&#30340;&#36317;&#31163;&#26469;&#27604;&#36739;&#23427;&#20204;&#65306;Gromov-Wasserstein&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31867;&#36317;&#31163;&#24573;&#30053;&#20102;&#36793;&#23646;&#24615;&#65292;&#32780;&#36825;&#23545;&#20110;&#35768;&#22810;&#32467;&#26500;&#21270;&#23545;&#35937;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;Gromov-Wasserstein&#36317;&#31163;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#20855;&#26377;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#30340;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36317;&#31163;&#21644;&#37325;&#24515;&#35745;&#31639;&#30340;&#26032;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#26032;&#36317;&#31163;&#22312;&#22270;&#20986;&#29616;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pairwise comparison of graphs is key to many applications in Machine learning ranging from clustering, kernel-based classification/regression and more recently supervised graph prediction. Distances between graphs usually rely on informative representations of these structured objects such as bag of substructures or other graph embeddings. A recently popular solution consists in representing graphs as metric measure spaces, allowing to successfully leverage Optimal Transport, which provides meaningful distances allowing to compare them: the Gromov-Wasserstein distances. However, this family of distances overlooks edge attributes, which are essential for many structured objects. In this work, we introduce an extension of Gromov-Wasserstein distance for comparing graphs whose both nodes and edges have features. We propose novel algorithms for distance and barycenter computation. We empirically show the effectiveness of the novel distance in learning tasks where graphs occur in either inp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65288;NNBF&#65289;&#29992;&#20110;&#35774;&#35745;&#19978;&#34892;&#25509;&#25910;&#22810;&#29992;&#25143;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MU-SIMO&#65289;&#27874;&#26463;&#25104;&#24418;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16603</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#19978;&#34892;&#22810;&#29992;&#25143;SIMO&#27874;&#26463;&#25104;&#24418;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Uplink Multi-User SIMO Beamforming Design. (arXiv:2309.16603v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65288;NNBF&#65289;&#29992;&#20110;&#35774;&#35745;&#19978;&#34892;&#25509;&#25910;&#22810;&#29992;&#25143;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MU-SIMO&#65289;&#27874;&#26463;&#25104;&#24418;&#65292;&#26088;&#22312;&#36890;&#36807;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#25552;&#20379;&#39640;&#25928;&#30340;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31532;&#20116;&#20195;&#65288;5G&#65289;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#30340;&#36827;&#23637;&#32473;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#25552;&#20986;&#20102;&#26356;&#39640;&#30340;&#35201;&#27714;&#65292;&#21253;&#25324;&#39640;&#25968;&#25454;&#36895;&#29575;&#12289;&#24191;&#27867;&#35206;&#30422;&#12289;&#26368;&#23567;&#24310;&#36831;&#21644;&#39640;&#25928;&#33410;&#33021;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36866;&#24212;&#21160;&#24577;&#26465;&#20214;&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#32570;&#28857;&#65292;&#23548;&#33268;&#29702;&#35770;&#20998;&#26512;&#19982;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#22312;&#26080;&#32447;&#36164;&#28304;&#31649;&#29702;&#26041;&#38754;&#30340;&#23454;&#38469;&#25191;&#34892;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25216;&#26415;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;NNBF&#65292;&#29992;&#20110;&#35774;&#35745;&#19978;&#34892;&#25509;&#25910;&#22810;&#29992;&#25143;&#21333;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MU-SIMO&#65289;&#27874;&#26463;&#25104;&#24418;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#19987;&#27880;&#20110;&#26368;&#22823;&#21270;&#24635;&#36895;&#29575;&#26469;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#21516;&#26102;&#25552;&#20379;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19982;&#29616;&#26377;&#30340;&#21327;&#35758;&#30456;&#27604;&#20855;&#26377;&#36739;&#22823;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of fifth generation (5G) wireless communication networks has created a greater demand for wireless resource management solutions that offer high data rates, extensive coverage, minimal latency and energy-efficient performance. Nonetheless, traditional approaches have shortcomings when it comes to computational complexity and their ability to adapt to dynamic conditions, creating a gap between theoretical analysis and the practical execution of algorithmic solutions for managing wireless resources. Deep learning-based techniques offer promising solutions for bridging this gap with their substantial representation capabilities. We propose a novel unsupervised deep learning framework, which is called NNBF, for the design of uplink receive multi-user single input multiple output (MU-SIMO) beamforming. The primary objective is to enhance the throughput by focusing on maximizing the sum-rate while also offering computationally efficient solution, in contrast to established co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#37319;&#29992;&#21435;&#20559;&#24046;&#26041;&#27861;&#32416;&#27491;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16598</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#39044;&#27979;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Cross-Prediction-Powered Inference. (arXiv:2309.16598v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#37319;&#29992;&#21435;&#20559;&#24046;&#26041;&#27861;&#32416;&#27491;&#39044;&#27979;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#28982;&#32780;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#32463;&#24120;&#38656;&#35201;&#32321;&#29712;&#30340;&#20154;&#24037;&#26631;&#27880;&#25110;&#32773;&#32531;&#24930;&#26114;&#36149;&#30340;&#31185;&#23398;&#27979;&#37327;&#12290;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#31934;&#23494;&#30340;&#39044;&#27979;&#25216;&#26415;&#21487;&#20197;&#24555;&#36895;&#12289;&#24265;&#20215;&#22320;&#20135;&#29983;&#22823;&#37327;&#39044;&#27979;&#26631;&#31614;&#65307;&#20363;&#22914;&#65292;&#39044;&#27979;&#30340;&#34507;&#30333;&#36136;&#32467;&#26500;&#34987;&#29992;&#26469;&#34917;&#20805;&#23454;&#39564;&#24471;&#21040;&#30340;&#32467;&#26500;&#65292;&#21355;&#26143;&#22270;&#20687;&#39044;&#27979;&#30340;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#34987;&#29992;&#26469;&#34917;&#20805;&#20934;&#30830;&#30340;&#35843;&#26597;&#25968;&#25454;&#31561;&#12290;&#30001;&#20110;&#39044;&#27979;&#20855;&#26377;&#19981;&#23436;&#32654;&#21644;&#28508;&#22312;&#20559;&#24046;&#30340;&#29305;&#28857;&#65292;&#36825;&#31181;&#20570;&#27861;&#23545;&#19979;&#28216;&#25512;&#29702;&#30340;&#26377;&#25928;&#24615;&#20135;&#29983;&#20102;&#36136;&#30097;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39044;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#19968;&#20010;&#23567;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#22823;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20132;&#21449;&#39044;&#27979;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#22635;&#34917;&#32570;&#22833;&#30340;&#26631;&#31614;&#65292;&#24182;&#24212;&#29992;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#26469;&#32416;&#27491;&#39044;&#27979;&#19981;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reliable data-driven decision-making hinges on high-quality labeled data, the acquisition of quality labels often involves laborious human annotations or slow and expensive scientific measurements. Machine learning is becoming an appealing alternative as sophisticated predictive techniques are being used to quickly and cheaply produce large amounts of predicted labels; e.g., predicted protein structures are used to supplement experimentally derived structures, predictions of socioeconomic indicators from satellite imagery are used to supplement accurate survey data, and so on. Since predictions are imperfect and potentially biased, this practice brings into question the validity of downstream inferences. We introduce cross-prediction: a method for valid inference powered by machine learning. With a small labeled dataset and a large unlabeled dataset, cross-prediction imputes the missing labels via machine learning and applies a form of debiasing to remedy the prediction inaccurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16597</link><description>&lt;p&gt;
&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Bayesian Optimization on Heterogeneous Search Spaces. (arXiv:2309.16597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MPHD&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#19978;&#23454;&#29616;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#65288;&#36890;&#24120;&#26159;&#39640;&#26031;&#36807;&#31243;&#65289;&#36827;&#34892;&#39034;&#24207;&#20915;&#31574;&#12290;&#20026;&#20102;&#30830;&#20445;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#26469;&#33258;&#8220;&#35757;&#32451;&#8221;&#20989;&#25968;&#30340;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#21160;&#35774;&#35745;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#12290;&#36825;&#20123;&#35757;&#32451;&#20989;&#25968;&#36890;&#24120;&#38656;&#35201;&#19982;&#8220;&#27979;&#35797;&#8221;&#20989;&#25968;&#65288;&#24453;&#20248;&#21270;&#30340;&#40657;&#30418;&#20989;&#25968;&#65289;&#20855;&#26377;&#30456;&#21516;&#30340;&#23450;&#20041;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MPHD&#30340;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#26144;&#23556;&#21040;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#35268;&#33539;&#12290;MPHD&#21487;&#20197;&#19982;&#36125;&#21494;&#26031;&#20248;&#21270;&#26080;&#32541;&#38598;&#25104;&#65292;&#23454;&#29616;&#24322;&#36136;&#25628;&#32034;&#31354;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;MPHD&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) is a popular black-box function optimization method, which makes sequential decisions based on a Bayesian model, typically a Gaussian process (GP), of the function. To ensure the quality of the model, transfer learning approaches have been developed to automatically design GP priors by learning from observations on "training" functions. These training functions are typically required to have the same domain as the "test" function (black-box function to be optimized). In this paper, we introduce MPHD, a model pre-training method on heterogeneous domains, which uses a neural net mapping from domain-specific contexts to specifications of hierarchical GPs. MPHD can be seamlessly integrated with BO to transfer knowledge across heterogeneous search spaces. Our theoretical and empirical results demonstrate the validity of MPHD and its superior performance on challenging black-box function optimization tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.16595</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#22270;&#23398;&#20064;&#65306;&#20309;&#26102;&#20309;&#22320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Effectively Leverage Structural Information for Graph Learning: When and Why. (arXiv:2309.16595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#22270;&#25968;&#25454;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65292;&#32780;LLM&#30340;&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#27809;&#26377;&#26174;&#33879;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#29305;&#21035;&#26159;&#22270;&#25968;&#25454;&#65289;&#19978;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;LLM&#25991;&#29486;&#20013;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#37325;&#35201;&#25968;&#25454;&#24418;&#24577;&#12290;&#25105;&#20204;&#26088;&#22312;&#20102;&#35299;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#20309;&#26102;&#20309;&#22320;&#24341;&#20837;&#22270;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#8220;&#20309;&#26102;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#32534;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#35774;&#32622;&#20013;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#20016;&#23500;&#25110;&#31232;&#32570;&#12290;&#23545;&#20110;&#8220;&#20026;&#20160;&#20040;&#8221;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#24615;&#33021;&#30340;&#20004;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#25968;&#25454;&#27844;&#38706;&#21644;&#21516;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;LLM&#21487;&#20197;&#20174;&#32467;&#26500;&#20449;&#24687;&#20013;&#21463;&#30410;&#65292;&#23588;&#20854;&#26159;&#22312;&#25991;&#26412;&#33410;&#28857;&#29305;&#24449;&#32570;&#20047;&#30340;&#24773;&#20917;&#19979;&#65307;&#65288;ii&#65289;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#35777;&#25454;&#34920;&#26126;LLM&#24615;&#33021;&#19982;&#25968;&#25454;&#27844;&#38706;&#26377;&#26174;&#33879;&#30456;&#20851;&#65307;&#65288;iii&#65289;LLM&#22312;&#30446;&#26631;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#19982;&#27491;&#21521;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies Large Language Models (LLMs) for structured data--particularly graphs--a crucial data modality that remains underexplored in the LLM literature. We aim to understand when and why the incorporation of structural information inherent in graph data can improve the prediction performance of LLMs on node classification tasks. To address the ``when'' question, we examine a variety of prompting methods for encoding structural information, in settings where textual node features are either rich or scarce. For the ``why'' questions, we probe into two potential contributing factors to the LLM performance: data leakage and homophily. Our exploration of these questions reveals that (i) LLMs can benefit from structural information, especially when textual node features are scarce; (ii) there is no substantial evidence indicating that the performance of LLMs is significantly attributed to data leakage; and (iii) the performance of LLMs on a target node is strongly positively relat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#24320;&#21457;&#21487;&#35299;&#37322;&#24615;AI&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;&#24378;&#35843;&#20102;&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#23398;&#20064;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16593</link><description>&lt;p&gt;
&#23548;&#33322;&#21307;&#30103;&#27934;&#35265;&#65306;&#30693;&#35782;&#22270;&#35889;&#20013;&#21487;&#35299;&#37322;&#24615;&#30340;&#40479;&#30640;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Navigating Healthcare Insights: A Birds Eye View of Explainability with Knowledge Graphs. (arXiv:2309.16593v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24433;&#21709;&#20197;&#21450;&#24320;&#21457;&#21487;&#35299;&#37322;&#24615;AI&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;&#24378;&#35843;&#20102;&#36890;&#36807;&#30693;&#35782;&#27880;&#20837;&#23398;&#20064;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#20154;&#24037;&#26234;&#33021;&#20013;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#65292;&#29305;&#21035;&#26159;&#22312;&#33647;&#29289;&#21457;&#29616;&#21644;&#21046;&#33647;&#30740;&#31350;&#20013;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#26469;&#25972;&#21512;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#28304;&#65292;&#22686;&#24378;&#20102;AI&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#36825;&#31181;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#20449;&#20219;&#21644;&#36879;&#26126;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#25903;&#25345;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#20915;&#31574;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24433;&#21709;&#21450;&#20854;&#22312;&#24320;&#21457;&#21487;&#35299;&#37322;&#24615;AI&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#30340;&#26368;&#26032;&#25991;&#29486;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#21253;&#25324;&#26500;&#24314;&#12289;&#20851;&#31995;&#25552;&#21462;&#12289;&#25512;&#29702;&#20197;&#21450;&#23427;&#20204;&#22312;&#33647;&#29289;-&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#12289;&#33647;&#29289;&#38774;&#28857;&#30456;&#20114;&#20316;&#29992;&#12289;&#33647;&#29289;&#24320;&#21457;&#12289;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24378;&#35843;&#36890;&#36807;&#22312;&#21307;&#30103;&#20013;&#36827;&#34892;&#30693;&#35782;&#27880;&#20837;&#23398;&#20064;&#26469;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#30740;&#31350;&#25361;&#25112;&#24182;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are gaining prominence in Healthcare AI, especially in drug discovery and pharmaceutical research as they provide a structured way to integrate diverse information sources, enhancing AI system interpretability. This interpretability is crucial in healthcare, where trust and transparency matter, and eXplainable AI (XAI) supports decision making for healthcare professionals. This overview summarizes recent literature on the impact of KGs in healthcare and their role in developing explainable AI models. We cover KG workflow, including construction, relationship extraction, reasoning, and their applications in areas like Drug-Drug Interactions (DDI), Drug Target Interactions (DTI), Drug Development (DD), Adverse Drug Reactions (ADR), and bioinformatics. We emphasize the importance of making KGs more interpretable through knowledge-infused learning in healthcare. Finally, we highlight research challenges and provide insights for future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#21463;&#38480;&#32418;&#22806;&#30446;&#26631;&#26816;&#27979;&#20013;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;RGB&#27169;&#24577;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#25193;&#23637;&#21040;&#32418;&#22806;&#27169;&#24577;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;RGB&#27169;&#24577;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;IR&#27169;&#24577;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#32418;&#22806;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16592</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#30340;&#24352;&#37327;&#20998;&#35299;&#22312;&#25968;&#25454;&#21463;&#38480;&#32418;&#22806;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection. (arXiv:2309.16592v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#21463;&#38480;&#32418;&#22806;&#30446;&#26631;&#26816;&#27979;&#20013;&#21033;&#29992;&#36328;&#27169;&#24577;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;RGB&#27169;&#24577;&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#25193;&#23637;&#21040;&#32418;&#22806;&#27169;&#24577;&#65292;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;RGB&#27169;&#24577;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;IR&#27169;&#24577;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#32418;&#22806;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#22270;&#20687;&#35782;&#21035;&#24615;&#33021;&#36739;&#24046;&#30340;&#20027;&#35201;&#29942;&#39048;&#26159;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#30001;&#20110;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#30340;&#25104;&#26412;&#39640;&#26114;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#35782;&#21040;RGB&#27169;&#24577;&#30340;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#30456;&#24403;&#31283;&#20581;&#65288;&#33267;&#23569;&#23545;&#20110;&#19968;&#20123;&#24120;&#35265;&#31867;&#21035;&#65292;&#22914;&#20154;&#12289;&#36710;&#31561;&#65289;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#29616;&#26377;&#30340;&#24040;&#22823;&#35757;&#32451;&#38598;&#65292;&#22240;&#27492;&#25105;&#20204;&#23581;&#35797;&#21033;&#29992;RGB&#27169;&#24577;&#30340;&#32447;&#32034;&#26469;&#25193;&#23637;&#30446;&#26631;&#26816;&#27979;&#22120;&#21040;&#32418;&#22806;&#27169;&#24577;&#65292;&#21516;&#26102;&#20445;&#25345;RGB&#27169;&#24577;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#31216;&#20026;TensorFact&#65292;&#23427;&#23558;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#19968;&#23618;&#21367;&#31215;&#26680;&#20998;&#35299;&#20026;&#20855;&#26377;&#27604;&#21407;&#22987;CNN&#26356;&#23569;&#21442;&#25968;&#30340;&#20302;&#31209;&#22240;&#23376;&#30697;&#38453;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;RGB&#27169;&#24577;&#19978;&#39044;&#35757;&#32451;&#36825;&#20123;&#22240;&#23376;&#30697;&#38453;&#65292;&#20551;&#23450;&#23384;&#22312;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28982;&#21518;&#20165;&#22312;IR&#27169;&#24577;&#19978;&#22686;&#21152;&#23569;&#37327;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20197;&#36991;&#20813;&#36807;&#25311;&#21512;&#65292;&#21516;&#26102;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The primary bottleneck towards obtaining good recognition performance in IR images is the lack of sufficient labeled training data, owing to the cost of acquiring such data. Realizing that object detection methods for the RGB modality are quite robust (at least for some commonplace classes, like person, car, etc.), thanks to the giant training sets that exist, in this work we seek to leverage cues from the RGB modality to scale object detectors to the IR modality, while preserving model performance in the RGB modality. At the core of our method, is a novel tensor decomposition method called TensorFact which splits the convolution kernels of a layer of a Convolutional Neural Network (CNN) into low-rank factor matrices, with fewer parameters than the original CNN. We first pretrain these factor matrices on the RGB modality, for which plenty of training data are assumed to exist and then augment only a few trainable parameters for training on the IR modality to avoid over-fitting, while e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16584</link><description>&lt;p&gt;
&#29992;&#20110;&#24320;&#21457;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#35774;&#35745;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
A Design Toolbox for the Development of Collaborative Distributed Machine Learning Systems. (arXiv:2309.16584v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;&#24320;&#21457;&#32773;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#30340;&#21516;&#26102;&#21033;&#29992;&#26469;&#33258;&#22810;&#26041;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#20805;&#20998;&#35757;&#32451;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21508;&#31181;&#21327;&#20316;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;CDML&#65289;&#31995;&#32479;&#35774;&#35745;&#65292;&#20363;&#22914;&#36741;&#21161;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#20998;&#35010;&#23398;&#20064;&#12290;CDML&#31995;&#32479;&#35774;&#35745;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#39640;&#24230;&#30340;&#20195;&#29702;&#20154;&#33258;&#27835;&#24615;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26426;&#23494;&#24615;&#21644;&#23481;&#38169;&#24615;&#12290;&#38754;&#23545;&#19981;&#21516;&#29305;&#24449;&#30340;&#21508;&#31181;CDML&#31995;&#32479;&#35774;&#35745;&#65292;&#24320;&#21457;&#32773;&#24456;&#38590;&#26377;&#38024;&#23545;&#24615;&#22320;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#19981;&#21512;&#36866;&#30340;CDML&#31995;&#32479;&#35774;&#35745;&#21487;&#33021;&#23548;&#33268;CDML&#31995;&#32479;&#26080;&#27861;&#23454;&#29616;&#20854;&#39044;&#26399;&#30446;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#25351;&#23548;CDML&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#22522;&#20110;CDML&#35774;&#35745;&#24037;&#20855;&#31665;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#20851;&#38190;&#29305;&#24449;&#30340;CDML&#31995;&#32479;&#20856;&#22411;&#65292;&#21487;&#20197;&#25903;&#25345;&#35774;&#35745;&#28385;&#36275;&#29992;&#20363;&#35201;&#27714;&#30340;CDML&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
To leverage training data for the sufficient training of ML models from multiple parties in a confidentiality-preserving way, various collaborative distributed machine learning (CDML) system designs have been developed, for example, to perform assisted learning, federated learning, and split learning. CDML system designs show different traits, for example, high agent autonomy, machine learning (ML) model confidentiality, and fault tolerance. Facing a wide variety of CDML system designs with different traits, it is difficult for developers to design CDML systems with traits that match use case requirements in a targeted way. However, inappropriate CDML system designs may result in CDML systems failing their envisioned purposes. We developed a CDML design toolbox that can guide the development of CDML systems. Based on the CDML design toolbox, we present CDML system archetypes with distinct key traits that can support the design of CDML systems to meet use case requirements.
&lt;/p&gt;</description></item><item><title>M-OFDFT&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#20998;&#23376;&#31995;&#32479;&#38382;&#39064;&#30340;OFDFT&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#23616;&#22495;&#24615;&#24314;&#31435;&#22312;&#27169;&#22411;&#20013;&#24182;&#20351;&#29992;&#32039;&#20945;&#30340;&#23494;&#24230;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#19982;Kohn-Sham DFT&#30456;&#36817;&#30340;&#31934;&#30830;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16578</link><description>&lt;p&gt;
M-OFDFT&#65306;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#20811;&#26381;&#20998;&#23376;&#31995;&#32479;&#20013;&#30340;&#26080;&#36712;&#36947;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
M-OFDFT: Overcoming the Barrier of Orbital-Free Density Functional Theory for Molecular Systems Using Deep Learning. (arXiv:2309.16578v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16578
&lt;/p&gt;
&lt;p&gt;
M-OFDFT&#26159;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#20998;&#23376;&#31995;&#32479;&#38382;&#39064;&#30340;OFDFT&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#23616;&#22495;&#24615;&#24314;&#31435;&#22312;&#27169;&#22411;&#20013;&#24182;&#20351;&#29992;&#32039;&#20945;&#30340;&#23494;&#24230;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#19982;Kohn-Sham DFT&#30456;&#36817;&#30340;&#31934;&#30830;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#33391;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#36712;&#36947;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;OFDFT&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#36739;&#20302;&#36816;&#31639;&#25104;&#26412;&#30340;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#26041;&#27861;&#65292;&#27604;&#36215;&#24120;&#29992;&#30340;Kohn-Sham&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#26356;&#21152;&#36866;&#29992;&#20110;&#24403;&#20195;&#20998;&#23376;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;OFDFT&#30340;&#31934;&#30830;&#24615;&#21463;&#21040;&#20102;&#21160;&#33021;&#23494;&#24230;&#27867;&#20989;&#30340;&#38480;&#21046;&#65292;&#23545;&#20110;&#38750;&#21608;&#26399;&#24615;&#20998;&#23376;&#31995;&#32479;&#30340;&#36817;&#20284;&#27714;&#35299;&#38750;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;M-OFDFT&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#20989;&#25968;&#27169;&#22411;&#35299;&#20915;&#20102;&#20998;&#23376;&#31995;&#32479;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#24517;&#35201;&#30340;&#38750;&#23616;&#22495;&#24615;&#24314;&#31435;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#21407;&#23376;&#22522;&#19979;&#30340;&#23637;&#24320;&#31995;&#25968;&#20316;&#20026;&#32039;&#20945;&#30340;&#23494;&#24230;&#34920;&#31034;&#26469;&#38477;&#20302;&#25104;&#26412;&#12290;&#36890;&#36807;&#35299;&#20915;&#20854;&#20013;&#30340;&#38750;&#20256;&#32479;&#23398;&#20064;&#25361;&#25112;&#30340;&#25216;&#26415;&#65292;M-OFDFT&#22312;&#19968;&#31995;&#21015;OFDFT&#26080;&#27861;&#35302;&#21450;&#30340;&#20998;&#23376;&#19978;&#23454;&#29616;&#20102;&#19982;Kohn-Sham DFT&#30456;&#24403;&#30340;&#31934;&#30830;&#24230;&#12290;&#26356;&#26377;&#21560;&#24341;&#21147;&#30340;&#26159;&#65292;M-OFDFT&#22312;&#35757;&#32451;&#26102;&#23646;&#20110;&#26356;&#22823;&#30340;&#20998;&#23376;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#22823;&#20998;&#23376;&#25552;&#20379;&#20102;&#26377;&#21560;&#24341;&#21147;&#30340;&#35268;&#27169;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Orbital-free density functional theory (OFDFT) is a quantum chemistry formulation that has a lower cost scaling than the prevailing Kohn-Sham DFT, which is increasingly desired for contemporary molecular research. However, its accuracy is limited by the kinetic energy density functional, which is notoriously hard to approximate for non-periodic molecular systems. In this work, we propose M-OFDFT, an OFDFT approach capable of solving molecular systems using a deep-learning functional model. We build the essential nonlocality into the model, which is made affordable by the concise density representation as expansion coefficients under an atomic basis. With techniques to address unconventional learning challenges therein, M-OFDFT achieves a comparable accuracy with Kohn-Sham DFT on a wide range of molecules untouched by OFDFT before. More attractively, M-OFDFT extrapolates well to molecules much larger than those in training, which unleashes the appealing scaling for studying large molecu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#20248;&#21270;&#30340;&#27169;&#22411;&#32534;&#35793;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20391;&#20449;&#36947;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#25928;&#26524;&#38477;&#20302;&#20102;43&#65285;&#12290;</title><link>http://arxiv.org/abs/2309.16577</link><description>&lt;p&gt;
&#20197;&#32534;&#35793;&#20026;&#38450;&#24481;&#65306;&#36890;&#36807;&#24352;&#37327;&#20248;&#21270;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25915;&#20987;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Compilation as a Defense: Enhancing DL Model Attack Robustness via Tensor Optimization. (arXiv:2309.16577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#24352;&#37327;&#20248;&#21270;&#30340;&#27169;&#22411;&#32534;&#35793;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#20391;&#20449;&#36947;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#25928;&#26524;&#38477;&#20302;&#20102;43&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;(AML)&#26159;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#23433;&#20840;&#30740;&#31350;&#39046;&#22495;&#65292;&#20854;&#20013;&#24120;&#34987;&#24573;&#35270;&#30340;&#39046;&#22495;&#26159;&#36890;&#36807;&#20391;&#20449;&#36947;&#36827;&#34892;&#27169;&#22411;&#25915;&#20987;&#12290;&#27492;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#36825;&#31181;&#25915;&#20987;&#26159;&#20005;&#37325;&#23041;&#32961;&#65292;&#20294;&#22312;&#36991;&#20813;&#26114;&#36149;&#30340;&#27169;&#22411;&#37325;&#26032;&#35774;&#35745;&#30340;&#39640;&#25928;&#34917;&#25937;&#31574;&#30053;&#26041;&#38754;&#36827;&#23637;&#29978;&#24494;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;AML&#20391;&#20449;&#36947;&#25915;&#20987;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21033;&#29992;&#27169;&#22411;&#32534;&#35793;&#25216;&#26415;&#65292;&#21363;&#24352;&#37327;&#20248;&#21270;&#12290;&#25105;&#20204;&#36890;&#36807;&#24352;&#37327;&#20248;&#21270;&#23637;&#31034;&#20102;&#25915;&#20987;&#25928;&#26524;&#30340;&#30456;&#23545;&#38477;&#20302;&#36798;43&#65285;&#65292;&#35752;&#35770;&#20102;&#20854;&#21547;&#20041;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning (AML) is a rapidly growing field of security research, with an often overlooked area being model attacks through side-channels. Previous works show such attacks to be serious threats, though little progress has been made on efficient remediation strategies that avoid costly model re-engineering. This work demonstrates a new defense against AML side-channel attacks using model compilation techniques, namely tensor optimization. We show relative model attack effectiveness decreases of up to 43% using tensor optimization, discuss the implications, and direction of future work.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#22686;&#26448;&#21046;&#36896;&#12290;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#26159;&#19968;&#31867;&#20855;&#26377;&#24179;&#28369;&#24615;&#36136;&#36807;&#28193;&#30340;&#39640;&#32423;&#22797;&#21512;&#26448;&#26009;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#21152;&#24037;&#21442;&#25968;&#12289;&#25552;&#39640;&#20135;&#21697;&#36136;&#37327;&#21644;&#26816;&#27979;&#21046;&#36896;&#32570;&#38519;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#38646;&#20214;&#24615;&#33021;&#21644;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2309.16571</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#22686;&#26448;&#21046;&#36896;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of Machine Learning Methods for Additive Manufacturing of Functionally Graded Materials. (arXiv:2309.16571v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16571
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#22686;&#26448;&#21046;&#36896;&#12290;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#26159;&#19968;&#31867;&#20855;&#26377;&#24179;&#28369;&#24615;&#36136;&#36807;&#28193;&#30340;&#39640;&#32423;&#22797;&#21512;&#26448;&#26009;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#21152;&#24037;&#21442;&#25968;&#12289;&#25552;&#39640;&#20135;&#21697;&#36136;&#37327;&#21644;&#26816;&#27979;&#21046;&#36896;&#32570;&#38519;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#38646;&#20214;&#24615;&#33021;&#21644;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#36890;&#36807;&#23454;&#29616;&#30452;&#25509;&#26448;&#26009;&#36830;&#25509;&#65292;&#38761;&#26032;&#20102;&#22797;&#26434;&#38646;&#20214;&#30340;&#21046;&#36896;&#65292;&#24182;&#25552;&#20379;&#20102;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#22797;&#26434;&#38646;&#20214;&#21046;&#36896;&#12289;&#20943;&#23569;&#21046;&#36896;&#24223;&#26009;&#20197;&#21450;&#20026;&#21046;&#36896;&#33258;&#21160;&#21270;&#24320;&#21551;&#26032;&#30340;&#21487;&#33021;&#24615;&#31561;&#22810;&#20010;&#20248;&#21183;&#12290;&#20854;&#20013;&#65292;&#21151;&#33021;&#26799;&#24230;&#26448;&#26009;&#65288;FGMs&#65289;&#20316;&#20026;&#19968;&#31867;&#26448;&#26009;&#65292;&#22312;&#25552;&#39640;&#38646;&#20214;&#24615;&#33021;&#21644;&#24615;&#36136;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;FGMs&#26159;&#39640;&#32423;&#22797;&#21512;&#26448;&#26009;&#65292;&#20854;&#24615;&#36136;&#21576;&#24179;&#28369;&#36807;&#28193;&#65292;&#22240;&#27492;&#34987;&#33322;&#31354;&#12289;&#27773;&#36710;&#12289;&#29983;&#29289;&#21307;&#23398;&#21644;&#22269;&#38450;&#31561;&#34892;&#19994;&#24191;&#27867;&#24212;&#29992;&#12290;&#19982;&#20256;&#32479;&#22797;&#21512;&#26448;&#26009;&#19981;&#21516;&#65292;FGMs&#20013;&#30340;&#25104;&#20998;&#20250;&#36880;&#28176;&#21464;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26448;&#26009;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#25104;&#20026;&#21046;&#36896;FGMs&#30340;&#26377;&#24076;&#26395;&#30340;&#25163;&#27573;&#65292;&#21487;&#20197;&#20248;&#21270;&#21152;&#24037;&#21442;&#25968;&#12289;&#25552;&#39640;&#20135;&#21697;&#36136;&#37327;&#24182;&#26816;&#27979;&#21046;&#36896;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing has revolutionized the manufacturing of complex parts by enabling direct material joining and offers several advantages such as cost-effective manufacturing of complex parts, reducing manufacturing waste, and opening new possibilities for manufacturing automation. One group of materials for which additive manufacturing holds great potential for enhancing component performance and properties is Functionally Graded Materials (FGMs). FGMs are advanced composite materials that exhibit smoothly varying properties making them desirable for applications in aerospace, automobile, biomedical, and defense industries. Such composition differs from traditional composite materials, since the location-dependent composition changes gradually in FGMs, leading to enhanced properties. Recently, machine learning techniques have emerged as a promising means for fabrication of FGMs through optimizing processing parameters, improving product quality, and detecting manufacturing defect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#24182;&#21033;&#29992;&#20445;&#25345;&#35821;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21019;&#24314;&#20102;&#35299;&#37322;&#24615;&#23884;&#20837;&#65292;&#24182;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2309.16564</link><description>&lt;p&gt;
&#22686;&#24378;&#35299;&#37322;&#24615;: &#26080;&#30417;&#30563;&#30340;&#21644;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#22270;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Augment to Interpret: Unsupervised and Inherently Interpretable Graph Embeddings. (arXiv:2309.16564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#36890;&#36807;&#23398;&#20064;&#24182;&#21033;&#29992;&#20445;&#25345;&#35821;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21019;&#24314;&#20102;&#35299;&#37322;&#24615;&#23884;&#20837;&#65292;&#24182;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#39046;&#22495;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#22823;&#37327;&#21487;&#29992;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#21019;&#24314;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#20856;&#22411;&#32570;&#20047;&#35299;&#37322;&#24615;&#24050;&#25104;&#20026;&#26368;&#36817;&#36879;&#26126;&#20154;&#24037;&#26234;&#33021;&#27861;&#35268;&#30340;&#38480;&#21046;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23637;&#31034;&#20102;&#23398;&#20064;&#20445;&#25345;&#35821;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21517;&#20026;INGENIOUS&#65292;&#21019;&#24314;&#20102;&#26412;&#36136;&#19978;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#65292;&#24182;&#28040;&#38500;&#20102;&#26114;&#36149;&#30340;&#21518;&#32493;&#20998;&#26512;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#38024;&#23545;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#39046;&#22495;&#32570;&#20047;&#24418;&#24335;&#21270;&#21644;&#24230;&#37327;&#30340;&#39069;&#22806;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36890;&#36807;&#24212;&#29992;&#20110;&#22270;&#32423;&#21644;&#33410;&#28857;&#32423;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21487;&#35299;&#37322;&#30340;&#23884;&#20837;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning allows us to leverage unlabelled data, which has become abundantly available, and to create embeddings that are usable on a variety of downstream tasks. However, the typical lack of interpretability of unsupervised representation learning has become a limiting factor with regard to recent transparent-AI regulations. In this paper, we study graph representation learning and we show that data augmentation that preserves semantics can be learned and used to produce interpretations. Our framework, which we named INGENIOUS, creates inherently interpretable embeddings and eliminates the need for costly additional post-hoc analysis. We also introduce additional metrics addressing the lack of formalism and metrics in the understudied area of unsupervised-representation learning interpretability. Our results are supported by an experimental study applied to both graph-level and node-level tasks and show that interpretable embeddings provide state-of-the-art performance on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20219;&#24847;&#30772;&#22351;&#30340;&#22810;&#33218;&#36172;&#24466;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#36951;&#25022;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CRIMED&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#24050;&#30693;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#36172;&#24466;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.16563</link><description>&lt;p&gt;
CRIMED&#65306;&#20855;&#26377;&#26080;&#30028;&#38543;&#26426;&#30772;&#22351;&#30340;&#36172;&#24466;&#38382;&#39064;&#30340;&#36951;&#25022;&#19979;&#30028;&#21644;&#19978;&#30028;
&lt;/p&gt;
&lt;p&gt;
CRIMED: Lower and Upper Bounds on Regret for Bandits with Unbounded Stochastic Corruption. (arXiv:2309.16563v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#20219;&#24847;&#30772;&#22351;&#30340;&#22810;&#33218;&#36172;&#24466;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#36951;&#25022;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CRIMED&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20855;&#26377;&#24050;&#30693;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#36172;&#24466;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#33218;&#36172;&#24466;&#38382;&#39064;&#20013;&#20855;&#26377;&#20219;&#24847;&#30772;&#22351;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#19982;&#32463;&#20856;&#35774;&#23450;&#31867;&#20284;&#65292;&#20195;&#29702;&#25509;&#25910;&#21040;&#30340;&#22870;&#21169;&#26159;&#20174;&#27599;&#20010;&#26102;&#38388;&#28857;&#36873;&#25321;&#30340;&#33218;&#30340;&#20998;&#24067;&#29420;&#31435;&#29983;&#25104;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22870;&#21169;&#24182;&#19981;&#30452;&#25509;&#35266;&#23519;&#21040;&#12290;&#30456;&#21453;&#65292;&#23545;&#20110;&#22266;&#23450;&#30340;&#949;&#8712;(0,12)&#65292;&#20195;&#29702;&#20197;&#27010;&#29575;1-&#949;&#20174;&#36873;&#25321;&#30340;&#33218;&#30340;&#20998;&#24067;&#20013;&#35266;&#27979;&#19968;&#20010;&#26679;&#26412;&#65292;&#25110;&#20197;&#27010;&#29575;&#949;&#20174;&#20219;&#24847;&#30772;&#22351;&#20998;&#24067;&#20013;&#35266;&#27979;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#30772;&#22351;&#20998;&#24067;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#23427;&#20204;&#21487;&#20197;&#26159;&#26080;&#30028;&#30340;&#12290;&#22312;&#36825;&#31181;&#21487;&#33021;&#20855;&#26377;&#26080;&#30028;&#30772;&#22351;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#32473;&#23450;&#30340;&#33218;&#20998;&#24067;&#26063;&#24314;&#31435;&#20102;&#19968;&#20010;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#36951;&#25022;&#19979;&#30028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CRIMED&#65292;&#36825;&#26159;&#19968;&#20010;&#28176;&#36817;&#26368;&#20248;&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;&#20855;&#26377;&#24050;&#30693;&#26041;&#24046;&#30340;&#39640;&#26031;&#20998;&#24067;&#36172;&#24466;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#36951;&#25022;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#26377;&#38480;&#26679;&#26412;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the regret-minimisation problem in a multi-armed bandit setting with arbitrary corruptions. Similar to the classical setup, the agent receives rewards generated independently from the distribution of the arm chosen at each time. However, these rewards are not directly observed. Instead, with a fixed $\varepsilon\in (0,\frac{1}{2})$, the agent observes a sample from the chosen arm's distribution with probability $1-\varepsilon$, or from an arbitrary corruption distribution with probability $\varepsilon$. Importantly, we impose no assumptions on these corruption distributions, which can be unbounded. In this setting, accommodating potentially unbounded corruptions, we establish a problem-dependent lower bound on regret for a given family of arm distributions. We introduce CRIMED, an asymptotically-optimal algorithm that achieves the exact lower bound on regret for bandits with Gaussian distributions with known variance. Additionally, we provide a finite-sample analysis of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25237;&#31080;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20013;&#20998;&#21106;&#21644;&#20998;&#31867;&#31561;&#39640;&#22564;&#20892;&#30000;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#26469;&#20943;&#23569;&#36793;&#30028;&#25197;&#26354;&#21644;&#31867;&#21035;&#28151;&#28102;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16561</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#32593;&#32476;&#30340;&#31561;&#39640;&#22564;&#20892;&#30000;&#20998;&#21106;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Voting Network for Contour Levee Farmland Segmentation and Classification. (arXiv:2309.16561v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25237;&#31080;&#32593;&#32476;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20013;&#20998;&#21106;&#21644;&#20998;&#31867;&#31561;&#39640;&#22564;&#20892;&#30000;&#12290;&#36890;&#36807;&#20351;&#29992;&#25237;&#31080;&#26426;&#21046;&#26469;&#20943;&#23569;&#36793;&#30028;&#25197;&#26354;&#21644;&#31867;&#21035;&#28151;&#28102;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20801;&#35768;&#22312;&#20892;&#30000;&#20998;&#21106;&#20013;&#33719;&#21462;&#32454;&#33410;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23567;&#29289;&#20307;&#21644;&#29305;&#24449;&#20250;&#23548;&#33268;&#29289;&#20307;&#36793;&#30028;&#30340;&#25197;&#26354;&#65292;&#38656;&#35201;&#26356;&#22823;&#30340;&#19978;&#19979;&#25991;&#35270;&#22270;&#26469;&#20943;&#23569;&#31867;&#21035;&#28151;&#28102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#29992;&#20110;&#20174;&#39640;&#20998;&#36776;&#29575;&#33322;&#31354;&#24433;&#20687;&#20013;&#20998;&#21106;&#31561;&#39640;&#22564;&#20892;&#30000;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#34701;&#21512;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#20010;&#25237;&#31080;&#22359;&#65292;&#20197;&#23454;&#29616;&#22270;&#20687;&#20998;&#21106;&#21644;&#20998;&#31867;&#12290;&#25105;&#20204;&#23558;&#34701;&#21512;&#22359;&#19982;&#39592;&#24178;&#32593;&#32476;&#32467;&#21512;&#65292;&#21516;&#26102;&#29983;&#25104;&#35821;&#20041;&#39044;&#27979;&#21644;&#20998;&#21106;&#29255;&#27573;&#12290;&#20998;&#21106;&#29255;&#27573;&#29992;&#20110;&#22312;&#39044;&#27979;&#19978;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#12290;&#32593;&#32476;&#34987;&#35757;&#32451;&#20026;&#23558;&#27573;&#33853;&#30340;&#26368;&#26377;&#21487;&#33021;&#31867;&#21035;&#26631;&#31614;&#20998;&#37197;&#32473;&#20854;&#20687;&#32032;&#65292;&#20174;&#32780;&#23398;&#20064;&#20892;&#30000;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20998;&#26512;&#20687;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-resolution aerial imagery allows fine details in the segmentation of farmlands. However, small objects and features introduce distortions to the delineation of object boundaries, and larger contextual views are needed to mitigate class confusion. In this work, we present an end-to-end trainable network for segmenting farmlands with contour levees from high-resolution aerial imagery. A fusion block is devised that includes multiple voting blocks to achieve image segmentation and classification. We integrate the fusion block with a backbone and produce both semantic predictions and segmentation slices. The segmentation slices are used to perform majority voting on the predictions. The network is trained to assign the most likely class label of a segment to its pixels, learning the concept of farmlands rather than analyzing constitutive pixels separately. We evaluate our method using images from the National Agriculture Imagery Program. Our method achieved an average accuracy of 94.3
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#30417;&#27979;&#20013;&#32416;&#27491;&#25968;&#25454;&#28304;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#26469;&#36817;&#20284;&#24322;&#36136;&#24615;&#24182;&#36890;&#36807;&#8220;&#24341;&#23548;&#8221;&#20449;&#21495;&#26469;&#32416;&#27491;&#20559;&#24046;&#65292;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2309.16546</link><description>&lt;p&gt;
&#22312;&#23454;&#26102;&#27969;&#34892;&#30149;&#23398;&#25351;&#26631;&#20013;&#32416;&#27491;&#24322;&#36136;&#24615;
&lt;/p&gt;
&lt;p&gt;
Correcting for heterogeneity in real-time epidemiological indicators. (arXiv:2309.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16546
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#30417;&#27979;&#20013;&#32416;&#27491;&#25968;&#25454;&#28304;&#20013;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#24322;&#36136;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20302;&#31209;&#30697;&#38453;&#26469;&#36817;&#20284;&#24322;&#36136;&#24615;&#24182;&#36890;&#36807;&#8220;&#24341;&#23548;&#8221;&#20449;&#21495;&#26469;&#32416;&#27491;&#20559;&#24046;&#65292;&#20197;&#25552;&#20379;&#26356;&#21487;&#38752;&#30340;&#24314;&#27169;&#21644;&#39044;&#27979;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36741;&#21161;&#25968;&#25454;&#28304;&#22312;&#27969;&#34892;&#30149;&#23398;&#30417;&#27979;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#27604;&#20256;&#32479;&#30417;&#27979;&#20449;&#21495;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#35206;&#30422;&#33539;&#22260;&#21644;&#24310;&#36831;&#26041;&#38754;&#26356;&#22909;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20174;&#36825;&#20123;&#25968;&#25454;&#28304;&#23548;&#20986;&#30340;&#25351;&#26631;&#20013;&#23384;&#22312;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20854;&#20013;&#23384;&#22312;&#31354;&#38388;&#21644;/&#25110;&#26102;&#38388;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#8220;&#24341;&#23548;&#8221;&#20449;&#21495;&#26469;&#32416;&#27491;&#36825;&#20123;&#20559;&#24046;&#24182;&#20135;&#29983;&#26356;&#21487;&#38752;&#30340;&#20449;&#21495;&#20379;&#24314;&#27169;&#21644;&#39044;&#27979;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20551;&#35774;&#24322;&#36136;&#24615;&#21487;&#20197;&#29992;&#20302;&#31209;&#30697;&#38453;&#26469;&#36817;&#20284;&#65292;&#24182;&#19988;&#26102;&#38388;&#24322;&#36136;&#24615;&#22312;&#26102;&#38388;&#19978;&#26159;&#24179;&#28369;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#21442;&#25968;&#36873;&#25321;&#31639;&#27861;&#26469;&#36873;&#25321;&#34920;&#31034;&#30697;&#38453;&#31209;&#21644;&#32416;&#27491;&#30340;&#26102;&#38388;&#24179;&#28369;&#24230;&#30340;&#21442;&#25968;&#12290;&#22312;&#32570;&#20047;&#22522;&#20934;&#20107;&#23454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#22320;&#22270;&#21644;&#22270;&#34920;&#26469;&#35770;&#35777;&#36825;&#31181;&#26041;&#27861;&#30830;&#23454;&#20943;&#23569;&#20102;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Auxiliary data sources have become increasingly important in epidemiological surveillance, as they are often available at a finer spatial and temporal resolution, larger coverage, and lower latency than traditional surveillance signals. We describe the problem of spatial and temporal heterogeneity in these signals derived from these data sources, where spatial and/or temporal biases are present. We present a method to use a ``guiding'' signal to correct for these biases and produce a more reliable signal that can be used for modeling and forecasting. The method assumes that the heterogeneity can be approximated by a low-rank matrix and that the temporal heterogeneity is smooth over time. We also present a hyperparameter selection algorithm to choose the parameters representing the matrix rank and degree of temporal smoothness of the corrections. In the absence of ground truth, we use maps and plots to argue that this method does indeed reduce heterogeneity. Reducing heterogeneity from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16540</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Fact Verification by Language Model Distillation. (arXiv:2309.16540v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SFAVEL&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#23454;&#29616;&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#12290;&#36825;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#21516;&#26102;&#20445;&#30041;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#26088;&#22312;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#24211;&#20013;&#30340;&#35777;&#25454;&#26469;&#39564;&#35777;&#20027;&#24352;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#25968;&#25454;&#27880;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#31639;&#27861;&#24517;&#39035;&#20026;&#27599;&#20010;&#20027;&#24352;&#29983;&#25104;&#26082;&#35821;&#20041;&#26126;&#30830;&#21448;&#32039;&#20945;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#19982;&#28304;&#20449;&#24687;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#21069;&#32773;&#36890;&#36807;&#23398;&#20064;&#21253;&#21547;&#20027;&#24352;&#21450;&#20854;&#30456;&#24212;&#26631;&#31614;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#26469;&#35299;&#20915;&#23545;&#40784;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SFAVEL&#65288;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#20107;&#23454;&#39564;&#35777;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#30417;&#30563;&#29305;&#24449;&#33976;&#39311;&#20026;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;-&#20107;&#23454;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#27880;&#37322;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#30340;&#65292;&#35813;&#20989;&#25968;&#40723;&#21169;&#29305;&#24449;&#22312;&#20445;&#25345;&#35821;&#26009;&#24211;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#20027;&#24352;&#21644;&#35777;&#25454;&#23545;&#40784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36798;&#21040;&#26032;&#39062;&#30340;&#29366;&#24577;&#19968;.
&lt;/p&gt;
&lt;p&gt;
Unsupervised fact verification aims to verify a claim using evidence from a trustworthy knowledge base without any kind of data annotation. To address this challenge, algorithms must produce features for every claim that are both semantically meaningful, and compact enough to find a semantic alignment with the source information. In contrast to previous work, which tackled the alignment problem by learning over annotated corpora of claims and their corresponding labels, we propose SFAVEL (Self-supervised Fact Verification via Language Model Distillation), a novel unsupervised framework that leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations. This is enabled by a novel contrastive loss function that encourages features to attain high-quality claim and evidence alignments whilst preserving the semantic relationships across the corpora. Notably, we present results that achieve a new state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20998;&#21106;&#21644;&#33945;&#29305;&#21345;&#27931;&#36749;&#23398;&#26469;&#37327;&#21270;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#65292;&#20026;&#35786;&#26029;&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#21644;&#27169;&#22411;&#24615;&#33021;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16536</link><description>&lt;p&gt;
&#23545;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification for Eosinophil Segmentation. (arXiv:2309.16536v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#22270;&#20687;&#20998;&#21106;&#21644;&#33945;&#29305;&#21345;&#27931;&#36749;&#23398;&#26469;&#37327;&#21270;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#65292;&#20026;&#35786;&#26029;&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#21644;&#27169;&#22411;&#24615;&#33021;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21980;&#37240;&#24615;&#39135;&#31649;&#28814;&#65288;EoE&#65289;&#26159;&#19968;&#31181;&#26085;&#30410;&#26222;&#21450;&#30340;&#36807;&#25935;&#24615;&#30142;&#30149;&#12290;&#20026;&#20102;&#35786;&#26029;EoE&#65292;&#30149;&#29702;&#23398;&#23478;&#24517;&#39035;&#22312;&#19968;&#20010;&#39640;&#20493;&#35270;&#22330;&#65288;400&#20493;&#25918;&#22823;&#29575;&#65289;&#20869;&#25214;&#21040;15&#20010;&#25110;&#26356;&#22810;&#30340;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#12290;&#30830;&#23450;&#19968;&#20010;&#24739;&#32773;&#26159;&#21542;&#24739;&#26377;EoE&#21487;&#20197;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#36807;&#31243;&#65292;&#24182;&#19988;&#29992;&#20110;&#36741;&#21161;&#35786;&#26029;&#30340;&#20219;&#20309;&#21307;&#23398;&#25104;&#20687;&#26041;&#27861;&#37117;&#24517;&#39035;&#32771;&#34385;&#25928;&#29575;&#21644;&#31934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;Adorno&#31561;&#20154;&#23545;&#20351;&#29992;&#28145;&#24230;&#22270;&#20687;&#20998;&#21106;&#36827;&#34892;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#23450;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#36749;&#23398;&#65288;Monte Carlo Dropout&#65289;&#65292;&#36825;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#19968;&#31181;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#22312;&#36755;&#20986;&#22270;&#20687;&#20013;&#21487;&#35270;&#21270;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#24110;&#21161;&#30149;&#29702;&#23398;&#23478;&#35782;&#21035;&#21980;&#37240;&#24615;&#31890;&#32454;&#32990;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eosinophilic Esophagitis (EoE) is an allergic condition increasing in prevalence. To diagnose EoE, pathologists must find 15 or more eosinophils within a single high-power field (400X magnification). Determining whether or not a patient has EoE can be an arduous process and any medical imaging approaches used to assist diagnosis must consider both efficiency and precision. We propose an improvement of Adorno et al's approach for quantifying eosinphils using deep image segmentation. Our new approach leverages Monte Carlo Dropout, a common approach in deep learning to reduce overfitting, to provide uncertainty quantification on current deep learning models. The uncertainty can be visualized in an output image to evaluate model performance, provide insight to how deep learning algorithms function, and assist pathologists in identifying eosinophils.
&lt;/p&gt;</description></item><item><title>MotionLM&#27169;&#22411;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#38170;&#28857;&#25110;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#20248;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#20851;&#20110;&#20132;&#20114;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#23454;&#29616;&#26102;&#38388;&#22240;&#26524;&#26465;&#20214;&#23637;&#24320;&#12290;</title><link>http://arxiv.org/abs/2309.16534</link><description>&lt;p&gt;
MotionLM: &#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MotionLM: Multi-Agent Motion Forecasting as Language Modeling. (arXiv:2309.16534v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16534
&lt;/p&gt;
&lt;p&gt;
MotionLM&#27169;&#22411;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#38170;&#28857;&#25110;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#20248;&#21270;&#65292;&#33021;&#22815;&#29983;&#25104;&#20851;&#20110;&#20132;&#20114;&#26234;&#33021;&#20307;&#26410;&#26469;&#30340;&#32852;&#21512;&#20998;&#24067;&#21644;&#23454;&#29616;&#26102;&#38388;&#22240;&#26524;&#26465;&#20214;&#23637;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#35268;&#21010;&#20013;&#65292;&#21487;&#38752;&#22320;&#39044;&#27979;&#36947;&#36335;&#19978;&#26234;&#33021;&#20307;&#30340;&#26410;&#26469;&#34892;&#20026;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#36830;&#32493;&#36712;&#36857;&#34920;&#31034;&#20026;&#31163;&#25955;&#36816;&#21160;&#20196;&#29260;&#30340;&#24207;&#21015;&#65292;&#24182;&#23558;&#22810;&#26234;&#33021;&#20307;&#36816;&#21160;&#39044;&#27979;&#35270;&#20026;&#23545;&#35813;&#39046;&#22495;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;MotionLM&#25552;&#20379;&#20102;&#20960;&#20010;&#20248;&#21183;&#65306;&#39318;&#20808;&#65292;&#23427;&#19981;&#38656;&#35201;&#38170;&#28857;&#25110;&#26174;&#24335;&#28508;&#22312;&#21464;&#37327;&#20248;&#21270;&#26469;&#23398;&#20064;&#22810;&#27169;&#24577;&#20998;&#24067;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#21333;&#20010;&#26631;&#20934;&#30340;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#65292;&#26368;&#22823;&#21270;&#24207;&#21015;&#20196;&#29260;&#30340;&#24179;&#22343;&#23545;&#25968;&#27010;&#29575;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32469;&#36807;&#20107;&#21518;&#20132;&#20114;&#21551;&#21457;&#24335;&#65292;&#20854;&#20013;&#22312;&#20132;&#20114;&#35780;&#20998;&#20043;&#21069;&#36827;&#34892;&#21333;&#20010;&#20195;&#29702;&#36712;&#36857;&#29983;&#25104;&#12290;&#30456;&#21453;&#65292;MotionLM&#22312;&#21333;&#20010;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#20013;&#29983;&#25104;&#20851;&#20110;&#20132;&#20114;&#20195;&#29702;&#26410;&#26469;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#26102;&#24207;&#22240;&#23376;&#21270;&#20351;&#20854;&#33021;&#22815;&#23454;&#29616;&#26102;&#38388;&#22240;&#26524;&#26465;&#20214;&#23637;&#24320;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable forecasting of the future behavior of road agents is a critical component to safe planning in autonomous vehicles. Here, we represent continuous trajectories as sequences of discrete motion tokens and cast multi-agent motion prediction as a language modeling task over this domain. Our model, MotionLM, provides several advantages: First, it does not require anchors or explicit latent variable optimization to learn multimodal distributions. Instead, we leverage a single standard language modeling objective, maximizing the average log probability over sequence tokens. Second, our approach bypasses post-hoc interaction heuristics where individual agent trajectory generation is conducted prior to interactive scoring. Instead, MotionLM produces joint distributions over interactive agent futures in a single autoregressive decoding process. In addition, the model's sequential factorization enables temporally causal conditional rollouts. The proposed approach establishes new state-of-t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20915;&#31574;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#36924;&#30495;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#26410;&#26469;&#32467;&#26524;&#36712;&#36857;&#65292;&#21487;&#20197;&#20026;&#20010;&#24615;&#21270;&#24739;&#32773;&#21382;&#21490;&#21305;&#37197;&#19988;&#38024;&#23545;&#26368;&#20339;&#26410;&#26469;&#25928;&#26524;&#30340;&#26032;&#22411;&#22810;&#21464;&#37327;&#27835;&#30103;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16521</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#26465;&#20214;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models. (arXiv:2309.16521v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#21644;&#20915;&#31574;&#29702;&#35770;&#30456;&#32467;&#21512;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;&#12290;&#36890;&#36807;&#23398;&#20064;&#29983;&#25104;&#36924;&#30495;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#26410;&#26469;&#32467;&#26524;&#36712;&#36857;&#65292;&#21487;&#20197;&#20026;&#20010;&#24615;&#21270;&#24739;&#32773;&#21382;&#21490;&#21305;&#37197;&#19988;&#38024;&#23545;&#26368;&#20339;&#26410;&#26469;&#25928;&#26524;&#30340;&#26032;&#22411;&#22810;&#21464;&#37327;&#27835;&#30103;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#19982;&#20915;&#31574;&#29702;&#35770;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#23427;&#21033;&#29992;&#21382;&#21490;&#24739;&#32773;&#36712;&#36857;&#25968;&#25454;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20849;&#21516;&#23398;&#20064;&#29983;&#25104;&#36924;&#30495;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#21644;&#26410;&#26469;&#32467;&#26524;&#36712;&#36857;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26681;&#25454;&#26465;&#20214;&#21270;&#26399;&#26395;&#25928;&#29992;&#26368;&#22823;&#21270;&#35757;&#32451;&#29983;&#25104;&#19982;&#20010;&#24615;&#21270;&#24739;&#32773;&#21382;&#21490;&#21305;&#37197;&#19988;&#38024;&#23545;&#26368;&#20339;&#26410;&#26469;&#25928;&#26524;&#30340;&#26032;&#22411;&#22810;&#21464;&#37327;&#27835;&#30103;&#31574;&#30053;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#20303;&#38498;&#31958;&#23615;&#30149;&#24739;&#32773;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#33008;&#23707;&#32032;&#27835;&#30103;&#31574;&#30053;&#21644;&#34880;&#31958;&#39044;&#27979;&#26469;&#23637;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#25913;&#36827;&#30340;&#20010;&#24615;&#21270;&#27835;&#30103;&#31574;&#30053;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework that combines deep generative time series models with decision theory for generating personalized treatment strategies. It leverages historical patient trajectory data to jointly learn the generation of realistic personalized treatment and future outcome trajectories through deep generative time series models. In particular, our framework enables the generation of novel multivariate treatment strategies tailored to the personalized patient history and trained for optimal expected future outcomes based on conditional expected utility maximization. We demonstrate our framework by generating personalized insulin treatment strategies and blood glucose predictions for hospitalized diabetes patients, showcasing the potential of our approach for generating improved personalized treatment strategies. Keywords: deep generative model, probabilistic decision support, personalized treatment generation, insulin and blood glucose prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16519</link><description>&lt;p&gt;
AtomSurf&#65306;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#30340;&#23398;&#20064;&#30340;&#34920;&#38754;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;Cryo-EM&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#32467;&#26500;&#21487;&#33719;&#24471;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21151;&#33021;&#27880;&#37322;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20851;&#27880;&#21019;&#24314;&#36866;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20174;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#23558;&#36825;&#20123;&#32467;&#26500;&#34920;&#31034;&#20026;&#20960;&#20309;&#23545;&#35937;&#65288;&#22914;&#32593;&#26684;&#12289;&#22270;&#25110;&#34920;&#38754;&#65289;&#24182;&#24212;&#29992;&#36866;&#21512;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#23558;&#21462;&#20915;&#20110;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;&#34507;&#30333;&#36136;&#34920;&#31034;&#20026;$\textit{3D mesh surfaces}$&#24182;&#23558;&#20854;&#32435;&#20837;&#24050;&#24314;&#31435;&#30340;&#34920;&#31034;&#22522;&#20934;&#20013;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21457;&#29616;&#26159;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#20165;&#21333;&#29420;&#34920;&#38754;&#34920;&#31034;&#20284;&#20046;&#26080;&#27861;&#19982;3D&#32593;&#26684;&#31454;&#20105;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#26041;&#27861;&#65292;&#23558;&#34920;&#38754;&#34920;&#31034;&#19982;&#22270;&#34920;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16512</link><description>&lt;p&gt;
&#20174;&#22797;&#26434;&#21040;&#28165;&#26224;&#65306;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#20998;&#26512;&#34920;&#36798;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;
&lt;/p&gt;
&lt;p&gt;
From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford's Geometric Algebra and Convexity. (arXiv:2309.16512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;Clifford&#30340;&#20960;&#20309;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#26512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#26469;&#33719;&#24471;&#65292;&#24182;&#19988;&#35757;&#32451;&#38382;&#39064;&#21487;&#20197;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20960;&#20309;&#65288;Clifford&#65289;&#20195;&#25968;&#21644;&#20984;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#21270;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#26435;&#37325;&#30001;&#35757;&#32451;&#26679;&#26412;&#30340;&#26964;&#31215;&#32473;&#20986;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#38382;&#39064;&#21487;&#31616;&#21270;&#20026;&#23545;&#26964;&#31215;&#29305;&#24449;&#36827;&#34892;&#20984;&#20248;&#21270;&#65292;&#22312;&#20854;&#20013;&#32534;&#30721;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#35813;&#32467;&#26500;&#20197;&#25968;&#25454;&#21521;&#37327;&#29983;&#25104;&#30340;&#19977;&#35282;&#24418;&#21644;&#24179;&#34892;&#20307;&#30340;&#26377;&#31526;&#21495;&#20307;&#31215;&#34920;&#31034;&#12290;&#20984;&#38382;&#39064;&#36890;&#36807;$\ell_1$&#27491;&#21017;&#21270;&#25214;&#21040;&#26679;&#26412;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#20197;&#21457;&#29616;&#20165;&#30456;&#20851;&#30340;&#26964;&#31215;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#30340;&#26032;&#35270;&#35282;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#23618;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford) algebra and convex optimization. We show that optimal weights of deep ReLU neural networks are given by the wedge product of training samples when trained with standard regularized loss. Furthermore, the training problem reduces to convex optimization over wedge product features, which encode the geometric structure of the training dataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by data vectors. The convex problem finds a small subset of samples via $\ell_1$ regularization to discover only relevant wedge product features. Our analysis provides a novel perspective on the inner workings of deep neural networks and sheds light on the role of the hidden layers.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#26631;&#35760;&#20572;&#36710;&#22330;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#20840;&#23616;&#26694;&#26550;&#30340;&#21019;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#20934;&#30830;&#25191;&#34892;&#20572;&#36710;&#31354;&#38388;&#30417;&#25511;&#20219;&#21153;&#65292;&#21152;&#36895;&#37096;&#32626;&#20572;&#36710;&#30417;&#25511;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16495</link><description>&lt;p&gt;
&#28145;&#24230;&#21333;&#19968;&#27169;&#22411;&#19982;&#38598;&#25104;&#65306;&#21152;&#36895;&#37096;&#32626;&#20572;&#36710;&#30417;&#25511;&#31995;&#32479;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
Deep Single Models vs. Ensembles: Insights for a Fast Deployment of Parking Monitoring Systems. (arXiv:2309.16495v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#26631;&#35760;&#20572;&#36710;&#22330;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#20840;&#23616;&#26694;&#26550;&#30340;&#21019;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#20934;&#30830;&#25191;&#34892;&#20572;&#36710;&#31354;&#38388;&#30417;&#25511;&#20219;&#21153;&#65292;&#21152;&#36895;&#37096;&#32626;&#20572;&#36710;&#30417;&#25511;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#23494;&#24230;&#22478;&#24066;&#20013;&#23547;&#25214;&#21487;&#29992;&#20572;&#36710;&#20301;&#23545;&#39550;&#36710;&#32773;&#26469;&#35828;&#26159;&#19968;&#39033;&#21387;&#21147;&#24040;&#22823;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#36890;&#36807;&#25552;&#21069;&#20102;&#35299;&#26368;&#36817;&#21487;&#29992;&#30340;&#20572;&#36710;&#31354;&#38388;&#30340;&#31995;&#32479;&#26469;&#32531;&#35299;&#12290;&#20026;&#27492;&#65292;&#22522;&#20110;&#22270;&#20687;&#30340;&#31995;&#32479;&#22312;&#23433;&#35013;&#21644;&#32500;&#25252;&#19978;&#30456;&#23545;&#20854;&#20182;&#20256;&#24863;&#22120;&#30340;&#36873;&#25321;&#65288;&#22914;&#36229;&#22768;&#27874;&#20256;&#24863;&#22120;&#65289;&#20855;&#26377;&#25104;&#26412;&#20248;&#21183;&#65292;&#38656;&#35201;&#26356;&#23569;&#30340;&#29289;&#29702;&#22522;&#30784;&#35774;&#26045;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#37096;&#32626;&#26234;&#33021;&#20572;&#36710;&#30417;&#25511;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#26041;&#27861;&#28041;&#21450;&#25910;&#38598;&#21644;&#26631;&#35760;&#22823;&#37327;&#25968;&#25454;&#65292;&#36825;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#22312;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#26631;&#35760;&#20572;&#36710;&#22330;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#30340;&#20840;&#23616;&#26694;&#26550;&#30340;&#21019;&#24314;&#20013;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#65292;&#20174;&#32780;&#23454;&#29616;&#20572;&#36710;&#31354;&#38388;&#30417;&#25511;&#20316;&#20026;&#19968;&#31181;&#21487;&#20197;&#31435;&#21363;&#25237;&#20837;&#26032;&#29615;&#22659;&#20351;&#29992;&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#28041;&#21450;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#30340;&#35814;&#23613;&#23454;&#39564;&#65292;&#21253;&#25324;&#34701;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for available parking spots in high-density urban centers is a stressful task for drivers that can be mitigated by systems that know in advance the nearest parking space available.  To this end, image-based systems offer cost advantages over other sensor-based alternatives (e.g., ultrasonic sensors), requiring less physical infrastructure for installation and maintenance.  Despite recent deep learning advances, deploying intelligent parking monitoring is still a challenge since most approaches involve collecting and labeling large amounts of data, which is laborious and time-consuming. Our study aims to uncover the challenges in creating a global framework, trained using publicly available labeled parking lot images, that performs accurately across diverse scenarios, enabling the parking space monitoring as a ready-to-use system to deploy in a new environment. Through exhaustive experiments involving different datasets and deep learning architectures, including fusion strateg
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#30340;&#25968;&#25454;&#20013;&#27602;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#23545;&#25239;&#24615;&#22330;&#26223;&#19979;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#35299;&#20915;&#20102;FRL&#26041;&#27861;&#22312;&#38754;&#23545;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16487</link><description>&lt;p&gt;
&#23545;&#20013;&#27602;&#20844;&#24179;&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Poisoning Fair Representations. (arXiv:2309.16487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16487
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#30340;&#25968;&#25454;&#20013;&#27602;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#23545;&#25239;&#24615;&#22330;&#26223;&#19979;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#35299;&#20915;&#20102;FRL&#26041;&#27861;&#22312;&#38754;&#23545;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#26102;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26088;&#22312;&#20943;&#23569;&#23545;&#26576;&#20123;&#20154;&#21475;&#23376;&#32676;&#20307;&#65288;&#22914;&#32769;&#24180;&#20154;&#21644;&#22899;&#24615;&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#20559;&#35265;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20844;&#24179;&#34920;&#31034;&#23398;&#20064;&#65288;FRL&#65289;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#19981;&#21253;&#21547;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#20998;&#31867;&#25110;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#30340;&#36755;&#20837;&#12290;&#23613;&#31649;FRL&#26041;&#27861;&#24471;&#21040;&#20102;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38754;&#23545;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#26102;&#30340;&#33030;&#24369;&#24615;&#65292;&#21363;&#22312;&#23545;&#25239;&#24615;&#22330;&#26223;&#19979;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#30340;&#27969;&#34892;&#21327;&#35758;&#20013;&#65292;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25968;&#25454;&#20013;&#27602;&#25915;&#20987;&#24050;&#32463;&#38024;&#23545;&#23558;&#20844;&#24179;&#24615;&#32422;&#26463;&#32435;&#20837;&#27973;&#23618;&#27169;&#22411;&#20998;&#31867;&#22120;&#30340;&#32463;&#20856;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20844;&#24179;&#30446;&#26631;&#21644;&#27169;&#22411;&#26550;&#26500;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;FRL&#20013;&#19981;&#22815;&#26377;&#25928;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#38024;&#23545;FRL&#30340;&#25968;&#25454;&#20013;&#27602;&#26694;&#26550;&#12290;&#25105;&#20204;&#35825;&#20351;&#27169;&#22411;&#36755;&#20986;&#21253;&#21547;&#23613;&#21487;&#33021;&#22810;&#20844;&#24179;&#34920;&#31034;&#30340;&#19981;&#20844;&#24179;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair machine learning seeks to mitigate model prediction bias against certain demographic subgroups such as elder and female. Recently, fair representation learning (FRL) trained by deep neural networks has demonstrated superior performance, whereby representations containing no demographic information are inferred from the data and then used as the input to classification or other downstream tasks. Despite the development of FRL methods, their vulnerability under data poisoning attack, a popular protocol to benchmark model robustness under adversarial scenarios, is under-explored. Data poisoning attacks have been developed for classical fair machine learning methods which incorporate fairness constraints into shallow-model classifiers. Nonetheless, these attacks fall short in FRL due to notably different fairness goals and model architectures. This work proposes the first data poisoning framework attacking FRL. We induce the model to output unfair representations that contain as much 
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#21270;&#27867;&#21270;&#26159;&#20154;&#31867;&#30340;&#20851;&#38190;&#25216;&#33021;&#20043;&#19968;&#65292;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;&#22120;&#65288;CPG&#65289;&#36890;&#36807;&#27169;&#22359;&#21270;&#12289;&#31867;&#22411;&#25277;&#35937;&#21644;&#36882;&#24402;&#32452;&#21512;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#26032;&#27010;&#24565;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#27867;&#21270;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#29983;&#20135;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16467</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#27867;&#21270;&#30340;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Compositional Program Generation for Systematic Generalization. (arXiv:2309.16467v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16467
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#21270;&#27867;&#21270;&#26159;&#20154;&#31867;&#30340;&#20851;&#38190;&#25216;&#33021;&#20043;&#19968;&#65292;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;&#22120;&#65288;CPG&#65289;&#36890;&#36807;&#27169;&#22359;&#21270;&#12289;&#31867;&#22411;&#25277;&#35937;&#21644;&#36882;&#24402;&#32452;&#21512;&#30340;&#29305;&#24449;&#65292;&#33021;&#22815;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#26032;&#27010;&#24565;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#27867;&#21270;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#29983;&#20135;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#24335;&#27867;&#21270;&#26159;&#20154;&#31867;&#30340;&#20851;&#38190;&#25216;&#33021;&#20043;&#19968;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20174;&#23569;&#25968;&#20363;&#23376;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#22914;&#20170;&#26080;&#22788;&#19981;&#22312;&#30340;transformers&#65292;&#22312;&#36825;&#26041;&#38754;&#24456;&#38590;&#36827;&#34892;&#27867;&#21270;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#25968;&#21315;&#20010;&#27010;&#24565;&#31034;&#20363;&#25165;&#33021;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#27867;&#21270;&#12290;&#20154;&#31867;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#22312;&#33021;&#21147;&#19978;&#30340;&#24046;&#24322;&#65292;&#20419;&#20351;&#20102;&#23545;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#24335;&#31243;&#24207;&#29983;&#25104;&#22120;&#65288;CPG&#65289;&#30340;&#31070;&#32463;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#36827;&#34892;&#30740;&#31350;&#12290;CPG&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;&#27169;&#22359;&#21270;&#12289;&#31867;&#22411;&#25277;&#35937;&#21644;&#36882;&#24402;&#32452;&#21512;&#65292;&#23427;&#20351;&#20854;&#33021;&#22815;&#20197;&#23569;&#26679;&#26412;&#30340;&#26041;&#24335;&#23545;&#26032;&#27010;&#24565;&#36827;&#34892;&#31995;&#32479;&#21270;&#30340;&#27867;&#21270;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35821;&#35328;&#20219;&#21153;&#19978;&#20855;&#26377;&#29983;&#20135;&#21147;&#12290;&#23545;&#20110;&#27599;&#20010;&#36755;&#20837;&#65292;CPG&#20351;&#29992;&#36755;&#20837;&#39046;&#22495;&#30340;&#35821;&#27861;&#21644;&#35299;&#26512;&#22120;&#29983;&#25104;&#19968;&#20010;&#31867;&#22411;&#23618;&#27425;&#32467;&#26500;&#65292;&#22312;&#36825;&#20010;&#32467;&#26500;&#20013;&#65292;&#27599;&#20010;&#35821;&#27861;&#35268;&#21017;&#37117;&#34987;&#20998;&#37197;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#35821;&#20041;&#27169;&#22359;&#8212;&#8212;&#19968;&#20010;&#27010;&#29575;&#24615;&#30340;&#22797;&#21046;&#25110;&#26367;&#25442;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization is a key ability of humans that enables us to learn new concepts from only a handful examples. Machine learning models, including the now ubiquitous transformers, struggle to generalize in this way, and typically require thousands of examples of a concept during training in order to generalize meaningfully. This difference in ability between humans and artificial neural architectures, motivates this study on a neuro-symbolic architecture called the Compositional Program Generator (CPG). CPG has three key features: modularity, type abstraction, and recursive composition, that enable it to generalize both systematically to new concepts in a few-shot manner, as well as productively by length on various sequence-to-sequence language tasks. For each input, CPG uses a grammar of the input domain and a parser to generate a type hierarchy in which each grammar rule is assigned its own unique semantic module, a probabilistic copy or substitution program. Instances w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#21464;&#25442;&#23454;&#29616;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#26080;&#26799;&#24230;&#21442;&#25968;&#25628;&#32034;&#65292;&#20174;&#32780;&#35299;&#20915;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16465</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#36827;&#34892;&#25674;&#38144;&#25628;&#32034;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Metaheuristic for Amortized Search in High-Dimensional Parameter Spaces. (arXiv:2309.16465v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16465
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#21464;&#25442;&#23454;&#29616;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#26080;&#26799;&#24230;&#21442;&#25968;&#25628;&#32034;&#65292;&#20174;&#32780;&#35299;&#20915;&#21442;&#25968;&#25512;&#26029;&#20013;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#65288;&#29983;&#29289;&#65289;&#29289;&#29702;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#25512;&#26029;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38590;&#20197;&#22788;&#29702;&#30340;&#26799;&#24230;&#12289;&#39640;&#32500;&#31354;&#38388;&#21644;&#38750;&#32447;&#24615;&#27169;&#22411;&#20989;&#25968;&#36890;&#24120;&#22312;&#27809;&#26377;&#22823;&#37327;&#35745;&#31639;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#25104;&#20026;&#38382;&#39064;&#12290;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#19968;&#31995;&#21015;&#24037;&#20316;&#38598;&#20013;&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#19978;&#65292;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#21442;&#25968;&#22312;&#20854;&#32479;&#35745;&#20998;&#24067;&#19979;&#65292;&#24182;&#19988;&#22240;&#27492;&#19981;&#24471;&#20986;&#26368;&#20248;&#21442;&#25968;&#20540;&#30340;&#28857;&#20272;&#35745;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#30340;&#21464;&#25442;&#65288;DR-FFIT&#65289;&#26469;&#38477;&#20302;&#32500;&#24230;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#29942;&#39048;&#38382;&#39064;&#12290;DR-FFIT&#23454;&#29616;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#25277;&#26679;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#36827;&#34892;&#26080;&#26799;&#24230;&#30340;&#21442;&#25968;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26469;&#33719;&#21462;&#27169;&#22411;&#24863;&#20852;&#36259;&#29305;&#24449;&#30340;&#21487;&#24494;&#20195;&#29702;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#26799;&#24230;&#20351;&#24471;&#22312;&#23450;&#20041;&#30340;&#25277;&#26679;&#21306;&#22495;&#20869;&#20272;&#35745;&#27169;&#22411;&#30340;&#26412;&#22320;&#27963;&#36291;&#23376;&#31354;&#38388;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#32500;&#24230;&#32422;&#31616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter inference for dynamical models of (bio)physical systems remains a challenging problem. Intractable gradients, high-dimensional spaces, and non-linear model functions are typically problematic without large computational budgets. A recent body of work in that area has focused on Bayesian inference methods, which consider parameters under their statistical distributions and therefore, do not derive point estimates of optimal parameter values. Here we propose a new metaheuristic that drives dimensionality reductions from feature-informed transformations (DR-FFIT) to address these bottlenecks. DR-FFIT implements an efficient sampling strategy that facilitates a gradient-free parameter search in high-dimensional spaces. We use artificial neural networks to obtain differentiable proxies for the model's features of interest. The resulting gradients enable the estimation of a local active subspace of the model within a defined sampling region. This approach enables efficient dimensio
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35775;&#38382;&#21644;&#25805;&#20316;&#33021;&#21147;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16459</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22686;&#24378;LLM&#65306;&#20851;&#20110;&#24187;&#35273;&#39044;&#38450;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Augmenting LLMs with Knowledge: A survey on hallucination prevention. (arXiv:2309.16459v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#35752;&#35770;&#20102;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#22806;&#37096;&#30693;&#35782;&#28304;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35775;&#38382;&#21644;&#25805;&#20316;&#33021;&#21147;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#21644;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#32467;&#26524;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29087;&#32451;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#20934;&#30830;&#35775;&#38382;&#21644;&#25805;&#20316;&#30693;&#35782;&#30340;&#33021;&#21147;&#20173;&#28982;&#21463;&#38480;&#65292;&#23548;&#33268;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#19982;&#29305;&#23450;&#20219;&#21153;&#30340;&#26550;&#26500;&#30456;&#27604;&#23384;&#22312;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#27169;&#22411;&#20915;&#31574;&#30340;&#26469;&#28304;&#21644;&#20445;&#25345;&#26368;&#26032;&#19990;&#30028;&#30693;&#35782;&#30340;&#25361;&#25112;&#20173;&#28982;&#26159;&#24320;&#25918;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21487;&#24494;&#20998;&#35775;&#38382;&#26426;&#21046;&#38598;&#25104;&#21040;&#26174;&#24335;&#30340;&#38750;&#21442;&#25968;&#35760;&#24518;&#20013;&#25104;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#35843;&#30740;&#25506;&#35752;&#20102;&#22686;&#24378;&#20102;&#19982;&#22806;&#37096;&#30693;&#35782;&#26469;&#28304;&#65288;&#21253;&#25324;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65289;&#30456;&#36830;&#25509;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#23454;&#39564;&#24182;&#25910;&#38598;&#20102;52&#21517;&#21442;&#19982;&#32773;&#30340;&#20840;&#38754;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#29366;&#24577;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#21644;&#30561;&#30496;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#20010;&#20307;&#19978;&#23545;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16457</link><description>&lt;p&gt;
&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65306;&#23558;&#35273;&#37266;&#21644;&#30561;&#30496;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#20110;&#19981;&#21516;&#20010;&#20307;&#38388;
&lt;/p&gt;
&lt;p&gt;
Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects. (arXiv:2309.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#23454;&#39564;&#24182;&#25910;&#38598;&#20102;52&#21517;&#21442;&#19982;&#32773;&#30340;&#20840;&#38754;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#29366;&#24577;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#21644;&#30561;&#30496;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#20010;&#20307;&#19978;&#23545;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33041;&#27963;&#21160;&#35299;&#30721;&#30561;&#30496;&#20013;&#30340;&#35760;&#24518;&#20869;&#23481;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#24050;&#30693;&#21870;&#40831;&#31867;&#21160;&#29289;&#22312;&#30561;&#30496;&#20013;&#33258;&#21457;&#22320;&#37325;&#26032;&#28608;&#27963;&#35760;&#24518;&#20197;&#25903;&#25345;&#35760;&#24518;&#24041;&#22266;&#21644;&#31163;&#32447;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#23436;&#25972;&#27880;&#37322;&#30340;&#30561;&#30496;&#25968;&#25454;&#38598;&#20197;&#21450;&#28165;&#37266;&#29366;&#24577;&#21644;&#30561;&#30496;&#29366;&#24577;&#20043;&#38388;&#31070;&#32463;&#27169;&#24335;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#25429;&#25417;&#20154;&#31867;&#30340;&#35760;&#24518;&#20877;&#29616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#65292;&#24182;&#20174;52&#21517;&#21442;&#19982;&#32773;&#25910;&#38598;&#20102;&#19968;&#20221;&#20840;&#38754;&#12289;&#23436;&#25972;&#27880;&#37322;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#20004;&#31181;&#29366;&#24577;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#19982;&#30561;&#30496;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20010;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;16.6%&#30340;top-1&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65292;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#23545;&#27979;&#35797;&#20010;&#20307;&#30340;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Snowball&#65292;&#19968;&#20010;&#36890;&#36807;&#20010;&#20307;&#35270;&#35282;&#19978;&#30340;&#21452;&#21521;&#36873;&#20030;&#26041;&#27861;&#26469;&#25269;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#36873;&#20030;&#36807;&#31243;&#65292;&#36880;&#27493;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#22810;&#26679;&#24615;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#28151;&#26434;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16456</link><description>&lt;p&gt;
&#25269;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#21452;&#21521;&#36873;&#20030;&#21644;&#20010;&#20307;&#35270;&#35282;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resisting Backdoor Attacks in Federated Learning via Bidirectional Elections and Individual Perspective. (arXiv:2309.16456v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Snowball&#65292;&#19968;&#20010;&#36890;&#36807;&#20010;&#20307;&#35270;&#35282;&#19978;&#30340;&#21452;&#21521;&#36873;&#20030;&#26041;&#27861;&#26469;&#25269;&#25239;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26694;&#26550;&#12290;&#23427;&#36890;&#36807;&#33258;&#19979;&#32780;&#19978;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#36873;&#20030;&#36807;&#31243;&#65292;&#36880;&#27493;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#22810;&#26679;&#24615;&#23548;&#33268;&#27169;&#22411;&#26356;&#26032;&#28151;&#26434;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#25269;&#24481;&#32852;&#37030;&#23398;&#20064;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20943;&#36731;&#24863;&#26579;&#27169;&#22411;&#30340;&#24433;&#21709;&#25110;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;&#21069;&#32773;&#20250;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#33391;&#24615;&#21644;&#24863;&#26579;&#27169;&#22411;&#26356;&#26032;&#20043;&#38388;&#30340;&#20840;&#23616;&#28165;&#26224;&#36793;&#30028;&#30340;&#21028;&#23450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#65292;&#27169;&#22411;&#26356;&#26032;&#22312;&#29616;&#23454;&#20013;&#23481;&#26131;&#28151;&#26434;&#24182;&#20998;&#25955;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#25490;&#38500;&#24863;&#26579;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#19982;&#20197;&#24448;&#20174;&#20840;&#23616;&#35270;&#35282;&#20986;&#21457;&#30340;&#35266;&#28857;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Snowball&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#21518;&#38376;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20010;&#20307;&#35270;&#35282;&#19978;&#30340;&#21452;&#21521;&#36873;&#20030;&#65292;&#21463;&#21040;&#25105;&#20204;&#25512;&#23548;&#20986;&#30340;&#19968;&#20010;&#21407;&#21017;&#21644;&#32852;&#37030;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#21407;&#21017;&#30340;&#21551;&#21457;&#12290;&#23427;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;a&#65289;&#33258;&#19979;&#32780;&#19978;&#30340;&#36873;&#20030;&#65292;&#27599;&#20010;&#20505;&#36873;&#27169;&#22411;&#26356;&#26032;&#23545;&#22810;&#20010;&#23545;&#31561;&#20505;&#36873;&#27169;&#22411;&#26356;&#26032;&#36827;&#34892;&#25237;&#31080;&#65292;&#20197;&#36873;&#20986;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20316;&#20026;&#32858;&#21512;&#30340;&#34987;&#36873;&#39033;&#65307;b&#65289;&#33258;&#19978;&#32780;&#19979;&#30340;&#36873;&#20030;&#65292;&#34987;&#36873;&#39033;&#36880;&#27493;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches defend against backdoor attacks in federated learning (FL) mainly through a) mitigating the impact of infected models, or b) excluding infected models. The former negatively impacts model accuracy, while the latter usually relies on globally clear boundaries between benign and infected model updates. However, model updates are easy to be mixed and scattered throughout in reality due to the diverse distributions of local data. This work focuses on excluding infected models in FL. Unlike previous perspectives from a global view, we propose Snowball, a novel anti-backdoor FL framework through bidirectional elections from an individual perspective inspired by one principle deduced by us and two principles in FL and deep learning. It is characterized by a) bottom-up election, where each candidate model update votes to several peer ones such that a few model updates are elected as selectees for aggregation; and b) top-down election, where selectees progressively enlarge t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#23545;&#21487;&#25805;&#20316;&#35299;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#21644;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16452</link><description>&lt;p&gt;
&#20851;&#20110;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#21487;&#25805;&#20316;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
On the Trade-offs between Adversarial Robustness and Actionable Explanations. (arXiv:2309.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#25239;&#40065;&#26834;&#27169;&#22411;&#23545;&#21487;&#25805;&#20316;&#35299;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#27604;&#36739;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#21644;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#19981;&#20165;&#20855;&#26377;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#32780;&#19988;&#36824;&#33021;&#21521;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#27010;&#24565;&#65292;&#25110;&#32773;&#23427;&#20204;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#23545;&#25239;&#24615;&#40065;&#26834;&#27169;&#22411;&#23545;&#21487;&#25805;&#20316;&#35299;&#37322;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#35299;&#37322;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#20102;&#36861;&#32034;&#26435;&#21033;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#23618;&#38754;&#19978;&#20998;&#26512;&#20102;&#20808;&#36827;&#31639;&#27861;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#65288;&#23454;&#26045;&#30340;&#23481;&#26131;&#31243;&#24230;&#65289;&#21644;&#26377;&#25928;&#24615;&#65288;&#33719;&#24471;&#27491;&#21521;&#27169;&#22411;&#39044;&#27979;&#30340;&#27010;&#29575;&#65289;&#65292;&#24182;&#27604;&#36739;&#20102;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#25239;&#24615;&#40065;&#26834;&#21644;&#38750;&#40065;&#26834;&#27169;&#22411;&#29983;&#25104;&#30340;&#36861;&#32034;&#32467;&#26524;&#30340;&#25104;&#26412;&#21644;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models are increasingly being employed in various high-stakes settings, it becomes important to ensure that predictions of these models are not only adversarially robust, but also readily explainable to relevant stakeholders. However, it is unclear if these two notions can be simultaneously achieved or if there exist trade-offs between them. In this work, we make one of the first attempts at studying the impact of adversarially robust models on actionable explanations which provide end users with a means for recourse. We theoretically and empirically analyze the cost (ease of implementation) and validity (probability of obtaining a positive model prediction) of recourses output by state-of-the-art algorithms when the underlying models are adversarially robust vs. non-robust. More specifically, we derive theoretical bounds on the differences between the cost and the validity of the recourses generated by state-of-the-art algorithms for adversarially robust vs. non-ro
&lt;/p&gt;</description></item><item><title>MPRS&#26159;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#12289;&#20855;&#26377;&#29289;&#29702;&#21551;&#21457;&#30340;&#31354;&#38388;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36317;&#31163;&#30456;&#20851;&#30340;&#8220;&#30456;&#20114;&#20316;&#29992;&#8221;&#26469;&#24341;&#20837;&#31354;&#38388;&#25110;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#31354;&#38388;&#32500;&#24230;&#30340;&#20998;&#25955;&#25968;&#25454;&#12290;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#65292;MPRS&#23637;&#29616;&#20102;&#19982;&#26631;&#20934;&#25554;&#20540;&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22635;&#34917;&#31895;&#31961;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#30340;&#32570;&#21475;&#12290;</title><link>http://arxiv.org/abs/2309.16448</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#32422;&#12289;&#35745;&#31639;&#39640;&#25928;&#30340;&#31354;&#38388;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A parsimonious, computationally efficient machine learning method for spatial regression. (arXiv:2309.16448v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16448
&lt;/p&gt;
&lt;p&gt;
MPRS&#26159;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#12289;&#20855;&#26377;&#29289;&#29702;&#21551;&#21457;&#30340;&#31354;&#38388;&#22238;&#24402;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36317;&#31163;&#30456;&#20851;&#30340;&#8220;&#30456;&#20114;&#20316;&#29992;&#8221;&#26469;&#24341;&#20837;&#31354;&#38388;&#25110;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#31354;&#38388;&#32500;&#24230;&#30340;&#20998;&#25955;&#25968;&#25454;&#12290;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#27979;&#35797;&#20013;&#65292;MPRS&#23637;&#29616;&#20102;&#19982;&#26631;&#20934;&#25554;&#20540;&#26041;&#27861;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22635;&#34917;&#31895;&#31961;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#30340;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25913;&#36827;&#30340;&#24179;&#38754;&#26059;&#36716;&#22120;&#26041;&#27861;&#65288;MPRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29992;&#20110;&#31354;&#38388;/&#26102;&#38388;&#22238;&#24402;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;MPRS&#26159;&#19968;&#20010;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#36890;&#36807;&#30701;&#31243;&#12289;&#36317;&#31163;&#30456;&#20851;&#30340;&#8220;&#30456;&#20114;&#20316;&#29992;&#8221;&#26469;&#24341;&#20837;&#31354;&#38388;&#25110;&#26102;&#38388;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#35774;&#24213;&#23618;&#27010;&#29575;&#20998;&#24067;&#30340;&#29305;&#23450;&#24418;&#24335;&#12290;&#39044;&#27979;&#26159;&#36890;&#36807;&#23436;&#20840;&#33258;&#20027;&#30340;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#30340;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#24179;&#34913;&#26465;&#20214;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#12290;MPRS&#33021;&#22815;&#22788;&#29702;&#20998;&#25955;&#30340;&#25968;&#25454;&#21644;&#20219;&#24847;&#30340;&#31354;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#22312;&#19968;&#32500;&#12289;&#20108;&#32500;&#21644;&#19977;&#32500;&#19978;&#23545;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;MPRS&#30340;&#39044;&#27979;&#24615;&#33021;&#65288;&#26080;&#38656;&#21442;&#25968;&#35843;&#25972;&#65289;&#19982;&#26222;&#36890;&#20811;&#37324;&#37329;&#27861;&#21644;&#36870;&#36317;&#31163;&#21152;&#26435;&#27861;&#31561;&#26631;&#20934;&#25554;&#20540;&#26041;&#27861;&#30456;&#24403;&#12290;&#29305;&#21035;&#26159;&#65292;MPRS&#26159;&#19968;&#31181;&#29305;&#21035;&#26377;&#25928;&#30340;&#22635;&#34917;&#32570;&#21475;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#31895;&#31961;&#21644;&#38750;&#39640;&#26031;&#25968;&#25454;&#65288;&#22914;&#27599;&#26085;&#38477;&#27700;&#26102;&#38388;&#24207;&#21015;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the modified planar rotator method (MPRS), a physically inspired machine learning method for spatial/temporal regression. MPRS is a non-parametric model which incorporates spatial or temporal correlations via short-range, distance-dependent ``interactions'' without assuming a specific form for the underlying probability distribution. Predictions are obtained by means of a fully autonomous learning algorithm which employs equilibrium conditional Monte Carlo simulations. MPRS is able to handle scattered data and arbitrary spatial dimensions. We report tests on various synthetic and real-word data in one, two and three dimensions which demonstrate that the MPRS prediction performance (without parameter tuning) is competitive with standard interpolation methods such as ordinary kriging and inverse distance weighting. In particular, MPRS is a particularly effective gap-filling method for rough and non-Gaussian data (e.g., daily precipitation time series). MPRS shows superior co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#32593;&#32476;&#23558;&#38899;&#39057;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#20840;&#23616;&#21644;&#26102;&#38388;&#19978;&#19982;&#36755;&#20837;&#38899;&#39057;&#23545;&#40784;&#30340;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.16429</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#33258;&#36866;&#24212;&#23454;&#29616;&#22810;&#26679;&#19988;&#23545;&#40784;&#30340;&#38899;&#39057;&#21040;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diverse and Aligned Audio-to-Video Generation via Text-to-Video Model Adaptation. (arXiv:2309.16429v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16429
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#32593;&#32476;&#23558;&#38899;&#39057;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#36755;&#20837;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;&#22312;&#20840;&#23616;&#21644;&#26102;&#38388;&#19978;&#19982;&#36755;&#20837;&#38899;&#39057;&#23545;&#40784;&#30340;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#26469;&#33258;&#21508;&#31181;&#35821;&#20041;&#31867;&#21035;&#30340;&#33258;&#28982;&#38899;&#39057;&#26679;&#26412;&#26469;&#24341;&#23548;&#29983;&#25104;&#22810;&#26679;&#19988;&#36924;&#30495;&#30340;&#35270;&#39057;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#35270;&#39057;&#38656;&#35201;&#22312;&#20840;&#23616;&#21644;&#26102;&#38388;&#19978;&#19982;&#36755;&#20837;&#38899;&#39057;&#36827;&#34892;&#23545;&#40784;&#65306;&#20840;&#23616;&#19978;&#65292;&#36755;&#20837;&#38899;&#39057;&#19982;&#25972;&#20010;&#36755;&#20986;&#35270;&#39057;&#26377;&#35821;&#20041;&#20851;&#32852;&#65307;&#26102;&#38388;&#19978;&#65292;&#36755;&#20837;&#38899;&#39057;&#30340;&#27599;&#20010;&#29255;&#27573;&#37117;&#19982;&#30456;&#24212;&#30340;&#35270;&#39057;&#29255;&#27573;&#20851;&#32852;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#39537;&#21160;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#36866;&#37197;&#22120;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#23398;&#20064;&#23558;&#22522;&#20110;&#38899;&#39057;&#30340;&#34920;&#31034;&#26144;&#23556;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#25152;&#26399;&#26395;&#30340;&#36755;&#20837;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#23427;&#20063;&#21487;&#20197;&#23454;&#29616;&#22522;&#20110;&#25991;&#26412;&#12289;&#38899;&#39057;&#20197;&#21450;&#25991;&#26412;&#21644;&#38899;&#39057;&#30340;&#35270;&#39057;&#29983;&#25104;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#38899;&#39057;-&#35270;&#39057;&#26679;&#26412;&#30340;&#26174;&#33879;&#35821;&#20041;&#22810;&#26679;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of generating diverse and realistic videos guided by natural audio samples from a wide variety of semantic classes. For this task, the videos are required to be aligned both globally and temporally with the input audio: globally, the input audio is semantically associated with the entire output video, and temporally, each segment of the input audio is associated with a corresponding segment of that video. We utilize an existing text-conditioned video generation model and a pre-trained audio encoder model. The proposed method is based on a lightweight adaptor network, which learns to map the audio-based representation to the input representation expected by the text-to-video generation model. As such, it also enables video generation conditioned on text, audio, and, for the first time as far as we can ascertain, on both text and audio. We validate our method extensively on three datasets demonstrating significant semantic diversity of audio-video samples and further
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;NMPC&#65289;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22686;&#37327;ISS&#31995;&#32479;&#12290;&#36890;&#36807;&#31616;&#21270;&#35745;&#31639;&#32456;&#31471;&#25104;&#20998;&#65292;&#24182;&#26126;&#30830;&#23450;&#20041;&#26368;&#23567;&#39044;&#27979;&#33539;&#22260;&#65292;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;GRU&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#36523;&#23450;&#21046;&#29366;&#24577;&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#25511;&#21046;&#26550;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16428</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;MPC&#35774;&#35745;&#24212;&#29992;&#20110;&#22686;&#37327;ISS&#31995;&#32479;&#65292;&#20197;GRU&#32593;&#32476;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Nonlinear MPC design for incrementally ISS systems with application to GRU networks. (arXiv:2309.16428v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;NMPC&#65289;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22686;&#37327;ISS&#31995;&#32479;&#12290;&#36890;&#36807;&#31616;&#21270;&#35745;&#31639;&#32456;&#31471;&#25104;&#20998;&#65292;&#24182;&#26126;&#30830;&#23450;&#20041;&#26368;&#23567;&#39044;&#27979;&#33539;&#22260;&#65292;&#23454;&#29616;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;GRU&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#36523;&#23450;&#21046;&#29366;&#24577;&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#35813;&#25511;&#21046;&#26550;&#26500;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#25351;&#25968;&#22686;&#37327;&#36755;&#20837;-&#29366;&#24577;&#31283;&#23450;&#65288;ISS&#65289;&#31995;&#32479;&#35774;&#35745;&#20102;&#19968;&#31181;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;NMPC&#65289;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20844;&#24335;&#65292;&#26080;&#38656;&#35745;&#31639;&#32456;&#31471;&#25104;&#20998;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#26368;&#23567;&#39044;&#27979;&#33539;&#22260;&#20197;&#30830;&#20445;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#35813;&#35774;&#35745;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#30001;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;RNNs&#20197;&#20854;&#22686;&#24378;&#24314;&#27169;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#32780;&#22686;&#37327;ISS&#23646;&#24615;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#20195;&#25968;&#26465;&#20214;&#36827;&#34892;&#30740;&#31350;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#32593;&#32476;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#20855;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#37327;&#36523;&#23450;&#21046;&#29366;&#24577;&#35266;&#23519;&#22120;&#30340;&#35774;&#35745;&#26041;&#27861;&#12290;&#25152;&#24471;&#21040;&#30340;&#25511;&#21046;&#26550;&#26500;&#22312;&#22522;&#20934;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#20854;&#33391;&#22909;&#30340;&#25511;&#21046;&#24615;&#33021;&#21644;&#39640;&#25928;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This brief addresses the design of a Nonlinear Model Predictive Control (NMPC) strategy for exponentially incremental Input-to-State Stable (ISS) systems. In particular, a novel formulation is devised, which does not necessitate the onerous computation of terminal ingredients, but rather relies on the explicit definition of a minimum prediction horizon ensuring closed-loop stability. The designed methodology is particularly suited for the control of systems learned by Recurrent Neural Networks (RNNs), which are known for their enhanced modeling capabilities and for which the incremental ISS properties can be studied thanks to simple algebraic conditions. The approach is applied to Gated Recurrent Unit (GRU) networks, providing also a method for the design of a tailored state observer with convergence guarantees. The resulting control architecture is tested on a benchmark system, demonstrating its good control performances and efficient applicability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2309.16414</link><description>&lt;p&gt;
AutoCLIP: &#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoCLIP&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35843;&#35856;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;AutoCLIP&#36890;&#36807;&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#20174;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#25512;&#23548;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#26500;&#24314;&#30340;&#20998;&#31867;&#22120;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#26681;&#25454;&#25552;&#31034;&#27169;&#26495;&#33258;&#21160;&#21019;&#24314;&#27599;&#20010;&#31867;&#21035;&#30340;&#25551;&#36848;&#31526;&#38598;&#30340;&#19981;&#21516;&#26041;&#24335;&#65292;&#21253;&#25324;&#25163;&#24037;&#35774;&#35745;&#30340;&#27169;&#26495;&#12289;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#30340;&#27169;&#26495;&#20197;&#21450;&#20174;&#38543;&#26426;&#21333;&#35789;&#21644;&#23383;&#31526;&#26500;&#24314;&#30340;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#20174;&#30456;&#24212;&#30340;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#23548;&#20986;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65306;&#23558;&#22270;&#20687;&#30340;&#24179;&#22343;&#32534;&#30721;&#31867;&#21035;&#25551;&#36848;&#31526;&#19982;&#32534;&#30721;&#22270;&#20687;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#26368;&#22823;&#21270;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#26576;&#20123;&#25551;&#36848;&#31526;&#27604;&#20854;&#20182;&#25551;&#36848;&#31526;&#26356;&#22909;&#22320;&#21305;&#37197;&#32473;&#23450;&#22270;&#20687;&#19978;&#30340;&#35270;&#35273;&#32447;&#32034;&#26102;&#65292;&#23558;&#25152;&#26377;&#31867;&#21035;&#25551;&#36848;&#31526;&#31561;&#26435;&#37325;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35843;&#35856;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;AutoCLIP&#12290;AutoCLIP&#20026;&#27599;&#20010;&#25552;&#31034;&#27169;&#26495;&#20998;&#37197;&#20102;&#22270;&#20687;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#36825;&#20123;&#26435;&#37325;&#26159;&#20174;s
&lt;/p&gt;
&lt;p&gt;
Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#39564;&#32473;&#23450;&#26465;&#20214;&#26041;&#24046;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20801;&#35768;&#32771;&#34385;&#26041;&#24046;&#26412;&#36523;&#30340;&#20540;&#20197;&#21450;&#23545;&#24212;&#26041;&#24046;&#39044;&#27979;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#39118;&#38505;&#30340;&#38750;&#28176;&#36817;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.16412</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#39564;&#36827;&#34892;&#36873;&#25321;&#24615;&#38750;&#21442;&#25968;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Selective Nonparametric Regression via Testing. (arXiv:2309.16412v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#39564;&#32473;&#23450;&#26465;&#20214;&#26041;&#24046;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#38750;&#21442;&#25968;&#22238;&#24402;&#26041;&#27861;&#65292;&#20801;&#35768;&#32771;&#34385;&#26041;&#24046;&#26412;&#36523;&#30340;&#20540;&#20197;&#21450;&#23545;&#24212;&#26041;&#24046;&#39044;&#27979;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20272;&#35745;&#22120;&#30340;&#39118;&#38505;&#30340;&#38750;&#28176;&#36817;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20855;&#26377;&#35823;&#24046;&#25935;&#24863;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#39044;&#27979;&#20013;&#30340;&#25918;&#24323;&#21487;&#33021;&#24615;&#65288;&#25110;&#36873;&#25321;&#24615;&#39044;&#27979;&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#34429;&#28982;&#20998;&#31867;&#35774;&#32622;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#36873;&#25321;&#24615;&#26041;&#27861;&#21457;&#23637;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#38750;&#21442;&#25968;&#24322;&#26041;&#24046;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#39564;&#32473;&#23450;&#28857;&#22788;&#26465;&#20214;&#26041;&#24046;&#30340;&#20551;&#35774;&#26469;&#24320;&#21457;&#19968;&#20010;&#25918;&#24323;&#31243;&#24207;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#20801;&#35768;&#32771;&#34385;&#26041;&#24046;&#26412;&#36523;&#30340;&#20540;&#65292;&#36824;&#20801;&#35768;&#32771;&#34385;&#23545;&#24212;&#26041;&#24046;&#39044;&#27979;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#24471;&#20272;&#35745;&#22120;&#30340;&#39118;&#38505;&#30340;&#38750;&#28176;&#36817;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#20960;&#31181;&#19981;&#21516;&#25910;&#25947;&#27169;&#24335;&#30340;&#23384;&#22312;&#12290;&#29702;&#35770;&#20998;&#26512;&#19982;&#19968;&#31995;&#21015;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#19968;&#36215;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction with the possibility of abstention (or selective prediction) is an important problem for error-critical machine learning applications. While well-studied in the classification setup, selective approaches to regression are much less developed. In this work, we consider the nonparametric heteroskedastic regression problem and develop an abstention procedure via testing the hypothesis on the value of the conditional variance at a given point. Unlike existing methods, the proposed one allows to account not only for the value of the variance itself but also for the uncertainty of the corresponding variance predictor. We prove non-asymptotic bounds on the risk of the resulting estimator and show the existence of several different convergence regimes. Theoretical analysis is illustrated with a series of experiments on simulated and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21512;&#25104;&#27835;&#30103;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#28304;&#32676;&#20307;&#30340;&#27835;&#30103;&#32452;&#21152;&#26435;&#28151;&#21512;&#26469;&#26500;&#24314;&#30446;&#26631;&#20154;&#32676;&#30340;&#21512;&#25104;&#27835;&#30103;&#32452;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#20272;&#35745;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#22312;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#20805;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16409</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21512;&#25104;&#27835;&#30103;&#32452;
&lt;/p&gt;
&lt;p&gt;
Constructing Synthetic Treatment Groups without the Mean Exchangeability Assumption. (arXiv:2309.16409v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27809;&#26377;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#21512;&#25104;&#27835;&#30103;&#32452;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#28304;&#32676;&#20307;&#30340;&#27835;&#30103;&#32452;&#21152;&#26435;&#28151;&#21512;&#26469;&#26500;&#24314;&#30446;&#26631;&#20154;&#32676;&#30340;&#21512;&#25104;&#27835;&#30103;&#32452;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#26465;&#20214;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#20272;&#35745;&#26435;&#37325;&#12290;&#35813;&#26041;&#27861;&#22312;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#20805;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#23558;&#22810;&#20010;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#20449;&#24687;&#20256;&#36882;&#32473;&#25105;&#20204;&#20165;&#26377;&#25511;&#21046;&#32452;&#25968;&#25454;&#30340;&#30446;&#26631;&#20154;&#32676;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22343;&#20540;&#20114;&#25442;&#24615;&#30340;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#25152;&#25351;&#20986;&#30340;&#65292;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#21487;&#33021;&#34987;&#36829;&#21453;&#12290;&#21463;&#21512;&#25104;&#25511;&#21046;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28304;&#32676;&#20307;&#30340;&#27835;&#30103;&#32452;&#21152;&#26435;&#28151;&#21512;&#26500;&#24314;&#20102;&#30446;&#26631;&#20154;&#32676;&#30340;&#21512;&#25104;&#27835;&#30103;&#32452;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#23567;&#21270;&#28304;&#32676;&#20307;&#30340;&#21152;&#26435;&#23545;&#29031;&#32452;&#19982;&#30446;&#26631;&#20154;&#32676;&#20043;&#38388;&#30340;&#26465;&#20214;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#26469;&#20272;&#35745;&#26435;&#37325;&#12290;&#25105;&#20204;&#22522;&#20110;&#31579;&#36873;&#21322;&#21442;&#25968;&#29702;&#35770;&#24314;&#31435;&#20102;&#21512;&#25104;&#27835;&#30103;&#32452;&#20272;&#35745;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#12290;&#24403;&#22343;&#20540;&#20114;&#25442;&#24615;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#34917;&#20805;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this work is to transport the information from multiple randomized controlled trials to the target population where we only have the control group data. Previous works rely critically on the mean exchangeability assumption. However, as pointed out by many current studies, the mean exchangeability assumption might be violated. Motivated by the synthetic control method, we construct a synthetic treatment group for the target population by a weighted mixture of treatment groups of source populations. We estimate the weights by minimizing the conditional maximum mean discrepancy between the weighted control groups of source populations and the target population. We establish the asymptotic normality of the synthetic treatment group estimator based on the sieve semiparametric theory. Our method can serve as a novel complementary approach when the mean exchangeability assumption is violated. Experiments are conducted on synthetic and real-world datasets to demonstrate the effe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#28508;&#31354;&#38388;&#36827;&#34892;RNO-G&#25968;&#25454;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#22312;&#26377;&#22122;&#22768;&#21644;&#38745;&#40664;&#35266;&#27979;&#31449;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#21306;&#20998;&#22810;&#20010;&#22122;&#22768;&#31867;&#21035;&#65292;&#24182;&#23450;&#37327;&#20998;&#26512;&#20854;&#20013;&#30340;&#29289;&#29702;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2309.16401</link><description>&lt;p&gt;
&#22522;&#20110;VAE&#30340;RNO-G&#25968;&#25454;&#28508;&#31354;&#38388;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
VAE-based latent-space classification of RNO-G data. (arXiv:2309.16401v1 [astro-ph.HE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#28508;&#31354;&#38388;&#36827;&#34892;RNO-G&#25968;&#25454;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#22312;&#26377;&#22122;&#22768;&#21644;&#38745;&#40664;&#35266;&#27979;&#31449;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#33258;&#21160;&#26816;&#27979;&#21644;&#21306;&#20998;&#22810;&#20010;&#22122;&#22768;&#31867;&#21035;&#65292;&#24182;&#23450;&#37327;&#20998;&#26512;&#20854;&#20013;&#30340;&#29289;&#29702;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26684;&#38517;&#20848;&#23707;&#30340;&#23556;&#30005;&#20013;&#24494;&#23376;&#35266;&#27979;&#31449;(RNO-G)&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#27979;&#36229;&#39640;&#33021;&#20013;&#24494;&#23376;&#30340;&#23556;&#30005;&#22825;&#32447;&#38453;&#21015;&#65292;&#20301;&#20110;&#26684;&#38517;&#20848;&#23792;&#31449;&#12290;&#30446;&#21069;&#20173;&#22312;&#24314;&#35774;&#20013;&#65292;&#30446;&#21069;&#24050;&#26377;7&#20010;&#35266;&#27979;&#31449;&#36816;&#20316;&#12290;&#20013;&#24494;&#23376;&#25506;&#27979;&#36890;&#36807;&#27979;&#37327;&#30001;&#20013;&#24494;&#23376;&#19982;&#26680;&#23376;&#30456;&#20114;&#20316;&#29992;&#20135;&#29983;&#30340;&#38463;&#26031;&#21345;&#26519;&#36752;&#23556;&#26469;&#23454;&#29616;&#12290;&#20013;&#24494;&#23376;&#20505;&#36873;&#20107;&#20214;&#24517;&#39035;&#20174;&#20854;&#20182;&#32972;&#26223;&#20013;&#25214;&#21040;&#65292;&#36825;&#20123;&#32972;&#26223;&#30340;&#35760;&#24405;&#29575;&#35201;&#39640;&#24471;&#22810;&#65292;&#21253;&#25324;&#23431;&#23449;&#23556;&#32447;&#21644;&#20154;&#20026;&#22122;&#22768;&#31561;&#65292;&#26377;&#26102;&#20854;&#26469;&#28304;&#26159;&#26410;&#30693;&#30340;&#12290;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#31354;&#38388;&#26469;&#23545;&#19981;&#21516;&#22122;&#22768;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#28508;&#31354;&#38388;&#24418;&#25104;&#20102;&#19968;&#31181;&#32039;&#20945;&#30340;&#34920;&#31034;&#65292;&#20351;&#24471;&#20998;&#31867;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#26377;&#22122;&#22768;&#21644;&#19968;&#20010;&#38745;&#40664;&#35266;&#27979;&#31449;&#30340;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#24182;&#20801;&#35768;&#25105;&#20204;&#23450;&#24615;&#22320;&#21306;&#20998;&#22810;&#20010;&#20107;&#20214;&#31867;&#21035;&#65292;&#21253;&#25324;&#39118;&#24341;&#36215;&#30340;&#29289;&#29702;&#20449;&#21495;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#26377;&#22122;&#22768;&#21644;&#38745;&#40664;&#35266;&#27979;&#31449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Radio Neutrino Observatory in Greenland (RNO-G) is a radio-based ultra-high energy neutrino detector located at Summit Station, Greenland. It is still being constructed, with 7 stations currently operational. Neutrino detection works by measuring Askaryan radiation produced by neutrino-nucleon interactions. A neutrino candidate must be found amidst other backgrounds which are recorded at much higher rates -- including cosmic-rays and anthropogenic noise -- the origins of which are sometimes unknown. Here we describe a method to classify different noise classes using the latent space of a variational autoencoder. The latent space forms a compact representation that makes classification tractable. We analyze data from a noisy and a silent station. The method automatically detects and allows us to qualitatively separate multiple event classes, including physical wind-induced signals, for both the noisy and the quiet station.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;&#24046;&#20998;&#38544;&#31169;&#22312;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23457;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12289;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#30340;&#25913;&#36827;&#12289;&#23545;&#21508;&#31181;&#23041;&#32961;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#12289;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16398</link><description>&lt;p&gt;
&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#24046;&#20998;&#38544;&#31169;&#30340;&#26368;&#26032;&#36827;&#23637;&#65306;&#31995;&#32479;&#24615;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Advances of Differential Privacy in Centralized Deep Learning: A Systematic Survey. (arXiv:2309.16398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;&#24046;&#20998;&#38544;&#31169;&#22312;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23457;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#12289;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#30340;&#25913;&#36827;&#12289;&#23545;&#21508;&#31181;&#23041;&#32961;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#12289;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#21463;&#27426;&#36814;&#30340;&#25968;&#25454;&#20445;&#25252;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22240;&#20026;&#23427;&#20801;&#35768;&#21046;&#23450;&#20005;&#26684;&#30340;&#25968;&#23398;&#38544;&#31169;&#20445;&#35777;&#12290;&#26412;&#35843;&#26597;&#27010;&#36848;&#20102;&#24046;&#20998;&#38544;&#31169;&#38598;&#20013;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#29366;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#21644;&#24320;&#25918;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#39046;&#22495;&#28508;&#22312;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;&#26681;&#25454;&#31995;&#32479;&#24615;&#25991;&#29486;&#32508;&#36848;&#25152;&#31034;&#65292;&#35752;&#35770;&#20102;&#20197;&#19979;&#20027;&#39064;&#65306;&#31169;&#26377;&#27169;&#22411;&#30340;&#23457;&#35745;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#38544;&#31169;&#25928;&#29992;&#26435;&#34913;&#30340;&#25913;&#36827;&#65292;&#23545;&#24191;&#27867;&#23041;&#32961;&#21644;&#25915;&#20987;&#30340;&#20445;&#25252;&#65292;&#24046;&#20998;&#38544;&#31169;&#29983;&#25104;&#27169;&#22411;&#20197;&#21450;&#26032;&#20852;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy has become a widely popular method for data protection in machine learning, especially since it allows formulating strict mathematical privacy guarantees. This survey provides an overview of the state-of-the-art of differentially private centralized deep learning, thorough analyses of recent advances and open problems, as well as a discussion of potential future developments in the field. Based on a systematic literature review, the following topics are addressed: auditing and evaluation methods for private models, improvements of privacy-utility trade-offs, protection against a broad range of threats and attacks, differentially private generative models, and emerging application domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.16397</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Decision Transformer for Stochastic Driving Environments. (arXiv:2309.16397v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#36890;&#36807;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#65292;&#21462;&#20195;&#20102;&#20840;&#23616;&#22238;&#25253;&#12290;&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#35299;&#20915;&#22312;&#19981;&#30830;&#23450;&#24615;&#29615;&#22659;&#20013;&#36807;&#20110;&#20048;&#35266;&#30340;&#38382;&#39064;&#65292;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26080;&#38656;&#20027;&#21160;&#20132;&#20114;&#30340;&#23398;&#20064;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#26694;&#26550;&#65292;&#22240;&#27492;&#22312;&#33258;&#20027;&#39550;&#39542;&#20219;&#21153;&#20013;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#26368;&#36817;Transformers&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#23558;&#31163;&#32447;RL&#35270;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#36825;&#22312;&#38271;&#26399;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20048;&#35266;&#65292;&#38169;&#35823;&#22320;&#20551;&#35774;&#30456;&#21516;&#30340;&#30446;&#26631;&#21487;&#20197;&#36890;&#36807;&#30456;&#21516;&#30340;&#21160;&#20316;&#19968;&#33268;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#38543;&#26426;&#39550;&#39542;&#29615;&#22659;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20915;&#31574;Transformer&#65288;UNREST&#65289;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#36716;&#25442;&#27169;&#22411;&#25110;&#22797;&#26434;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;UNREST&#36890;&#36807;&#36716;&#25442;&#19982;&#22238;&#25253;&#20043;&#38388;&#30340;&#26465;&#20214;&#20114;&#20449;&#24687;&#26469;&#20272;&#35745;&#29366;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#30456;&#24212;&#22320;&#20998;&#21106;&#24207;&#21015;&#12290;&#36890;&#36807;&#21457;&#29616;&#39550;&#39542;&#29615;&#22659;&#30340;&#8220;&#19981;&#30830;&#23450;&#24615;&#32047;&#31215;&#8221;&#21644;&#8220;&#26102;&#38388;&#23616;&#37096;&#24615;&#8221;&#29305;&#24615;&#65292;UNREST&#23558;&#20915;&#31574;Transformer&#20013;&#30340;&#20840;&#23616;&#22238;&#25253;&#26367;&#25442;&#20026;&#36739;&#23569;&#30340;&#37096;&#20998;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning (RL) has emerged as a promising framework for learning policies without active interactions, making it especially appealing for autonomous driving tasks. Recent successes of Transformers inspire casting offline RL as sequence modeling, which performs well in long-horizon tasks. However, they are overly optimistic in stochastic environments with incorrect assumptions that the same goal can be consistently achieved by identical actions. In this paper, we introduce an UNcertainty-awaRE deciSion Transformer (UNREST) for planning in stochastic driving environments without introducing additional transition or complex generative models. Specifically, UNREST estimates state uncertainties by the conditional mutual information between transitions and returns, and segments sequences accordingly. Discovering the `uncertainty accumulation' and `temporal locality' properties of driving environments, UNREST replaces the global returns in decision transformers with less 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.16391</link><description>&lt;p&gt;
&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;&#20108;&#32500;Copula&#36924;&#36817;&#21464;&#25442;&#65306;2-Cats&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Differential 2D Copula Approximating Transforms via Sobolev Training: 2-Cats Networks. (arXiv:2309.16391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;Sobolev&#35757;&#32451;&#30340;2-Cats&#32593;&#32476;&#65292;&#23427;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#65292;&#24182;&#19988;&#22312;&#20272;&#35745;&#36755;&#20986;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Copula&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#32479;&#35745;&#24037;&#20855;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#32500;&#24230;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#24212;&#29992;Copula&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#39318;&#20808;&#20272;&#35745;&#29420;&#31435;&#30340;&#36793;&#38469;&#20998;&#24067;&#65288;&#19968;&#20010;&#31616;&#21333;&#20219;&#21153;&#65289;&#65292;&#28982;&#21518;&#20272;&#35745;&#36830;&#25509;&#36793;&#38469;&#30340;&#21333;&#20010;Copula&#20989;&#25968;C&#65288;&#19968;&#20010;&#22256;&#38590;&#20219;&#21153;&#65289;&#26469;&#20272;&#35745;&#22810;&#20803;&#20998;&#24067;&#20989;&#25968;&#12290;&#23545;&#20110;&#20108;&#32500;&#25968;&#25454;&#65292;Copula&#26159;&#19968;&#20010;&#24418;&#22914;C&#65306;(u&#65292;v)&#8712;\mathbf{I}^2\rightarrow \mathbf{I}&#30340;&#20108;&#27425;&#22686;&#20989;&#25968;&#65292;&#20854;&#20013;\mathbf{I}=[0&#65292;1]&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#65288;NNs&#65289;&#22914;&#20309;&#33021;&#22815;&#38750;&#21442;&#25968;&#22320;&#36924;&#36817;&#20219;&#20309;&#20108;&#32500;Copula&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;2-Cats&#65292;&#21463;&#21040;&#29289;&#29702;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;Sobolev&#35757;&#32451;&#25991;&#29486;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#19981;&#20165;&#35777;&#26126;&#20102;&#25105;&#20204;&#33021;&#22815;&#27604;&#29616;&#26377;&#25216;&#26415;&#26356;&#22909;&#22320;&#20272;&#35745;2D Copula&#30340;&#36755;&#20986;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#19988;&#31526;&#21512;Copula C&#30340;&#25968;&#23398;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Copulas are a powerful statistical tool that captures dependencies across data dimensions. When applying Copulas, we can estimate multivariate distribution functions by initially estimating independent marginals, an easy task, and then a single copulating function, $C$, to connect the marginals, a hard task. For two-dimensional data, a copula is a two-increasing function of the form $C: (u,v)\in \mathbf{I}^2 \rightarrow \mathbf{I}$, where $\mathbf{I} = [0, 1]$. In this paper, we show how Neural Networks (NNs) can approximate any two-dimensional copula non-parametrically. Our approach, denoted as 2-Cats, is inspired by the Physics-Informed Neural Networks and Sobolev Training literature. Not only do we show that we can estimate the output of a 2d Copula better than the state-of-the-art, our approach is non-parametric and respects the mathematical properties of a Copula $C$.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#20132;&#25442;$k$-Means++&#31639;&#27861;&#30340;&#25913;&#36827;&#21644;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$k$-means&#32858;&#31867;&#38382;&#39064;&#20013;&#33021;&#22815;&#33719;&#24471;$9 + \varepsilon$&#36817;&#20284;&#27604;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.16384</link><description>&lt;p&gt;
&#22810;&#20132;&#25442;$k$-Means++&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-Swap $k$-Means++. (arXiv:2309.16384v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#20132;&#25442;$k$-Means++&#31639;&#27861;&#30340;&#25913;&#36827;&#21644;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;$k$-means&#32858;&#31867;&#38382;&#39064;&#20013;&#33021;&#22815;&#33719;&#24471;$9 + \varepsilon$&#36817;&#20284;&#27604;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Arthur&#21644;Vassilvitskii&#25552;&#20986;&#30340;$k$-means++&#31639;&#27861;&#36890;&#24120;&#34987;&#23454;&#36341;&#32773;&#36873;&#25321;&#29992;&#20110;&#20248;&#21270;&#27969;&#34892;&#30340;$k$-means&#32858;&#31867;&#30446;&#26631;&#65292;&#24182;&#22312;&#26399;&#26395;&#20013;&#33719;&#24471;$O(\log k)$&#30340;&#36817;&#20284;&#24230;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#65292;Lattanzi&#21644;Sohler&#25552;&#20986;&#20102;&#36890;&#36807;$k$-means++&#37319;&#26679;&#20998;&#24067;&#33719;&#24471;&#30340;$O(k \log \log k)$&#20010;&#23616;&#37096;&#25628;&#32034;&#27493;&#39588;&#30340;&#22686;&#24378;$k$-means++&#31639;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;$k$-means&#32858;&#31867;&#38382;&#39064;&#30340;$c$&#36817;&#20284;&#35299;&#65292;&#20854;&#20013;$c$&#26159;&#19968;&#20010;&#36739;&#22823;&#30340;&#24120;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#26356;&#22823;&#26356;&#22797;&#26434;&#30340;&#23616;&#37096;&#25628;&#32034;&#37051;&#22495;&#26469;&#25512;&#24191;&#21644;&#25193;&#23637;&#20182;&#20204;&#30340;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#20174;&#32780;&#21487;&#20197;&#21516;&#26102;&#20132;&#25442;&#22810;&#20010;&#20013;&#24515;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;$9 + \varepsilon$&#30340;&#36817;&#20284;&#27604;&#65292;&#36825;&#26159;&#23616;&#37096;&#25628;&#32034;&#21487;&#33021;&#30340;&#26368;&#20339;&#32467;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#30340;&#25913;&#36827;&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#19982;Lattanzi&#21644;Sohler&#30340;&#26041;&#27861;&#30456;&#27604;&#30340;&#26174;&#33879;&#36136;&#37327;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The $k$-means++ algorithm of Arthur and Vassilvitskii (SODA 2007) is often the practitioners' choice algorithm for optimizing the popular $k$-means clustering objective and is known to give an $O(\log k)$-approximation in expectation. To obtain higher quality solutions, Lattanzi and Sohler (ICML 2019) proposed augmenting $k$-means++ with $O(k \log \log k)$ local search steps obtained through the $k$-means++ sampling distribution to yield a $c$-approximation to the $k$-means clustering problem, where $c$ is a large absolute constant. Here we generalize and extend their local search algorithm by considering larger and more sophisticated local search neighborhoods hence allowing to swap multiple centers at the same time. Our algorithm achieves a $9 + \varepsilon$ approximation ratio, which is the best possible for local search. Importantly we show that our approach yields substantial practical improvements, we show significant quality improvements over the approach of Lattanzi and Sohler 
&lt;/p&gt;</description></item><item><title>RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.16382</link><description>&lt;p&gt;
RLLTE&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#38271;&#26399;&#28436;&#36827;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
RLLTE: Long-Term Evolution Project of Reinforcement Learning. (arXiv:2309.16382v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16382
&lt;/p&gt;
&lt;p&gt;
RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RLLTE&#65306;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#19982;&#24212;&#29992;&#26694;&#26550;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#27969;&#30340;&#31639;&#27861;&#23454;&#29616;&#20043;&#22806;&#65292;RLLTE&#36824;&#20316;&#20026;&#19968;&#20010;&#31639;&#27861;&#24320;&#21457;&#24037;&#20855;&#21253;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;RLLTE&#23436;&#20840;&#35299;&#32806;&#20102;RL&#31639;&#27861;&#19982;&#24320;&#21457;&#31639;&#27861;&#30340;&#23454;&#36341;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#32452;&#20214;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#28436;&#36827;&#12290;&#29305;&#21035;&#22320;&#65292;RLLTE&#26159;&#31532;&#19968;&#20010;&#26500;&#24314;&#20102;&#23436;&#25972;&#20016;&#23500;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;RL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#37096;&#32626;&#12289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24515;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#30340;&#21103;&#39550;&#39542;&#12290;&#39044;&#26399;RLLTE&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#65292;&#24182;&#23545;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#20855;&#26377;&#39640;&#24230;&#21050;&#28608;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of components to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.
&lt;/p&gt;</description></item><item><title>MHG-GNN&#26159;&#19968;&#31181;&#23558;&#20998;&#23376;&#36229;&#22270;&#35821;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#22810;&#26679;&#26448;&#26009;&#30340;&#29289;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16374</link><description>&lt;p&gt;
MHG-GNN: &#20998;&#23376;&#36229;&#22270;&#35821;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
MHG-GNN: Combination of Molecular Hypergraph Grammar with Graph Neural Network. (arXiv:2309.16374v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16374
&lt;/p&gt;
&lt;p&gt;
MHG-GNN&#26159;&#19968;&#31181;&#23558;&#20998;&#23376;&#36229;&#22270;&#35821;&#27861;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#22810;&#26679;&#26448;&#26009;&#30340;&#29289;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#24615;&#39044;&#27979;&#22312;&#26448;&#26009;&#21457;&#29616;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#20316;&#20026;&#26368;&#32456;&#20026;&#26448;&#26009;&#31185;&#23398;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;MHG-GNN&#65292;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#19982;&#20998;&#23376;&#36229;&#22270;&#35821;&#27861;&#65288;MHG&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#22810;&#26679;&#26448;&#26009;&#30340;&#21508;&#31181;&#29289;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MHG-GNN&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Property prediction plays an important role in material discovery. As an initial step to eventually develop a foundation model for material science, we introduce a new autoencoder called the MHG-GNN, which combines graph neural network (GNN) with Molecular Hypergraph Grammar (MHG). Results on a variety of property prediction tasks with diverse materials show that MHG-GNN is promising.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38899;&#39057;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#38160;&#24230;&#26356;&#39640;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20174;&#20043;&#21069;&#26410;&#35265;&#35774;&#22791;&#24405;&#21046;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#26469;&#35828;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#26159;&#26368;&#23567;&#20540;&#38160;&#24230;&#30340;&#20027;&#35201;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21487;&#27604;&#24615;&#26041;&#38754;&#30340;&#30456;&#20851;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.16369</link><description>&lt;p&gt;
&#25226;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#30340;&#35752;&#35770;&#24102;&#20837;&#21040;&#38899;&#39057;&#39046;&#22495;:&#19968;&#20010;&#29992;&#20110;&#22768;&#23398;&#22330;&#26223;&#20998;&#31867;&#30340;&#28388;&#27874;&#22120;&#24402;&#19968;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification. (arXiv:2309.16369v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#38899;&#39057;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#21457;&#29616;&#38160;&#24230;&#26356;&#39640;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20174;&#20043;&#21069;&#26410;&#35265;&#35774;&#22791;&#24405;&#21046;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#26469;&#35828;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#26159;&#26368;&#23567;&#20540;&#38160;&#24230;&#30340;&#20027;&#35201;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21487;&#27604;&#24615;&#26041;&#38754;&#30340;&#30456;&#20851;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#25439;&#22833;&#20989;&#25968;&#26368;&#23567;&#20540;&#30340;&#38160;&#24230;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#36873;&#23450;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#20294;&#25105;&#20204;&#22312;DCASE2020&#25361;&#25112;&#25968;&#25454;&#30340;&#38899;&#39057;&#22330;&#26223;&#20998;&#31867;&#20219;&#21153;&#20013;&#25506;&#32034;&#20102;&#36825;&#19968;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#20108;&#32500;&#28388;&#27874;&#22120;&#24402;&#19968;&#21270;&#21487;&#35270;&#21270;&#21644;&#27966;&#29983;&#30340;&#38160;&#24230;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#38160;&#24230;&#26356;&#39640;&#30340;&#26368;&#23567;&#20540;&#36890;&#24120;&#27604;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20174;&#20043;&#21069;&#26410;&#35265;&#35774;&#22791;&#24405;&#21046;&#30340;&#39046;&#22495;&#22806;&#25968;&#25454;&#26469;&#35828;&#12290;&#36825;&#36827;&#19968;&#27493;&#21152;&#28145;&#20102;&#20851;&#20110;&#24179;&#22374;&#26368;&#23567;&#20540;&#27867;&#21270;&#33021;&#21147;&#30340;&#20105;&#35758;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#29305;&#21035;&#26159;&#20248;&#21270;&#22120;&#30340;&#36873;&#25321;&#26159;&#26368;&#23567;&#20540;&#38160;&#24230;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#35752;&#35770;&#20102;&#22312;&#21487;&#27604;&#24615;&#26041;&#38754;&#30340;&#30456;&#20851;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#35757;&#32451;&#27169;&#22411;&#29366;&#24577;&#21644;&#25439;&#22833;&#22270;&#26223;&#21487;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The correlation between the sharpness of loss minima and generalisation in the context of deep neural networks has been subject to discussion for a long time. Whilst mostly investigated in the context of selected benchmark data sets in the area of computer vision, we explore this aspect for the audio scene classification task of the DCASE2020 challenge data. Our analysis is based on twodimensional filter-normalised visualisations and a derived sharpness measure. Our exploratory analysis shows that sharper minima tend to show better generalisation than flat minima -even more so for out-of-domain data, recorded from previously unseen devices-, thus adding to the dispute about better generalisation capabilities of flat minima. We further find that, in particular, the choice of optimisers is a main driver of the sharpness of minima and we discuss resulting limitations with respect to comparability. Our code, trained model states and loss landscape visualisations are publicly available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;TEMT&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#22686;&#24378;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#20013;&#36827;&#34892;&#26102;&#38388;&#38388;&#38548;&#39044;&#27979;&#21644;&#24402;&#32435;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.16357</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#22686;&#24378;&#30340;&#26102;&#38388;&#38388;&#38548;&#39044;&#27979;&#22312;&#26102;&#24577;&#30693;&#35782;&#22270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs. (arXiv:2309.16357v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16357
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;TEMT&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#22686;&#24378;&#30340;&#26102;&#24577;&#30693;&#35782;&#22270;&#20013;&#36827;&#34892;&#26102;&#38388;&#38388;&#38548;&#39044;&#27979;&#21644;&#24402;&#32435;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#30693;&#35782;&#22270;&#23436;&#25104;&#26041;&#27861;&#36890;&#36807;&#23558;&#32473;&#23450;&#22270;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#31354;&#38388;&#26469;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#38745;&#24577;&#30340;&#30693;&#35782;&#22270;&#65292;&#20294;&#26159;&#35768;&#22810;&#20844;&#24320;&#21487;&#29992;&#30340;&#30693;&#35782;&#22270;&#21253;&#21547;&#25551;&#36848;&#26576;&#20010;&#20107;&#23454;&#20026;&#30495;&#30340;&#26102;&#38388;&#28857;/&#26102;&#38388;&#27573;&#30340;&#26102;&#38388;&#20449;&#24687;&#12290;&#36825;&#26679;&#30340;&#22270;&#36890;&#24120;&#34987;&#31216;&#20026;&#26102;&#24577;&#30693;&#35782;&#22270;&#12290;&#27492;&#22806;&#65292;&#30693;&#35782;&#22270;&#36824;&#21487;&#33021;&#21253;&#21547;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#38745;&#24577;&#30340;&#30693;&#35782;&#22270;&#23436;&#25104;&#26041;&#27861;&#22312;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#32771;&#34385;&#26102;&#38388;&#20449;&#24687;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#32780;&#21482;&#21033;&#29992;&#22270;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#26368;&#36817;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#25552;&#39640;&#20102;&#38142;&#25509;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#19981;&#25903;&#25345;&#24402;&#32435;&#25512;&#29702;&#65288;&#22312;&#35757;&#32451;&#20013;&#26410;&#34987;&#30475;&#21040;&#30340;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#27979;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;TEM&#21644;TEM&#26469;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#26102;&#24577;&#30693;&#35782;&#22270;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#20174;&#32780;&#36827;&#34892;&#26102;&#38388;&#38388;&#38548;&#30340;&#39044;&#27979;&#21644;&#24402;&#32435;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most knowledge graph completion (KGC) methods learn latent representations of entities and relations of a given graph by mapping them into a vector space. Although the majority of these methods focus on static knowledge graphs, a large number of publicly available KGs contain temporal information stating the time instant/period over which a certain fact has been true. Such graphs are often known as temporal knowledge graphs. Furthermore, knowledge graphs may also contain textual descriptions of entities and relations. Both temporal information and textual descriptions are not taken into account during representation learning by static KGC methods, and only structural information of the graph is leveraged. Recently, some studies have used temporal information to improve link prediction, yet they do not exploit textual descriptions and do not support inductive inference (prediction on entities that have not been seen in training).  We propose a novel framework called TEMT that exploits t
&lt;/p&gt;</description></item><item><title>Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.16354</link><description>&lt;p&gt;
Transformer-VQ: &#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ: Linear-Time Transformers via Vector Quantization. (arXiv:2309.16354v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16354
&lt;/p&gt;
&lt;p&gt;
Transformer-VQ&#26159;&#19968;&#31181;&#22522;&#20110;&#21521;&#37327;&#37327;&#21270;&#23454;&#29616;&#32447;&#24615;&#26102;&#38388;&#30340;Transformer&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#35745;&#31639;&#33258;&#27880;&#24847;&#21147;&#65292;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Transformer-VQ&#65292;&#19968;&#31181;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#65292;&#33021;&#22815;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#35745;&#31639;&#22522;&#20110;softmax&#30340;&#23494;&#38598;&#33258;&#27880;&#24847;&#21147;&#12290;Transformer-VQ&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26159;&#36890;&#36807;&#21521;&#37327;&#37327;&#21270;&#38190;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#32531;&#23384;&#26426;&#21046;&#23454;&#29616;&#30340;&#12290;&#22312;&#22823;&#35268;&#27169;&#23454;&#39564;&#20013;&#65292;Transformer-VQ&#22312;&#36136;&#37327;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;Enwik8(0.99 bpb)&#65292;PG-19(26.6 ppl)&#21644;ImageNet64(3.16 bpb)&#37117;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26032;&#22411;&#30340;ShapeDTW Barycenter Averaging&#26041;&#27861;&#29983;&#25104;&#36924;&#30495;&#19988;&#26377;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#21644;&#21407;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#19981;&#31526;&#21512;&#20998;&#24067;&#30340;&#20154;&#24037;&#21046;&#21697;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16353</link><description>&lt;p&gt;
ShapeDBA: &#20351;&#29992;ShapeDTW Barycenter Averaging&#29983;&#25104;&#26377;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;&#21407;&#22411;
&lt;/p&gt;
&lt;p&gt;
ShapeDBA: Generating Effective Time Series Prototypes using ShapeDTW Barycenter Averaging. (arXiv:2309.16353v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16353
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26032;&#22411;&#30340;ShapeDTW Barycenter Averaging&#26041;&#27861;&#29983;&#25104;&#36924;&#30495;&#19988;&#26377;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#21644;&#21407;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#29983;&#25104;&#19981;&#31526;&#21512;&#20998;&#24067;&#30340;&#20154;&#24037;&#21046;&#21697;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20960;&#20046;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#37117;&#26377;&#25152;&#24212;&#29992;&#65292;&#20174;&#21307;&#23398;&#39046;&#22495;&#21040;&#21046;&#36896;&#19994;&#21644;&#26080;&#32447;&#36890;&#20449;&#12290;&#29983;&#25104;&#36924;&#30495;&#19988;&#26377;&#29992;&#30340;&#26679;&#26412;&#21644;&#21407;&#22411;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#29983;&#25104;&#36924;&#30495;&#19988;&#26377;&#29992;&#30340;&#26679;&#26412;&#21644;&#21407;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24179;&#22343;&#24418;&#24335;&#65292;&#21363;ShapeDTW Barycentric Average&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20934;&#30830;&#29983;&#25104;&#26102;&#38388;&#24207;&#21015;&#21407;&#22411;&#12290;&#29616;&#26377;&#30340;&#26102;&#38388;&#24207;&#21015;&#21407;&#22411;&#21270;&#26041;&#27861;&#20381;&#36182;&#20110;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;(DTW)&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#22914;DTW Barycentering Average(DBA)&#21644;SoftDBA&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#20854;&#21407;&#22411;&#20013;&#23384;&#22312;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#20102;&#19981;&#31526;&#21512;&#20998;&#24067;&#30340;&#20154;&#24037;&#21046;&#21697;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#25152;&#20351;&#29992;&#30340;DTW&#21464;&#20307;&#21450;&#20854;&#26080;&#27861;&#26816;&#27979;&#37051;&#22495;&#30456;&#20284;&#24615;&#65292;&#32780;&#21482;&#33021;&#26816;&#27979;&#32477;&#23545;&#30456;&#20284;&#24615;&#25152;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;ShapeDBA&#36890;&#36807;&#25913;&#36827;DTW&#21464;&#20307;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
Time series data can be found in almost every domain, ranging from the medical field to manufacturing and wireless communication. Generating realistic and useful exemplars and prototypes is a fundamental data analysis task. In this paper, we investigate a novel approach to generating realistic and useful exemplars and prototypes for time series data. Our approach uses a new form of time series average, the ShapeDTW Barycentric Average. We therefore turn our attention to accurately generating time series prototypes with a novel approach. The existing time series prototyping approaches rely on the Dynamic Time Warping (DTW) similarity measure such as DTW Barycentering Average (DBA) and SoftDBA. These last approaches suffer from a common problem of generating out-of-distribution artifacts in their prototypes. This is mostly caused by the DTW variant used and its incapability of detecting neighborhood similarities, instead it detects absolute similarities. Our proposed method, ShapeDBA, us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16347</link><description>&lt;p&gt;
&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#20869;&#22312;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic Manipulation Tasks. (arXiv:2309.16347v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;&#35299;&#20915;&#20102;&#22797;&#26434;&#38271;&#35270;&#31243;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#36739;&#39640;&#30340;&#24615;&#33021;&#21644;&#27169;&#22359;&#21270;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#31232;&#30095;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#20013;&#38754;&#20020;&#22256;&#22659;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#20247;&#22810;&#19981;&#21516;&#24207;&#21015;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#24341;&#23548;&#25506;&#32034;&#65288;IGE-LLMs&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#20316;&#20026;&#36741;&#21161;&#20869;&#22312;&#22870;&#21169;&#65292;IGE-LLMs&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#36807;&#31243;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#35270;&#31243;&#25805;&#20316;&#20219;&#21153;&#20013;&#22870;&#21169;&#31232;&#30095;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#25506;&#32034;&#25361;&#25112;&#30340;&#29615;&#22659;&#21644;&#19968;&#20010;&#21516;&#26102;&#38754;&#20020;&#25506;&#32034;&#21644;&#38271;&#35270;&#31243;&#25361;&#25112;&#30340;&#22797;&#26434;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#30456;&#20851;&#30340;&#20869;&#22312;&#23398;&#20064;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;IGE-LLMs(i)&#22312;&#30456;&#20851;&#30340;&#20869;&#22312;&#26041;&#27861;&#21644;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#20915;&#31574;&#30340;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#36739;&#39640;&#27700;&#24179;&#65292;(ii)&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#21644;&#20114;&#34917;&#65292;&#31361;&#20986;&#20854;&#27169;&#22359;&#21270;&#24615;&#33021;&#65292;(iii)&#23545;&#20110;&#19981;&#21516;&#30340;&#20869;&#22312;&#32553;&#25918;&#21442;&#25968;&#27604;&#36739;&#19981;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current reinforcement learning algorithms struggle in sparse and complex environments, most notably in long-horizon manipulation tasks entailing a plethora of different sequences. In this work, we propose the Intrinsically Guided Exploration from Large Language Models (IGE-LLMs) framework. By leveraging LLMs as an assistive intrinsic reward, IGE-LLMs guides the exploratory process in reinforcement learning to address intricate long-horizon with sparse rewards robotic manipulation tasks. We evaluate our framework and related intrinsic learning methods in an environment challenged with exploration, and a complex robotic manipulation task challenged by both exploration and long-horizons. Results show IGE-LLMs (i) exhibit notably higher performance over related intrinsic methods and the direct use of LLMs in decision-making, (ii) can be combined and complement existing learning methods highlighting its modularity, (iii) are fairly insensitive to different intrinsic scaling parameters, and 
&lt;/p&gt;</description></item><item><title>LagrangeBench&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#19971;&#20010;&#26032;&#30340;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#21644;&#29289;&#29702;&#29305;&#24615;&#65289;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;API&#21644;&#24050;&#24314;&#31435;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;JAX&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.16342</link><description>&lt;p&gt;
LagrangeBench: &#19968;&#31181;&#25289;&#26684;&#26391;&#26085;&#27969;&#20307;&#21147;&#23398;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite. (arXiv:2309.16342v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16342
&lt;/p&gt;
&lt;p&gt;
LagrangeBench&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#19971;&#20010;&#26032;&#30340;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#21644;&#29289;&#29702;&#29305;&#24615;&#65289;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;API&#21644;&#24050;&#24314;&#31435;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;JAX&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#25104;&#21151;&#24212;&#29992;&#20110;&#22522;&#20110;&#32593;&#26684;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#31163;&#25955;&#21270;&#30340;&#23398;&#20064;PDE&#27714;&#35299;&#22120;&#65292;&#22312;&#28041;&#21450;&#33258;&#30001;&#34920;&#38754;&#25110;&#22797;&#26434;&#29289;&#29702;&#38382;&#39064;&#26102;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LagrangeBench&#65292;&#36825;&#26159;&#38024;&#23545;&#25289;&#26684;&#26391;&#26085;&#31890;&#23376;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#37325;&#28857;&#26159;&#26102;&#38388;&#31895;&#31890;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;(a)&#20351;&#29992;&#24179;&#28369;&#31890;&#23376;&#27969;&#20307;&#21160;&#21147;&#23398;&#65288;SPH&#65289;&#26041;&#27861;&#29983;&#25104;&#30340;&#19971;&#20010;&#26032;&#30340;&#27969;&#20307;&#21147;&#23398;&#25968;&#25454;&#38598;&#65288;&#20854;&#20013;&#22235;&#20010;&#26159;2D&#30340;&#65292;&#19977;&#20010;&#26159;3D&#30340;&#65289;&#65292;&#21253;&#25324;&#20102;Taylor-Green&#28065;&#26059;&#12289;&#39537;&#21160;&#19978;&#30422;&#12289;&#21453;Poiseuille&#27969;&#21160;&#21644;&#26029;&#22365;&#31561;&#19981;&#21516;&#29289;&#29702;&#29305;&#24615;&#65292;&#22914;&#22266;&#20307;&#22721;&#30456;&#20114;&#20316;&#29992;&#25110;&#33258;&#30001;&#34920;&#38754;&#65292;(b)&#39640;&#25928;&#30340;&#22522;&#20110;JAX&#30340;API&#65292;&#37197;&#22791;&#19981;&#21516;&#30340;&#36817;&#26399;&#35757;&#32451;&#31574;&#30053;&#21644;&#37051;&#23621;&#25628;&#32034;&#20363;&#31243;&#65292;&#20197;&#21450;(c)&#24050;&#24314;&#31435;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22914;GNS&#21644;SEGNN&#30340;JAX&#23454;&#29616;&#19982;&#22522;&#20934;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#24230;&#37327;...
&lt;/p&gt;
&lt;p&gt;
Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free surfaces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hydrodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and neighbors search routine, and (c) JAX implementation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measu
&lt;/p&gt;</description></item><item><title>EFFL&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#22343;&#31561;&#20844;&#24179;&#24615;&#20197;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#20445;&#20840;&#23616;&#27169;&#22411;&#20855;&#26377;&#22343;&#31561;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#20559;&#35265;&#26469;&#20943;&#23569;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36164;&#28304;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2309.16338</link><description>&lt;p&gt;
EFFL: &#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#22343;&#31561;&#20844;&#24179;&#24615;&#20197;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect. (arXiv:2309.16338v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16338
&lt;/p&gt;
&lt;p&gt;
EFFL&#26159;&#19968;&#31181;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#22343;&#31561;&#20844;&#24179;&#24615;&#20197;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30830;&#20445;&#20840;&#23616;&#27169;&#22411;&#20855;&#26377;&#22343;&#31561;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#20559;&#35265;&#26469;&#20943;&#23569;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#36164;&#28304;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#20351;&#24471;&#21487;&#20197;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#20174;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#30340;&#21327;&#20316;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#38598;&#24322;&#26500;&#26102;&#65292;&#20256;&#32479;&#30340;FL&#26426;&#21046;&#20135;&#29983;&#30340;&#20840;&#23616;&#27169;&#22411;&#19981;&#33021;&#20805;&#20998;&#20195;&#34920;&#25317;&#26377;&#26377;&#38480;&#25968;&#25454;&#36164;&#28304;&#30340;&#36739;&#36139;&#22256;&#23458;&#25143;&#31471;&#65292;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#19978;&#23548;&#33268;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#39640;&#30340;&#20559;&#35265;&#12290;&#26681;&#25454;&#39532;&#22826;&#25928;&#24212;&#30340;&#25551;&#36848;&#65292;&#21363;&#20248;&#21183;&#32773;&#33719;&#24471;&#26356;&#22810;&#20248;&#21183;&#65292;&#21155;&#21183;&#32773;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#22833;&#21435;&#26356;&#22810;&#65292;&#23558;&#36825;&#26679;&#30340;&#20840;&#23616;&#27169;&#22411;&#37096;&#32626;&#22312;&#23458;&#25143;&#31471;&#24212;&#29992;&#20013;&#21487;&#33021;&#21152;&#21095;&#23458;&#25143;&#20043;&#38388;&#30340;&#36164;&#28304;&#24046;&#36317;&#65292;&#24182;&#25439;&#23475;&#31038;&#20250;&#31119;&#21033;&#21644;&#20844;&#24179;&#30340;&#21407;&#21017;&#12290;&#20026;&#20102;&#20943;&#36731;&#39532;&#22826;&#25928;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22343;&#31561;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#65288;EFFL&#65289;&#65292;&#20854;&#20013;&#22343;&#31561;&#20844;&#24179;&#24615;&#25351;&#30340;&#26159;&#20174;FL&#23398;&#20064;&#21040;&#30340;&#20840;&#23616;&#27169;&#22411;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;1&#65289;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#30456;&#31561;&#65307;&#65288;2&#65289;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#20915;&#31574;&#20559;&#35265;&#30456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in federated learning (FL) enable collaborative training of machine learning (ML) models from large-scale and widely dispersed clients while protecting their privacy. However, when different clients' datasets are heterogeneous, traditional FL mechanisms produce a global model that does not adequately represent the poorer clients with limited data resources, resulting in lower accuracy and higher bias on their local data. According to the Matthew effect, which describes how the advantaged gain more advantage and the disadvantaged lose more over time, deploying such a global model in client applications may worsen the resource disparity among the clients and harm the principles of social welfare and fairness. To mitigate the Matthew effect, we propose Egalitarian Fairness Federated Learning (EFFL), where egalitarian fairness refers to the global model learned from FL has: (1) equal accuracy among clients; (2) equal decision bias among clients. Besides achieving egalitaria
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#39044;&#27979;&#25151;&#39076;&#30340;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;&#20013;&#35782;&#21035;&#20986;&#26410;&#26469;&#20250;&#21457;&#23637;&#25151;&#39076;&#30340;&#24739;&#32773;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#39118;&#38505;&#31561;&#32423;&#35780;&#20272;&#20182;&#20204;&#22312;&#19968;&#23450;&#26102;&#38388;&#20869;&#21457;&#23637;&#25151;&#39076;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16335</link><description>&lt;p&gt;
&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#36827;&#34892;&#30340;&#25151;&#39076;&#39118;&#38505;&#39044;&#27979;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-end Risk Prediction of Atrial Fibrillation from the 12-Lead ECG by Deep Neural Networks. (arXiv:2309.16335v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;12&#23548;&#32852;&#24515;&#30005;&#22270;&#20013;&#39044;&#27979;&#25151;&#39076;&#30340;&#39118;&#38505;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;&#20013;&#35782;&#21035;&#20986;&#26410;&#26469;&#20250;&#21457;&#23637;&#25151;&#39076;&#30340;&#24739;&#32773;&#65292;&#24182;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#30340;&#39118;&#38505;&#31561;&#32423;&#35780;&#20272;&#20182;&#20204;&#22312;&#19968;&#23450;&#26102;&#38388;&#20869;&#21457;&#23637;&#25151;&#39076;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#25151;&#39076;&#26159;&#26368;&#24120;&#35265;&#30340;&#24515;&#24459;&#22833;&#24120;&#20043;&#19968;&#65292;&#27599;&#24180;&#24433;&#21709;&#20840;&#29699;&#25968;&#30334;&#19975;&#20154;&#65292;&#19982;&#20013;&#39118;&#21644;&#24515;&#21147;&#34928;&#31469;&#31561;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#23494;&#20999;&#30456;&#20851;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35780;&#20272;&#20174;&#24515;&#30005;&#22270;&#20013;&#21457;&#23637;&#25151;&#39076;&#30340;&#39118;&#38505;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#24052;&#35199;&#25910;&#38598;&#30340;&#22823;&#22411;CODE&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#21644;&#35780;&#20272;&#36825;&#26679;&#30340;&#31639;&#27861;&#12290;&#32467;&#26524;&#65306;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35782;&#21035;&#20986;&#22312;&#30446;&#21069;&#30340;&#24515;&#30005;&#22270;&#20013;&#27809;&#26377;&#25151;&#39076;&#36857;&#35937;&#65292;&#20294;&#23558;&#26469;&#20250;&#21457;&#23637;&#25151;&#39076;&#30340;&#24739;&#32773;&#65292;AUC&#24471;&#20998;&#20026;0.845&#12290;&#20174;&#25105;&#20204;&#30340;&#29983;&#23384;&#27169;&#22411;&#20013;&#24471;&#30693;&#65292;&#39640;&#39118;&#38505;&#32452;&#65288;&#21363;&#26410;&#26469;&#25151;&#39076;&#21457;&#30149;&#27010;&#29575;&#22823;&#20110;0.7&#65289;&#30340;&#24739;&#32773;&#22312;40&#21608;&#20869;&#21457;&#23637;&#25151;&#39076;&#30340;&#21487;&#33021;&#24615;&#35201;&#39640;&#20986;50%&#65292;&#32780;&#23646;&#20110;&#26368;&#20302;&#39118;&#38505;&#32452;&#65288;&#21363;&#26410;&#26469;&#25151;&#39076;&#21457;&#30149;&#27010;&#29575;&#23567;&#20110;&#25110;&#31561;&#20110;0.1&#65289;&#30340;&#24739;&#32773;&#26377;&#36229;&#36807;85%&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Atrial fibrillation (AF) is one of the most common cardiac arrhythmias that affects millions of people each year worldwide and it is closely linked to increased risk of cardiovascular diseases such as stroke and heart failure. Machine learning methods have shown promising results in evaluating the risk of developing atrial fibrillation from the electrocardiogram. We aim to develop and evaluate one such algorithm on a large CODE dataset collected in Brazil.  Results: The deep neural network model identified patients without indication of AF in the presented ECG but who will develop AF in the future with an AUC score of 0.845. From our survival model, we obtain that patients in the high-risk group (i.e. with the probability of a future AF case being greater than 0.7) are 50% more likely to develop AF within 40 weeks, while patients belonging to the minimal-risk group (i.e. with the probability of a future AF case being less than or equal to 0.1) have more than 85% chance of r
&lt;/p&gt;</description></item><item><title>DeepPCR&#26159;&#19968;&#31181;&#26032;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#38477;&#35299;&#31639;&#27861;&#23558;&#24120;&#35268;&#30340;&#39034;&#24207;&#25805;&#20316;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#36895;&#24230;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.16318</link><description>&lt;p&gt;
DeepPCR&#65306;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24182;&#34892;&#21270;&#24207;&#21015;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
DeepPCR: Parallelizing Sequential Operations in Neural Networks. (arXiv:2309.16318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16318
&lt;/p&gt;
&lt;p&gt;
DeepPCR&#26159;&#19968;&#31181;&#26032;&#22411;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#38477;&#35299;&#31639;&#27861;&#23558;&#24120;&#35268;&#30340;&#39034;&#24207;&#25805;&#20316;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#24182;&#34892;&#21270;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#36895;&#24230;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#21270;&#25216;&#26415;&#24050;&#32463;&#22312;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20173;&#28982;&#26377;&#19968;&#20123;&#25805;&#20316;&#26159;&#25353;&#39034;&#24207;&#36827;&#34892;&#30340;&#12290;&#20363;&#22914;&#65292;&#21069;&#21521;&#20256;&#36882;&#21644;&#21453;&#21521;&#20256;&#36882;&#26159;&#36880;&#23618;&#25191;&#34892;&#30340;&#65292;&#24182;&#19988;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#36890;&#36807;&#24212;&#29992;&#19968;&#31995;&#21015;&#21435;&#22122;&#27493;&#39588;&#20135;&#29983;&#30340;&#12290;&#36825;&#31181;&#39034;&#24207;&#26041;&#27861;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#19982;&#25152;&#28041;&#21450;&#27493;&#39588;&#30340;&#25968;&#37327;&#25104;&#27491;&#27604;&#65292;&#38543;&#30528;&#27493;&#39588;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#21487;&#33021;&#20986;&#29616;&#28508;&#22312;&#29942;&#39048;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;DeepPCR&#65292;&#23427;&#24182;&#34892;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#21644;&#35757;&#32451;&#20013;&#36890;&#24120;&#26159;&#39034;&#24207;&#25805;&#20316;&#30340;&#27493;&#39588;&#12290;DeepPCR&#22522;&#20110;&#23558;$L$&#27493;&#39588;&#30340;&#24207;&#21015;&#35299;&#37322;&#20026;&#29305;&#23450;&#26041;&#31243;&#32452;&#30340;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#34892;&#24490;&#29615;&#38477;&#35299;&#31639;&#27861;&#24674;&#22797;&#36825;&#20123;&#26041;&#31243;&#12290;&#36825;&#23558;&#39034;&#24207;&#25805;&#20316;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20174;$\mathcal{O}(L)$&#38477;&#20302;&#21040;$\mathcal{O}(\log_2L)$&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parallelization techniques have become ubiquitous for accelerating inference and training of deep neural networks. Despite this, several operations are still performed in a sequential manner. For instance, the forward and backward passes are executed layer-by-layer, and the output of diffusion models is produced by applying a sequence of denoising steps. This sequential approach results in a computational cost proportional to the number of steps involved, presenting a potential bottleneck as the number of steps increases. In this work, we introduce DeepPCR, a novel algorithm which parallelizes typically sequential operations used in inference and training of neural networks. DeepPCR is based on interpreting a sequence of $L$ steps as the solution of a specific system of equations, which we recover using the Parallel Cyclic Reduction algorithm. This reduces the complexity of computing the sequential operations from $\mathcal{O}(L)$ to $\mathcal{O}(\log_2L)$, thus yielding a speedup for 
&lt;/p&gt;</description></item><item><title>Astroconformer&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#24658;&#26143;&#20809;&#26354;&#32447;&#20013;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Kepler&#20809;&#26354;&#32447;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#24658;&#26143;&#34920;&#38754;&#37325;&#21147;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.16316</link><description>&lt;p&gt;
Astroconformer&#65306;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#24658;&#26143;&#20809;&#26354;&#32447;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Astroconformer: The Prospects of Analyzing Stellar Light Curves with Transformer-Based Deep Learning Models. (arXiv:2309.16316v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16316
&lt;/p&gt;
&lt;p&gt;
Astroconformer&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#24658;&#26143;&#20809;&#26354;&#32447;&#20013;&#25429;&#25417;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;Kepler&#20809;&#26354;&#32447;&#25968;&#25454;&#38598;&#65292;&#23454;&#29616;&#20102;&#23545;&#24658;&#26143;&#34920;&#38754;&#37325;&#21147;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24658;&#26143;&#30340;&#20809;&#21464;&#26354;&#32447;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#20851;&#20110;&#24658;&#26143;&#25391;&#33633;&#21644;&#39063;&#31890;&#36816;&#21160;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24658;&#26143;&#20869;&#37096;&#32467;&#26500;&#21644;&#28436;&#21270;&#29366;&#24577;&#30340;&#37325;&#35201;&#27934;&#35265;&#12290;&#20256;&#32479;&#30340;&#26143;&#38663;&#23398;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21151;&#29575;&#35889;&#20998;&#26512;&#65292;&#24573;&#30053;&#20102;&#20809;&#21464;&#26354;&#32447;&#20013;&#21253;&#21547;&#30340;&#23453;&#36149;&#30340;&#30456;&#20301;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#22312;&#26143;&#38663;&#23398;&#20013;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#20174;&#20809;&#21464;&#26354;&#32447;&#20013;&#25104;&#21151;&#22320;&#25512;&#26029;&#20986;&#24658;&#26143;&#23646;&#24615;&#65292;&#20294;&#24448;&#24448;&#21463;&#38480;&#20110;&#21367;&#31215;&#25805;&#20316;&#20013;&#22266;&#26377;&#30340;&#23616;&#37096;&#29305;&#24449;&#25552;&#21462;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Astroconformer&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#25429;&#25417;&#24658;&#26143;&#20809;&#26354;&#32447;&#20013;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#20027;&#35201;&#38598;&#20013;&#22312;&#20272;&#35745;&#34920;&#38754;&#37325;&#21147;&#65288;log g&#65289;&#65292;&#24182;&#22522;&#20110;&#20174;Kepler&#20809;&#26354;&#32447;&#20013;&#31934;&#24515;&#31579;&#36873;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#12290;&#36825;&#20123;&#20809;&#26354;&#32447;&#21253;&#21547;&#20102;&#22823;&#37327;&#24658;&#26143;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Light curves of stars encapsulate a wealth of information about stellar oscillations and granulation, thereby offering key insights into the internal structure and evolutionary state of stars. Conventional asteroseismic techniques have been largely confined to power spectral analysis, neglecting the valuable phase information contained within light curves. While recent machine learning applications in asteroseismology utilizing Convolutional Neural Networks (CNNs) have successfully inferred stellar attributes from light curves, they are often limited by the local feature extraction inherent in convolutional operations. To circumvent these constraints, we present $\textit{Astroconformer}$, a Transformer-based deep learning framework designed to capture long-range dependencies in stellar light curves. Our empirical analysis, which focuses on estimating surface gravity ($\log g$), is grounded in a carefully curated dataset derived from $\textit{Kepler}$ light curves. These light curves fe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20854;&#22312;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#26631;&#35835;&#32773;&#21253;&#25324;&#20855;&#22791;&#36125;&#21494;&#26031;&#26041;&#27861;&#32972;&#26223;&#20294;&#32570;&#20047;&#28145;&#24230;&#23398;&#20064;&#19987;&#19994;&#30693;&#35782;&#30340;&#32479;&#35745;&#23398;&#23478;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19987;&#19994;&#20294;&#23545;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26377;&#38480;&#20102;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#12290;</title><link>http://arxiv.org/abs/2309.16314</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#32508;&#36848;&#21644;&#35752;&#35770;&#30340;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Primer on Bayesian Neural Networks: Review and Debates. (arXiv:2309.16314v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#20854;&#22312;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#23616;&#38480;&#24615;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#30446;&#26631;&#35835;&#32773;&#21253;&#25324;&#20855;&#22791;&#36125;&#21494;&#26031;&#26041;&#27861;&#32972;&#26223;&#20294;&#32570;&#20047;&#28145;&#24230;&#23398;&#20064;&#19987;&#19994;&#30693;&#35782;&#30340;&#32479;&#35745;&#23398;&#23478;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19987;&#19994;&#20294;&#23545;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26377;&#38480;&#20102;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#38382;&#39064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#22266;&#26377;&#23616;&#38480;&#30340;&#38480;&#21046;&#65292;&#22914;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20316;&#20026;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#25193;&#23637;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25972;&#21512;&#21040;&#20854;&#39044;&#27979;&#33021;&#21147;&#20013;&#12290;&#26412;&#31687;&#32508;&#36848;&#24615;&#20837;&#38376;&#25351;&#21335;&#31995;&#32479;&#20171;&#32461;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#22312;BNNs&#24320;&#21457;&#20013;&#30340;&#21327;&#21516;&#25972;&#21512;&#12290;&#30446;&#26631;&#35835;&#32773;&#21253;&#25324;&#20855;&#22791;&#36125;&#21494;&#26031;&#26041;&#27861;&#32972;&#26223;&#20294;&#32570;&#20047;&#28145;&#24230;&#23398;&#20064;&#19987;&#19994;&#30693;&#35782;&#30340;&#32479;&#35745;&#23398;&#23478;&#65292;&#20197;&#21450;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19987;&#19994;&#20294;&#23545;&#36125;&#21494;&#26031;&#32479;&#35745;&#23398;&#26377;&#38480;&#20102;&#35299;&#30340;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#24120;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32771;&#23519;&#20102;&#23427;&#20204;&#23545;&#27169;&#22411;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have achieved remarkable performance across various problem domains, but their widespread applicability is hindered by inherent limitations such as overconfidence in predictions, lack of interpretability, and vulnerability to adversarial attacks. To address these challenges, Bayesian neural networks (BNNs) have emerged as a compelling extension of conventional neural networks, integrating uncertainty estimation into their predictive capabilities.  This comprehensive primer presents a systematic introduction to the fundamental concepts of neural networks and Bayesian inference, elucidating their synergistic integration for the development of BNNs. The target audience comprises statisticians with a potential background in Bayesian methods but lacking deep learning expertise, as well as machine learners proficient in deep neural networks but with limited exposure to Bayesian statistics. We provide an overview of commonly employed priors, examining their impact on model beh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CasIL&#30340;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#35748;&#30693;&#20808;&#39564;&#24182;&#23558;&#34892;&#21160;&#27010;&#24565;&#25193;&#23637;&#20026;&#21452;&#37325;&#35748;&#30693;-&#34892;&#21160;&#26550;&#26500;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#21407;&#22987;&#35270;&#35273;&#28436;&#31034;&#20013;&#26377;&#25928;&#22320;&#35748;&#30693;&#21644;&#27169;&#20223;&#20851;&#38190;&#25216;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#23454;&#29616;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#21452;&#37325;&#27169;&#20223;&#65292;&#20197;&#25552;&#20379;&#25972;&#20010;&#36807;&#31243;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16299</link><description>&lt;p&gt;
CasIL: &#36890;&#36807;&#21452;&#37325;&#35748;&#30693;-&#34892;&#21160;&#26550;&#26500;&#35748;&#30693;&#21644;&#27169;&#20223;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
CasIL: Cognizing and Imitating Skills via a Dual Cognition-Action Architecture. (arXiv:2309.16299v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CasIL&#30340;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#35748;&#30693;&#20808;&#39564;&#24182;&#23558;&#34892;&#21160;&#27010;&#24565;&#25193;&#23637;&#20026;&#21452;&#37325;&#35748;&#30693;-&#34892;&#21160;&#26550;&#26500;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#20174;&#21407;&#22987;&#35270;&#35273;&#28436;&#31034;&#20013;&#26377;&#25928;&#22320;&#35748;&#30693;&#21644;&#27169;&#20223;&#20851;&#38190;&#25216;&#33021;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#23454;&#29616;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#21452;&#37325;&#27169;&#20223;&#65292;&#20197;&#25552;&#20379;&#25972;&#20010;&#36807;&#31243;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26102;&#38388;&#20219;&#21153;&#65288;&#22914;&#36816;&#21160;&#12289;&#25805;&#20316;&#31561;&#65289;&#20013;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#20223;&#19987;&#23478;&#25216;&#33021;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20173;&#38754;&#20020;&#27425;&#20248;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#32771;&#34385;&#22914;&#20309;&#22312;&#20154;&#31867;&#35748;&#30693;&#20808;&#39564;&#30340;&#33539;&#22260;&#20869;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#30452;&#35266;&#30340;&#20154;&#31867;&#35748;&#30693;&#20808;&#39564;&#65292;&#21551;&#21457;&#24615;&#22320;&#23558;&#34892;&#21160;&#30340;&#27010;&#24565;&#25193;&#23637;&#20026;&#21452;&#37325;&#35748;&#30693;&#65288;&#39640;&#23618;&#65289;-&#34892;&#21160;&#65288;&#20302;&#23618;&#65289;&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65288;CasIL&#65289;&#65292;&#36890;&#36807;&#20154;&#26426;&#20132;&#20114;&#26469;&#20351;&#26426;&#22120;&#20154;&#26377;&#25928;&#22320;&#35748;&#30693;&#21644;&#27169;&#20223;&#21407;&#22987;&#35270;&#35273;&#28436;&#31034;&#20013;&#30340;&#20851;&#38190;&#25216;&#33021;&#12290;CasIL&#23454;&#29616;&#20102;&#35748;&#30693;&#21644;&#34892;&#21160;&#30340;&#21452;&#37325;&#27169;&#20223;&#65292;&#39640;&#32423;&#25216;&#33021;&#35748;&#30693;&#26126;&#30830;&#22320;&#25351;&#23548;&#20302;&#32423;&#22522;&#26412;&#21160;&#20316;&#65292;&#20026;&#25972;&#20010;&#25216;&#33021;&#27169;&#20223;&#23398;&#20064;&#36807;&#31243;&#25552;&#20379;&#20102;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling robots to effectively imitate expert skills in longhorizon tasks such as locomotion, manipulation, and more, poses a long-standing challenge. Existing imitation learning (IL) approaches for robots still grapple with sub-optimal performance in complex tasks. In this paper, we consider how this challenge can be addressed within the human cognitive priors. Heuristically, we extend the usual notion of action to a dual Cognition (high-level)-Action (low-level) architecture by introducing intuitive human cognitive priors, and propose a novel skill IL framework through human-robot interaction, called Cognition-Action-based Skill Imitation Learning (CasIL), for the robotic agent to effectively cognize and imitate the critical skills from raw visual demonstrations. CasIL enables both cognition and action imitation, while high-level skill cognition explicitly guides low-level primitive actions, providing robustness and reliability to the entire skill IL process. We evaluated our method 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#23384;&#22312;&#26681;&#26412;&#30340;&#38480;&#21046;&#65292;&#20294;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#26041;&#27861;&#21644;&#26500;&#24314;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#31639;&#27861;&#19981;&#21463;&#35813;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.16291</link><description>&lt;p&gt;
RL&#26041;&#27861;&#30340;&#25928;&#29575;&#20998;&#31163;&#65306;&#26080;&#27169;&#22411;&#12289;&#26377;&#27169;&#22411;&#21644;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#25928;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficiency Separation between RL Methods: Model-Free, Model-Based and Goal-Conditioned. (arXiv:2309.16291v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#25928;&#29575;&#19978;&#23384;&#22312;&#26681;&#26412;&#30340;&#38480;&#21046;&#65292;&#20294;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#26041;&#27861;&#21644;&#26500;&#24314;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#31639;&#27861;&#19981;&#21463;&#35813;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#31867;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#25928;&#29575;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;&#36825;&#20010;&#38480;&#21046;&#36866;&#29992;&#20110;&#26080;&#27169;&#22411;&#30340;RL&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#30340;&#26377;&#27169;&#22411;&#26041;&#27861;&#65292;&#22914;&#26641;&#25628;&#32034;&#30340;&#35268;&#21010;&#12290;&#22312;&#23545;&#36825;&#31867;&#38382;&#39064;&#30340;&#25277;&#35937;&#23450;&#20041;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;RL&#38382;&#39064;&#65292;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#26469;&#35828;&#65292;&#23427;&#20204;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#23547;&#25214;&#26368;&#20248;&#34892;&#20026;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#19979;&#30028;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#38024;&#23545;&#36825;&#20010;&#29305;&#23450;&#30340;&#38382;&#39064;&#23478;&#26063;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23478;&#26063;&#20013;&#30340;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#38480;&#21046;&#19981;&#36866;&#29992;&#20110;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#30446;&#26631;&#26465;&#20214;&#19979;&#30340;&#26041;&#27861;&#25110;&#26500;&#24314;&#36870;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a fundamental limitation on the efficiency of a wide class of Reinforcement Learning (RL) algorithms. This limitation applies to model-free RL methods as well as a broad range of model-based methods, such as planning with tree search.  Under an abstract definition of this class, we provide a family of RL problems for which these methods suffer a lower bound exponential in the horizon for their interactions with the environment to find an optimal behavior. However, there exists a method, not tailored to this specific family of problems, which can efficiently solve the problems in the family.  In contrast, our limitation does not apply to several types of methods proposed in the literature, for instance, goal-conditioned methods or other algorithms that construct an inverse dynamics model.
&lt;/p&gt;</description></item><item><title>LawBench&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#20174;&#35760;&#24518;&#65292;&#29702;&#35299;&#21644;&#24212;&#29992;&#19977;&#20010;&#23618;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#27861;&#24459;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16289</link><description>&lt;p&gt;
LawBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LawBench: Benchmarking Legal Knowledge of Large Language Models. (arXiv:2309.16289v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16289
&lt;/p&gt;
&lt;p&gt;
LawBench&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#30693;&#35782;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#20174;&#35760;&#24518;&#65292;&#29702;&#35299;&#21644;&#24212;&#29992;&#19977;&#20010;&#23618;&#38754;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#27861;&#24459;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#39640;&#24230;&#19987;&#19994;&#21270;&#12289;&#23433;&#20840;&#20851;&#38190;&#30340;&#27861;&#24459;&#39046;&#22495;&#26102;&#65292;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#25152;&#20855;&#22791;&#30340;&#27861;&#24459;&#30693;&#35782;&#37327;&#20197;&#21450;&#23427;&#20204;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#25191;&#34892;&#19982;&#27861;&#24459;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#35780;&#20272;&#22522;&#20934;LawBench&#12290;LawBench&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#20174;&#19977;&#20010;&#35748;&#30693;&#23618;&#38754;&#23545;LLMs&#30340;&#27861;&#24459;&#33021;&#21147;&#36827;&#34892;&#31934;&#30830;&#35780;&#20272;&#65306;&#65288;1&#65289;&#27861;&#24459;&#30693;&#35782;&#35760;&#24518;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#35760;&#20303;&#25152;&#38656;&#30340;&#27861;&#24459;&#27010;&#24565;&#12289;&#26465;&#27454;&#21644;&#20107;&#23454;&#65307;&#65288;2&#65289;&#27861;&#24459;&#30693;&#35782;&#29702;&#35299;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#27861;&#24459;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#12289;&#20107;&#20214;&#21644;&#20851;&#31995;&#65307;&#65288;3&#65289;&#27861;&#24459;&#30693;&#35782;&#24212;&#29992;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#36816;&#29992;&#33258;&#24049;&#30340;&#27861;&#24459;&#30693;&#35782;&#24182;&#36827;&#34892;&#24517;&#35201;&#30340;&#25512;&#29702;&#27493;&#39588;&#26469;&#35299;&#20915;&#29616;&#23454;&#30340;&#27861;&#24459;&#20219;&#21153;&#12290;LawBench&#21253;&#21547;20&#20010;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;5&#31181;&#20219;&#21153;&#31867;&#22411;&#65306;&#21333;&#26631;&#31614;&#20998;&#31867;&#65288;SLC&#65289;&#12289;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLC&#65289;&#12289;&#22238;&#24402;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated strong capabilities in various aspects. However, when applying them to the highly specialized, safe-critical legal domain, it is unclear how much legal knowledge they possess and whether they can reliably perform legal-related tasks. To address this gap, we propose a comprehensive evaluation benchmark LawBench. LawBench has been meticulously crafted to have precise assessment of the LLMs' legal capabilities from three cognitive levels: (1) Legal knowledge memorization: whether LLMs can memorize needed legal concepts, articles and facts; (2) Legal knowledge understanding: whether LLMs can comprehend entities, events and relationships within legal text; (3) Legal knowledge applying: whether LLMs can properly utilize their legal knowledge and make necessary reasoning steps to solve realistic legal tasks. LawBench contains 20 diverse tasks covering 5 task types: single-label classification (SLC), multi-label classification (MLC), regression, e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#26500;&#32852;&#37030;&#20132;&#21449;&#30456;&#20851;&#21644;&#23454;&#20363;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#30446;&#26631;&#33976;&#39311;&#26469;&#35299;&#20915;&#27169;&#22411;&#24322;&#36136;&#24615;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16286</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#24322;&#26500;&#32852;&#37030;&#20132;&#21449;&#30456;&#20851;&#21644;&#23454;&#20363;&#30456;&#20284;&#24615;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generalizable Heterogeneous Federated Cross-Correlation and Instance Similarity Learning. (arXiv:2309.16286v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#24322;&#26500;&#32852;&#37030;&#20132;&#21449;&#30456;&#20851;&#21644;&#23454;&#20363;&#30456;&#20284;&#24615;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#30446;&#26631;&#33976;&#39311;&#26469;&#35299;&#20915;&#27169;&#22411;&#24322;&#36136;&#24615;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#20445;&#25252;&#38544;&#31169;&#30340;&#22810;&#26041;&#23398;&#20064;&#33539;&#24335;&#65292;&#28041;&#21450;&#19982;&#20182;&#20154;&#30340;&#21512;&#20316;&#23398;&#20064;&#21644;&#23545;&#31169;&#26377;&#25968;&#25454;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#27169;&#22411;&#24322;&#36136;&#24615;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#26159;&#20004;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#24212;&#29992;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;FCCL+&#26041;&#27861;&#65292;&#21363;&#32852;&#37030;&#30456;&#20851;&#24615;&#21644;&#30456;&#20284;&#24615;&#23398;&#20064;&#19982;&#38750;&#30446;&#26631;&#33976;&#39311;&#65292;&#20419;&#36827;&#20102;&#22495;&#20869;&#21306;&#20998;&#33021;&#21147;&#21644;&#22495;&#38388;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#20110;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#26080;&#20851;&#30340;&#26410;&#26631;&#35760;&#20844;&#20849;&#25968;&#25454;&#26469;&#36827;&#34892;&#24322;&#26500;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#25105;&#20204;&#26500;&#24314;&#20132;&#21449;&#30456;&#20851;&#30697;&#38453;&#65292;&#24182;&#22312;&#26631;&#24535;&#21644;&#29305;&#24449;&#27700;&#24179;&#19978;&#23545;&#23454;&#20363;&#30456;&#20284;&#24615;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#36890;&#20449;&#38556;&#30861;&#24182;&#25552;&#39640;&#20102;&#24191;&#27867;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#26412;&#22320;&#26356;&#26032;&#38454;&#27573;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;FCCL+&#24341;&#20837;&#20102;&#32852;&#37030;&#38750;&#30446;&#26631;&#33976;&#39311;&#65292;&#26082;&#20445;&#30041;&#20102;&#22495;&#38388;&#30693;&#35782;&#21448;&#36991;&#20813;&#20102;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is an important privacy-preserving multi-party learning paradigm, involving collaborative learning with others and local updating on private data. Model heterogeneity and catastrophic forgetting are two crucial challenges, which greatly limit the applicability and generalizability. This paper presents a novel FCCL+, federated correlation and similarity learning with non-target distillation, facilitating the both intra-domain discriminability and inter-domain generalization. For heterogeneity issue, we leverage irrelevant unlabeled public data for communication between the heterogeneous participants. We construct cross-correlation matrix and align instance similarity distribution on both logits and feature levels, which effectively overcomes the communication barrier and improves the generalizable ability. For catastrophic forgetting in local updating stage, FCCL+ introduces Federated Non Target Distillation, which retains inter-domain knowledge while avoiding the opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#37197;&#23545;&#26679;&#26412;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#22402;&#30452;&#24179;&#20998;&#32447;&#29983;&#25104;&#35780;&#20998;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#20266;&#20013;&#20301;&#25968;&#27714;&#24471;&#26368;&#20248;&#35780;&#20998;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.16274</link><description>&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#37197;&#23545;&#26679;&#26412;&#20551;&#35774;&#26816;&#39564;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A framework for paired-sample hypothesis testing for high-dimensional data. (arXiv:2309.16274v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#39640;&#32500;&#25968;&#25454;&#37197;&#23545;&#26679;&#26412;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#36890;&#36807;&#22402;&#30452;&#24179;&#20998;&#32447;&#29983;&#25104;&#35780;&#20998;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#20266;&#20013;&#20301;&#25968;&#27714;&#24471;&#26368;&#20248;&#35780;&#20998;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#32500;&#25968;&#25454;&#30340;&#37197;&#23545;&#26679;&#26412;&#26816;&#39564;&#20013;&#65292;&#26631;&#20934;&#30340;&#26041;&#27861;&#26159;&#23545;&#27599;&#20010;&#29305;&#24449;&#24212;&#29992;&#22810;&#20010;&#21333;&#21464;&#37327;&#26816;&#39564;&#65292;&#28982;&#21518;&#36827;&#34892;p&#20540;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#21547;&#26377;&#22823;&#37327;&#29305;&#24449;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#38382;&#39064;&#12290;&#24050;&#26377;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#20998;&#31867;&#20934;&#30830;&#29575;&#21487;&#20197;&#20316;&#20026;&#21452;&#26679;&#26412;&#26816;&#39564;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#25552;&#20986;&#29702;&#35770;&#22522;&#30784;&#25110;&#23454;&#38469;&#26041;&#27861;&#26469;&#23558;&#36825;&#31181;&#31574;&#30053;&#25193;&#23637;&#21040;&#22810;&#32500;&#37197;&#23545;&#26679;&#26412;&#26816;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#27861;&#65292;&#21363;&#36890;&#36807;&#27599;&#23545;&#23454;&#20363;&#36830;&#25509;&#32447;&#27573;&#30340;&#22402;&#30452;&#24179;&#20998;&#32447;&#23450;&#20041;&#30340;&#20915;&#31574;&#35268;&#21017;&#26469;&#29983;&#25104;&#35780;&#20998;&#20989;&#25968;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#36825;&#20123;&#35268;&#21017;&#30340;&#20266;&#20013;&#20301;&#25968;&#26469;&#20272;&#35745;&#26368;&#20248;&#35780;&#20998;&#20989;&#25968;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#22320;&#25193;&#23637;Hodges-Lehmann&#20272;&#35745;&#37327;&#26469;&#36827;&#34892;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#26816;&#39564;&#36807;&#31243;&#30340;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20272;&#35745;&#27599;&#20010;&#29305;&#24449;&#30340;&#24179;&#20998;&#32447;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26681;&#25454;&#36825;&#20123;&#24179;&#20998;&#32447;&#23450;&#20041;&#35780;&#20998;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#20266;&#20013;&#20301;&#25968;&#27714;&#24471;&#26368;&#20248;&#35780;&#20998;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard paired-sample testing approach in the multidimensional setting applies multiple univariate tests on the individual features, followed by p-value adjustments. Such an approach suffers when the data carry numerous features. A number of studies have shown that classification accuracy can be seen as a proxy for two-sample testing. However, neither theoretical foundations nor practical recipes have been proposed so far on how this strategy could be extended to multidimensional paired-sample testing. In this work, we put forward the idea that scoring functions can be produced by the decision rules defined by the perpendicular bisecting hyperplanes of the line segments connecting each pair of instances. Then, the optimal scoring function can be obtained by the pseudomedian of those rules, which we estimate by extending naturally the Hodges-Lehmann estimator. We accordingly propose a framework of a two-step testing procedure. First, we estimate the bisecting hyperplanes for each p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#26694;&#26550;(H-NDAF)&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#24067;&#22312;&#22810;&#20010;&#21494;&#23376;NWDAF&#19978;&#65292;&#23454;&#29616;&#20102;&#20998;&#26512;&#32467;&#26524;&#30340;&#21450;&#26102;&#25552;&#20379;&#21644;&#26356;&#24555;&#30340;&#20998;&#26512;&#25552;&#20379;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.16269</link><description>&lt;p&gt;
B5G&#32593;&#32476;&#33258;&#21160;&#21270;&#30340;&#23618;&#32423;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#26694;&#26550;: &#35774;&#35745;&#19982;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Network Data Analytics Framework for B5G Network Automation: Design and Implementation. (arXiv:2309.16269v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16269
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#26694;&#26550;(H-NDAF)&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#20219;&#21153;&#20998;&#24067;&#22312;&#22810;&#20010;&#21494;&#23376;NWDAF&#19978;&#65292;&#23454;&#29616;&#20102;&#20998;&#26512;&#32467;&#26524;&#30340;&#21450;&#26102;&#25552;&#20379;&#21644;&#26356;&#24555;&#30340;&#20998;&#26512;&#25552;&#20379;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
5G&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#30340;&#32593;&#32476;&#21151;&#33021;(NFs)&#65292;&#20197;&#26356;&#28789;&#27963;&#12289;&#24377;&#24615;&#22320;&#25903;&#25345;&#26032;&#20852;&#26381;&#21153;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#31181;&#27169;&#22359;&#21270;NF&#31649;&#29702;&#20013;&#30340;&#22797;&#26434;&#24615;&#65292;&#33258;&#21160;&#21270;&#30340;&#32593;&#32476;&#25805;&#20316;&#21644;&#31649;&#29702;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#27492;&#31532;&#19977;&#20195;&#21512;&#20316;&#20249;&#20276;&#35745;&#21010;(3GPP)&#24341;&#20837;&#20102;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#21151;&#33021;(NWDAF)&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;NWDAF&#38656;&#35201;&#36827;&#34892;&#25512;&#29702;&#21644;&#35757;&#32451;&#20219;&#21153;&#65292;&#22240;&#27492;&#24456;&#38590;&#21450;&#26102;&#22320;&#23558;&#20998;&#26512;&#32467;&#26524;&#25552;&#20379;&#32473;NF&#65292;&#23588;&#20854;&#26159;&#22312;&#22686;&#21152;&#30340;&#20998;&#26512;&#35831;&#27714;&#25968;&#37327;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#32423;&#32593;&#32476;&#25968;&#25454;&#20998;&#26512;&#26694;&#26550;(H-NDAF)&#65292;&#20854;&#20013;&#25512;&#29702;&#20219;&#21153;&#20998;&#24067;&#22312;&#22810;&#20010;&#21494;&#23376;NWDAF&#19978;&#65292;&#35757;&#32451;&#20219;&#21153;&#22312;&#26681;NWDAF&#19978;&#36827;&#34892;&#12290;&#20351;&#29992;&#24320;&#28304;&#36719;&#20214;(free5GC)&#36827;&#34892;&#30340;&#22823;&#37327;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;NWDAF&#30456;&#27604;&#65292;H-NDAF&#21487;&#20197;&#25552;&#20379;&#36275;&#22815;&#20934;&#30830;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#19988;&#20998;&#26512;&#25552;&#20379;&#26102;&#38388;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
5G introduced modularized network functions (NFs) to support emerging services in a more flexible and elastic manner. To mitigate the complexity in such modularized NF management, automated network operation and management are indispensable, and thus the 3rd generation partnership project (3GPP) has introduced a network data analytics function (NWDAF). However, a conventional NWDAF needs to conduct both inference and training tasks, and thus it is difficult to provide the analytics results to NFs in a timely manner for an increased number of analytics requests. In this article, we propose a hierarchical network data analytics framework (H-NDAF) where inference tasks are distributed to multiple leaf NWDAFs and training tasks are conducted at the root NWDAF. Extensive simulation results using open-source software (i.e., free5GC) demonstrate that H-NDAF can provide sufficiently accurate analytics and faster analytics provision time compared to the conventional NWDAF.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#24046;&#24322;&#32422;&#26463;&#25512;&#24191;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#24182;&#31616;&#21270;&#20102;&#22870;&#21169;&#21644;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16240</link><description>&lt;p&gt;
&#36229;&#36234;&#36870;KL&#65306;&#36890;&#36807;&#22810;&#26679;&#30340;&#24046;&#24322;&#32422;&#26463;&#25512;&#24191;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints. (arXiv:2309.16240v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#22810;&#26679;&#24046;&#24322;&#32422;&#26463;&#25512;&#24191;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28040;&#38500;&#20102;&#23545;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#24182;&#31616;&#21270;&#20102;&#22870;&#21169;&#21644;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#33021;&#21147;&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#21516;&#26102;&#20063;&#25918;&#22823;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#22914;AI&#31995;&#32479;&#30340;&#28508;&#22312;&#28389;&#29992;&#65292;&#36825;&#38656;&#35201;&#26377;&#25928;&#30340;AI&#23545;&#40784;&#12290;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#32463;&#25104;&#20026;AI&#23545;&#40784;&#30340;&#19968;&#26465;&#26377;&#24076;&#26395;&#30340;&#36335;&#24452;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#23545;&#29420;&#31435;&#22870;&#21169;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#32780;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#22312;&#36870;KL&#27491;&#21017;&#21270;&#32422;&#26463;&#19979;&#31561;&#21516;&#20110;RLHF&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;f-DPO&#65292;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#22810;&#26679;&#30340;&#24046;&#24322;&#32422;&#26463;&#26469;&#25512;&#24191;DPO&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;f-&#25955;&#24230;&#19979;&#65292;&#21253;&#25324;Jensen-Shannon&#25955;&#24230;&#12289;&#27491;&#21521;KL&#25955;&#24230;&#21644;&#945;-&#25955;&#24230;&#65292;&#22870;&#21169;&#21644;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20063;&#21487;&#20197;&#36890;&#36807;&#35299;&#20915;Karush-Kuhn-Tucker&#26465;&#20214;&#26469;&#31616;&#21270;&#12290;&#36825;&#28040;&#38500;&#20102;&#23545;&#20272;&#35745;&#26041;&#27861;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing capabilities of large language models (LLMs) raise opportunities for artificial general intelligence but concurrently amplify safety concerns, such as potential misuse of AI systems, necessitating effective AI alignment. Reinforcement Learning from Human Feedback (RLHF) has emerged as a promising pathway towards AI alignment but brings forth challenges due to its complexity and dependence on a separate reward model. Direct Preference Optimization (DPO) has been proposed as an alternative, and it remains equivalent to RLHF under the reverse KL regularization constraint. This paper presents $f$-DPO, a generalized approach to DPO by incorporating diverse divergence constraints. We show that under certain $f$-divergences, including Jensen-Shannon divergence, forward KL divergences and $\alpha$-divergences, the complex relationship between the reward and optimal policy can also be simplified by addressing the Karush-Kuhn-Tucker conditions. This eliminates the need for estimat
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#12290;&#21516;&#26102;&#65292;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#65292;&#26410;&#26469;&#23558;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.16235</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language models in molecular discovery. (arXiv:2309.16235v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16235
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#24212;&#29992;&#20026;&#21152;&#36895;&#33647;&#29289;&#21457;&#29616;&#21644;&#20998;&#23376;&#35774;&#35745;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#12290;&#21516;&#26102;&#65292;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#65292;&#26410;&#26469;&#23558;&#32467;&#21512;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24050;&#32463;&#28183;&#36879;&#21040;&#20854;&#20182;&#39046;&#22495;&#65292;&#20986;&#29616;&#20102;&#22312;&#23567;&#20998;&#23376;&#12289;&#34507;&#30333;&#36136;&#25110;&#32858;&#21512;&#29289;&#19978;&#36816;&#20316;&#30340;&#8220;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#8221;&#12290;&#22312;&#21270;&#23398;&#39046;&#22495;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#21152;&#36895;&#20998;&#23376;&#21457;&#29616;&#21608;&#26399;&#26041;&#38754;&#21457;&#25381;&#20102;&#20316;&#29992;&#65292;&#26368;&#36817;&#22312;&#26089;&#26399;&#33647;&#29289;&#21457;&#29616;&#26041;&#38754;&#26377;&#20102;&#26377;&#24076;&#26395;&#30340;&#21457;&#29616;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#21457;&#29616;&#20013;&#30340;&#35282;&#33394;&#65292;&#24378;&#35843;&#20102;&#20174;&#22836;&#35774;&#35745;&#33647;&#29289;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#21453;&#24212;&#21270;&#23398;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#26377;&#20215;&#20540;&#30340;&#24320;&#28304;&#36719;&#20214;&#36164;&#28304;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#30340;&#38376;&#27099;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21246;&#30011;&#20102;&#26410;&#26469;&#20998;&#23376;&#35774;&#35745;&#30340;&#24895;&#26223;&#65292;&#23558;&#32842;&#22825;&#26426;&#22120;&#20154;&#30028;&#38754;&#19982;&#35745;&#31639;&#21270;&#23398;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#21270;&#23398;&#23478;&#21644;&#23545;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21152;&#36895;&#21270;&#23398;&#21457;&#29616;&#24863;&#20852;&#36259;&#30340;&#20154;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of language models, especially transformer-based architectures, has trickled into other domains giving rise to "scientific language models" that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. Here, we review the role of language models in molecular discovery, underlining their strength in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets thus lowering the entry barrier to the field of scientific language modeling. Last, we sketch a vision for future molecular design that combines a chatbot interface with access to computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16223</link><description>&lt;p&gt;
GInX-Eval: &#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GInX-Eval: Towards In-Distribution Evaluation of Graph Neural Network Explanations. (arXiv:2309.16223v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#30340;&#20869;&#20998;&#24067;&#35780;&#20272;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;GInX-Eval&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#35780;&#20272;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#35299;&#37322;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20026;&#20102;&#31361;&#20986;&#22270;&#20013;&#23545;&#27169;&#22411;&#39044;&#27979;&#26368;&#26377;&#36129;&#29486;&#30340;&#36793;&#21644;&#33410;&#28857;&#65292;&#20154;&#20204;&#24320;&#21457;&#20102;&#21508;&#31181;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#22914;&#20309;&#20174;&#20154;&#31867;&#25110;&#27169;&#22411;&#30340;&#35282;&#24230;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#12290;&#24403;&#21069;&#35780;&#20272;&#36807;&#31243;&#20013;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#29942;&#39048;&#38382;&#39064;&#26159;&#35299;&#37322;&#30340;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#12290;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#20250;&#24433;&#21709;&#21040;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#27969;&#34892;&#30340;&#24544;&#23454;&#24230;&#25110;&#20445;&#30495;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24544;&#23454;&#24230;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GInX-Eval&#65288;&#22270;&#20869;&#20998;&#24067;&#35299;&#37322;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#35299;&#37322;&#30340;&#36807;&#31243;&#65292;&#20811;&#26381;&#20102;&#24544;&#23454;&#24230;&#30340;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#35299;&#37322;&#26041;&#27861;&#30340;&#26032;&#35265;&#35299;&#12290;&#20351;&#29992;&#37325;&#26032;&#35757;&#32451;&#31574;&#30053;&#65292;GInX&#24471;&#20998;&#21487;&#34913;&#37327;&#24050;&#31227;&#38500;&#36793;&#23545;&#27169;&#22411;&#30340;&#20449;&#24687;&#37327;&#20197;&#21450;&#36793;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diverse explainability methods of graph neural networks (GNN) have recently been developed to highlight the edges and nodes in the graph that contribute the most to the model predictions. However, it is not clear yet how to evaluate the correctness of those explanations, whether it is from a human or a model perspective. One unaddressed bottleneck in the current evaluation procedure is the problem of out-of-distribution explanations, whose distribution differs from those of the training data. This important issue affects existing evaluation metrics such as the popular faithfulness or fidelity score. In this paper, we show the limitations of faithfulness metrics. We propose GInX-Eval (Graph In-distribution eXplanation Evaluation), an evaluation procedure of graph explanations that overcomes the pitfalls of faithfulness and offers new insights on explainability methods. Using a retraining strategy, the GInX score measures how informative removed edges are for the model and the EdgeRank s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;ODD&#26816;&#27979;&#65292;&#20026;&#20102;&#23454;&#29616;&#22312;&#23454;&#38469;&#21307;&#30103;&#31995;&#32479;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#36991;&#20813;&#23545;ODD&#25968;&#25454;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#35813;&#22522;&#20934;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;ICU&#24739;&#32773;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#26041;&#27861;&#21644;&#39044;&#27979;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16220</link><description>&lt;p&gt;
&#25581;&#31034;&#21464;&#33394;&#40857;&#65306;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#20013;&#30340;ODD&#26816;&#27979;&#30340;&#22522;&#20934;&#12290; (arXiv:2309.16220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Unmasking the Chameleons: A Benchmark for Out-of-Distribution Detection in Medical Tabular Data. (arXiv:2309.16220v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#20013;&#36827;&#34892;ODD&#26816;&#27979;&#65292;&#20026;&#20102;&#23454;&#29616;&#22312;&#23454;&#38469;&#21307;&#30103;&#31995;&#32479;&#20013;&#21487;&#38752;&#22320;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#36991;&#20813;&#23545;ODD&#25968;&#25454;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#35813;&#22522;&#20934;&#21033;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;ICU&#24739;&#32773;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#22810;&#31181;&#26041;&#27861;&#21644;&#39044;&#27979;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#26469;&#33258;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#27809;&#26377;&#26377;&#25928;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#31995;&#32479;&#20013;&#20351;&#29992;ML&#27169;&#22411;&#65292;&#24182;&#36991;&#20813;&#23545;ODD&#25968;&#25454;&#36827;&#34892;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#26816;&#27979;ODD&#26679;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22312;&#20854;&#20182;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;ODD&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#24403;&#22788;&#29702;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#26102;&#65292;&#26159;&#21542;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#27867;&#21487;&#22797;&#21046;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#22810;&#20010;&#27979;&#35797;&#26469;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#36817;&#21644;&#36828;ODDs&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21033;&#29992;&#20102;&#26368;&#26032;&#29256;&#26412;&#30340;eICU&#21644;MIMIC-IV&#65292;&#36825;&#26159;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#25968;&#19975;&#21517;ICU&#24739;&#32773;&#22312;&#22810;&#23478;&#21307;&#38498;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#26041;&#27861;&#21644;SOTA&#21518;&#32622;&#26816;&#27979;&#22120;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#39044;&#27979;&#26550;&#26500;&#65292;&#21253;&#25324;MLP&#12289;ResNet&#21644;Transformer&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their success, Machine Learning (ML) models do not generalize effectively to data not originating from the training distribution. To reliably employ ML models in real-world healthcare systems and avoid inaccurate predictions on out-of-distribution (OOD) data, it is crucial to detect OOD samples. Numerous OOD detection approaches have been suggested in other fields - especially in computer vision - but it remains unclear whether the challenge is resolved when dealing with medical tabular data. To answer this pressing need, we propose an extensive reproducible benchmark to compare different methods across a suite of tests including both near and far OODs. Our benchmark leverages the latest versions of eICU and MIMIC-IV, two public datasets encompassing tens of thousands of ICU patients in several hospitals. We consider a wide array of density-based methods and SOTA post-hoc detectors across diverse predictive architectures, including MLP, ResNet, and Transformer. Our findings sho
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Swinunter&#27169;&#22411;&#22312;CT&#20013;&#36827;&#34892;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20811;&#26381;&#22120;&#23448;&#30340;&#27169;&#31946;&#36793;&#30028;&#12289;&#22797;&#26434;&#30340;&#32972;&#26223;&#21644;&#19981;&#21516;&#23610;&#24230;&#30340;&#22120;&#23448;&#22823;&#23567;&#24046;&#24322;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20844;&#24320;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#21644;&#36739;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.16210</link><description>&lt;p&gt;
&#20351;&#29992;Swinunter&#22312;CT&#20013;&#36827;&#34892;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Abdominal multi-organ segmentation in CT using Swinunter. (arXiv:2309.16210v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Swinunter&#27169;&#22411;&#22312;CT&#20013;&#36827;&#34892;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20811;&#26381;&#22120;&#23448;&#30340;&#27169;&#31946;&#36793;&#30028;&#12289;&#22797;&#26434;&#30340;&#32972;&#26223;&#21644;&#19981;&#21516;&#23610;&#24230;&#30340;&#22120;&#23448;&#22823;&#23567;&#24046;&#24322;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20844;&#24320;&#39564;&#35777;&#38598;&#19978;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#21644;&#36739;&#24555;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#20013;&#65292;&#33145;&#37096;&#22810;&#22120;&#23448;&#20998;&#21106;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#21253;&#25324;&#30142;&#30149;&#26816;&#27979;&#21644;&#27835;&#30103;&#35745;&#21010;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22120;&#23448;&#30340;&#27169;&#31946;&#36793;&#30028;&#12289;&#22797;&#26434;&#30340;&#32972;&#26223;&#21644;&#19981;&#21516;&#23610;&#24230;&#30340;&#22120;&#23448;&#22823;&#23567;&#24046;&#24322;&#65292;&#21033;&#29992;&#21333;&#19968;&#32593;&#32476;&#20934;&#30830;&#22320;&#20998;&#21106;&#19981;&#21516;&#22120;&#23448;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#21069;&#20960;&#24180;&#30340;&#27604;&#36187;&#21457;&#29616;&#65292;&#22522;&#26412;&#19978;&#25152;&#26377;&#21069;&#20116;&#21517;&#30340;&#26041;&#27861;&#37117;&#20351;&#29992;&#20102;&#22522;&#20110;CNN&#30340;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#22240;&#20026;&#25968;&#25454;&#37327;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;transformer-based&#26041;&#27861;&#21457;&#25381;&#20854;&#20840;&#37096;&#20248;&#21183;&#12290;&#26412;&#27425;&#27604;&#36187;&#20013;&#30340;&#25968;&#21315;&#20010;&#26679;&#26412;&#21487;&#33021;&#20351;transformer-based&#27169;&#22411;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#20844;&#24320;&#39564;&#35777;&#38598;&#19978;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;transformer-based&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#21487;&#25509;&#21463;&#30340;&#32467;&#26524;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abdominal multi-organ segmentation in computed tomography (CT) is crucial for many clinical applications including disease detection and treatment planning. Deep learning methods have shown unprecedented performance in this perspective. However, it is still quite challenging to accurately segment different organs utilizing a single network due to the vague boundaries of organs, the complex background, and the substantially different organ size scales. In this work we used make transformer-based model for training. It was found through previous years' competitions that basically all of the top 5 methods used CNN-based methods, which is likely due to the lack of data volume that prevents transformer-based methods from taking full advantage. The thousands of samples in this competition may enable the transformer-based model to have more excellent results. The results on the public validation set also show that the transformer-based model can achieve an acceptable result and inference time
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26368;&#22823;&#20999;&#29255;&#20114;&#20449;&#24687;&#65288;mSMI&#65289;&#65292;&#26469;&#37327;&#21270;&#39640;&#32500;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;mSMI&#22312;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#21516;&#26102;&#20063;&#36866;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2309.16200</link><description>&lt;p&gt;
&#26368;&#22823;&#20999;&#29255;&#20114;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Max-Sliced Mutual Information. (arXiv:2309.16200v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#26368;&#22823;&#20999;&#29255;&#20114;&#20449;&#24687;&#65288;mSMI&#65289;&#65292;&#26469;&#37327;&#21270;&#39640;&#32500;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;mSMI&#22312;&#25429;&#25417;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#30340;&#21516;&#26102;&#20063;&#36866;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#39640;&#32500;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#23545;&#20110;&#32479;&#35745;&#23398;&#20064;&#21644;&#25512;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#20004;&#31181;&#20256;&#32479;&#26041;&#27861;&#26159;&#26631;&#20934;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#20854;&#35782;&#21035;&#20986;&#21407;&#22987;&#21464;&#37327;&#30340;&#26368;&#22823;&#30456;&#20851;&#25237;&#24433;&#29256;&#26412;&#65292;&#20197;&#21450;&#39321;&#20892;&#30340;&#20114;&#20449;&#24687;&#65292;&#23427;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#20381;&#36182;&#24230;&#37327;&#65292;&#20063;&#33021;&#25429;&#25417;&#39640;&#38454;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;CCA&#20165;&#32771;&#34385;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#21487;&#33021;&#19981;&#36275;&#22815;&#65292;&#32780;&#20114;&#20449;&#24687;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#36890;&#24120;&#38590;&#20197;&#35745;&#31639;/&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25240;&#34935;&#26041;&#26696;&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#65292;&#31216;&#20026;&#26368;&#22823;&#20999;&#29255;&#20114;&#20449;&#24687;&#65288;mSMI&#65289;&#12290;mSMI&#31561;&#20110;&#39640;&#32500;&#21464;&#37327;&#30340;&#20302;&#32500;&#25237;&#24433;&#20043;&#38388;&#30340;&#26368;&#22823;&#20114;&#20449;&#24687;&#65292;&#32780;&#22312;&#39640;&#26031;&#24773;&#20917;&#19979;&#20943;&#23569;&#21040;CCA&#12290;&#23427;&#20860;&#20855;&#20004;&#32773;&#30340;&#20248;&#28857;&#65306;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#24555;&#36895;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantifying the dependence between high-dimensional random variables is central to statistical learning and inference. Two classical methods are canonical correlation analysis (CCA), which identifies maximally correlated projected versions of the original variables, and Shannon's mutual information, which is a universal dependence measure that also captures high-order dependencies. However, CCA only accounts for linear dependence, which may be insufficient for certain applications, while mutual information is often infeasible to compute/estimate in high dimensions. This work proposes a middle ground in the form of a scalable information-theoretic generalization of CCA, termed max-sliced mutual information (mSMI). mSMI equals the maximal mutual information between low-dimensional projections of the high-dimensional variables, which reduces back to CCA in the Gaussian case. It enjoys the best of both worlds: capturing intricate dependencies in the data while being amenable to fast comput
&lt;/p&gt;</description></item><item><title>Stackelberg&#25209;&#37327;&#31574;&#30053;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35266;&#28857;&#65292;&#23545;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#20998;&#23618;&#20915;&#31574;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.16188</link><description>&lt;p&gt;
Stackelberg&#25209;&#37327;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Batch Policy Learning. (arXiv:2309.16188v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16188
&lt;/p&gt;
&lt;p&gt;
Stackelberg&#25209;&#37327;&#31574;&#30053;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35266;&#28857;&#65292;&#23545;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#32771;&#34385;&#20102;&#20248;&#21270;&#26223;&#35266;&#20013;&#30340;&#20998;&#23618;&#20915;&#31574;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#23450;&#20041;&#20102;&#20174;&#22266;&#23450;&#30340;&#25968;&#25454;&#25209;&#27425;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#32570;&#20047;&#35814;&#23613;&#30340;&#25506;&#32034;&#12290;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#31639;&#27861;&#20351;&#29992;&#32463;&#39564;&#25968;&#25454;&#26469;&#26657;&#20934;&#20215;&#20540;&#20989;&#25968;&#27169;&#22411;&#65292;&#24182;&#22312;&#23398;&#20064;&#27169;&#22411;&#19979;&#25191;&#34892;&#26576;&#31181;&#24754;&#35266;&#35780;&#20272;&#65292;&#24050;&#32463;&#25104;&#20026;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20010;&#27969;&#27966;&#30340;&#29616;&#20195;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#20248;&#21270;&#26223;&#35266;&#20013;&#38544;&#34255;&#30340;&#20998;&#23618;&#20915;&#31574;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#21338;&#24328;&#35770;&#30340;&#35266;&#28857;&#65292;&#23558;&#31574;&#30053;&#23398;&#20064;&#22270;&#34920;&#24314;&#27169;&#20026;&#20855;&#26377;&#39046;&#23548;&#32773;-&#36319;&#38543;&#32773;&#32467;&#26500;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#38543;&#26426;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#65306;StackelbergLearner&#65292;&#39046;&#23548;&#32773;&#26681;&#25454;&#20854;&#30446;&#26631;&#30340;&#20840;&#23548;&#25968;&#36827;&#34892;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#36890;&#24120;&#30340;&#20010;&#20307;&#26799;&#24230;&#65292;&#32780;&#36319;&#38543;&#32773;&#36827;&#34892;&#20010;&#20307;&#26356;&#26032;&#24182;&#30830;&#20445;&#36807;&#28193;&#19968;&#33268;&#30340;&#24754;&#35266;&#25512;&#29702;&#12290;&#25512;&#23548;&#20986;&#30340;&#23398;&#20064;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Batch reinforcement learning (RL) defines the task of learning from a fixed batch of data lacking exhaustive exploration. Worst-case optimality algorithms, which calibrate a value-function model class from logged experience and perform some type of pessimistic evaluation under the learned model, have emerged as a promising paradigm for batch RL. However, contemporary works on this stream have commonly overlooked the hierarchical decision-making structure hidden in the optimization landscape. In this paper, we adopt a game-theoretical viewpoint and model the policy learning diagram as a two-player general-sum game with a leader-follower structure. We propose a novel stochastic gradient-based learning algorithm: StackelbergLearner, in which the leader player updates according to the total derivative of its objective instead of the usual individual gradient, and the follower player makes individual updates and ensures transition-consistent pessimistic reasoning. The derived learning dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#29289;&#29702;-&#26426;&#22120;&#23398;&#20064;&#27668;&#20505;&#27169;&#25311;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#22312;&#32447;&#24314;&#27169;&#38169;&#35823;&#37319;&#26679;&#21644;&#35780;&#20272;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#35774;&#35745;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16177</link><description>&lt;p&gt;
&#31995;&#32479;&#21270;&#37319;&#26679;&#21644;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#22312;&#27668;&#20505;&#27169;&#22411;&#20013;&#30340;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Systematic Sampling and Validation of Machine Learning-Parameterizations in Climate Models. (arXiv:2309.16177v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28151;&#21512;&#29289;&#29702;-&#26426;&#22120;&#23398;&#20064;&#27668;&#20505;&#27169;&#25311;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#22312;&#32447;&#24314;&#27169;&#38169;&#35823;&#37319;&#26679;&#21644;&#35780;&#20272;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#35774;&#35745;&#20013;&#21457;&#29616;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#29289;&#29702;-&#26426;&#22120;&#23398;&#20064;&#27668;&#20505;&#27169;&#25311;&#30340;&#36827;&#23637;&#21463;&#21040;&#33719;&#21462;&#39640;&#24615;&#33021;&#32806;&#21512;&#65288;&#21363;&#22312;&#32447;&#65289;&#27169;&#25311;&#30340;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#22312;&#33073;&#26426;&#29615;&#22659;&#20013;&#35780;&#20272;&#25968;&#30334;&#20010;&#26426;&#22120;&#23398;&#20064;&#21442;&#25968;&#21270;&#23376;&#32593;&#26684;&#38381;&#21512;&#65288;&#22914;&#23545;&#27969;&#21644;&#36752;&#23556;&#65289;&#26159;&#30452;&#25509;&#30340;&#65292;&#20294;&#22312;&#30456;&#21516;&#35268;&#27169;&#19978;&#30340;&#22312;&#32447;&#35780;&#20272;&#22312;&#25216;&#26415;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#36719;&#20214;&#33258;&#21160;&#21270;&#23454;&#29616;&#20102;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#22810;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#22312;&#32447;&#24314;&#27169;&#38169;&#35823;&#37319;&#26679;&#12290;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35780;&#20272;&#28151;&#21512;&#27668;&#20505;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21046;&#23450;&#25913;&#36827;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21253;&#21547;&#35760;&#24518;&#12289;&#30456;&#23545;&#28287;&#24230;&#36755;&#20837;&#29305;&#24449;&#36716;&#25442;&#21644;&#39069;&#22806;&#36755;&#20837;&#21464;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#22312;&#32447;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#12290;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;&#22312;&#32447;&#38169;&#35823;&#30340;&#26174;&#33879;&#24046;&#24322;&#20197;&#21450;&#33073;&#26426;&#19982;&#22312;&#32447;&#38169;&#35823;&#32479;&#35745;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#22312;&#32447;&#35780;&#20272;&#25968;&#30334;&#20010;&#20505;&#36873;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#26816;&#27979;&#21442;&#25968;&#21270;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in hybrid physics-machine learning (ML) climate simulations has been limited by the difficulty of obtaining performant coupled (i.e. online) simulations. While evaluating hundreds of ML parameterizations of subgrid closures (here of convection and radiation) offline is straightforward, online evaluation at the same scale is technically challenging. Our software automation achieves an order-of-magnitude larger sampling of online modeling errors than has previously been examined. Using this, we evaluate the hybrid climate model performance and define strategies to improve it. We show that model online performance improves when incorporating memory, a relative humidity input feature transformation, and additional input variables. We also reveal substantial variation in online error and inconsistencies between offline vs. online error statistics. The implication is that hundreds of candidate ML models should be evaluated online to detect the effects of parameterization design choi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#20351;&#29992;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#21644;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16175</link><description>&lt;p&gt;
&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#20351;&#29992;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Using Weak Supervision and Data Augmentation in Question Answering. (arXiv:2309.16175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38382;&#31572;&#31995;&#32479;&#20013;&#20351;&#29992;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20316;&#29992;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#21644;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#30340;&#29190;&#21457;&#24378;&#35843;&#20102;&#33719;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20197;&#22238;&#31572;&#21450;&#26102;&#21644;&#19982;&#30142;&#30149;&#30456;&#20851;&#30340;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#22312;&#30123;&#24773;&#21021;&#26399;&#65292;&#25105;&#20204;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#38382;&#31572;&#27169;&#22411;&#30340;&#32463;&#36807;&#21516;&#34892;&#35780;&#23457;&#30340;COVID-19&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24369;&#30417;&#30563;&#21644;&#25968;&#25454;&#22686;&#24378;&#22312;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38382;&#31572;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#31639;&#27861;BM25&#20174;&#23398;&#26415;&#35770;&#25991;&#30340;&#32467;&#26500;&#21270;&#25688;&#35201;&#20013;&#33258;&#21160;&#29983;&#25104;&#26631;&#31614;&#65292;&#25506;&#31350;&#36825;&#20123;&#26631;&#31614;&#26159;&#21542;&#25552;&#20379;&#20102;&#24369;&#30417;&#30563;&#20449;&#21495;&#26469;&#35757;&#32451;&#19968;&#20010;&#25277;&#21462;&#24335;&#38382;&#31572;&#27169;&#22411;&#12290;&#22312;&#21307;&#23398;&#39046;&#22495;&#19987;&#23478;&#30340;&#27880;&#37322;&#25968;&#25454;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#22522;&#20110;clinicaltrials.gov&#26550;&#26500;&#21644;&#25991;&#31456;&#30340;&#32467;&#26500;&#21270;&#25688;&#35201;&#26469;&#31574;&#21010;&#26032;&#30340;&#38382;&#31572;&#23545;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25193;&#20805;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The onset of the COVID-19 pandemic accentuated the need for access to biomedical literature to answer timely and disease-specific questions. During the early days of the pandemic, one of the biggest challenges we faced was the lack of peer-reviewed biomedical articles on COVID-19 that could be used to train machine learning models for question answering (QA). In this paper, we explore the roles weak supervision and data augmentation play in training deep neural network QA models. First, we investigate whether labels generated automatically from the structured abstracts of scholarly papers using an information retrieval algorithm, BM25, provide a weak supervision signal to train an extractive QA model. We also curate new QA pairs using information retrieval techniques, guided by the clinicaltrials.gov schema and the structured abstracts of articles, in the absence of annotated data from biomedical domain experts. Furthermore, we explore augmenting the training data of a deep neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D2DGN&#30340;&#22270;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#21024;&#38500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#20381;&#36182;&#21644;&#38468;&#21152;&#24320;&#38144;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20943;&#23569;&#35757;&#32451;&#37325;&#22797;&#24102;&#26469;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2309.16173</link><description>&lt;p&gt;
Distill to Delete: &#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#36827;&#34892;&#22270;&#32593;&#32476;&#20013;&#30340;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Distill to Delete: Unlearning in Graph Networks with Knowledge Distillation. (arXiv:2309.16173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D2DGN&#30340;&#22270;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#24335;&#21024;&#38500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#20381;&#36182;&#21644;&#38468;&#21152;&#24320;&#38144;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20943;&#23569;&#35757;&#32451;&#37325;&#22797;&#24102;&#26469;&#30340;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#36951;&#24536;&#24050;&#25104;&#20026;&#20174;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#21024;&#38500;&#20449;&#24687;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#21487;&#20197;&#21024;&#38500;&#33410;&#28857;&#12289;&#33410;&#28857;&#31867;&#12289;&#36793;&#25110;&#36793;&#31867;&#12290;&#36951;&#24536;&#26041;&#27861;&#20351;GNN&#27169;&#22411;&#31526;&#21512;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65288;&#21363;&#34987;&#36951;&#24536;&#26435;&#65289;&#65292;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#36890;&#36807;&#36991;&#20813;&#37325;&#22797;&#35757;&#32451;&#26469;&#20943;&#23569;GPU&#23567;&#26102;&#30340;&#30899;&#36275;&#36857;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#21306;&#21644;&#32858;&#21512;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#22270;&#20381;&#36182;&#21644;&#38468;&#21152;&#24320;&#38144;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26368;&#36817;&#65292;GNNDelete&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#32531;&#35299;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#36951;&#24536;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#21363;GNN&#20013;&#30340;&#36328;&#36724;&#33976;&#39311;&#36827;&#34892;&#21024;&#38500;&#65288;D2DGN&#65289;&#12290;&#36825;&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#33976;&#39311;&#26694;&#26550;&#65292;&#23558;&#23436;&#25972;&#30340;&#22270;&#30693;&#35782;&#21010;&#20998;&#24182;&#26631;&#35760;&#20026;&#20445;&#30041;&#21644;&#21024;&#38500;&#12290;&#23427;&#20351;&#29992;&#21709;&#24212;&#20026;&#22522;&#30784;&#36827;&#34892;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph unlearning has emerged as a pivotal method to delete information from a pre-trained graph neural network (GNN). One may delete nodes, a class of nodes, edges, or a class of edges. An unlearning method enables the GNN model to comply with data protection regulations (i.e., the right to be forgotten), adapt to evolving data distributions, and reduce the GPU-hours carbon footprint by avoiding repetitive retraining. Existing partitioning and aggregation-based methods have limitations due to their poor handling of local graph dependencies and additional overhead costs. More recently, GNNDelete offered a model-agnostic approach that alleviates some of these issues. Our work takes a novel approach to address these challenges in graph unlearning through knowledge distillation, as it distills to delete in GNN (D2DGN). It is a model-agnostic distillation framework where the complete graph knowledge is divided and marked for retention and deletion. It performs distillation with response-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#30340;&#19968;&#33268;&#24615;&#23545;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#27169;&#22411;&#35757;&#32451;&#25152;&#24471;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;RM&#19968;&#33268;&#24615;&#30340;&#23545;&#27604;&#25552;&#31034;&#30340;&#22522;&#20934;&#27979;&#35797;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.16155</link><description>&lt;p&gt;
&#22870;&#21169;&#65288;&#19981;&#65289;&#19968;&#33268;&#24615;&#23545;RLHF&#30340;&#28051;&#28404;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
The Trickle-down Impact of Reward (In-)consistency on RLHF. (arXiv:2309.16155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#30340;&#19968;&#33268;&#24615;&#23545;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#27169;&#22411;&#35757;&#32451;&#25152;&#24471;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;RM&#19968;&#33268;&#24615;&#30340;&#23545;&#27604;&#25552;&#31034;&#30340;&#22522;&#20934;&#27979;&#35797;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#26631;&#20934;&#23454;&#36341;&#28041;&#21450;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#65292;&#32780;RM&#26412;&#36523;&#26159;&#36890;&#36807;&#35757;&#32451;&#26469;&#21453;&#26144;&#20154;&#31867;&#23545;&#26399;&#26395;&#29983;&#25104;&#30340;&#20559;&#22909;&#12290;&#19968;&#20010;&#20540;&#24471;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#26159;RM&#30340;&#65288;&#19981;&#65289;&#19968;&#33268;&#24615; - &#21363;&#23427;&#20204;&#33021;&#21542;&#35782;&#21035;&#19981;&#21516;&#25552;&#31034;&#30340;&#35821;&#20041;&#21464;&#21270;&#24182;&#36866;&#24403;&#22320;&#35843;&#25972;&#22870;&#21169;&#20998;&#37197; - &#20197;&#21450;&#23427;&#20204;&#23545;&#19979;&#28216;RLHF&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;RM&#19981;&#19968;&#33268;&#24615;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30456;&#20851;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#65288;1&#65289;&#25105;&#20204;&#22914;&#20309;&#34913;&#37327;&#22870;&#21169;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65311;&#65288;2&#65289;&#29616;&#26377;&#30340;RM&#26377;&#22810;&#19968;&#33268;&#65292;&#25105;&#20204;&#22914;&#20309;&#25913;&#36827;&#23427;&#20204;&#65311;&#65288;3&#65289;&#22870;&#21169;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#20309;&#31181;&#26041;&#24335;&#24433;&#21709;RLHF&#27169;&#22411;&#35757;&#32451;&#25152;&#24471;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;RM&#19968;&#33268;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#31574;&#30053;"&#23545;&#27604;&#25552;&#31034;"&#12290;&#27599;&#20010;&#23545;&#27604;&#25552;&#31034;&#31034;&#20363;&#37117;&#21253;&#21547;&#19968;&#23545;&#20855;&#26377;&#19981;&#21516;&#30495;&#23454;&#21709;&#24212;&#30340;&#35789;&#27719;&#30456;&#20284;&#30340;&#25351;&#20196;&#12290;&#19968;&#33268;&#30340;RM&#26159;&#26399;&#26395;&#23545;&#36825;&#23545;&#25351;&#20196;&#32473;&#20986;&#30456;&#20284;&#22870;&#21169;&#20998;&#37197;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard practice within Reinforcement Learning from Human Feedback (RLHF) involves optimizing against a Reward Model (RM), which itself is trained to reflect human preferences for desirable generations. A notable subject that is understudied is the (in-)consistency of RMs -- whether they can recognize the semantic changes to different prompts and appropriately adapt their reward assignments -- and their impact on the downstream RLHF model.  In this paper, we visit a series of research questions relevant to RM inconsistency: (1) How can we measure the consistency of reward models? (2) How consistent are the existing RMs and how can we improve them? (3) In what ways does reward inconsistency influence the chatbots resulting from the RLHF model training?  We propose Contrast Instructions -- a benchmarking strategy for the consistency of RM. Each example in Contrast Instructions features a pair of lexically similar instructions with different ground truth responses. A consistent RM is exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#27861;&#33719;&#21462;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.16143</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#20248;&#21270;&#21512;&#25104;&#26679;&#26412;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative Semi-supervised Learning with Meta-Optimized Synthetic Samples. (arXiv:2309.16143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#20013;&#26080;&#27861;&#33719;&#21462;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20351;&#29992;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#30001;&#20110;&#27861;&#24459;&#38480;&#21046;&#65288;&#20363;&#22914;&#65292;GDPR&#65289;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#12290;&#26412;&#25991;&#30740;&#31350;&#19968;&#20010;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#22312;&#27809;&#26377;&#23454;&#38469;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#21322;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20174;&#21253;&#21547;&#25968;&#30334;&#19975;&#26679;&#26412;&#30340;&#22810;&#26679;&#39046;&#22495;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;ImageNet&#65289;&#35757;&#32451;&#30340;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#35782;&#21035;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#20013;&#20223;&#30495;&#26080;&#26631;&#31614;&#26679;&#26412;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21512;&#25104;&#26679;&#26412;&#26469;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#20132;&#26367;&#20248;&#21270;&#38382;&#39064;&#65306;&#65288;i&#65289;&#20803;&#23398;&#20064;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#21644;&#65288;ii&#65289;&#20351;&#29992;&#30495;&#23454;&#26631;&#35760;&#26679;&#26412;&#21644;&#21512;&#25104;&#26679;&#26412;&#36827;&#34892;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) is a promising approach for training deep classification models using labeled and unlabeled datasets. However, existing SSL methods rely on a large unlabeled dataset, which may not always be available in many real-world applications due to legal constraints (e.g., GDPR). In this paper, we investigate the research question: Can we train SSL models without real unlabeled datasets? Instead of using real unlabeled datasets, we propose an SSL method using synthetic datasets generated from generative foundation models trained on datasets containing millions of samples in diverse domains (e.g., ImageNet). Our main concepts are identifying synthetic samples that emulate unlabeled samples from generative foundation models and training classifiers using these synthetic samples. To achieve this, our method is formulated as an alternating optimization problem: (i) meta-learning of generative foundation models and (ii) SSL of classifiers using real labeled and synthet
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#37319;&#26679;&#30340;&#20004;&#27493;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#27880;&#65292;&#25552;&#39640;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.16139</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#37319;&#26679;&#30340;&#20004;&#27493;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Two-Step Active Learning for Instance Segmentation with Uncertainty and Diversity Sampling. (arXiv:2309.16139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16139
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#37319;&#26679;&#30340;&#20004;&#27493;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#27880;&#65292;&#25552;&#39640;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#36136;&#37327;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#24102;&#26377;&#23454;&#20363;&#25513;&#30721;&#21644;&#20998;&#31867;&#30340;&#26631;&#35760;&#22270;&#20687;&#65292;&#32780;&#36825;&#24448;&#24448;&#26159;&#26114;&#36149;&#30340;&#12290;&#20027;&#21160;&#23398;&#20064;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#21644;&#20195;&#34920;&#24615;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#27880;&#65292;&#20197;&#22312;&#26368;&#23567;&#26631;&#27880;&#25104;&#26412;&#19979;&#36861;&#27714;&#26368;&#20339;&#24615;&#33021;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#23613;&#31649;&#20027;&#21160;&#23398;&#20064;&#22312;&#23454;&#20363;&#20998;&#21106;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#19982;&#20854;&#20182;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#30456;&#27604;&#65292;&#23454;&#20363;&#20998;&#21106;&#39046;&#22495;&#30340;&#30740;&#31350;&#36739;&#23569;&#65292;&#21518;&#32773;&#30340;&#26631;&#27880;&#38656;&#27714;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#32493;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#23558;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#37319;&#26679;&#19982;&#22522;&#20110;&#22810;&#26679;&#24615;&#30340;&#37319;&#26679;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#19981;&#20165;&#31616;&#21333;&#26131;&#23454;&#29616;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#36965;&#24863;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#23427;&#20351;&#26631;&#27880;&#25928;&#29575;&#25552;&#39640;&#20102;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training high-quality instance segmentation models requires an abundance of labeled images with instance masks and classifications, which is often expensive to procure. Active learning addresses this challenge by striving for optimum performance with minimal labeling cost by selecting the most informative and representative images for labeling. Despite its potential, active learning has been less explored in instance segmentation compared to other tasks like image classification, which require less labeling. In this study, we propose a post-hoc active learning algorithm that integrates uncertainty-based sampling with diversity-based sampling. Our proposed algorithm is not only simple and easy to implement, but it also delivers superior performance on various datasets. Its practical application is demonstrated on a real-world overhead imagery dataset, where it increases the labeling efficiency fivefold.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35889;&#23637;&#24320;&#23398;&#20064;&#26102;&#31354;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#30028;&#30340;&#26102;&#31354;&#26041;&#31243;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#35777;&#26126;&#19982;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#26356;&#22823;&#30340;&#38382;&#39064;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.16131</link><description>&lt;p&gt;
&#23398;&#20064;&#26102;&#31354;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#30340;&#35889;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Spectral Approach for Learning Spatiotemporal Neural Differential Equations. (arXiv:2309.16131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35889;&#23637;&#24320;&#23398;&#20064;&#26102;&#31354;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#21487;&#20197;&#22788;&#29702;&#26080;&#30028;&#30340;&#26102;&#31354;&#26041;&#31243;&#12290;&#36825;&#20010;&#26041;&#27861;&#34987;&#35777;&#26126;&#19982;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#26356;&#22823;&#30340;&#38382;&#39064;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23545;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35745;&#31639;&#37325;&#26500;&#24494;&#20998;&#26041;&#31243;&#65288;DE&#65289;&#30340;&#30740;&#31350;&#20805;&#28385;&#20102;&#20852;&#36259;&#65292;&#36825;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#26426;&#21046;&#30340;&#39069;&#22806;&#27934;&#23519;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#31354;&#38388;&#20013;&#30340;&#35889;&#23637;&#24320;&#26469;&#23398;&#20064;&#26102;&#31354;DE&#12290;&#25105;&#20204;&#35889;&#31070;&#32463;DE&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#23427;&#19981;&#20381;&#36182;&#20110;&#31354;&#38388;&#31163;&#25955;&#21270;&#65292;&#22240;&#27492;&#20801;&#35768;&#30446;&#26631;&#26102;&#31354;&#26041;&#31243;&#21253;&#21547;&#38271;&#33539;&#22260;&#12289;&#38750;&#23616;&#37096;&#30340;&#31354;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#20316;&#29992;&#20110;&#26080;&#30028;&#30340;&#31354;&#38388;&#22495;&#12290;&#25105;&#20204;&#30340;&#35889;&#26041;&#27861;&#34987;&#35777;&#26126;&#19982;&#19968;&#20123;&#26368;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19968;&#26679;&#20934;&#30830;&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#20316;&#29992;&#20110;&#26377;&#30028;&#22495;&#30340;PDE&#12290;&#36890;&#36807;&#20026;&#23398;&#20064;PDE&#21644;&#31215;&#20998;&#24494;&#20998;&#26041;&#31243;&#24320;&#21457;&#19968;&#20010;&#35889;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25193;&#23637;&#21040;&#36866;&#29992;&#20110;&#26080;&#30028;DE&#21644;&#26356;&#22823;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly developing machine learning methods has stimulated research interest in computationally reconstructing differential equations (DEs) from observational data which may provide additional insight into underlying causative mechanisms. In this paper, we propose a novel neural-ODE based method that uses spectral expansions in space to learn spatiotemporal DEs. The major advantage of our spectral neural DE learning approach is that it does not rely on spatial discretization, thus allowing the target spatiotemporal equations to contain long range, nonlocal spatial interactions that act on unbounded spatial domains. Our spectral approach is shown to be as accurate as some of the latest machine learning approaches for learning PDEs operating on bounded domains. By developing a spectral framework for learning both PDEs and integro-differential equations, we extend machine learning methods to apply to unbounded DEs and a larger class of problems.
&lt;/p&gt;</description></item><item><title>ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.16119</link><description>&lt;p&gt;
ModuLoRA:&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#38598;&#25104;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#23545;3 Bit LLMs&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers. (arXiv:2309.16119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16119
&lt;/p&gt;
&lt;p&gt;
ModuLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#12289;&#33021;&#22815;&#22312;&#28040;&#36153;&#32423;GPU&#19978;&#25903;&#25345;3&#27604;&#29305;LLMs&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#19982;&#27169;&#22359;&#21270;&#37327;&#21270;&#22120;&#30340;&#38598;&#25104;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24494;&#35843;&#31639;&#27861;&#65292;&#21487;&#25903;&#25345;&#22312;&#20165;&#20351;&#29992;1&#20010;48GB GPU&#19978;&#20197;3&#27604;&#29305;&#25110;4&#27604;&#29305;&#31934;&#24230;&#24494;&#35843;&#20855;&#26377;65B&#21442;&#25968;&#30340;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#8212;&#8212;&#27169;&#22359;&#21270;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;ModuLoRA&#65289;&#65292;&#36890;&#36807;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#23558;&#20219;&#20309;&#29992;&#25143;&#25351;&#23450;&#30340;&#26435;&#37325;&#37327;&#21270;&#22120;&#19982;&#24494;&#35843;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#31616;&#21333;&#30340;&#37327;&#21270;&#26080;&#20851;&#30340;&#21453;&#21521;&#20256;&#25773;&#65292;&#36890;&#36807;&#33258;&#23450;&#20041;&#30340;&#40657;&#30418;&#37327;&#21270;&#27169;&#22359;&#20174;&#20302;&#31934;&#24230;LLM&#26435;&#37325;&#20013;&#33258;&#36866;&#24212;&#22320;&#29983;&#25104;&#26435;&#37325;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#39318;&#27425;&#33021;&#22815;&#36827;&#34892;3&#27604;&#29305;LLMs&#30340;&#24494;&#35843;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;3&#27604;&#29305;OPTQ&#37327;&#21270;&#24448;&#24448;&#20248;&#20110;&#20381;&#36182;&#20110;&#36739;&#19981;&#22797;&#26434;&#30340;4&#27604;&#29305;&#21644;8&#27604;&#29305;&#26041;&#27861;&#30340;&#24494;&#35843;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;ModuLoRA&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;&#30340;&#20869;&#23384;&#27604;&#29616;&#26377;&#26041;&#27861;&#23569;&#24456;&#22810;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#27969;&#34892;&#30340;&#25688;&#35201;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;ROUGE&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a memory-efficient finetuning algorithm for large language models (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA), integrates any user-specified weight quantizer with finetuning via low-rank adapters (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that adaptively materializes low-precision LLM weights from a custom black-box quantization module. This approach enables finetuning 3-bit LLMs for the first time--leveraging state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains competitive performance on text classification, natural language infernece, and instruction following tasks using significantly less memory than existing approaches, and we also surpass the state-of-the-art ROUGE score on a popular summarization task. W
&lt;/p&gt;</description></item><item><title>D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.16118</link><description>&lt;p&gt;
D$^3$Fields: &#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#29992;&#20110;&#38646;&#26679;&#26412;&#21487;&#27867;&#21270;&#26426;&#22120;&#20154;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation. (arXiv:2309.16118v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16118
&lt;/p&gt;
&lt;p&gt;
D$^3$Fields&#26159;&#19968;&#20010;&#21160;&#24577;&#30340;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#65292;&#23558;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#20197;&#21450;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#32534;&#30721;&#36215;&#26469;&#12290;&#23427;&#21487;&#20197;&#28789;&#27963;&#22320;&#20351;&#29992;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#25351;&#23450;&#30446;&#26631;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22330;&#26223;&#34920;&#31034;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#31995;&#32479;&#20013;&#19968;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#19968;&#20010;&#29702;&#24819;&#30340;&#34920;&#31034;&#24212;&#35813;&#26159;&#19977;&#32500;&#30340;&#12289;&#21160;&#24577;&#30340;&#21644;&#35821;&#20041;&#21270;&#30340;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#24448;&#24448;&#21516;&#26102;&#32570;&#20047;&#36825;&#19977;&#20010;&#23646;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;D$^3$Fields&#21160;&#24577;&#19977;&#32500;&#25551;&#36848;&#31526;&#22330;&#12290;&#36825;&#20123;&#22330;&#25429;&#25417;&#20102;&#24213;&#23618;&#19977;&#32500;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#65292;&#32534;&#30721;&#20102;&#35821;&#20041;&#29305;&#24449;&#21644;&#23454;&#20363;&#25513;&#27169;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24037;&#20316;&#21306;&#22495;&#20013;&#30340;&#20219;&#24847;&#19977;&#32500;&#28857;&#25237;&#24433;&#21040;&#22810;&#35270;&#35282;&#30340;&#20108;&#32500;&#35270;&#35273;&#35266;&#23519;&#20013;&#65292;&#24182;&#25554;&#20540;&#20174;&#22522;&#26412;&#27169;&#22411;&#20013;&#24471;&#21040;&#30340;&#29305;&#24449;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#34701;&#21512;&#25551;&#36848;&#31526;&#22330;&#21487;&#20197;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#32972;&#26223;&#12289;&#39118;&#26684;&#21644;&#23454;&#20363;&#30340;&#20108;&#32500;&#22270;&#20687;&#28789;&#27963;&#22320;&#25351;&#23450;&#30446;&#26631;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25551;&#36848;&#31526;&#22330;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#23558;&#25105;&#20204;&#30340;&#34920;&#31034;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#22330;&#26223;&#21644;&#27169;&#25311;&#20013;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scene representation has been a crucial design choice in robotic manipulation systems. An ideal representation should be 3D, dynamic, and semantic to meet the demands of diverse manipulation tasks. However, previous works often lack all three properties simultaneously. In this work, we introduce D$^3$Fields dynamic 3D descriptor fields. These fields capture the dynamics of the underlying 3D environment and encode both semantic features and instance masks. Specifically, we project arbitrary 3D points in the workspace onto multi-view 2D visual observations and interpolate features derived from foundational models. The resulting fused descriptor fields allow for flexible goal specifications using 2D images with varied contexts, styles, and instances. To evaluate the effectiveness of these descriptor fields, we apply our representation to a wide range of robotic manipulation tasks in a zero-shot manner. Through extensive evaluation in both real-world scenarios and simulations, we demonst
&lt;/p&gt;</description></item><item><title>E2Net&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#23376;&#32593;&#33976;&#39311;&#21644;&#31934;&#30830;&#30340;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23567;&#30340;&#36951;&#24536;&#65292;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38480;&#21046;&#19979;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.16117</link><description>&lt;p&gt;
E2Net: &#24377;&#24615;&#25193;&#23637;&#32593;&#32476;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E2Net: Resource-Efficient Continual Learning with Elastic Expansion Network. (arXiv:2309.16117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16117
&lt;/p&gt;
&lt;p&gt;
E2Net&#26159;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26680;&#24515;&#23376;&#32593;&#33976;&#39311;&#21644;&#31934;&#30830;&#30340;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#65292;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#36739;&#23567;&#30340;&#36951;&#24536;&#65292;&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38480;&#21046;&#19979;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#32780;&#19981;&#28040;&#38500;&#20197;&#21069;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#25345;&#32493;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#23481;&#37327;&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24377;&#24615;&#25193;&#23637;&#32593;&#32476;&#65288;E2Net&#65289;&#12290;&#36890;&#36807;&#26680;&#24515;&#23376;&#32593;&#33976;&#39311;&#21644;&#31934;&#30830;&#30340;&#22238;&#25918;&#26679;&#26412;&#36873;&#25321;&#65292;E2Net&#22312;&#30456;&#21516;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38480;&#21046;&#19979;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24179;&#22343;&#20934;&#30830;&#24615;&#21644;&#36739;&#23567;&#30340;&#36951;&#24536;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#22788;&#29702;&#26102;&#38388;&#12290;&#22312;E2Net&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20195;&#34920;&#24615;&#32593;&#32476;&#33976;&#39311;&#65292;&#36890;&#36807;&#35780;&#20272;&#21442;&#25968;&#25968;&#37327;&#21644;&#19982;&#24037;&#20316;&#32593;&#32476;&#30340;&#36755;&#20986;&#30456;&#20284;&#24615;&#26469;&#35782;&#21035;&#20195;&#34920;&#24615;&#30340;&#26680;&#24515;&#23376;&#32593;&#65292;&#33976;&#39311;&#24037;&#20316;&#32593;&#32476;&#20869;&#30340;&#31867;&#20284;&#23376;&#32593;&#20197;&#20943;&#36731;&#23545;&#37325;&#28436;&#32531;&#20914;&#21306;&#30340;&#20381;&#36182;&#65292;&#24182;&#20419;&#36827;&#36328;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#20026;&#20102;&#25552;&#39640;&#23384;&#20648;&#36164;&#28304;&#21033;&#29992;&#29575;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23376;&#32593;&#32422;&#26463;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning methods are designed to learn new tasks without erasing previous knowledge. However, Continual Learning often requires massive computational power and storage capacity for satisfactory performance. In this paper, we propose a resource-efficient continual learning method called the Elastic Expansion Network (E2Net). Leveraging core subnet distillation and precise replay sample selection, E2Net achieves superior average accuracy and diminished forgetting within the same computational and storage constraints, all while minimizing processing time. In E2Net, we propose Representative Network Distillation to identify the representative core subnet by assessing parameter quantity and output similarity with the working network, distilling analogous subnets within the working network to mitigate reliance on rehearsal buffers and facilitating knowledge transfer across previous tasks. To enhance storage resource utilization, we then propose Subnet Constraint Experience Replay t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#22609;&#36896;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#23545;&#36825;&#20123;&#32452;&#21512;&#30340;&#37319;&#26679;&#12290;&#23637;&#31034;&#20102;&#22312;GFlowNets&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#32452;&#21512;&#22609;&#36896;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20108;&#20803;&#36816;&#31639;&#30340;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2309.16115</link><description>&lt;p&gt;
&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Compositional Sculpting of Iterative Generative Processes. (arXiv:2309.16115v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#22609;&#36896;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#65292;&#36890;&#36807;&#20998;&#31867;&#22120;&#25351;&#23548;&#23454;&#29616;&#23545;&#36825;&#20123;&#32452;&#21512;&#30340;&#37319;&#26679;&#12290;&#23637;&#31034;&#20102;&#22312;GFlowNets&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#32452;&#21512;&#22609;&#36896;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#20108;&#20803;&#36816;&#31639;&#30340;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#35757;&#32451;&#25104;&#26412;&#21644;&#20026;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#38656;&#27714;&#65292;&#24341;&#21457;&#20102;&#23545;&#27169;&#22411;&#37325;&#29992;&#21644;&#32452;&#21512;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#32452;&#21512;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;(&#22914;GFlowNets&#21644;&#25193;&#25955;&#27169;&#22411;)&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#65292;&#20026;&#20102;&#23454;&#29616;&#25152;&#38656;&#30340;&#30446;&#26631;&#20998;&#24067;&#65292;&#29983;&#25104;&#36807;&#31243;&#30340;&#25152;&#26377;&#27493;&#39588;&#37117;&#38656;&#35201;&#21327;&#35843;&#65292;&#24182;&#28385;&#36275;&#24494;&#22937;&#30340;&#24179;&#34913;&#26465;&#20214;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#32452;&#21512;&#22609;&#36896;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#29992;&#20110;&#23450;&#20041;&#36845;&#20195;&#29983;&#25104;&#36807;&#31243;&#30340;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#31867;&#22120;&#25351;&#23548;&#30340;&#20174;&#36825;&#20123;&#32452;&#21512;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;GFlowNets&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#32452;&#21512;&#22609;&#36896;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20004;&#31181;&#20108;&#20803;&#36816;&#31639;&#8212;&#8212;&#35843;&#21644;&#24179;&#22343;($p_1 \otimes p_2$)&#21644;&#23545;&#27604;&#24230;($p_1 \unicode{x25D1}\,p_2$)&#20043;&#38388;&#30340;&#36816;&#31639;&#65292;&#20197;&#21450;&#23558;&#36825;&#20123;&#36816;&#31639;&#25512;&#24191;&#21040;&#22810;&#20010;&#32452;&#20998;&#20998;&#24067;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22270;&#20687;&#21644;...
&lt;/p&gt;
&lt;p&gt;
High training costs of generative models and the need to fine-tune them for specific tasks have created a strong interest in model reuse and composition. A key challenge in composing iterative generative processes, such as GFlowNets and diffusion models, is that to realize the desired target distribution, all steps of the generative process need to be coordinated, and satisfy delicate balance conditions. In this work, we propose Compositional Sculpting: a general approach for defining compositions of iterative generative processes. We then introduce a method for sampling from these compositions built on classifier guidance. We showcase ways to accomplish compositional sculpting in both GFlowNets and diffusion models. We highlight two binary operations $\unicode{x2014}$ the harmonic mean ($p_1 \otimes p_2$) and the contrast ($p_1 \unicode{x25D1}\,p_2$) between pairs, and the generalization of these operations to multiple component distributions. We offer empirical results on image and m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#25110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#21463;&#38480;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.16114</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#25110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#24615;&#33021;&#27604;&#36739;&#29992;&#20110;&#21463;&#38480;&#36712;&#36857;&#25506;&#32034;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Comparing Active Learning Performance Driven by Gaussian Processes or Bayesian Neural Networks for Constrained Trajectory Exploration. (arXiv:2309.16114v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#25110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#21463;&#38480;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#26426;&#22120;&#20154;&#36890;&#36807;&#22686;&#21152;&#33258;&#20027;&#24615;&#26469;&#25512;&#21160;&#25105;&#20204;&#30340;&#22826;&#31354;&#25506;&#27979;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#29992;&#20110;&#20195;&#26367;&#20154;&#31867;&#25506;&#38505;&#32773;&#36827;&#34892;&#29616;&#22330;&#25506;&#32034;&#21644;&#21462;&#26679;&#12290;&#30446;&#21069;&#65292;&#20154;&#31867;&#39537;&#21160;&#26426;&#22120;&#20154;&#20197;&#28385;&#36275;&#31185;&#23398;&#30446;&#26631;&#65292;&#20294;&#26681;&#25454;&#26426;&#22120;&#20154;&#30340;&#20301;&#32622;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#21644;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#20449;&#24687;&#20132;&#27969;&#21644;&#39537;&#21160;&#21629;&#20196;&#21487;&#33021;&#20250;&#23548;&#33268;&#20219;&#21153;&#23436;&#25104;&#30340;&#19981;&#24517;&#35201;&#24310;&#36831;&#12290;&#32534;&#30721;&#26377;&#31185;&#23398;&#30446;&#26631;&#21644;&#25506;&#32034;&#31574;&#30053;&#30340;&#33258;&#20027;&#26426;&#22120;&#20154;&#19981;&#20250;&#20986;&#29616;&#36890;&#20449;&#24310;&#36831;&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#26234;&#33021;&#25506;&#32034;&#30340;&#33021;&#21147;&#65292;&#20294;&#24213;&#23618;&#30340;&#27169;&#22411;&#32467;&#26500;&#20250;&#24433;&#21709;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#20934;&#30830;&#24418;&#25104;&#23545;&#29615;&#22659;&#30340;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#25110;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#39537;&#21160;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#25506;&#32034;&#31574;&#30053;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#27492;&#25506;&#32034;&#31574;&#30053;&#32534;&#30721;&#22312;&#21463;&#38480;&#22330;&#26223;&#20013;&#30340;&#20195;&#29702;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robots with increasing autonomy progress our space exploration capabilities, particularly for in-situ exploration and sampling to stand in for human explorers. Currently, humans drive robots to meet scientific objectives, but depending on the robot's location, the exchange of information and driving commands between the human operator and robot may cause undue delays in mission fulfillment. An autonomous robot encoded with a scientific objective and an exploration strategy incurs no communication delays and can fulfill missions more quickly. Active learning algorithms offer this capability of intelligent exploration, but the underlying model structure varies the performance of the active learning algorithm in accurately forming an understanding of the environment. In this paper, we investigate the performance differences between active learning algorithms driven by Gaussian processes or Bayesian neural networks for exploration strategies encoded on agents that are constrained in their 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21160;&#21147;&#23849;&#28291;&#38382;&#39064;&#65292;&#21457;&#29616;&#29305;&#24449;&#24402;&#19968;&#21270;&#21487;&#20197;&#38450;&#27490;&#27492;&#38382;&#39064;&#30340;&#20986;&#29616;&#65292;&#20026;&#35299;&#20915;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.16109</link><description>&lt;p&gt;
&#29305;&#24449;&#24402;&#19968;&#21270;&#38450;&#27490;&#38750;&#23545;&#27604;&#23398;&#20064;&#21160;&#21147;&#30340;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
Feature Normalization Prevents Collapse of Non-contrastive Learning Dynamics. (arXiv:2309.16109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38750;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21160;&#21147;&#23849;&#28291;&#38382;&#39064;&#65292;&#21457;&#29616;&#29305;&#24449;&#24402;&#19968;&#21270;&#21487;&#20197;&#38450;&#27490;&#27492;&#38382;&#39064;&#30340;&#20986;&#29616;&#65292;&#20026;&#35299;&#20915;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#30340;&#20004;&#20010;&#27491;&#35270;&#22270;&#22312;&#25968;&#25454;&#34920;&#31034;&#31354;&#38388;&#20013;&#36890;&#36807;&#21560;&#24341;&#21147;&#20351;&#23427;&#20204;&#30456;&#20284;&#65292;&#32780;&#36890;&#36807;&#25490;&#26021;&#21147;&#20351;&#23427;&#20204;&#36828;&#31163;&#36127;&#26679;&#26412;&#12290;&#38750;&#23545;&#27604;&#23398;&#20064;&#36890;&#36807;BYOL&#21644;SimSiam&#31561;&#25163;&#27573;&#21435;&#38500;&#20102;&#36127;&#26679;&#26412;&#65292;&#24182;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;&#34429;&#28982;&#30001;&#20110;&#32570;&#20047;&#25490;&#26021;&#21147;&#65292;&#23398;&#21040;&#30340;&#34920;&#31034;&#21487;&#33021;&#20250;&#23849;&#28291;&#25104;&#19968;&#20010;&#21333;&#28857;&#65292;&#20294;&#30000;&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#23398;&#20064;&#21160;&#21147;&#20998;&#26512;&#25581;&#31034;&#65292;&#22914;&#26524;&#25968;&#25454;&#22686;&#24378;&#36275;&#22815;&#24378;&#20110;&#27491;&#21017;&#21270;&#65292;&#21017;&#34920;&#31034;&#21487;&#20197;&#36991;&#20813;&#23849;&#28291;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#20998;&#26512;&#27809;&#26377;&#32771;&#34385;&#24120;&#29992;&#30340;&#29305;&#24449;&#24402;&#19968;&#21270;&#65292;&#21363;&#22312;&#34913;&#37327;&#34920;&#31034;&#30456;&#20284;&#24615;&#20043;&#21069;&#36827;&#34892;&#30340;&#24402;&#19968;&#21270;&#25805;&#20316;&#65292;&#22240;&#27492;&#36807;&#24378;&#30340;&#27491;&#21017;&#21270;&#21487;&#33021;&#20250;&#23548;&#33268;&#21160;&#21147;&#23849;&#28291;&#65292;&#36825;&#22312;&#29305;&#24449;&#24402;&#19968;&#21270;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#26159;&#19981;&#33258;&#28982;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning is a self-supervised representation learning framework, where two positive views generated through data augmentation are made similar by an attraction force in a data representation space, while a repulsive force makes them far from negative examples. Non-contrastive learning, represented by BYOL and SimSiam, further gets rid of negative examples and improves computational efficiency. While learned representations may collapse into a single point due to the lack of the repulsive force at first sight, Tian et al. (2021) revealed through the learning dynamics analysis that the representations can avoid collapse if data augmentation is sufficiently stronger than regularization. However, their analysis does not take into account commonly-used feature normalization, a normalizer before measuring the similarity of representations, and hence excessively strong regularization may collapse the dynamics, which is an unnatural behavior under the presence of feature normalizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2309.16108</link><description>&lt;p&gt;
&#39057;&#36947;&#35270;&#35273;Transformer&#65306;&#19968;&#24352;&#22270;&#20540;C x 16 x 16&#20010;&#35789;
&lt;/p&gt;
&lt;p&gt;
Channel Vision Transformers: An Image Is Worth C x 16 x 16 Words. (arXiv:2309.16108v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ChannelViT&#27169;&#22411;&#65292;&#36890;&#36807;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#21644;&#24341;&#20837;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#23545;&#22810;&#36890;&#36947;&#22270;&#20687;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#31561;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#22312;&#29616;&#20195;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26576;&#20123;&#22270;&#20687;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#22914;&#26174;&#24494;&#38236;&#21644;&#21355;&#26143;&#25104;&#20687;&#65292;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#22810;&#20010;&#36890;&#36947;&#65292;&#27599;&#20010;&#36890;&#36947;&#37117;&#25658;&#24102;&#30528;&#35821;&#20041;&#19978;&#19981;&#21516;&#21644;&#29420;&#31435;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#24517;&#39035;&#23545;&#36755;&#20837;&#36890;&#36947;&#30340;&#31232;&#30095;&#24615;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#65292;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#36807;&#31243;&#20013;&#21487;&#33021;&#27809;&#26377;&#23494;&#38598;&#21487;&#29992;&#30340;&#36890;&#36947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;ViT&#26550;&#26500;&#30340;&#20462;&#25913;&#65292;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#36890;&#36947;&#20043;&#38388;&#30340;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#20998;&#23618;&#36890;&#36947;&#37319;&#26679;(HCS)&#20316;&#20026;&#19968;&#31181;&#38468;&#21152;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#20197;&#30830;&#20445;&#22312;&#27979;&#35797;&#36807;&#31243;&#20013;&#20165;&#20986;&#29616;&#37096;&#20998;&#36890;&#36947;&#26102;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;ChannelViT&#29420;&#31435;&#22320;&#26500;&#24314;&#34917;&#19969;&#20196;&#29260;&#24182;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#36890;&#36947;&#23884;&#20837;&#23558;&#20854;&#28155;&#21152;&#21040;&#34917;&#19969;&#20196;&#29260;&#20013;&#65292;&#31867;&#20284;&#20110;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has emerged as a powerful architecture in the realm of modern computer vision. However, its application in certain imaging fields, such as microscopy and satellite imaging, presents unique challenges. In these domains, images often contain multiple channels, each carrying semantically distinct and independent information. Furthermore, the model must demonstrate robustness to sparsity in input channels, as they may not be densely available during training or testing. In this paper, we propose a modification to the ViT architecture that enhances reasoning across the input channels and introduce Hierarchical Channel Sampling (HCS) as an additional regularization technique to ensure robustness when only partial channels are presented during test time. Our proposed model, ChannelViT, constructs patch tokens independently from each input channel and utilizes a learnable channel embedding that is added to the patch tokens, similar to positional embeddings. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#20013;&#65292;&#20801;&#35768;&#20449;&#24687;&#27844;&#28431;&#21644;&#36817;&#20284;&#20056;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#35802;&#23454;&#33410;&#28857;&#25968;&#37327;&#20026;&#23569;&#25968;&#26102;&#65292;&#24046;&#20998;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2309.16105</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#23433;&#20840;&#20056;&#27861;&#65306;&#22312;&#22122;&#22768;&#20013;&#38544;&#34255;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Secure Multiplication: Hiding Information in the Rubble of Noise. (arXiv:2309.16105v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#20013;&#65292;&#20801;&#35768;&#20449;&#24687;&#27844;&#28431;&#21644;&#36817;&#20284;&#20056;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#24403;&#35802;&#23454;&#33410;&#28857;&#25968;&#37327;&#20026;&#23569;&#25968;&#26102;&#65292;&#24046;&#20998;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#31169;&#23494;&#20998;&#24067;&#24335;&#22810;&#26041;&#20056;&#27861;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#30830;&#35748;&#65292;Shamir&#31192;&#23494;&#20849;&#20139;&#32534;&#30721;&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;Ben Or&#65292;Goldwasser&#65292;Wigderson&#31639;&#27861;&#65288;&#8220;BGW&#31639;&#27861;&#8221;&#65289;&#22312;&#20998;&#24067;&#24335;&#35745;&#31639;&#20013;&#23454;&#29616;&#23436;&#32654;&#30340;&#20449;&#24687;&#29702;&#35770;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#23436;&#32654;&#30340;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#38656;&#35201;&#19968;&#20010;&#35802;&#23454;&#30340;&#22810;&#25968;&#65292;&#21363;&#38656;&#35201;$N \geq 2t+1$&#20010;&#35745;&#31639;&#33410;&#28857;&#20197;&#30830;&#20445;&#23545;&#25239;&#24615;&#33410;&#28857;&#30340;&#38544;&#31169;&#12290;&#25105;&#20204;&#36890;&#36807;&#20801;&#35768;&#19968;&#23450;&#37327;&#30340;&#20449;&#24687;&#27844;&#28431;&#21644;&#36817;&#20284;&#20056;&#27861;&#26469;&#30740;&#31350;&#22312;&#35802;&#23454;&#33410;&#28857;&#25968;&#37327;&#20026;&#23569;&#25968;&#26102;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#21363;$N&lt; 2t+1$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#32780;&#19981;&#26159;&#23436;&#32654;&#38544;&#31169;&#26469;&#27979;&#37327;&#20449;&#24687;&#27844;&#28431;&#65292;&#24182;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#24230;&#37327;&#20934;&#30830;&#24615;&#65292;&#23545;$N &lt; 2t+1$&#30340;&#24773;&#20917;&#19979;&#30340;&#38544;&#31169;-&#20934;&#30830;&#24615;&#26435;&#34913;&#36827;&#34892;&#20102;&#32039;&#23494;&#30340;&#21051;&#30011;&#12290;&#19968;&#20010;&#26032;&#39062;&#30340;&#25216;&#26415;&#26041;&#38754;&#26159;&#22797;&#26434;&#22320;&#25511;&#21046;&#20449;&#24687;&#27844;&#28431;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of private distributed multi-party multiplication. It is well-established that Shamir secret-sharing coding strategies can enable perfect information-theoretic privacy in distributed computation via the celebrated algorithm of Ben Or, Goldwasser and Wigderson (the "BGW algorithm"). However, perfect privacy and accuracy require an honest majority, that is, $N \geq 2t+1$ compute nodes are required to ensure privacy against any $t$ colluding adversarial nodes. By allowing for some controlled amount of information leakage and approximate multiplication instead of exact multiplication, we study coding schemes for the setting where the number of honest nodes can be a minority, that is $N&lt; 2t+1.$ We develop a tight characterization privacy-accuracy trade-off for cases where $N &lt; 2t+1$ by measuring information leakage using {differential} privacy instead of perfect privacy, and using the mean squared error metric for accuracy. A novel technical aspect is an intricately 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.16096</link><description>&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#21487;&#20197;&#36991;&#20813;&#30340;&#65306;&#25968;&#25454;&#38598;&#20013;&#24615;&#22312;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness. (arXiv:2309.16096v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#35777;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#38598;&#20013;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#23384;&#22312;&#19982;&#21542;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#33719;&#24471;&#20855;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#23545;&#20110;&#23545;&#25239;&#26679;&#26412;&#30340;&#25935;&#24863;&#24615;&#24341;&#21457;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#26263;&#31034;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#21487;&#33021;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32467;&#26524;&#21487;&#33021;&#36807;&#20110;&#19968;&#33324;&#21270;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#25968;&#25454;&#20998;&#24067;&#12290;&#20107;&#23454;&#19978;&#65292;&#20154;&#31867;&#22312;&#28041;&#21450;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#30456;&#24403;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#31181;&#26126;&#26174;&#30340;&#30683;&#30462;&#25512;&#21160;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#25506;&#32034;&#19968;&#20010;&#38382;&#39064;&#65306;&#23545;&#25239;&#26679;&#26412;&#26159;&#21542;&#30495;&#30340;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#20010;&#20851;&#38190;&#23646;&#24615;&#8212;&#8212;&#23545;&#36755;&#20837;&#31354;&#38388;&#30340;&#23567;&#23481;&#31215;&#23376;&#38598;&#30340;&#38598;&#20013;&#31243;&#24230;&#65292;&#20915;&#23450;&#20102;&#26159;&#21542;&#23384;&#22312;&#19968;&#20010;&#40065;&#26834;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#22312;&#25968;&#25454;&#20998;&#24067;&#38598;&#20013;&#22312;&#20302;&#32500;&#32447;&#24615;&#23376;&#31354;&#38388;&#30340;&#24182;&#38598;&#26102;&#65292;&#21033;&#29992;&#25968;&#25454;&#32467;&#26500;&#33258;&#28982;&#22320;&#24471;&#21040;&#20139;&#26377;&#33391;&#22909;&#40065;&#26834;&#24615;&#20445;&#35777;&#30340;&#20998;&#31867;&#22120;&#65292;&#25913;&#36827;&#20102;&#22312;&#29305;&#23450;&#33539;&#22260;&#20869;&#21487;&#35777;&#26126;&#35748;&#35777;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The susceptibility of modern machine learning classifiers to adversarial examples has motivated theoretical results suggesting that these might be unavoidable. However, these results can be too general to be applicable to natural data distributions. Indeed, humans are quite robust for tasks involving vision. This apparent conflict motivates a deeper dive into the question: Are adversarial examples truly unavoidable? In this work, we theoretically demonstrate that a key property of the data distribution -- concentration on small-volume subsets of the input space -- determines whether a robust classifier exists. We further demonstrate that, for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees, improving upon methods for provable certification in certain regimes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;Koopman&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#32534;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26469;&#21516;&#26102;&#23398;&#20064;Koopman&#28508;&#22312;&#23884;&#20837;&#12289;&#31639;&#23376;&#21644;&#30456;&#20851;&#32447;&#24615;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20219;&#21153;&#25104;&#26412;&#20316;&#20026;&#20027;&#35201;&#30446;&#26631;&#65292;&#20943;&#23569;&#20102;&#23545;&#20110;&#26126;&#30830;&#23450;&#20041;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#20381;&#36182;&#65292;&#23558;Koopman&#25511;&#21046;&#25193;&#23637;&#21040;&#39640;&#32500;&#12289;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#20687;&#32032;&#30340;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2309.16077</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;Koopman&#25511;&#21046;&#21644;&#23545;&#27604;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Koopman-Based Control with Contrastive Encoder. (arXiv:2309.16077v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;Koopman&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#23545;&#27604;&#32534;&#30721;&#22120;&#21644;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#26469;&#21516;&#26102;&#23398;&#20064;Koopman&#28508;&#22312;&#23884;&#20837;&#12289;&#31639;&#23376;&#21644;&#30456;&#20851;&#32447;&#24615;&#25511;&#21046;&#22120;&#12290;&#36890;&#36807;&#20248;&#20808;&#32771;&#34385;&#20219;&#21153;&#25104;&#26412;&#20316;&#20026;&#20027;&#35201;&#30446;&#26631;&#65292;&#20943;&#23569;&#20102;&#23545;&#20110;&#26126;&#30830;&#23450;&#20041;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#20381;&#36182;&#65292;&#23558;Koopman&#25511;&#21046;&#25193;&#23637;&#21040;&#39640;&#32500;&#12289;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#21253;&#25324;&#22522;&#20110;&#20687;&#32032;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31471;&#21040;&#31471;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#27604;&#32534;&#30721;&#22120;&#21516;&#26102;&#23398;&#20064;Koopman&#28508;&#22312;&#23884;&#20837;&#65292;&#31639;&#23376;&#21644;&#30456;&#20851;&#32447;&#24615;&#25511;&#21046;&#22120;&#30340;&#20219;&#21153;&#23548;&#21521;Koopman&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#25104;&#26412;&#20316;&#20026;&#20027;&#35201;&#30446;&#26631;&#36827;&#34892;&#25511;&#21046;&#22120;&#23398;&#20064;&#65292;&#25105;&#20204;&#20943;&#23569;&#20102;&#23545;&#20110;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#35774;&#35745;&#30340;&#20381;&#36182;&#65292;&#23558;Koopman&#25511;&#21046;&#25193;&#23637;&#21040;&#21253;&#25324;&#22522;&#20110;&#20687;&#32032;&#30340;&#22330;&#26223;&#22312;&#20869;&#30340;&#39640;&#32500;&#12289;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present task-oriented Koopman-based control that utilizes end-to-end reinforcement learning and contrastive encoder to simultaneously learn the Koopman latent embedding, operator and associated linear controller within an iterative loop. By prioritizing the task cost as main objective for controller learning, we reduce the reliance of controller design on a well-identified model, which extends Koopman control beyond low-dimensional systems to high-dimensional, complex nonlinear systems, including pixel-based scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#22320;&#24418;&#19978;&#30340;&#21452;&#36275;&#34892;&#36208;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#23398;&#20064;&#21040;&#30340;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#25512;&#29702;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#21452;&#36275;&#34892;&#36208;&#31574;&#30053;&#30340;&#34892;&#36208;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16074</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#20174;&#31034;&#33539;&#20013;&#25512;&#29702;&#21644;&#35843;&#25972;&#65306;&#21452;&#36275;&#21160;&#20316;&#22870;&#21169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Infer and Adapt: Bipedal Locomotion Reward Learning from Demonstrations via Inverse Reinforcement Learning. (arXiv:2309.16074v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#22320;&#24418;&#19978;&#30340;&#21452;&#36275;&#34892;&#36208;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#23398;&#20064;&#21040;&#30340;&#20989;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#25512;&#29702;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#21487;&#20197;&#25552;&#39640;&#21452;&#36275;&#34892;&#36208;&#31574;&#30053;&#30340;&#34892;&#36208;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#21452;&#36275;&#34892;&#36208;&#26426;&#22120;&#20154;&#23398;&#20064;&#22914;&#20309;&#22312;&#39640;&#24230;&#19981;&#24179;&#22374;&#12289;&#21160;&#24577;&#21464;&#21270;&#30340;&#22320;&#24418;&#19978;&#34892;&#36827;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#26159;&#30001;&#20110;&#26426;&#22120;&#20154;&#21160;&#21147;&#23398;&#21644;&#30456;&#20114;&#20316;&#29992;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#31034;&#33539;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#33391;&#22909;&#32467;&#26524;&#12290;&#34429;&#28982;&#27169;&#20223;&#23398;&#20064;&#19987;&#23478;&#31574;&#30053;&#30340;&#30740;&#31350;&#24050;&#32463;&#24471;&#21040;&#20102;&#24456;&#22909;&#30340;&#25506;&#32034;&#65292;&#20294;&#22312;&#33151;&#37096;&#36816;&#21160;&#20013;&#23398;&#20064;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#30340;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#23558;&#20808;&#36827;&#30340;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(IRL)&#25216;&#26415;&#24341;&#20837;&#35299;&#20915;&#21452;&#36275;&#21160;&#20316;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#19987;&#23478;&#22870;&#21169;&#20989;&#25968;&#30340;&#31639;&#27861;&#65292;&#24182;&#23545;&#23398;&#20064;&#21040;&#30340;&#20989;&#25968;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19987;&#23478;&#30340;&#21160;&#20316;&#31574;&#30053;&#20013;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25512;&#29702;&#20986;&#30340;&#22870;&#21169;&#20989;&#25968;&#35757;&#32451;&#21452;&#36275;&#34892;&#36208;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#20854;&#34892;&#36208;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabling bipedal walking robots to learn how to maneuver over highly uneven, dynamically changing terrains is challenging due to the complexity of robot dynamics and interacted environments. Recent advancements in learning from demonstrations have shown promising results for robot learning in complex environments. While imitation learning of expert policies has been well-explored, the study of learning expert reward functions is largely under-explored in legged locomotion. This paper brings state-of-the-art Inverse Reinforcement Learning (IRL) techniques to solving bipedal locomotion problems over complex terrains. We propose algorithms for learning expert reward functions, and we subsequently analyze the learned functions. Through nonlinear function approximation, we uncover meaningful insights into the expert's locomotion strategies. Furthermore, we empirically demonstrate that training a bipedal locomotion policy with the inferred reward functions enhances its walking performance on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#26631;&#35760;&#28857;&#26816;&#27979;&#20219;&#21153;&#30340;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20165;&#26631;&#31614;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#25552;&#39640;&#26631;&#35760;&#28857;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16066</link><description>&lt;p&gt;
&#21307;&#23398;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#26631;&#35760;&#28857;&#26816;&#27979;&#30340;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label Augmentation Method for Medical Landmark Detection in Hip Radiograph Images. (arXiv:2309.16066v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21307;&#23398;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#26631;&#35760;&#28857;&#26816;&#27979;&#20219;&#21153;&#30340;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#20165;&#26631;&#31614;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26041;&#27861;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22312;&#26679;&#26412;&#21033;&#29992;&#25928;&#29575;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#25552;&#39640;&#26631;&#35760;&#28857;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25253;&#36947;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#39627;&#20851;&#33410;X&#23556;&#32447;&#22270;&#20687;&#20013;&#20020;&#24202;&#26631;&#35760;&#28857;&#30340;&#33258;&#21160;&#21270;&#21307;&#23398;&#26631;&#35760;&#28857;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#26816;&#27979;&#26041;&#27861;&#26159;&#20351;&#29992;&#20165;&#26631;&#31614;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#30340;&#65307;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#22686;&#24378;&#24418;&#24335;&#20248;&#20110;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#19988;&#20135;&#29983;&#39640;&#25928;&#30340;&#26679;&#26412;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#36890;&#29992;U-Net&#26550;&#26500;&#30340;&#35838;&#31243;&#35757;&#32451;&#65292;&#35813;&#35757;&#32451;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39318;&#20808;&#36890;&#36807;&#23558;&#26631;&#35760;&#28857;&#25193;&#22823;&#21040;&#21306;&#22495;&#26469;&#25918;&#26494;&#26631;&#35760;&#20219;&#21153;&#65292;&#28982;&#21518;&#36880;&#28176;&#23558;&#36825;&#20123;&#26631;&#31614;&#21306;&#22495;&#22238;&#24402;&#21040;&#22522;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21547;&#26377;&#40644;&#37329;&#26631;&#20934;&#19987;&#23478;&#27880;&#37322;&#30340;&#20845;&#20010;&#25918;&#23556;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work reports the empirical performance of an automated medical landmark detection method for predict clinical markers in hip radiograph images. Notably, the detection method was trained using a label-only augmentation scheme; our results indicate that this form of augmentation outperforms traditional data augmentation and produces highly sample efficient estimators. We train a generic U-Net-based architecture under a curriculum consisting of two phases: initially relaxing the landmarking task by enlarging the label points to regions, then gradually eroding these label regions back to the base task. We measure the benefits of this approach on six datasets of radiographs with gold-standard expert annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.16064</link><description>&lt;p&gt;
&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#23398;&#20064;&#32454;&#32990;&#24418;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;
Masked autoencoders are scalable learners of cellular morphology. (arXiv:2309.16064v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#25512;&#26029;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#20869;&#23481;&#26174;&#24494;&#38236;&#26816;&#26597;&#20013;&#20174;&#32454;&#32990;&#34920;&#22411;&#20013;&#25512;&#26029;&#29983;&#29289;&#20851;&#31995;&#22312;&#29983;&#29289;&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#27604;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#26356;&#33021;&#25429;&#25417;&#29983;&#29289;&#20449;&#21495;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24369;&#30417;&#30563;&#21644;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35757;&#32451;&#26356;&#22823;&#30340;&#27169;&#22411;&#21644;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#26102;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;CNN&#21644;ViT&#30340;&#21463;&#23631;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#24369;&#30417;&#30563;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#26368;&#39640;&#23610;&#24230;&#19978;&#65292;&#19968;&#20010;&#22312;&#20844;&#20849;&#25968;&#25454;&#24211;&#20013;&#26500;&#24314;&#30340;&#32454;&#32990;&#24418;&#24577;&#23398;&#20851;&#31995;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#35206;&#30422;&#36229;&#36807;35&#20159;&#20010;&#21807;&#19968;&#21098;&#35009;&#22270;&#20687;&#30340;ViT-L/8&#27169;&#22411;&#65292;&#22312;&#25512;&#26029;&#24050;&#30693;&#29983;&#29289;&#20851;&#31995;&#26102;&#30456;&#23545;&#25913;&#36827;&#39640;&#36798;28%&#12290;
&lt;/p&gt;
&lt;p&gt;
Inferring biological relationships from cellular phenotypes in high-content microscopy screens provides significant opportunity and challenge in biological research. Prior results have shown that deep vision models can capture biological signal better than hand-crafted features. This work explores how weakly supervised and self-supervised deep learning approaches scale when training larger models on larger datasets. Our results show that both CNN- and ViT-based masked autoencoders significantly outperform weakly supervised models. At the high-end of our scale, a ViT-L/8 trained on over 3.5-billion unique crops sampled from 95-million microscopy images achieves relative improvements as high as 28% over our best weakly supervised models at inferring known biological relationships curated from public databases.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;COVID-19&#21518;&#24739;&#32773;&#30340;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#65292;&#20026;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2309.16059</link><description>&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;COVID-19&#21518;&#24739;&#32773;&#30340;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;
&lt;/p&gt;
&lt;p&gt;
Predicting Cardiovascular Complications in Post-COVID-19 Patients Using Data-Driven Machine Learning Models. (arXiv:2309.16059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16059
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25104;&#21151;&#39044;&#27979;COVID-19&#21518;&#24739;&#32773;&#30340;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#65292;&#20026;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#24739;&#32773;&#32467;&#26524;&#25552;&#20379;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#27969;&#34892;&#30149;&#20840;&#29699;&#24615;&#22320;&#24102;&#26469;&#20102;&#35768;&#22810;&#20581;&#24247;&#25361;&#25112;&#65292;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;COVID-19&#21518;&#20986;&#29616;&#30340;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#39044;&#27979;&#20234;&#25289;&#20811;352&#21517;COVID-19&#21518;&#24739;&#32773;&#30340;&#36825;&#20123;&#24182;&#21457;&#30151;&#12290;&#25910;&#38598;&#20102;&#21253;&#25324;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#12289;&#20849;&#30149;&#30151;&#12289;&#23454;&#39564;&#23460;&#32467;&#26524;&#21644;&#24433;&#20687;&#23398;&#22312;&#20869;&#30340;&#20020;&#24202;&#25968;&#25454;&#65292;&#24182;&#29992;&#20110;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#35782;&#21035;&#22788;&#20110;&#39118;&#38505;&#20013;&#30340;&#24739;&#32773;&#26041;&#38754;&#21487;&#35266;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20123;&#27169;&#22411;&#30340;&#26089;&#26399;&#26816;&#27979;&#65292;&#21487;&#20197;&#23454;&#29616;&#21450;&#26102;&#24178;&#39044;&#21644;&#25913;&#21892;&#32467;&#26524;&#12290;&#24635;&#20043;&#65292;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#39044;&#27979;COVID-19&#21518;&#24515;&#34880;&#31649;&#24182;&#21457;&#30151;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#26377;&#24517;&#35201;&#22312;&#19981;&#21516;&#30340;&#20020;&#24202;&#29615;&#22659;&#20013;&#32487;&#32493;&#39564;&#35777;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has globally posed numerous health challenges, notably the emergence of post-COVID-19 cardiovascular complications. This study addresses this by utilizing data-driven machine learning models to predict such complications in 352 post-COVID-19 patients from Iraq. Clinical data, including demographics, comorbidities, lab results, and imaging, were collected and used to construct predictive models. These models, leveraging various machine learning algorithms, demonstrated commendable performance in identifying patients at risk. Early detection through these models promises timely interventions and improved outcomes. In conclusion, this research underscores the potential of data-driven machine learning for predicting post-COVID-19 cardiovascular complications, emphasizing the need for continued validation and research in diverse clinical settings.
&lt;/p&gt;</description></item><item><title>AnyMAL&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20219;&#24847;&#27169;&#24577;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16058</link><description>&lt;p&gt;
AnyMAL:&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20219;&#24847;&#27169;&#24577;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. (arXiv:2309.16058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16058
&lt;/p&gt;
&lt;p&gt;
AnyMAL&#26159;&#19968;&#31181;&#39640;&#25928;&#21487;&#25193;&#23637;&#30340;&#20219;&#24847;&#27169;&#24577;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Any-Modality Augmented Language Model (AnyMAL)&#65292;&#23427;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27169;&#24577;&#20449;&#21495;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#12289;IMU&#36816;&#21160;&#20256;&#24863;&#22120;&#65289;&#65292;&#24182;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#12290;AnyMAL&#32487;&#25215;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#21253;&#25324;LLaMA-2&#65288;70B&#65289;&#65289;&#30340;&#24378;&#22823;&#25991;&#26412;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#23545;&#40784;&#27169;&#22359;&#23558;&#27169;&#24577;&#29305;&#23450;&#20449;&#21495;&#36716;&#25442;&#20026;&#32852;&#21512;&#25991;&#26412;&#31354;&#38388;&#12290;&#20026;&#36827;&#19968;&#27493;&#22686;&#24378;&#22810;&#27169;&#24577;LLM&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#21160;&#25910;&#38598;&#30340;&#22810;&#27169;&#24577;&#25351;&#20196;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#28085;&#30422;&#31616;&#21333;&#38382;&#31572;&#20197;&#22806;&#30340;&#21508;&#31181;&#20027;&#39064;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Any-Modality Augmented Language Model (AnyMAL), a unified model that reasons over diverse input modality signals (i.e. text, image, video, audio, IMU motion sensor), and generates textual responses. AnyMAL inherits the powerful text-based reasoning abilities of the state-of-the-art LLMs including LLaMA-2 (70B), and converts modality-specific signals to the joint textual space through a pre-trained aligner module. To further strengthen the multimodal LLM's capabilities, we fine-tune the model with a multimodal instruction set manually collected to cover diverse topics and tasks beyond simple QAs. We conduct comprehensive empirical analysis comprising both human and automatic evaluations, and demonstrate state-of-the-art performance on various multimodal tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#37492;&#21035;&#20102;COVID-19&#21518;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#21457;&#29616;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#23621;&#20303;&#22320;&#22320;&#29702;&#21306;&#22495;&#12289;&#21512;&#24182;&#30151;&#12289;COVID-19&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#20197;&#21450;&#24515;&#29702;&#31038;&#20250;&#22240;&#32032;&#26159;&#21457;&#23637;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#24212;&#32771;&#34385;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#26469;&#35774;&#35745;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#21644;&#25903;&#25345;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16055</link><description>&lt;p&gt;
&#37492;&#21035;COVID-19&#21518;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#39118;&#38505;&#22240;&#32032;&#65306;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Identifying Risk Factors for Post-COVID-19 Mental Health Disorders: A Machine Learning Perspective. (arXiv:2309.16055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#37492;&#21035;&#20102;COVID-19&#21518;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#39118;&#38505;&#22240;&#32032;&#65292;&#21457;&#29616;&#24180;&#40836;&#12289;&#24615;&#21035;&#12289;&#23621;&#20303;&#22320;&#22320;&#29702;&#21306;&#22495;&#12289;&#21512;&#24182;&#30151;&#12289;COVID-19&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#20197;&#21450;&#24515;&#29702;&#31038;&#20250;&#22240;&#32032;&#26159;&#21457;&#23637;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#24212;&#32771;&#34385;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#26469;&#35774;&#35745;&#26377;&#38024;&#23545;&#24615;&#30340;&#24178;&#39044;&#25514;&#26045;&#21644;&#25903;&#25345;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#35782;&#21035;&#20102;&#19982;COVID-19&#21518;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#30456;&#20851;&#30340;&#39118;&#38505;&#22240;&#32032;&#12290;&#36890;&#36807;&#23545;&#20234;&#25289;&#20811;&#21508;&#30465;669&#21517;&#24739;&#32773;&#30340;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#23621;&#20303;&#22320;&#22320;&#29702;&#21306;&#22495;&#26159;&#24433;&#21709;COVID-19&#21518;&#24739;&#32773;&#21457;&#23637;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#21487;&#33021;&#24615;&#30340;&#37325;&#35201;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#21512;&#24182;&#30151;&#21644;COVID-19&#30142;&#30149;&#30340;&#20005;&#37325;&#31243;&#24230;&#20063;&#26159;&#37325;&#35201;&#30340;&#20020;&#24202;&#39044;&#27979;&#22240;&#23376;&#12290;&#24515;&#29702;&#31038;&#20250;&#22240;&#32032;&#65292;&#22914;&#31038;&#20250;&#25903;&#25345;&#12289;&#24212;&#23545;&#31574;&#30053;&#21644;&#20027;&#35266;&#21387;&#21147;&#27700;&#24179;&#65292;&#20063;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;COVID-19&#24247;&#22797;&#21518;&#31934;&#31070;&#20581;&#24247;&#38556;&#30861;&#21457;&#23637;&#20013;&#22810;&#20010;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#12290;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#22312;&#21046;&#23450;&#38024;&#23545;&#24739;&#32773;&#30340;&#26377;&#38024;&#23545;&#24615;&#24178;&#39044;&#21644;&#25903;&#25345;&#31995;&#32479;&#26102;&#24212;&#32771;&#34385;&#36825;&#20123;&#39118;&#38505;&#22240;&#32032;&#12290;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In this study, we leveraged machine learning techniques to identify risk factors associated with post-COVID-19 mental health disorders. Our analysis, based on data collected from 669 patients across various provinces in Iraq, yielded valuable insights. We found that age, gender, and geographical region of residence were significant demographic factors influencing the likelihood of developing mental health disorders in post-COVID-19 patients. Additionally, comorbidities and the severity of COVID-19 illness were important clinical predictors. Psychosocial factors, such as social support, coping strategies, and perceived stress levels, also played a substantial role. Our findings emphasize the complex interplay of multiple factors in the development of mental health disorders following COVID-19 recovery. Healthcare providers and policymakers should consider these risk factors when designing targeted interventions and support systems for individuals at risk. Machine learning-based approach
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#23558;&#26799;&#24230;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#20174;&#27425;&#20248;&#30340;$O(\sqrt{V_T\log V_T})$&#25913;&#36827;&#21040;&#26368;&#20248;&#36895;&#29575;$O(\sqrt{V_T})$&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#26410;&#30693;Lipschitz&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.16044</link><description>&lt;p&gt;
&#25913;&#36827;&#30340;&#31934;&#32454;&#31163;&#25955;&#21270;&#26041;&#27861;&#25552;&#39640;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Adaptive Online Learning Using Refined Discretization. (arXiv:2309.16044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16044
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#23558;&#26799;&#24230;&#26041;&#24046;&#30340;&#20381;&#36182;&#24615;&#20174;&#27425;&#20248;&#30340;$O(\sqrt{V_T\log V_T})$&#25913;&#36827;&#21040;&#26368;&#20248;&#36895;&#29575;$O(\sqrt{V_T})$&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#26410;&#30693;Lipschitz&#24120;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;Lipschitz&#25439;&#22833;&#30340;&#38750;&#32422;&#26463;&#22312;&#32447;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#21516;&#26102;&#36798;&#21040;&#65288;i&#65289;&#20108;&#38454;&#26799;&#24230;&#33258;&#36866;&#24212;&#24615;&#65307;&#21644;&#65288;ii&#65289;&#27604;&#36739;&#22120;&#33539;&#25968;&#33258;&#36866;&#24212;&#24615;&#65292;&#20063;&#34987;&#31216;&#20026;&#25991;&#29486;&#20013;&#30340;&#8220;&#21442;&#25968;&#33258;&#30001;&#24615;&#8221;&#12290;&#29616;&#26377;&#30340;&#36951;&#25022;&#30028;&#65288;Cutkosky&#21644;Orabona&#65292;2018&#65307;Mhammedi&#21644;Koolen&#65292;2020&#65307;Jacobsen&#21644;Cutkosky&#65292;2022&#65289;&#23545;&#20110;&#26799;&#24230;&#26041;&#24046;$V_T$&#26377;&#27425;&#20248;&#30340;$O(\sqrt{V_T\log V_T})$&#20381;&#36182;&#24615;&#65292;&#32780;&#26412;&#24037;&#20316;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#36830;&#32493;&#26102;&#38388;&#21551;&#21457;&#24335;&#31639;&#27861;&#23558;&#20854;&#25913;&#36827;&#20026;&#26368;&#20248;&#36895;&#29575;$O(\sqrt{V_T})$&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#19981;&#20999;&#23454;&#38469;&#30340;&#21152;&#20493;&#25216;&#24039;&#12290;&#36825;&#19968;&#32467;&#26524;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;Lipschitz&#24120;&#25968;&#30340;&#24773;&#20917;&#65292;&#28040;&#38500;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#33539;&#22260;&#27604;&#29575;&#38382;&#39064;&#65288;Mhammedi&#21644;Koolen&#65292;2020&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#38382;&#39064;&#30340;&#36830;&#32493;&#26102;&#38388;&#31867;&#27604;&#20013;&#21487;&#20197;&#30456;&#24403;&#23481;&#26131;&#22320;&#23454;&#29616;&#30446;&#26631;&#30340;&#21516;&#26102;&#36866;&#24212;&#24615;&#65292;&#20854;&#20013;&#29615;&#22659;&#30001;&#20219;&#24847;&#36830;&#32493;&#21322;&#38808;&#24335;&#24314;&#27169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
We study unconstrained Online Linear Optimization with Lipschitz losses. The goal is to simultaneously achieve ($i$) second order gradient adaptivity; and ($ii$) comparator norm adaptivity also known as "parameter freeness" in the literature. Existing regret bounds (Cutkosky and Orabona, 2018; Mhammedi and Koolen, 2020; Jacobsen and Cutkosky, 2022) have the suboptimal $O(\sqrt{V_T\log V_T})$ dependence on the gradient variance $V_T$, while the present work improves it to the optimal rate $O(\sqrt{V_T})$ using a novel continuous-time-inspired algorithm, without any impractical doubling trick. This result can be extended to the setting with unknown Lipschitz constant, eliminating the range ratio problem from prior works (Mhammedi and Koolen, 2020).  Concretely, we first show that the aimed simultaneous adaptivity can be achieved fairly easily in a continuous time analogue of the problem, where the environment is modeled by an arbitrary continuous semimartingale. Then, our key innovation 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16042</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#27169;&#22411;&#20013;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#65306;&#24230;&#37327;&#21644;&#26041;&#27861;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#35299;&#37322;&#24615;&#26088;&#22312;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20854;&#20013;&#23450;&#20301;-&#35782;&#21035;&#37325;&#35201;&#30340;&#27169;&#22411;&#32452;&#20214;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#65292;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#25110;&#20132;&#25442;&#24178;&#39044;&#65292;&#26159;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20294;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#65292;&#23545;&#36229;&#21442;&#25968;&#25110;&#26041;&#27861;&#36873;&#25321;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22351;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#21644;&#30005;&#36335;&#21457;&#29616;&#30340;&#20960;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#25903;&#25345;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#25351;&#26631;&#25110;&#26041;&#27861;&#21487;&#33021;&#26356;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#24615;&#35770;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#20998;&#26512;&#24314;&#27169;&#65292;&#20998;&#26512;&#20102;&#32435;&#31859;&#35774;&#22791;&#30340;&#36890;&#20449;&#21644;&#33021;&#28304;&#32422;&#26463;&#23545;&#23450;&#20301;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.16034</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#20998;&#26512;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale Localization. (arXiv:2309.16034v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21407;&#22987;&#25968;&#25454;&#30340;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#20998;&#26512;&#24314;&#27169;&#65292;&#20998;&#26512;&#20102;&#32435;&#31859;&#35774;&#22791;&#30340;&#36890;&#20449;&#21644;&#33021;&#28304;&#32422;&#26463;&#23545;&#23450;&#20301;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32435;&#31859;&#25216;&#26415;&#21644;&#26448;&#26009;&#31185;&#23398;&#30340;&#36827;&#23637;&#20026;&#32435;&#31859;&#23610;&#24230;&#35774;&#22791;&#30340;&#21457;&#23637;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#36825;&#20123;&#35774;&#22791;&#32467;&#21512;&#20102;&#20256;&#24863;&#12289;&#35745;&#31639;&#12289;&#25968;&#25454;&#21644;&#33021;&#28304;&#20648;&#23384;&#20197;&#21450;&#26080;&#32447;&#36890;&#20449;&#12290;&#22312;&#31934;&#23494;&#21307;&#23398;&#20013;&#65292;&#36825;&#20123;&#32435;&#31859;&#35774;&#22791;&#23545;&#20110;&#30142;&#30149;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#30417;&#27979;&#21576;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#32780;&#20307;&#20869;&#32435;&#31859;&#23610;&#24230;&#23450;&#20301;&#30340;&#27969;&#24341;&#23548;&#23450;&#20301;&#65292;&#21363;&#23558;&#25152;&#24863;&#30693;&#30340;&#29983;&#29289;&#20107;&#20214;&#19982;&#20107;&#20214;&#26412;&#36523;&#30340;&#20301;&#32622;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#31934;&#23494;&#21307;&#23398;&#30340;&#35282;&#24230;&#30475;&#23558;&#20855;&#26377;&#26497;&#22823;&#30340;&#30410;&#22788;&#12290;&#32435;&#31859;&#35774;&#22791;&#30340;&#32435;&#31859;&#23610;&#24230;&#29305;&#24615;&#20197;&#21450;&#34880;&#28082;&#27969;&#21160;&#29615;&#22659;&#30340;&#25361;&#25112;&#24615;&#23548;&#33268;&#30446;&#21069;&#30340;&#27969;&#24341;&#23548;&#23450;&#20301;&#26041;&#27861;&#22312;&#36890;&#20449;&#21644;&#33021;&#28304;&#30456;&#20851;&#33021;&#21147;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#12290;&#32435;&#31859;&#35774;&#22791;&#30340;&#36890;&#20449;&#21644;&#33021;&#28304;&#32422;&#26463;&#23548;&#33268;&#27969;&#24341;&#23548;&#23450;&#20301;&#30340;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#24314;&#27169;&#30740;&#31350;&#20102;&#36825;&#20123;&#19981;&#23436;&#32654;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in nanotechnology and material science are paving the way toward nanoscale devices that combine sensing, computing, data and energy storage, and wireless communication. In precision medicine, these nanodevices show promise for disease diagnostics, treatment, and monitoring from within the patients' bloodstreams. Assigning the location of a sensed biological event with the event itself, which is the main proposition of flow-guided in-body nanoscale localization, would be immensely beneficial from the perspective of precision medicine. The nanoscale nature of the nanodevices and the challenging environment that the bloodstream represents, result in current flow-guided localization approaches being constrained in their communication and energy-related capabilities. The communication and energy constraints of the nanodevices result in different features of raw data for flow-guided localization, in turn affecting its performance. An analytical modeling of the effects of imperfe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#23548;&#20986;&#26465;&#20214;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#32791;&#25955;&#24615;&#36136;&#65292;&#24182;&#22312;&#20445;&#25345;&#36924;&#36817;&#33021;&#21147;&#30340;&#21516;&#26102;&#25200;&#21160;&#20559;&#24046;&#12290;&#36890;&#36807;&#29420;&#31435;&#27714;&#35299;&#36825;&#20004;&#20010;&#25200;&#21160;&#38382;&#39064;&#65292;&#24471;&#21040;&#19968;&#20010;&#20445;&#35777;&#20026;&#32791;&#25955;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#21516;&#26102;&#32039;&#23494;&#36924;&#36817;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.16032</link><description>&lt;p&gt;
&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Learning Dissipative Neural Dynamical Systems. (arXiv:2309.16032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#39318;&#20808;&#23398;&#20064;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#27169;&#22411;&#65292;&#28982;&#21518;&#23548;&#20986;&#26465;&#20214;&#26469;&#20445;&#35777;&#27169;&#22411;&#30340;&#32791;&#25955;&#24615;&#36136;&#65292;&#24182;&#22312;&#20445;&#25345;&#36924;&#36817;&#33021;&#21147;&#30340;&#21516;&#26102;&#25200;&#21160;&#20559;&#24046;&#12290;&#36890;&#36807;&#29420;&#31435;&#27714;&#35299;&#36825;&#20004;&#20010;&#25200;&#21160;&#38382;&#39064;&#65292;&#24471;&#21040;&#19968;&#20010;&#20445;&#35777;&#20026;&#32791;&#25955;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#21516;&#26102;&#32039;&#23494;&#36924;&#36817;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#19968;&#20010;&#26410;&#30693;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#31995;&#32479;&#65292;&#20854;&#24050;&#30693;&#26159;&#32791;&#25955;&#30340;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#20197;&#36924;&#36817;&#36825;&#20010;&#31995;&#32479;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#20445;&#25345;&#32791;&#25955;&#24615;&#36136;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#26045;&#21152;&#32791;&#25955;&#24615;&#32422;&#26463;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#27809;&#26377;&#24050;&#30693;&#30340;&#25216;&#26415;&#21487;&#20197;&#35299;&#20915;&#12290;&#26412;&#25991;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#35299;&#20915;&#20102;&#23398;&#20064;&#32791;&#25955;&#31070;&#32463;&#21160;&#21147;&#31995;&#32479;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#26080;&#32422;&#26463;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#32039;&#23494;&#36924;&#36817;&#31995;&#32479;&#30340;&#21160;&#21147;&#23398;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23548;&#20986;&#36275;&#22815;&#30340;&#26465;&#20214;&#26469;&#25200;&#21160;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20197;&#30830;&#20445;&#32791;&#25955;&#24615;&#36136;&#65292;&#24182;&#22312;&#20445;&#25345;&#27169;&#22411;&#19982;&#38750;&#32447;&#24615;&#31995;&#32479;&#36712;&#36857;&#25311;&#21512;&#30340;&#21516;&#26102;&#25200;&#21160;&#20559;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#25200;&#21160;&#38382;&#39064;&#21487;&#20197;&#29420;&#31435;&#27714;&#35299;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#20010;&#20445;&#35777;&#20026;&#32791;&#25955;&#30340;&#31070;&#32463;&#21160;&#21147;&#27169;&#22411;&#65292;&#21516;&#26102;&#32039;&#23494;&#36924;&#36817;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider an unknown nonlinear dynamical system that is known to be dissipative. The objective of this paper is to learn a neural dynamical model that approximates this system, while preserving the dissipativity property in the model. In general, imposing dissipativity constraints during neural network training is a hard problem for which no known techniques exist. In this work, we address the problem of learning a dissipative neural dynamical system model in two stages. First, we learn an unconstrained neural dynamical model that closely approximates the system dynamics. Next, we derive sufficient conditions to perturb the weights of the neural dynamical model to ensure dissipativity, followed by perturbation of the biases to retain the fit of the model to the trajectories of the nonlinear system. We show that these two perturbation problems can be solved independently to obtain a neural dynamical model that is guaranteed to be dissipative while closely approximating the nonlinear syst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16025</link><description>&lt;p&gt;
&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65306;&#20174;&#40657;&#30418;&#21040;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Symbolic Imitation Learning: From Black-Box to Explainable Driving Policies. (arXiv:2309.16025v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#26469;&#23398;&#20064;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25913;&#36827;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#20174;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#33719;&#21462;&#39550;&#39542;&#31574;&#30053;&#30340;&#26377;&#25928;&#25163;&#27573;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#32570;&#28857;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#23588;&#20026;&#20196;&#20154;&#25285;&#24551;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31526;&#21495;&#21270;&#27169;&#20223;&#23398;&#20064;&#65288;SIL&#65289;&#65292;&#19968;&#31181;&#20351;&#29992;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#23398;&#20064;&#20174;&#21487;&#29992;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#36879;&#26126;&#12289;&#21487;&#35299;&#37322;&#21644;&#27867;&#21270;&#30340;&#39550;&#39542;&#31574;&#30053;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;highD&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#27604;&#36739;&#20998;&#26512;&#65292;&#19982;&#24403;&#21069;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SIL&#19981;&#20165;&#25552;&#39640;&#20102;&#39550;&#39542;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#31181;&#39550;&#39542;&#24773;&#20917;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#30340;&#39550;&#39542;&#31574;&#30053;&#25171;&#24320;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current methods of imitation learning (IL), primarily based on deep neural networks, offer efficient means for obtaining driving policies from real-world data but suffer from significant limitations in interpretability and generalizability. These shortcomings are particularly concerning in safety-critical applications like autonomous driving. In this paper, we address these limitations by introducing Symbolic Imitation Learning (SIL), a groundbreaking method that employs Inductive Logic Programming (ILP) to learn driving policies which are transparent, explainable and generalisable from available datasets. Utilizing the real-world highD dataset, we subject our method to a rigorous comparative analysis against prevailing neural-network-based IL methods. Our results demonstrate that SIL not only enhances the interpretability of driving policies but also significantly improves their applicability across varied driving situations. Hence, this work offers a novel pathway to more reliable an
&lt;/p&gt;</description></item><item><title>GNNHLS&#26159;&#19968;&#20010;&#36890;&#36807;&#39640;&#32423;&#32508;&#21512;&#35780;&#20272;FPGAs&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36895;&#21152;&#36895;&#21644;&#33021;&#37327;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.16022</link><description>&lt;p&gt;
GNNHLS: &#36890;&#36807;&#39640;&#32423;&#32508;&#21512;&#35780;&#20272;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
GNNHLS: Evaluating Graph Neural Network Inference via High-Level Synthesis. (arXiv:2309.16022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16022
&lt;/p&gt;
&lt;p&gt;
GNNHLS&#26159;&#19968;&#20010;&#36890;&#36807;&#39640;&#32423;&#32508;&#21512;&#35780;&#20272;FPGAs&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#39640;&#36895;&#21152;&#36895;&#21644;&#33021;&#37327;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#36880;&#28176;&#27969;&#34892;&#65292;&#39640;&#25928;&#30340;GNN&#25512;&#26029;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#30001;&#20110;&#20854;&#31934;&#32454;&#32423;&#24182;&#34892;&#24615;&#12289;&#20302;&#21151;&#32791;&#12289;&#21487;&#37325;&#26500;&#24615;&#21644;&#24182;&#21457;&#25191;&#34892;&#30340;&#29305;&#28857;&#65292;Field-Programming Gate Arrays (FPGAs) &#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25191;&#34892;&#24179;&#21488;&#12290;&#26356;&#22909;&#30340;&#26159;&#65292;&#39640;&#32423;&#32508;&#21512;&#65288;HLS&#65289;&#24037;&#20855;&#24357;&#34917;&#20102;&#38750;&#24120;&#35268;&#30340;FPGA&#24320;&#21457;&#24037;&#20316;&#21644;&#26032;GNN&#27169;&#22411;&#30340;&#24555;&#36895;&#28044;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GNNHLS&#65292;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#36890;&#36807;HLS&#20840;&#38754;&#35780;&#20272;FPGAs&#19978;&#30340;GNN&#25512;&#26029;&#21152;&#36895;&#65292;&#21253;&#25324;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21644;&#22522;&#20934;&#37096;&#32626;&#30340;&#36719;&#20214;&#26632;&#20197;&#21450;6&#20010;&#32463;&#36807;&#33391;&#22909;&#35843;&#20248;&#30340;GNN HLS&#20869;&#26680;&#30340;FPGA&#23454;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#20855;&#26377;&#19981;&#21516;&#25299;&#25169;&#32467;&#26500;&#21644;&#35268;&#27169;&#30340;&#22270;&#25968;&#25454;&#38598;&#23545;GNNHLS&#36827;&#34892;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;CPU&#22522;&#20934;&#30456;&#27604;&#65292;GNNHLS&#23454;&#29616;&#20102;&#39640;&#36798;50.8&#20493;&#30340;&#21152;&#36895;&#21644;423&#20493;&#30340;&#33021;&#37327;&#38477;&#20302;&#12290;&#19982;GPU&#22522;&#20934;&#30456;&#27604;&#65292;GNNHLS&#23454;&#29616;&#20102;&#39640;&#36798;5.16&#20493;&#30340;&#21152;&#36895;&#21644;74.5&#20493;&#30340;&#33021;&#37327;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the ever-growing popularity of Graph Neural Networks (GNNs), efficient GNN inference is gaining tremendous attention. Field-Programming Gate Arrays (FPGAs) are a promising execution platform due to their fine-grained parallelism, low-power consumption, reconfigurability, and concurrent execution. Even better, High-Level Synthesis (HLS) tools bridge the gap between the non-trivial FPGA development efforts and rapid emergence of new GNN models. In this paper, we propose GNNHLS, an open-source framework to comprehensively evaluate GNN inference acceleration on FPGAs via HLS, containing a software stack for data generation and baseline deployment, and FPGA implementations of 6 well-tuned GNN HLS kernels. We evaluate GNNHLS on 4 graph datasets with distinct topologies and scales. The results show that GNNHLS achieves up to 50.8x speedup and 423x energy reduction relative to the CPU baselines. Compared with the GPU baselines, GNNHLS achieves up to 5.16x speedup and 74.5x energy reductio
&lt;/p&gt;</description></item><item><title>GeoCLIP&#26159;&#19968;&#31181;&#21463;Clip&#21551;&#21457;&#30340;&#22270;&#20687;&#21040;GPS&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;&#12290;&#23427;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;&#21644;&#20854;&#23545;&#24212;&#30340;GPS&#20301;&#32622;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#22266;&#23450;&#20998;&#31867;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16020</link><description>&lt;p&gt;
GeoCLIP&#65306;&#21463;Clip&#21551;&#21457;&#30340;&#22320;&#28857;&#21644;&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#30340;&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16020
&lt;/p&gt;
&lt;p&gt;
GeoCLIP&#26159;&#19968;&#31181;&#21463;Clip&#21551;&#21457;&#30340;&#22270;&#20687;&#21040;GPS&#26816;&#32034;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;&#12290;&#23427;&#36890;&#36807;&#23545;&#40784;&#22270;&#20687;&#21644;&#20854;&#23545;&#24212;&#30340;GPS&#20301;&#32622;&#26469;&#25552;&#39640;&#23450;&#20301;&#31934;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#22266;&#23450;&#20998;&#31867;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#22320;&#29702;&#23450;&#20301;&#26088;&#22312;&#30830;&#23450;&#36965;&#24863;&#22270;&#20687;&#30340;&#31934;&#30830;&#20301;&#32622;&#12290;&#30001;&#20110;&#22320;&#29702;&#26223;&#35266;&#30340;&#24040;&#22823;&#21464;&#21270;&#65292;&#36825;&#39033;&#20219;&#21153;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290; &#22522;&#20110;&#22270;&#20687;&#26816;&#32034;&#30340;&#26041;&#27861;&#26080;&#27861;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#26500;&#24314;&#28085;&#30422;&#25972;&#20010;&#19990;&#30028;&#30340;&#22823;&#22411;&#22270;&#20687;&#24211;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290; &#30456;&#21453;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#20840;&#29699;&#21010;&#20998;&#20026;&#31163;&#25955;&#30340;&#22320;&#29702;&#21333;&#20803;&#65292;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026;&#20998;&#31867;&#20219;&#21153;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#39044;&#23450;&#20041;&#31867;&#21035;&#30340;&#38480;&#21046;&#65292;&#24182;&#19988;&#24403;&#22270;&#20687;&#30340;&#20301;&#32622;&#19982;&#20854;&#31867;&#21035;&#20013;&#24515;&#26174;&#33879;&#20559;&#31163;&#26102;&#65292;&#24448;&#24448;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#23450;&#20301;&#12290; &#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GeoCLIP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;Clip&#21551;&#21457;&#30340;&#22270;&#20687;&#21040;GPS&#26816;&#32034;&#26041;&#27861;&#65292;&#24378;&#21046;&#36827;&#34892;&#22270;&#20687;&#19982;&#20854;&#23545;&#24212;&#30340;GPS&#20301;&#32622;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290; GeoCLIP&#30340;&#20301;&#32622;&#32534;&#30721;&#22120;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#23558;&#22320;&#29699;&#24314;&#27169;&#20026;&#36830;&#32493;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Worldwide Geo-localization aims to pinpoint the precise location of images taken anywhere on Earth. This task has considerable challenges due to immense variation in geographic landscapes. The image-to-image retrieval-based approaches fail to solve this problem on a global scale as it is not feasible to construct a large gallery of images covering the entire world. Instead, existing approaches divide the globe into discrete geographic cells, transforming the problem into a classification task. However, their performance is limited by the predefined classes and often results in inaccurate localizations when an image's location significantly deviates from its class center. To overcome these limitations, we propose GeoCLIP, a novel CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between the image and its corresponding GPS locations. GeoCLIP's location encoder models the Earth as a continuous function by employing positional encoding through random Fourier features an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32423;&#34920;&#31034;&#30340;&#26377;&#25928;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.16014</link><description>&lt;p&gt;
&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#36827;&#34892;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32423;&#34920;&#31034;&#30340;&#26377;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#26368;&#36817;&#20986;&#29616;&#12290;&#23427;&#20204;&#26088;&#22312;&#36890;&#36807;&#20174;&#19978;&#19979;&#25991;&#20449;&#21495;x&#20013;&#39044;&#27979;&#30446;&#26631;&#20449;&#21495;y&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#12290;JEPAs&#32469;&#36807;&#20102;&#23545;&#25968;&#25454;&#22686;&#24378;&#21644;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#65292;&#36825;&#36890;&#24120;&#26159;&#23545;&#27604;&#23398;&#20064;&#25152;&#35201;&#27714;&#30340;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#19982;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#22270;&#32423;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;Graph-JEPA&#65292;&#36825;&#26159;&#22270;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;JEPA&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#36171;&#20104;&#34920;&#31034;&#38544;&#21547;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#26159;&#39044;&#27979;&#32534;&#30721;&#23376;&#22270;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#23398;&#20064;&#20248;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#38590;&#24230;&#24471;&#20998;&#37319;&#26679;&#12290;</title><link>http://arxiv.org/abs/2309.15995</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#35838;&#31243;&#23398;&#20064;&#22312;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Digital Twin-based Anomaly Detection with Curriculum Learning in Cyber-physical Systems. (arXiv:2309.15995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15995
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#23398;&#20064;&#20248;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#38590;&#24230;&#24471;&#20998;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#20445;&#35777;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#30340;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25915;&#20987;&#21644;CPS&#26412;&#36523;&#30340;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;CPS&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#23383;&#23402;&#29983;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;ATTAIN&#65292;&#23427;&#21033;&#29992;&#20102;CPS&#30340;&#21382;&#21490;&#21644;&#23454;&#26102;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#22312;&#38590;&#24230;&#19978;&#26377;&#26174;&#33879;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;&#22914;ATTAIN&#65289;&#21487;&#20197;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#30340;&#35838;&#31243;&#20013;&#21463;&#30410;&#12290;&#20026;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25968;&#23383;&#23402;&#29983;&#35838;&#31243;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;LATTICE&#65292;&#23427;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#23398;&#20064;&#26469;&#20248;&#21270;ATTAIN&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;LATTICE&#20026;&#27599;&#20010;&#26679;&#26412;&#36171;&#20104;&#19968;&#20010;&#38590;&#24230;&#24471;&#20998;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#21040;&#35757;&#32451;&#35843;&#24230;&#31243;&#24207;&#20013;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#30340;&#25209;&#22788;&#29702;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is critical to ensure the security of cyber-physical systems (CPS). However, due to the increasing complexity of attacks and CPS themselves, anomaly detection in CPS is becoming more and more challenging. In our previous work, we proposed a digital twin-based anomaly detection method, called ATTAIN, which takes advantage of both historical and real-time data of CPS. However, such data vary significantly in terms of difficulty. Therefore, similar to human learning processes, deep learning models (e.g., ATTAIN) can benefit from an easy-to-difficult curriculum. To this end, in this paper, we present a novel approach, named digitaL twin-based Anomaly deTecTion wIth Curriculum lEarning (LATTICE), which extends ATTAIN by introducing curriculum learning to optimize its learning paradigm. LATTICE attributes each sample with a difficulty score, before being fed into a training scheduler. The training scheduler samples batches of training data based on these difficulty scores s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#27493;&#24577;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;&#27493;&#24577;&#20998;&#26512;&#22312;&#35780;&#20272;&#19979;&#32930;&#39592;&#25240;&#24739;&#32773;&#24182;&#21457;&#30151;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#26159;&#26368;&#20339;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.15990</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27493;&#24577;&#20998;&#26512;&#22312;&#30417;&#27979;&#21644;&#31649;&#29702;&#19979;&#32930;&#25439;&#20260;&#20013;&#30340;&#24212;&#29992;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Based Analytics for the Significance of Gait Analysis in Monitoring and Managing Lower Extremity Injuries. (arXiv:2309.15990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15990
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20998;&#26512;&#27493;&#24577;&#25968;&#25454;&#65292;&#25506;&#35752;&#20102;&#27493;&#24577;&#20998;&#26512;&#22312;&#35780;&#20272;&#19979;&#32930;&#39592;&#25240;&#24739;&#32773;&#24182;&#21457;&#30151;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;XGBoost&#26159;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#27493;&#24577;&#20998;&#26512;&#20316;&#20026;&#19968;&#31181;&#35780;&#20272;&#19979;&#32930;&#39592;&#25240;&#24739;&#32773;&#26415;&#21518;&#24182;&#21457;&#30151;&#65288;&#22914;&#24863;&#26579;&#12289;&#38169;&#20301;&#25110;&#30828;&#20214;&#21050;&#28608;&#65289;&#30340;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#37325;&#28857;&#26159;&#20351;&#29992;&#36830;&#32493;&#27493;&#24577;&#25968;&#25454;&#39044;&#27979;&#24182;&#21457;&#30151;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19968;&#23478;&#23398;&#26415;&#20013;&#24515;&#30830;&#23450;&#20102;&#19979;&#32930;&#39592;&#25240;&#24739;&#32773;&#12290;&#24739;&#32773;&#20351;&#29992;&#33016;&#37096;&#35013;&#32622;&#36827;&#34892;&#27493;&#24577;&#20998;&#26512;&#12290;&#20351;&#29992;&#36719;&#20214;&#23545;&#21407;&#22987;&#27493;&#24577;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#24182;&#24378;&#35843;12&#20010;&#37325;&#35201;&#30340;&#27493;&#24577;&#21464;&#37327;&#12290;&#35757;&#32451;&#12289;&#27979;&#35797;&#21644;&#35780;&#20272;&#20102;&#21253;&#25324;XGBoost&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;LightGBM&#21644;&#38543;&#26426;&#26862;&#26519;&#22312;&#20869;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27880;&#24847;&#21040;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#24182;&#37319;&#29992;&#20102;SMOTE&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#27493;&#24577;&#20998;&#26512;&#20043;&#38388;&#30340;&#26102;&#38388;&#24046;&#24322;&#29420;&#31435;&#35745;&#31639;&#27493;&#24577;&#21464;&#37327;&#21464;&#21270;&#29575;&#65288;ROC&#65289;&#30340;&#26041;&#27861;&#12290;&#22312;&#24212;&#29992;SMOTE&#20043;&#21069;&#21644;&#20043;&#21518;&#65292;XGBoost&#37117;&#26159;&#26368;&#20248;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explored the potential of gait analysis as a tool for assessing post-injury complications, e.g., infection, malunion, or hardware irritation, in patients with lower extremity fractures. The research focused on the proficiency of supervised machine learning models predicting complications using consecutive gait datasets. We identified patients with lower extremity fractures at an academic center. Patients underwent gait analysis with a chest-mounted IMU device. Using software, raw gait data was preprocessed, emphasizing 12 essential gait variables. Machine learning models including XGBoost, Logistic Regression, SVM, LightGBM, and Random Forest were trained, tested, and evaluated. Attention was given to class imbalance, addressed using SMOTE. We introduced a methodology to compute the Rate of Change (ROC) for gait variables, independent of the time difference between gait analyses. XGBoost was the optimal model both before and after applying SMOTE. Prior to SMOTE, the model ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#30456;&#20851;&#27867;&#20989;&#12290;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#22242;&#38431;&#30340;&#21069;&#27839;&#25216;&#26415;&#65292;&#26088;&#22312;&#35268;&#33539;&#21270;&#22788;&#29702;&#27969;&#31243;&#12290;&#24050;&#32463;&#22312;DeepChem&#24211;&#20013;&#24320;&#28304;&#20102;&#27169;&#22411;&#65292;&#20026;&#21487;&#24494;&#20998;&#37327;&#23376;&#21270;&#23398;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2309.15985</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;
&lt;/p&gt;
&lt;p&gt;
Open Source Infrastructure for Differentiable Density Functional Theory. (arXiv:2309.15985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#24494;&#20998;&#30340;&#20132;&#25442;&#30456;&#20851;&#27867;&#20989;&#12290;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#22242;&#38431;&#30340;&#21069;&#27839;&#25216;&#26415;&#65292;&#26088;&#22312;&#35268;&#33539;&#21270;&#22788;&#29702;&#27969;&#31243;&#12290;&#24050;&#32463;&#22312;DeepChem&#24211;&#20013;&#24320;&#28304;&#20102;&#27169;&#22411;&#65292;&#20026;&#21487;&#24494;&#20998;&#37327;&#23376;&#21270;&#23398;&#26041;&#27861;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#20013;&#20351;&#29992;&#30340;&#20132;&#25442;&#30456;&#20851;&#27867;&#20989;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#35757;&#32451;&#36825;&#26679;&#30340;&#27867;&#20989;&#38656;&#35201;&#22797;&#26434;&#30340;&#36719;&#20214;&#22522;&#30784;&#35774;&#26045;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#24320;&#28304;&#22522;&#30784;&#35774;&#26045;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20132;&#25442;&#30456;&#20851;&#27867;&#20989;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#22242;&#38431;&#30340;&#21069;&#27839;&#25216;&#26415;&#65292;&#35268;&#33539;&#21270;&#22788;&#29702;&#27969;&#31243;&#12290;&#25105;&#20204;&#24050;&#32463;&#22312;DeepChem&#24211;&#20013;&#24320;&#28304;&#20102;&#27169;&#22411;&#65292;&#20026;&#21487;&#24494;&#20998;&#37327;&#23376;&#21270;&#23398;&#26041;&#27861;&#30340;&#20854;&#20182;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning exchange correlation functionals, used in quantum chemistry calculations, from data has become increasingly important in recent years, but training such a functional requires sophisticated software infrastructure. For this reason, we build open source infrastructure to train neural exchange correlation functionals. We aim to standardize the processing pipeline by adapting state-of-the-art techniques from work done by multiple groups. We have open sourced the model in the DeepChem library to provide a platform for additional research on differentiable quantum chemistry methods.
&lt;/p&gt;</description></item><item><title>TraCE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#23427;&#33021;&#22815;&#23558;&#39640;&#24230;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#20957;&#32451;&#20026;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15965</link><description>&lt;p&gt;
TraCE: &#36712;&#36857;&#21453;&#20107;&#23454;&#35299;&#37322;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
TraCE: Trajectory Counterfactual Explanation Scores. (arXiv:2309.15965v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15965
&lt;/p&gt;
&lt;p&gt;
TraCE&#26159;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#23427;&#33021;&#22815;&#23558;&#39640;&#24230;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#20957;&#32451;&#20026;&#19968;&#20010;&#21333;&#19968;&#20540;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#21644;&#30456;&#20851;&#31639;&#27861;&#34917;&#25937;&#36890;&#24120;&#34987;&#29992;&#20110;&#29702;&#35299;&#12289;&#35299;&#37322;&#21644;&#21487;&#33021;&#25913;&#21464;&#26469;&#33258;&#40657;&#30418;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#21453;&#20107;&#23454;&#25193;&#23637;&#24212;&#29992;&#20110;&#35780;&#20272;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#36827;&#23637;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;TraCE&#65288;&#36712;&#36857;&#21453;&#20107;&#23454;&#35299;&#37322;&#65289;&#20998;&#25968;&#65292;&#33021;&#22815;&#23558;&#39640;&#24230;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#36827;&#23637;&#20957;&#32451;&#20026;&#19968;&#20010;&#21333;&#19968;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#28085;&#30422;&#21307;&#30103;&#20445;&#20581;&#21644;&#27668;&#20505;&#21464;&#21270;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;TraCE&#30340;&#23454;&#29992;&#24615;&#26469;&#35777;&#26126;&#20854;&#20027;&#35201;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations, and their associated algorithmic recourse, are typically leveraged to understand, explain, and potentially alter a prediction coming from a black-box classifier. In this paper, we propose to extend the use of counterfactuals to evaluate progress in sequential decision making tasks. To this end, we introduce a model-agnostic modular framework, TraCE (Trajectory Counterfactual Explanation) scores, which is able to distill and condense progress in highly complex scenarios into a single value. We demonstrate TraCE's utility across domains by showcasing its main properties in two case studies spanning healthcare and climate change.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#21270;&#25910;&#32553;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#22797;&#26657;&#20934;&#19981;&#20339;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20943;&#23569;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.15963</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27491;&#21017;&#21270;&#25910;&#32553;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Uncertainty-Aware Pseudo-Label Selection Framework using Regularized Conformal Prediction. (arXiv:2309.15963v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#21270;&#25910;&#32553;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#20266;&#26631;&#31614;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#22797;&#26657;&#20934;&#19981;&#20339;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20943;&#23569;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#22312;&#25968;&#25454;&#22686;&#24378;&#19981;&#23454;&#29992;&#30340;&#39046;&#22495;&#20013;&#19981;&#21487;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20266;&#26631;&#31614;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#12289;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#19982;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#19981;&#20381;&#36182;&#20110;&#39046;&#22495;&#12290;&#30001;&#20110;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;&#30340;&#39640;&#32622;&#20449;&#24230;&#38169;&#35823;&#39044;&#27979;&#20351;&#20266;&#26631;&#31614;&#25928;&#26524;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;&#26694;&#26550;&#65292;&#21033;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#31639;&#27861;&#20135;&#29983;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#20462;&#22797;&#26657;&#20934;&#19981;&#20339;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20943;&#23569;&#22122;&#22768;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency regularization-based methods are prevalent in semi-supervised learning (SSL) algorithms due to their exceptional performance. However, they mainly depend on domain-specific data augmentations, which are not usable in domains where data augmentations are less practicable. On the other hand, Pseudo-labeling (PL) is a general and domain-agnostic SSL approach that, unlike consistency regularization-based methods, does not rely on the domain. PL underperforms due to the erroneous high-confidence predictions from poorly calibrated models. This paper proposes an uncertainty-aware pseudo-label selection framework that employs uncertainty sets yielded by the conformal regularization algorithm to fix the poor calibration neural networks, reducing noisy training data. The codes of this work are available at: https://github.com/matinmoezzi/ups conformal classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#36807;&#28388;&#30340;&#26032;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#21333;&#27169;&#24577;&#36807;&#28388;&#12289;&#36328;&#27169;&#24577;&#36807;&#28388;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#31561;&#31574;&#30053;&#12290;&#36890;&#36807;&#32508;&#21512;&#29616;&#26377;&#26041;&#27861;&#21644;&#25552;&#20986;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25913;&#21892;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.15954</link><description>&lt;p&gt;
&#32454;&#33410;&#20915;&#23450;&#25104;&#36133;&#65306;&#28145;&#20837;&#25506;&#32034;&#25968;&#25454;&#36807;&#28388;&#30340;&#20820;&#23376;&#27934;
&lt;/p&gt;
&lt;p&gt;
The Devil is in the Details: A Deep Dive into the Rabbit Hole of Data Filtering. (arXiv:2309.15954v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#36807;&#28388;&#30340;&#26032;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#21333;&#27169;&#24577;&#36807;&#28388;&#12289;&#36328;&#27169;&#24577;&#36807;&#28388;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#31561;&#31574;&#30053;&#12290;&#36890;&#36807;&#32508;&#21512;&#29616;&#26377;&#26041;&#27861;&#21644;&#25552;&#20986;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25913;&#21892;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#35774;&#35745;&#33258;&#24049;&#30340;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#65292;&#36825;&#20351;&#24471;&#20998;&#26512;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#21464;&#24471;&#22256;&#38590;&#12290;DataComp&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#30340;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#19982;DataComp&#25361;&#25112;&#26102;&#30340;&#23398;&#20064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#36807;&#28388;&#31574;&#30053;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#21333;&#27169;&#24577;&#36807;&#28388;&#12289;&#36328;&#27169;&#24577;&#36807;&#28388;&#21644;&#25968;&#25454;&#20998;&#24067;&#23545;&#40784;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20363;&#22914;&#22312;&#27700;&#24179;&#32763;&#36716;&#22270;&#20687;&#19978;&#35745;&#31639;CLIP&#20998;&#25968;&#20197;&#20943;&#23569;&#22330;&#26223;&#25991;&#23383;&#30340;&#24178;&#25200;&#65292;&#20351;&#29992;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#37325;&#26032;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#20197;&#25552;&#39640;&#20998;&#37197;&#35745;&#31639;&#39044;&#31639;&#30340;&#25928;&#29575;&#31561;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#36873;&#25321;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#20998;&#26512;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of pre-training data plays a critical role in the performance of foundation models. Popular foundation models often design their own recipe for data filtering, which makes it hard to analyze and compare different data filtering approaches. DataComp is a new benchmark dedicated to evaluating different methods for data filtering. This paper describes our learning and solution when participating in the DataComp challenge. Our filtering strategy includes three stages: single-modality filtering, cross-modality filtering, and data distribution alignment. We integrate existing methods and propose new solutions, such as computing CLIP score on horizontally flipped images to mitigate the interference of scene text, using vision and language models to retrieve training samples for target downstream tasks, rebalancing the data distribution to improve the efficiency of allocating the computational budget, etc. We slice and dice our design choices, provide in-depth analysis, and discuss
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#24615;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.15946</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Unified Long-Term Time-Series Forecasting Benchmark. (arXiv:2309.15946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15946
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#22522;&#20934;&#20998;&#26512;&#65292;&#21457;&#29616;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#24615;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#38271;&#26399;&#26102;&#24207;&#39044;&#27979;&#30340;&#32508;&#21512;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#37319;&#38598;&#20102;&#26469;&#33258;&#21508;&#31181;&#19981;&#21516;&#12289;&#21160;&#24577;&#31995;&#32479;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#32463;&#36807;&#26631;&#20934;&#21270;&#22788;&#29702;&#65292;&#20998;&#20026;&#35757;&#32451;&#21644;&#27979;&#35797;&#36712;&#36857;&#65292;&#24182;&#39044;&#20808;&#30830;&#23450;&#20102;&#22238;&#28335;&#38271;&#24230;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#38271;&#24230;&#20026;2000&#30340;&#36712;&#36857;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#22320;&#35780;&#20272;&#38271;&#26399;&#39044;&#27979;&#33021;&#21147;&#12290;&#20026;&#20102;&#30830;&#23450;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65288;&#21253;&#25324;LSTM&#12289;DeepAR&#12289;NLinear&#12289;N-Hits&#12289;PatchTST&#21644;LatentODE&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20043;&#38388;&#26377;&#36259;&#30340;&#24615;&#33021;&#27604;&#36739;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#26377;&#25928;&#24615;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#24615;&#30340;&#29305;&#28857;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#28508;&#22312;NLinear&#27169;&#22411;&#65292;&#24182;&#22312;DeepAR&#20013;&#22686;&#21152;&#20102;&#19968;&#20010;&#35838;&#31243;&#23398;&#20064;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to support the advancement of machine learning methods for predicting time-series data, we present a comprehensive dataset designed explicitly for long-term time-series forecasting. We incorporate a collection of datasets obtained from diverse, dynamic systems and real-life records. Each dataset is standardized by dividing it into training and test trajectories with predetermined lookback lengths. We include trajectories of length up to $2000$ to ensure a reliable evaluation of long-term forecasting capabilities. To determine the most effective model in diverse scenarios, we conduct an extensive benchmarking analysis using classical and state-of-the-art models, namely LSTM, DeepAR, NLinear, N-Hits, PatchTST, and LatentODE. Our findings reveal intriguing performance comparisons among these models, highlighting the dataset-dependent nature of model effectiveness. Notably, we introduce a custom latent NLinear model and enhance DeepAR with a curriculum learning phase. Both consist
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#36890;&#36947;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#31354;&#38388;&#38899;&#39057;&#30340;&#8220;&#20160;&#20040;&#8221;&#21644;&#8220;&#21738;&#37324;&#8221;&#12290;&#36890;&#36807;&#22810;&#32423;&#25968;&#25454;&#22686;&#24378;&#21644;&#36890;&#36947;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#20107;&#20214;&#20998;&#31867;&#21644;&#22768;&#38899;&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.15938</link><description>&lt;p&gt;
&#25506;&#32034;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#31354;&#38388;&#22768;&#38899;&#20107;&#20214;&#34920;&#31034;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring Self-Supervised Contrastive Learning of Spatial Sound Event Representation. (arXiv:2309.15938v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#36890;&#36947;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#31354;&#38388;&#38899;&#39057;&#30340;&#8220;&#20160;&#20040;&#8221;&#21644;&#8220;&#21738;&#37324;&#8221;&#12290;&#36890;&#36807;&#22810;&#32423;&#25968;&#25454;&#22686;&#24378;&#21644;&#36890;&#36947;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#20107;&#20214;&#20998;&#31867;&#21644;&#22768;&#38899;&#23450;&#20301;&#26041;&#38754;&#20248;&#20110;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#36890;&#36947;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;(MC-SimCLR)&#65292;&#20197;&#32534;&#30721;&#31354;&#38388;&#38899;&#39057;&#30340;&#8220;&#20160;&#20040;&#8221;&#21644;&#8220;&#21738;&#37324;&#8221;&#12290;MC-SimCLR&#20174;&#26410;&#26631;&#35760;&#30340;&#31354;&#38388;&#38899;&#39057;&#20013;&#23398;&#20064;&#32852;&#21512;&#39057;&#35889;&#21644;&#31354;&#38388;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#20107;&#20214;&#20998;&#31867;&#21644;&#22768;&#38899;&#23450;&#20301;&#33021;&#21147;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32423;&#25968;&#25454;&#22686;&#24378;&#27969;&#27700;&#32447;&#65292;&#23545;&#19981;&#21516;&#32423;&#21035;&#30340;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#22686;&#24378;&#65292;&#21253;&#25324;&#27874;&#24418;&#12289;Mel&#39057;&#35889;&#22270;&#21644;&#24191;&#20041;&#20114;&#30456;&#20851;(GCC)&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#36890;&#36947;&#22686;&#24378;&#26041;&#27861;&#65292;&#38543;&#26426;&#20132;&#25442;&#40614;&#20811;&#39118;&#39034;&#24207;&#21644;&#23631;&#34109;Mel&#21644;GCC&#36890;&#36947;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#23398;&#20064;&#34920;&#31034;&#20043;&#19978;&#30340;&#32447;&#24615;&#23618;&#22312;&#20107;&#20214;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#23450;&#20301;&#35823;&#24046;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#27599;&#31181;&#22686;&#24378;&#26041;&#27861;&#30340;&#25928;&#26524;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present a simple multi-channel framework for contrastive learning (MC-SimCLR) to encode 'what' and 'where' of spatial audios. MC-SimCLR learns joint spectral and spatial representations from unlabeled spatial audios, thereby enhancing both event classification and sound localization in downstream tasks. At its core, we propose a multi-level data augmentation pipeline that augments different levels of audio features, including waveforms, Mel spectrograms, and generalized cross-correlation (GCC) features. In addition, we introduce simple yet effective channel-wise augmentation methods to randomly swap the order of the microphones and mask Mel and GCC channels. By using these augmentations, we find that linear layers on top of the learned representation significantly outperform supervised models in terms of both event classification accuracy and localization error. We also perform a comprehensive analysis of the effect of each augmentation method and a comparison of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.15889</link><description>&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#39640;&#24863;&#30693;&#36136;&#37327;&#30340;&#26080;&#32447;&#22270;&#20687;&#20256;&#36755;
&lt;/p&gt;
&lt;p&gt;
High Perceptual Quality Wireless Image Delivery with Denoising Diffusion Models. (arXiv:2309.15889v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#21644;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#21644;&#36880;&#27493;&#20248;&#21270;&#38646;&#31354;&#38388;&#20869;&#23481;&#65292;&#23454;&#29616;&#20102;&#22312;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36890;&#36807;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32852;&#21512;&#28304;-&#20449;&#36947;&#32534;&#30721;&#65288;DeepJSCC&#65289;&#20197;&#21450;&#25509;&#25910;&#31471;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#22122;&#22768;&#26080;&#32447;&#20449;&#36947;&#19978;&#36827;&#34892;&#22270;&#20687;&#20256;&#36755;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22312;&#23454;&#38469;&#26377;&#38480;&#22359;&#38271;&#24230;&#30340;&#24773;&#20917;&#19979;&#30340;&#24863;&#30693;&#22833;&#30495;&#26435;&#34913;&#38382;&#39064;&#65292;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20998;&#31163;&#30340;&#28304;&#32534;&#30721;&#21644;&#20449;&#36947;&#32534;&#30721;&#21487;&#33021;&#20250;&#39640;&#24230;&#19981;&#29702;&#24819;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#22270;&#20687;&#30340;&#33539;&#22260;-&#38646;&#31354;&#38388;&#20998;&#35299;&#30340;&#26032;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#32534;&#30721;&#21518;&#20256;&#36755;&#22270;&#20687;&#30340;&#33539;&#22260;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;DDPM&#36880;&#27493;&#20248;&#21270;&#20854;&#38646;&#31354;&#38388;&#20869;&#23481;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#26631;&#20934;&#30340;DeepJSCC&#21644;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#37325;&#26500;&#22270;&#20687;&#30340;&#22833;&#30495;&#21644;&#24863;&#30693;&#36136;&#37327;&#26041;&#38754;&#23454;&#29616;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#21487;&#37325;&#29616;&#24615;&#65292;&#25105;&#20204;&#23558;&#20844;&#24320;&#20998;&#20139;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the image transmission problem over a noisy wireless channel via deep learning-based joint source-channel coding (DeepJSCC) along with a denoising diffusion probabilistic model (DDPM) at the receiver. Specifically, we are interested in the perception-distortion trade-off in the practical finite block length regime, in which separate source and channel coding can be highly suboptimal. We introduce a novel scheme that utilizes the range-null space decomposition of the target image. We transmit the range-space of the image after encoding and employ DDPM to progressively refine its null space contents. Through extensive experiments, we demonstrate significant improvements in distortion and perceptual quality of reconstructed images compared to standard DeepJSCC and the state-of-the-art generative learning-based method. We will publicly share our source code to facilitate further research and reproducibility.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27169;&#31946;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#26426;&#26041;&#27861;&#26469;&#24212;&#23545;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#22122;&#22768;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24341;&#20837;&#30452;&#35273;&#27169;&#31946;&#25104;&#21592;&#20851;&#31995;&#21644;&#36229;&#24179;&#38754;&#27169;&#31946;&#25104;&#21592;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15886</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#24433;&#30340;&#27169;&#31946;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#26426;&#29992;&#20110;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Projection based fuzzy least squares twin support vector machine for class imbalance problems. (arXiv:2309.15886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15886
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27169;&#31946;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#26426;&#26041;&#27861;&#26469;&#24212;&#23545;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#21644;&#22122;&#22768;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#24341;&#20837;&#30452;&#35273;&#27169;&#31946;&#25104;&#21592;&#20851;&#31995;&#21644;&#36229;&#24179;&#38754;&#27169;&#31946;&#25104;&#21592;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#38382;&#39064;&#26102;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#19981;&#24179;&#34913;&#26159;&#35768;&#22810;&#23454;&#38469;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#30001;&#20110;&#26679;&#26412;&#25968;&#37327;&#19981;&#24179;&#34913;&#65292;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#20998;&#31867;&#22120;&#23545;&#22810;&#25968;&#31867;&#21035;&#26377;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26679;&#26412;&#36890;&#24120;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#31946;&#26041;&#27861;&#26469;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#21644;&#20855;&#26377;&#22122;&#22768;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22522;&#20110;&#30452;&#35273;&#27169;&#31946;&#25104;&#21592;&#20851;&#31995;&#65292;&#31216;&#20026;&#40065;&#26834;&#33021;&#37327;&#22522;&#27169;&#31946;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;IF-RELSTSVM&#65289;&#12290;&#27492;&#22806;&#65292;&#22312;&#31532;&#20108;&#31181;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#36229;&#24179;&#38754;&#30340;&#27169;&#31946;&#25104;&#21592;&#20851;&#31995;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#26368;&#32456;&#20998;&#31867;&#22120;&#31216;&#20026;&#40065;&#26834;&#33021;&#37327;&#22522;&#27169;&#31946;&#26368;&#23567;&#20108;&#20056;&#21452;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;F-RELSTSVM&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#25104;&#21592;&#20851;&#31995;&#20540;&#22522;&#20110;&#22522;&#20110;&#25237;&#24433;&#30340;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#28857;&#25237;&#24433;&#21040;&#36229;&#24179;&#38754;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a major problem in many real world classification tasks. Due to the imbalance in the number of samples, the support vector machine (SVM) classifier gets biased toward the majority class. Furthermore, these samples are often observed with a certain degree of noise. Therefore, to remove these problems we propose a novel fuzzy based approach to deal with class imbalanced as well noisy datasets. We propose two approaches to address these problems. The first approach is based on the intuitionistic fuzzy membership, termed as robust energy-based intuitionistic fuzzy least squares twin support vector machine (IF-RELSTSVM). Furthermore, we introduce the concept of hyperplane-based fuzzy membership in our second approach, where the final classifier is termed as robust energy-based fuzzy least square twin support vector machine (F-RELSTSVM). By using this technique, the membership values are based on a projection based approach, where the data points are projected on the hyper
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#65288;MLET&#65289;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#36328;&#31867;&#21035;&#23398;&#20064;&#20135;&#29983;&#20248;&#31168;&#30340;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23884;&#20837;&#23618;&#30340;&#20998;&#35299;&#35757;&#32451;&#23884;&#20837;&#65292;&#20869;&#37096;&#32500;&#24230;&#39640;&#20110;&#30446;&#26631;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35299;&#37322;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.15881</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#36328;&#31867;&#21035;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cross-Category Learning in Recommendation Systems with Multi-Layer Embedding Training. (arXiv:2309.15881v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#65288;MLET&#65289;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#36328;&#31867;&#21035;&#23398;&#20064;&#20135;&#29983;&#20248;&#31168;&#30340;&#23884;&#20837;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23884;&#20837;&#23618;&#30340;&#20998;&#35299;&#35757;&#32451;&#23884;&#20837;&#65292;&#20869;&#37096;&#32500;&#24230;&#39640;&#20110;&#30446;&#26631;&#23884;&#20837;&#32500;&#24230;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;&#35813;&#25216;&#26415;&#30340;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#24778;&#35766;&#65292;&#24182;&#19988;&#36890;&#36807;&#29702;&#35770;&#35299;&#37322;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;DNN&#30340;&#25512;&#33616;&#31995;&#32479;&#20381;&#36182;&#20110;&#23545;&#31232;&#30095;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#24471;&#21040;&#30340;&#23884;&#20837;&#12290;&#36755;&#20837;&#31232;&#30095;&#24615;&#20351;&#24471;&#24456;&#38590;&#33719;&#24471;&#23569;&#20986;&#29616;&#31867;&#21035;&#30340;&#39640;&#36136;&#37327;&#23884;&#20837;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#34920;&#31034;&#24456;&#23569;&#26356;&#26032;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#35757;&#32451;&#26102;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#36328;&#31867;&#21035;&#23398;&#20064;&#20135;&#29983;&#20248;&#31168;&#30340;&#23884;&#20837;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20102;&#20854;&#20196;&#20154;&#24778;&#35766;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#26696;&#34987;&#31216;&#20026;&#22810;&#23618;&#23884;&#20837;&#35757;&#32451;&#65288;MLET&#65289;&#65292;&#36890;&#36807;&#23884;&#20837;&#23618;&#30340;&#20998;&#35299;&#35757;&#32451;&#23884;&#20837;&#65292;&#20869;&#37096;&#32500;&#24230;&#39640;&#20110;&#30446;&#26631;&#23884;&#20837;&#32500;&#24230;&#12290;&#20026;&#20102;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#65292;MLET&#23558;&#35757;&#32451;&#24471;&#21040;&#30340;&#21452;&#23618;&#23884;&#20837;&#36716;&#25442;&#20026;&#21333;&#23618;&#23884;&#20837;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25512;&#29702;&#26102;&#30340;&#27169;&#22411;&#22823;&#23567;&#19981;&#21464;&#12290;MLET&#30340;&#23454;&#39564;&#20248;&#36234;&#24615;&#20196;&#20154;&#22256;&#24785;&#65292;&#22240;&#20026;&#20854;&#25628;&#32034;&#31354;&#38388;&#24182;&#19981;&#27604;&#21333;&#23618;&#23884;&#20837;&#26356;&#22823;&#12290;MLET&#23545;&#20869;&#37096;&#32500;&#24230;&#30340;&#24378;&#20381;&#36182;&#29978;&#33267;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#20010;&#29702;&#35770;&#26469;&#35299;&#37322;&#36825;&#20004;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern DNN-based recommendation systems rely on training-derived embeddings of sparse features. Input sparsity makes obtaining high-quality embeddings for rarely-occurring categories harder as their representations are updated infrequently. We demonstrate a training-time technique to produce superior embeddings via effective cross-category learning and theoretically explain its surprising effectiveness. The scheme, termed the multi-layer embeddings training (MLET), trains embeddings using factorization of the embedding layer, with an inner dimension higher than the target embedding dimension. For inference efficiency, MLET converts the trained two-layer embedding into a single-layer one thus keeping inference-time model size unchanged.  Empirical superiority of MLET is puzzling as its search space is not larger than that of the single-layer embedding. The strong dependence of MLET on the inner dimension is even more surprising. We develop a theory that explains both of these behaviors 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2309.15877</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#21644;&#22788;&#29702;&#26469;&#33258;&#22810;&#31181;&#20449;&#24687;&#28304;&#25110;&#27169;&#24577;&#23545;&#20110;&#33719;&#24471;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#35770;&#20998;&#23618;&#24863;&#30693;(ITHP)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27010;&#24565;&#12290;&#19982;&#22823;&#22810;&#25968;&#26088;&#22312;&#23558;&#25152;&#26377;&#27169;&#24577;&#32435;&#20837;&#36755;&#20837;&#30340;&#20256;&#32479;&#34701;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20027;&#35201;&#27169;&#24577;&#25351;&#23450;&#20026;&#36755;&#20837;&#65292;&#32780;&#20854;&#20313;&#27169;&#24577;&#21017;&#20316;&#20026;&#20449;&#24687;&#36335;&#24452;&#20013;&#30340;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#36755;&#20837;&#27169;&#24577;&#29366;&#24577;&#20043;&#38388;&#26368;&#23567;&#21270;&#30456;&#20114;&#20449;&#24687;&#24182;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#20854;&#20313;&#27169;&#24577;&#20043;&#38388;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#30340;&#24179;&#34913;&#65292;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#24182;&#26368;&#23567;&#21270;&#20887;&#20313;&#30340;&#32039;&#20945;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
&lt;/p&gt;</description></item><item><title>STAG&#26159;&#19968;&#20010;GNN&#26381;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20013;&#30340;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#21644;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#26469;&#20248;&#21270;&#33410;&#28857;&#34920;&#31034;&#30340;&#26356;&#26032;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2309.15875</link><description>&lt;p&gt;
STAG: &#23454;&#29616;&#21160;&#24577;&#22270;&#20013;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;
&lt;/p&gt;
&lt;p&gt;
STAG: Enabling Low Latency and Low Staleness of GNN-based Services with Dynamic Graphs. (arXiv:2309.15875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15875
&lt;/p&gt;
&lt;p&gt;
STAG&#26159;&#19968;&#20010;GNN&#26381;&#21153;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20013;&#30340;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;&#38382;&#39064;&#12290;&#23427;&#37319;&#29992;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#21644;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#26469;&#20248;&#21270;&#33410;&#28857;&#34920;&#31034;&#30340;&#26356;&#26032;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26032;&#20852;&#30340;&#29992;&#25143;&#38754;&#21521;&#26381;&#21153;&#37319;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#25552;&#39640;&#26381;&#21153;&#20934;&#30830;&#24615;&#12290;&#24403;GNN&#27169;&#22411;&#20351;&#29992;&#30340;&#22270;&#21457;&#29983;&#21464;&#21270;&#26102;&#65292;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#24212;&#30456;&#24212;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#33410;&#28857;&#34920;&#31034;&#30340;&#26356;&#26032;&#36895;&#24230;&#36807;&#24930;&#65292;&#23548;&#33268;&#29992;&#25143;&#26597;&#35810;&#30340;&#21709;&#24212;&#24310;&#36831;&#36739;&#38271;&#65288;&#26356;&#26032;&#23436;&#25104;&#21518;&#36827;&#34892;&#25512;&#29702;&#65289;&#25110;&#23384;&#22312;&#36739;&#39640;&#30340;&#38472;&#26087;&#24230;&#38382;&#39064;&#65288;&#22522;&#20110;&#38472;&#26087;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#65289;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#26356;&#26032;&#36807;&#24930;&#20027;&#35201;&#26159;&#30001;&#20110;&#22270;&#20013;&#30340;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#21644;&#37325;&#22797;&#35745;&#31639;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STAG&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#23454;&#29616;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#20302;&#24310;&#36831;&#21644;&#20302;&#38472;&#26087;&#24230;&#30340;GNN&#26381;&#21153;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#21644;&#22522;&#20110;&#21487;&#21152;&#24615;&#30340;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#12290;&#36890;&#36807;&#21327;&#21516;&#26381;&#21153;&#26426;&#21046;&#65292;&#21482;&#26377;&#37096;&#20998;&#33410;&#28857;&#34920;&#31034;&#22312;&#26356;&#26032;&#38454;&#27573;&#36827;&#34892;&#26356;&#26032;&#65292;&#26368;&#32456;&#30340;&#34920;&#31034;&#26159;&#22312;&#22686;&#37327;&#20256;&#25773;&#31574;&#30053;&#20013;&#35745;&#31639;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many emerging user-facing services adopt Graph Neural Networks (GNNs) to improve serving accuracy. When the graph used by a GNN model changes, representations (embedding) of nodes in the graph should be updated accordingly. However, the node representation update is too slow, resulting in either long response latency of user queries (the inference is performed after the update completes) or high staleness problem (the inference is performed based on stale data). Our in-depth analysis shows that the slow update is mainly due to neighbor explosion problem in graphs and duplicated computation. Based on such findings, we propose STAG, a GNN serving framework that enables low latency and low staleness of GNN-based services. It comprises a collaborative serving mechanism and an additivity-based incremental propagation strategy. With the collaborative serving mechanism, only part of node representations are updated during the update phase, and the final representations are calculated in the i
&lt;/p&gt;</description></item><item><title>Telescope&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#21487;&#38752;&#22320;&#36827;&#34892;&#39044;&#27979;&#65292;&#26080;&#38656;&#21442;&#25968;&#21270;&#25110;&#35757;&#32451;&#21644;&#36866;&#37197;&#22823;&#37327;&#21442;&#25968;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;Telescope&#33021;&#22815;&#24555;&#36895;&#25552;&#20379;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15871</link><description>&lt;p&gt;
Telescope:&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Telescope: An Automated Hybrid Forecasting Approach on a Level-Playing Field. (arXiv:2309.15871v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15871
&lt;/p&gt;
&lt;p&gt;
Telescope&#26159;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#28151;&#21512;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#21487;&#38752;&#22320;&#36827;&#34892;&#39044;&#27979;&#65292;&#26080;&#38656;&#21442;&#25968;&#21270;&#25110;&#35757;&#32451;&#21644;&#36866;&#37197;&#22823;&#37327;&#21442;&#25968;&#65292;&#24182;&#19988;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;Telescope&#33021;&#22815;&#24555;&#36895;&#25552;&#20379;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#30340;&#35768;&#22810;&#39046;&#22495;&#65292;&#39044;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25903;&#26609;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32463;&#39564;&#65292;&#26368;&#36817;&#25552;&#20986;&#30340;&#39044;&#27979;&#26041;&#27861;&#35745;&#31639;&#23494;&#38598;&#65292;&#33258;&#21160;&#21270;&#31243;&#24230;&#20302;&#65292;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#23450;&#21046;&#65292;&#25110;&#32773;&#32570;&#20047;&#21487;&#39044;&#27979;&#30340;&#32467;&#26524;&#26102;&#38388;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Telescope&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#20174;&#32473;&#23450;&#30340;&#26102;&#38388;&#24207;&#21015;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#20998;&#21106;&#25104;&#37096;&#20998;&#65292;&#20998;&#21035;&#22788;&#29702;&#27599;&#20010;&#37096;&#20998;&#12290;&#19982;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#21442;&#25968;&#21270;&#25110;&#35757;&#32451;&#21644;&#36866;&#37197;&#22823;&#37327;&#21442;&#25968;&#12290;&#23427;&#21482;&#20351;&#29992;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#20960;&#31186;&#38047;&#20869;&#25552;&#20379;&#39044;&#27979;&#65292;&#26080;&#38656;&#39069;&#22806;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;Telescope&#36890;&#36807;&#25552;&#20379;&#20934;&#30830;&#21487;&#38752;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#23545;&#20998;&#26512;&#30340;&#26102;&#38388;&#24207;&#21015;&#19981;&#20570;&#20219;&#20309;&#20551;&#35774;&#65292;&#32988;&#36807;&#20102;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many areas of decision-making, forecasting is an essential pillar. Consequently, many different forecasting methods have been proposed. From our experience, recently presented forecasting methods are computationally intensive, poorly automated, tailored to a particular data set, or they lack a predictable time-to-result. To this end, we introduce Telescope, a novel machine learning-based forecasting approach that automatically retrieves relevant information from a given time series and splits it into parts, handling each of them separately. In contrast to deep learning methods, our approach doesn't require parameterization or the need to train and fit a multitude of parameters. It operates with just one time series and provides forecasts within seconds without any additional setup. Our experiments show that Telescope outperforms recent methods by providing accurate and reliable forecasts while making no assumptions about the analyzed time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20102;&#30524;&#21387;&#22686;&#39640;&#24739;&#32773;&#20013;&#20855;&#26377;&#19981;&#21516;&#35270;&#37326;&#36864;&#21270;&#36235;&#21183;&#30340;&#20122;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#24555;&#36895;&#35270;&#37326;&#36864;&#21270;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2309.15867</link><description>&lt;p&gt;
&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#30524;&#21387;&#22686;&#39640;&#24739;&#32773;&#24555;&#36895;&#35270;&#37326;&#36864;&#21270;&#30340;&#30456;&#20851;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Identifying factors associated with fast visual field progression in patients with ocular hypertension based on unsupervised machine learning. (arXiv:2309.15867v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35782;&#21035;&#20102;&#30524;&#21387;&#22686;&#39640;&#24739;&#32773;&#20013;&#20855;&#26377;&#19981;&#21516;&#35270;&#37326;&#36864;&#21270;&#36235;&#21183;&#30340;&#20122;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#24555;&#36895;&#35270;&#37326;&#36864;&#21270;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#22522;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#20855;&#26377;&#19981;&#21516;&#35270;&#37326;&#36864;&#21270;&#36235;&#21183;&#30340;&#30524;&#21387;&#22686;&#39640; (OHT) &#20122;&#22411;&#65292;&#24182;&#21457;&#29616;&#19982;&#24555;&#36895;&#35270;&#37326;&#36864;&#21270;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#21442;&#19982;&#32773;&#65306;&#30740;&#31350;&#32435;&#20837;&#20102;1568&#21517;&#30524;&#21387;&#22686;&#39640;&#27835;&#30103;&#30740;&#31350; (OHTS&#65289;&#21442;&#19982;&#32773;&#30340;3133&#21482;&#30524;&#65292;&#36825;&#20123;&#21442;&#19982;&#32773;&#33267;&#23569;&#26377;&#20116;&#27425;&#38543;&#35775;&#35270;&#37326;&#27979;&#35797;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#31867;&#28151;&#21512;&#27169;&#22411; (LCMM) &#26681;&#25454;&#26631;&#20934;&#33258;&#21160;&#21270;&#35270;&#37326;&#26816;&#27979; (SAP) &#24179;&#22343;&#20559;&#24046; (MD) &#36712;&#36857;&#35782;&#21035;&#30524;&#21387;&#22686;&#39640;&#20122;&#22411;&#12290;&#25105;&#20204;&#26681;&#25454;&#22522;&#32447;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#12289;&#20020;&#24202;&#12289;&#30524;&#37096;&#21644;&#35270;&#37326;&#25351;&#26631;&#25551;&#36848;&#20122;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#24191;&#20041;&#20272;&#35745;&#26041;&#31243; (GEE) &#35782;&#21035;&#25512;&#36827;&#24555;&#36895;&#35270;&#37326;&#36864;&#21270;&#30340;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#26041;&#24335;&#39564;&#35777;&#32467;&#26524;&#12290;&#32467;&#26524;&#65306;LCMM&#27169;&#22411;&#21457;&#29616;&#20102;&#22235;&#20010;&#30524;&#30555;&#32676;&#38598;&#65288;&#20122;&#22411;&#65289;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;MD&#24694;&#21270;&#36712;&#36857;&#12290;&#20854;&#20013;&#65292;&#32858;&#31867;&#20013;&#30340;&#30524;&#30555;&#25968;&#20998;&#21035;&#20026;794&#21482; (25%)&#12289;1675&#21482; (54%)&#12289;531&#21482; (17%) &#21644;133&#21482; (4%)&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To identify ocular hypertension (OHT) subtypes with different trends of visual field (VF) progression based on unsupervised machine learning and to discover factors associated with fast VF progression. Participants: A total of 3133 eyes of 1568 ocular hypertension treatment study (OHTS) participants with at least five follow-up VF tests were included in the study. Methods: We used a latent class mixed model (LCMM) to identify OHT subtypes using standard automated perimetry (SAP) mean deviation (MD) trajectories. We characterized the subtypes based on demographic, clinical, ocular, and VF factors at the baseline. We then identified factors driving fast VF progression using generalized estimating equation (GEE) and justified findings qualitatively and quantitatively. Results: The LCMM model discovered four clusters (subtypes) of eyes with different trajectories of MD worsening. The number of eyes in clusters were 794 (25%), 1675 (54%), 531 (17%) and 133 (4%). We labelled the clu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.15757</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Latent Graph Powered Semi-Supervised Learning on Biomedical Tabular Data. (arXiv:2309.15757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22270;&#30340;&#34920;&#31034;&#26469;&#25429;&#25417;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23454;&#29616;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#30340;&#26377;&#25928;&#34701;&#21512;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21322;&#30417;&#30563;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#65288;&#26377;&#65289;&#26631;&#35760;&#25968;&#25454;&#20043;&#38388;&#30340;&#23454;&#20363;&#38388;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25512;&#26029;&#25429;&#25417;&#20869;&#22312;&#25968;&#25454;&#20851;&#31995;&#30340;&#28508;&#22312;&#22270;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20419;&#36827;&#20102;&#20449;&#24687;&#22312;&#25972;&#20010;&#22270;&#20013;&#30340;&#26080;&#32541;&#20256;&#25773;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#34701;&#21512;&#20840;&#23616;&#21644;&#23616;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#21307;&#23398;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#21457;&#29616;&#23454;&#20363;&#38388;&#20851;&#31995;&#20316;&#20026;&#26500;&#24314;&#24378;&#21270;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#30340;&#40065;&#26834;&#28508;&#22312;&#22270;&#30340;&#23454;&#38469;&#25163;&#27573;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the domain of semi-supervised learning, the current approaches insufficiently exploit the potential of considering inter-instance relationships among (un)labeled data. In this work, we address this limitation by providing an approach for inferring latent graphs that capture the intrinsic data relationships. By leveraging graph-based representations, our approach facilitates the seamless propagation of information throughout the graph, enabling the effective incorporation of global and local knowledge. Through evaluations on biomedical tabular datasets, we compare the capabilities of our approach to other contemporary methods. Our work demonstrates the significance of inter-instance relationship discovery as practical means for constructing robust latent graphs to enhance semi-supervised learning techniques. Our method achieves state-of-the-art results on three biomedical datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#30340;&#26041;&#27861;&#65288;VaSSO&#65289;&#22686;&#24378;&#20102;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#21644;&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.15639</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#22686;&#24378;&#38160;&#24230;&#24863;&#30693;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Sharpness-Aware Optimization Through Variance Suppression. (arXiv:2309.15639v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#30340;&#26041;&#27861;&#65288;VaSSO&#65289;&#22686;&#24378;&#20102;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#21644;&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#22312;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#26377;&#30528;&#33391;&#22909;&#30340;&#35760;&#24405;&#65292;&#21363;&#20351;&#27809;&#26377;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;SAM&#20511;&#21161;&#25439;&#22833;&#20989;&#25968;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#22312;&#37051;&#22495;&#20869;&#21442;&#25968;&#23545;&#25932;&#23545;&#25200;&#21160;&#24341;&#36215;&#30340;&#26368;&#22823;&#25439;&#22833;&#65292;&#23547;&#25214;&#8220;&#24179;&#22374;&#26368;&#23567;&#20540;&#8221;&#25152;&#22312;&#30340;&#8220;&#24179;&#22374;&#23665;&#35895;&#8221;&#65292;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#32771;&#34385;&#20102;&#25439;&#22833;&#20989;&#25968;&#30340;&#38160;&#24230;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#36825;&#31181;&#8220;&#36807;&#20110;&#21451;&#22909;&#30340;&#25932;&#23545;&#32773;&#8221;&#21487;&#33021;&#20250;&#38480;&#21046;&#27867;&#21270;&#30340;&#26368;&#39640;&#27700;&#24179;&#12290;&#26412;&#25991;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#26041;&#24046;&#25233;&#21046;&#65288;VaSSO&#65289;&#26469;&#31283;&#23450;&#25932;&#23545;&#32773;&#65292;&#36991;&#20813;&#36825;&#31181;&#21451;&#22909;&#24615;&#12290; VaSSO&#30340;&#31283;&#23450;&#24615;&#21487;&#35777;&#26126;&#65292;&#24182;&#22312;&#27169;&#22411;&#26080;&#20851;&#20219;&#21153;&#20013;&#65288;&#21253;&#25324;&#22270;&#20687;&#20998;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#30456;&#23545;&#20110;SAM&#26377;&#30528;&#25968;&#20540;&#19978;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#35777;&#23454;VaSSO&#36171;&#20104;SAM&#23545;&#39640;&#27700;&#24179;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an adversary perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an 'over-friendly adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through variance suppression (VaSSO) to avoid such friendliness. VaSSO's provable stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15564</link><description>&lt;p&gt;
&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jointly Training Large Autoregressive Multimodal Models. (arXiv:2309.15564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20849;&#21516;&#35757;&#32451;&#22823;&#22411;&#33258;&#22238;&#24402;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#34701;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#20351;&#24471;&#35813;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#35821;&#35328;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26080;&#32541;&#22810;&#27169;&#24577;&#36755;&#20986;&#30340;&#21333;&#19968;&#24378;&#22823;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#33258;&#22238;&#24402;&#28151;&#21512;&#65288;JAM&#65289;&#26694;&#26550;&#65292;&#19968;&#31181;&#31995;&#32479;&#34701;&#21512;&#29616;&#26377;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#19987;&#38376;&#30340;&#12289;&#25968;&#25454;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#20248;&#31574;&#30053;&#65292;&#38024;&#23545;&#28151;&#21512;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;&#25105;&#20204;&#26368;&#32456;&#30340;&#35843;&#20248;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22810;&#27169;&#24577;&#36755;&#20986;&#26041;&#38754;&#34920;&#29616;&#20986;&#26080;&#19982;&#20262;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#26126;&#30830;&#20026;&#27492;&#30446;&#30340;&#32780;&#35774;&#35745;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, advances in the large-scale pretraining of language and text-to-image models have revolutionized the field of machine learning. Yet, integrating these two modalities into a single, robust model capable of generating seamless multimodal outputs remains a significant challenge. To address this gap, we present the Joint Autoregressive Mixture (JAM) framework, a modular approach that systematically fuses existing text and image generation models. We also introduce a specialized, data-efficient instruction-tuning strategy, tailored for mixed-modal generation tasks. Our final instruct-tuned model demonstrates unparalleled performance in generating high-quality multimodal outputs and represents the first model explicitly designed for this purpose.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;NFL&#29699;&#21592;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#25214;&#21040;&#26368;&#20339;&#38453;&#23481;&#26469;&#26368;&#22823;&#21270;&#22855;&#24187;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38453;&#23481;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15253</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32447;&#24615;&#35268;&#21010;&#36827;&#34892;&#27599;&#26085;&#22855;&#24187;&#36275;&#29699;&#26368;&#20339;&#38453;&#23481;&#30340;&#26041;&#27861;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Method and Validation for Optimal Lineup Creation for Daily Fantasy Football Using Machine Learning and Linear Programming. (arXiv:2309.15253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#39044;&#27979;NFL&#29699;&#21592;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#32447;&#24615;&#35268;&#21010;&#25214;&#21040;&#26368;&#20339;&#38453;&#23481;&#26469;&#26368;&#22823;&#21270;&#22855;&#24187;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38453;&#23481;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#26085;&#22855;&#24187;&#20307;&#32946;&#26159;&#27599;&#21608;&#25110;&#27599;&#26085;&#30340;&#22312;&#32447;&#27604;&#36187;&#65292;&#20854;&#20013;&#20010;&#20154;&#36816;&#21160;&#21592;&#30340;&#30495;&#23454;&#27604;&#36187;&#34920;&#29616;&#34987;&#36716;&#25442;&#20026;&#22855;&#24187;&#20998;&#25968;&#65288;FPTS&#65289;&#12290;&#29992;&#25143;&#26681;&#25454;&#35774;&#23450;&#30340;&#29699;&#21592;&#24037;&#36164;&#19978;&#38480;&#36873;&#25321;&#38453;&#23481;&#20197;&#26368;&#22823;&#21270;&#20182;&#20204;&#30340;FPTS&#12290;&#26412;&#25991;&#38024;&#23545;&#20004;&#20010;&#37325;&#28857;&#36827;&#34892;&#30740;&#31350;&#65306;&#65288;1&#65289;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#39044;&#27979;NFL&#29699;&#21592;&#34920;&#29616;&#30340;&#26041;&#27861;&#30340;&#24320;&#21457;&#65292;&#65288;2&#65289;&#22312;&#35774;&#23450;&#30340;&#24037;&#36164;&#38480;&#21046;&#19979;&#30830;&#23450;&#26368;&#20339;&#38453;&#23481;&#20197;&#26368;&#22823;&#21270;FPTS&#12290;&#21019;&#24314;&#20102;&#19968;&#20010;&#30417;&#30563;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#36807;&#21435;&#30340;&#29699;&#21592;&#34920;&#29616;&#65288;&#26412;&#24037;&#20316;&#20351;&#29992;2018NFL&#24120;&#35268;&#36187;&#23395;&#65289;&#39044;&#27979;FPTS&#65292;&#24182;&#23558;&#36825;&#20123;&#39044;&#27979;&#30340;FPTS&#29992;&#20110;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#20013;&#23547;&#25214;&#26368;&#20339;&#38453;&#23481;&#12290;&#23558;&#29983;&#25104;&#30340;&#38453;&#23481;&#30340;&#24615;&#33021;&#19982;&#38543;&#26426;&#29983;&#25104;&#30340;&#38453;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#26368;&#20339;&#38453;&#23481;&#30340;&#24615;&#33021;&#20248;&#20110;&#38543;&#26426;&#38453;&#23481;&#12290;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#38453;&#23481;&#19982;DraftKings&#29992;&#25143;&#30340;&#30495;&#23454;&#38453;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#29983;&#25104;&#30340;&#38453;&#23481;&#36890;&#24120;&#25509;&#36817;31%&#30340;&#30495;&#23454;&#38453;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily fantasy sports (DFS) are weekly or daily online contests where real-game performances of individual players are converted to fantasy points (FPTS). Users select players for their lineup to maximize their FPTS within a set player salary cap. This paper focuses on (1) the development of a method to forecast NFL player performance under uncertainty and (2) determining an optimal lineup to maximize FPTS under a set salary limit. A supervised learning neural network was created and used to project FPTS based on past player performance (2018 NFL regular season for this work) prior to the upcoming week. These projected FPTS were used in a mixed integer linear program to find the optimal lineup. The performance of resultant lineups was compared to randomly-created lineups. On average, the optimal lineups outperformed the random lineups. The generated lineups were then compared to real-world lineups from users on DraftKings. The generated lineups generally fell in approximately the 31st p
&lt;/p&gt;</description></item><item><title>DPA-WNO&#26159;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;/&#35782;&#21035;&#32570;&#22833;&#30340;&#29289;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.15128</link><description>&lt;p&gt;
DPA-WNO&#65306;&#19968;&#31867;&#38543;&#26426;&#21147;&#23398;&#38382;&#39064;&#30340;&#28784;&#31665;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DPA-WNO: A gray box model for a class of stochastic mechanics problem. (arXiv:2309.15128v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15128
&lt;/p&gt;
&lt;p&gt;
DPA-WNO&#26159;&#19968;&#31181;&#23558;&#21487;&#35299;&#37322;&#24615;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#19982;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;/&#35782;&#21035;&#32570;&#22833;&#30340;&#29289;&#29702;&#65292;&#24182;&#35299;&#20915;&#20102;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#30340;&#29289;&#29702;&#23450;&#24459;&#24120;&#24120;&#22522;&#20110;&#26576;&#20123;&#20551;&#35774;&#21644;&#36817;&#20284;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#36825;&#20123;&#26041;&#31243;&#36827;&#34892;&#30340;&#20998;&#26512;&#21644;&#35774;&#35745;&#20063;&#26159;&#36817;&#20284;&#30340;&#12290;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#30340;&#20986;&#29616;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35299;&#20915;&#20102;&#36825;&#20010;&#25361;&#25112;&#65307;&#28982;&#32780;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#24448;&#24448;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(a)&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;(b)&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#65292;(c)&#26080;&#27861;&#36229;&#36234;&#35757;&#32451;&#33539;&#22260;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#25805;&#20316;&#31526;&#23398;&#20064;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#20010;&#28508;&#22312;&#30340;&#26367;&#20195;&#26041;&#26696;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#35748;&#20026;&#65292;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#23384;&#22312;&#20110;&#25968;&#25454;&#29289;&#29702;&#34701;&#21512;&#20013;&#65292;&#20854;&#20013;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#29992;&#20110;&#32416;&#27491;/&#35782;&#21035;&#32570;&#22833;&#30340;&#29289;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#29289;&#29702;&#22686;&#24378;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;(DPA-WNO)&#12290;&#35813;&#25552;&#20986;&#30340;DPA-WNO&#23558;&#21487;&#24494;&#20998;&#29289;&#29702;&#27714;&#35299;&#22120;&#19982;&#23567;&#27874;&#31070;&#32463;&#25805;&#20316;&#31526;(WNO)&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#20854;&#20013;WNO&#30340;&#20316;&#29992;&#26159;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
The well-known governing physics in science and engineering is often based on certain assumptions and approximations. Therefore, analyses and designs carried out based on these equations are also approximate. The emergence of data-driven models has, to a certain degree, addressed this challenge; however, the purely data-driven models often (a) lack interpretability, (b) are data-hungry, and (c) do not generalize beyond the training window. Operator learning has recently been proposed as a potential alternative to address the aforementioned challenges; however, the challenges are still persistent. We here argue that one of the possible solutions resides in data-physics fusion, where the data-driven model is used to correct/identify the missing physics. To that end, we propose a novel Differentiable Physics Augmented Wavelet Neural Operator (DPA-WNO). The proposed DPA-WNO blends a differentiable physics solver with the Wavelet Neural Operator (WNO), where the role of WNO is to model the 
&lt;/p&gt;</description></item><item><title>&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#31070;&#32463;&#32553;&#25918;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#37327;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#19968;&#33268;&#24130;&#24459;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.15123</link><description>&lt;p&gt;
&#25581;&#31034;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Uncovering Neural Scaling Laws in Molecular Representation Learning. (arXiv:2309.15123v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15123
&lt;/p&gt;
&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#31070;&#32463;&#32553;&#25918;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#37327;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#19968;&#33268;&#24130;&#24459;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#65288;MRL&#65289;&#24050;&#32463;&#25104;&#20026;&#33647;&#29289;&#21644;&#26448;&#26009;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22312;&#34394;&#25311;&#31579;&#36873;&#21644;&#21453;&#21521;&#35774;&#35745;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#34429;&#28982;&#23545;&#20110;&#20998;&#23376;&#34920;&#31034;&#30340;&#25968;&#25454;&#25968;&#37327;&#21644;&#36136;&#37327;&#23545;MRL&#30340;&#24433;&#21709;&#30340;&#27169;&#22411;&#20013;&#24515;&#25216;&#26415;&#30340;&#25512;&#36827;&#24341;&#36215;&#20102;&#22823;&#37327;&#20851;&#27880;&#65292;&#20294;&#22312;&#36825;&#20010;&#39046;&#22495;&#20869;&#65292;&#23545;&#20110;&#36825;&#20004;&#20010;&#22240;&#32032;&#30340;&#24433;&#21709;&#36824;&#19981;&#23436;&#20840;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#20013;&#24515;&#30340;&#35282;&#24230;&#28145;&#20837;&#30740;&#31350;&#20102;MRL&#30340;&#31070;&#32463;&#32553;&#25918;&#34892;&#20026;&#65292;&#32771;&#23519;&#20102;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#65288;1&#65289;&#25968;&#25454;&#27169;&#24577;&#65292;&#65288;2&#65289;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#65288;3&#65289;&#39044;&#35757;&#32451;&#30340;&#20316;&#29992;&#65292;&#21644;&#65288;4&#65289;&#27169;&#22411;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;&#25968;&#25454;&#37327;&#21644;MRL&#24615;&#33021;&#20043;&#38388;&#30340;&#19968;&#33268;&#30340;&#24130;&#24459;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;&#20026;&#20102;&#25361;&#25112;&#36825;&#20123;&#32553;&#25918;&#23450;&#24459;&#65292;&#25105;&#20204;&#23558;&#19971;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#20462;&#21098;&#31574;&#30053;&#24212;&#29992;&#20110;&#20998;&#23376;&#25968;&#25454;&#24182;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25913;&#21892;&#23398;&#20064;&#25928;&#29575;&#30340;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity. Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency. To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the im
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#26041;&#24335;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;1D&#20449;&#21495;&#21644;2D&#20449;&#21495;&#65288;&#22270;&#20687;&#65289;&#39044;&#27979;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#31181;&#23398;&#20064;&#26041;&#24335;&#19979;&#65292;&#24182;&#19981;&#38656;&#35201;&#29306;&#29298;&#20934;&#30830;&#24615;&#26469;&#33719;&#24471;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.13752</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#25552;&#39640;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness of Deep Convolutional Neural Networks via Multiresolution Learning. (arXiv:2309.13752v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#26041;&#24335;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;1D&#20449;&#21495;&#21644;2D&#20449;&#21495;&#65288;&#22270;&#20687;&#65289;&#39044;&#27979;&#38382;&#39064;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#36825;&#31181;&#23398;&#20064;&#26041;&#24335;&#19979;&#65292;&#24182;&#19981;&#38656;&#35201;&#29306;&#29298;&#20934;&#30830;&#24615;&#26469;&#33719;&#24471;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#28145;&#24230;&#23398;&#20064;&#36807;&#31243;&#65292;&#26080;&#35770;&#20351;&#29992;&#21738;&#31181;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#26550;&#26500;&#21644;/&#25110;&#23398;&#20064;&#31639;&#27861;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#21333;&#20998;&#36776;&#29575;&#35757;&#32451;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#65292;&#24182;&#34920;&#26126;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;1D&#20449;&#21495;&#21644;2D&#20449;&#21495;&#65288;&#22270;&#20687;&#65289;&#39044;&#27979;&#38382;&#39064;&#30340;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22122;&#22768;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#20197;&#21450;&#23567;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23637;&#31034;&#36825;&#31181;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#36890;&#36807;&#22810;&#20998;&#36776;&#29575;&#23398;&#20064;&#65292;&#19981;&#38656;&#35201;&#29306;&#29298;&#26631;&#20934;&#20934;&#30830;&#24615;&#26469;&#33719;&#24471;&#40065;&#26834;&#24615;&#65292;&#36825;&#19982;&#20256;&#32479;&#30340;&#21333;&#20998;&#36776;&#29575;&#23398;&#20064;&#35774;&#32622;&#30340;&#35266;&#23519;&#32467;&#26524;&#24688;&#24688;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current learning process of deep learning, regardless of any deep neural network (DNN) architecture and/or learning algorithm used, is essentially a single resolution training. We explore multiresolution learning and show that multiresolution learning can significantly improve robustness of DNN models for both 1D signal and 2D signal (image) prediction problems. We demonstrate this improvement in terms of both noise and adversarial robustness as well as with small training dataset size. Our results also suggest that it may not be necessary to trade standard accuracy for robustness with multiresolution learning, which is, interestingly, contrary to the observation obtained from the traditional single resolution learning setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26725;&#22359;&#20998;&#35299;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22823;&#35268;&#27169;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23558;&#22823;&#38382;&#39064;&#25286;&#20998;&#20026;&#23567;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.13405</link><description>&lt;p&gt;
&#36890;&#36807;&#26725;&#22359;&#20998;&#35299;&#23398;&#20064;&#22823;&#35268;&#27169;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition. (arXiv:2309.13405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26725;&#22359;&#20998;&#35299;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22823;&#35268;&#27169;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#23558;&#22823;&#38382;&#39064;&#25286;&#20998;&#20026;&#23567;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;&#31639;&#27861;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#22823;&#35268;&#27169;&#30340;MTP$_2$&#39640;&#26031;&#22270;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26725;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25972;&#20010;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#23545;&#38408;&#20540;&#26679;&#26412;&#21327;&#26041;&#24046;&#22270;&#19978;&#30340;&#20960;&#20010;&#36739;&#23567;&#35268;&#27169;&#30340;&#23376;&#38382;&#39064;&#36827;&#34892;&#26725;&#22359;&#20998;&#35299;&#26469;&#31561;&#25928;&#20248;&#21270;&#65292;&#20197;&#21450;&#23545;&#24212;&#20110;&#26725;&#30340;&#26465;&#30446;&#30340;&#19968;&#32452;&#26126;&#30830;&#35299;&#20915;&#26041;&#26696;&#12290;&#20174;&#23454;&#36341;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20010;&#31616;&#21333;&#32780;&#21487;&#35777;&#26126;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#19968;&#20010;&#22823;&#38382;&#39064;&#25286;&#20998;&#20026;&#23567;&#30340;&#21487;&#22788;&#29702;&#30340;&#38382;&#39064;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#26174;&#33879;&#25913;&#36827;&#20102;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#12290;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#35758;&#30340;&#26041;&#27861;&#19982;&#29616;&#26377;&#25216;&#26415;&#22522;&#20934;&#30456;&#27604;&#65292;&#36895;&#24230;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two ($\text{MTP}_2$). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a \emph{bridge-block decomposition} on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to \emph{bridges}. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.
&lt;/p&gt;</description></item><item><title>S-GBDT&#26159;&#19968;&#31181;&#33410;&#20461;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#23398;&#20064;&#22120;&#65292;&#21033;&#29992;&#20102;&#22235;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21253;&#25324;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#26356;&#32039;&#23494;&#35745;&#31639;&#21644;&#25972;&#21512;&#20010;&#20307;R&#233;nyi&#28388;&#27874;&#22120;&#20197;&#23398;&#20064;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#25968;&#25454;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.12041</link><description>&lt;p&gt;
S-GBDT: &#33410;&#20461;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
S-GBDT: Frugal Differentially Private Gradient Boosting Decision Trees. (arXiv:2309.12041v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12041
&lt;/p&gt;
&lt;p&gt;
S-GBDT&#26159;&#19968;&#31181;&#33410;&#20461;&#30340;&#24046;&#20998;&#38544;&#31169;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#23398;&#20064;&#22120;&#65292;&#21033;&#29992;&#20102;&#22235;&#31181;&#25216;&#26415;&#26469;&#25913;&#21892;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21253;&#25324;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#26356;&#32039;&#23494;&#35745;&#31639;&#21644;&#25972;&#21512;&#20010;&#20307;R&#233;nyi&#28388;&#27874;&#22120;&#20197;&#23398;&#20064;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#25968;&#25454;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;(GBDT)&#22312;&#34920;&#26684;&#25968;&#25454;(&#22914;&#20154;&#21475;&#26222;&#26597;&#25968;&#25454;&#25110;&#21307;&#30103;&#20803;&#25968;&#25454;)&#20013;&#20855;&#26377;&#24456;&#24378;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#28508;&#21147;&#65306;&#32463;&#20856;&#30340;GBDT&#23398;&#20064;&#22120;&#21487;&#20197;&#20174;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#38750;&#32447;&#24615;&#27169;&#24335;&#12290;&#21487;&#35777;&#26126;&#20855;&#26377;&#38544;&#31169;&#24615;&#36136;&#30340;&#24403;&#21069;&#26041;&#27861;&#26159;&#24046;&#20998;&#38544;&#31169;&#65292;&#35813;&#26041;&#27861;&#35201;&#27714;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#26377;&#38480;&#19988;&#21487;&#21542;&#35748;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;GBDT&#23398;&#20064;&#22120;&#65292;&#24182;&#21033;&#29992;&#22235;&#31181;&#20027;&#35201;&#25216;&#26415;&#26469;&#25913;&#21892;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;(1)&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22122;&#22768;&#32553;&#25918;&#26041;&#27861;&#65292;&#26356;&#32039;&#23494;&#22320;&#35745;&#31639;&#20102;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#20915;&#31574;&#26641;&#21494;&#23376;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#20174;&#32780;&#23548;&#33268;&#22122;&#22768;&#30340;&#26399;&#26395;&#19982;&#25968;&#25454;&#28857;&#25968;&#37327;n&#30340;&#27604;&#20363;&#20026;$O(1/n)$&#65292;&#20854;&#20013;n&#20026;&#25968;&#25454;&#28857;&#25968;&#37327;&#12290;(2)&#25105;&#20204;&#23558;&#20010;&#20307;R&#233;nyi&#28388;&#27874;&#22120;&#25972;&#21512;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#20197;&#20174;&#22312;&#36845;&#20195;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#25968;&#25454;&#28857;&#20013;&#23398;&#20064;&#65292;&#36825;&#21487;&#33021;&#26159;&#29420;&#31435;&#20110;&#20852;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving learning of gradient boosting decision trees (GBDT) has the potential for strong utility-privacy tradeoffs for tabular data, such as census data or medical meta data: classical GBDT learners can extract non-linear patterns from small sized datasets. The state-of-the-art notion for provable privacy-properties is differential privacy, which requires that the impact of single data points is limited and deniable. We introduce a novel differentially private GBDT learner and utilize four main techniques to improve the utility-privacy tradeoff. (1) We use an improved noise scaling approach with tighter accounting of privacy leakage of a decision tree leaf compared to prior work, resulting in noise that in expectation scales with $O(1/n)$, for $n$ data points. (2) We integrate individual R\'enyi filters to our method to learn from data points that have been underutilized during an iterative training process, which -- potentially of independent interest -- results in a natura
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#22238;&#24402;&#38382;&#39064;&#20013;&#22788;&#29702;&#21152;&#27861;&#26080;&#24847;&#35782;&#22122;&#22768;&#30340;&#31639;&#27861;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#26469;&#20934;&#30830;&#22320;&#24674;&#22797;&#21442;&#25968;&#21521;&#37327;&#65292;&#20351;&#24471;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#20540;&#30340;&#35823;&#24046;&#23613;&#21487;&#33021;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.11657</link><description>&lt;p&gt;
GLM&#22238;&#24402;&#19982;&#26080;&#24847;&#35782;&#25968;&#25454;&#25439;&#22351;
&lt;/p&gt;
&lt;p&gt;
GLM Regression with Oblivious Corruptions. (arXiv:2309.11657v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11657
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#22238;&#24402;&#38382;&#39064;&#20013;&#22788;&#29702;&#21152;&#27861;&#26080;&#24847;&#35782;&#22122;&#22768;&#30340;&#31639;&#27861;&#12290;&#31639;&#27861;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26679;&#26412;&#35775;&#38382;&#26469;&#20934;&#30830;&#22320;&#24674;&#22797;&#21442;&#25968;&#21521;&#37327;&#65292;&#20351;&#24471;&#27169;&#22411;&#30340;&#39044;&#27979;&#19982;&#30495;&#23454;&#20540;&#30340;&#35823;&#24046;&#23613;&#21487;&#33021;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLMs&#65289;&#30340;&#22238;&#24402;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#21152;&#27861;&#26080;&#24847;&#35782;&#22122;&#22768;&#30340;&#31532;&#19968;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#26377;&#26679;&#26412;&#35775;&#38382;&#21040;&#30340;&#20363;&#23376;$(x, y)$&#65292;&#20854;&#20013;$y$&#26159;$g(w^* \cdot x)$&#30340;&#24102;&#22122;&#22768;&#27979;&#37327;&#20540;&#12290;&#29305;&#21035;&#22320;&#65292;&#22122;&#22768;&#26631;&#31614;&#30340;&#24418;&#24335;&#20026;$y = g(w^* \cdot x) + \xi + \epsilon$&#65292;&#20854;&#20013;$\xi$&#26159;&#19982;$x$&#29420;&#31435;&#25277;&#21462;&#30340;&#26080;&#24847;&#35782;&#22122;&#22768;&#28385;&#36275;$\Pr[\xi = 0] \geq o(1)$&#65292;&#32780;$\epsilon \sim \mathcal N(0, \sigma^2)$&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20934;&#30830;&#22320;&#24674;&#22797;&#19968;&#20010;&#21442;&#25968;&#21521;&#37327;$w$&#65292;&#20351;&#24471;&#20989;&#25968;$g(w \cdot x)$&#19982;&#30495;&#23454;&#20540;$g(w^* \cdot x)$&#30456;&#27604;&#20855;&#26377;&#20219;&#24847;&#23567;&#30340;&#35823;&#24046;&#65292;&#32780;&#19981;&#26159;&#19982;&#22122;&#22768;&#27979;&#37327;$y$&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#26368;&#19968;&#33324;&#30340;&#19982;&#20998;&#24067;&#26080;&#20851;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#35299;&#21487;&#33021;&#29978;&#33267;&#19981;&#21487;&#35782;&#21035;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36820;&#22238;&#19968;&#20010;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#22914;&#26524;&#23427;&#26159;&#21487;&#35782;&#21035;&#30340;&#65292;&#21542;&#21017;
&lt;/p&gt;
&lt;p&gt;
We demonstrate the first algorithms for the problem of regression for generalized linear models (GLMs) in the presence of additive oblivious noise. We assume we have sample access to examples $(x, y)$ where $y$ is a noisy measurement of $g(w^* \cdot x)$. In particular, \new{the noisy labels are of the form} $y = g(w^* \cdot x) + \xi + \epsilon$, where $\xi$ is the oblivious noise drawn independently of $x$ \new{and satisfies} $\Pr[\xi = 0] \geq o(1)$, and $\epsilon \sim \mathcal N(0, \sigma^2)$. Our goal is to accurately recover a \new{parameter vector $w$ such that the} function $g(w \cdot x)$ \new{has} arbitrarily small error when compared to the true values $g(w^* \cdot x)$, rather than the noisy measurements $y$.  We present an algorithm that tackles \new{this} problem in its most general distribution-independent setting, where the solution may not \new{even} be identifiable. \new{Our} algorithm returns \new{an accurate estimate of} the solution if it is identifiable, and otherwise
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;&#26469;&#36991;&#20813;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#19981;&#38656;&#35201;&#30340;&#28857;&#65292;&#26041;&#27861;&#26159;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;&#30446;&#26631;&#28857;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;</title><link>http://arxiv.org/abs/2309.11475</link><description>&lt;p&gt;
&#36991;&#20813;&#22312;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#20986;&#29616;&#19981;&#38656;&#35201;&#30340;&#28857;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;
&lt;/p&gt;
&lt;p&gt;
Multiplying poles to avoid unwanted points in root finding and optimization. (arXiv:2309.11475v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#26497;&#28857;&#26469;&#36991;&#20813;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#19981;&#38656;&#35201;&#30340;&#28857;&#65292;&#26041;&#27861;&#26159;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;&#30446;&#26631;&#28857;&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26681;&#26597;&#25214;&#21644;&#20248;&#21270;&#20013;&#65292;&#23384;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#26080;&#27861;&#20445;&#35777;&#33258;&#24049;&#36873;&#25321;&#30340;&#26041;&#27861;&#26500;&#36896;&#30340;&#24207;&#21015;&#25910;&#25947;&#20110;&#19968;&#20010;&#38381;&#38598;&#21512;A&#65288;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24182;&#19981;&#20551;&#35774;A&#26377;&#20854;&#20182;&#38468;&#21152;&#23646;&#24615;&#65292;&#22914;&#20984;&#24615;&#25110;&#36830;&#36890;&#24615;&#65289;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#19968;&#20010;&#26426;&#21046;&#26469;&#36991;&#20813;&#22312;&#31639;&#27861;&#30340;&#19979;&#19968;&#27425;&#36816;&#34892;&#20013;&#20877;&#27425;&#36935;&#21040;&#36825;&#20010;&#28857;z*&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65306;&#25105;&#20204;&#23558;&#20195;&#20215;&#20989;&#25968;&#38500;&#20197;&#21040;A&#30340;&#36317;&#31163;&#20989;&#25968;&#30340;&#36866;&#24403;&#24130;&#12290;&#36825;&#20010;&#24819;&#27861;&#21463;&#21040;&#20102;&#22312;&#19968;&#32500;&#20989;&#25968;&#20013;&#23581;&#35797;&#25214;&#21040;&#25152;&#26377;&#26681;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#39318;&#20808;&#35299;&#37322;&#20102;&#22312;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#24688;&#22909;&#20026;0&#30340;&#24773;&#20917;&#19979;&#36825;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#28982;&#21518;&#35299;&#37322;&#20102;&#22914;&#26524;&#26368;&#23567;&#20540;&#19981;&#20026;&#38646;&#35813;&#22914;&#20309;&#36827;&#34892;&#65288;&#21516;&#26102;&#20801;&#35768;&#27491;&#30340;&#26368;&#23567;&#20540;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In root finding and optimization, there are many cases where there is a closed set $A$ one does not the sequence constructed by one's favourite method will converge to A (here, we do not assume extra properties on $A$ such as being convex or connected). For example, if one wants to find roots, and one chooses initial points in the basin of attraction for 1 root $x^*$ (a fact which one may not know before hand), then one will always end up in that root. In this case, one would like to have a mechanism to avoid this point $z^*$ in the next runs of one's algorithm.  In this paper, we propose a new method aiming to achieve this: we divide the cost function by an appropriate power of the distance function to $A$. This idea is inspired by how one would try to find all roots of a function in 1 variable. We first explain the heuristic for this method in the case where the minimum of the cost function is exactly 0, and then explain how to proceed if the minimum is non-zero (allowing both positi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.10003</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#29575;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A novel approach to measuring patent claim scope based on probabilities obtained from (large) language models. (arXiv:2309.10003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#33258;&#20449;&#24687;&#26469;&#27979;&#37327;&#19987;&#21033;&#26435;&#35201;&#27714;&#33539;&#22260;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35745;&#31639;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#21644;&#33258;&#20449;&#24687;&#26469;&#35780;&#20272;&#35201;&#27714;&#30340;&#20449;&#24687;&#37327;&#65292;&#36827;&#32780;&#21453;&#26144;&#20986;&#35201;&#27714;&#30340;&#33539;&#22260;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#33539;&#22260;&#27979;&#37327;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#33539;&#22260;&#24230;&#37327;&#31616;&#21270;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#12290;&#27492;&#26041;&#27861;&#22312;&#20061;&#20010;&#31995;&#21015;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#21508;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19987;&#21033;&#26435;&#35201;&#27714;&#30340;&#33539;&#22260;&#27979;&#37327;&#20026;&#35813;&#35201;&#27714;&#25152;&#21253;&#21547;&#30340;&#33258;&#20449;&#24687;&#30340;&#20498;&#25968;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#20449;&#24687;&#35770;&#65292;&#22522;&#20110;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#32597;&#35265;&#30340;&#27010;&#24565;&#27604;&#24179;&#24120;&#30340;&#27010;&#24565;&#26356;&#20855;&#20449;&#24687;&#37327;&#65292;&#22240;&#20026;&#23427;&#26356;&#20196;&#20154;&#24778;&#35766;&#12290;&#33258;&#20449;&#24687;&#26159;&#20174;&#35813;&#35201;&#27714;&#30340;&#21457;&#29983;&#27010;&#29575;&#35745;&#31639;&#24471;&#20986;&#30340;&#65292;&#20854;&#20013;&#27010;&#29575;&#26159;&#26681;&#25454;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30340;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20116;&#20010;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#26368;&#31616;&#21333;&#30340;&#27169;&#22411;&#65288;&#27599;&#20010;&#21333;&#35789;&#25110;&#23383;&#31526;&#22343;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#21462;&#65289;&#21040;&#20013;&#31561;&#27169;&#22411;&#65288;&#20351;&#29992;&#24179;&#22343;&#35789;&#25110;&#23383;&#31526;&#39057;&#29575;&#65289;&#65292;&#20877;&#21040;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT2&#65289;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26368;&#31616;&#21333;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#33539;&#22260;&#24230;&#37327;&#20943;&#23569;&#20026;&#21333;&#35789;&#25110;&#23383;&#31526;&#35745;&#25968;&#30340;&#20498;&#25968;&#65292;&#36825;&#26159;&#20808;&#21069;&#20316;&#21697;&#20013;&#24050;&#32463;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#20061;&#20010;&#31995;&#21015;&#30340;&#38024;&#23545;&#19981;&#21516;&#21457;&#26126;&#30340;&#19987;&#21033;&#26435;&#35201;&#27714;&#65292;&#20854;&#20013;&#27599;&#20010;&#31995;&#21015;&#30340;&#35201;&#27714;&#33539;&#22260;&#36880;&#28176;&#20943;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes to measure the scope of a patent claim as the reciprocal of the self-information contained in this claim. Grounded in information theory, this approach is based on the assumption that a rare concept is more informative than a usual concept, inasmuch as it is more surprising. The self-information is calculated from the probability of occurrence of that claim, where the probability is calculated in accordance with a language model. Five language models are considered, ranging from the simplest models (each word or character is drawn from a uniform distribution) to intermediate models (using average word or character frequencies), to a large language model (GPT2). Interestingly, the simplest language models reduce the scope measure to the reciprocal of the word or character count, a metric already used in previous works. Application is made to nine series of patent claims directed to distinct inventions, where the claims in each series have a gradually decreasing scope.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#23454;&#29616;&#25163;&#20013;&#29289;&#20307;&#36890;&#29992;&#26059;&#36716;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#27169;&#25311;&#25512;&#26029;&#29289;&#20307;&#30340;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.09979</link><description>&lt;p&gt;
&#20855;&#22791;&#35270;&#35273;&#21644;&#35302;&#35273;&#30340;&#25163;&#20013;&#29289;&#20307;&#36890;&#29992;&#26059;&#36716;
&lt;/p&gt;
&lt;p&gt;
General In-Hand Object Rotation with Vision and Touch. (arXiv:2309.09979v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#23454;&#29616;&#25163;&#20013;&#29289;&#20307;&#36890;&#29992;&#26059;&#36716;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#27169;&#25311;&#25512;&#26029;&#29289;&#20307;&#30340;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#65292;&#24182;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;RotateIt&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#27169;&#24577;&#24863;&#30693;&#36755;&#20837;&#65292;&#20351;&#25351;&#23574;&#33021;&#22815;&#22312;&#22810;&#20010;&#36724;&#19978;&#36827;&#34892;&#29289;&#20307;&#26059;&#36716;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21487;&#20197;&#33719;&#21462;&#29289;&#20307;&#30340;&#30495;&#23454;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#20854;&#31616;&#21270;&#20026;&#22312;&#30495;&#23454;&#20294;&#22122;&#22768;&#24178;&#25200;&#19979;&#30340;&#27169;&#25311;&#35302;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#36755;&#20837;&#12290;&#36825;&#20123;&#22810;&#27169;&#24577;&#36755;&#20837;&#36890;&#36807;&#35270;&#35273;&#35302;&#35273;&#21464;&#25442;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20351;&#24471;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#21487;&#20197;&#36827;&#34892;&#29289;&#20307;&#24418;&#29366;&#21644;&#29289;&#29702;&#23646;&#24615;&#30340;&#22312;&#32447;&#25512;&#26029;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#30340;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#35270;&#35273;&#21644;&#35302;&#35273;&#24863;&#30693;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce RotateIt, a system that enables fingertip-based object rotation along multiple axes by leveraging multimodal sensory inputs. Our system is trained in simulation, where it has access to ground-truth object shapes and physical properties. Then we distill it to operate on realistic yet noisy simulated visuotactile and proprioceptive sensory inputs. These multimodal inputs are fused via a visuotactile transformer, enabling online inference of object shapes and physical properties during deployment. We show significant performance improvements over prior methods and the importance of visual and tactile sensing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#38750;&#31283;&#24577;&#28418;&#31227;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26694;&#26550;&#35774;&#35745;&#65292;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20845;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#20102;&#19971;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20004;&#31181;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.09175</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#36827;&#34892;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imbalanced Data Stream Classification using Dynamic Ensemble Selection. (arXiv:2309.09175v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#38750;&#31283;&#24577;&#28418;&#31227;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26694;&#26550;&#35774;&#35745;&#65292;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20845;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#65292;&#24182;&#19988;&#36890;&#36807;&#35780;&#20272;&#20102;&#19971;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20004;&#31181;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27969;&#25968;&#25454;&#20998;&#31867;&#38754;&#20020;&#30528;&#27010;&#24565;&#28418;&#31227;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#23545;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20998;&#31867;&#19981;&#27491;&#30830;&#12290;&#27492;&#22806;&#65292;&#22810;&#20010;&#31867;&#21035;&#30340;&#37325;&#21472;&#31561;&#20854;&#20182;&#22240;&#32032;&#38480;&#21046;&#20102;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#38750;&#31283;&#24577;&#28418;&#31227;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#27969;&#36827;&#34892;&#20998;&#31867;&#26694;&#26550;&#35774;&#35745;&#65292;&#37319;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20845;&#20010;&#20154;&#24037;&#29983;&#25104;&#30340;&#25968;&#25454;&#27969;&#65292;&#36825;&#20123;&#25968;&#25454;&#27969;&#20855;&#26377;&#19981;&#21516;&#30340;&#19981;&#24179;&#34913;&#27604;&#20363;&#65292;&#24182;&#19988;&#21253;&#21547;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#27599;&#20010;&#25968;&#25454;&#27969;&#30001;200&#20010;500&#20010;&#23545;&#35937;&#30340;&#22359;&#32452;&#25104;&#65292;&#27599;&#20010;&#23545;&#35937;&#30001;&#20843;&#20010;&#29305;&#24449;&#25551;&#36848;&#65292;&#24182;&#19988;&#21253;&#21547;&#20116;&#20010;&#27010;&#24565;&#28418;&#31227;&#12290;&#32771;&#34385;&#20102;&#19971;&#31181;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#20004;&#31181;&#21160;&#24577;&#38598;&#25104;&#36873;&#25321;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern streaming data categorization faces significant challenges from concept drift and class imbalanced data. This negatively impacts the output of the classifier, leading to improper classification. Furthermore, other factors such as the overlapping of multiple classes limit the extent of the correctness of the output. This work proposes a novel framework for integrating data pre-processing and dynamic ensemble selection, by formulating the classification framework for the nonstationary drifting imbalanced data stream, which employs the data pre-processing and dynamic ensemble selection techniques. The proposed framework was evaluated using six artificially generated data streams with differing imbalance ratios in combination with two different types of concept drifts. Each stream is composed of 200 chunks of 500 objects described by eight features and contains five concept drifts. Seven pre-processing techniques and two dynamic ensemble selection methods were considered. According 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2309.07461</link><description>&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#26816;&#27979;&#26410;&#30693;&#25915;&#20987;: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07461
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#29289;&#32852;&#32593;&#29615;&#22659;&#23450;&#21046;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#24320;&#25918;&#38598;&#20998;&#31867;&#22120;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#34920;&#31034;&#21644;&#22534;&#21472;&#23376;&#32858;&#31867;&#25216;&#26415;&#26469;&#35782;&#21035;&#26410;&#30693;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#35774;&#22791;&#22312;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#36825;&#24341;&#20837;&#20102;&#20114;&#32852;&#30340;&#26102;&#20195;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24378;&#22823;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23433;&#20840;&#31995;&#32479;&#26159;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#35270;&#35282;&#35774;&#35745;&#30340;&#65292;&#24448;&#24448;&#38754;&#20020;&#19982;&#19981;&#26029;&#21457;&#23637;&#30340;&#23041;&#32961;&#29615;&#22659;&#20013;&#26032;&#30340;&#12289;&#38476;&#29983;&#30340;&#25915;&#20987;&#30456;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#29289;&#32852;&#32593;&#29615;&#22659;&#19979;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#20013;&#30340;&#24320;&#25918;&#38598;&#35782;&#21035;&#65288;OSR&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#22270;&#20687;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20174;&#32593;&#32476;&#27969;&#37327;&#20013;&#25552;&#21462;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#38598;&#25104;&#20102;&#22534;&#21472;&#21644;&#23376;&#32858;&#31867;&#25216;&#26415;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#30340;&#33391;&#24615;&#34892;&#20026;&#65292;&#23454;&#29616;&#23545;&#26410;&#30693;&#25915;&#20987;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread integration of Internet of Things (IoT) devices across all facets of life has ushered in an era of interconnectedness, creating new avenues for cybersecurity challenges and underscoring the need for robust intrusion detection systems. However, traditional security systems are designed with a closed-world perspective and often face challenges in dealing with the ever-evolving threat landscape, where new and unfamiliar attacks are constantly emerging. In this paper, we introduce a framework aimed at mitigating the open set recognition (OSR) problem in the realm of Network Intrusion Detection Systems (NIDS) tailored for IoT environments. Our framework capitalizes on image-based representations of packet-level data, extracting spatial and temporal patterns from network traffic. Additionally, we integrate stacking and sub-clustering techniques, enabling the identification of unknown attacks by effectively modeling the complex and diverse nature of benign behavior. The empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;Harmonic-NAS&#65292;&#36890;&#36807;&#20004;&#23618;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.06612</link><description>&lt;p&gt;
&#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;(Harmonic-NAS)
&lt;/p&gt;
&lt;p&gt;
Harmonic-NAS: Hardware-Aware Multimodal Neural Architecture Search on Resource-constrained Devices. (arXiv:2309.06612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30828;&#20214;&#24863;&#30693;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#30340;&#22810;&#27169;&#24577;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;Harmonic-NAS&#65292;&#36890;&#36807;&#20004;&#23618;&#20248;&#21270;&#23454;&#29616;&#20102;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22810;&#27169;&#24577;&#31070;&#32463;&#32593;&#32476;&#65288;MM-NN&#65289;&#30340;&#20852;&#36259;&#28608;&#22686;&#65292;&#36825;&#24402;&#21151;&#20110;&#23427;&#20204;&#26377;&#25928;&#22788;&#29702;&#21644;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;MM-NN&#20013;&#65292;&#20351;&#29992;&#36866;&#24403;&#30340;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#29305;&#23450;&#30340;&#34701;&#21512;&#32593;&#32476;&#20174;&#22810;&#20010;&#27169;&#24577;&#25552;&#21462;&#21644;&#34701;&#21512;&#29305;&#24449;&#12290;&#23613;&#31649;&#36825;&#26377;&#21161;&#20110;&#22686;&#24378;&#22810;&#27169;&#24577;&#20449;&#24687;&#34920;&#36798;&#65292;&#20294;&#35774;&#35745;&#27492;&#31867;&#32593;&#32476;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#12290;&#23427;&#38656;&#35201;&#35843;&#25972;&#21333;&#27169;&#24577;&#39592;&#24178;&#30340;&#26550;&#26500;&#21442;&#25968;&#65292;&#36873;&#25321;&#34701;&#21512;&#28857;&#65292;&#24182;&#36873;&#25321;&#34701;&#21512;&#30340;&#25805;&#20316;&#12290;&#27492;&#22806;&#65292;&#22810;&#27169;&#24615;&#20154;&#24037;&#26234;&#33021;&#27491;&#22312;&#25104;&#20026;&#29289;&#32852;&#32593;&#31995;&#32479;&#20013;&#30340;&#19968;&#31181;&#23574;&#31471;&#36873;&#25321;&#65292;&#20854;&#20013;&#25512;&#26029;&#24310;&#36831;&#21644;&#33021;&#37327;&#28040;&#32791;&#26159;&#38500;&#20934;&#30830;&#24615;&#22806;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harmonic-NAS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#20855;&#26377;&#30828;&#20214;&#24863;&#30693;&#30340;&#21333;&#27169;&#24577;&#39592;&#24178;&#21644;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of interest surrounding Multimodal Neural Networks (MM-NN) is attributed to their ability to effectively process and integrate information from diverse data sources. In MM-NN, features are extracted and fused from multiple modalities using adequate unimodal backbones and specific fusion networks. Although this helps strengthen the multimodal information representation, designing such networks is labor-intensive. It requires tuning the architectural parameters of the unimodal backbones, choosing the fusing point, and selecting the operations for fusion. Furthermore, multimodality AI is emerging as a cutting-edge option in Internet of Things (IoT) systems where inference latency and energy consumption are critical metrics in addition to accuracy. In this paper, we propose Harmonic-NAS, a framework for the joint optimization of unimodal backbones and multimodal fusion networks with hardware awareness on resource-constrained devices. Harmonic-NAS involves a two-tier optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#26080;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20960;&#20309;&#20316;&#20026;&#20849;&#20139;&#34920;&#31034;&#65292;&#23558;&#35270;&#35273;&#19982;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#20174;RGBD&#35270;&#39057;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21018;&#24615;&#21644;&#20984;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#25913;&#36827;&#20102;&#36319;&#36394;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2309.05832</link><description>&lt;p&gt;
&#23454;&#20363;&#26080;&#20851;&#30340;&#20960;&#20309;&#19982;&#25509;&#35302;&#21160;&#21147;&#23398;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instance-Agnostic Geometry and Contact Dynamics Learning. (arXiv:2309.05832v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#26080;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20960;&#20309;&#20316;&#20026;&#20849;&#20139;&#34920;&#31034;&#65292;&#23558;&#35270;&#35273;&#19982;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#20174;RGBD&#35270;&#39057;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21018;&#24615;&#21644;&#20984;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#25913;&#36827;&#20102;&#36319;&#36394;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20363;&#26080;&#20851;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20960;&#20309;&#20316;&#20026;&#20849;&#20139;&#34920;&#31034;&#65292;&#23558;&#35270;&#35273;&#19982;&#21160;&#21147;&#23398;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#23398;&#20064;&#24418;&#29366;&#12289;&#23039;&#24577;&#36712;&#36857;&#21644;&#29289;&#29702;&#24615;&#36136;&#12290;&#19982;&#35768;&#22810;&#25509;&#35302;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20174;RGBD&#35270;&#39057;&#20013;&#23398;&#20064;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#23646;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#31867;&#21035;&#32423;&#21035;&#25110;&#23454;&#20363;&#32423;&#21035;&#30340;&#24418;&#29366;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#31995;&#32479;BundleSDF&#19982;&#21160;&#21147;&#23398;&#31995;&#32479;ContactNets&#38598;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#24490;&#29615;&#35757;&#32451;&#31649;&#36947;&#65292;&#20351;&#29992;&#21160;&#21147;&#23398;&#27169;&#22359;&#30340;&#36755;&#20986;&#36890;&#36807;&#36879;&#35270;&#37325;&#25237;&#24433;&#26469;&#25913;&#36827;&#35270;&#35273;&#27169;&#22359;&#30340;&#23039;&#24577;&#21644;&#20960;&#20309;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#23398;&#20064;&#21018;&#24615;&#21644;&#20984;&#29289;&#20307;&#30340;&#20960;&#20309;&#21644;&#21160;&#21147;&#23398;&#65292;&#24182;&#25913;&#36827;&#20102;&#24403;&#21069;&#30340;&#36319;&#36394;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents an instance-agnostic learning framework that fuses vision with dynamics to simultaneously learn shape, pose trajectories and physical properties via the use of geometry as a shared representation. Unlike many contact learning approaches that assume motion capture input and a known shape prior for the collision model, our proposed framework learns an object's geometric and dynamic properties from RGBD video, without requiring either category-level or instance-level shape priors. We integrate a vision system, BundleSDF, with a dynamics system, ContactNets and propose a cyclic training pipeline to use the output from the dynamics module to refine the poses and the geometry from the vision module, using perspective reprojection. Experiments demonstrate our framework's ability to learn the geometry and dynamics of rigid and convex objects and improve upon the current tracking framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;6G&#29615;&#22659;&#20013;&#25903;&#25345;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#26550;&#26500;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#32858;&#21512;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.05525</link><description>&lt;p&gt;
6G&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#21457;&#23637;&#65306;&#22522;&#20110;&#22270;&#20998;&#26512;&#30340;&#21487;&#20449;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Advancing Federated Learning in 6G: A Trusted Architecture with Graph-based Analysis. (arXiv:2309.05525v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05525
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;6G&#29615;&#22659;&#20013;&#25903;&#25345;&#32852;&#37030;&#23398;&#20064;&#30340;&#21487;&#20449;&#26550;&#26500;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#20102;&#38544;&#31169;&#21644;&#23433;&#20840;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#32858;&#21512;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#21407;&#29983;AI&#25903;&#25345;&#38598;&#25104;&#21040;&#32593;&#32476;&#26550;&#26500;&#20013;&#26159;6G&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#28508;&#22312;&#30340;&#33539;&#24335;&#20986;&#29616;&#65292;&#21487;&#20197;&#22312;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#21327;&#35843;&#19979;&#65292;&#20419;&#36827;&#20998;&#25955;&#30340;AI&#27169;&#22411;&#35757;&#32451;&#36328;&#36234;&#22810;&#31181;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#22312;6G&#29615;&#22659;&#19979;&#65292;&#26377;&#20960;&#20010;&#25361;&#25112;&#38459;&#30861;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#24694;&#24847;&#25915;&#20987;&#21644;&#23545;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#30340;&#38544;&#31169;&#30417;&#35270;&#65292;&#20197;&#21450;&#38598;&#20013;&#21270;&#30340;&#32570;&#28857;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;FL&#30340;&#21487;&#20449;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21033;&#29992;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#65288;DLT&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#29305;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#21516;&#24577;&#21152;&#23494;&#30340;&#39044;&#22788;&#29702;&#23618;&#29992;&#20110;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#27169;&#22411;&#65292;&#20445;&#25252;&#20010;&#20307;&#27169;&#22411;&#30340;&#38544;&#31169;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#39044;&#22788;&#29702;&#23618;&#20013;&#23458;&#25143;&#31471;&#21644;&#33410;&#28857;&#20043;&#38388;&#30340;&#20998;&#24067;&#24335;&#24615;&#36136;&#21644;&#22270;&#32467;&#26500;&#65292;&#21033;&#29992;GNN&#26469;&#35782;&#21035;&#24322;&#24120;&#26412;&#22320;&#27169;&#22411;&#65292;&#22686;&#24378;&#31995;&#32479;&#23433;&#20840;&#24615;&#12290;&#31532;&#19977;&#65292;&#21033;&#29992;DLT&#26469;&#32500;&#25252;&#32593;&#32476;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#20449;&#20219;&#20851;&#31995;&#65292;&#24182;&#25552;&#20379;&#23433;&#20840;&#30340;&#20849;&#35782;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating native AI support into the network architecture is an essential objective of 6G. Federated Learning (FL) emerges as a potential paradigm, facilitating decentralized AI model training across a diverse range of devices under the coordination of a central server. However, several challenges hinder its wide application in the 6G context, such as malicious attacks and privacy snooping on local model updates, and centralization pitfalls. This work proposes a trusted architecture for supporting FL, which utilizes Distributed Ledger Technology (DLT) and Graph Neural Network (GNN), including three key features. First, a pre-processing layer employing homomorphic encryption is incorporated to securely aggregate local models, preserving the privacy of individual models. Second, given the distributed nature and graph structure between clients and nodes in the pre-processing layer, GNN is leveraged to identify abnormal local models, enhancing system security. Third, DLT is utilized to d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.05516</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#31526;&#21495;&#26799;&#24230;&#19979;&#38477;&#20248;&#21270;LLMs&#37327;&#21270;&#20013;&#30340;&#26435;&#37325;&#33293;&#20837;
&lt;/p&gt;
&lt;p&gt;
Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;SignRound&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#36827;&#34892;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#37327;&#21270;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25191;&#34892;&#35821;&#35328;&#30456;&#20851;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#21644;&#23384;&#20648;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;3&#20301;&#21644;4&#20301;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#65292;&#24050;&#32463;&#25104;&#20026;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#38543;&#30528;&#20301;&#25968;&#30340;&#20943;&#23569;&#65292;&#37327;&#21270;&#32593;&#26684;&#21464;&#24471;&#26356;&#21152;&#23485;&#27867;&#65292;&#20174;&#32780;&#24378;&#35843;&#20102;&#19978;&#19979;&#33293;&#20837;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#28155;&#21152;&#25200;&#21160;&#32454;&#35843;&#19978;&#19979;&#33293;&#20837;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21463;&#21046;&#20110;&#36825;&#20123;&#25200;&#21160;&#30340;&#31934;&#30830;&#19988;&#26377;&#38480;&#30340;&#36793;&#30028;&#65292;&#21482;&#26377;&#25913;&#21464;&#33293;&#20837;&#20540;&#30340;&#38408;&#20540;&#25165;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#39640;&#25928;&#30340;&#20248;&#21270;&#26435;&#37325;&#33293;&#20837;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;SignRound&#65292;&#23427;&#28041;&#21450;&#20351;&#29992;&#26377;&#31526;&#21495;&#26799;&#24230;&#30340;&#36731;&#37327;&#32423;&#20998;&#22359;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.03847</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#29289;&#21487;&#20197;&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;(DP)&#32422;&#26463;&#19979;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#20351;&#29992;$\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$&#20010;&#26679;&#26412;&#21363;&#21487;&#22312;&#28385;&#36275;$(\varepsilon, \delta)$-DP&#30340;&#26465;&#20214;&#19979;&#20272;&#35745;$k$&#20010;&#39640;&#26031;&#28151;&#21512;&#29289;&#65292;&#20351;&#20854;&#36798;&#21040;&#24635;&#21464;&#24046;&#36317;&#31163;$\alpha$&#12290;&#36825;&#26159;&#35813;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#38480;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#20063;&#26377;&#29992;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#20998;&#24067;&#31867;&#65288;&#27604;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#26159;&#65288;1&#65289;&#21487;&#21015;&#34920;&#35793;&#30721;&#30340;&#24182;&#19988;&#65288;2&#65289;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#8220;&#23616;&#37096;&#23567;&#8221;&#35206;&#30422;[ BKSW19]&#65292;&#21017;&#20854;&#28151;&#21512;&#29289;&#31867;&#26159;&#31169;&#23494;&#21487;&#23398;&#20064;&#30340;&#12290;&#35777;&#26126;&#32469;&#36807;&#20102;&#19968;&#20010;&#24050;&#30693;&#38556;&#30861;&#65292;&#34920;&#26126;&#19982;&#39640;&#26031;&#20998;&#24067;&#19981;&#21516;&#65292;GMMs&#19981;&#20855;&#26377;&#23616;&#37096;&#23567;&#30340;&#35206;&#30422;[AAL21]&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2308.16900</link><description>&lt;p&gt;
&#23398;&#20064;&#21697;&#21619;&#65306;&#19968;&#20010;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Learning to Taste: A Multimodal Wine Dataset. (arXiv:2308.16900v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16900
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#65292;&#25913;&#36827;&#20102;&#21475;&#21619;&#20998;&#31867;&#65292;&#24182;&#19982;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#22810;&#27169;&#24577;&#33889;&#33796;&#37202;&#25968;&#25454;&#38598;WineSensed&#65292;&#29992;&#20110;&#30740;&#31350;&#35270;&#35273;&#24863;&#30693;&#12289;&#35821;&#35328;&#21644;&#21475;&#24863;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;89.7&#19975;&#24352;&#33889;&#33796;&#37202;&#26631;&#31614;&#22270;&#29255;&#21644;82.4&#19975;&#26465;&#26469;&#33258;Vivino&#24179;&#21488;&#30340;&#33889;&#33796;&#37202;&#35780;&#35770;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#36229;&#36807;35&#19975;&#20010;&#29420;&#29305;&#30340;&#24180;&#20221;&#65292;&#38468;&#24102;&#20102;&#24180;&#20221;&#12289;&#20135;&#22320;&#12289;&#35780;&#20998;&#12289;&#37202;&#31934;&#21547;&#37327;&#12289;&#20215;&#26684;&#21644;&#33889;&#33796;&#32452;&#25104;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#21697;&#37202;&#23454;&#39564;&#23545;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#20102;&#32454;&#31890;&#24230;&#30340;&#21475;&#21619;&#27880;&#37322;&#65292;&#20849;&#26377;256&#21517;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#26681;&#25454;&#21475;&#21619;&#30340;&#30456;&#20284;&#24615;&#23545;&#33889;&#33796;&#37202;&#36827;&#34892;&#25490;&#24207;&#65292;&#24471;&#21040;&#20102;&#36229;&#36807;5&#21315;&#20010;&#37197;&#23545;&#30340;&#21475;&#21619;&#36317;&#31163;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#32500;&#27010;&#24565;&#23884;&#20837;&#31639;&#27861;&#65292;&#23558;&#20154;&#31867;&#32463;&#39564;&#19982;&#33258;&#21160;&#26426;&#22120;&#30456;&#20284;&#24230;&#26680;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20010;&#20849;&#20139;&#30340;&#27010;&#24565;&#23884;&#20837;&#31354;&#38388;&#22312;&#31895;&#31890;&#24230;&#21475;&#21619;&#20998;&#31867;&#65288;&#37202;&#31934;&#21547;&#37327;&#65292;&#22269;&#23478;&#65292;&#33889;&#33796;&#65292;&#20215;&#26684;&#65292;&#35780;&#20998;&#65289;&#19978;&#25913;&#36827;&#65292;&#24182;&#19988;&#19982;&#22797;&#26434;&#30340;&#20154;&#31867;&#21475;&#21619;&#30693;&#35273;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique vintages, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification (alcohol percentage, country, grape, price, rating) and aligns with the intricate human perception of flavor.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...</title><link>http://arxiv.org/abs/2308.13978</link><description>&lt;p&gt;
&#29702;&#35299;&#22522;&#20110;QUBO&#30340;&#21704;&#23494;&#39039;&#20989;&#25968;&#22312;&#22270;&#19978;&#30340;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#20351;&#29992;&#65306;&#20197;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20026;&#20363;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Understanding the Usage of QUBO-based Hamiltonian Function in Combinatorial Optimization over Graphs: A Discussion Using Max Cut (MC) Problem. (arXiv:2308.13978v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13978
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22270;&#19978;&#22522;&#20110;QUBO&#20844;&#24335;&#30340;&#26368;&#22823;&#20999;&#21106;&#38382;&#39064;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#26469;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#20844;&#24335;&#24418;&#24335;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#36827;&#21046;&#20248;&#21270;&#65288;QUBO&#65289;&#26159;&#19968;&#31181;&#24191;&#20041;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21508;&#31181;NP&#22256;&#38590;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#21464;&#37327;&#30340;&#24418;&#24335;&#12290;&#21704;&#23494;&#39039;&#20989;&#25968;&#32463;&#24120;&#29992;&#20110;&#24418;&#25104;QUBO&#38382;&#39064;&#65292;&#20854;&#20013;&#23427;&#22312;&#20248;&#21270;&#30340;&#19978;&#19979;&#25991;&#20013;&#34987;&#29992;&#20316;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#33539;&#24335;&#21644;&#21704;&#23494;&#39039;&#20989;&#25968;&#35299;&#20915;QUBO&#20844;&#24335;&#20013;&#30340;&#22270;&#19978;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#20449;&#24687;&#20256;&#36882;&#26550;&#26500;&#22312;&#33410;&#28857;&#20043;&#38388;&#20256;&#36882;&#20449;&#24687;&#12290;&#25105;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;&#19977;&#31181;&#20844;&#24335;&#65292;Monty-Carlo Tree Search with GNN-based RL&#65288;MCTS-GNN&#65289;&#12289;DQN with GNN-based RL&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#30340;&#36890;&#29992;GNN&#65288;GRL&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Quadratic Unconstrained Binary Optimization (QUBO) is a generic technique to model various NP-hard combinatorial optimization problems in the form of binary variables. The Hamiltonian function is often used to formulate QUBO problems where it is used as the objective function in the context of optimization. In this study, we investigate how reinforcement learning-based (RL) paradigms with the presence of the Hamiltonian function can address combinatorial optimization problems over graphs in QUBO formulations. We use Graph Neural Network (GNN) as the message-passing architecture to convey the information among the nodes. We have centered our discussion on QUBO formulated Max-Cut problem but the intuitions can be extended to any QUBO supported canonical NP-Hard combinatorial optimization problems. We mainly investigate three formulations, Monty-Carlo Tree Search with GNN-based RL (MCTS-GNN), DQN with GNN-based RL, and a generic GNN with attention-based RL (GRL). Our findings state that i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#29983;&#23384;&#26102;&#38271;&#26469;&#23450;&#20041;&#26102;&#24207;&#21306;&#21035;&#24230;&#65292;&#26500;&#24314;&#36127;&#26679;&#26412;&#23545;&#12290;</title><link>http://arxiv.org/abs/2308.13104</link><description>&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#29983;&#23384;&#20998;&#26512;&#30340;&#23545;&#27604;&#23398;&#20064;&#65306;&#26500;&#24314;&#26102;&#24207;&#21306;&#21035;&#24230;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Temporal Distinctiveness for Survival Analysis in Electronic Health Records. (arXiv:2308.13104v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#29983;&#23384;&#20998;&#26512;&#26694;&#26550;&#65292;&#20805;&#20998;&#21033;&#29992;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#29983;&#23384;&#26102;&#38271;&#26469;&#23450;&#20041;&#26102;&#24207;&#21306;&#21035;&#24230;&#65292;&#26500;&#24314;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#23384;&#20998;&#26512;&#22312;&#35768;&#22810;&#21307;&#30103;&#20915;&#31574;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#24739;&#32773;&#21307;&#30103;&#36807;&#31243;&#20013;&#24863;&#20852;&#36259;&#20107;&#20214;&#30340;&#39118;&#38505;&#39044;&#27979;&#12290;&#37492;&#20110;&#25968;&#25454;&#25130;&#26029;&#30340;&#23384;&#22312;&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#29983;&#23384;&#20998;&#26512;&#26041;&#27861;&#26159;&#24378;&#21046;&#20445;&#25345;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#20043;&#38388;&#30340;&#26102;&#24207;&#19968;&#33268;&#24615;&#65292;&#26088;&#22312;&#21033;&#29992;&#25130;&#26029;&#21069;&#30340;&#26102;&#38388;&#38388;&#38548;&#20316;&#20026;&#37096;&#20998;&#35266;&#27979;&#30340;&#20107;&#20214;&#21457;&#29983;&#26102;&#38388;&#26631;&#31614;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#23613;&#31649;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#37319;&#29992;&#25490;&#24207;&#26041;&#27861;&#26469;&#36861;&#27714;&#25490;&#24207;&#30446;&#26631;&#65292;&#20294;&#23578;&#26410;&#23545;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#28145;&#20837;&#25506;&#32034;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#27604;&#26469;&#23398;&#20064;&#26377;&#21306;&#21035;&#24615;&#30340;&#23884;&#20837;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26412;&#20307;&#24863;&#30693;&#30340;&#26102;&#24207;&#23545;&#27604;&#29983;&#23384;&#20998;&#26512;&#65288;OTCSurv&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#25130;&#26029;&#21644;&#35266;&#23519;&#25968;&#25454;&#30340;&#29983;&#23384;&#26102;&#38271;&#23450;&#20041;&#26102;&#24207;&#21306;&#21035;&#24230;&#65292;&#24182;&#26500;&#24314;&#36127;&#26679;&#26412;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Survival analysis plays a crucial role in many healthcare decisions, where the risk prediction for the events of interest can support an informative outlook for a patient's medical journey. Given the existence of data censoring, an effective way of survival analysis is to enforce the pairwise temporal concordance between censored and observed data, aiming to utilize the time interval before censoring as partially observed time-to-event labels for supervised learning. Although existing studies mostly employed ranking methods to pursue an ordering objective, contrastive methods which learn a discriminative embedding by having data contrast against each other, have not been explored thoroughly for survival analysis. Therefore, in this paper, we propose a novel Ontology-aware Temporality-based Contrastive Survival (OTCSurv) analysis framework that utilizes survival durations from both censored and observed data to define temporal distinctiveness and construct negative sample pairs with adj
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10425</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#65292;&#26222;&#36890;Transformer&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#20132;&#36890;&#39044;&#27979;&#24615;&#33021;&#65292;&#36890;&#36807;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#23454;&#29616;&#20102;&#20248;&#31168;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20854;&#26680;&#24515;&#29942;&#39048;&#22312;&#20110;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#20132;&#36890;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#20855;&#26377;&#22797;&#26434;&#26550;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#36935;&#21040;&#20102;&#24615;&#33021;&#25910;&#30410;&#38477;&#20302;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#30340;&#26032;&#32452;&#20214;&#65292;&#21487;&#20197;&#22312;&#26222;&#36890;&#30340;Transformer&#20013;&#33719;&#24471;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;Spatio-Temporal Adaptive Embedding transformer&#65288;STAEformer&#65289;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26102;&#31354;&#33258;&#36866;&#24212;&#23884;&#20837;&#36890;&#36807;&#26377;&#25928;&#25429;&#25417;&#20132;&#36890;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20869;&#22312;&#26102;&#31354;&#20851;&#31995;&#21644;&#26102;&#38388;&#20449;&#24687;&#65292;&#22312;&#20132;&#36890;&#39044;&#27979;&#20013;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
&lt;/p&gt;</description></item><item><title>Kairos&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#33539;&#22260;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#12289;&#26102;&#25928;&#24615;&#21644;&#25915;&#20987;&#37325;&#24314;&#32500;&#24230;&#35201;&#27714;&#30340;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.05034</link><description>&lt;p&gt;
Kairos: &#20351;&#29992;&#25972;&#20307;&#31995;&#32479;&#28335;&#28304;&#36827;&#34892;&#23454;&#29992;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Kairos: : Practical Intrusion Detection and Investigation using Whole-system Provenance. (arXiv:2308.05034v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05034
&lt;/p&gt;
&lt;p&gt;
Kairos&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#33539;&#22260;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#12289;&#26102;&#25928;&#24615;&#21644;&#25915;&#20987;&#37325;&#24314;&#32500;&#24230;&#35201;&#27714;&#30340;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#21644;&#35843;&#26597;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28335;&#28304;&#22270;&#26159;&#25551;&#36848;&#31995;&#32479;&#25191;&#34892;&#21382;&#21490;&#30340;&#32467;&#26500;&#21270;&#23457;&#35745;&#26085;&#24535;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21508;&#31181;&#25216;&#26415;&#26469;&#20998;&#26512;&#28335;&#28304;&#22270;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#20027;&#26426;&#20837;&#20405;&#26816;&#27979;&#65292;&#29305;&#21035;&#20851;&#27880;&#39640;&#32423;&#25345;&#20037;&#24615;&#23041;&#32961;&#12290;&#36890;&#36807;&#30740;&#31350;&#20854;&#35774;&#35745;&#25991;&#26723;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#24120;&#35265;&#32500;&#24230;&#65292;&#25512;&#21160;&#28335;&#28304;&#20026;&#22522;&#30784;&#30340;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;PIDS&#65289;&#30340;&#21457;&#23637;&#65306;&#33539;&#22260;&#65288;PIDS&#33021;&#21542;&#26816;&#27979;&#36328;&#24212;&#29992;&#36793;&#30028;&#28183;&#36879;&#30340;&#29616;&#20195;&#25915;&#20987;&#65311;&#65289;&#12289;&#25915;&#20987;&#19981;&#21487;&#30693;&#24615;&#65288;PIDS&#33021;&#21542;&#22312;&#27809;&#26377;&#25915;&#20987;&#29305;&#24449;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#26032;&#22411;&#25915;&#20987;&#65311;&#65289;&#12289;&#26102;&#25928;&#24615;&#65288;PIDS&#33021;&#21542;&#39640;&#25928;&#30417;&#35270;&#20027;&#26426;&#31995;&#32479;&#36816;&#34892;&#65311;&#65289;&#21644;&#25915;&#20987;&#37325;&#24314;&#65288;PIDS&#33021;&#21542;&#20174;&#22823;&#22411;&#28335;&#28304;&#22270;&#20013;&#25552;&#28860;&#25915;&#20987;&#27963;&#21160;&#65292;&#20197;&#20415;&#31995;&#32479;&#31649;&#29702;&#21592;&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#24182;&#36805;&#36895;&#24212;&#23545;&#31995;&#32479;&#20837;&#20405;&#65311;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KAIROS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21516;&#26102;&#28385;&#36275;&#25152;&#26377;&#22235;&#20010;&#32500;&#24230;&#35201;&#27714;&#30340;PIDS&#65292;&#32780;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#33021;&#20570;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provenance graphs are structured audit logs that describe the history of a system's execution. Recent studies have explored a variety of techniques to analyze provenance graphs for automated host intrusion detection, focusing particularly on advanced persistent threats. Sifting through their design documents, we identify four common dimensions that drive the development of provenance-based intrusion detection systems (PIDSes): scope (can PIDSes detect modern attacks that infiltrate across application boundaries?), attack agnosticity (can PIDSes detect novel attacks without a priori knowledge of attack characteristics?), timeliness (can PIDSes efficiently monitor host systems as they run?), and attack reconstruction (can PIDSes distill attack activity from large provenance graphs so that sysadmins can easily understand and quickly respond to system intrusion?). We present KAIROS, the first PIDS that simultaneously satisfies the desiderata in all four dimensions, whereas existing approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04412</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Invariant Learning with Randomized Linear Classifiers. (arXiv:2308.04412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#36827;&#34892;&#27010;&#29575;&#19981;&#21464;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#65292;&#35774;&#35745;&#20102;&#33021;&#21516;&#26102;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21644;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26082;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#21448;&#33021;&#20445;&#25345;&#20219;&#21153;&#24050;&#30693;&#19981;&#21464;&#24615;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#22312;&#19981;&#21464;&#24615;&#21644;&#35745;&#31639;&#25110;&#20869;&#23384;&#36164;&#28304;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#38543;&#26426;&#24615;&#35774;&#35745;&#26082;&#20855;&#34920;&#36798;&#33021;&#21147;&#21448;&#20855;&#19981;&#21464;&#24615;&#20294;&#20351;&#29992;&#26356;&#23569;&#36164;&#28304;&#30340;&#27169;&#22411;&#12290;&#21463;&#38543;&#26426;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#25509;&#21463;&#27010;&#29575;&#21270;&#30340;&#26222;&#36941;&#36924;&#36817;&#21644;&#19981;&#21464;&#24615;&#21487;&#20197;&#20943;&#23569;&#36164;&#28304;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;&#38543;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120; (RLCs) &#30340;&#20108;&#20998;&#31867;&#27169;&#22411;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#21442;&#25968;&#21644;&#26679;&#26412;&#22823;&#23567;&#30340;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;RLCs &#21487;&#20197;&#20197;&#39640;&#27010;&#29575;&#36924;&#36817;&#20219;&#20309;&#65288;&#24179;&#28369;&#65289;&#20989;&#25968;&#65292;&#24182;&#20445;&#25345;&#23545;&#32039;&#33268;&#32676;&#21464;&#25442;&#30340;&#19981;&#21464;&#24615;&#12290;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#21487;&#39564;&#35777;&#22320;&#27010;&#29575;&#19981;&#21464;&#30340; RLCs&#65292;&#29992;&#20110;&#38598;&#21512;&#12289;&#22270;&#21644;&#29699;&#24418;&#25968;&#25454;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#23454;&#29616;&#27010;&#29575;&#19981;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing models that are both expressive and preserve known invariances of tasks is an increasingly hard problem. Existing solutions tradeoff invariance for computational or memory resources. In this work, we show how to leverage randomness and design models that are both expressive and invariant but use less resources. Inspired by randomized algorithms, our key insight is that accepting probabilistic notions of universal approximation and invariance can reduce our resource requirements. More specifically, we propose a class of binary classification models called Randomized Linear Classifiers (RLCs). We give parameter and sample size conditions in which RLCs can, with high probability, approximate any (smooth) function while preserving invariance to compact group transformations. Leveraging this result, we design three RLCs that are provably probabilistic invariant for classification tasks over sets, graphs, and spherical data. We show how these models can achieve probabilistic invari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#21253;&#25324;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#19981;&#36275;&#12289;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#19981;&#36866;&#24212;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#21487;&#20449;&#24230;&#32593;&#32476;&#30340;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03666</link><description>&lt;p&gt;
&#26550;&#36215;&#21487;&#20449;&#24230;&#19982;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#30340;&#26725;&#26753;&#65306;&#19968;&#31181;&#25506;&#32034;&#24615;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging Trustworthiness and Open-World Learning: An Exploratory Neural Approach for Enhancing Interpretability, Generalization, and Robustness. (arXiv:2308.03666v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#30340;&#21487;&#20449;&#24230;&#38382;&#39064;&#65292;&#21253;&#25324;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#19981;&#36275;&#12289;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#19981;&#36275;&#21644;&#19981;&#36866;&#24212;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#38382;&#39064;&#65292;&#20197;&#25552;&#39640;&#21487;&#20449;&#24230;&#32593;&#32476;&#30340;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#32553;&#23567;&#26426;&#22120;&#26234;&#33021;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36890;&#36807;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25105;&#20204;&#24517;&#39035;&#35748;&#35782;&#21040;&#21487;&#20449;&#24230;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#37325;&#35201;&#24615;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#23545;&#27599;&#20010;&#20154;&#37117;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23384;&#22312;&#20960;&#20010;&#25361;&#25112;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20449;&#20219;&#21361;&#26426;&#65306;1&#65289;&#23545;&#39044;&#27979;&#32467;&#26524;&#30340;&#35299;&#37322;&#19981;&#36275;&#65307;2&#65289;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#19981;&#36275;&#65307;3&#65289;&#23545;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#36866;&#24212;&#33021;&#21147;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31243;&#24207;&#65292;&#29992;&#20110;&#26550;&#36215;&#21487;&#20449;&#24230;&#19982;&#24320;&#25918;&#19990;&#30028;&#23398;&#20064;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#20174;&#21333;&#27169;&#24577;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#22330;&#26223;&#65292;&#20197;&#20379;&#35835;&#32773;&#20351;&#29992;&#12290;1&#65289;&#20026;&#20102;&#22686;&#24378;&#35774;&#35745;&#32423;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#21046;&#20102;&#20855;&#26377;&#29305;&#23450;&#29289;&#29702;&#21547;&#20041;&#30340;&#21487;&#20449;&#32593;&#32476;&#65307;2&#65289;&#28982;&#21518;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#23398;&#20064;&#27491;&#21017;&#21270;&#22120;&#35774;&#35745;&#29615;&#22659;&#31119;&#31049;&#20219;&#21153;&#25509;&#21475;&#65292;&#20197;&#25913;&#21892;&#21487;&#20449;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As researchers strive to narrow the gap between machine intelligence and human through the development of artificial intelligence technologies, it is imperative that we recognize the critical importance of trustworthiness in open-world, which has become ubiquitous in all aspects of daily life for everyone. However, several challenges may create a crisis of trust in current artificial intelligence systems that need to be bridged: 1) Insufficient explanation of predictive results; 2) Inadequate generalization for learning models; 3) Poor adaptability to uncertain environments. Consequently, we explore a neural program to bridge trustworthiness and open-world learning, extending from single-modal to multi-modal scenarios for readers. 1) To enhance design-level interpretability, we first customize trustworthy networks with specific physical meanings; 2) We then design environmental well-being task-interfaces via flexible learning regularizers for improving the generalization of trustworthy
&lt;/p&gt;</description></item><item><title>&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#25439;&#36716;&#25442;&#21644;&#36807;&#37327;&#39118;&#38505;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#25439;&#36716;&#25442;&#30340;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#21028;&#26029;&#32473;&#23450;&#36716;&#25442;&#26159;&#21542;&#26159;&#26080;&#25439;&#30340;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;delta-&#26080;&#25439;&#36716;&#25442;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20123;&#30740;&#31350;&#22312;&#20998;&#31867;&#12289;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#25237;&#36164;&#32452;&#21512;&#31574;&#30053;&#31561;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2307.16735</link><description>&lt;p&gt;
&#26080;&#25439;&#36716;&#25442;&#21644;&#32479;&#35745;&#25512;&#26029;&#20013;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lossless Transformations and Excess Risk Bounds in Statistical Inference. (arXiv:2307.16735v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16735
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#25512;&#26029;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#25439;&#36716;&#25442;&#21644;&#36807;&#37327;&#39118;&#38505;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#25439;&#36716;&#25442;&#30340;&#29305;&#24449;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#21028;&#26029;&#32473;&#23450;&#36716;&#25442;&#26159;&#21542;&#26159;&#26080;&#25439;&#30340;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;delta-&#26080;&#25439;&#36716;&#25442;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#20805;&#20998;&#26465;&#20214;&#12290;&#36825;&#20123;&#30740;&#31350;&#22312;&#20998;&#31867;&#12289;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#25237;&#36164;&#32452;&#21512;&#31574;&#30053;&#31561;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#32479;&#35745;&#25512;&#26029;&#20013;&#30340;&#36807;&#37327;&#26368;&#23567;&#39118;&#38505;&#65292;&#23450;&#20041;&#20026;&#20174;&#35266;&#27979;&#21040;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#20272;&#35745;&#38543;&#26426;&#21464;&#37327;&#30340;&#26368;&#23567;&#26399;&#26395;&#25439;&#22833;&#19982;&#20174;&#29305;&#24449;&#21521;&#37327;&#30340;&#36716;&#25442;&#65288;&#32479;&#35745;&#37327;&#65289;&#20013;&#20272;&#35745;&#30456;&#21516;&#38543;&#26426;&#21464;&#37327;&#30340;&#26368;&#23567;&#26399;&#26395;&#25439;&#22833;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#25551;&#36848;&#20102;&#26080;&#25439;&#36716;&#25442;&#65288;&#21363;&#23545;&#20110;&#25152;&#26377;&#25439;&#22833;&#20989;&#25968;&#65292;&#36807;&#37327;&#39118;&#38505;&#20026;&#38646;&#30340;&#36716;&#25442;&#65289;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#20551;&#35774;&#36827;&#34892;&#20998;&#21306;&#26816;&#39564;&#30340;&#32479;&#35745;&#37327;&#65292;&#29992;&#20110;&#21028;&#26029;&#32473;&#23450;&#36716;&#25442;&#26159;&#21542;&#20026;&#26080;&#25439;&#36716;&#25442;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;i.i.d.&#25968;&#25454;&#65292;&#35813;&#26816;&#39564;&#26159;&#24378;&#19968;&#33268;&#30340;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#20449;&#24687;&#29702;&#35770;&#32473;&#20986;&#20102;&#36807;&#37327;&#39118;&#38505;&#30340;&#19978;&#30028;&#65292;&#35813;&#19978;&#30028;&#22312;&#30456;&#24403;&#19968;&#33324;&#30340;&#25439;&#22833;&#20989;&#25968;&#31867;&#19978;&#37117;&#26159;&#19968;&#33268;&#30340;&#12290;&#22522;&#20110;&#36825;&#20123;&#30028;&#38480;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;delta-&#26080;&#25439;&#36716;&#25442;&#8221;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#32473;&#23450;&#36716;&#25442;&#26222;&#36941;&#26159;delta-&#26080;&#25439;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#35813;&#30740;&#31350;&#22312;&#20998;&#31867;&#12289;&#38750;&#21442;&#25968;&#22238;&#24402;&#12289;&#25237;&#36164;&#32452;&#21512;&#31574;&#30053;&#31561;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the excess minimum risk in statistical inference, defined as the difference between the minimum expected loss in estimating a random variable from an observed feature vector and the minimum expected loss in estimating the same random variable from a transformation (statistic) of the feature vector. After characterizing lossless transformations, i.e., transformations for which the excess risk is zero for all loss functions, we construct a partitioning test statistic for the hypothesis that a given transformation is lossless and show that for i.i.d. data the test is strongly consistent. More generally, we develop information-theoretic upper bounds on the excess risk that uniformly hold over fairly general classes of loss functions. Based on these bounds, we introduce the notion of a delta-lossless transformation and give sufficient conditions for a given transformation to be universally delta-lossless. Applications to classification, nonparametric regression, portfolio strategie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.12499</link><description>&lt;p&gt;
AdvDiff:&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion Models. (arXiv:2307.12499v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;AdvDiff&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#25915;&#20987;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#23427;&#20204;&#23545;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#36896;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#32469;&#36807;&#38450;&#24481;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25915;&#20987;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#29702;&#35770;&#19978;&#26080;&#27861;&#35777;&#26126;&#65292;&#22240;&#27492;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65288;&#22914;ImageNet&#65289;&#19978;&#36890;&#36807;&#24341;&#20837;&#23545;&#25239;&#30446;&#26631;&#29983;&#25104;&#30340;&#20363;&#23376;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;AdvDiff&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24341;&#23548;&#25216;&#26415;&#65292;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#36870;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#25239;&#37319;&#26679;&#12290;&#36825;&#20004;&#31181;&#25216;&#26415;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#30446;&#26631;&#20998;&#31867;&#22120;&#26799;&#24230;&#38598;&#25104;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#38750;&#24120;&#26377;&#25928;&#21644;&#31283;&#23450;&#12290;&#22312;MNIST&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AdvDiff&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#36924;&#30495;&#30340;&#23545;&#25239;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unrestricted adversarial attacks present a serious threat to deep learning models and adversarial defense techniques. They pose severe security problems for deep learning applications because they can effectively bypass defense mechanisms. However, previous attack methods often utilize Generative Adversarial Networks (GANs), which are not theoretically provable and thus generate unrealistic examples by incorporating adversarial objectives, especially for large-scale datasets like ImageNet. In this paper, we propose a new method, called AdvDiff, to generate unrestricted adversarial examples with diffusion models. We design two novel adversarial guidance techniques to conduct adversarial sampling in the reverse generation process of diffusion models. These two techniques are effective and stable to generate high-quality, realistic adversarial examples by integrating gradients of the target classifier interpretably. Experimental results on MNIST and ImageNet datasets demonstrate that AdvD
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#20013;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#39640;&#25928;&#22320;&#27169;&#25311;&#20855;&#26377;&#38750;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#35768;&#22810;&#20855;&#26377;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08079</link><description>&lt;p&gt;
&#36890;&#36807;&#21464;&#20998;&#33258;&#21160; &#32534;&#30721;&#22120;&#23454;&#29616;&#28789;&#27963;&#39640;&#25928;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Flexible and efficient spatial extremes emulation via variational autoencoders. (arXiv:2307.08079v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#20013;&#65292;&#21487;&#20197;&#28789;&#27963;&#12289;&#39640;&#25928;&#22320;&#27169;&#25311;&#20855;&#26377;&#38750;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#26497;&#31471;&#20107;&#20214;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26102;&#38388;&#25928;&#29575;&#21644;&#24615;&#33021;&#19978;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#21644;&#35768;&#22810;&#20855;&#26377;&#24179;&#31283;&#30456;&#20851;&#24615;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#23614;&#20381;&#36182;&#32467;&#26500;&#65292;&#36825;&#31181;&#32467;&#26500;&#26080;&#27861;&#20351;&#29992;&#20256;&#32479;&#30340;&#39640;&#26031;&#36807;&#31243;&#26469;&#25551;&#36848;&#12290;&#26356;&#28789;&#27963;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292; &#22914;&#39640;&#26031;&#23610;&#24230;&#28151;&#21512;&#27169;&#22411;&#21644;&#21333;&#31449;&#28857;&#35843;&#33410;&#27169;&#22411;&#65292;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#26497;&#31471;&#20381;&#36182;&#24615;&#36136;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#25311;&#21512;&#21644;&#27169;&#25311;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#65292;&#20855;&#26377;&#28789;&#27963;&#21644;&#38750;&#24179;&#31283;&#30340;&#30456;&#20851;&#24615;&#23646;&#24615;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120; (extVAE) &#30340;&#32534;&#30721;-&#35299;&#30721;&#32467;&#26500;&#20013;&#12290; extVAE &#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#27169;&#25311;&#22120;&#65292;&#23545;&#28508;&#22312;&#30340;&#26426;&#21046;&#27169;&#22411;&#36755;&#20986;&#29366;&#24577;&#30340;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#20135;&#29983;&#20855;&#26377;&#19982;&#36755;&#20837;&#30456;&#21516;&#23646;&#24615;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#26159;&#22312;&#23614;&#37096;&#21306;&#22495;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;extVAE&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377; &#24179;&#31283;&#30456;&#20851;&#24615;&#32467;&#26500;&#30340;&#35768;&#22810;&#31354;&#38388;&#26497;&#31471;&#20540;&#27169;&#22411;&#20013;&#34920;&#29616; &#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models such as Gaussian scale mixtures and single-station conditioning models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from. In this paper, we develop a new spatial extremes model that has flexible and non-stationary dependence properties, and we integrate it in the encoding-decoding structure of a variational autoencoder (extVAE). The extVAE can be used as a spatio-temporal emulator that characterizes the distribution of potential mechanistic model output states and produces outputs that have the same properties as the inputs, especially in the tail. Through extensive simulation studies, we show that our extVAE is vastly more time-efficient than traditional Bayesian inference while also outperforming many spatial extremes models with a stationary dependence str
&lt;/p&gt;</description></item><item><title>TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2307.06822</link><description>&lt;p&gt;
TinyMetaFed: &#39640;&#25928;&#30340;&#29992;&#20110;TinyML&#30340;&#32852;&#37030;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed: Efficient Federated Meta-Learning for TinyML. (arXiv:2307.06822v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06822
&lt;/p&gt;
&lt;p&gt;
TinyMetaFed&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#39640;&#25928;&#32852;&#37030;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21327;&#21516;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#65292;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#33021;&#22815;&#24555;&#36895;&#24494;&#35843;&#65292;&#21516;&#26102;&#23454;&#29616;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tiny Machine Learning (TinyML)&#39046;&#22495;&#22312;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#65288;&#22914;&#24494;&#25511;&#21046;&#22120;&#65289;&#19978;&#23454;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#24494;&#22411;&#35774;&#22791;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#32858;&#21512;&#23427;&#20204;&#30340;&#30693;&#35782;&#26159;&#21542;&#33021;&#22815;&#20351;TinyML&#24212;&#29992;&#21463;&#30410;&#12290;&#32852;&#37030;&#20803;&#23398;&#20064;&#26159;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#31572;&#26696;&#65292;&#22240;&#20026;&#23427;&#35299;&#20915;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#35774;&#22791;&#20043;&#38388;&#30340;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;TinyML&#30828;&#20214;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#36164;&#28304;&#38480;&#21046;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#28304;&#12289;&#38544;&#31169;&#21644;&#36890;&#20449;&#38480;&#21046;&#32780;&#19981;&#23454;&#29992;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TinyMetaFed&#65292;&#19968;&#20010;&#36866;&#29992;&#20110;TinyML&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;TinyMetaFed&#20419;&#36827;&#20102;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#26032;&#35774;&#22791;&#19978;&#24555;&#36895;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#37096;&#20998;&#26412;&#22320;&#37325;&#26500;&#21644;Top-P%&#36873;&#25321;&#24615;&#36890;&#20449;&#25552;&#20379;&#36890;&#20449;&#33410;&#30465;&#21644;&#38544;&#31169;&#20445;&#25252;&#65292;&#20855;&#26377;&#35745;&#31639;&#25928;&#26524;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Tiny Machine Learning (TinyML) has made substantial advancements in democratizing machine learning on low-footprint devices, such as microcontrollers. The prevalence of these miniature devices raises the question of whether aggregating their knowledge can benefit TinyML applications. Federated meta-learning is a promising answer to this question, as it addresses the scarcity of labeled data and heterogeneous data distribution across devices in the real world. However, deploying TinyML hardware faces unique resource constraints, making existing methods impractical due to energy, privacy, and communication limitations. We introduce TinyMetaFed, a model-agnostic meta-learning framework suitable for TinyML. TinyMetaFed facilitates collaborative training of a neural network initialization that can be quickly fine-tuned on new devices. It offers communication savings and privacy protection through partial local reconstruction and Top-P% selective communication, computational eff
&lt;/p&gt;</description></item><item><title>&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2307.04870</link><description>&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#65306;&#22312;&#24369;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04870
&lt;/p&gt;
&lt;p&gt;
&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#27169;&#22411;&#12290;&#23427;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#12289;&#35745;&#31639;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#26080;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#23427;&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#27915;&#33905;&#23431;&#23449;&#31639;&#27861;(OUA)&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#23398;&#20064;&#20998;&#31867;&#26041;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;OUA&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#25968;&#25454;&#25110;&#24369;&#20449;&#21495;&#30340;&#20219;&#20309;&#20551;&#35774;&#12290;&#35813;&#27169;&#22411;&#38750;&#24120;&#36866;&#29992;&#20110;&#27809;&#26377;&#23436;&#20840;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#23545;&#30001;&#24369;&#20449;&#21495;&#25152;&#26500;&#25104;&#30340;&#31354;&#38388;&#30340;&#20960;&#20309;&#35299;&#37322;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;OUA&#22312;&#19968;&#33324;&#30340;&#24369;&#20449;&#21495;&#38598;&#21512;&#19979;&#20855;&#26377;&#28508;&#22312;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#23637;&#31034;&#65292;OUA&#22312;&#24120;&#35265;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26631;&#31614;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;(OKO)&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02245</link><description>&lt;p&gt;
&#20934;&#30830;&#21644;&#26657;&#20934;&#27169;&#22411;&#30340;&#38598;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;(OKO)&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#26631;&#20934;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26102;&#24456;&#38590;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22855;&#25968;-$k$-&#21435;&#38500;&#23398;&#20064;&#65288;OKO&#65289;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#32780;&#19981;&#26159;&#21333;&#20010;&#31034;&#20363;&#30340;&#35823;&#24046;&#26469;&#23454;&#29616;&#12290;&#36825;&#33258;&#28982;&#22320;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#30828;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#24182;&#19988;&#19981;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#26657;&#20934;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;OKO&#36890;&#24120;&#20063;&#33021;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;OKO&#33258;&#28982;&#22320;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;OKO&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;&#35768;&#22810;&#19981;&#21516;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the
&lt;/p&gt;</description></item><item><title>TGB&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26102;&#24577;&#22270;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#21644;&#36793;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#22810;&#31181;&#39046;&#22495;&#30340;&#26102;&#24577;&#22270;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#26377;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#21160;&#24577;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#21487;&#33021;&#27604;&#29616;&#26377;&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01026</link><description>&lt;p&gt;
&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26102;&#24577;&#22270;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Temporal Graph Benchmark for Machine Learning on Temporal Graphs. (arXiv:2307.01026v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01026
&lt;/p&gt;
&lt;p&gt;
TGB&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26102;&#24577;&#22270;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#65292;&#28085;&#30422;&#20102;&#33410;&#28857;&#21644;&#36793;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#23545;&#22810;&#31181;&#39046;&#22495;&#30340;&#26102;&#24577;&#22270;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#26377;&#24040;&#22823;&#24046;&#24322;&#12290;&#22312;&#21160;&#24577;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#21487;&#33021;&#27604;&#29616;&#26377;&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Temporal Graph Benchmark&#65288;TGB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#26102;&#24577;&#22270;&#19978;&#36827;&#34892;&#30495;&#23454;&#12289;&#21487;&#37325;&#29616;&#21644;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#38598;&#21512;&#12290;TGB&#25968;&#25454;&#38598;&#20855;&#26377;&#22823;&#35268;&#27169;&#12289;&#36328;&#24180;&#30340;&#26102;&#38271;&#65292;&#21253;&#25324;&#33410;&#28857;&#21644;&#36793;&#32423;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102;&#31038;&#20132;&#12289;&#36152;&#26131;&#12289;&#20132;&#26131;&#21644;&#20132;&#36890;&#32593;&#32476;&#31561;&#22810;&#31181;&#39046;&#22495;&#12290;&#38024;&#23545;&#36825;&#20004;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#23454;&#38469;&#20351;&#29992;&#26696;&#20363;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;&#25105;&#20204;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#24120;&#35265;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#22312;&#21160;&#24577;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#26041;&#27861;&#24120;&#24120;&#27604;&#29616;&#26377;&#30340;&#26102;&#24577;&#22270;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#30340;&#26102;&#24577;&#22270;&#30740;&#31350;&#24320;&#36767;&#20102;&#26426;&#20250;&#12290;&#26368;&#21518;&#65292;TGB&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#65292;&#29992;&#20110;&#21487;&#37325;&#29616;&#21644;&#21487;&#35775;&#38382;&#30340;&#26102;&#24577;&#22270;&#30740;&#31350;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, inclu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#12289;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#26469;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15891</link><description>&lt;p&gt;
&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#25429;&#25417;&#22810;&#23610;&#24230;&#32447;&#24615;&#36755;&#36816;&#26041;&#31243;&#30340;&#25193;&#25955;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Asymptotic-Preserving Convolutional DeepONets Capture the Diffusive Behavior of the Multiscale Linear Transport Equations. (arXiv:2306.15891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#12289;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#26469;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#65292;&#24182;&#39564;&#35777;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#22411;&#30340;&#28176;&#36817;&#20445;&#25345;&#30340;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65288;APCONs&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#23610;&#24230;&#26102;&#21464;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#20462;&#25913;&#36807;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;&#22522;&#26412;&#29289;&#29702;&#32422;&#26463;DeepONets&#21487;&#33021;&#22312;&#20445;&#25345;&#26399;&#26395;&#30340;&#23439;&#35266;&#34892;&#20026;&#19978;&#34920;&#29616;&#20986;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20351;&#29992;&#28176;&#36817;&#20445;&#25345;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#21463;&#25193;&#25955;&#26041;&#31243;&#20013;&#30340;&#28909;&#26680;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#21367;&#31215;Deep Operator&#32593;&#32476;&#65292;&#23427;&#22312;&#27599;&#20010;&#28388;&#27874;&#22120;&#23618;&#20013;&#37319;&#29992;&#22810;&#20010;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#32780;&#19981;&#26159;&#20840;&#23616;&#28909;&#26680;&#65292;&#24182;&#32467;&#21512;&#27744;&#21270;&#21644;&#28608;&#27963;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;APCON&#26041;&#27861;&#30340;&#21442;&#25968;&#25968;&#37327;&#19982;&#32593;&#26684;&#22823;&#23567;&#26080;&#20851;&#65292;&#24182;&#33021;&#22815;&#25429;&#25417;&#32447;&#24615;&#36755;&#36816;&#38382;&#39064;&#30340;&#25193;&#25955;&#34892;&#20026;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#20363;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce two types of novel Asymptotic-Preserving Convolutional Deep Operator Networks (APCONs) designed to address the multiscale time-dependent linear transport problem. We observe that the vanilla physics-informed DeepONets with modified MLP may exhibit instability in maintaining the desired limiting macroscopic behavior. Therefore, this necessitates the utilization of an asymptotic-preserving loss function. Drawing inspiration from the heat kernel in the diffusion equation, we propose a new architecture called Convolutional Deep Operator Networks, which employ multiple local convolution operations instead of a global heat kernel, along with pooling and activation operations in each filter layer. Our APCON methods possess a parameter count that is independent of the grid size and are capable of capturing the diffusive behavior of the linear transport problem. Finally, we validate the effectiveness of our methods through several numerical examples.
&lt;/p&gt;</description></item><item><title>DynaBench&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#31232;&#30095;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.05805</link><description>&lt;p&gt;
DynaBench: &#20174;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
DynaBench: A benchmark dataset for learning dynamical systems from low-resolution data. (arXiv:2306.05805v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05805
&lt;/p&gt;
&lt;p&gt;
DynaBench&#26159;&#19968;&#20010;&#26032;&#30340;&#27169;&#25311;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#20302;&#20998;&#36776;&#29575;&#30340;&#31232;&#30095;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65292;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#29289;&#29702;&#31995;&#32479;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#39640;&#20998;&#36776;&#29575;&#32593;&#26684;&#32467;&#26500;&#27979;&#37327;&#12290;&#20294;&#26159;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#36825;&#31181;&#31995;&#32479;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#22825;&#27668;&#25968;&#25454;&#65289;&#20381;&#36182;&#20110;&#31232;&#30095;&#20998;&#24067;&#30340;&#27979;&#37327;&#31449;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#25311;&#22522;&#20934;&#25968;&#25454;&#38598;DynaBench&#65292;&#29992;&#20110;&#30452;&#25509;&#20174;&#31232;&#30095;&#25955;&#24067;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#21160;&#21147;&#31995;&#32479;&#65292;&#19981;&#38656;&#35201;&#20808;&#21069;&#20102;&#35299;&#35813;&#26041;&#31243;&#24335;&#12290;&#35813;&#25968;&#25454;&#38598;&#19987;&#27880;&#20110;&#39044;&#27979;&#21160;&#21147;&#31995;&#32479;&#30340;&#28436;&#21464;&#65292;&#20351;&#29992;&#20302;&#20998;&#36776;&#29575;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#27979;&#37327;&#26041;&#24335;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;&#20845;&#20010;&#19981;&#21516;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65292;&#28085;&#30422;&#20102;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#21508;&#31181;&#29289;&#29702;&#31995;&#32479;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#28857;&#20113;&#22788;&#29702;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#31995;&#32479;&#30340;&#28436;&#21464;&#12290;&#35813;&#22522;&#20934;&#25968;&#25454;&#38598;&#21487;&#20197;&#26399;&#26395;&#25104;&#20026;&#19968;&#31181;&#24320;&#31665;&#21363;&#29992;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work on learning physical systems from data has focused on high-resolution grid-structured measurements. However, real-world knowledge of such systems (e.g. weather data) relies on sparsely scattered measuring stations. In this paper, we introduce a novel simulated benchmark dataset, DynaBench, for learning dynamical systems directly from sparsely scattered data without prior knowledge of the equations. The dataset focuses on predicting the evolution of a dynamical system from low-resolution, unstructured measurements. We simulate six different partial differential equations covering a variety of physical systems commonly used in the literature and evaluate several machine learning models, including traditional graph neural networks and point cloud processing models, with the task of predicting the evolution of the system. The proposed benchmark dataset is expected to advance the state of art as an out-of-the-box easy-to-use tool for evaluating models in a setting where only u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20165;&#26377;&#20114;&#26131;&#22122;&#22768;&#26041;&#24046;&#21644;&#26377;&#30028;&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#38750;&#20984;&#30446;&#26631;&#30340;AdaGrad&#31639;&#27861;&#30340;&#31616;&#21333;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#35777;&#26126;&#20013;&#22522;&#20110;&#36741;&#21161;&#20989;&#25968;$\xi$&#65292;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#32039;&#23494;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#20445;&#26799;&#24230;&#33539;&#25968;&#23567;&#20110;$\varepsilon$&#30340;&#36845;&#20195;&#27425;&#25968;&#20026;$\mathcal{O}(\frac{1}{\varepsilon^2})$&#65292;&#24182;&#32771;&#34385;&#20102;&#19968;&#31181;&#23454;&#38469;&#24179;&#28369;&#20551;&#35774;$(L_0,L_1)$-&#24179;&#28369;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.18471</link><description>&lt;p&gt;
AdaGrad&#31639;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;: &#31616;&#26126;&#35777;&#26126;&#21644;&#23485;&#26494;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed Assumptions. (arXiv:2305.18471v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20165;&#26377;&#20114;&#26131;&#22122;&#22768;&#26041;&#24046;&#21644;&#26377;&#30028;&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#38750;&#20984;&#30446;&#26631;&#30340;AdaGrad&#31639;&#27861;&#30340;&#31616;&#21333;&#25910;&#25947;&#24615;&#35777;&#26126;&#65292;&#35777;&#26126;&#20013;&#22522;&#20110;&#36741;&#21161;&#20989;&#25968;$\xi$&#65292;&#27604;&#29616;&#26377;&#32467;&#26524;&#26356;&#32039;&#23494;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#30830;&#20445;&#26799;&#24230;&#33539;&#25968;&#23567;&#20110;$\varepsilon$&#30340;&#36845;&#20195;&#27425;&#25968;&#20026;$\mathcal{O}(\frac{1}{\varepsilon^2})$&#65292;&#24182;&#32771;&#34385;&#20102;&#19968;&#31181;&#23454;&#38469;&#24179;&#28369;&#20551;&#35774;$(L_0,L_1)$-&#24179;&#28369;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#20165;&#26377;&#20114;&#26131;&#22122;&#22768;&#26041;&#24046;&#21644;&#26377;&#30028;&#24179;&#28369;&#24615;&#20551;&#35774;&#30340;&#38750;&#20984;&#30446;&#26631;&#30340;AdaGrad&#31639;&#27861;&#30340;&#31616;&#21333;&#25910;&#25947;&#24615;&#35777;&#26126;&#12290;&#35813;&#35777;&#26126;&#22522;&#26412;&#19978;&#22522;&#20110;&#19968;&#20010;&#26032;&#39062;&#30340;&#36741;&#21161;&#20989;&#25968;$\xi$&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#22788;&#29702;AdaGrad&#26356;&#26032;&#30340;&#20998;&#23376;&#21644;&#20998;&#27597;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#27604;&#29616;&#26377;&#32467;&#26524;\citep{faw2022power}&#26356;&#32039;&#23494;&#30340;&#32467;&#26524;&#65292;&#24182;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20960;&#31181;&#26032;&#30340;&#37325;&#35201;&#24773;&#20917;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#34920;&#26126;AdaGrad&#21482;&#38656;&#35201;$\mathcal{O}(\frac{1}{\varepsilon^2})$&#27425;&#36845;&#20195;&#65292;&#23601;&#21487;&#20197;&#30830;&#20445;&#26799;&#24230;&#33539;&#25968;&#23567;&#20110;$\varepsilon$&#65292;&#36825;&#19982;SGD&#30340;&#36895;&#29575;&#30456;&#21305;&#37197;&#65292;&#24182;&#19988;&#27604;AdaGrad&#30340;&#29616;&#26377;&#36895;&#29575;$\mathcal{O}(\frac{1}{\varepsilon^4})$&#26126;&#26174;&#26356;&#32039;&#23494;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25918;&#24323;&#26377;&#30028;&#24179;&#28369;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#19968;&#31181;&#31216;&#20026;$(L_0,L_1)$-&#24179;&#28369;&#26465;&#20214;&#30340;&#23454;&#38469;&#24179;&#28369;&#20551;&#35774;&#65292;&#35813;&#20551;&#35774;&#20801;&#35768;&#26412;&#22320;&#24179;&#28369;&#24615;&#22686;&#38271;
&lt;/p&gt;
&lt;p&gt;
We provide a simple convergence proof for AdaGrad optimizing non-convex objectives under only affine noise variance and bounded smoothness assumptions. The proof is essentially based on a novel auxiliary function $\xi$ that helps eliminate the complexity of handling the correlation between the numerator and denominator of AdaGrad's update. Leveraging simple proofs, we are able to obtain tighter results than existing results \citep{faw2022power} and extend the analysis to several new and important cases. Specifically, for the over-parameterized regime, we show that AdaGrad needs only $\mathcal{O}(\frac{1}{\varepsilon^2})$ iterations to ensure the gradient norm smaller than $\varepsilon$, which matches the rate of SGD and significantly tighter than existing rates $\mathcal{O}(\frac{1}{\varepsilon^4})$ for AdaGrad. We then discard the bounded smoothness assumption and consider a realistic assumption on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness to grow w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#23454;&#20363;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#28040;&#23696;&#27880;&#24847;&#23884;&#20837;&#31639;&#27861;&#65288;DEMIPL&#65289;&#65292;&#36890;&#36807;&#23558;&#22810;&#23454;&#20363;&#21253;&#23884;&#20837;&#21040;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#20013;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24573;&#35270;&#20840;&#23616;&#21253;&#32423;&#20449;&#24687;&#21644;&#23545;&#36127;&#23454;&#20363;&#39044;&#27979;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.16912</link><description>&lt;p&gt;
&#22810;&#23454;&#20363;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#28040;&#23696;&#27880;&#24847;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Disambiguated Attention Embedding for Multi-Instance Partial-Label Learning. (arXiv:2305.16912v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#23454;&#20363;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#28040;&#23696;&#27880;&#24847;&#23884;&#20837;&#31639;&#27861;&#65288;DEMIPL&#65289;&#65292;&#36890;&#36807;&#23558;&#22810;&#23454;&#20363;&#21253;&#23884;&#20837;&#21040;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#20013;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#24573;&#35270;&#20840;&#23616;&#21253;&#32423;&#20449;&#24687;&#21644;&#23545;&#36127;&#23454;&#20363;&#39044;&#27979;&#25935;&#24863;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#20219;&#21153;&#20013;&#65292;&#30456;&#20851;&#23545;&#35937;&#21487;&#20197;&#34920;&#31034;&#20026;&#19982;&#20505;&#36873;&#26631;&#31614;&#38598;&#30456;&#20851;&#32852;&#30340;&#22810;&#23454;&#20363;&#21253;&#65292;&#35813;&#38598;&#21512;&#21253;&#25324;&#19968;&#20010;&#30495;&#23454;&#30340;&#26631;&#31614;&#21644;&#22810;&#20010;&#35823;&#25253;&#26631;&#31614;&#12290;&#22810;&#23454;&#20363;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;MIPL&#65289;&#26159;&#19968;&#31181;&#24212;&#23545;&#27492;&#31867;&#20219;&#21153;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#24182;&#24050;&#21462;&#24471;&#33391;&#22909;&#34920;&#29616;&#12290;&#29616;&#26377;&#30340;MIPL&#26041;&#27861;&#36981;&#24490;&#23454;&#20363;&#31354;&#38388;&#33539;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#23454;&#20363;&#20998;&#37197;&#34955;&#23376;&#30340;&#25193;&#20805;&#20505;&#36873;&#26631;&#31614;&#38598;&#24182;&#20174;&#23454;&#20363;&#32423;&#26631;&#31614;&#27719;&#24635;&#21253;&#32423;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#26696;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#24573;&#35270;&#20102;&#20840;&#23616;&#30340;&#21253;&#32423;&#20449;&#24687;&#65292;&#24182;&#19988;&#21253;&#30340;&#39044;&#27979;&#26631;&#31614;&#23545;&#20110;&#36127;&#23454;&#20363;&#30340;&#39044;&#27979;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#23558;&#22810;&#23454;&#20363;&#21253;&#23884;&#20837;&#21040;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#31639;&#27861;DEMIPL&#65292;&#21363;&#29992;&#20110;&#22810;&#23454;&#20363;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#30340;&#28040;&#23696;&#27880;&#24847;&#23884;&#20837;&#12290;DEMIPL&#21033;&#29992;&#20102;&#19968;&#31181;&#28040;&#23696;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world tasks, the concerned objects can be represented as a multi-instance bag associated with a candidate label set, which consists of one ground-truth label and several false positive labels. Multi-instance partial-label learning (MIPL) is a learning paradigm to deal with such tasks and has achieved favorable performances. Existing MIPL approach follows the instance-space paradigm by assigning augmented candidate label sets of bags to each instance and aggregating bag-level labels from instance-level labels. However, this scheme may be suboptimal as global bag-level information is ignored and the predicted labels of bags are sensitive to predictions of negative instances. In this paper, we study an alternative scheme where a multi-instance bag is embedded into a single vector representation. Accordingly, an intuitive algorithm named DEMIPL, i.e., Disambiguated attention Embedding for Multi-Instance Partial-Label learning, is proposed. DEMIPL employs a disambiguation atten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;X-Ray Microbeam&#25968;&#25454;&#38598;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;&#22810;&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65292;&#36827;&#32780;&#25913;&#36827;&#20102;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10775</link><description>&lt;p&gt;
&#37319;&#29992;X&#23556;&#32447;&#24494;&#26463;&#25968;&#25454;&#20960;&#20309;&#21464;&#25442;&#22686;&#24378;&#35821;&#38899;&#21457;&#38899;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset. (arXiv:2305.10775v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10775
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#26041;&#27861;&#65292;&#23558;X-Ray Microbeam&#25968;&#25454;&#38598;&#20013;&#30340;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;&#22810;&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65292;&#36827;&#32780;&#25913;&#36827;&#20102;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20998;&#26512;&#35821;&#38899;&#21457;&#38899;&#23545;&#20110;&#35821;&#38899;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22768;&#38376;&#30340;X-Y&#22352;&#26631;&#20005;&#37325;&#20381;&#36182;&#20110;&#21457;&#35328;&#32773;&#30340;&#35299;&#21078;&#32467;&#26500;&#21644;&#39063;&#31890;&#20301;&#32622;&#30340;&#21487;&#21464;&#24615;&#65292;&#29616;&#26377;&#30340;X&#23556;&#32447;&#24494;&#26463;&#25968;&#25454;&#38598;&#65288;XRMB&#65289;&#20013;&#30340;&#35299;&#21078;&#26631;&#24535;&#29289;&#26144;&#23556;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#21040;&#21457;&#38899;&#36947;&#30340;&#25972;&#20010;&#35299;&#21078;&#23398;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#21464;&#25442;&#65292;&#25913;&#36827;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#21464;&#25442;&#23558;&#35299;&#21078;&#26631;&#35760;&#29289;&#30340;X-Y&#22352;&#26631;&#27839;&#20013;&#30690;&#29366;&#38754;&#26144;&#23556;&#21040;6&#20010;&#30456;&#23545;&#27979;&#37327;&#20013;&#65306;&#21767;&#32541;&#24352;&#24230;&#65288;LA&#65289;&#12289;&#21767;&#37096;&#31361;&#20986;&#65288;LP&#65289;&#12289;&#33292;&#20307;&#25910;&#32553;&#20301;&#32622;&#65288;TTCL&#65289;&#12289;&#24230;&#25968;&#65288;TBCD&#65289;&#12289;&#33292;&#23574;&#25910;&#32553;&#20301;&#32622;&#65288;TTCL&#65289;&#21644;&#24230;&#25968;&#65288;TTCD&#65289;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#36129;&#29486;&#26159;&#23558;&#33133;&#26495;&#36861;&#36394;&#24310;&#20280;&#21040;&#25512;&#27979;&#30340;&#21693;&#21897;&#21069;&#32447;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#33292;&#20307;&#25910;&#32553;&#30340;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and the variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue Body Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL) and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2305.04866</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#20840;&#36523;&#25805;&#20316;&#30340;&#22240;&#26524;&#31574;&#30053;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#21487;&#20197;&#35757;&#32451;&#36866;&#29992;&#20110;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#21487;&#20197;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#25110;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19979;&#19968;&#20195;&#23478;&#24237;&#26426;&#22120;&#20154;&#21161;&#25163;&#38656;&#35201;&#32467;&#21512;&#26426;&#21160;&#21644;&#20132;&#20114;&#33021;&#21147;&#65292;&#21363;&#36890;&#24120;&#25152;&#35828;&#30340;&#31227;&#21160;&#25805;&#20316;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#30340;&#22823;&#21160;&#20316;&#31354;&#38388;&#21644;&#20219;&#21153;&#24120;&#35265;&#30340;&#22810;&#30446;&#26631;&#24615;&#36136;&#65292;&#20363;&#22914;&#33021;&#22815;&#26377;&#25928;&#22320;&#36798;&#21040;&#30446;&#26631;&#19988;&#36991;&#20813;&#38556;&#30861;&#65292;&#31227;&#21160;&#25805;&#20316;&#20219;&#21153;&#24456;&#38590;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26681;&#25454;&#20154;&#24037;&#21305;&#37197;&#21160;&#20316;&#31354;&#38388;&#30340;&#37096;&#20998;&#21040;&#31227;&#21160;&#25805;&#20316;&#23376;&#30446;&#26631;&#65288;&#20363;&#22914;&#29992;&#20110;&#31227;&#21160;&#30446;&#26631;&#30340;&#22522;&#30784;&#21160;&#20316;&#21644;&#29992;&#20110;&#25805;&#20316;&#30340;&#25163;&#33218;&#21160;&#20316;&#65289;&#23558;&#20219;&#21153;&#20998;&#20026;&#19981;&#24102;&#25805;&#20316;&#30340;&#23548;&#33322;&#21644;&#19981;&#24102;&#26426;&#21160;&#30340;&#22266;&#23450;&#25805;&#20316;&#12290;&#27492;&#35299;&#20915;&#26041;&#26696;&#38450;&#27490;&#20102;&#26426;&#21160;&#21644;&#20132;&#20114;&#33258;&#30001;&#24230;&#30340;&#21516;&#26102;&#32452;&#21512;&#65292;&#24182;&#19988;&#38656;&#35201;&#20154;&#31867;&#39046;&#22495;&#30693;&#35782;&#26469;&#21010;&#20998;&#21160;&#20316;&#31354;&#38388;&#24182;&#23558;&#21160;&#20316;&#37096;&#20998;&#19982;&#23376;&#30446;&#26631;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#22240;&#26524;MoMa&#65292;&#35813;&#26694;&#26550;&#29992;&#20110;&#35757;&#32451;&#20856;&#22411;MoMa&#20219;&#21153;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19978;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20197;&#21450;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.04811</link><description>&lt;p&gt;
&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;: 2020-2022&#24180;&#30340;&#26368;&#26032;&#36827;&#23637;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep learning techniques for financial time series forecasting: A review of recent advancements: 2020-2022. (arXiv:2305.04811v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#19978;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20197;&#21450;&#23454;&#29616;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21560;&#24341;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#30340;&#20851;&#27880;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#37117;&#34987;&#25506;&#32034;&#29992;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#21457;&#23637;&#65292;&#36825;&#20123;&#26032;&#36827;&#23637;&#24448;&#24448;&#38590;&#20197;&#36319;&#19978;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36825;&#31687;&#25991;&#29486;&#32508;&#36848;&#65292;&#23545;2020&#24180;&#33267;2022&#24180;&#38388;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#37329;&#34701;&#20215;&#26684;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#20171;&#32461;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#26469;&#28304;&#21644;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#23454;&#29616;&#32454;&#33410;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#20445;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#20415;&#20110;&#26681;&#25454;&#20808;&#21069;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#36873;&#25321;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting financial time series has long been a challenging problem that has attracted attention from both researchers and practitioners. Statistical and machine learning techniques have both been explored to develop effective forecasting models in the past few decades. With recent developments in deep learning models, financial time series forecasting models have advanced significantly, and these developments are often difficult to keep up with. Hence, we have conducted this literature review to provide a comprehensive assessment of recent research from 2020 to 2022 on deep learning models used to predict prices based on financial time series. Our review presents different data sources and neural network structures, as well as their implementation details. Our goals are to ensure that interested researchers remain up-to-date on recent developments in the field and facilitate the selection of baselines based on models used in prior studies. Additionally, we provide suggestions for fu
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#21551;&#31034;&#30340;&#33258;&#28982;&#32452;&#32455;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#20013;&#30340;&#26377;&#24207;&#34892;&#20026;&#21644;&#30456;&#24178;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#39046;&#22495;&#31185;&#23398;&#38382;&#39064;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.12586</link><description>&lt;p&gt;
&#22797;&#26434;&#21160;&#21147;&#31995;&#32479;&#20013;&#29289;&#29702;&#21551;&#31034;&#19979;&#30340;&#34920;&#24449;&#23398;&#20064;&#29992;&#20110;&#33258;&#28982;&#32452;&#32455;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Representation Learning for Emergent Organization in Complex Dynamical Systems. (arXiv:2304.12586v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#21551;&#31034;&#30340;&#33258;&#28982;&#32452;&#32455;&#34920;&#24449;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#20013;&#30340;&#26377;&#24207;&#34892;&#20026;&#21644;&#30456;&#24178;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#39046;&#22495;&#31185;&#23398;&#38382;&#39064;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;&#32452;&#20214;&#32463;&#24120;&#24341;&#20837;&#19981;&#31283;&#23450;&#24615;&#65292;&#20135;&#29983;&#20855;&#26377;&#26032;&#23646;&#24615;&#21644;&#19981;&#21516;&#26102;&#31354;&#23610;&#24230;&#30340;&#29616;&#35937;&#12290;&#36825;&#34987;&#31216;&#20026;&#33258;&#21457;&#33258;&#32452;&#32455;&#65292;&#26159;&#22312;&#36828;&#31163;&#28909;&#21147;&#23398;&#24179;&#34913;&#30340;&#31995;&#32479;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#31639;&#27861;&#23454;&#29616;&#33258;&#28982;&#32452;&#32455;&#12290;&#23427;&#30340;&#22522;&#26412;&#26500;&#20214;&#26159;&#36890;&#36807;&#23616;&#37096;&#20132;&#20114;&#25429;&#33719;&#20449;&#24687;&#22312;&#31995;&#32479;&#20013;&#20256;&#25773;&#26041;&#24335;&#30340;&#26102;&#31354;&#20809;&#38181;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39044;&#27979;&#20809;&#38181;&#31561;&#20215;&#31867;&#8212;&#8212;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#8212;&#8212;&#25429;&#33719;&#22797;&#26434;&#26102;&#31354;&#31995;&#32479;&#20013;&#30340;&#26377;&#24207;&#34892;&#20026;&#21644;&#30456;&#24178;&#32467;&#26500;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#29289;&#29702;&#21551;&#31034;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#39640;&#24615;&#33021;&#35745;&#31639;&#23454;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#22312;&#23454;&#38469;&#39046;&#22495;&#31185;&#23398;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23616;&#37096;&#22240;&#26524;&#29366;&#24577;&#25429;&#33719;&#26059;&#28065;&#21450;&#20854;&#21151;&#29575;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#26159;&#33258;&#28982;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonlinearly interacting system components often introduce instabilities that generate phenomena with new properties and at different space-time scales than the components. This is known as spontaneous self-organization and is ubiquitous in systems far from thermodynamic equilibrium. We introduce a theoretically-grounded framework for emergent organization that, via data-driven algorithms, is constructive in practice. Its building blocks are spacetime lightcones that capture how information propagates across a system through local interactions. We show that predictive equivalence classes of lightcones, local causal states, capture organized behaviors and coherent structures in complex spatiotemporal systems. Using our unsupervised physics-informed machine learning algorithm and a high-performance computing implementation, we demonstrate the applicability of the local causal states for real-world domain science problems. We show that the local causal states capture vortices and their pow
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#26041;&#21644;&#20248;&#21270;&#65292;&#32508;&#21512;&#21160;&#24577;&#30340;&#12289;&#38477;&#38454;&#30340;&#36755;&#20986;&#21453;&#39304;&#22810;&#39033;&#24335;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#21644;&#35270;&#35273;&#35266;&#27979;&#26469;&#31283;&#23450;&#22320;&#36816;&#34892;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.12405</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#21644;&#20248;&#21270;&#32508;&#21512;&#31283;&#23450;&#30340;&#38477;&#38454;&#35270;&#21160;&#24577;&#25511;&#21046;&#22120;&#65292;&#24212;&#29992;&#20110;&#38750;&#32447;&#24615;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization. (arXiv:2304.12405v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12405
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24179;&#26041;&#21644;&#20248;&#21270;&#65292;&#32508;&#21512;&#21160;&#24577;&#30340;&#12289;&#38477;&#38454;&#30340;&#36755;&#20986;&#21453;&#39304;&#22810;&#39033;&#24335;&#25511;&#21046;&#22120;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#21644;&#35270;&#35273;&#35266;&#27979;&#26469;&#31283;&#23450;&#22320;&#36816;&#34892;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#22312;&#35266;&#27979;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#32508;&#21512;&#21160;&#24577;&#30340;&#12289;&#38477;&#38454;&#30340;&#36755;&#20986;&#21453;&#39304;&#22810;&#39033;&#24335;&#25511;&#21046;&#22120;&#65292;&#24212;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#30340;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#24182;&#20351;&#29992;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#21644;&#35270;&#35273;&#35266;&#27979;&#26469;&#20445;&#35777;&#22312;&#21453;&#39304;&#25511;&#21046;&#24490;&#29615;&#20013;&#31283;&#23450;&#22320;&#36816;&#34892;&#21040;&#30446;&#26631;&#29366;&#24577;&#12290;&#25105;&#20204;&#21033;&#29992;Lyapunov&#20998;&#26512;&#26469;&#21046;&#23450;&#32508;&#21512;&#36825;&#31181;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#22312;&#31574;&#30053;&#21442;&#25968;&#21644;&#29992;&#20110;&#35777;&#26126;&#31574;&#30053;&#31283;&#23450;&#24615;&#30340;Lyapunov&#20989;&#25968;&#26041;&#38754;&#26159;&#38750;&#20984;&#30340;&#12290;&#20026;&#20102;&#36817;&#20284;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#35299;&#20915;&#20102;&#19968;&#31995;&#21015;&#24179;&#26041;&#21644;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#36845;&#20195;&#25913;&#36827;&#19968;&#20010;&#36890;&#36807;&#26500;&#36896;&#21487;&#35777;&#26126;&#30340;&#31283;&#23450;&#31574;&#30053;&#65292;&#32780;&#31532;&#20108;&#31181;&#21017;&#30452;&#25509;&#23545;&#22810;&#39033;&#24335;&#31574;&#30053;&#30340;&#21442;&#25968;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#21518;&#39564;&#19978;&#39564;&#35777;&#20854;&#38381;&#29615;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#35266;&#27979;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#31283;&#23450;&#24615;&#20445;&#35777;&#65292;&#35813;&#24773;&#20917;&#23454;&#38469;&#19978;&#26159;&#30001;&#20110;&#35266;&#27979;&#35823;&#24046;&#32780;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for synthesizing dynamic, reduced-order output-feedback polynomial control policies for control-affine nonlinear systems which guarantees runtime stability to a goal state, when using visual observations and a learned perception module in the feedback control loop. We leverage Lyapunov analysis to formulate the problem of synthesizing such policies. This problem is nonconvex in the policy parameters and the Lyapunov function that is used to prove the stability of the policy. To solve this problem approximately, we propose two approaches: the first solves a sequence of sum-of-squares optimization problems to iteratively improve a policy which is provably-stable by construction, while the second directly performs gradient-based optimization on the parameters of the polynomial policy, and its closed-loop stability is verified a posteriori. We extend our approach to provide stability guarantees in the presence of observation noise, which realistically arises due to erro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.04934</link><description>&lt;p&gt;
&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#31616;&#21270;&#26426;&#22120;&#21453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model sparsification can simplify machine unlearning. (arXiv:2304.04934v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#31232;&#30095;&#21270;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26041;&#26696;&#65292;&#31216;&#20026;prune first, then unlearn&#21644;sparsity-aware unlearning&#12290;&#27492;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25968;&#25454;&#31649;&#21046;&#35201;&#27714;&#26426;&#22120;&#21453;&#23398;&#20064;&#65288;MU&#65289;&#65306;&#20174;&#27169;&#22411;&#20013;&#31227;&#38500;&#25351;&#23450;&#26679;&#20363;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#26469;&#36827;&#34892;&#31934;&#30830;&#21453;&#23398;&#20064;&#65292;&#20294;&#26159;&#20854;&#35745;&#31639;&#25104;&#26412;&#23548;&#33268;&#20102;&#36817;&#20284;&#20294;&#39640;&#25928;&#30340;&#21453;&#23398;&#20064;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;&#38500;&#20102;&#25968;&#25454;&#20013;&#24515;&#30340;MU&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#35270;&#35282;&#25512;&#36827;MU&#65306;&#36890;&#36807;&#26435;&#20540;&#20462;&#21098;&#36827;&#34892;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#31232;&#30095;&#24615;&#21487;&#20197;&#25552;&#39640;&#36817;&#20284;&#21453;&#23398;&#20064;&#22120;&#30340;&#22810;&#26631;&#20934;&#21453;&#23398;&#20064;&#24615;&#33021;&#65292;&#32553;&#23567;&#36817;&#20284;&#38388;&#38553;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#26377;&#20102;&#36825;&#20010;&#35748;&#35782;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#20004;&#20010;&#26032;&#30340;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#20803;&#26041;&#26696;&#65292;&#31216;&#20026;&#8220;&#20808;&#20462;&#21098;&#65292;&#28982;&#21518;&#21453;&#23398;&#20064;&#8221;&#21644;&#8220;&#31232;&#30095;&#24863;&#30693;&#21453;&#23398;&#20064;&#8221;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#21644;&#25552;&#35758;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#22987;&#32456;&#26377;&#30410;&#20110;MU&#65292;&#21253;&#25324;&#25353;&#31867;&#25968;&#25454;&#25830;&#38500;&#12289;&#38543;&#26426;&#25968;&#25454;&#25830;&#38500;&#21644;&#21518;&#38376;&#25968;&#25454;&#20266;&#36896;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent data regulations necessitate machine unlearning (MU): The removal of the effect of specific examples from the model. While exact unlearning is possible by conducting a model retraining with the remaining data from scratch, its computational cost has led to the development of approximate but efficient unlearning schemes. Beyond data-centric MU solutions, we advance MU through a novel model-based viewpoint: sparsification via weight pruning. Our results in both theory and practice indicate that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. With this insight, we develop two new sparsity-aware unlearning meta-schemes, termed `prune first, then unlearn' and `sparsity-aware unlearning'. Extensive experiments show that our findings and proposals consistently benefit MU in various scenarios, including class-wise data scrubbing, random data scrubbing, and backdoor data forge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2303.16296</link><description>&lt;p&gt;
Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65306;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Dice&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. (arXiv:2303.16296v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#65292;&#22312;&#21307;&#30103;&#25104;&#20687;&#39046;&#22495;&#30340;&#20998;&#21106;&#26041;&#26696;&#20013;&#19982;&#20351;&#29992;&#36719;&#26631;&#31614;&#30340;&#30740;&#31350;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#30340;&#35768;&#22810;&#33258;&#21160;&#20998;&#21106;&#26041;&#26696;&#20013;&#65292;&#36719;Dice&#25439;&#22833;&#65288;SDL&#65289;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25581;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#32972;&#21518;&#30340;&#19968;&#20123;&#21407;&#22240;&#24182;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#20854;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23454;&#29616;&#25903;&#25345;&#30452;&#25509;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#23427;&#30340;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#22312;&#20351;&#29992;SDL&#21644;&#30740;&#31350;&#21033;&#29992;&#36719;&#26631;&#31614;&#30340;&#21516;&#26102;&#36827;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21327;&#21516;&#20316;&#29992;&#20173;&#28982;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Dice&#21322;&#24230;&#37327;&#25439;&#22833;&#20989;&#25968;&#65288;DMLs&#65289;&#65292;&#23427;&#20204;&#65288;i&#65289;&#22312;&#30828;&#26631;&#31614;&#30340;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;SDL&#30456;&#21516;&#65292;&#20294;&#65288;ii&#65289;&#20063;&#21487;&#22312;&#36719;&#26631;&#31614;&#35774;&#32622;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#22312;&#20844;&#20849;&#30340;QUBIQ&#12289;LiTS&#21644;KiTS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DMLs&#19982;&#36719;&#26631;&#31614;&#65288;&#22914;&#24179;&#22343;&#12289;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#65289;&#30340;&#28508;&#22312;&#21327;&#21516;&#20316;&#29992;&#65292;&#32780;DMLs&#19982;&#30828;&#26631;&#31614;&#65288;&#22914;&#22823;&#22810;&#25968;&#25237;&#31080;&#21644;&#38543;&#26426;&#36873;&#25321;&#65289;&#30456;&#27604;&#65292;&#20135;&#29983;&#20102;&#26356;&#20248;&#31168;&#30340;Dice&#20998;&#25968;&#21644;&#27169;&#22411;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The soft Dice loss (SDL) has taken a pivotal role in many automated segmentation pipelines in the medical imaging community. Over the last years, some reasons behind its superior functioning have been uncovered and further optimizations have been explored. However, there is currently no implementation that supports its direct use in settings with soft labels. Hence, a synergy between the use of SDL and research leveraging the use of soft labels, also in the context of model calibration, is still missing. In this work, we introduce Dice semimetric losses (DMLs), which (i) are by design identical to SDL in a standard setting with hard labels, but (ii) can be used in settings with soft labels. Our experiments on the public QUBIQ, LiTS and KiTS benchmarks confirm the potential synergy of DMLs with soft labels (e.g. averaging, label smoothing, and knowledge distillation) over hard labels (e.g. majority voting and random selection). As a result, we obtain superior Dice scores and model calib
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2303.12414</link><description>&lt;p&gt;
&#24310;&#36831;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#35757;&#32451;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24310;&#36831;&#24863;&#30693;&#30340;&#32852;&#37030;&#23398;&#20064;(DFL)&#65292;&#36890;&#36807;&#35299;&#20915;&#36793;&#32536;&#21644;&#20113;&#20043;&#38388;&#30340;&#36890;&#20449;&#24310;&#36831;&#65292;&#25552;&#39640;&#20102;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#12290;DFL&#22312;&#27599;&#20010;&#20840;&#23616;&#32858;&#21512;&#38388;&#38548;&#26399;&#38388;&#23545;&#35774;&#22791;&#25968;&#25454;&#38598;&#25191;&#34892;&#22810;&#20010;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#36845;&#20195;&#65292;&#24182;&#36890;&#36807;&#36793;&#32536;&#26381;&#21153;&#22120;&#22312;&#26412;&#22320;&#23376;&#32593;&#32476;&#20013;&#38388;&#26029;&#22320;&#32858;&#21512;&#27169;&#22411;&#21442;&#25968;&#12290;&#20113;&#26381;&#21153;&#22120;&#36890;&#36807;&#23616;&#37096;-&#20840;&#23616;&#21512;&#24182;&#22120;&#23558;&#26412;&#22320;&#27169;&#22411;&#19982;&#20840;&#23616;&#37096;&#32626;&#27169;&#22411;&#21516;&#27493;&#12290;DFL&#30340;&#25910;&#25947;&#34892;&#20026;&#22312;&#24191;&#20041;&#25968;&#25454;&#24322;&#36136;&#24615;&#24230;&#37327;&#19979;&#36827;&#34892;&#20102;&#29702;&#35770;&#30740;&#31350;&#12290;&#24471;&#20986;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#20197;&#23454;&#29616;O(1/k)&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#25511;&#21046;&#31639;&#27861;&#26469;&#23454;&#29616;DFL&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20123;&#25919;&#31574;&#20197;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#21644;&#36793;&#32536;&#21040;&#20113;&#31471;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#20998;&#31867;&#22120;&#30005;&#36335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#34920;&#26684;&#24335;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#21033;&#29992;&#28436;&#21270;&#31639;&#27861;&#25628;&#32034;&#36923;&#36753;&#38376;&#31354;&#38388;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#22823;&#21270;&#35757;&#32451;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24494;&#23567;&#20998;&#31867;&#22120;&#30005;&#36335;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.00031</link><description>&lt;p&gt;
&#24494;&#23567;&#20998;&#31867;&#22120;&#30005;&#36335;&#65306;&#29992;&#20110;&#34920;&#26684;&#24335;&#25968;&#25454;&#30340;&#28436;&#21270;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
Tiny Classifier Circuits: Evolving Accelerators for Tabular Data. (arXiv:2303.00031v2 [cs.AR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#20998;&#31867;&#22120;&#30005;&#36335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#34920;&#26684;&#24335;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#21033;&#29992;&#28436;&#21270;&#31639;&#27861;&#25628;&#32034;&#36923;&#36753;&#38376;&#31354;&#38388;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#22823;&#21270;&#35757;&#32451;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24494;&#23567;&#20998;&#31867;&#22120;&#30005;&#36335;&#65292;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#21644;&#21151;&#32791;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#35745;&#31639;&#20013;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#24320;&#21457;&#24490;&#29615;&#26159;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#26368;&#22823;&#21270;&#24615;&#33021;&#65292;&#28982;&#21518;&#22312;&#37096;&#32626;&#22312;&#38024;&#23545;CPU&#12289;GPU&#12289;&#24494;&#25511;&#21046;&#22120;&#25110;&#23450;&#21046;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#26102;&#26368;&#23567;&#21270;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20869;&#23384;/&#38754;&#31215;&#21344;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#20998;&#31867;&#22120;&#30005;&#36335;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#34920;&#26684;&#24335;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;ML&#25216;&#26415;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#26356;&#23569;&#30340;&#30828;&#20214;&#36164;&#28304;&#21644;&#21151;&#32791;&#12290;&#35813;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#28436;&#21270;&#31639;&#27861;&#22312;&#36923;&#36753;&#38376;&#31354;&#38388;&#19978;&#36827;&#34892;&#25628;&#32034;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#20855;&#26377;&#26368;&#22823;&#21270;&#35757;&#32451;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#20998;&#31867;&#22120;&#30005;&#36335;&#12290;&#20998;&#31867;&#22120;&#30005;&#36335;&#38750;&#24120;&#24494;&#23567;(&#21363;&#30001;&#19981;&#36229;&#36807;300&#20010;&#36923;&#36753;&#38376;&#32452;&#25104;)&#65292;&#34987;&#31216;&#20026;"&#24494;&#23567;&#20998;&#31867;&#22120;"&#30005;&#36335;&#65292;&#24182;&#21487;&#20197;&#22312;ASIC&#19978;&#25110;&#32773;FPGA&#19978;&#39640;&#25928;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#33258;&#21160;&#29983;&#25104;&#24494;&#23567;&#20998;&#31867;&#22120;&#30005;&#36335;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical machine learning (ML) development cycle for edge computing is to maximise the performance during model training and then minimise the memory/area footprint of the trained model for deployment on edge devices targeting CPUs, GPUs, microcontrollers, or custom hardware accelerators. This paper proposes a methodology for automatically generating predictor circuits for classification of tabular data with comparable prediction performance to conventional ML techniques while using substantially fewer hardware resources and power. The proposed methodology uses an evolutionary algorithm to search over the space of logic gates and automatically generates a classifier circuit with maximised training prediction accuracy. Classifier circuits are so tiny (i.e., consisting of no more than 300 logic gates) that they are called "Tiny Classifier" circuits, and can efficiently be implemented in ASIC or on an FPGA. We empirically evaluate the automatic Tiny Classifier circuit generation methodol
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#28145;&#23618;&#32423;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25233;&#21046;&#21518;&#39564;&#22349;&#22604;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#20462;&#25913;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#24182;&#19988;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.09976</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#25233;&#21046;&#23618;&#32423;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#30340;&#21518;&#39564;&#22349;&#22604;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Discouraging posterior collapse in hierarchical Variational Autoencoders using context. (arXiv:2302.09976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#28145;&#23618;&#32423;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#25233;&#21046;&#21518;&#39564;&#22349;&#22604;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#20462;&#25913;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#24182;&#19988;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#32423;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAEs)&#26159;&#26368;&#20026;&#27969;&#34892;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#20043;&#19968;&#12290;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#33258;&#39030;&#21521;&#19979;&#30340;&#23618;&#32423;VAEs&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#28145;&#23618;&#28508;&#22312;&#32467;&#26500;&#24182;&#36991;&#20813;&#21518;&#39564;&#22349;&#22604;&#31561;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#65292;&#21518;&#39564;&#22349;&#22604;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#19978;&#19979;&#25991;&#30340;&#28145;&#23618;&#32423;VAE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;&#26469;&#33719;&#21462;&#26368;&#21518;&#19968;&#20010;&#28508;&#22312;&#21464;&#37327;&#12290;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#20462;&#25913;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#19981;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Variational Autoencoders (VAEs) are among the most popular likelihood-based generative models. There is a consensus that the top-down hierarchical VAEs allow effective learning of deep latent structures and avoid problems like posterior collapse. Here, we show that this is not necessarily the case, and the problem of collapsing posteriors remains. To discourage this issue, we propose a deep hierarchical VAE with a context on top. Specifically, we use a Discrete Cosine Transform to obtain the last latent variable. In a series of experiments, we observe that the proposed modification allows us to achieve better utilization of the latent space and does not harm the model's generative abilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#65292;&#23427;&#20351;&#29992;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;&#21487;&#20197;&#20248;&#21270;&#27979;&#22320;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#31454;&#20105;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2302.06807</link><description>&lt;p&gt;
&#36229;&#20284;&#26354;&#31354;&#38388;&#30340;&#22823;&#38388;&#38548;&#20998;&#31867;&#30340;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Horospherical Decision Boundaries for Large Margin Classification in Hyperbolic Space. (arXiv:2302.06807v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#65292;&#23427;&#20351;&#29992;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;&#21487;&#20197;&#20248;&#21270;&#27979;&#22320;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#31454;&#20105;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29992;&#36229;&#20284;&#26354;&#31354;&#38388;&#34920;&#31034;&#23618;&#27425;&#32467;&#26500;&#21270;&#25968;&#25454;&#24050;&#32463;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#21516;&#26102;&#65292;&#25991;&#29486;&#20013;&#20063;&#25552;&#20986;&#20102;&#20960;&#20010;&#38024;&#23545;&#36825;&#20123;&#31354;&#38388;&#20013;&#25968;&#25454;&#20998;&#31867;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#20027;&#35201;&#20351;&#29992;&#36229;&#24179;&#38754;&#25110;&#27979;&#22320;&#32447;&#20316;&#20026;&#20915;&#31574;&#36793;&#30028;&#65292;&#20351;&#29992;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#35774;&#32622;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#20010;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27985;&#25311;&#22278;&#20915;&#31574;&#36793;&#30028;&#30340;&#26032;&#22411;&#22823;&#38388;&#38548;&#20998;&#31867;&#22120;&#65292;&#23427;&#21487;&#20197;&#23548;&#33268;&#19968;&#20010;&#27979;&#22320;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#25216;&#26415;&#26469;&#20248;&#21270;&#65292;&#20445;&#35777;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#20110; SOTA &#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic spaces have been quite popular in the recent past for representing hierarchically organized data. Further, several classification algorithms for data in these spaces have been proposed in the literature. These algorithms mainly use either hyperplanes or geodesics for decision boundaries in a large margin classifiers setting leading to a non-convex optimization problem. In this paper, we propose a novel large margin classifier based on horospherical decision boundaries that leads to a geodesically convex optimization problem that can be optimized using any Riemannian gradient descent technique guaranteeing a globally optimal solution. We present several experiments depicting the competitive performance of our classifier in comparison to SOTA.
&lt;/p&gt;</description></item><item><title>&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.11562</link><description>&lt;p&gt;
&#39044;&#27979;&#26159;&#21542;&#38543;&#24847;&#65311;&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#35780;&#20272;&#33258;&#27965;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification. (arXiv:2301.11562v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11562
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#24615;&#26631;&#20934;&#26469;&#34913;&#37327;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#12290;&#20316;&#32773;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#22788;&#29702;&#38543;&#24847;&#24615;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25581;&#31034;&#20102;&#24403;&#21069;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20844;&#24179;&#20998;&#31867;&#20013;&#65292;&#19981;&#21516;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#39044;&#27979;&#26041;&#24046;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#35823;&#24046;&#26469;&#28304;&#38382;&#39064;&#12290; &#23454;&#35777;&#34920;&#26126;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#30340;&#26041;&#24046;&#24046;&#24322;&#38750;&#24120;&#22823;&#65292;&#20197;&#33267;&#20110;&#20915;&#31574;&#23454;&#38469;&#19978;&#26159;&#38543;&#24847;&#30340;&#12290; &#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#20570;&#20986;&#20102;&#22235;&#20010;&#24635;&#20307;&#36129;&#29486;&#65306;&#25105;&#20204;1&#65289;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#26041;&#24046;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#33258;&#27965;&#24615;&#65292;&#22312;&#27979;&#37327;&#21644;&#20943;&#23569;&#38543;&#24847;&#24615;&#26102;&#20351;&#29992;&#65307; 2&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#21512;&#29702;&#30340;&#31639;&#27861;&#65292;&#24403;&#39044;&#27979;&#26080;&#27861;&#20570;&#20986;&#20915;&#31574;&#26102;&#65292;&#21487;&#20197;&#25918;&#24323;&#20998;&#31867;&#65307; 3&#65289;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26377;&#20851;&#20844;&#24179;&#20998;&#31867;&#20013;&#26041;&#24046;&#65288;&#30456;&#23545;&#20110;&#33258;&#27965;&#24615;&#21644;&#38543;&#24847;&#24615;&#65289;&#20316;&#29992;&#30340;&#26368;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65307; 4&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#24037;&#20855;&#21253;&#65292;&#20351;&#32654;&#22269;&#20303;&#25151;&#25269;&#25276;&#36151;&#27454;&#25259;&#38706;&#27861;&#26696;&#65288;HMDA&#65289;&#25968;&#25454;&#38598;&#26131;&#20110;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#12290; &#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#25581;&#31034;&#20102;&#20851;&#20110;&#21487;&#37325;&#22797;&#24615;&#30340;&#20196;&#20154;&#38663;&#24778;&#30340;&#35265;&#35299;&#12290;&#24403;&#32771;&#34385;&#21040;&#26041;&#24046;&#21644;&#38543;&#24847;&#39044;&#27979;&#30340;&#21487;&#33021;&#24615;&#26102;&#65292;&#22823;&#22810;&#25968;&#20844;&#24179;&#20998;&#31867;&#22522;&#20934;&#25509;&#36817;&#20844;&#24179;&#12290; &#20294;&#26159;&#65292;&#19968;&#23567;&#37096;&#20998;&#23454;&#20363;&#26174;&#31034;&#20986;&#26497;&#22823;&#30340;&#38543;&#24847;&#24615;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#26576;&#20123;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variance in predictions across different trained models is a significant, under-explored source of error in fair classification. Empirically, the variance on some instances is so large that decisions can be effectively arbitrary. To study this problem, we perform a large-scale empirical study and make four overarching contributions: We 1) Define a metric called self-consistency, derived from variance, which we use as a proxy for measuring and reducing arbitrariness; 2) Develop an ensembling algorithm that abstains from classification when a prediction would be arbitrary; 3) Conduct the largest to-date empirical study of the role of variance (vis-a-vis self-consistency and arbitrariness) in fair classification; and, 4) Release a toolkit that makes the US Home Mortgage Disclosure Act (HMDA) datasets easily usable for future research. Altogether, our empirical results reveal shocking insights about reproducibility. Most fairness classification benchmarks are close-to-fair when taking into
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#26657;&#27491;CMIP6&#32423;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#21644;&#31354;&#38388;&#27169;&#24335;&#20013;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2301.01253</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#32416;&#27491;CMIP6&#32423;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Deep learning for bias-correcting CMIP6-class Earth system models. (arXiv:2301.01253v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#26657;&#27491;CMIP6&#32423;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#21644;&#31354;&#38388;&#27169;&#24335;&#20013;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#27169;&#25311;&#38477;&#27700;&#23545;&#20110;&#21487;&#38752;&#39044;&#27979;&#20154;&#20026;&#20840;&#29699;&#21464;&#26262;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#38477;&#27700;&#30340;&#22797;&#26434;&#36328;&#23610;&#24230;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#24456;&#38590;&#27169;&#25311;&#65292;&#23548;&#33268;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;ESMs&#65289;&#30340;&#23383;&#27573;&#21487;&#33021;&#23384;&#22312;&#24378;&#28872;&#30340;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#20165;&#22312;&#27599;&#20010;&#21333;&#29420;&#30340;&#26684;&#28857;&#23616;&#37096;&#22320;&#32416;&#27491;&#27169;&#25311;&#39057;&#29575;&#20998;&#24067;&#30340;&#35823;&#24046;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25913;&#21892;ESM&#36755;&#20986;&#30340;&#19981;&#30495;&#23454;&#31354;&#38388;&#27169;&#24335;&#65292;&#21363;&#38656;&#35201;&#31354;&#38388;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#19968;&#30452;&#26080;&#27861;&#35299;&#20915;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#32422;&#26463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;cGANs&#65289;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#26657;&#27491;&#26368;&#20808;&#36827;&#30340;CMIP6&#32423;ESM&#30340;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#21644;&#31354;&#38388;&#27169;&#24335;&#20013;&#30340;&#20559;&#24046;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25913;&#21892;&#23616;&#37096;&#39057;&#29575;&#20998;&#24067;&#26041;&#38754;&#19982;&#37329;&#26631;&#20934;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#19968;&#26679;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate representation of precipitation in Earth system models (ESMs) is crucial for reliable projections of the ecological and socioeconomic impacts in response to anthropogenic global warming. The complex cross-scale interactions of processes that produce precipitation are challenging to model, however, inducing potentially strong biases in ESM fields, especially regarding extremes. State-of-the-art bias correction methods only address errors in the simulated frequency distributions locally at every individual grid cell. Improving unrealistic spatial patterns of the ESM output, which would require spatial context, has not been possible so far. Here, we show that a post-processing method based on physically constrained generative adversarial networks (cGANs) can correct biases of a state-of-the-art, CMIP6-class ESM both in local frequency distributions and in the spatial patterns at once. While our method improves local frequency distributions equally well as gold-standard bias-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperBO+&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#39044;&#20808;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#36755;&#20837;&#31354;&#38388;&#30340;&#20989;&#25968;&#19978;&#26222;&#36866;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#27493;&#39044;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#21560;&#24341;&#20154;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2212.10538</link><description>&lt;p&gt;
HyperBO+&#65306;&#20351;&#29992;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#20026;&#36125;&#21494;&#26031;&#20248;&#21270;&#39044;&#20808;&#35757;&#32451;&#36890;&#29992;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
HyperBO+: Pre-training a universal prior for Bayesian optimization with hierarchical Gaussian processes. (arXiv:2212.10538v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyperBO+&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#39044;&#20808;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#36755;&#20837;&#31354;&#38388;&#30340;&#20989;&#25968;&#19978;&#26222;&#36866;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#19968;&#31181;&#20004;&#27493;&#39044;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#21560;&#24341;&#20154;&#30340;&#28176;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#35768;&#22810;&#40657;&#30418;&#20989;&#25968;&#20248;&#21270;&#20219;&#21153;&#20013;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#38656;&#35201;&#23454;&#36341;&#32773;&#31934;&#24515;&#36873;&#25321;&#36866;&#21512;&#20854;&#24863;&#20852;&#36259;&#20989;&#25968;&#30340;&#20808;&#39564;&#27010;&#29575;&#27169;&#22411;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;&#22810;&#20219;&#21153;BO&#12289;&#23569;&#26679;&#26412;BO&#21644;HyperBO&#65292;&#26469;&#33258;&#21160;&#23398;&#20064;&#20808;&#39564;&#27010;&#29575;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#20219;&#21153;&#30340;&#36755;&#20837;&#31354;&#38388;&#30456;&#21516;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#31354;&#38388;&#19978;&#20351;&#29992;&#35266;&#27979;&#32467;&#26524;&#25110;&#25512;&#24191;&#23398;&#20064;&#21040;&#30340;&#20808;&#39564;&#27010;&#29575;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HyperBO+&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#39640;&#26031;&#36807;&#31243;&#30340;&#39044;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20855;&#26377;&#19981;&#21516;&#36755;&#20837;&#31354;&#38388;&#30340;&#20989;&#25968;&#19978;&#26222;&#36941;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#39044;&#20808;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#21560;&#24341;&#20154;&#30340;&#28176;&#36817;&#24615;&#36136;&#21450;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO), while proved highly effective for many black-box function optimization tasks, requires practitioners to carefully select priors that well model their functions of interest. Rather than specifying by hand, researchers have investigated transfer learning based methods to automatically learn the priors, e.g. multi-task BO (Swersky et al., 2013), few-shot BO (Wistuba and Grabocka, 2021) and HyperBO (Wang et al., 2022). However, those prior learning methods typically assume that the input domains are the same for all tasks, weakening their ability to use observations on functions with different domains or generalize the learned priors to BO on different search spaces. In this work, we present HyperBO+: a pre-training approach for hierarchical Gaussian processes that enables the same prior to work universally for Bayesian optimization on functions with different domains. We propose a two-step pre-training method and analyze its appealing asymptotic properties and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#20026;&#28145;&#24230;&#22270;&#32858;&#31867;&#25552;&#20379;&#39640;&#36136;&#37327;&#21644;&#36866;&#21512;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#36890;&#36807;&#25913;&#36827;&#30697;&#38453;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.03559</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Attribute Graph Clustering via Learnable Augmentation. (arXiv:2212.03559v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#20026;&#28145;&#24230;&#22270;&#32858;&#31867;&#25552;&#20379;&#39640;&#36136;&#37327;&#21644;&#36866;&#21512;&#30340;&#22686;&#24378;&#26679;&#26412;&#65292;&#36890;&#36807;&#25913;&#36827;&#30697;&#38453;&#25552;&#39640;&#20102;&#32858;&#31867;&#24615;&#33021;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#28145;&#24230;&#22270;&#32858;&#31867;&#65288;CDGC&#65289;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23558;&#33410;&#28857;&#20998;&#32452;&#21040;&#19981;&#21516;&#30340;&#31751;&#20013;&#12290;&#26356;&#22909;&#30340;&#22686;&#24378;&#25216;&#26415;&#26377;&#21161;&#20110;&#25552;&#39640;&#23545;&#27604;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#22240;&#27492;&#25104;&#20026;&#25913;&#21892;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#22686;&#24378;&#26679;&#26412;&#22987;&#32456;&#30001;&#20154;&#31867;&#32463;&#39564;&#39044;&#23450;&#20041;&#65292;&#24182;&#19988;&#19982;&#19979;&#28216;&#20219;&#21153;&#32858;&#31867;&#26080;&#20851;&#65292;&#20174;&#32780;&#23548;&#33268;&#20154;&#21147;&#36164;&#28304;&#25104;&#26412;&#39640;&#21644;&#24615;&#33021;&#24046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21487;&#23398;&#20064;&#22686;&#24378;&#25216;&#26415;&#36827;&#34892;&#23646;&#24615;&#22270;&#32858;&#31867;&#30340;&#26041;&#27861;&#65288;AGCLA&#65289;&#65292;&#20026;CDGC&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#65292;&#20197;&#33719;&#24471;&#39640;&#36136;&#37327;&#19988;&#36866;&#21512;&#30340;&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#21487;&#23398;&#20064;&#30340;&#22686;&#24378;&#22120;&#20998;&#21035;&#29992;&#20110;&#23646;&#24615;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29983;&#25104;&#20102;&#20004;&#20010;&#25913;&#36827;&#30697;&#38453;&#65292;&#21253;&#25324;&#39640;&#32622;&#20449;&#24230;&#20266;&#26631;&#31614;&#30697;&#38453;&#21644;&#36328;&#35270;&#22270;&#26679;&#26412;&#30456;&#20284;&#24230;&#30697;&#38453;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#30340;&#20146;&#21644;&#30697;&#38453;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#19981;&#26029;&#35843;&#25972;&#36825;&#20123;&#22686;&#24378;&#22120;&#21644;&#25913;&#36827;&#30697;&#38453;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive deep graph clustering (CDGC) utilizes contrastive learning to group nodes into different clusters. Better augmentation techniques benefit the quality of the contrastive samples, thus being one of key factors to improve performance. However, the augmentation samples in existing methods are always predefined by human experiences, and agnostic from the downstream task clustering, thus leading to high human resource costs and poor performance. To this end, we propose an Attribute Graph Clustering method via Learnable Augmentation (\textbf{AGCLA}), which introduces learnable augmentors for high-quality and suitable augmented samples for CDGC. Specifically, we design two learnable augmentors for attribute and structure information, respectively. Besides, two refinement matrices, including the high-confidence pseudo-label matrix and the cross-view sample similarity matrix, are generated to improve the reliability of the learned affinity matrix. During the training procedure, we no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#36827;&#34892;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(NMPC)&#30340;&#23433;&#20840;&#36817;&#20284;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#24555;&#36895;&#25511;&#21046;&#12290;&#19982;NMPC&#30456;&#27604;&#65292;&#22312;&#20445;&#35777;&#23433;&#20840;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25913;&#21892;&#20102;8&#20493;&#20197;&#19978;&#12290;</title><link>http://arxiv.org/abs/2212.02941</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#22312;&#26580;&#24615;&#26426;&#22120;&#20154;&#20013;&#30340;&#23433;&#20840;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Imitation Learning of Nonlinear Model Predictive Control for Flexible Robots. (arXiv:2212.02941v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#36827;&#34892;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(NMPC)&#30340;&#23433;&#20840;&#36817;&#20284;&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#23545;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#24555;&#36895;&#25511;&#21046;&#12290;&#19982;NMPC&#30456;&#27604;&#65292;&#22312;&#20445;&#35777;&#23433;&#20840;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25913;&#21892;&#20102;8&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#26426;&#22120;&#20154;&#21487;&#20197;&#35299;&#20915;&#19968;&#20123;&#24037;&#19994;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#22914;&#23454;&#29616;&#22266;&#26377;&#23433;&#20840;&#30340;&#20154;&#26426;&#21327;&#20316;&#21644;&#23454;&#29616;&#26356;&#39640;&#30340;&#36127;&#36733;&#37325;&#37327;&#27604;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#65292;&#21253;&#25324;&#25391;&#33633;&#34892;&#20026;&#21644;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#65292;&#25511;&#21046;&#26580;&#24615;&#26426;&#22120;&#20154;&#38750;&#24120;&#22797;&#26434;&#12290;&#38750;&#32447;&#24615;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(NMPC)&#33021;&#22815;&#26377;&#25928;&#25511;&#21046;&#27492;&#31867;&#26426;&#22120;&#20154;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#27714;&#36739;&#39640;&#24120;&#24120;&#38480;&#21046;&#20854;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#26580;&#24615;&#26426;&#22120;&#20154;&#30340;&#24555;&#36895;&#25511;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#21644;&#39044;&#27979;&#23433;&#20840;&#36807;&#28388;&#22120;&#36827;&#34892;NMPC&#30340;&#23433;&#20840;&#36817;&#20284;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#26102;&#38388;&#65292;&#21516;&#26102;&#22312;&#24615;&#33021;&#19978;&#30053;&#26377;&#25439;&#22833;&#12290;&#19982;NMPC&#30456;&#27604;&#65292;&#22312;&#27169;&#25311;&#20013;&#25511;&#21046;&#19968;&#20010;&#19977;&#32500;&#26580;&#24615;&#26426;&#26800;&#33218;&#26102;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25913;&#21892;&#20102;8&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#23433;&#20840;&#32422;&#26463;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible robots may overcome some of the industry's major challenges, such as enabling intrinsically safe human-robot collaboration and achieving a higher load-to-mass ratio. However, controlling flexible robots is complicated due to their complex dynamics, which include oscillatory behavior and a high-dimensional state space. NMPC offers an effective means to control such robots, but its extensive computational demands often limit its application in real-time scenarios. To enable fast control of flexible robots, we propose a framework for a safe approximation of NMPC using imitation learning and a predictive safety filter. Our framework significantly reduces computation time while incurring a slight loss in performance. Compared to NMPC, our framework shows more than a eightfold improvement in computation time when controlling a three-dimensional flexible robot arm in simulation, all while guaranteeing safety constraints. Notably, our approach outperforms conventional reinforcement le
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20462;&#34917;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#20197;&#35299;&#20915;&#24314;&#31435;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#20462;&#34917;&#21385;&#23475;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#29983;&#25104;&#30340;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Adversarial Input Generation via Neural Net Patching. (arXiv:2211.16808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20462;&#34917;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#29983;&#25104;&#23545;&#25239;&#24615;&#36755;&#20837;&#65292;&#20197;&#35299;&#20915;&#24314;&#31435;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#29983;&#25104;&#24050;&#25104;&#20026;&#24314;&#31435;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#22312;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21644;&#31934;&#30830;&#21307;&#23398;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#39046;&#22495;&#20351;&#29992;&#26102;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#38754;&#20020;&#22810;&#20010;&#23454;&#38469;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#32593;&#32476;&#35268;&#27169;&#22823;&#32780;&#23548;&#33268;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#29983;&#25104;&#32570;&#20047;&#33258;&#28982;&#24615;&#21644;&#36755;&#20986;&#20013;&#31435;&#24615;&#31561;&#37325;&#35201;&#29305;&#24449;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#36825;&#20010;&#38382;&#39064;&#19982;&#20462;&#34917;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#26368;&#32456;&#30446;&#26631;&#65292;&#20854;&#20013;&#38656;&#35201;&#21457;&#29616;&#19968;&#20123;&#32593;&#32476;&#26435;&#37325;&#30340;&#24494;&#23567;&#21464;&#21270;&#65292;&#20197;&#20415;&#22312;&#24212;&#29992;&#36825;&#20123;&#21464;&#21270;&#21518;&#65292;&#20462;&#25913;&#21518;&#30340;&#32593;&#32476;&#23545;&#20110;&#32473;&#23450;&#30340;&#36755;&#20837;&#38598;&#20135;&#29983;&#29702;&#24819;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#20174;&#19968;&#20010;&#20462;&#34917;&#34917;&#19969;&#20013;&#33719;&#21462;&#23545;&#25239;&#24615;&#36755;&#20837;&#26469;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#20854;&#20013;&#30340;&#35266;&#23519;&#26159;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#20063;&#21487;&#20197;&#23454;&#29616;&#25913;&#21464;&#26435;&#37325;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of adversarial inputs has become a crucial issue in establishing the robustness and trustworthiness of deep neural nets, especially when they are used in safety-critical application domains such as autonomous vehicles and precision medicine. However, the problem poses multiple practical challenges, including scalability issues owing to large-sized networks, and the generation of adversarial inputs that lack important qualities such as naturalness and output-impartiality. This problem shares its end goal with the task of patching neural nets where small changes in some of the network's weights need to be discovered so that upon applying these changes, the modified net produces the desirable output for a given set of inputs. We exploit this connection by proposing to obtain an adversarial input from a patch, with the underlying observation that the effect of changing the weights can also be brought about by changing the inputs instead. Thus, this paper presents a novel way
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12814</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65306;&#27010;&#24565;&#12289;&#36827;&#23637;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning: Concepts, Advances and Challenges. (arXiv:2211.12814v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12814
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;VFL&#27010;&#24565;&#12289;&#31639;&#27861;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#21644;&#25361;&#25112;&#30340;&#32508;&#21512;&#22238;&#39038;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#32771;&#34385;&#22810;&#20010;&#32422;&#26463;&#26465;&#20214;&#30340;&#32479;&#19968;&#26694;&#26550;VFLow&#12290;&#27492;&#22806;&#65292;&#36824;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;VFL&#38754;&#20020;&#30340;&#26410;&#26469;&#25361;&#25112;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#32852;&#21512;&#23398;&#20064;&#35774;&#32622;&#65292;&#22810;&#20010;&#20855;&#26377;&#20851;&#20110;&#21516;&#19968;&#32452;&#29992;&#25143;&#19981;&#21516;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#20849;&#21516;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#25110;&#27169;&#22411;&#21442;&#25968;&#12290;&#21463;VFL&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#30340;&#25512;&#21160;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;VFL&#30340;&#27010;&#24565;&#21644;&#31639;&#27861;&#65292;&#20197;&#21450;&#21508;&#20010;&#26041;&#38754;&#30340;&#24403;&#21069;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#21644;&#38544;&#31169;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;VFL&#35774;&#32622;&#21644;&#38544;&#31169;&#20445;&#25252;&#21327;&#35758;&#30340;&#35814;&#23613;&#20998;&#31867;&#65292;&#24182;&#23545;&#27599;&#20010;&#21327;&#35758;&#30340;&#38544;&#31169;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;VFLow&#65292;&#23427;&#32771;&#34385;&#20102;VFL&#38382;&#39064;&#22312;&#36890;&#20449;&#12289;&#35745;&#31639;&#12289;&#38544;&#31169;&#20197;&#21450;&#26377;&#25928;&#24615;&#21644;&#20844;&#24179;&#24615;&#32422;&#26463;&#19979;&#30340;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24037;&#19994;&#24212;&#29992;&#20013;&#26368;&#26032;&#30340;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;VFL&#38754;&#20020;&#30340;&#24320;&#25918;&#24615;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a federated learning setting where multiple parties with different features about the same set of users jointly train machine learning models without exposing their raw data or model parameters. Motivated by the rapid growth in VFL research and real-world applications, we provide a comprehensive review of the concept and algorithms of VFL, as well as current advances and challenges in various aspects, including effectiveness, efficiency, and privacy. We provide an exhaustive categorization for VFL settings and privacy-preserving protocols and comprehensively analyze the privacy attacks and defense strategies for each protocol. In the end, we propose a unified framework, termed VFLow, which considers the VFL problem under communication, computation, privacy, as well as effectiveness and fairness constraints. Finally, we review the most recent advances in industrial applications, highlighting open challenges and future directions for VFL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26041;&#27861;&#26469;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#20855;&#26377;&#20302;&#35823;&#25253;&#29575;&#21644;&#39640;&#25928;&#29575;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2211.09916</link><description>&lt;p&gt;
&#22312;&#32447;&#20998;&#24067;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Online Distribution Shift Detection via Recency Prediction. (arXiv:2211.09916v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26041;&#27861;&#26469;&#26377;&#25928;&#26816;&#27979;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#28418;&#31227;&#65292;&#20855;&#26377;&#20302;&#35823;&#25253;&#29575;&#21644;&#39640;&#25928;&#29575;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#37096;&#32626;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#26102;&#65292;&#26816;&#27979;&#20998;&#24067;&#28418;&#31227;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#24067;&#28418;&#31227;&#26816;&#27979;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#26426;&#22120;&#20154;&#29615;&#22659;&#65292;&#22240;&#20026;&#25968;&#25454;&#36890;&#24120;&#20197;&#27969;&#24335;&#26041;&#24335;&#21040;&#36798;&#24182;&#19988;&#21487;&#33021;&#20855;&#26377;&#38750;&#24120;&#39640;&#30340;&#32500;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#28418;&#31227;&#65292;&#24182;&#23545;&#35823;&#25253;&#29575;&#25552;&#20379;&#20102;&#20445;&#35777; - &#21363;&#24403;&#27809;&#26377;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#38750;&#24120;&#19981;&#21487;&#33021;&#65288;&#27010;&#29575;&#23567;&#20110; epsilon&#65289;&#21457;&#20986;&#38169;&#35823;&#30340;&#35686;&#25253;&#65307;&#22240;&#27492;&#65292;&#20219;&#20309;&#21457;&#20986;&#30340;&#35686;&#25253;&#24212;&#35813;&#34987;&#37325;&#35270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#20026;&#39640;&#32500;&#25968;&#25454;&#30340;&#39640;&#25928;&#26816;&#27979;&#32780;&#35774;&#35745;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;11&#20493;&#30340;&#24555;&#36895;&#26816;&#27979;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#30340;&#35823;&#25253;&#29575;&#65288;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#24403;&#23384;&#22312;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30830;&#23454;&#21457;&#20986;&#20102;&#35686;&#25253;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
When deploying modern machine learning-enabled robotic systems in high-stakes applications, detecting distribution shift is critical. However, most existing methods for detecting distribution shift are not well-suited to robotics settings, where data often arrives in a streaming fashion and may be very high-dimensional. In this work, we present an online method for detecting distribution shift with guarantees on the false positive rate - i.e., when there is no distribution shift, our system is very unlikely (with probability $&lt; \epsilon$) to falsely issue an alert; any alerts that are issued should therefore be heeded. Our method is specifically designed for efficient detection even with high dimensional data, and it empirically achieves up to 11x faster detection on realistic robotics settings compared to prior work while maintaining a low false negative rate in practice (whenever there is a distribution shift in our experiments, our method indeed emits an alert). We demonstrate our a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#32479;&#19968;Lipschitz&#21442;&#25968;&#30340;&#25509;&#36817;&#26368;&#20248;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.07403</link><description>&lt;p&gt;
&#20855;&#26377;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#30340;&#31169;&#26377;&#38543;&#26426;&#20248;&#21270;&#65306;&#65288;&#38750;&#20809;&#28369;&#65289;&#20984;&#25439;&#22833;&#30340;&#26368;&#20248;&#36895;&#29575;&#21450;&#20854;&#23545;&#38750;&#20984;&#25439;&#22833;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Private Stochastic Optimization With Large Worst-Case Lipschitz Parameter: Optimal Rates for (Non-Smooth) Convex Losses and Extension to Non-Convex Losses. (arXiv:2209.07403v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22823;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#20110;&#32479;&#19968;Lipschitz&#21442;&#25968;&#30340;&#25509;&#36817;&#26368;&#20248;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#21487;&#33021;&#38750;&#24120;&#22823;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#38543;&#26426;&#20248;&#21270;&#65288;SO&#65289;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#37096;&#20998;&#20851;&#20110;DP SO&#30340;&#24037;&#20316;&#37117;&#20551;&#35774;&#25439;&#22833;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#19978;&#26159;&#22343;&#21248;Lipschitz&#36830;&#32493;&#30340;&#65288;&#21363;&#38543;&#26426;&#26799;&#24230;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#19978;&#37117;&#26377;&#30028;&#65289;&#12290;&#34429;&#28982;&#36825;&#31181;&#20551;&#35774;&#24456;&#26041;&#20415;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#24754;&#35266;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#20013;&#65292;&#30001;&#20110;&#24322;&#24120;&#20540;&#65292;&#25439;&#22833;&#22312;&#25152;&#26377;&#25968;&#25454;&#28857;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#65288;&#32479;&#19968;&#65289;Lipschitz&#21442;&#25968;&#21487;&#33021;&#38750;&#24120;&#22823;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;DP SO&#30340;&#35823;&#24046;&#30028;&#38480;&#19982;&#25439;&#22833;&#30340;&#26368;&#22351;&#24773;&#20917;Lipschitz&#21442;&#25968;&#25104;&#27604;&#20363;&#65292;&#23558;&#20250;&#26159;&#31354;&#27934;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#25509;&#36817;&#26368;&#20248;&#30340;&#36807;&#37327;&#39118;&#38505;&#30028;&#38480;&#65292;&#19981;&#20381;&#36182;&#20110;&#25439;&#22833;&#30340;&#32479;&#19968;Lipschitz&#21442;&#25968;&#12290;&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;Wang&#31561;&#20154;&#65292;2020; Kamath&#31561;&#20154;&#65292;2022&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20551;&#35774;&#38543;&#26426;&#26799;&#24230;&#20855;&#26377;&#26377;&#30028;&#30340;k&#38454;&#30697;
&lt;/p&gt;
&lt;p&gt;
We study differentially private (DP) stochastic optimization (SO) with loss functions whose worst-case Lipschitz parameter over all data points may be extremely large. To date, the vast majority of work on DP SO assumes that the loss is uniformly Lipschitz continuous over data (i.e. stochastic gradients are uniformly bounded over all data points). While this assumption is convenient, it often leads to pessimistic excess risk bounds. In many practical problems, the worst-case (uniform) Lipschitz parameter of the loss over all data points may be extremely large due to outliers. In such cases, the error bounds for DP SO, which scale with the worst-case Lipschitz parameter of the loss, are vacuous. To address these limitations, this work provides near-optimal excess risk bounds that do not depend on the uniform Lipschitz parameter of the loss. Building on a recent line of work (Wang et al., 2020; Kamath et al., 2022), we assume that stochastic gradients have bounded $k$-th order moments fo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#21487;&#23519;&#35273;&#24046;&#24322;&#65288;JND&#65289;&#65292;&#24314;&#31435;&#20102;JND&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;JND&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25552;&#21319;&#33258;&#21160;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.05856</link><description>&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#21487;&#23519;&#35273;&#24046;&#24322;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Just Noticeable Difference Modeling for Face Recognition System. (arXiv:2209.05856v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#20013;&#30340;&#21487;&#23519;&#35273;&#24046;&#24322;&#65288;JND&#65289;&#65292;&#24314;&#31435;&#20102;JND&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;JND&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#25552;&#21319;&#33258;&#21160;&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#25511;&#21644;&#23433;&#20840;&#22330;&#26223;&#20013;&#65292;&#20026;&#30830;&#20445;&#33258;&#21160;&#20154;&#33080;&#35782;&#21035;&#65288;FR&#65289;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#20154;&#33080;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20256;&#36755;&#25110;&#23384;&#20648;&#30340;&#38480;&#21046;&#65292;&#36890;&#24120;&#20250;&#23545;&#22823;&#37327;&#20154;&#33080;&#25968;&#25454;&#36827;&#34892;&#21387;&#32553;&#20998;&#26512;&#12290;&#21387;&#32553;&#21518;&#30340;&#22270;&#20687;&#21487;&#33021;&#20250;&#20002;&#22833;&#37325;&#35201;&#30340;&#36523;&#20221;&#20449;&#24687;&#65292;&#23548;&#33268;FR&#31995;&#32479;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;FR&#31995;&#32479;&#30340;&#21487;&#23519;&#35273;&#24046;&#24322;&#65288;JND&#65289;&#65292;&#21363;FR&#31995;&#32479;&#26080;&#27861;&#27880;&#24847;&#21040;&#30340;&#26368;&#22823;&#22833;&#30495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;3530&#20010;&#21407;&#22987;&#22270;&#20687;&#21644;137,670&#20010;&#30001;&#20808;&#36827;&#30340;&#21442;&#32771;&#32534;&#35299;&#30721;&#36719;&#20214;&#22522;&#20110;Versatile Video Coding&#65288;VVC&#65289;&#26631;&#20934;&#65288;VTM-15.0&#65289;&#29983;&#25104;&#30340;&#21387;&#32553;&#22270;&#20687;&#30340;JND&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;JND&#39044;&#27979;&#27169;&#22411;&#65292;&#29992;&#20110;&#30452;&#25509;&#25512;&#26029;FR&#31995;&#32479;&#30340;JND&#22270;&#20687;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26088;&#22312;&#26368;&#22823;&#38480;&#24230;&#22320;&#21435;&#38500;&#20887;&#20313;&#32780;&#19981;&#25439;&#23475; FR &#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality face images are required to guarantee the stability and reliability of automatic face recognition (FR) systems in surveillance and security scenarios. However, a massive amount of face data is usually compressed before being analyzed due to limitations on transmission or storage. The compressed images may lose the powerful identity information, resulting in the performance degradation of the FR system. Herein, we make the first attempt to study just noticeable difference (JND) for the FR system, which can be defined as the maximum distortion that the FR system cannot notice. More specifically, we establish a JND dataset including 3530 original images and 137,670 compressed images generated by advanced reference encoding/decoding software based on the Versatile Video Coding (VVC) standard (VTM-15.0). Subsequently, we develop a novel JND prediction model to directly infer JND images for the FR system. In particular, in order to maximum redundancy removal without impairment o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32463;&#20856;&#21040;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#32463;&#20856;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#36739;&#23567;&#30340;QCNN&#22312;&#22797;&#26434;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#35299;&#20915;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;QCNN&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.14708</link><description>&lt;p&gt;
&#32463;&#20856;&#21040;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Classical-to-quantum convolutional neural network transfer learning. (arXiv:2208.14708v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.14708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32463;&#20856;&#21040;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#32463;&#20856;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#36739;&#23567;&#30340;QCNN&#22312;&#22797;&#26434;&#20998;&#31867;&#38382;&#39064;&#19978;&#30340;&#35299;&#20915;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;QCNN&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNN&#65289;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#37327;&#23376;&#21644;&#32463;&#20856;&#25968;&#25454;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;QCNN&#22312;&#23569;&#21442;&#25968;&#24773;&#20917;&#19979;&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#27604;&#20854;&#32463;&#20856;&#23545;&#24212;&#29289;&#26356;&#39640;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21487;&#38752;&#23454;&#29616;&#30340;&#37327;&#23376;&#30005;&#36335;&#35268;&#27169;&#26377;&#38480;&#65292;&#22312;&#22823;&#35268;&#27169;&#37327;&#23376;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#19978;&#24456;&#38590;&#36827;&#34892;&#26816;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#22312;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#26102;&#20195;&#20013;&#36739;&#23567;&#30340;QCNN&#12290;&#22312;&#32463;&#20856;&#21040;&#37327;&#23376;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;QCNN&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#32463;&#20856;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#35299;&#20915;&#22797;&#26434;&#30340;&#20998;&#31867;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#22823;&#35268;&#27169;&#37327;&#23376;&#30005;&#36335;&#12290;&#25105;&#20204;&#23545;&#20855;&#26377;&#19981;&#21516;&#37327;&#23376;&#21367;&#31215;&#21644;&#27744;&#21270;&#25805;&#20316;&#38598;&#30340;QCNN&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#65292;&#20197;&#22788;&#29702;MNIST&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning using quantum convolutional neural networks (QCNNs) has demonstrated success in both quantum and classical data classification. In previous studies, QCNNs attained a higher classification accuracy than their classical counterparts under the same training conditions in the few-parameter regime. However, the general performance of large-scale quantum models is difficult to examine because of the limited size of quantum circuits, which can be reliably implemented in the near future. We propose transfer learning as an effective strategy for utilizing small QCNNs in the noisy intermediate-scale quantum era to the full extent. In the classical-to-quantum transfer learning framework, a QCNN can solve complex classification problems without requiring a large-scale quantum circuit by utilizing a pre-trained classical convolutional neural network (CNN). We perform numerical simulations of QCNN models with various sets of quantum convolution and pooling operations for MNIST data 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20844;&#27491;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#21644;&#24212;&#29992;&#25193;&#23637;&#21040;&#26032;&#39046;&#22495;&#30340;&#20844;&#27491;&#25351;&#26631;&#65292;&#36890;&#36807;&#25552;&#20986;&#29305;&#23450;&#33539;&#22260;&#30340;&#35268;&#33539;&#21407;&#21017;&#26469;&#20351;&#20844;&#27491;&#25351;&#26631;&#33021;&#22815;&#21453;&#26144;&#20986;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20844;&#27491;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2208.06308</link><description>&lt;p&gt;
&#20026;&#20844;&#27491;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#21746;&#23398;&#26694;&#26550;&#65306;&#20174;&#31639;&#27861;&#20018;&#36890;&#26696;&#20363;&#20013;&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Developing a Philosophical Framework for Fair Machine Learning: Lessons From The Case of Algorithmic Collusion. (arXiv:2208.06308v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#20844;&#27491;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#21644;&#24212;&#29992;&#25193;&#23637;&#21040;&#26032;&#39046;&#22495;&#30340;&#20844;&#27491;&#25351;&#26631;&#65292;&#36890;&#36807;&#25552;&#20986;&#29305;&#23450;&#33539;&#22260;&#30340;&#35268;&#33539;&#21407;&#21017;&#26469;&#20351;&#20844;&#27491;&#25351;&#26631;&#33021;&#22815;&#21453;&#26144;&#20986;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20844;&#27491;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#27491;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23548;&#33268;&#27495;&#35270;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#26032;&#30340;&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#20135;&#29983;&#30340;&#20260;&#23475;&#21644;&#19981;&#20844;&#27491;&#19982;&#30446;&#21069;&#30740;&#31350;&#30340;&#26412;&#36136;&#19978;&#19981;&#21516;&#12290;&#26426;&#22120;&#23398;&#20064;&#20013;&#29616;&#26377;&#30340;&#30740;&#31350;&#33539;&#24335;&#26080;&#27861;&#35299;&#37322;&#36825;&#20123;&#26412;&#36136;&#19978;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#20844;&#27491;&#12290;&#31639;&#27861;&#20018;&#36890;&#21644;&#24066;&#22330;&#20844;&#24179;&#24615;&#38382;&#39064;&#26159;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#12290;&#31639;&#27861;&#20018;&#36890;&#30340;&#36127;&#38754;&#21518;&#26524;&#24433;&#21709;&#25152;&#26377;&#28040;&#36153;&#32773;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26576;&#20010;&#21463;&#20445;&#25252;&#31867;&#21035;&#30340;&#25104;&#21592;&#12290;&#20511;&#37492;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#20262;&#29702;&#26694;&#26550;&#65292;&#20379;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#22312;&#24320;&#21457;&#21644;&#24212;&#29992;&#20844;&#27491;&#25351;&#26631;&#26102;&#20351;&#29992;&#65292;&#20197;&#25193;&#23637;&#21040;&#26032;&#30340;&#39046;&#22495;&#12290;&#36825;&#20010;&#36129;&#29486;&#23558;&#20844;&#27491;&#30340;&#24418;&#24335;&#25351;&#26631;&#30340;&#21457;&#23637;&#19982;&#29305;&#23450;&#33539;&#22260;&#30340;&#35268;&#33539;&#21407;&#21017;&#32852;&#31995;&#36215;&#26469;&#12290;&#36825;&#20351;&#24471;&#20844;&#27491;&#25351;&#26631;&#33021;&#22815;&#21453;&#26144;&#20986;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20844;&#27491;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fair machine learning research has been primarily concerned with classification tasks that result in discrimination. However, as machine learning algorithms are applied in new contexts the harms and injustices that result are qualitatively different than those presently studied. The existing research paradigm in machine learning which develops metrics and definitions of fairness cannot account for these qualitatively different types of injustice. One example of this is the problem of algorithmic collusion and market fairness. The negative consequences of algorithmic collusion affect all consumers, not only particular members of a protected class. Drawing on this case study, I propose an ethical framework for researchers and practitioners in machine learning seeking to develop and apply fairness metrics that extends to new domains. This contribution ties the development of formal metrics of fairness to specifically scoped normative principles. This enables fairness metrics to reflect di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;QSANN&#65289;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#20837;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#25237;&#24433;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;QSANN&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2205.05625</link><description>&lt;p&gt;
&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Quantum Self-Attention Neural Networks for Text Classification. (arXiv:2205.05625v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.05625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;QSANN&#65289;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#20837;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#21033;&#29992;&#39640;&#26031;&#25237;&#24433;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;QSANN&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#19968;&#20010;&#26032;&#20852;&#26041;&#21521;&#26159;&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#20869;&#30340;&#20154;&#24037;&#26234;&#33021;&#21508;&#20010;&#39046;&#22495;&#24314;&#31435;&#26377;&#24847;&#20041;&#30340;&#37327;&#23376;&#24212;&#29992;&#12290;&#23613;&#31649;&#22522;&#20110;&#21477;&#27861;&#20998;&#26512;&#30340;&#19968;&#20123;&#24037;&#20316;&#20026;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20294;&#26159;&#35832;&#22914;&#32321;&#37325;&#30340;&#21477;&#27861;&#39044;&#22788;&#29702;&#21644;&#21477;&#27861;&#30456;&#20851;&#30340;&#32593;&#32476;&#32467;&#26500;&#31561;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#22312;&#26356;&#22823;&#35268;&#27169;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#31216;&#20026;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#65288;QSANN&#65289;&#65292;&#21487;&#20197;&#24357;&#34917;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#33258;&#27880;&#24847;&#26426;&#21046;&#24341;&#20837;&#21040;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#28982;&#21518;&#21033;&#29992;&#39640;&#26031;&#25237;&#24433;&#30340;&#37327;&#23376;&#33258;&#27880;&#24847;&#21147;&#20316;&#20026;&#33258;&#27880;&#24847;&#21147;&#30340;&#37327;&#23376;&#29256;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;QSANN&#22312;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#22312;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#19978;&#21487;&#23454;&#29616;&#30340;&#29702;&#24819;&#24615;&#36136;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;QSANN&#20248;&#20110;&#26368;&#20339;&#30340;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best
&lt;/p&gt;</description></item><item><title>DIRA&#26159;&#19968;&#20010;&#29992;&#20110;DNN&#20998;&#31867;&#22120;&#30340;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23454;&#29616;&#37325;&#26032;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.00147</link><description>&lt;p&gt;
DIRA: &#19968;&#31181;&#29992;&#20110;&#21160;&#24577;&#39046;&#22495;&#22686;&#37327;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.00147
&lt;/p&gt;
&lt;p&gt;
DIRA&#26159;&#19968;&#20010;&#29992;&#20110;DNN&#20998;&#31867;&#22120;&#30340;&#21160;&#24577;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23454;&#29616;&#37325;&#26032;&#35757;&#32451;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#65288;AS&#65289;&#32463;&#24120;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20998;&#31867;&#22120;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#22797;&#26434;&#12289;&#39640;&#32500;&#12289;&#38750;&#32447;&#24615;&#21644;&#21160;&#24577;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#30001;&#20110;&#36825;&#20123;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#65292;&#24403;DNN&#20998;&#31867;&#22120;&#38754;&#23545;&#24320;&#21457;&#36807;&#31243;&#20013;&#26410;&#35782;&#21035;&#30340;&#39046;&#22495;&#26102;&#65292;&#21487;&#33021;&#20250;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#36755;&#20986;&#38169;&#35823;&#20998;&#31867;&#12290;&#38543;&#30528;AS&#30340;&#25968;&#37327;&#22686;&#21152;&#65292;&#23558;&#31995;&#32479;&#20174;&#36816;&#34892;&#20013;&#31227;&#38500;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#22686;&#21152;AS&#30340;&#21487;&#38752;&#24615;&#24182;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;DNN&#20998;&#31867;&#22120;&#38656;&#35201;&#22312;&#25805;&#20316;&#36807;&#31243;&#20013;&#36866;&#24212;&#19981;&#21516;&#30340;&#25805;&#20316;&#39046;&#22495;&#65292;&#24182;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#65288;&#20363;&#22914;100&#20010;&#26679;&#26412;&#65289;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#22312;&#23569;&#37327;&#26679;&#26412;&#19978;&#37325;&#26032;&#35757;&#32451;DNN&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22686;&#37327;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#65288;DIRA&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#23454;&#29616;DNN&#20998;&#31867;&#22120;&#30340;&#25805;&#20316;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#24182;&#23454;&#29616;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to allow them to operate in complex, high dimensional, non-linear, and dynamically changing environments. Due to the complexity of these environments, DNN classifiers may output misclassifications during operation when they face domains not identified during development. Removing a system from operation for retraining becomes impractical as the number of such AS increase. To increase AS reliability and overcome this limitation, DNN classifiers need to have the ability to adapt during operation when faced with different operational domains using a few samples (e.g. 100 samples). However, retraining DNNs on a few samples is known to cause catastrophic forgetting. In this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), a framework for operational domain adaption of DNN classifiers using regularisation techniques to overcome catastrophic forgetting and achieve adaptation when retraining using few
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2202.09134</link><description>&lt;p&gt;
&#22312;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#24335;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09134
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#37327;&#21270;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#32467;&#26524;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#20010;&#20855;&#20307;&#27169;&#22411;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20123;&#35266;&#23519;&#65292;&#20294;&#20063;&#24471;&#20986;&#20102;&#24847;&#22806;&#30340;&#21457;&#29616;&#65306;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#32780;&#19981;&#26159;&#20943;&#23569;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#39044;&#27979;&#39118;&#38505;&#12290;&#23427;&#21487;&#20197;&#20805;&#24403;&#27491;&#21017;&#21270;&#22120;&#65292;&#20294;&#22312;&#26576;&#20123;&#39640;&#32500;&#38382;&#39064;&#20013;&#21364;&#26080;&#27861;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25913;&#21464;&#32463;&#39564;&#39118;&#38505;&#30340;&#21452;&#37325;&#19979;&#38477;&#23792;&#20540;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20998;&#26512;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#34987;&#36171;&#20104;&#30340;&#20960;&#20010;&#23646;&#24615;&#35201;&#20040;&#26159;&#30495;&#30340;&#65292;&#35201;&#20040;&#26159;&#20551;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#32452;&#21512;-&#29305;&#21035;&#26159;&#25968;&#25454;&#20998;&#24067;&#65292;&#20272;&#35745;&#22120;&#30340;&#23646;&#24615;&#20197;&#21450;&#26679;&#26412;&#22823;&#23567;&#65292;&#22686;&#24378;&#25968;&#37327;&#21644;&#32500;&#25968;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#24037;&#20855;&#26159;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
&lt;/p&gt;</description></item><item><title>&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#26469;&#25552;&#39640;&#27599;&#20010;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;DDAL&#26469;&#25903;&#25345;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2202.05135</link><description>&lt;p&gt;
&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Group-Agent Reinforcement Learning. (arXiv:2202.05135v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05135
&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#21512;&#20316;&#26469;&#25552;&#39640;&#27599;&#20010;&#20195;&#29702;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#27010;&#24565;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;DDAL&#26469;&#25903;&#25345;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22810;&#20010;&#22320;&#29702;&#20998;&#24067;&#30340;&#20195;&#29702;&#36827;&#34892;&#21512;&#20316;&#24615;&#30340;&#20010;&#21035;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#65292;&#21487;&#20197;&#20026;&#27599;&#20010;&#20195;&#29702;&#30340;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#24102;&#26469;&#24456;&#22823;&#30340;&#22909;&#22788;&#12290;&#19982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#19981;&#21516;&#65292;MARL&#20013;&#22810;&#20010;&#20195;&#29702;&#20849;&#21516;&#23384;&#22312;&#20110;&#19968;&#20010;&#29615;&#22659;&#20013;&#65292;&#24182;&#19988;&#38656;&#35201;&#23398;&#20064;&#22914;&#20309;&#21512;&#20316;&#25110;&#31454;&#20105;&#12290;&#22312;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#26377;&#33258;&#24049;&#30340;&#29615;&#22659;&#65292;&#24182;&#19988;&#21482;&#19982;&#20854;&#20182;&#20195;&#29702;&#36827;&#34892;&#36890;&#20449;&#20197;&#20998;&#20139;&#30693;&#35782;&#65292;&#27809;&#26377;&#21512;&#20316;&#25110;&#31454;&#20105;&#34892;&#20026;&#20316;&#20026;&#23398;&#20064;&#32467;&#26524;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#31181;&#24773;&#26223;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20854;&#27010;&#24565;&#21487;&#20197;&#24212;&#29992;&#20110;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#23578;&#26410;&#24456;&#22909;&#29702;&#35299;&#21644;&#34920;&#36848;&#12290;&#20316;&#20026;&#39318;&#27425;&#23581;&#35797;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20316;&#20026;&#23545;&#21333;&#20010;&#20195;&#29702;&#21644;&#22810;&#20010;&#20195;&#29702;&#31995;&#32479;&#30340;&#31532;&#19977;&#31867;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#34920;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;DDAL&#65288;&#20998;&#25955;&#24335;&#20998;&#24067;&#24335;&#24322;&#27493;&#23398;&#20064;&#65289;&#65292;&#19987;&#20026;&#32676;&#20307;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
It can largely benefit the reinforcement learning (RL) process of each agent if multiple geographically distributed agents perform their separate RL tasks cooperatively. Different from multi-agent reinforcement learning (MARL) where multiple agents are in a common environment and should learn to cooperate or compete with each other, in this case each agent has its separate environment and only communicates with others to share knowledge without any cooperative or competitive behaviour as a learning outcome. In fact, this scenario exists widely in real life whose concept can be utilised in many applications, but is not well understood yet and not well formulated. As the first effort, we propose group-agent system for RL as a formulation of this scenario and the third type of RL system with respect to single-agent and multi-agent systems. We then propose a distributed RL framework called DDAL (Decentralised Distributed Asynchronous Learning) designed for group-agent reinforcement learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;&#8734;-NNQS&#65289;&#65292;&#20854;&#36890;&#36807;&#38598;&#21512;&#32479;&#35745;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#21487;&#34892;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#23376;&#21644;&#37327;&#23376;&#24577;&#31070;&#32463;&#20999;&#32447;&#26680;&#25237;&#20837;&#21040;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#65292;&#21487;&#20197;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#24674;&#22797;&#20219;&#24847;&#30446;&#26631;&#27874;&#20989;&#25968;&#12290;&#23545;&#26377;&#38480;&#21644;&#26080;&#38480;NNQS&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#32416;&#32544;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2112.00723</link><description>&lt;p&gt;
&#26080;&#38480;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65306;&#32416;&#32544;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Infinite Neural Network Quantum States: Entanglement and Training Dynamics. (arXiv:2112.00723v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.00723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;&#8734;-NNQS&#65289;&#65292;&#20854;&#36890;&#36807;&#38598;&#21512;&#32479;&#35745;&#34920;&#29616;&#20986;&#39640;&#24230;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#21487;&#34892;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#23376;&#21644;&#37327;&#23376;&#24577;&#31070;&#32463;&#20999;&#32447;&#26680;&#25237;&#20837;&#21040;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#65292;&#21487;&#20197;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#24674;&#22797;&#20219;&#24847;&#30446;&#26631;&#27874;&#20989;&#25968;&#12290;&#23545;&#26377;&#38480;&#21644;&#26080;&#38480;NNQS&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#32416;&#32544;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;&#8734;-NNQS&#65289;&#30340;&#26080;&#38480;&#26497;&#38480;&#65292;&#36890;&#36807;&#38598;&#21512;&#32479;&#35745;&#34920;&#29616;&#20986;&#20854;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#34892;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#12290;&#20197;&#31070;&#32463;&#32593;&#32476;&#30456;&#20851;&#23376;&#34920;&#36798;&#25289;&#23612;&#29109;&#30340;&#38598;&#21512;&#24179;&#22343;&#20540;&#65292;&#21576;&#29616;&#20102;&#20307;&#31215;&#23450;&#24459;&#32416;&#32544;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#37327;&#23376;&#24577;&#31070;&#32463;&#20999;&#32447;&#26680;&#65288;QS-NTK&#65289;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#37327;&#23376;&#24577;&#65288;NNQS&#65289;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#23545;&#20110;&#8734;-NNQS&#65292;&#35757;&#32451;&#21160;&#21147;&#23398;&#34987;&#31616;&#21270;&#65292;&#22240;&#20026;QS-NTK&#21464;&#24471;&#30830;&#23450;&#21644;&#24658;&#23450;&#12290;&#23545;&#20110;&#37327;&#23376;&#24577;&#30417;&#30563;&#23398;&#20064;&#23548;&#20986;&#20102;&#19968;&#20010;&#35299;&#26512;&#35299;&#65292;&#20801;&#35768;&#8734;-NNQS&#24674;&#22797;&#20219;&#20309;&#30446;&#26631;&#27874;&#20989;&#25968;&#12290;&#22312;&#27178;&#21521;&#22330;&#20234;&#36763;&#27169;&#22411;&#21644;&#36153;&#31859;&#21704;&#20271;&#27169;&#22411;&#20013;&#23545;&#26377;&#38480;&#21644;&#26080;&#38480;NNQS&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#19982;&#29702;&#35770;&#32467;&#26524;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;&#8734;-NNQS&#20026;&#30740;&#31350;&#32416;&#32544;&#21644;&#35757;&#32451;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study infinite limits of neural network quantum states ($\infty$-NNQS), which exhibit representation power through ensemble statistics, and also tractable gradient descent dynamics. Ensemble averages of Renyi entropies are expressed in terms of neural network correlators, and architectures that exhibit volume-law entanglement are presented. A general framework is developed for studying the gradient descent dynamics of neural network quantum states (NNQS), using a quantum state neural tangent kernel (QS-NTK). For $\infty$-NNQS the training dynamics is simplified, since the QS-NTK becomes deterministic and constant. An analytic solution is derived for quantum state supervised learning, which allows an $\infty$-NNQS to recover any target wavefunction. Numerical experiments on finite and infinite NNQS in the transverse field Ising model and Fermi Hubbard model demonstrate excellent agreement with theory. $\infty$-NNQS opens up new opportunities for studying entanglement and training dyn
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#35299;&#37322;&#20013;&#22810;&#20010;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#24050;&#30693;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2109.03890</link><description>&lt;p&gt;
&#12298;&#25512;&#26029;&#35299;&#37322;&#30340;&#20844;&#29702;&#32858;&#21512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Axiomatic Aggregations of Abductive Explanations. (arXiv:2109.03890v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65292;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#25512;&#26029;&#35299;&#37322;&#20013;&#22810;&#20010;&#26377;&#25928;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#21644;&#24050;&#30693;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#21518;&#32493;&#27169;&#22411;&#36924;&#36817;&#35299;&#37322;&#26041;&#27861;&#65288;&#22914;LIME&#21644;SHAP&#65289;&#30340;&#40065;&#26834;&#24615;&#30340;&#36817;&#26399;&#25209;&#35780;&#23548;&#33268;&#20102;&#27169;&#22411;&#31934;&#30830;&#30340;&#25512;&#26029;&#35299;&#37322;&#30340;&#20852;&#36215;&#12290;&#23545;&#20110;&#27599;&#20010;&#25968;&#25454;&#28857;&#65292;&#25512;&#26029;&#35299;&#37322;&#25552;&#20379;&#20102;&#19968;&#20010;&#36275;&#20197;&#29983;&#25104;&#32467;&#26524;&#30340;&#26368;&#23567;&#23376;&#38598;&#29305;&#24449;&#12290;&#23613;&#31649;&#22312;&#29702;&#35770;&#19978;&#26159;&#20005;&#26684;&#21644;&#21487;&#38752;&#30340;&#65292;&#20294;&#25512;&#26029;&#35299;&#37322;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;&#21516;&#19968;&#25968;&#25454;&#28857;&#21487;&#20197;&#26377;&#22810;&#20010;&#26377;&#25928;&#30340;&#25512;&#26029;&#35299;&#37322;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#30340;&#25512;&#26029;&#35299;&#37322;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#25552;&#20379;&#25152;&#26377;&#26377;&#25928;&#30340;&#25512;&#26029;&#35299;&#37322;&#21487;&#33021;&#30001;&#20110;&#20854;&#35268;&#27169;&#32780;&#38590;&#20197;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21508;&#31181;&#21487;&#33021;&#30340;&#25512;&#26029;&#35299;&#37322;&#32858;&#21512;&#25104;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#32858;&#21512;&#26041;&#27861;&#65306;&#20004;&#31181;&#22522;&#20110;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#30340;&#26435;&#21147;&#25351;&#25968;&#26041;&#27861;&#21644;&#19968;&#31181;&#22522;&#20110;&#33879;&#21517;&#30340;&#22240;&#26524;&#24378;&#24230;&#24230;&#37327;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#20844;&#29702;&#19978;&#23545;&#36825;&#19977;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35777;&#26126;&#27599;&#20010;&#26041;&#27861;&#37117;&#26159;&#33391;&#23450;&#20041;&#30340;&#19988;&#31526;&#21512;&#20844;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#21160;&#24577;&#36873;&#25321;&#38382;&#39064;&#65292;&#38024;&#23545;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20013;&#25968;&#25454;&#30340;&#20869;&#29983;&#24615;&#23548;&#33268;&#30340;&#20559;&#24046;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#30340;&#32416;&#27491;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#30495;&#23454;&#21442;&#25968;&#20540;&#21644;&#36739;&#20302;&#36951;&#25022;&#27700;&#24179;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#25512;&#26029;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;</title><link>http://arxiv.org/abs/2108.12547</link><description>&lt;p&gt;
&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#21160;&#24577;&#36873;&#25321;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dynamic Selection in Algorithmic Decision-making. (arXiv:2108.12547v3 [econ.EM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.12547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31639;&#27861;&#20915;&#31574;&#20013;&#30340;&#21160;&#24577;&#36873;&#25321;&#38382;&#39064;&#65292;&#38024;&#23545;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20013;&#25968;&#25454;&#30340;&#20869;&#29983;&#24615;&#23548;&#33268;&#30340;&#20559;&#24046;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#30340;&#32416;&#27491;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#30495;&#23454;&#21442;&#25968;&#20540;&#21644;&#36739;&#20302;&#36951;&#25022;&#27700;&#24179;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#32479;&#35745;&#25512;&#26029;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35782;&#21035;&#21644;&#35299;&#20915;&#20102;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#21160;&#24577;&#36873;&#25321;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#19982;&#20869;&#29983;&#25968;&#25454;&#26377;&#20851;&#12290;&#22312;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#27169;&#22411;&#20013;&#65292;&#30001;&#20110;&#25968;&#25454;&#30340;&#20869;&#29983;&#24615;&#24433;&#21709;&#20915;&#31574;&#30340;&#36873;&#25321;&#65292;&#20250;&#20135;&#29983;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#65288;&#33258;&#25105;&#23454;&#29616;&#20559;&#24046;&#65289;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#26410;&#26469;&#24453;&#25910;&#38598;&#21644;&#20998;&#26512;&#30340;&#25968;&#25454;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24037;&#20855;&#21464;&#37327;&#30340;&#31639;&#27861;&#65292;&#20197;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#33719;&#24471;&#30495;&#23454;&#21442;&#25968;&#20540;&#65292;&#24182;&#33719;&#24471;&#36739;&#20302;&#65288;&#31867;&#20284;&#23545;&#25968;&#30340;&#65289;&#36951;&#25022;&#27700;&#24179;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#32479;&#35745;&#25512;&#26029;&#30340;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#12290;&#20026;&#20102;&#24314;&#31435;&#29702;&#35770;&#24615;&#36136;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#25216;&#26415;&#65292;&#20197;&#35299;&#24320;&#25968;&#25454;&#21644;&#34892;&#21160;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper identifies and addresses dynamic selection problems in online learning algorithms with endogenous data. In a contextual multi-armed bandit model, a novel bias (self-fulfilling bias) arises because the endogeneity of the data influences the choices of decisions, affecting the distribution of future data to be collected and analyzed. We propose an instrumental-variable-based algorithm to correct for the bias. It obtains true parameter values and attains low (logarithmic-like) regret levels. We also prove a central limit theorem for statistical inference. To establish the theoretical properties, we develop a general technique that untangles the interdependence between data and actions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pani&#30340;&#36890;&#29992;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#38750;&#23616;&#37096;&#34920;&#31034;&#65292;&#24182;&#23558;&#37051;&#22495;&#34917;&#19969;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/1911.09307</link><description>&lt;p&gt;
Patch-level Neighborhood Interpolation: &#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#22522;&#20110;&#22270;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Patch-level Neighborhood Interpolation: A General and Effective Graph-based Regularization Strategy. (arXiv:1911.09307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1911.09307
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pani&#30340;&#36890;&#29992;&#27491;&#21017;&#21270;&#22120;&#65292;&#23427;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#38750;&#23616;&#37096;&#34920;&#31034;&#65292;&#24182;&#23558;&#37051;&#22495;&#34917;&#19969;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#25554;&#20540;&#65292;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#65292;&#24182;&#19988;&#20165;&#32771;&#34385;&#24403;&#21069;&#26679;&#26412;&#30340;&#30693;&#35782;&#65292;&#27809;&#26377;&#21033;&#29992;&#26679;&#26412;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;Patch-level Neighborhood Interpolation&#65288;Pani&#65289;&#8221;&#30340;&#36890;&#29992;&#27491;&#21017;&#21270;&#22120;&#65292;&#22312;&#32593;&#32476;&#35745;&#31639;&#20013;&#36827;&#34892;&#38750;&#23616;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26126;&#30830;&#22320;&#26500;&#24314;&#20102;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#19969;&#32423;&#22270;&#65292;&#28982;&#21518;&#32447;&#24615;&#25554;&#20540;&#37051;&#22495;&#34917;&#19969;&#29305;&#24449;&#65292;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#19988;&#26377;&#25928;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#23450;&#21046;&#20026;&#20004;&#31181;&#27969;&#34892;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21363;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#65288;VAT&#65289;&#21644;MixUp&#20197;&#21450;&#20854;&#21464;&#20307;&#12290;&#39318;&#20808;&#27966;&#29983;&#30340;&#8220;Pani VAT&#8221;&#36890;&#36807;&#20351;&#29992;&#34917;&#19969;&#32423;&#25554;&#20540;&#25200;&#21160;&#26500;&#24314;&#38750;&#23616;&#37096;&#23545;&#25239;&#24179;&#28369;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization plays a crucial role in machine learning models, especially for deep neural networks. The existing regularization techniques mainly rely on the i.i.d. assumption and only consider the knowledge from the current sample, without the leverage of the neighboring relationship between samples. In this work, we propose a general regularizer called \textbf{Patch-level Neighborhood Interpolation~(Pani)} that conducts a non-local representation in the computation of networks. Our proposal explicitly constructs patch-level graphs in different layers and then linearly interpolates neighborhood patch features, serving as a general and effective regularization strategy. Further, we customize our approach into two kinds of popular regularization methods, namely Virtual Adversarial Training (VAT) and MixUp as well as its variants. The first derived \textbf{Pani VAT} presents a novel way to construct non-local adversarial smoothness by employing patch-level interpolated perturbations. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20915;&#31574;&#26862;&#26519;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#26680;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#21494;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26680;&#24179;&#22343;&#23884;&#20837;&#38543;&#26426;&#26862;&#26519;&#65288;KMERF&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#19978;&#37117;&#34920;&#29616;&#20986;&#28176;&#36827;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;KMERF&#22312;&#22810;&#31181;&#39640;&#32500;&#25968;&#25454;&#27979;&#35797;&#20013;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/1812.00029</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#26862;&#26519;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#26680;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Characteristic Kernels via Decision Forests. (arXiv:1812.00029v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1812.00029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20915;&#31574;&#26862;&#26519;&#26500;&#24314;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#26680;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#21494;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26680;&#24179;&#22343;&#23884;&#20837;&#38543;&#26426;&#26862;&#26519;&#65288;KMERF&#65289;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#19978;&#37117;&#34920;&#29616;&#20986;&#28176;&#36827;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;KMERF&#22312;&#22810;&#31181;&#39640;&#32500;&#25968;&#25454;&#27979;&#35797;&#20013;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26862;&#26519;&#34987;&#24191;&#27867;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;&#26641;&#26041;&#27861;&#30340;&#19968;&#20010;&#36739;&#23569;&#34987;&#30693;&#26195;&#30340;&#29305;&#24615;&#26159;&#21487;&#20197;&#20174;&#26641;&#26500;&#24314;&#30456;&#20284;&#24615;&#30697;&#38453;&#65292;&#24182;&#19988;&#36825;&#20123;&#30456;&#20284;&#24615;&#30697;&#38453;&#26159;&#30001;&#26680;&#35825;&#23548;&#30340;&#12290;&#23613;&#31649;&#23545;&#20110;&#26680;&#30340;&#24212;&#29992;&#21644;&#24615;&#36136;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#30001;&#20915;&#31574;&#26862;&#26519;&#35825;&#23548;&#30340;&#26680;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22522;&#20110;&#21494;&#33410;&#28857;&#30456;&#20284;&#24615;&#30340;&#26680;&#24179;&#22343;&#23884;&#20837;&#38543;&#26426;&#26862;&#26519;&#65288;KMERF&#65289;&#65292;&#23427;&#21487;&#20197;&#20174;&#38543;&#26426;&#26641;&#25110;&#26862;&#26519;&#20013;&#35825;&#23548;&#26680;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28176;&#36827;&#29305;&#24449;&#26680;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;KMERF&#26680;&#23545;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#25968;&#25454;&#37117;&#26159;&#28176;&#36827;&#29305;&#24449;&#30340;&#12290;&#30001;&#20110;KMERF&#26159;&#25968;&#25454;&#33258;&#36866;&#24212;&#30340;&#65292;&#25105;&#20204;&#24576;&#30097;&#23427;&#23558;&#22312;&#26377;&#38480;&#26679;&#26412;&#25968;&#25454;&#19978;&#32988;&#36807;&#39044;&#20808;&#36873;&#25321;&#30340;&#26680;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;KMERF&#22312;&#21508;&#31181;&#39640;&#32500;&#20004;&#26679;&#26412;&#21644;&#29420;&#31435;&#24615;&#27979;&#35797;&#22330;&#26223;&#20013;&#20960;&#20046;&#21344;&#25454;&#20102;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#26680;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision forests are widely used for classification and regression tasks. A lesser known property of tree-based methods is that one can construct a proximity matrix from the tree(s), and these proximity matrices are induced kernels. While there has been extensive research on the applications and properties of kernels, there is relatively little research on kernels induced by decision forests. We construct Kernel Mean Embedding Random Forests (KMERF), which induce kernels from random trees and/or forests using leaf-node proximity. We introduce the notion of an asymptotically characteristic kernel, and prove that KMERF kernels are asymptotically characteristic for both discrete and continuous data. Because KMERF is data-adaptive, we suspected it would outperform kernels selected a priori on finite sample data. We illustrate that KMERF nearly dominates current state-of-the-art kernel-based tests across a diverse range of high-dimensional two-sample and independence testing settings. Furth
&lt;/p&gt;</description></item></channel></rss>