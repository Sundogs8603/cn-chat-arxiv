<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2404.01318</link><description>&lt;p&gt;
JailbreakBench: &#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01318
&lt;/p&gt;
&lt;p&gt;
JailbreakBench&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#25239;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#29425;&#30340;&#24320;&#25918;&#22522;&#20934;&#65292;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#23545;&#25239;&#25552;&#31034;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25915;&#20987;&#20250;&#23548;&#33268;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#23475;&#12289;&#19981;&#36947;&#24503;&#25110;&#20196;&#20154;&#21453;&#24863;&#30340;&#20869;&#23481;&#12290;&#35780;&#20272;&#36825;&#20123;&#25915;&#20987;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#25216;&#26415;&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;JailbreakBench&#65292;&#19968;&#20010;&#24320;&#28304;&#22522;&#20934;&#65292;&#21253;&#25324;&#20855;&#26377;100&#20010;&#29420;&#29305;&#34892;&#20026;&#30340;&#26032;&#36234;&#29425;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;JBB-Behaviors&#65289;&#12289;&#19968;&#32452;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25552;&#31034;&#65288;&#31216;&#20026;&#36234;&#29425;&#24037;&#20214;&#65289;&#21644;&#19968;&#20010;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01318v1 Announce Type: cross  Abstract: Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) a new jailbreaking dataset containing 100 unique behaviors, which we call JBB-Behaviors; (2) an evolving repository of state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts; (3) a standardized evaluation framework that i
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.16829</link><description>&lt;p&gt;
&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#24674;&#22797;&#19968;&#20010;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#22870;&#21169;&#65292;&#37319;&#29992;&#38543;&#26426;&#36719;&#31574;&#30053;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#20351;&#29992;$\mathcal{O}(1/\varepsilon^{2})$&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26679;&#26412;&#24674;&#22797;&#20986;&#19968;&#20010;&#20351;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;$\mathcal{O}(1/\varepsilon^{4})$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#24674;&#22797;&#22870;&#21169;&#23545;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#19982;&#19987;&#23478;&#31574;&#30053;$\varepsilon$-&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.
&lt;/p&gt;</description></item><item><title>&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;</title><link>https://arxiv.org/abs/2403.12459</link><description>&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Non-negative Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12459
&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;(NCL)&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#30340;&#37325;&#26032;&#28436;&#32462;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#26045;&#21152;&#38750;&#36127;&#32422;&#26463;&#26469;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#65292;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#27604;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;(CL)&#26356;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#34920;&#31034;&#22312;&#20197;&#40657;&#30418;&#26041;&#24335;&#36716;&#31227;&#21040;&#19979;&#28216;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22266;&#26377;&#30340;&#19981;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#29305;&#24449;&#36890;&#24120;&#23545;&#20154;&#31867;&#29702;&#35299;&#32780;&#35328;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#36127;&#23545;&#27604;&#23398;&#20064;&#65288;NCL&#65289;&#65292;&#36825;&#26159;&#23545;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30340;&#22797;&#20852;&#65292;&#26088;&#22312;&#24471;&#20986;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;NCL&#30340;&#21147;&#37327;&#22312;&#20110;&#24378;&#21046;&#23558;&#38750;&#36127;&#32422;&#26463;&#24212;&#29992;&#20110;&#29305;&#24449;&#65292;&#36825;&#35753;&#20154;&#24819;&#36215;NMF&#33021;&#22815;&#25552;&#21462;&#19982;&#26679;&#26412;&#38598;&#32676;&#32039;&#23494;&#23545;&#40784;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;NCL&#19981;&#20165;&#22312;&#25968;&#23398;&#19978;&#19982;NMF&#30446;&#26631;&#24456;&#22909;&#22320;&#23545;&#40784;&#65292;&#32780;&#19988;&#20445;&#30041;&#20102;NMF&#30340;&#21487;&#35299;&#37322;&#23646;&#24615;&#65292;&#20351;&#24471;&#19982;&#26631;&#20934;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#30456;&#27604;&#65292;&#24471;&#21040;&#20102;&#26356;&#21152;&#31232;&#30095;&#21644;&#35299;&#32806;&#30340;&#34920;&#31034;&#12290;&#20174;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#20026;NCL&#30340;&#21487;&#35782;&#21035;&#24615;&#21644;&#19979;&#28216;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#20445;&#35777;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12459v1 Announce Type: cross  Abstract: Deep representations have shown promising performance when transferred to downstream tasks in a black-box manner. Yet, their inherent lack of interpretability remains a significant challenge, as these features are often opaque to human understanding. In this paper, we propose Non-negative Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization (NMF) aimed at deriving interpretable features. The power of NCL lies in its enforcement of non-negativity constraints on features, reminiscent of NMF's capability to extract features that align closely with sample clusters. NCL not only aligns mathematically well with an NMF objective but also preserves NMF's interpretability attributes, resulting in a more sparse and disentangled representation compared to standard contrastive learning (CL). Theoretically, we establish guarantees on the identifiability and downstream generalization of NCL. Empirically, we show that these 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;</title><link>https://arxiv.org/abs/2403.11330</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#19968;&#20010;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#26469;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Agents by Decomposing One Global Explicit Annotation with Local Implicit Multimodal Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11330
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20840;&#23616;&#26126;&#30830;&#26631;&#27880;&#25286;&#35299;&#25104;&#26412;&#22320;&#38544;&#24335;&#22810;&#27169;&#24577;&#21453;&#39304;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#35805;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#23637;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#65288;&#21363;&#65292;&#23545;&#35805;&#32423;&#65289;&#22870;&#21169;&#23545;&#40784;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#33258;&#28982;&#21457;&#29983;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;&#21517;&#20026;GELI&#65289;&#36890;&#36807;&#23558;&#20154;&#31867;&#25552;&#20379;&#30340;&#20840;&#23616;&#26126;&#30830;&#65288;GE&#65289;&#20250;&#35805;&#32423;&#22870;&#21169;&#25286;&#20998;&#65292;&#21033;&#29992;&#26412;&#22320;&#38544;&#24335;&#65288;LI&#65289;&#22810;&#27169;&#24577;&#22870;&#21169;&#20449;&#21495;&#26469;&#36328;&#27169;&#24577;&#22320;&#22609;&#36896;&#22870;&#21169;&#20998;&#35299;&#27493;&#39588;&#12290;&#28982;&#21518;&#23558;&#36825;&#31181;&#20998;&#35299;&#30340;&#22870;&#21169;&#27169;&#22411;&#20316;&#20026;&#26631;&#20934;RHLF&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26469;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#23545;&#35805;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#20154;&#31867;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;GELI&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#21508;&#31181;&#23545;&#35805;&#24230;&#37327;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11330v1 Announce Type: cross  Abstract: We describe an approach for aligning an LLM-based dialogue agent based on global (i.e., dialogue-level) rewards, while also taking into account naturally-occurring multimodal signals. At a high level, our approach (dubbed GELI) learns a local, turn-level reward model by decomposing the human-provided Global Explicit (GE) session-level reward, using Local Implicit (LI} multimodal reward signals to crossmodally shape the reward decomposition step. This decomposed reward model is then used as part of the standard RHLF pipeline improve an LLM-based dialog agent. We run quantitative and qualitative human studies to evaluate the performance of our GELI approach, and find that it shows consistent improvements across various conversational metrics compared to baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;MixUp&#31574;&#30053;&#23545;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#36974;&#30422;&#65292;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10558</link><description>&lt;p&gt;
&#38024;&#23545;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#38544;&#31169;&#20445;&#25252;&#20154;&#33080;&#35782;&#21035;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hybrid Masking Strategy for Privacy-Preserving Face Recognition Against Model Inversion Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;MixUp&#31574;&#30053;&#23545;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#36974;&#30422;&#65292;&#20197;&#22312;&#38544;&#31169;&#20445;&#25252;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10558v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22312;&#35757;&#32451;&#20154;&#33080;&#35782;&#21035;&#65288;FR&#65289;&#27169;&#22411;&#20013;&#21033;&#29992;&#20010;&#20154;&#25935;&#24863;&#25968;&#25454;&#23384;&#22312;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#22240;&#20026;&#23545;&#25163;&#21487;&#20197;&#21033;&#29992;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#65288;MIA&#65289;&#25512;&#26029;&#20986;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#25968;&#25454;&#22686;&#24378;&#21644;&#24046;&#20998;&#38544;&#31169;&#65292;&#24050;&#34987;&#29992;&#26469;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#26080;&#27861;&#22312;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#36798;&#21040;&#26368;&#20339;&#24179;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;MIA&#30340;&#33258;&#36866;&#24212;&#28151;&#21512;&#36974;&#30422;&#31639;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#39057;&#22495;&#20013;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;MixUp&#31574;&#30053;&#23545;&#20154;&#33080;&#22270;&#20687;&#36827;&#34892;&#36974;&#30422;&#12290;&#19982;&#20027;&#35201;&#29992;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#20256;&#32479;MixUp&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20462;&#25913;&#30340;&#26041;&#27861;&#34701;&#21512;&#20102;&#39057;&#22495;&#28151;&#21512;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#21152;MixUp&#20013;&#28151;&#21512;&#30340;&#22270;&#20687;&#25968;&#37327;&#21487;&#20197;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#20294;&#20250;&#38477;&#20302;&#20154;&#33080;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10558v1 Announce Type: cross  Abstract: The utilization of personal sensitive data in training face recognition (FR) models poses significant privacy concerns, as adversaries can employ model inversion attacks (MIA) to infer the original training data. Existing defense methods, such as data augmentation and differential privacy, have been employed to mitigate this issue. However, these methods often fail to strike an optimal balance between privacy and accuracy. To address this limitation, this paper introduces an adaptive hybrid masking algorithm against MIA. Specifically, face images are masked in the frequency domain using an adaptive MixUp strategy. Unlike the traditional MixUp algorithm, which is predominantly used for data augmentation, our modified approach incorporates frequency domain mixing. Previous studies have shown that increasing the number of images mixed in MixUp can enhance privacy preservation but at the expense of reduced face recognition accuracy. To ove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08755</link><description>&lt;p&gt;
DAM:&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DAM: Dynamic Adapter Merging for Continual Video QA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#65288;VidQA&#65289;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DAM&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26469;&#65288;i&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#65288;ii&#65289;&#23454;&#29616;&#23545;&#25345;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#65288;iii&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22788;&#29702;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#65292;&#65288;iv&#65289;&#23454;&#29616;&#36328;&#30456;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#22312;&#32473;&#23450;&#19968;&#32452;&#25345;&#32493;&#27969;&#24335;&#20256;&#36755;&#30340;VidQA&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#39034;&#24207;&#35757;&#32451;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#39592;&#24178;&#30340;&#21442;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#26469;&#33258;&#26410;&#30693;&#39046;&#22495;&#30340;&#35270;&#39057;&#38382;&#39064;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#36335;&#30001;&#22120;&#20989;&#25968;&#35745;&#31639;&#27599;&#20010;&#36866;&#37197;&#22120;&#30340;&#27010;&#29575;&#65292;&#21453;&#26144;&#20986;&#35813;&#36866;&#37197;&#22120;&#19982;&#24403;&#21069;&#35270;&#39057;&#38382;&#39064;&#36755;&#20837;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#26696;&#32858;&#21512;&#25152;&#26377;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaddingFlow&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#20102;&#27491;&#35268;&#21270;&#27969;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#22312;&#27969;&#24418;&#21644;&#31163;&#25955;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08216</link><description>&lt;p&gt;
PaddingFlow&#65306;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#24402;&#19968;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
PaddingFlow: Improving Normalizing Flows with Padding-Dimensional Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08216
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PaddingFlow&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#20102;&#27491;&#35268;&#21270;&#27969;&#65292;&#35299;&#20915;&#20102;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#22312;&#27969;&#24418;&#21644;&#31163;&#25955;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#26159;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#37319;&#26679;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65292;&#21363;&#27969;&#24418;&#21644;&#31163;&#25955;&#25968;&#25454;&#12290; &#22914;&#26524;&#30446;&#26631;&#20998;&#24067;&#26159;&#19968;&#20010;&#27969;&#24418;&#65292;&#20063;&#23601;&#26159;&#35828;&#28508;&#22312;&#30446;&#26631;&#20998;&#24067;&#30340;&#32500;&#24230;&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#32500;&#24230;&#19981;&#21305;&#37197;&#65292;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#31163;&#25955;&#25968;&#25454;&#20250;&#23548;&#33268;&#22522;&#20110;&#27969;&#30340;&#27169;&#22411;&#22349;&#32553;&#20026;&#36864;&#21270;&#30340;&#28857;&#36136;&#37327;&#28151;&#21512;&#12290; &#20026;&#20102;&#35268;&#36991;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PaddingFlow&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#37327;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22635;&#20805;&#32500;&#24230;&#22122;&#22768;&#25913;&#36827;&#20102;&#27491;&#35268;&#21270;&#27969;&#12290;PaddingFlow&#26131;&#20110;&#23454;&#29616;&#12289;&#35745;&#31639;&#24265;&#20215;&#12289;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#29983;&#25104;&#26080;&#20559;&#20272;&#35745;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#29616;&#26377;&#21435;&#37327;&#21270;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#24517;&#39035;&#25913;&#21464;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08216v1 Announce Type: new  Abstract: Normalizing flow is a generative modeling approach with efficient sampling. However, Flow-based models suffer two issues, which are manifold and discrete data. If the target distribution is a manifold, which means the dimension of the latent target distribution and the dimension of the data distribution are unmatched, flow-based models might perform badly. Discrete data makes flow-based models collapse into a degenerate mixture of point masses. In this paper, to sidestep such two issues we propose PaddingFlow, a novel dequantization method, which improves normalizing flows with padding-dimensional noise. PaddingFlow is easy to implement, computationally cheap, widely suitable for various tasks, and generates samples that are unbiased estimations of the data. Especially, our method can overcome the limitation of existing dequantization methods that have to change the data distribution, which might degrade performance. We validate our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07890</link><description>&lt;p&gt;
$\widetilde{O}(T^{-1})$ &#25910;&#25947;&#21040;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
No-regret&#23398;&#20064;&#19982;&#21338;&#24328;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#32806;&#21512;&#30340;&#26080;&#24724;&#23398;&#20064;&#21160;&#24577;&#65292;&#24403;&#25152;&#26377;&#29609;&#23478;&#22312;&#27491;&#21017;&#24418;&#24335;&#28216;&#25103;&#20013;&#37319;&#29992;&#26102;&#65292;&#20197;$\widetilde{O}(T^{-1})$&#30340;&#25509;&#36817;&#26368;&#20248;&#36895;&#29575;&#25910;&#25947;&#21040;&#21508;&#31181;&#22343;&#34913;&#35299;&#65292;&#36825;&#26174;&#30528;&#25913;&#36827;&#20102;&#32463;&#20856;&#26080;&#24724;&#23398;&#20064;&#32773;&#30340;$O(1/\sqrt{T})$&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#31867;&#20284;&#30340;&#25910;&#25947;&#32467;&#26524;&#24456;&#23569;&#35265;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#65292;&#36830;&#21516;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;$\widetilde{O}(T^{-1})$&#36817;&#20284;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#20063;&#21253;&#25324;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07890v1 Announce Type: cross  Abstract: No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>NGG&#26159;&#19968;&#20010;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;&#65292;&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#22270;&#27169;&#24335;&#21644;&#25511;&#21046;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01535</link><description>&lt;p&gt;
&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65306;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01535
&lt;/p&gt;
&lt;p&gt;
NGG&#26159;&#19968;&#20010;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#20102;&#29305;&#24449;&#26465;&#20214;&#22270;&#29983;&#25104;&#65292;&#20855;&#26377;&#27169;&#25311;&#22797;&#26434;&#22270;&#27169;&#24335;&#21644;&#25511;&#21046;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#38754;&#20020;&#30528;&#29983;&#25104;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#29305;&#23450;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31070;&#32463;&#22270;&#29983;&#25104;&#22120;&#65288;NGG&#65289;&#36825;&#19968;&#26032;&#39062;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22270;&#29983;&#25104;&#12290;NGG&#23637;&#31034;&#20102;&#23545;&#22797;&#26434;&#22270;&#27169;&#24335;&#36827;&#34892;&#24314;&#27169;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#23545;&#22270;&#29983;&#25104;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;NGG&#21033;&#29992;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#21387;&#32553;&#65292;&#21033;&#29992;&#22312;&#28508;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#30001;&#24635;&#32467;&#22270;&#32479;&#35745;&#20449;&#24687;&#30340;&#21521;&#37327;&#25351;&#23548;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NGG&#22312;&#21508;&#31181;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36890;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#25429;&#25417;&#26399;&#26395;&#22270;&#23646;&#24615;&#24182;&#25512;&#24191;&#21040;&#26410;&#35265;&#22270;&#24418;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01535v1 Announce Type: new  Abstract: Graph generation has emerged as a crucial task in machine learning, with significant challenges in generating graphs that accurately reflect specific properties. Existing methods often fall short in efficiently addressing this need as they struggle with the high-dimensional complexity and varied nature of graph properties. In this paper, we introduce the Neural Graph Generator (NGG), a novel approach which utilizes conditioned latent diffusion models for graph generation. NGG demonstrates a remarkable capacity to model complex graph patterns, offering control over the graph generation process. NGG employs a variational graph autoencoder for graph compression and a diffusion process in the latent vector space, guided by vectors summarizing graph statistics. We demonstrate NGG's versatility across various graph generation tasks, showing its capability to capture desired graph properties and generalize to unseen graphs. This work signifies 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16358</link><description>&lt;p&gt;
&#19968;&#20010;&#25972;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Integrated Data Processing Framework for Pretraining Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#24182;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#32463;&#24120;&#38656;&#35201;&#25163;&#21160;&#20174;&#19981;&#21516;&#26469;&#28304;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20026;&#27599;&#20010;&#25968;&#25454;&#23384;&#20648;&#24211;&#24320;&#21457;&#19987;&#38376;&#30340;&#25968;&#25454;&#28165;&#27927;&#27969;&#31243;&#12290;&#32570;&#20047;&#32479;&#19968;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#36825;&#19968;&#36807;&#31243;&#37325;&#22797;&#32780;&#32321;&#29712;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#20102;&#22788;&#29702;&#27169;&#22359;&#21644;&#20998;&#26512;&#27169;&#22359;&#30340;&#25968;&#25454;&#22788;&#29702;&#26694;&#26550;&#65292;&#22788;&#29702;&#27169;&#22359;&#21253;&#25324;&#19968;&#31995;&#21015;&#19981;&#21516;&#31890;&#24230;&#27700;&#24179;&#30340;&#25805;&#20316;&#31526;&#65292;&#32780;&#20998;&#26512;&#27169;&#22359;&#25903;&#25345;&#23545;&#31934;&#28860;&#25968;&#25454;&#36827;&#34892;&#25506;&#26597;&#21644;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#26131;&#20110;&#20351;&#29992;&#19988;&#39640;&#24230;&#28789;&#27963;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#24182;&#23637;&#31034;&#23427;&#22312;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#31471;&#21040;&#31471;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16358v1 Announce Type: cross  Abstract: The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in 
&lt;/p&gt;</description></item><item><title>CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.15300</link><description>&lt;p&gt;
&#35265;&#35777;&#20026;&#20449;&#65306;&#36890;&#36807;CLIP&#24341;&#23548;&#35299;&#30721;&#32531;&#35299;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Seeing is Believing: Mitigating Hallucination in Large Vision-Language Models via CLIP-Guided Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15300
&lt;/p&gt;
&lt;p&gt;
CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#26356;&#24378;&#22823;&#21644;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#26631;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#25928;&#20943;&#23569;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#23481;&#26131;&#20986;&#29616;&#23545;&#35937;&#24187;&#35273;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#21253;&#21547;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#21477;&#23376;&#32423;LVLM&#24187;&#35273;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#65292;&#21457;&#29616;&#19982;&#22270;&#20687;&#30340;CLIP&#30456;&#20284;&#24615;&#20316;&#20026;&#19968;&#20010;&#27604;&#21333;&#35789;&#21487;&#33021;&#24615;&#26356;&#24378;&#22823;&#12289;&#26356;&#31283;&#20581;&#30340;&#24187;&#35273;&#25351;&#31034;&#22120;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLIP&#24341;&#23548;&#35299;&#30721;&#65288;CGD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#35299;&#30721;&#26102;&#30340;&#23545;&#35937;&#24187;&#35273;&#12290;CGD&#21033;&#29992;CLIP&#26469;&#24341;&#23548;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#65292;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25991;&#26412;&#19982;&#22270;&#20687;&#30340;&#35270;&#35273;&#32852;&#31995;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;CGD&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15300v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are susceptible to object hallucinations, an issue in which their generated text contains non-existent objects, greatly limiting their reliability and practicality. Current approaches often rely on the model's token likelihoods or other internal information, instruction tuning on additional datasets, or incorporating complex external tools. We first perform empirical analysis on sentence-level LVLM hallucination, finding that CLIP similarity to the image acts as a stronger and more robust indicator of hallucination compared to token likelihoods. Motivated by this, we introduce our CLIP-Guided Decoding (CGD) approach, a straightforward but effective training-free approach to reduce object hallucination at decoding time. CGD uses CLIP to guide the model's decoding process by enhancing visual grounding of generated text with the image. Experiments demonstrate that CGD effectively mitigates object hallu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;</title><link>https://arxiv.org/abs/2402.13251</link><description>&lt;p&gt;
FlashTex&#65306;&#20855;&#26377;LightControlNet&#30340;&#24555;&#36895;&#21487;&#37325;&#22609;&#32593;&#26684;&#32441;&#29702;
&lt;/p&gt;
&lt;p&gt;
FlashTex: Fast Relightable Mesh Texturing with LightControlNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FlashTex&#26041;&#27861;&#65292;&#22522;&#20110;LightControlNet&#23454;&#29616;&#20102;&#24555;&#36895;&#33258;&#21160;&#21270;3D&#32593;&#26684;&#32441;&#29702;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;&#30340;&#35299;&#32806;&#65292;&#20351;&#24471;&#32593;&#26684;&#33021;&#22815;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#19979;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#21160;&#20026;3D&#32593;&#26684;&#21019;&#24314;&#32441;&#29702;&#36153;&#26102;&#36153;&#21147;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#23478;&#35270;&#35273;&#20869;&#23481;&#21019;&#24314;&#32773;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#33258;&#21160;&#20026;&#36755;&#20837;&#30340;3D&#32593;&#26684;&#30528;&#33394;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#29031;&#26126;&#19982;&#34920;&#38754;&#26448;&#36136;/&#21453;&#23556;&#22312;&#29983;&#25104;&#30340;&#32441;&#29702;&#20013;&#35299;&#32806;&#65292;&#20197;&#20415;&#32593;&#26684;&#21487;&#20197;&#22312;&#20219;&#20309;&#29031;&#26126;&#29615;&#22659;&#20013;&#27491;&#30830;&#37325;&#29031;&#21644;&#28210;&#26579;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LightControlNet&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;ControlNet&#26550;&#26500;&#30340;&#26032;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#20801;&#35768;&#23558;&#25152;&#38656;&#29031;&#26126;&#35268;&#26684;&#20316;&#20026;&#23545;&#27169;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#32441;&#29702;&#31649;&#36947;&#28982;&#21518;&#20998;&#20004;&#20010;&#38454;&#27573;&#26500;&#24314;&#32441;&#29702;&#12290;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;LightControlNet&#29983;&#25104;&#32593;&#26684;&#30340;&#19968;&#32452;&#31232;&#30095;&#30340;&#35270;&#35273;&#19968;&#33268;&#30340;&#21442;&#32771;&#35270;&#22270;&#12290;&#31532;&#20108;&#38454;&#27573;&#24212;&#29992;&#22522;&#20110;&#20998;&#25968;&#33976;&#39311;&#37319;&#26679;&#65288;SDS&#65289;&#30340;&#32441;&#29702;&#20248;&#21270;&#65292;&#36890;&#36807;LightControlNet&#26469;&#25552;&#39640;&#32441;&#29702;&#36136;&#37327;&#21516;&#26102;&#35299;&#32806;&#34920;&#38754;&#26448;&#36136;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13251v1 Announce Type: cross  Abstract: Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09126</link><description>&lt;p&gt;
MPIrigen: &#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
MPIrigen: MPI Code Generation through Domain-Specific Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;MPI&#20195;&#30721;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;MonoCoder&#36827;&#34892;MPI-based&#31243;&#24207;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#20013;&#65292;&#39640;&#25928;&#30340;&#24182;&#34892;&#35745;&#31639;&#23588;&#20026;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#28040;&#24687;&#20256;&#36882;&#25509;&#21475;&#65288;MPI&#65289;&#38598;&#25104;&#39046;&#22495;&#12290;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#34892;&#32534;&#31243;&#20219;&#21153;&#65292;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#25506;&#35752;&#20102;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#31243;&#24207;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#21644;PolyCoder&#65288;&#19987;&#38376;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#27169;&#22411;&#65289;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;MPI&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#30456;&#27604;&#36890;&#29992;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;MPI&#30456;&#20851;&#32534;&#31243;&#35821;&#35328;C&#21644;C++&#39044;&#35757;&#32451;&#30340;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;MonoCoder&#30340;&#24615;&#33021;&#26356;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;HPCorpusMPI&#19978;&#23545;MonoCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;MPI-based&#31243;&#24207;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09126v1 Announce Type: cross Abstract: The imperative need to scale computation across numerous nodes highlights the significance of efficient parallel computing, particularly in the realm of Message Passing Interface (MPI) integration. The challenging parallel programming task of generating MPI-based parallel programs has remained unexplored. This study first investigates the performance of state-of-the-art language models in generating MPI-based parallel programs. Findings reveal that widely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual code models) exhibit notable performance degradation, when generating MPI-based programs compared to general-purpose programs. In contrast, domain-specific models such as MonoCoder, which are pretrained on MPI-related programming languages of C and C++, outperform larger models. Subsequently, we introduce a dedicated downstream task of MPI-based program generation by fine-tuning MonoCoder on HPCorpusMPI. We call the r
&lt;/p&gt;</description></item><item><title>GenSTL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#29983;&#25104;&#29305;&#24449;&#22495;&#26469;&#23454;&#29616;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.07232</link><description>&lt;p&gt;
GenSTL: &#36890;&#36807;&#29305;&#24449;&#22495;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#23454;&#29616;&#36890;&#29992;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07232
&lt;/p&gt;
&lt;p&gt;
GenSTL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#29983;&#25104;&#29305;&#24449;&#22495;&#26469;&#23454;&#29616;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#26159;&#26102;&#38388;&#25139;&#20301;&#32622;&#26679;&#26412;&#30340;&#24207;&#21015;&#12290;&#22312;&#31232;&#30095;&#36712;&#36857;&#20013;&#65292;&#20301;&#32622;&#26679;&#26412;&#30340;&#37319;&#26679;&#26159;&#19981;&#39057;&#32321;&#30340;&#65307;&#23613;&#31649;&#36825;&#31181;&#36712;&#36857;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#35201;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#19982;&#20132;&#36890;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#36712;&#36857;&#26159;&#23494;&#38598;&#37319;&#26679;&#30340;&#24182;&#19988;&#32463;&#36807;&#20934;&#30830;&#30340;&#22320;&#22270;&#21305;&#37197;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20004;&#38454;&#27573;&#26041;&#26696;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#25193;&#23637;&#31232;&#30095;&#36712;&#36857;&#30340;&#25928;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;GenSTL&#12290;&#35813;&#26694;&#26550;&#32463;&#36807;&#39044;&#35757;&#32451;&#20197;&#20351;&#29992;&#29305;&#24449;&#22495;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#24418;&#25104;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;GenSTL&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#25110;&#32773;&#21487;&#20197;&#20808;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GenSTL&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#21644;&#22320;&#22270;&#21305;&#37197;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#20854;&#20013;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#22495;&#32534;&#30721;&#23618;&#21644;&#20998;&#23618;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.03923</link><description>&lt;p&gt;
&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Return-Aligned Decision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65288;&#21363;&#22238;&#25253;&#65289;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24212;&#29992;&#33539;&#22260;&#30340;&#25193;&#22823;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#20351;&#23454;&#38469;&#22238;&#25253;&#19982;&#25351;&#23450;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#30340;&#26234;&#33021;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20174;&#32780;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#20915;&#31574;Transformer&#65288;DT&#65289;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#20248;&#21270;&#29983;&#25104;&#20197;&#30446;&#26631;&#22238;&#25253;&#20026;&#26465;&#20214;&#30340;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#24182;&#37197;&#22791;&#20102;&#20351;&#29992;&#30446;&#26631;&#22238;&#25253;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;DT&#26088;&#22312;&#23545;&#40784;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#65292;&#20294;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#20102;DT&#20013;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22238;&#25253;&#20174;&#20256;&#32479;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#22238;&#25253;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#21069;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#27979;&#35797;&#21644;&#35780;&#20272;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.01795</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#23569;&#26679;&#26412;&#22330;&#26223;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Scenario Testing for Autonomous Vehicles Based on Neighborhood Coverage and Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#27979;&#35797;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#21069;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#27979;&#35797;&#21644;&#35780;&#20272;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#37096;&#32626;&#20043;&#21069;&#65292;&#23545;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#36827;&#34892;&#27979;&#35797;&#21644;&#35780;&#20272;&#20854;&#23433;&#20840;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27979;&#35797;&#25104;&#26412;&#25110;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#38024;&#23545;&#29305;&#23450;AV&#27169;&#22411;&#30340;&#21487;&#25509;&#21463;&#27979;&#35797;&#25104;&#26412;&#36890;&#24120;&#20250;&#34987;&#26497;&#38480;&#21046;&#22320;&#38477;&#20302;&#12290;&#29616;&#26377;&#30340;&#27979;&#35797;&#26041;&#27861;&#20005;&#26684;&#38480;&#21046;&#30340;&#27979;&#35797;&#25968;&#30446;&#20250;&#23548;&#33268;&#27979;&#35797;&#32467;&#26524;&#30340;&#26174;&#33879;&#19981;&#30830;&#23450;&#24615;&#25110;&#25361;&#25112;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#8220;&#23569;&#26679;&#26412;&#27979;&#35797;&#65288;FST&#65289;&#8221;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;FST&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#23567;&#35268;&#27169;&#27979;&#35797;&#22330;&#26223;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20248;&#21270;&#22330;&#26223;&#21033;&#29992;&#65292;&#25105;&#20204;&#23558;FST&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22522;&#20110;&#37051;&#22495;&#35206;&#30422;&#21644;&#30456;&#20284;&#24615;&#23547;&#25214;&#19968;&#20010;&#23567;&#30340;&#22330;&#26223;&#38598;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#39564;&#20449;&#24687;&#20013;&#30340;&#20195;&#29702;&#27169;&#22411;&#65288;SMs&#65289;&#65292;&#25105;&#20204;&#21160;&#24577;&#35843;&#25972;&#27979;&#35797;&#22330;&#26223;&#38598;&#21512;&#21644;&#20854;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Testing and evaluating the safety performance of autonomous vehicles (AVs) is essential before the large-scale deployment. Practically, the acceptable cost of testing specific AV model can be restricted within an extremely small limit because of testing cost or time. With existing testing methods, the limitations imposed by strictly restricted testing numbers often result in significant uncertainties or challenges in quantifying testing results. In this paper, we formulate this problem for the first time the "few-shot testing" (FST) problem and propose a systematic FST framework to address this challenge. To alleviate the considerable uncertainty inherent in a small testing scenario set and optimize scenario utilization, we frame the FST problem as an optimization problem and search for a small scenario set based on neighborhood coverage and similarity. By leveraging the prior information on surrogate models (SMs), we dynamically adjust the testing scenario set and the contribution of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.11176</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#30446;&#26631;&#23450;&#20301;: &#20351;&#29992;Cram&#233;r-Rao&#30028;&#38480;&#23545;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Target Localization: Benchmarking Gradient Descent Using the Cram\'er-Rao Bound. (arXiv:2401.11176v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#12290;&#36825;&#19968;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#38647;&#36798;&#31995;&#32479;&#20013;&#65292;&#20351;&#29992;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#36827;&#34892;&#31934;&#30830;&#30340;&#30446;&#26631;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26080;&#20559;&#20272;&#35745;&#26041;&#27861;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#36798;&#21040;Cram&#233;r-Rao&#30028;&#38480;&#65288;CRB&#65289;&#30340;&#29702;&#35770;&#26497;&#38480;&#65292;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#35823;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#20248;&#20110;&#36825;&#20123;&#20256;&#32479;&#25216;&#26415;&#65292;&#22312;&#30446;&#26631;&#26041;&#20301;&#21644;&#36895;&#24230;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;&#20195;&#34920;&#24615;&#30340;&#27169;&#25311;&#22330;&#26223;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22987;&#32456;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#21442;&#25968;&#20272;&#35745;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#22266;&#26377;&#30340;&#20559;&#35265;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#20943;&#23567;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#38647;&#36798;&#31995;&#32479;&#20013;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20026;&#22312;&#26434;&#20081;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#26356;&#20934;&#30830;&#30340;&#23450;&#20301;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern radar systems, precise target localization using azimuth and velocity estimation is paramount. Traditional unbiased estimation methods have leveraged gradient descent algorithms to reach the theoretical limits of the Cram\'er Rao Bound (CRB) for the error of the parameter estimates. In this study, we present a data-driven neural network approach that outperforms these traditional techniques, demonstrating improved accuracies in target azimuth and velocity estimation. Using a representative simulated scenario, we show that our proposed neural network model consistently achieves improved parameter estimates due to its inherently biased nature, yielding a diminished mean squared error (MSE). Our findings underscore the potential of employing deep learning methods in radar systems, paving the way for more accurate localization in cluttered and dynamic environments.
&lt;/p&gt;</description></item><item><title>PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.09819</link><description>&lt;p&gt;
PPNet: &#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09819
&lt;/p&gt;
&lt;p&gt;
PPNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#35299;&#20915;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27714;&#35299;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;EDaGe-PP&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PPNet&#22312;&#35745;&#31639;&#26102;&#38388;&#21644;&#25104;&#21151;&#29575;&#26041;&#38754;&#27604;&#20854;&#20182;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22914;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#22312;&#21021;&#22987;&#35299;&#25935;&#24863;&#24615;&#21644;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#36895;&#24230;&#19978;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#20855;&#26377;&#26377;&#38480;&#21151;&#29575;/&#29123;&#26009;&#30340;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#65292;&#22312;&#30701;&#26102;&#38388;&#20869;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#23454;&#29616;&#31471;&#21040;&#31471;&#36817;&#20284;&#26368;&#20248;&#36335;&#24452;&#35268;&#21010;&#22120;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#21363;&#36335;&#24452;&#31354;&#38388;&#20998;&#27573;&#21644;&#32473;&#23450;&#36335;&#24452;&#31354;&#38388;&#20013;&#30340;&#33322;&#28857;&#29983;&#25104;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36335;&#24452;&#35268;&#21010;&#32593;&#32476;&#65288;PPNet&#65289;&#30340;&#20004;&#32423;&#32423;&#32852;&#31070;&#32463;&#32593;&#32476;&#26469;&#35299;&#20915;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#19978;&#36848;&#23376;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDaGe-PP&#30340;&#29992;&#20110;&#36335;&#24452;&#35268;&#21010;&#30340;&#39640;&#25928;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;PPNet&#35757;&#32451;&#38598;&#30001;EDaGe-PP&#29983;&#25104;&#30340;&#25104;&#21151;&#29575;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#25552;&#21319;&#20102;$2\times$&#65292;&#24635;&#35745;&#31639;&#26102;&#38388;&#23569;&#20110;1/33&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;PPNet&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The classical path planners, such as sampling-based path planners, have the limitations of sensitivity to the initial solution and slow convergence to the optimal solution. However, finding a near-optimal solution in a short period is challenging in many applications such as the autonomous vehicle with limited power/fuel. To achieve an end-to-end near-optimal path planner, we first divide the path planning problem into two subproblems, which are path's space segmentation and waypoints generation in the given path's space. We further propose a two-level cascade neural network named Path Planning Network (PPNet) to solve the path planning problem by solving the abovementioned subproblems. Moreover, we propose a novel efficient data generation method for path planning named EDaGe-PP. The results show the total computation time is less than 1/33 and the success rate of PPNet trained by the dataset that is generated by EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#27867;&#21270;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#26032;&#23450;&#20041;OoD&#25968;&#25454;&#20197;&#21450;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#20445;&#35777;&#20102;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.16243</link><description>&lt;p&gt;
&#26159;&#21542;&#25152;&#26377;&#26410;&#35265;&#25968;&#25454;&#37117;&#26159;OoD&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#27867;&#21270;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#37325;&#26032;&#23450;&#20041;OoD&#25968;&#25454;&#20197;&#21450;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#65292;&#20445;&#35777;&#20102;&#23545;&#26410;&#35265;&#25968;&#25454;&#30340;&#27169;&#22411;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#26410;&#35265;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#30452;&#34987;&#35270;&#20026;&#26159;OoD&#65288;out-of-distribution&#65289;&#65292;&#20351;&#20854;&#27867;&#21270;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24456;&#22810;&#35777;&#25454;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#22686;&#21152;&#21487;&#20197;&#21333;&#35843;&#22320;&#38477;&#20302;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#28982;&#32780;&#65292;&#20174;&#20854;&#20182;&#35266;&#23519;&#21644;&#20998;&#26512;&#26469;&#30475;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#12290;&#29305;&#21035;&#26159;&#24403;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#65292;&#24182;&#19988;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#20998;&#24067;&#28418;&#31227;&#26102;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#27979;&#35797;&#25968;&#25454;&#30340;&#27867;&#21270;&#35823;&#24046;&#37117;&#20250;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#21333;&#35843;&#20943;&#23567;&#12290;&#22312;&#32447;&#24615;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#20102;&#36825;&#31181;&#38750;&#21333;&#35843;&#29616;&#35937;&#65292;&#24182;&#36890;&#36807;&#22312;&#19981;&#21516;&#35270;&#35273;&#22522;&#20934;&#19978;&#36827;&#34892;&#32463;&#39564;&#35777;&#23454;&#12290;&#37492;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#20102;OoD&#25968;&#25454;&#65292;&#23558;&#20854;&#35270;&#20026;&#35757;&#32451;&#22495;&#30340;&#20984;&#21253;&#20043;&#22806;&#30340;&#25968;&#25454;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#26032;&#23450;&#20041;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#27867;&#21270;&#19978;&#30028;&#12290;&#23427;&#24847;&#21619;&#30528;&#23545;&#20110;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#35265;&#36807;&#30340;&#25968;&#25454;&#65292;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21487;&#20197;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.16877</link><description>&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#22635;&#20805;&#21644;&#36890;&#36807;&#26631;&#31614;&#22635;&#20805;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22635;&#20805;&#26041;&#27861;&#26469;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#37117;&#26377;&#26631;&#31614;&#65292;&#20294;&#24120;&#35265;&#30340;&#22635;&#20805;&#26041;&#27861;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#36755;&#20837;&#32780;&#24573;&#30053;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23558;&#26631;&#31614;&#22534;&#21472;&#21040;&#36755;&#20837;&#20013;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#36755;&#20837;&#30340;&#22635;&#20805;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23558;&#39044;&#27979;&#30340;&#27979;&#35797;&#26631;&#31614;&#21021;&#22987;&#21270;&#20026;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#22312;&#19968;&#36215;&#36827;&#34892;&#22635;&#20805;&#12290;&#36825;&#26679;&#21487;&#20197;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#32780;&#19988;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#22635;&#20805;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36830;&#32493;&#22411;&#12289;&#20998;&#31867;&#22411;&#25110;&#28151;&#21512;&#22411;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20272;&#35745;&#26925;&#22278;&#21644;&#25243;&#29289;&#22411;&#38382;&#39064;&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00259</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#26377;&#38480;&#24046;&#20998;&#30340;&#26080;&#30417;&#30563;&#23567;&#22411;&#32447;&#24615;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26925;&#22278;&#21644;&#25243;&#29289;&#22411;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solutions to Elliptic and Parabolic Problems via Finite Difference Based Unsupervised Small Linear Convolutional Neural Networks. (arXiv:2311.00259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00259
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20272;&#35745;&#26925;&#22278;&#21644;&#25243;&#29289;&#22411;&#38382;&#39064;&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#31185;&#23398;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;PDE&#27714;&#35299;&#22120;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#26631;&#35760;&#30340;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#25512;&#24191;&#21040;&#20998;&#24067;&#20043;&#22806;&#30340;&#31034;&#20363;&#26102;&#23481;&#26131;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#23569;&#20256;&#32479;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#20272;&#35745;PDE&#35299;&#26102;&#36935;&#21040;&#30340;&#24191;&#20041;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#23567;&#22411;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#20272;&#35745;PDE&#30340;&#26377;&#38480;&#24046;&#20998;&#35299;&#12290;&#19982;&#26377;&#38480;&#24046;&#20998;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#20960;&#20010;&#36873;&#23450;&#30340;&#26925;&#22278;&#21644;&#25243;&#29289;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#19982;&#30495;&#35299;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a growing interest in leveraging deep learning and neural networks to address scientific problems, particularly in solving partial differential equations (PDEs). However, current neural network-based PDE solvers often rely on extensive training data or labeled input-output pairs, making them prone to challenges in generalizing to out-of-distribution examples. To mitigate the generalization gap encountered by conventional neural network-based methods in estimating PDE solutions, we formulate a fully unsupervised approach, requiring no training data, to estimate finite difference solutions for PDEs directly via small convolutional neural networks. Our proposed algorithms demonstrate a comparable accuracy to the true solution for several selected elliptic and parabolic problems compared to the finite difference method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.18144</link><description>&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#26469;&#25913;&#36827;&#20869;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#24314;&#22266;&#23450;&#30446;&#26631;&#65292;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#22312;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25506;&#32034;&#22870;&#21169;&#36890;&#36807;&#23450;&#20041;&#33258;&#23450;&#20041;&#30340;&#20869;&#22312;&#30446;&#26631;&#26469;&#24341;&#23548;&#38271;&#26399;&#25506;&#32034;&#12290;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#20351;&#29992;&#29366;&#24577;&#35775;&#38382;&#39057;&#29575;&#26469;&#33719;&#24471;&#25506;&#32034;&#22870;&#21169;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20219;&#20309;&#20174;&#22522;&#20110;&#35745;&#25968;&#30340;&#26041;&#27861;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#37117;&#26159;&#38750;&#22266;&#23450;&#30340;&#65292;&#22240;&#27492;&#20026;&#20195;&#29702;&#20154;&#26500;&#24314;&#20102;&#19968;&#20010;&#38590;&#20197;&#20248;&#21270;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;&#22686;&#24378;&#29366;&#24577;&#34920;&#31034;&#23558;&#21407;&#22987;&#30340;&#38750;&#22266;&#23450;&#22870;&#21169;&#36716;&#21270;&#20026;&#22266;&#23450;&#22870;&#21169;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25506;&#32034;&#30340;&#22266;&#23450;&#30446;&#26631;&#65288;SOFE&#65289;&#26694;&#26550;&#12290;SOFE&#38656;&#35201;&#35782;&#21035;&#19981;&#21516;&#25506;&#32034;&#22870;&#21169;&#30340;&#36275;&#22815;&#32479;&#35745;&#37327;&#65292;&#24182;&#25214;&#21040;&#19968;&#31181;&#23558;&#36825;&#20123;&#32479;&#35745;&#37327;&#39640;&#25928;&#32534;&#30721;&#20316;&#20026;&#28145;&#24230;&#32593;&#32476;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;SOFE&#22522;&#20110;&#25552;&#20986;&#25193;&#23637;&#29366;&#24577;&#31354;&#38388;&#30340;&#29366;&#24577;&#22686;&#24378;&#65292;&#20294;&#26377;&#24076;&#26395;&#31616;&#21270;&#20195;&#29702;&#30446;&#26631;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SOFE&#25913;&#21892;&#20102;&#25506;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03720</link><description>&lt;p&gt;
HeaP: &#20351;&#29992;LLMs&#36827;&#34892;&#23618;&#27425;&#21270;Web&#21160;&#20316;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
HeaP: Hierarchical Policies for Web Actions using LLMs. (arXiv:2310.03720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;HeaP&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;Web&#20219;&#21153;&#30340;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#30340;&#31574;&#30053;&#26469;&#25191;&#34892;&#65292;&#30456;&#27604;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#21516;&#30340;Web&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23569;&#37327;&#25968;&#25454;&#21644;&#38646;-shot&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25945;&#25480;LLMs&#22312;Web&#19978;&#25191;&#34892;&#20219;&#21153;&#38754;&#20020;&#30528;&#22522;&#26412;&#25361;&#25112; - &#32452;&#21512;&#24615;&#22823;&#30340;&#24320;&#25918;&#19990;&#30028;&#20219;&#21153;&#21644;Web&#25509;&#21475;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;LLMs&#23558;Web&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#23376;&#20219;&#21153;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#27599;&#20010;&#23376;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20302;&#32423;&#30340;&#38381;&#29615;&#31574;&#30053;&#26469;&#35299;&#20915;&#12290;&#36825;&#20123;&#31574;&#30053;&#26500;&#25104;&#20102;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#35821;&#27861;&#65292;&#21363;&#26032;&#30340;Web&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#36825;&#20123;&#31574;&#30053;&#30340;&#32452;&#21512;&#26469;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;LLMs&#30340;Hierarchical Policies for Web Actions&#65288;HeaP&#65289;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#19968;&#32452;&#23618;&#27425;&#21270;&#30340;LLM&#25552;&#31034;&#26469;&#35268;&#21010;&#39640;&#32423;&#20219;&#21153;&#24182;&#36890;&#36807;&#19968;&#31995;&#21015;&#20302;&#32423;&#31574;&#30053;&#25191;&#34892;&#23427;&#20204;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#22871;Web&#20219;&#21153;&#65292;&#21253;&#25324;MiniWoB++&#65292;WebArena&#65292;&#27169;&#25311;&#33322;&#31354;CRM&#20197;&#21450;&#23454;&#38469;&#32593;&#31449;&#26469;&#35780;&#20272;HeaP&#19982;&#19968;&#31995;&#21015;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#24182;&#20135;&#29983;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.00105</link><description>&lt;p&gt;
&#28508;&#31354;&#38388;&#30340;&#23545;&#31216;&#24615;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Latent Space Symmetry Discovery. (arXiv:2310.00105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00105
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#24182;&#20135;&#29983;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#26126;&#30830;&#30693;&#36947;&#23545;&#31216;&#32676;&#12290;&#33258;&#21160;&#23545;&#31216;&#24615;&#21457;&#29616;&#26041;&#27861;&#26088;&#22312;&#25918;&#23485;&#36825;&#20010;&#32422;&#26463;&#65292;&#24182;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#31216;&#24615;&#21457;&#29616;&#26041;&#27861;&#22312;&#25628;&#32034;&#31354;&#38388;&#20013;&#20165;&#38480;&#20110;&#32447;&#24615;&#23545;&#31216;&#24615;&#65292;&#26080;&#27861;&#22788;&#29702;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23545;&#31216;&#24615;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#26159;&#39640;&#32500;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#28508;&#31354;&#38388;LieGAN&#65288;LaLiGAN&#65289;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#12290;&#23427;&#23398;&#20064;&#20102;&#19968;&#31181;&#20174;&#25968;&#25454;&#21040;&#28508;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#22312;&#20854;&#20013;&#23545;&#31216;&#24615;&#21464;&#24471;&#32447;&#24615;&#65292;&#24182;&#21516;&#26102;&#21457;&#29616;&#28508;&#31354;&#38388;&#20013;&#30340;&#23545;&#31216;&#24615;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#34920;&#31034;&#20219;&#20309;&#38750;&#32447;&#24615;&#23545;&#31216;&#24615;&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#39640;&#32500;&#35266;&#27979;&#20013;&#30340;&#20869;&#22312;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#32467;&#26500;&#33391;&#22909;&#30340;&#28508;&#31354;&#38388;&#65292;&#23545;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LaLiGAN&#22312;&#25913;&#36827;&#26041;&#31243;&#21457;&#29616;&#26041;&#38754;&#30340;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariant neural networks require explicit knowledge of the symmetry group. Automatic symmetry discovery methods aim to relax this constraint and learn invariance and equivariance from data. However, existing symmetry discovery methods are limited to linear symmetries in their search space and cannot handle the complexity of symmetries in real-world, often high-dimensional data. We propose a novel generative model, Latent LieGAN (LaLiGAN), which can discover nonlinear symmetries from data. It learns a mapping from data to a latent space where the symmetries become linear and simultaneously discovers symmetries in the latent space. Theoretically, we show that our method can express any nonlinear symmetry under certain conditions. Experimentally, our method can capture the intrinsic symmetry in high-dimensional observations, which results in a well-structured latent space that is useful for other downstream tasks. We demonstrate the use cases for LaLiGAN in improving equation discovery
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;</title><link>http://arxiv.org/abs/2309.15877</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15877
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#21551;&#21457;&#30340;&#20998;&#23618;&#22810;&#27169;&#24577;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#29702;&#35770;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#65292;&#23454;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#21644;&#22788;&#29702;&#26469;&#33258;&#22810;&#31181;&#20449;&#24687;&#28304;&#25110;&#27169;&#24577;&#23545;&#20110;&#33719;&#24471;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20449;&#24687;&#35770;&#20998;&#23618;&#24863;&#30693;(ITHP)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#20449;&#24687;&#29942;&#39048;&#30340;&#27010;&#24565;&#12290;&#19982;&#22823;&#22810;&#25968;&#26088;&#22312;&#23558;&#25152;&#26377;&#27169;&#24577;&#32435;&#20837;&#36755;&#20837;&#30340;&#20256;&#32479;&#34701;&#21512;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#20027;&#35201;&#27169;&#24577;&#25351;&#23450;&#20026;&#36755;&#20837;&#65292;&#32780;&#20854;&#20313;&#27169;&#24577;&#21017;&#20316;&#20026;&#20449;&#24687;&#36335;&#24452;&#20013;&#30340;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#36755;&#20837;&#27169;&#24577;&#29366;&#24577;&#20043;&#38388;&#26368;&#23567;&#21270;&#30456;&#20114;&#20449;&#24687;&#24182;&#22312;&#28508;&#22312;&#29366;&#24577;&#21644;&#20854;&#20313;&#27169;&#24577;&#20043;&#38388;&#26368;&#22823;&#21270;&#30456;&#20114;&#20449;&#24687;&#30340;&#24179;&#34913;&#65292;&#26500;&#24314;&#19968;&#31181;&#26377;&#25928;&#19988;&#32039;&#20945;&#30340;&#20449;&#24687;&#27969;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20445;&#30041;&#30456;&#20851;&#20449;&#24687;&#24182;&#26368;&#23567;&#21270;&#20887;&#20313;&#30340;&#32039;&#20945;&#28508;&#22312;&#29366;&#24577;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07136</link><description>&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;Transformer&#30340;&#24515;&#30005;&#22270;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;MTECG&#65292;&#25193;&#23637;&#20102;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#65292;&#24182;&#36827;&#34892;&#20102;&#28040;&#34701;&#23454;&#39564;&#39564;&#35777;&#20102;&#37325;&#26500;&#30446;&#26631;&#30340;&#27874;&#21160;&#24615;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26159;&#20020;&#24202;&#24212;&#29992;&#20013;&#26368;&#37325;&#35201;&#30340;&#35786;&#26029;&#24037;&#20855;&#20043;&#19968;&#12290;&#38543;&#30528;&#20808;&#36827;&#31639;&#27861;&#30340;&#20986;&#29616;&#65292;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#24212;&#29992;&#20110;ECG&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;&#20854;&#22312;ECG&#25968;&#25454;&#19978;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#23454;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#29992;&#30340;&#22522;&#20110;&#25513;&#30721;Transformer&#30340;ECG&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;MTECG&#65292;&#23427;&#23558;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#20102;ECG&#26102;&#38388;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;220,251&#20010;ECG&#35760;&#24405;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#35760;&#24405;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35786;&#26029;&#27880;&#37322;&#65292;&#20197;&#25506;&#32034;MTECG&#30340;&#29305;&#24615;&#12290;&#22312;&#25552;&#20986;&#30340;&#35757;&#32451;&#31574;&#30053;&#19979;&#65292;&#19968;&#20010;&#21482;&#26377;5.7M&#21442;&#25968;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#25513;&#30721;&#27604;&#20363;&#65288;5%-75%&#65289;&#19979;&#34920;&#29616;&#31283;&#23450;&#33391;&#22909;&#12290;&#28040;&#34701;&#30740;&#31350;&#31361;&#20986;&#20102;&#27874;&#21160;&#37325;&#26500;&#30446;&#26631;&#12289;&#35757;&#32451;&#35745;&#21010;&#38271;&#24230;&#12289;&#36880;&#23618;&#23398;&#20064;&#29575;&#34928;&#20943;&#21644;DropPath&#29575;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;MTECG&#32791;&#26102;&#36739;&#23569;&#19988;&#33021;&#22815;&#26377;&#25928;&#20998;&#31867;&#21508;&#31181;&#24515;&#30005;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram (ECG) is one of the most important diagnostic tools in clinical applications. With the advent of advanced algorithms, various deep learning models have been adopted for ECG tasks. However, the potential of Transformers for ECG data is not yet realized, despite their widespread success in computer vision and natural language processing. In this work, we present a useful masked Transformer method for ECG classification referred to as MTECG, which expands the application of masked autoencoders to ECG time series. We construct a dataset comprising 220,251 ECG recordings with a broad range of diagnoses annoated by medical experts to explore the properties of MTECG. Under the proposed training strategies, a lightweight model with 5.7M parameters performs stably well on a broad range of masking ratios (5%-75%). The ablation studies highlight the importance of fluctuated reconstruction targets, training schedule length, layer-wise LR decay and DropPath rate. The experiments o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2309.03847</link><description>&lt;p&gt;
&#39640;&#26031;&#28151;&#21512;&#29289;&#21487;&#20197;&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Mixtures of Gaussians are Privately Learnable with a Polynomial Number of Samples. (arXiv:2309.03847v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#21644;&#24046;&#20998;&#38544;&#31169;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#24046;&#20998;&#38544;&#31169;(DP)&#32422;&#26463;&#19979;&#20272;&#35745;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;&#20351;&#29992;$\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$&#20010;&#26679;&#26412;&#21363;&#21487;&#22312;&#28385;&#36275;$(\varepsilon, \delta)$-DP&#30340;&#26465;&#20214;&#19979;&#20272;&#35745;$k$&#20010;&#39640;&#26031;&#28151;&#21512;&#29289;&#65292;&#20351;&#20854;&#36798;&#21040;&#24635;&#21464;&#24046;&#36317;&#31163;$\alpha$&#12290;&#36825;&#26159;&#35813;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24615;&#19978;&#38480;&#65292;&#32780;&#26080;&#38656;&#23545;GMMs&#20570;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#21487;&#33021;&#20063;&#26377;&#29992;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#19968;&#20010;&#20998;&#24067;&#31867;&#65288;&#27604;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#26159;&#65288;1&#65289;&#21487;&#21015;&#34920;&#35793;&#30721;&#30340;&#24182;&#19988;&#65288;2&#65289;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#26041;&#38754;&#20855;&#26377;&#8220;&#23616;&#37096;&#23567;&#8221;&#35206;&#30422;[ BKSW19]&#65292;&#21017;&#20854;&#28151;&#21512;&#29289;&#31867;&#26159;&#31169;&#23494;&#21487;&#23398;&#20064;&#30340;&#12290;&#35777;&#26126;&#32469;&#36807;&#20102;&#19968;&#20010;&#24050;&#30693;&#38556;&#30861;&#65292;&#34920;&#26126;&#19982;&#39640;&#26031;&#20998;&#24067;&#19981;&#21516;&#65292;GMMs&#19981;&#20855;&#26377;&#23616;&#37096;&#23567;&#30340;&#35206;&#30422;[AAL21]&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of estimating mixtures of Gaussians under the constraint of differential privacy (DP). Our main result is that $\tilde{O}(k^2 d^4 \log(1/\delta) / \alpha^2 \varepsilon)$ samples are sufficient to estimate a mixture of $k$ Gaussians up to total variation distance $\alpha$ while satisfying $(\varepsilon, \delta)$-DP. This is the first finite sample complexity upper bound for the problem that does not make any structural assumptions on the GMMs.  To solve the problem, we devise a new framework which may be useful for other tasks. On a high level, we show that if a class of distributions (such as Gaussians) is (1) list decodable and (2) admits a "locally small'' cover [BKSW19] with respect to total variation distance, then the class of its mixtures is privately learnable. The proof circumvents a known barrier indicating that, unlike Gaussians, GMMs do not admit a locally small cover [AAL21].
&lt;/p&gt;</description></item><item><title>R2D2&#26159;&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#20013;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#37319;&#29992;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#65292;&#37325;&#24314;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#24378;&#24230;&#25104;&#20687;&#12290;</title><link>http://arxiv.org/abs/2309.03291</link><description>&lt;p&gt;
R2D2: &#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#23398;&#20013;&#36817;&#23454;&#26102;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;
&lt;/p&gt;
&lt;p&gt;
R2D2: Deep neural network series for near real-time high-dynamic range imaging in radio astronomy. (arXiv:2309.03291v1 [astro-ph.IM] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03291
&lt;/p&gt;
&lt;p&gt;
R2D2&#26159;&#29992;&#20110;&#23556;&#30005;&#22825;&#25991;&#20013;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#37319;&#29992;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#65292;&#37325;&#24314;&#20026;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#65292;&#21487;&#29992;&#20110;&#39640;&#20998;&#36776;&#29575;&#24378;&#24230;&#25104;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#23556;&#30005;&#24178;&#28041;&#27979;&#37327;&#65288;RI&#65289;&#22312;&#22825;&#25991;&#23398;&#20013;&#23454;&#29616;&#39640;&#20998;&#36776;&#29575;&#39640;&#21160;&#24577;&#33539;&#22260;&#21512;&#25104;&#25104;&#20687;&#12290;R2D2&#20195;&#34920;&#8220;&#39640;&#21160;&#24577;&#33539;&#22260;&#25104;&#20687;&#30340;&#27531;&#24046;&#21040;&#27531;&#24046;DNN&#31995;&#21015;&#8221;&#65292;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#28151;&#21512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#26356;&#26032;&#12290;&#23427;&#30340;&#37325;&#24314;&#26159;&#30001;&#19968;&#31995;&#21015;&#27531;&#24046;&#22270;&#20687;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#27531;&#24046;&#22270;&#20687;&#34987;&#20272;&#35745;&#20026;DNN&#30340;&#36755;&#20986;&#65292;&#27599;&#20010;DNN&#37117;&#20197;&#19978;&#19968;&#27425;&#36845;&#20195;&#30340;&#27531;&#24046;&#33039;&#22270;&#29255;&#20316;&#20026;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#20026;&#21305;&#37197;&#36861;&#36394;&#26041;&#27861;&#30340;&#23398;&#20064;&#29256;&#26412;&#65292;&#20854;&#20013;&#27169;&#22411;&#32452;&#20214;&#20174;&#27531;&#24046;&#33039;&#22270;&#29255;&#20013;&#36845;&#20195;&#22320;&#35782;&#21035;&#20986;&#26469;&#65292;CLEAN&#23601;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;R2D2&#27169;&#22411;&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#20998;&#21035;&#22522;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;DNN&#26550;&#26500;&#65306;&#26631;&#20934;&#30340;U-Net&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#23637;&#24320;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;S&#27874;&#27573;&#23545;&#23556;&#30005;&#26143;&#31995;Cygnus~A&#30340;&#39640;&#28789;&#25935;&#24230;&#35266;&#27979;&#20013;&#29992;&#20110;&#21333;&#33394;&#24378;&#24230;&#25104;&#20687;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel AI approach for high-resolution high-dynamic range synthesis imaging by radio interferometry (RI) in astronomy. R2D2, standing for "{R}esidual-to-{R}esidual {D}NN series for high-{D}ynamic range imaging", is a model-based data-driven approach relying on hybrid deep neural networks (DNNs) and data-consistency updates. Its reconstruction is built as a series of residual images estimated as the outputs of DNNs, each taking the residual dirty image of the previous iteration as an input. The approach can be interpreted as a learned version of a matching pursuit approach, whereby model components are iteratively identified from residual dirty images, and of which CLEAN is a well-known example. We propose two variants of the R2D2 model, built upon two distinctive DNN architectures: a standard U-Net, and a novel unrolled architecture. We demonstrate their use for monochromatic intensity imaging on highly-sensitive observations of the radio galaxy Cygnus~A at S band, from the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2309.00236</link><description>&lt;p&gt;
&#22270;&#20687;&#21163;&#25345;&#65306;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#23545;&#25239;&#24615;&#22270;&#20687;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#12290;&#36890;&#36807;&#30740;&#31350;&#19977;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#25915;&#20987;&#23545;&#26368;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#39640;&#36798;90&#65285;&#20197;&#19978;&#30340;&#25104;&#21151;&#29575;&#12290;&#35813;&#30740;&#31350;&#24341;&#21457;&#20102;&#23545;&#22522;&#30784;&#27169;&#22411;&#23433;&#20840;&#24615;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#20813;&#21463;&#24694;&#24847;&#34892;&#20026;&#32773;&#30340;&#25915;&#20987;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#22270;&#20687;&#36755;&#20837;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#22270;&#20687;&#21163;&#25345;&#65292;&#21363;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#25511;&#21046;&#29983;&#25104;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34892;&#20026;&#21305;&#37197;&#8221;&#30340;&#36890;&#29992;&#26041;&#27861;&#26469;&#21019;&#24314;&#22270;&#20687;&#21163;&#25345;&#65292;&#24182;&#29992;&#23427;&#26469;&#25506;&#32034;&#19977;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#20855;&#20307;&#23383;&#31526;&#20018;&#25915;&#20987;&#21487;&#20197;&#29983;&#25104;&#20219;&#24847;&#34987;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#36755;&#20986;&#65307;&#27844;&#38706;&#19978;&#19979;&#25991;&#25915;&#20987;&#21487;&#20197;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#30340;&#20449;&#24687;&#27844;&#38706;&#21040;&#36755;&#20986;&#20013;&#65307;&#36234;&#29425;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#27169;&#22411;&#30340;&#23433;&#20840;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;CLIP&#21644;LLaMA-2&#30340;&#26368;&#26032;VLM&#27169;&#22411;LLaVA-2&#36827;&#34892;&#20102;&#36825;&#20123;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#25152;&#26377;&#30340;&#25915;&#20987;&#31867;&#22411;&#25104;&#21151;&#29575;&#22343;&#22312;90&#65285;&#20197;&#19978;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#30340;&#25915;&#20987;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#21482;&#38656;&#35201;&#23545;&#22270;&#20687;&#36827;&#34892;&#23567;&#30340;&#25200;&#21160;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#25285;&#24551;&#12290;&#22914;&#26524;&#22270;&#20687;&#21163;&#25345;&#19982;CIFAR-10&#20013;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19968;&#26679;&#38590;&#20197;&#38450;&#24481;&#65292;&#37027;&#20040;&#21487;&#33021;&#38656;&#35201;&#24456;&#22810;&#24180;&#25165;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2307.14642</link><description>&lt;p&gt;
&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#32447;&#24615;&#25910;&#25947;&#24615;&#65306;&#25105;&#20204;&#24212;&#35813;&#22362;&#25345;&#21040;&#24213;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?. (arXiv:2307.14642v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#20197;&#20960;&#20309;&#36895;&#24230;&#25910;&#25947;&#65292;&#20026;BBVI&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#23545;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#25913;&#36827;&#65292;&#23545;&#27604;&#20102;STL&#20272;&#35745;&#22120;&#65292;&#24182;&#32473;&#20986;&#20102;&#26126;&#30830;&#30340;&#38750;&#28176;&#36817;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#24102;&#26377;&#25511;&#21046;&#21464;&#37327;&#30340;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#65288;BBVI&#65289;&#65292;&#29305;&#21035;&#26159;&#30528;&#38470;&#31283;&#23450;&#65288;STL&#65289;&#20272;&#35745;&#22120;&#65292;&#22312;&#23436;&#32654;&#21464;&#20998;&#26063;&#35268;&#33539;&#19979;&#25910;&#25947;&#20110;&#20960;&#20309;&#65288;&#20256;&#32479;&#19978;&#31216;&#20026;&#8220;&#32447;&#24615;&#8221;&#65289;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;STL&#20272;&#35745;&#22120;&#30340;&#26799;&#24230;&#26041;&#24046;&#30340;&#20108;&#27425;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#21253;&#25324;&#20102;&#35823;&#25351;&#23450;&#30340;&#21464;&#20998;&#26063;&#12290;&#32467;&#21512;&#20808;&#21069;&#20851;&#20110;&#20108;&#27425;&#26041;&#24046;&#26465;&#20214;&#30340;&#24037;&#20316;&#65292;&#36825;&#30452;&#25509;&#26263;&#31034;&#20102;&#22312;&#20351;&#29992;&#25237;&#24433;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;BBVI&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#36824;&#25913;&#36827;&#20102;&#29616;&#26377;&#23545;&#20110;&#27491;&#24120;&#23553;&#38381;&#24418;&#24335;&#29109;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20998;&#26512;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#20854;&#19982;STL&#20272;&#35745;&#22120;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#20026;&#20004;&#32773;&#25552;&#20379;&#26126;&#30830;&#30340;&#38750;&#28176;&#36827;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove that black-box variational inference (BBVI) with control variates, particularly the sticking-the-landing (STL) estimator, converges at a geometric (traditionally called "linear") rate under perfect variational family specification. In particular, we prove a quadratic bound on the gradient variance of the STL estimator, one which encompasses misspecified variational families. Combined with previous works on the quadratic variance condition, this directly implies convergence of BBVI with the use of projected stochastic gradient descent. We also improve existing analysis on the regular closed-form entropy gradient estimators, which enables comparison against the STL estimator and provides explicit non-asymptotic complexity guarantees for both.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2305.15557</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#29575;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#26469;&#35782;&#21035;&#38750;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#33539;&#24335;&#20381;&#36182;&#20110;&#29366;&#24577;&#30340;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#30456;&#24212;&#30340;Fokker-Planck&#26041;&#31243;&#30340;&#22522;&#20110;RKHS&#30340;&#36817;&#20284;&#25311;&#21512;&#21040;&#36825;&#20123;&#35266;&#27979;&#20540;&#65292;&#20174;&#32780;&#24471;&#20986;&#29702;&#35770;&#23398;&#20064;&#36895;&#29575;&#30340;&#20272;&#35745;&#20540;&#65292;&#36825;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#24403;&#26410;&#30693;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#36234;&#39640;&#26102;&#65292;&#29702;&#35770;&#20272;&#35745;&#20540;&#36234;&#26469;&#36234;&#32039;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20869;&#26680;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;&#39044;&#22788;&#29702;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#24471;&#21040;&#26377;&#25928;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel non-parametric learning paradigm for the identification of drift and diffusion coefficients of non-linear stochastic differential equations, which relies upon discrete-time observations of the state. The key idea essentially consists of fitting a RKHS-based approximation of the corresponding Fokker-Planck equation to such observations, yielding theoretical estimates of learning rates which, unlike previous works, become increasingly tighter when the regularity of the unknown drift and diffusion coefficients becomes higher. Our method being kernel-based, offline pre-processing may in principle be profitably leveraged to enable efficient numerical implementation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#24863;&#26579;&#31867;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#27700;&#24179;10-40 mg/L&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#23545;&#20110;&#35786;&#26029;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07877</link><description>&lt;p&gt;
&#22522;&#20110;&#20363;&#34892;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37492;&#21035;&#30149;&#27602;&#21644;&#32454;&#33740;&#24863;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiating Viral and Bacterial Infections: A Machine Learning Model Based on Routine Blood Test Values. (arXiv:2305.07877v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#34880;&#28082;&#26816;&#26597;&#25968;&#20540;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#35782;&#21035;&#24863;&#26579;&#31867;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#27700;&#24179;10-40 mg/L&#33539;&#22260;&#20869;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#30340;&#20934;&#30830;&#24615;&#65292;&#35777;&#26126;&#20102;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#23545;&#20110;&#35786;&#26029;&#20915;&#31574;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25239;&#29983;&#32032;&#32784;&#33647;&#24615;&#26085;&#30410;&#23041;&#32961;&#65292;&#27491;&#30830;&#21306;&#20998;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#20197;&#36827;&#34892;&#27491;&#30830;&#30340;&#25239;&#29983;&#32032;&#20351;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;16&#20010;&#20363;&#34892;&#34880;&#28082;&#26816;&#26597;&#32467;&#26524;&#12289;C-&#21453;&#24212;&#34507;&#30333;&#27700;&#24179;&#12289;&#29983;&#29289;&#24615;&#21035;&#21644;&#24180;&#40836;&#30340;&#30149;&#27602;&#19982;&#32454;&#33740;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#21306;&#20998;&#36825;&#20123;&#24863;&#26579;&#31867;&#22411;&#12290;&#20351;&#29992;&#21333;&#20010;&#21307;&#30103;&#20013;&#24515;&#30340;44,120&#20010;&#26696;&#20363;&#25968;&#25454;&#38598;&#65292;"&#30149;&#27602; vs. &#32454;&#33740;"&#27169;&#22411;&#34920;&#29616;&#20986;&#20196;&#20154;&#30633;&#30446;&#30340;82.2%&#30340;&#20934;&#30830;&#29575;&#65292;0.129&#30340;Brier&#24471;&#20998;&#21644;0.91&#30340;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;CRP&#20915;&#31574;&#35268;&#21017;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22312;CRP&#33539;&#22260;&#20026;10-40 mg/L&#26102;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#20934;&#30830;&#24615;&#65292;&#36825;&#20010;&#33539;&#22260;&#20869;&#20165;&#38752;CRP&#26080;&#27861;&#20026;&#32454;&#33740;&#21644;&#30149;&#27602;&#24863;&#26579;&#36827;&#34892;&#21306;&#20998;&#30340;&#35786;&#26029;&#20215;&#20540;&#26377;&#38480;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#22312;&#35786;&#26029;&#20915;&#31574;&#20013;&#32771;&#34385;&#22810;&#31181;&#34880;&#28082;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24314;&#35758;&#30149;&#27602; vs. &#32454;&#33740;&#27169;&#22411;&#20351;&#24471;&#24212;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing threat of antibiotic resistance necessitates accurate differentiation between bacterial and viral infections for proper antibiotic administration. In this study, a Virus vs. Bacteria machine learning model was developed to discern between these infection types using 16 routine blood test results, C-reactive protein levels, biological sex, and age. With a dataset of 44,120 cases from a single medical center, the Virus vs. Bacteria model demonstrated remarkable accuracy of 82.2%, a Brier score of 0.129, and an area under the ROC curve of 0.91, surpassing the performance of traditional CRP decision rule models. The model demonstrates substantially improved accuracy within the CRP range of 10 40 mg/L, an interval in which CRP alone offers limited diagnostic value for distinguishing between bacterial and viral infections. These findings underscore the importance of considering multiple blood parameters for diagnostic decision-making and suggest that the Virus vs. Bacteria model 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20445;&#23432;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#19982;&#21407;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#31561;&#20215;&#12290;&#31616;&#21270;&#21518;&#21487;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;5&#65285;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30456;&#24212;&#30340;&#39564;&#35777;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.01932</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Specification-Driven Neural Network Reduction for Scalable Formal Verification. (arXiv:2305.01932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20445;&#23432;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#19982;&#21407;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#31561;&#20215;&#12290;&#31616;&#21270;&#21518;&#21487;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;5&#65285;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30456;&#24212;&#30340;&#39564;&#35777;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#37096;&#32626;&#20043;&#21069;&#65292;&#24418;&#24335;&#39564;&#35777;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#36824;&#26080;&#27861;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#31070;&#32463;&#20803;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20445;&#23432;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#20986;&#21407;&#32593;&#32476;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#26500;&#36896;&#31616;&#21270;&#32593;&#32476;&#65292;&#39564;&#35777;&#21407;&#22987;&#32593;&#32476;&#21450;&#20854;&#35268;&#33539;&#12290;&#31616;&#21270;&#23558;&#25152;&#26377;&#36755;&#20986;&#30456;&#20284;&#30340;&#38750;&#32447;&#24615;&#23618;&#31070;&#32463;&#20803;&#21512;&#24182;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20219;&#20309;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#65292;sigmoid&#21644;tanh&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;&#31070;&#32463;&#20803;&#25968;&#30340;5&#65285;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;&#39564;&#35777;&#26102;&#38388;&#30456;&#20284;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formal verification of neural networks is essential before their deployment in safety-critical settings. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems that involve a large number of neurons. In this work, we propose a novel approach to address this challenge: A conservative neural network reduction approach that ensures that the verification of the reduced network implies the verification of the original network. Our approach constructs the reduction on-the-fly, while simultaneously verifying the original network and its specifications. The reduction merges all neurons of a nonlinear layer with similar outputs and is applicable to neural networks with any type of activation function such as ReLU, sigmoid, and tanh. Our evaluation shows that our approach can reduce a network to less than 5% of the number of neurons and thus to a similar degree the verification time is reduced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#21407;&#21017;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#35299;&#20915;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;MMGN&#12290;&#36890;&#36807;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#65292;MMGN&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#19981;&#22826;&#21463;&#21040;&#28508;&#22312;&#30697;&#38453;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.13940</link><description>&lt;p&gt;
1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#30340;&#20027;&#23548;-&#26368;&#23567;&#21270;&#39640;&#26031;&#29275;&#39039;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Majorization-Minimization Gauss-Newton Method for 1-Bit Matrix Completion. (arXiv:2304.13940v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#21407;&#21017;&#65292;&#36890;&#36807;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#35299;&#20915;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;MMGN&#12290;&#36890;&#36807;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#65292;MMGN&#20855;&#26377;&#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#36824;&#19981;&#22826;&#21463;&#21040;&#28508;&#22312;&#30697;&#38453;&#23574;&#38160;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#20013;&#65292;&#26088;&#22312;&#20174;&#37096;&#20998;&#20108;&#36827;&#21046;&#35266;&#27979;&#20540;&#20013;&#20272;&#35745;&#28508;&#22312;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MMGN&#30340;1&#27604;&#29305;&#30697;&#38453;&#34917;&#20840;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20027;&#23548;-&#26368;&#23567;&#21270;&#65288;MM&#65289;&#21407;&#21017;&#65292;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#20135;&#29983;&#19968;&#31995;&#21015;&#26631;&#20934;&#20302;&#31209;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#24378;&#21046;&#20551;&#23450;&#30340;&#20302;&#31209;&#32467;&#26500;&#30340;&#20998;&#35299;&#26041;&#27861;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#24212;&#29992;&#39640;&#26031;-&#29275;&#39039;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#30740;&#31350;&#21644;&#23545;&#23454;&#38469;&#25968;&#25454;&#30340;&#24212;&#29992;&#34920;&#26126;&#65292;MMGN&#36755;&#20986;&#30340;&#20272;&#35745;&#32467;&#26524;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#36739;&#20855;&#26377;&#21487;&#27604;&#24615;&#19988;&#26356;&#20934;&#30830;&#12289;&#36895;&#24230;&#36890;&#24120;&#26356;&#24555;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#30697;&#38453;&#30340;&#23574;&#38160;&#24230;&#19981;&#22826;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 1-bit matrix completion, the aim is to estimate an underlying low-rank matrix from a partial set of binary observations. We propose a novel method for 1-bit matrix completion called MMGN. Our method is based on the majorization-minimization (MM) principle, which yields a sequence of standard low-rank matrix completion problems in our setting. We solve each of these sub-problems by a factorization approach that explicitly enforces the assumed low-rank structure and then apply a Gauss-Newton method. Our numerical studies and application to a real-data example illustrate that MMGN outputs comparable if not more accurate estimates, is often significantly faster, and is less sensitive to the spikiness of the underlying matrix than existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;&#25216;&#26415;&#65288;LOPT&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#21040;OPT&#38382;&#39064;&#19978;&#65292;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#24182;&#19988;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.03232</link><description>&lt;p&gt;
&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Linear Optimal Partial Transport Embedding. (arXiv:2302.03232v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#23884;&#20837;&#25216;&#26415;&#65288;LOPT&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#21040;OPT&#38382;&#39064;&#19978;&#65292;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#24182;&#19988;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#30001;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#32479;&#35745;&#23398;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#24179;&#34913;&#36136;&#37327;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;OT&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#21253;&#25324;&#19981;&#24179;&#34913;OT&#65292;&#26368;&#20248;&#20559;&#31227;&#20256;&#36755;&#65288;OPT&#65289;&#21644;Hellinger Kantorovich&#65288;HK&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32447;&#24615;&#26368;&#20248;&#20559;&#31227;&#65288;LOPT&#65289;&#23884;&#20837;&#25216;&#26415;&#65292;&#23427;&#23558;OT&#21644;HK&#19978;&#30340;&#65288;&#23616;&#37096;&#65289;&#32447;&#24615;&#21270;&#25216;&#26415;&#25193;&#23637;&#21040;OPT&#38382;&#39064;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#23884;&#20837;&#25216;&#26415;&#25552;&#39640;&#20102;&#27491;&#27979;&#24230;&#23545;&#20043;&#38388;&#30340;LOPT&#36317;&#31163;&#30340;&#35745;&#31639;&#36895;&#24230;&#12290;&#38500;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LOPT&#23884;&#20837;&#25216;&#26415;&#22312;&#28857;&#20113;&#20869;&#25554;&#21644;PCA&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) has gained popularity due to its various applications in fields such as machine learning, statistics, and signal processing. However, the balanced mass requirement limits its performance in practical problems. To address these limitations, variants of the OT problem, including unbalanced OT, Optimal partial transport (OPT), and Hellinger Kantorovich (HK), have been proposed. In this paper, we propose the Linear optimal partial transport (LOPT) embedding, which extends the (local) linearization technique on OT and HK to the OPT problem. The proposed embedding allows for faster computation of OPT distance between pairs of positive measures. Besides our theoretical contributions, we demonstrate the LOPT embedding technique in point-cloud interpolation and PCA analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#949;-PrivateSMOTE&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22122;&#22768;&#24341;&#20837;&#25554;&#20540;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#20445;&#25252;&#20813;&#21463;&#37325;&#26032;&#35782;&#21035;&#21644;&#38142;&#25509;&#25915;&#20987;&#30340;&#39118;&#38505;&#30340;&#30446;&#30340;&#65292;&#24182;&#22312;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.00484</link><description>&lt;p&gt;
&#39640;&#25928;&#25511;&#21046;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control. (arXiv:2212.00484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#949;-PrivateSMOTE&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22122;&#22768;&#24341;&#20837;&#25554;&#20540;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#36798;&#21040;&#20445;&#25252;&#20813;&#21463;&#37325;&#26032;&#35782;&#21035;&#21644;&#38142;&#25509;&#25915;&#20987;&#30340;&#39118;&#38505;&#30340;&#30446;&#30340;&#65292;&#24182;&#22312;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#25345;&#25968;&#25454;&#25928;&#29992;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#25252;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#21487;&#20197;&#36890;&#36807;&#22810;&#31181;&#26041;&#27861;&#23454;&#29616;&#65292;&#20174;&#32479;&#35745;&#36716;&#25442;&#21040;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#23384;&#22312;&#37325;&#35201;&#30340;&#32570;&#38519;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#20256;&#32479;&#25216;&#26415;&#21019;&#24314;&#36716;&#25442;&#25968;&#25454;&#38598;&#38750;&#24120;&#32791;&#26102;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#38500;&#20102;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#38454;&#27573;&#22806;&#65292;&#36824;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#32780;&#24046;&#20998;&#38544;&#31169;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20250;&#21066;&#24369;&#25968;&#25454;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#20102;&#19968;&#31181;&#21517;&#20026;&#949;-PrivateSMOTE&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20445;&#25252;&#20813;&#21463;&#37325;&#26032;&#35782;&#21035;&#21644;&#38142;&#25509;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24182;&#29305;&#21035;&#35299;&#20915;&#39640;&#37325;&#26032;&#35782;&#21035;&#39118;&#38505;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#36890;&#36807;&#22122;&#22768;&#24341;&#20837;&#25554;&#20540;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#20197;&#27169;&#31946;&#39640;&#39118;&#38505;&#26696;&#20363;&#65292;&#21516;&#26102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#30340;&#25968;&#25454;&#25928;&#29992;&#12290;&#19982;17&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;&#20256;&#32479;&#21644;&#26368;&#26032;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#30456;&#27604;&#65292;&#949;-PrivateSMOTE&#22312;&#38544;&#31169;&#39118;&#38505;&#21644;&#25968;&#25454;&#25928;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting user data privacy can be achieved via many methods, from statistical transformations to generative models. However, all of them have critical drawbacks. For example, creating a transformed data set using traditional techniques is highly time-consuming. Also, recent deep learning-based solutions require significant computational resources in addition to long training phases, and differentially private-based solutions may undermine data utility. In this paper, we propose $\epsilon$-PrivateSMOTE, a technique designed for safeguarding against re-identification and linkage attacks, particularly addressing cases with a high re-identification risk. Our proposal combines synthetic data generation via noise-induced interpolation to obfuscate high-risk cases while maximising the data utility of the original data. Compared to multiple traditional and state-of-the-art privacy-preservation methods on 17 data sets, $\epsilon$-PrivateSMOTE achieves competitive results in privacy risk and b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#36798;&#21040;&#20102;&#19982;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2210.12860</link><description>&lt;p&gt;
&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explicit Second-Order Min-Max Optimization Methods with Optimal Convergence Guarantee. (arXiv:2210.12860v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26368;&#20248;&#25910;&#25947;&#20445;&#35777;&#30340;&#26174;&#24335;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#36845;&#20195;&#36807;&#31243;&#20013;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#36798;&#21040;&#20102;&#19982;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#19981;&#31934;&#30830;&#27491;&#21017;&#21270;&#29275;&#39039;&#22411;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#20984;&#20985;&#26080;&#32422;&#26463;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#38797;&#28857;&#12290;&#19982;&#19968;&#38454;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#23545;&#20110;&#20108;&#38454;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#26041;&#27861;&#30340;&#29702;&#35299;&#30456;&#23545;&#36739;&#23569;&#65292;&#22240;&#20026;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#33719;&#24471;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#26356;&#21152;&#22797;&#26434;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#20108;&#38454;&#20449;&#24687;&#21152;&#36895;&#39069;&#22806;&#26799;&#24230;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#19981;&#31934;&#30830;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#36845;&#20195;&#20445;&#25345;&#22312;&#26377;&#30028;&#38598;&#20869;&#65292;&#24182;&#19988;&#24179;&#22343;&#36845;&#20195;&#25910;&#25947;&#21040;&#19968;&#20010; $\epsilon$-&#38797;&#28857;&#65292;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#20026; $O(\epsilon^{-2/3})$&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#21463;&#38480;&#38388;&#38553;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#19982;&#35813;&#39046;&#22495;&#24050;&#32463;&#24314;&#31435;&#30340;&#29702;&#35770;&#19979;&#30028;&#30456;&#21305;&#37197;&#65292;&#32780;&#19988;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#35266;&#30340;&#20108;&#38454;&#26041;&#27861;&#25910;&#25947;&#20998;&#26512;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#26377;&#30028;&#24615;&#35201;&#27714;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
We propose and analyze exact and inexact regularized Newton-type methods for finding a global saddle point of \emph{convex-concave} unconstrained min-max optimization problems. Compared to first-order methods, our understanding of second-order methods for min-max optimization is relatively limited, as obtaining global rates of convergence with second-order information is much more involved. In this paper, we examine how second-order information can be used to speed up extra-gradient methods, even under inexactness. Specifically, we show that the proposed algorithms generate iterates that remain within a bounded set and the averaged iterates converge to an $\epsilon$-saddle point within $O(\epsilon^{-2/3})$ iterations in terms of a restricted gap function. Our algorithms match the theoretically established lower bound in this context and our analysis provides a simple and intuitive convergence analysis for second-order methods without any boundedness requirements. Finally, we present a 
&lt;/p&gt;</description></item></channel></rss>