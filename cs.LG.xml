<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01542</link><description>&lt;p&gt;
&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#30340;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Learning Collective Variables for Protein Folding with Labeled Data Augmentation through Geodesic Interpolation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#23454;&#39564;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#22686;&#24378;&#21644;&#27979;&#22320;&#25554;&#20540;&#26041;&#27861;&#23398;&#20064;&#34507;&#30333;&#36136;&#25240;&#21472;&#30340;&#38598;&#20307;&#21464;&#37327;&#30340;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#37319;&#26679;&#25928;&#29575;&#65292;&#24182;&#22312;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#36890;&#36807;&#22686;&#24378;&#37319;&#26679;&#25216;&#26415;&#26469;&#30740;&#31350;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#32597;&#35265;&#20107;&#20214;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#27839;&#30528;&#21152;&#36895;&#21457;&#29983;&#30340;&#38598;&#20307;&#21464;&#37327;&#65288;CV&#65289;&#30340;&#23450;&#20041;&#12290;&#33719;&#24471;&#23500;&#26377;&#34920;&#36798;&#21147;&#30340;CV&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#24448;&#24448;&#21463;&#21040;&#20851;&#20110;&#29305;&#23450;&#20107;&#20214;&#30340;&#20449;&#24687;&#19981;&#36275;&#30340;&#38459;&#30861;&#65292;&#20363;&#22914;&#20174;&#26410;&#25240;&#21472;&#21040;&#25240;&#21472;&#26500;&#35937;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#26080;&#20851;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#24230;&#37327;&#26469;&#29983;&#25104;&#31867;&#20284;&#34507;&#30333;&#36136;&#25240;&#21472;&#36716;&#21464;&#30340;&#27979;&#22320;&#25554;&#20540;&#65292;&#20174;&#32780;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#30340;&#36807;&#28193;&#24577;&#26679;&#26412;&#12290;&#36890;&#36807;&#21033;&#29992;&#25554;&#20540;&#36827;&#24230;&#21442;&#25968;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22238;&#24402;&#30340;&#23398;&#20064;&#26041;&#26696;&#26469;&#26500;&#24314;CV&#27169;&#22411;&#65292;&#24403;&#36807;&#28193;&#24577;&#25968;&#25454;&#26377;&#38480;&#19988;&#22024;&#26434;&#26102;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In molecular dynamics (MD) simulations, rare events, such as protein folding, are typically studied by means of enhanced sampling techniques, most of which rely on the definition of a collective variable (CV) along which the acceleration occurs. Obtaining an expressive CV is crucial, but often hindered by the lack of information about the particular event, e.g., the transition from unfolded to folded conformation. We propose a simulation-free data augmentation strategy using physics-inspired metrics to generate geodesic interpolations resembling protein folding transitions, thereby improving sampling efficiency without true transition state samples. Leveraging interpolation progress parameters, we introduce a regression-based learning scheme for CV models, which outperforms classifier-based methods when transition state data is limited and noisy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://rss.arxiv.org/abs/2311.13870</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#34892;&#20026;&#34920;&#31034;&#30340;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-intention Inverse Q-learning for Interpretable Behavior Representation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.13870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#24847;&#22270;&#36870;Q&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#25512;&#26029;&#31163;&#25955;&#26102;&#21464;&#22870;&#21169;&#26102;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#32858;&#31867;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#24182;&#29420;&#31435;&#35299;&#20915;&#27599;&#20010;&#24847;&#22270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#21160;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#26041;&#38754;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#22312;&#37325;&#26500;&#21160;&#29289;&#22797;&#26434;&#34892;&#20026;&#20013;&#30340;&#22810;&#20010;&#24847;&#22270;&#26041;&#38754;&#35777;&#26126;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;&#37492;&#20110;&#26368;&#36817;&#21457;&#23637;&#30340;&#36830;&#32493;&#26102;&#38388;&#22810;&#24847;&#22270;IRL&#26694;&#26550;&#65292;&#20154;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#22914;&#20309;&#20351;&#29992;IRL&#25512;&#26029;&#31163;&#25955;&#30340;&#26102;&#21464;&#22870;&#21169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#65288;&#39532;&#23572;&#31185;&#22827;&#65289;&#21464;&#37327;&#36870;Q&#23398;&#20064;&#65288;L(M)V-IQL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#36866;&#24212;&#31163;&#25955;&#20869;&#22312;&#22870;&#21169;&#20989;&#25968;&#30340;IRL&#31639;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#35266;&#23519;&#21040;&#30340;&#19987;&#23478;&#36712;&#36857;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#24847;&#22270;&#65292;&#24182;&#20026;&#27599;&#20010;&#24847;&#22270;&#29420;&#31435;&#35299;&#20915;IRL&#38382;&#39064;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#23545;&#19981;&#21516;&#30495;&#23454;&#40736;&#31867;&#34892;&#20026;&#25968;&#25454;&#38598;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21160;&#29289;&#34892;&#20026;&#39044;&#27979;&#26041;&#38754;&#36229;&#36234;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#65292;&#20135;&#29983;&#20102;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#19968;&#36827;&#23637;&#26377;&#26395;&#25171;&#24320;&#25512;&#21160;&#31185;&#23398;&#19982;&#24037;&#31243;&#24212;&#29992;&#30340;&#26032;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
In advancing the understanding of decision-making processes, Inverse Reinforcement Learning (IRL) have proven instrumental in reconstructing animal's multiple intentions amidst complex behaviors. Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To tackle the challenge, we introduce Latent (Markov) Variable Inverse Q-learning (L(M)V-IQL), a novel class of IRL algorthms tailored for accommodating discrete intrinsic reward functions. Leveraging an Expectation-Maximization approach, we cluster observed expert trajectories into distinct intentions and independently solve the IRL problem for each. Demonstrating the efficacy of L(M)V-IQL through simulated experiments and its application to different real mouse behavior datasets, our approach surpasses current benchmarks in animal behavior prediction, producing interpretable reward functions. This advancement holds promise f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MedCLIP-SAM&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20998;&#21106;</title><link>https://arxiv.org/abs/2403.20253</link><description>&lt;p&gt;
MedCLIP-SAM&#65306;&#23558;&#25991;&#26412;&#21644;&#22270;&#20687;&#36827;&#34892;&#26725;&#25509;&#65292;&#23454;&#29616;&#36890;&#29992;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MedCLIP-SAM&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#23454;&#29616;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#22270;&#20687;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20013;&#35299;&#21078;&#32467;&#26500;&#21644;&#30149;&#21464;&#30340;&#20998;&#21106;&#22312;&#29616;&#20195;&#20020;&#24202;&#35786;&#26029;&#12289;&#30142;&#30149;&#30740;&#31350;&#21644;&#27835;&#30103;&#35268;&#21010;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;MedCLIP-SAM&#65292;&#32467;&#21512;&#20102;CLIP&#21644;SAM&#27169;&#22411;&#65292;&#20197;&#29983;&#25104;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20020;&#24202;&#25195;&#25551;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20253v1 Announce Type: cross  Abstract: Medical image segmentation of anatomical structures and pathology is crucial in modern clinical diagnosis, disease study, and treatment planning. To date, great progress has been made in deep learning-based segmentation techniques, but most methods still lack data efficiency, generalizability, and interactability. Consequently, the development of new, precise segmentation methods that demand fewer labeled datasets is of utmost importance in medical image analysis. Recently, the emergence of foundation models, such as CLIP and Segment-Anything-Model (SAM), with comprehensive cross-domain representation opened the door for interactive and universal image segmentation. However, exploration of these models for data-efficient medical image segmentation is still limited, but is highly necessary. In this paper, we propose a novel framework, called MedCLIP-SAM that combines CLIP and SAM models to generate segmentation of clinical scans using t
&lt;/p&gt;</description></item><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19050</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;&#26816;&#27979;&#29983;&#25104;&#24615;&#27169;&#20223;
&lt;/p&gt;
&lt;p&gt;
Detecting Generative Parroting through Overfitting Masked Autoencoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#21033;&#29992;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26816;&#27979;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#29983;&#25104;&#24615;&#27169;&#20223;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#25439;&#22833;&#30340;&#26816;&#27979;&#38408;&#20540;&#65292;&#20026;&#20102;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#21644;&#25552;&#21319;&#20854;&#27861;&#24459;&#21512;&#35268;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#25968;&#23383;&#20869;&#23481;&#21019;&#24314;&#30340;&#26041;&#24335;&#65292;&#28982;&#32780;&#30001;&#20110;&#29983;&#25104;&#24615;&#27169;&#20223;&#38382;&#39064;&#65292;&#27169;&#22411;&#36807;&#20110;&#27169;&#20223;&#20854;&#35757;&#32451;&#25968;&#25454;&#32780;&#32473;&#29256;&#26435;&#23436;&#25972;&#24615;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#36974;&#34109;&#33258;&#32534;&#30721;&#22120;(MAE)&#26469;&#26377;&#25928;&#22320;&#26816;&#27979;&#36825;&#31181;&#27169;&#20223;&#26679;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#25439;&#22833;&#24314;&#31435;&#19968;&#20010;&#26816;&#27979;&#38408;&#20540;&#65292;&#20174;&#32780;&#31934;&#30830;&#23450;&#20301;&#20462;&#25913;&#21518;&#25968;&#25454;&#38598;&#20013;&#30340;&#27169;&#20223;&#20869;&#23481;&#12290;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30830;&#20445;&#29983;&#25104;&#27169;&#22411;&#30340;&#21512;&#27861;&#20351;&#29992;&#24182;&#21152;&#24378;&#27861;&#24459;&#21512;&#35268;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19050v1 Announce Type: cross  Abstract: The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#36830;&#32493;&#20307;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#23610;&#24230;&#38382;&#39064;&#20013;&#21333;&#19968;&#36830;&#32493;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.14084</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#23380;&#20171;&#36136;&#27169;&#22411;&#29992;&#20110;&#22810;&#23610;&#24230;&#27969;&#21160;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning-based Multi-continuum Model for Multiscale Flow Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14084
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#36830;&#32493;&#20307;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#22810;&#23610;&#24230;&#38382;&#39064;&#20013;&#21333;&#19968;&#36830;&#32493;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#38382;&#39064;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#25968;&#20540;&#22343;&#36136;&#21270;&#26469;&#36817;&#20284;&#65292;&#36890;&#36807;&#20855;&#26377;&#26576;&#20123;&#26377;&#25928;&#21442;&#25968;&#30340;&#26041;&#31243;&#26469;&#25429;&#33719;&#21407;&#22987;&#31995;&#32479;&#22312;&#31895;&#32593;&#26684;&#19978;&#30340;&#23439;&#35266;&#34892;&#20026;&#65292;&#20197;&#21152;&#24555;&#27169;&#25311;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#23610;&#24230;&#20998;&#31163;&#65292;&#24182;&#19988;&#35299;&#30340;&#24322;&#36136;&#24615;&#21487;&#20197;&#36890;&#36807;&#27599;&#20010;&#31895;&#22359;&#20013;&#30340;&#35299;&#30340;&#24179;&#22343;&#20540;&#26469;&#36817;&#20284;&#12290;&#23545;&#20110;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#38382;&#39064;&#65292;&#35745;&#31639;&#30340;&#21333;&#19968;&#26377;&#25928;&#24615;&#29305;&#24615;/&#36830;&#32493;&#20307;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#22810;&#36830;&#32493;&#20307;&#27169;&#22411;&#65292;&#29992;&#20110;&#20016;&#23500;&#22343;&#36136;&#21270;&#26041;&#31243;&#24182;&#25552;&#39640;&#22810;&#23610;&#24230;&#38382;&#39064;&#21333;&#19968;&#36830;&#32493;&#20307;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#32473;&#23450;&#19968;&#20123;&#25968;&#25454;&#12290;&#19981;&#22833;&#19968;&#33324;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21452;&#36830;&#32493;&#20307;&#30340;&#24773;&#20917;&#12290;&#31532;&#19968;&#20010;&#27969;&#21160;&#26041;&#31243;&#20445;&#30041;&#20102;&#21407;&#22987;&#22343;&#36136;&#21270;&#26041;&#31243;&#30340;&#20449;&#24687;&#65292;&#20855;&#26377;&#39069;&#22806;&#30340;&#20132;&#20114;&#39033;&#12290;&#31532;&#20108;&#20010;&#36830;&#32493;&#20307;&#26159;&#26032;&#24341;&#20837;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14084v1 Announce Type: cross  Abstract: Multiscale problems can usually be approximated through numerical homogenization by an equation with some effective parameters that can capture the macroscopic behavior of the original system on the coarse grid to speed up the simulation. However, this approach usually assumes scale separation and that the heterogeneity of the solution can be approximated by the solution average in each coarse block. For complex multiscale problems, the computed single effective properties/continuum might be inadequate. In this paper, we propose a novel learning-based multi-continuum model to enrich the homogenized equation and improve the accuracy of the single continuum model for multiscale problems with some given data. Without loss of generalization, we consider a two-continuum case. The first flow equation keeps the information of the original homogenized equation with an additional interaction term. The second continuum is newly introduced, and t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.13658</link><description>&lt;p&gt;
&#29992;&#20110;&#20302;&#25104;&#26412;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoder for Low-cost Cardiac Hemodynamics Instability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13658
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#65292;&#23558;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#23454;&#29616;&#20102;&#20849;&#20139;&#29305;&#24449;&#21644;&#29420;&#29305;&#29305;&#24449;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38750;&#20405;&#20837;&#24615;&#26816;&#27979;&#24515;&#33039;&#34880;&#28082;&#21160;&#21147;&#23398;&#19981;&#31283;&#23450;&#24615;&#65288;CHDI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21333;&#19968;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22312;&#26631;&#35760;&#30340;&#24739;&#32773;&#25968;&#25454;&#37327;&#26377;&#38480;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#30740;&#31350;CHDI&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#26114;&#36149;&#30340;&#25968;&#25454;&#24418;&#24335;&#65292;&#22914;&#24515;&#33039;MRI&#21644;&#24515;&#33039;&#36229;&#22768;&#22270;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;$\text{CardioVAE}_\text{X,G}$&#65289;&#26469;&#25972;&#21512;&#20302;&#25104;&#26412;&#33016;&#37096;X&#23556;&#32447;&#65288;CXR&#65289;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25968;&#25454;&#24418;&#24335;&#65292;&#24182;&#22312;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;$\text{CardioVAE}_\text{X,G}$&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27969;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#20849;&#20139;&#29305;&#24449;&#21644;&#21508;&#25968;&#25454;&#24418;&#24335;&#29420;&#26377;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;fi
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13658v1 Announce Type: new  Abstract: Recent advancements in non-invasive detection of cardiac hemodynamic instability (CHDI) primarily focus on applying machine learning techniques to a single data modality, e.g. cardiac magnetic resonance imaging (MRI). Despite their potential, these approaches often fall short especially when the size of labeled patient data is limited, a common challenge in the medical domain. Furthermore, only a few studies have explored multimodal methods to study CHDI, which mostly rely on costly modalities such as cardiac MRI and echocardiogram. In response to these limitations, we propose a novel multimodal variational autoencoder ($\text{CardioVAE}_\text{X,G}$) to integrate low-cost chest X-ray (CXR) and electrocardiogram (ECG) modalities with pre-training on a large unlabeled dataset. Specifically, $\text{CardioVAE}_\text{X,G}$ introduces a novel tri-stream pre-training strategy to learn both shared and modality-specific features, thus enabling fi
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.13086</link><description>&lt;p&gt;
&#21487;&#21548;&#22270;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Listenable Maps for Audio Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13086
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Listenable Maps for Audio Classifiers (L-MAC)&#30340;&#21487;&#21548;&#22270;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#30340;&#38899;&#39057;&#20998;&#31867;&#22120;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20854;&#22797;&#26434;&#24615;&#32473;&#35299;&#37322;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#19968;&#25361;&#25112;&#22312;&#38899;&#39057;&#20449;&#21495;&#20013;&#23588;&#20026;&#26126;&#26174;&#65292;&#20256;&#36798;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#38899;&#39057;&#20998;&#31867;&#22120;&#30340;&#21487;&#21548;&#22270;&#65288;Listenable Maps for Audio Classifiers&#65292;L-MAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#24544;&#23454;&#19988;&#21487;&#21548;&#35299;&#37322;&#30340;&#21518;&#22788;&#29702;&#35299;&#37322;&#26041;&#27861;&#12290;L-MAC&#21033;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#20043;&#19978;&#30340;&#35299;&#30721;&#22120;&#29983;&#25104;&#20108;&#20540;&#25513;&#30721;&#65292;&#31361;&#20986;&#26174;&#31034;&#36755;&#20837;&#38899;&#39057;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#25105;&#20204;&#29992;&#19968;&#31181;&#29305;&#27530;&#25439;&#22833;&#26469;&#35757;&#32451;&#35299;&#30721;&#22120;&#65292;&#35813;&#25439;&#22833;&#26368;&#22823;&#21270;&#20998;&#31867;&#22120;&#23545;&#36755;&#20837;&#38899;&#39057;&#30340;&#25513;&#30721;&#37096;&#20998;&#30340;&#32622;&#20449;&#24230;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27169;&#22411;&#23545;&#25513;&#30721;&#37096;&#20998;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#23545;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#30340;&#23450;&#37327;&#35780;&#20272;&#34920;&#26126;&#65292;L-MAC&#22987;&#32456;&#20135;&#29983;&#27604;&#20960;&#31181;&#26799;&#24230;&#21644;&#25513;&#30721;&#26041;&#27861;&#26356;&#24544;&#23454;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13086v1 Announce Type: cross  Abstract: Despite the impressive performance of deep learning models across diverse tasks, their complexity poses challenges for interpretation. This challenge is particularly evident for audio signals, where conveying interpretations becomes inherently difficult. To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations. L-MAC utilizes a decoder on top of a pretrained classifier to generate binary masks that highlight relevant portions of the input audio. We train the decoder with a special loss that maximizes the confidence of the classifier decision on the masked-in portion of the audio while minimizing the probability of model output for the masked-out portion. Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and maskin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.10506</link><description>&lt;p&gt;
HumanoidBench&#65306;&#29992;&#20110;&#20840;&#36523;&#36816;&#21160;&#21644;&#25805;&#20316;&#30340;&#20223;&#30495;&#20154;&#22411;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;HumanoidBench&#65292;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#38754;&#20020;&#25361;&#25112;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#21327;&#21161;&#20154;&#31867;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20219;&#21153;&#20013;&#26377;&#30528;&#24040;&#22823;&#28508;&#21147;&#65292;&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#21487;&#20197;&#21033;&#29992;&#31867;&#20154;&#24418;&#24577;&#12290;&#28982;&#32780;&#65292;&#20154;&#22411;&#26426;&#22120;&#20154;&#30340;&#30740;&#31350;&#24120;&#24120;&#21463;&#21040;&#26114;&#36149;&#19988;&#26131;&#25439;&#30340;&#30828;&#20214;&#35774;&#32622;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#21152;&#36895;&#20154;&#22411;&#26426;&#22120;&#20154;&#31639;&#27861;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#32500;&#24230;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;HumanoidBench&#65292;&#35813;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#37197;&#22791;&#28789;&#24039;&#25163;&#37096;&#21644;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20840;&#36523;&#25805;&#20316;&#21644;&#36816;&#21160;&#20219;&#21153;&#30340;&#20154;&#22411;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20855;&#22791;&#40065;&#26834;&#30340;&#20302;&#32423;&#31574;&#30053;&#25903;&#25345;&#30340;&#20998;&#23618;&#23398;&#20064;&#22522;&#32447;&#22312;&#34892;&#36208;&#25110;&#21040;&#36798;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#20511;&#21161;HumanoidBench&#65292;&#25105;&#20204;&#20026;&#26426;&#22120;&#20154;&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#21488;&#65292;&#29992;&#20110;&#35782;&#21035;&#35299;&#20915;&#20154;&#22411;&#26426;&#22120;&#20154;&#22312;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20419;&#36827;&#31639;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10506v1 Announce Type: cross  Abstract: Humanoid robots hold great promise in assisting humans in diverse environments and tasks, due to their flexibility and adaptability leveraging human-like morphology. However, research in humanoid robots is often bottlenecked by the costly and fragile hardware setups. To accelerate algorithmic research in humanoid robots, we present a high-dimensional, simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot equipped with dexterous hands and a variety of challenging whole-body manipulation and locomotion tasks. Our findings reveal that state-of-the-art reinforcement learning algorithms struggle with most tasks, whereas a hierarchical learning baseline achieves superior performance when supported by robust low-level policies, such as walking or reaching. With HumanoidBench, we provide the robotics community with a platform to identify the challenges arising when solving diverse tasks with humanoid robots, facilitatin
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.09502</link><description>&lt;p&gt;
EquiAV: &#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09502
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#23454;&#29616;&#29305;&#24449;&#32858;&#21512;&#21644;&#23884;&#20837;&#34920;&#31034;&#65292;&#26377;&#25928;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#30417;&#30563;&#65292;&#19988;&#35745;&#31639;&#24320;&#38144;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#38899;&#39057;-&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26368;&#36817;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20986;&#25429;&#25417;&#20016;&#23500;&#32508;&#21512;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#25968;&#25454;&#22686;&#24378;&#22312;&#35768;&#22810;&#23398;&#20064;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#65292;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#20173;&#28982;&#24456;&#38590;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20248;&#21183;&#65292;&#22240;&#20026;&#22686;&#24378;&#21487;&#33021;&#20250;&#36731;&#26131;&#30772;&#22351;&#36755;&#20837;&#23545;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EquiAV&#65292;&#19968;&#31181;&#21033;&#29992;&#31561;&#21464;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#25193;&#23637;&#31561;&#21464;&#24615;&#24320;&#22987;&#36827;&#34892;&#38899;&#39057;-&#35270;&#35273;&#23398;&#20064;&#65292;&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#21464;&#25442;&#39044;&#27979;&#22120;&#26469;&#20419;&#36827;&#12290;&#23427;&#20351;&#24471;&#26469;&#33258;&#19981;&#21516;&#22686;&#24378;&#30340;&#29305;&#24449;&#33021;&#22815;&#32858;&#21512;&#21040;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#23884;&#20837;&#20013;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#30417;&#30563;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#26159;&#22312;&#26368;&#23567;&#35745;&#31639;&#24320;&#38144;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#30340;&#12290;&#22823;&#37327;&#28040;&#34701;&#30740;&#31350;&#21644;&#23450;&#24615;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.07322</link><description>&lt;p&gt;
&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Question-centric Multi-experts Contrastive Learning Framework for Improving the Accuracy and Interpretability of Deep Sequential Knowledge Tracing Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07322
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#20013;&#24515;&#30340;&#22810;&#19987;&#23478;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#39640;&#28145;&#24230;&#24207;&#21015;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#36861;&#36394;&#20013;&#20010;&#20307;&#38382;&#39064;&#20449;&#24687;&#24314;&#27169;&#21644;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#35299;&#37322;&#30340;&#37325;&#35201;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#22312;&#36890;&#36807;&#20998;&#26512;&#23398;&#29983;&#21382;&#21490;&#23398;&#20064;&#36807;&#31243;&#26469;&#39044;&#27979;&#20854;&#26410;&#26469;&#34920;&#29616;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35299;&#20915;&#30693;&#35782;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#25311;&#30693;&#35782;&#36861;&#36394;&#36807;&#31243;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#12290;&#31532;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#23558;&#38382;&#39064;&#30340;&#20010;&#20307;&#20449;&#24687;&#34701;&#20837;&#24314;&#27169;&#20013;&#12290;&#36825;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#23613;&#31649;&#38382;&#39064;&#20849;&#20139;&#30456;&#21516;&#30340;&#30693;&#35782;&#32452;&#20214;&#65288;KC&#65289;&#65292;&#20294;&#23398;&#29983;&#23545;&#21516;&#36136;&#38382;&#39064;&#30340;&#30693;&#35782;&#20064;&#24471;&#21487;&#20197;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#22312;&#20110;&#35299;&#37322;&#29616;&#26377;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#65292;&#34429;&#28982;&#21487;&#33021;&#24182;&#19981;&#38656;&#35201;&#23436;&#20840;&#36879;&#26126;&#21644;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#20851;&#38190;&#26159;&#20197;&#32769;&#24072;&#33021;&#29702;&#35299;&#30340;&#26041;&#24335;&#21576;&#29616;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07322v1 Announce Type: cross  Abstract: Knowledge tracing (KT) plays a crucial role in predicting students' future performance by analyzing their historical learning processes. Deep neural networks (DNNs) have shown great potential in solving the KT problem. However, there still exist some important challenges when applying deep learning techniques to model the KT process. The first challenge lies in taking the individual information of the question into modeling. This is crucial because, despite questions sharing the same knowledge component (KC), students' knowledge acquisition on homogeneous questions can vary significantly. The second challenge lies in interpreting the prediction results from existing deep learning-based KT models. In real-world applications, while it may not be necessary to have complete transparency and interpretability of the model parameters, it is crucial to present the model's prediction results in a manner that teachers find interpretable. This ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#20316;&#29992;&#21644;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24615;&#25345;&#32493;&#33258;&#21160;&#35843;&#25972;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#24207;&#21015;&#20219;&#21153;&#23398;&#20064;&#29305;&#24615;&#26469;&#25552;&#39640;&#36229;&#21442;&#25968;&#20248;&#21270;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25345;&#32493;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.07015</link><description>&lt;p&gt;
&#38024;&#23545;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#30340;&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Hyperparameter Optimization for Continual Learning Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#20316;&#29992;&#21644;&#26681;&#25454;&#20219;&#21153;&#22797;&#26434;&#24615;&#25345;&#32493;&#33258;&#21160;&#35843;&#25972;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#24207;&#21015;&#20219;&#21153;&#23398;&#20064;&#29305;&#24615;&#26469;&#25552;&#39640;&#36229;&#21442;&#25968;&#20248;&#21270;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#25345;&#32493;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;&#36229;&#21442;&#25968;&#36873;&#25321;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#23454;&#38469;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#36229;&#21442;&#25968;&#36873;&#25321;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#20197;&#21450;&#26681;&#25454;&#25163;&#22836;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#25345;&#32493;&#33258;&#21160;&#35843;&#25972;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#24207;&#21015;&#20219;&#21153;&#23398;&#20064;&#30340;&#29305;&#24615;&#26469;&#25552;&#39640;&#36229;&#21442;&#25968;&#20248;&#21270;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26041;&#24046;&#30340;&#21151;&#33021;&#20998;&#26512;&#25216;&#26415;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#30340;&#26368;&#20851;&#38190;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#26080;&#35270;&#25345;&#32493;&#22330;&#26223;&#21644;&#31574;&#30053;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25345;&#32493;&#21152;&#24555;&#36229;&#21442;&#25968;&#22312;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07015v1 Announce Type: new  Abstract: Hyperparameter selection in continual learning scenarios is a challenging and underexplored aspect, especially in practical non-stationary environments. Traditional approaches, such as grid searches with held-out validation data from all tasks, are unrealistic for building accurate lifelong learning systems. This paper aims to explore the role of hyperparameter selection in continual learning and the necessity of continually and automatically tuning them according to the complexity of the task at hand. Hence, we propose leveraging the nature of sequence task learning to improve Hyperparameter Optimization efficiency. By using the functional analysis of variance-based techniques, we identify the most crucial hyperparameters that have an impact on performance. We demonstrate empirically that this approach, agnostic to continual scenarios and strategies, allows us to speed up hyperparameters optimization continually across tasks and exhibit
&lt;/p&gt;</description></item><item><title>&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.06398</link><description>&lt;p&gt;
&#20851;&#20110;&#25345;&#32493;&#23398;&#20064;&#20013;&#23485;&#24230;&#36882;&#20943;&#22238;&#25253;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Diminishing Returns of Width for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06398
&lt;/p&gt;
&lt;p&gt;
&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#65292;&#24182;&#19988;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#33539;&#22260;&#20869;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#22312;&#25353;&#39034;&#24207;&#35757;&#32451;&#26032;&#20219;&#21153;&#26102;&#32463;&#24120;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#12290; &#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#23485;&#24230;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#20943;&#23569;&#65292;&#20294;&#23578;&#26410;&#20934;&#30830;&#21051;&#30011;&#23485;&#24230;&#21644;&#25345;&#32493;&#23398;&#20064;&#20043;&#38388;&#30340;&#30830;&#20999;&#20851;&#31995;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#26089;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#25345;&#32493;&#23398;&#20064;&#29702;&#35770;&#65292;&#24182;&#35777;&#26126;&#23485;&#24230;&#19982;&#21069;&#39304;&#32593;&#32476;&#65288;FFN&#65289;&#20013;&#30340;&#36951;&#24536;&#30452;&#25509;&#30456;&#20851;&#12290; &#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#20197;&#20943;&#23569;&#36951;&#24536;&#20250;&#24102;&#26469;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#22312;&#20808;&#21069;&#30740;&#31350;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#23485;&#24230;&#19978;&#32463;&#39564;&#24615;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35770;&#26029;&#65292;&#32467;&#26524;&#26174;&#31034;&#36882;&#20943;&#22238;&#25253;&#22914;&#25105;&#20204;&#30340;&#29702;&#35770;&#25152;&#39044;&#27979;&#30340;&#37027;&#26679;&#28165;&#26224;&#21487;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.05750</link><description>&lt;p&gt;
&#35299;&#35835;AI&#31508;: &#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#25216;&#26415;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05750
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25506;&#32034;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#23637;&#31034;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#24443;&#24213;&#39072;&#35206;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24191;&#27867;&#30340;&#24212;&#29992;&#24102;&#26469;&#25361;&#25112;&#65292;&#38656;&#35201;&#28145;&#20837;&#23457;&#26597;&#12289;&#20262;&#29702;&#23457;&#26597;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#37325;&#28857;&#26159;&#35782;&#21035;AI&#29983;&#25104;&#25991;&#26412;&#20316;&#20026;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#35282;&#24230;&#35780;&#20272;&#20102;&#26816;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#24403;&#21069;&#39046;&#22495;&#38480;&#21046;&#30340;&#26032;&#39062;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05750v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.
&lt;/p&gt;</description></item><item><title>Vlearn&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;Vlearn&#65292;&#23427;&#36890;&#36807;&#20165;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04453</link><description>&lt;p&gt;
Vlearn&#65306;&#20351;&#29992;&#39640;&#25928;&#29366;&#24577;&#20540;&#20989;&#25968;&#20272;&#35745;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04453
&lt;/p&gt;
&lt;p&gt;
Vlearn&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;Vlearn&#65292;&#23427;&#36890;&#36807;&#20165;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#34920;&#31034;&#65292;&#36825;&#22312;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#21464;&#24471;&#26840;&#25163;&#12290;&#36825;&#20123;&#31639;&#27861;&#32463;&#24120;&#36935;&#21040;&#25361;&#25112;&#65292;&#21363;&#23427;&#20204;&#22312;&#22788;&#29702;&#32500;&#24230;&#28798;&#38590;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#32500;&#25252;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#21464;&#24471;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vlearn&#30340;&#26032;&#22411;&#31163;&#31574;&#30053;&#20449;&#20219;&#21306;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#35201;&#27714;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#20174;&#32780;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#20960;&#20010;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;Vlearn&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;Vlearn&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19982;&#32431;&#29366;&#24577;&#20540;&#20989;&#25968;&#23398;&#20064;&#30456;&#20851;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04453v1 Announce Type: new  Abstract: Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy se
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#30340;&#27963;&#36291;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#21644;&#20542;&#21521;&#24471;&#20998;&#26469;&#38477;&#20302;&#28176;&#36817;&#26041;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.03589</link><description>&lt;p&gt;
&#29992;&#20110;&#22788;&#29702;&#22240;&#21464;&#37327;&#36873;&#25321;&#30340;&#27963;&#36291;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03589
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#22320;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#30340;&#27963;&#36291;&#33258;&#36866;&#24212;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#21644;&#20542;&#21521;&#24471;&#20998;&#26469;&#38477;&#20302;&#28176;&#36817;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#20272;&#35745;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATEs&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#23454;&#39564;&#65292;&#20854;&#20013;&#23454;&#39564;&#32773;&#25353;&#39034;&#24207;&#20174;&#30001;&#23454;&#39564;&#32773;&#20915;&#23450;&#30340;&#21327;&#21464;&#37327;&#23494;&#24230;&#20013;&#25277;&#26679;&#19968;&#20010;&#23454;&#39564;&#21333;&#20803;&#65292;&#24182;&#20998;&#37197;&#19968;&#31181;&#22788;&#29702;&#12290;&#22312;&#20998;&#37197;&#22788;&#29702;&#21518;&#65292;&#23454;&#39564;&#32773;&#31435;&#21363;&#35266;&#23519;&#30456;&#24212;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#39564;&#32467;&#26463;&#26102;&#65292;&#23454;&#39564;&#32773;&#21033;&#29992;&#25910;&#38598;&#30340;&#26679;&#26412;&#20272;&#31639;&#20986;&#19968;&#20010;ATE&#12290;&#23454;&#39564;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#36739;&#23567;&#30340;&#28176;&#36817;&#26041;&#24046;&#20272;&#35745;ATE&#12290;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#35774;&#35745;&#20102;&#19968;&#20123;&#33021;&#22815;&#33258;&#36866;&#24212;&#20248;&#21270;&#20542;&#21521;&#24471;&#20998;&#65288;&#22788;&#29702;&#20998;&#37197;&#27010;&#29575;&#65289;&#30340;&#23454;&#39564;&#12290;&#20316;&#20026;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#27010;&#25324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#19979;&#23454;&#39564;&#32773;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#20197;&#21450;&#20542;&#21521;&#24471;&#20998;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21327;&#21464;&#37327;&#23494;&#24230;&#21644;&#20542;&#21521;&#24471;&#20998;&#27604;&#20165;&#20248;&#21270;&#20542;&#21521;&#24471;&#20998;&#21487;&#20197;&#20943;&#23569;&#28176;&#36817;&#26041;&#24046;&#26356;&#22810;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03589v1 Announce Type: cross  Abstract: This study designs an adaptive experiment for efficiently estimating average treatment effect (ATEs). We consider an adaptive experiment where an experimenter sequentially samples an experimental unit from a covariate density decided by the experimenter and assigns a treatment. After assigning a treatment, the experimenter observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using gathered samples. The objective of the experimenter is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose a framework under which an experimenter optimizes the covariate density, as well as the propensity score, and find that optimizing both covariate density and propensity score reduces the asymptotic variance more than o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19977;&#31181;&#24120;&#29992;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.02347</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Federated Learning Algorithms without Data Similarity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#25910;&#25947;&#24615;&#20998;&#26512;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#19977;&#31181;&#24120;&#29992;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#20256;&#32479;&#19978;&#34987;&#24191;&#27867;&#20381;&#36182;&#20110;&#29702;&#35299;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#26681;&#25454;&#25968;&#25454;&#30456;&#20284;&#24615;&#31243;&#24230;&#24494;&#35843;&#27493;&#38271;&#12290;&#24403;&#25968;&#25454;&#30456;&#20284;&#24615;&#36739;&#20302;&#26102;&#65292;&#36825;&#20123;&#23567;&#27493;&#38271;&#20250;&#23548;&#33268;&#32852;&#37030;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19981;&#21487;&#25509;&#21463;&#22320;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#38656;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#19968;&#20010;&#19981;&#31561;&#24335;&#19978;&#65292;&#36825;&#20010;&#19981;&#31561;&#24335;&#25429;&#25417;&#20102;&#27493;&#38271;&#23545;&#31639;&#27861;&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#23450;&#29702;&#24212;&#29992;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19977;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#27493;&#38271;&#35843;&#24230;&#30340;&#31934;&#30830;&#34920;&#36798;&#24335;&#65306;&#22266;&#23450;&#27493;&#38271;&#12289;&#36882;&#20943;&#27493;&#38271;&#21644;&#27493;&#34928;&#20943;&#27493;&#38271;&#65292;&#36825;&#20123;&#34920;&#36798;&#24335;&#29420;&#31435;&#20110;&#25968;&#25454;&#30456;&#20284;&#24615;&#26465;&#20214;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02347v1 Announce Type: new  Abstract: Data similarity assumptions have traditionally been relied upon to understand the convergence behaviors of federated learning methods. Unfortunately, this approach often demands fine-tuning step sizes based on the level of data similarity. When data similarity is low, these small step sizes result in an unacceptably slow convergence speed for federated methods. In this paper, we present a novel and unified framework for analyzing the convergence of federated learning algorithms without the need for data similarity conditions. Our analysis centers on an inequality that captures the influence of step sizes on algorithmic convergence performance. By applying our theorems to well-known federated algorithms, we derive precise expressions for three widely used step size schedules: fixed, diminishing, and step-decay step sizes, which are independent of data similarity conditions. Finally, we conduct comprehensive evaluations of the performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#32447;&#24615;&#37327;&#21270;&#24212;&#29992;&#20110;FPGA-based soft sensors&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31934;&#24230;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#21644;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#65292;&#20026;&#23454;&#26102;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;&#25552;&#20379;&#20102;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#26367;&#20195;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.01922</link><description>&lt;p&gt;
FlowPrecision: &#21033;&#29992;&#32447;&#24615;&#37327;&#21270;&#25512;&#36827;&#22522;&#20110;FPGA&#30340;&#23454;&#26102;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#32447;&#24615;&#37327;&#21270;&#24212;&#29992;&#20110;FPGA-based soft sensors&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#31934;&#24230;&#65292;&#36890;&#36807;&#30828;&#20214;&#20248;&#21270;&#23454;&#29616;&#20102;&#38477;&#20302;&#22343;&#26041;&#35823;&#24046;&#21644;&#25552;&#21319;&#25512;&#29702;&#36895;&#24230;&#65292;&#20026;&#23454;&#26102;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;&#25552;&#20379;&#20102;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21644;&#29615;&#22659;&#30417;&#27979;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21644;&#31934;&#30830;&#30340;&#27969;&#20307;&#27969;&#37327;&#27979;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#23558;&#32447;&#24615;&#37327;&#21270;&#24212;&#29992;&#20110;&#22522;&#20110;FPGA&#30340;&#36719;&#20256;&#24863;&#22120;&#20013;&#65292;&#29992;&#20110;&#27969;&#20307;&#27969;&#37327;&#20272;&#35745;&#65292;&#36890;&#36807;&#20811;&#26381;&#20256;&#32479;&#23450;&#28857;&#37327;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#31934;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#38024;&#23545;&#24615;&#30340;&#30828;&#20214;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#22343;&#26041;&#35823;&#24046;&#30340;&#26368;&#22810;&#20943;&#23569;10.10&#65285;&#65292;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#20102;9.39&#65285;&#12290;&#32463;&#36807;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#21518;&#30340;&#22522;&#20110;FPGA&#30340;&#37327;&#21270;&#27169;&#22411;&#21487;&#20197;&#25552;&#20379;&#39640;&#25928;&#12289;&#31934;&#30830;&#30340;&#23454;&#26102;&#25512;&#29702;&#65292;&#20026;&#26222;&#36941;&#33258;&#20027;&#31995;&#32479;&#20013;&#30340;&#20113;&#22788;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01922v1 Announce Type: new  Abstract: In industrial and environmental monitoring, achieving real-time and precise fluid flow measurement remains a critical challenge. This study applies linear quantization in FPGA-based soft sensors for fluid flow estimation, significantly enhancing Neural Network model precision by overcoming the limitations of traditional fixed-point quantization. Our approach achieves up to a 10.10% reduction in Mean Squared Error and a notable 9.39% improvement in inference speed through targeted hardware optimizations. Validated across multiple data sets, our findings demonstrate that the optimized FPGA-based quantized models can provide efficient, accurate real-time inference, offering a viable alternative to cloud-based processing in pervasive autonomous systems.
&lt;/p&gt;</description></item><item><title>&#23454;&#26045;&#36229;&#36807;60&#31181;&#19981;&#21516;&#30340;&#31163;&#31574;&#30053;&#20195;&#29702;&#65292;&#21457;&#29616;&#26576;&#20123;&#32452;&#21512;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#27491;&#21017;&#21270;&#35774;&#32622;&#19982;&#20219;&#21153;&#30340;&#20851;&#32852;&#24615;&#32422;&#20960;&#32500;&#22810;&#22810;&#12290;</title><link>https://arxiv.org/abs/2403.00514</link><description>&lt;p&gt;
Actor-Critic&#20013;&#30340;&#36807;&#24230;&#20272;&#35745;&#12289;&#36807;&#25311;&#21512;&#21644;&#21487;&#22609;&#24615;&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#33510;&#28073;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00514
&lt;/p&gt;
&lt;p&gt;
&#23454;&#26045;&#36229;&#36807;60&#31181;&#19981;&#21516;&#30340;&#31163;&#31574;&#30053;&#20195;&#29702;&#65292;&#21457;&#29616;&#26576;&#20123;&#32452;&#21512;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#29305;&#23450;&#27491;&#21017;&#21270;&#35774;&#32622;&#19982;&#20219;&#21153;&#30340;&#20851;&#32852;&#24615;&#32422;&#20960;&#32500;&#22810;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#31163;&#31574;&#30053;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21508;&#31181;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#30340;&#24212;&#29992;&#65292;&#20351;&#20854;&#27604;&#20256;&#32479;&#30340;&#20195;&#29702;&#26356;&#33021;&#36827;&#34892;&#26799;&#24230;&#26356;&#26032;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#36825;&#20123;&#25216;&#26415;&#37117;&#22312;&#26377;&#38480;&#30340;&#24773;&#26223;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36890;&#24120;&#21482;&#22312;&#21333;&#20010;&#20223;&#30495;&#22522;&#20934;&#20219;&#21153;&#19978;&#27979;&#35797;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#32780;&#19981;&#26159;&#19982;&#19968;&#31995;&#21015;&#27491;&#21017;&#21270;&#26041;&#27861;&#30456;&#27604;&#12290;&#36825;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#25512;&#21160;RL&#25913;&#36827;&#30340;&#20855;&#20307;&#26426;&#21046;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36229;&#36807;60&#31181;&#19981;&#21516;&#30340;&#31163;&#31574;&#30053;&#20195;&#29702;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#25972;&#21512;&#20102;&#26368;&#36817;&#26368;&#20808;&#36827;&#31639;&#27861;&#20013;&#30340;&#24050;&#24314;&#31435;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;2&#20010;&#20223;&#30495;&#22522;&#20934;&#30340;14&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#36825;&#20123;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#29305;&#23450;&#30340;&#27491;&#21017;&#21270;&#35774;&#32622;&#30340;&#25928;&#26524;&#22240;&#20219;&#21153;&#32780;&#24322;&#65292;&#20294;&#26576;&#20123;&#32452;&#21512;&#22987;&#32456;&#34920;&#29616;&#20986;&#31283;&#20581;&#21644;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00514v1 Announce Type: new  Abstract: Recent advancements in off-policy Reinforcement Learning (RL) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. However, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. This limits our understanding of the specific mechanisms driving RL improvements. To address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. We tested these agents across 14 diverse tasks from 2 simulation benchmarks. Our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior perform
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25195;&#25551;&#24207;&#21015;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#36229;&#22768;B&#27169;&#24335;&#22270;&#20687;&#65292;&#24182;&#20248;&#21270;&#36229;&#22768;&#25104;&#20687;&#32534;&#30721;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.00289</link><description>&lt;p&gt;
&#36229;&#22768;&#25104;&#20687;&#38453;&#21015;&#32534;&#30721;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization of Array Encoding for Ultrasound Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00289
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25195;&#25551;&#24207;&#21015;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#36229;&#22768;B&#27169;&#24335;&#22270;&#20687;&#65292;&#24182;&#20248;&#21270;&#36229;&#22768;&#25104;&#20687;&#32534;&#30721;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#21512;&#25104;&#23380;&#24452;&#25104;&#20687;&#30340;&#21457;&#23556;&#32534;&#30721;&#27169;&#22411;&#26159;&#29702;&#35299;&#36229;&#22768;&#25104;&#20687;&#37325;&#24314;&#30340;&#22768;&#23398;&#20256;&#36755;&#25928;&#24212;&#30340;&#31283;&#20581;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26500;&#24314;&#30001;&#26102;&#38388;&#24310;&#36831;&#21644;&#31783;&#24212;&#26435;&#21442;&#25968;&#21270;&#30340;&#25195;&#25551;&#24207;&#21015;&#65292;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;B&#27169;&#24335;&#22270;&#20687;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;PyTorch&#20013;&#30340;ML&#27169;&#22411;&#65292;&#24182;&#20174;Field II&#29983;&#25104;&#27169;&#25311;RF&#25968;&#25454;&#65292;&#25506;&#31350;&#21487;&#33021;&#32534;&#30721;&#24207;&#21015;&#31354;&#38388;&#65292;&#20197;&#25214;&#21040;&#33021;&#22815;&#26368;&#23567;&#21270;&#25551;&#36848;&#22270;&#20687;&#36136;&#37327;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24207;&#21015;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#24310;&#36831;-&#27714;&#21644;&#27874;&#26463;&#25104;&#24418;&#23548;&#25968;&#30340;&#20844;&#24335;&#21270;&#65292;&#22312;&#35745;&#31639;&#19978;&#21464;&#24471;&#21487;&#34892;&#12290;&#25105;&#20204;&#22312;&#37329;&#23646;&#19997;&#38774;&#21644;&#32452;&#32455;&#27169;&#25311;&#24187;&#20687;&#19978;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;&#20027;&#35201;&#32467;&#26524;&#65306;&#26681;&#25454;&#32473;&#23450;&#30340;&#25104;&#20687;&#21442;&#25968;&#65288;&#25104;&#20687;&#22495;&#65292;&#30828;&#20214;&#38480;&#21046;&#65289;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#30340;ML&#25104;&#20687;&#27169;&#22411;&#20135;&#29983;&#20102;&#20248;&#21270;&#30340;&#32534;&#30721;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00289v1 Announce Type: cross  Abstract: Objective: The transmit encoding model for synthetic aperture imaging is a robust and flexible framework for understanding the effect of acoustic transmission on ultrasound image reconstruction. Our objective is to use machine learning (ML) to construct scanning sequences, parameterized by time delays and apodization weights, that produce high quality B-mode images. Approach: We use an ML model in PyTorch and simulated RF data from Field II to probe the space of possible encoding sequences for those that minimize a loss function that describes image quality. This approach is made computationally feasible by a novel formulation of the derivative for delay-and-sum beamforming. We demonstrate these results experimentally on wire targets and a tissue-mimicking phantom. Main Results: When trained according to a given set of imaging parameters (imaging domain, hardware restrictions), our ML imaging model produces optimized encoding sequences
&lt;/p&gt;</description></item><item><title>FedStruct&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#22312;&#20114;&#32852;&#22270;&#19978;&#36827;&#34892;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25252;&#38544;&#31169;&#24182;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.19163</link><description>&lt;p&gt;
FedStruct&#65306;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#22312;&#20114;&#32852;&#22270;&#19978;
&lt;/p&gt;
&lt;p&gt;
FedStruct: Federated Decoupled Learning over Interconnected Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19163
&lt;/p&gt;
&lt;p&gt;
FedStruct&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#22312;&#20114;&#32852;&#22270;&#19978;&#36827;&#34892;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25252;&#38544;&#31169;&#24182;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#20114;&#32852;&#23376;&#22270;&#30340;&#26222;&#36941;&#24773;&#20917;&#65292;&#20854;&#20013;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#20114;&#36830;&#25509;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21517;&#20026;FedStruct&#65292;&#23427;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#32500;&#25252;&#38544;&#31169;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FedStruct&#28040;&#38500;&#20102;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#25110;&#29983;&#25104;&#25935;&#24863;&#33410;&#28857;&#29305;&#24449;&#25110;&#23884;&#20837;&#30340;&#24517;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#23427;&#21033;&#29992;&#26174;&#24335;&#20840;&#23616;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;FedStruct&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65288;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#12289;&#19981;&#21516;&#26631;&#31614;&#21487;&#29992;&#24615;&#20197;&#21450;&#23458;&#25143;&#20010;&#25968;&#30340;&#65289;&#25509;&#36817;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19163v1 Announce Type: new  Abstract: We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of cl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2402.18129</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#30340;&#20844;&#24179;&#23398;&#20064;&#31639;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
On the Inductive Biases of Demographic Parity-based Fair Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18129
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#26631;&#20934; DP &#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#26041;&#27861;&#26469;&#25511;&#21046;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#31639;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20998;&#37197;&#26631;&#31614;&#26102;&#24456;&#23569;&#20381;&#36182;&#25935;&#24863;&#23646;&#24615;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#26631;&#20934;DP&#65288;&#20154;&#21475;&#32479;&#35745;&#23398;&#24179;&#31561;&#65289;&#22522;&#30784;&#27491;&#21017;&#21270;&#26041;&#27861;&#23545;&#32473;&#23450;&#25935;&#24863;&#23646;&#24615;&#30340;&#39044;&#27979;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#38750;&#22343;&#21248;&#20998;&#24067;&#25935;&#24863;&#23646;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#20998;&#31867;&#35268;&#21017;&#20559;&#21521;&#21344;&#25454;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#30340;&#25935;&#24863;&#23646;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#25511;&#21046;DP-based&#20844;&#24179;&#23398;&#20064;&#20013;&#30340;&#36825;&#31181;&#24402;&#32435;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#30340;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#65288;SA&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18129v1 Announce Type: cross  Abstract: Fair supervised learning algorithms assigning labels with little dependence on a sensitive attribute have attracted great attention in the machine learning community. While the demographic parity (DP) notion has been frequently used to measure a model's fairness in training fair classifiers, several studies in the literature suggest potential impacts of enforcing DP in fair learning algorithms. In this work, we analytically study the effect of standard DP-based regularization methods on the conditional distribution of the predicted label given the sensitive attribute. Our analysis shows that an imbalanced training dataset with a non-uniform distribution of the sensitive attribute could lead to a classification rule biased toward the sensitive attribute outcome holding the majority of training data. To control such inductive biases in DP-based fair learning, we propose a sensitive attribute-based distributionally robust optimization (SA
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;GNN&#22312;&#25311;&#21512;&#27700;&#31995;&#32479;&#21183;&#33021;&#34920;&#38754;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.15921</link><description>&lt;p&gt;
&#31070;&#32463;&#21183;&#39044;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Pretraining Strategy for Neural Potentials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15921
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#25913;&#21892;&#20102;GNN&#22312;&#25311;&#21512;&#27700;&#31995;&#32479;&#21183;&#33021;&#34920;&#38754;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#20248;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#25513;&#30721;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#20854;&#22312;&#25311;&#21512;&#21183;&#33021;&#34920;&#38754;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#27700;&#31995;&#32479;&#20013;&#12290;GNNs&#36890;&#36807;&#20174;&#20998;&#23376;&#20013;&#24674;&#22797;&#19982;&#25513;&#30721;&#30340;&#21407;&#23376;&#30456;&#20851;&#30340;&#31354;&#38388;&#20449;&#24687;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#21407;&#23376;&#21147;&#22330;&#19978;&#36827;&#34892;&#36716;&#31227;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#39044;&#35757;&#32451;&#65292;GNNs&#23398;&#20064;&#20102;&#20851;&#20110;&#20998;&#23376;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#28508;&#22312;&#29289;&#29702;&#20449;&#24687;&#30340;&#26377;&#24847;&#20041;&#20808;&#39564;&#65292;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#26377;&#29992;&#30340;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#25110;&#20351;&#29992;&#20854;&#20182;&#39044;&#35757;&#32451;&#25216;&#26415;&#65288;&#22914;&#21435;&#22122;&#65289;&#30340;GNNs&#65292;&#22312;&#31934;&#24230;&#21644;&#25910;&#25947;&#36895;&#24230;&#19978;&#30340;&#25552;&#39640;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#36866;&#29992;&#20110;&#20197;&#33021;&#37327;&#20026;&#20013;&#24515;&#21644;&#20197;&#21147;&#20026;&#20013;&#24515;&#30340;GNNs&#12290;&#36825;&#31181;&#26041;&#27861;&#23637;&#31034;&#20102;&#23427;&#22312;&#25311;&#21512;&#20998;&#23376;&#21147;&#22330;&#26041;&#38754;&#25552;&#39640;&#20102;GNNs&#30340;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15921v1 Announce Type: new  Abstract: We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.
&lt;/p&gt;</description></item><item><title>OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;</title><link>https://arxiv.org/abs/2402.15810</link><description>&lt;p&gt;
OAG-Bench&#65306;&#38754;&#21521;&#23398;&#26415;&#22270;&#25366;&#25496;&#30340;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OAG-Bench: A Human-Curated Benchmark for Academic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15810
&lt;/p&gt;
&lt;p&gt;
OAG-Bench&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#23454;&#39564;&#32467;&#26524;&#65292;&#26088;&#22312;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#22810;&#21151;&#33021;&#30340;&#23398;&#26415;&#30693;&#35782;&#26381;&#21153;&#36234;&#26469;&#36234;&#20381;&#36182;&#20840;&#38754;&#30340;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#23613;&#31649;&#20844;&#24320;&#23398;&#26415;&#22270;&#12289;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#24050;&#32463;&#26377;&#20102;&#65292;&#20294;&#36825;&#20123;&#36164;&#28304;&#36890;&#24120;&#22312;&#22810;&#26041;&#38754;&#21644;&#32454;&#31890;&#24230;&#27880;&#37322;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#31867;&#22411;&#21644;&#39046;&#22495;&#65292;&#25110;&#32773;&#32570;&#20047;&#30495;&#23454;&#23398;&#26415;&#22270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24320;&#25918;&#23398;&#26415;&#22270;&#65288;OAG&#65289;&#30340;&#20840;&#38754;&#12289;&#22810;&#26041;&#38754;&#21644;&#31934;&#32454;&#21270;&#20154;&#24037;&#31579;&#36873;&#22522;&#20934;OAG-Bench&#12290;OAG-Bench&#28085;&#30422;&#20102;10&#20010;&#20219;&#21153;&#65292;20&#20010;&#25968;&#25454;&#38598;&#65292;70+&#20010;&#22522;&#20934;&#21644;120+&#20010;&#25130;&#33267;&#30446;&#21069;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#38024;&#23545;&#26576;&#20123;&#20219;&#21153;&#25552;&#20986;&#20102;&#26032;&#30340;&#25968;&#25454;&#27880;&#37322;&#31574;&#30053;&#65292;&#24182;&#25552;&#20379;&#19968;&#22871;&#25968;&#25454;&#39044;&#22788;&#29702;&#20195;&#30721;&#12289;&#31639;&#27861;&#23454;&#29616;&#21644;&#26631;&#20934;&#21270;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#20419;&#36827;&#23398;&#26415;&#22270;&#25366;&#25496;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36825;&#26679;&#30340;&#20808;&#36827;&#31639;&#27861;&#20063;&#20250;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15810v1 Announce Type: cross  Abstract: With the rapid proliferation of scientific literature, versatile academic knowledge services increasingly rely on comprehensive academic graph mining. Despite the availability of public academic graphs, benchmarks, and datasets, these resources often fall short in multi-aspect and fine-grained annotations, are constrained to specific task types and domains, or lack underlying real academic graphs. In this paper, we present OAG-Bench, a comprehensive, multi-aspect, and fine-grained human-curated benchmark based on the Open Academic Graph (OAG). OAG-Bench covers 10 tasks, 20 datasets, 70+ baselines, and 120+ experimental results to date. We propose new data annotation strategies for certain tasks and offer a suite of data pre-processing codes, algorithm implementations, and standardized evaluation protocols to facilitate academic graph mining. Extensive experiments reveal that even advanced algorithms like large language models (LLMs) en
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.15537</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Performance of ChatGPT for Spam Email Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15537
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#37038;&#20214;&#32487;&#32493;&#26159;&#19987;&#19994;&#21644;&#21830;&#19994;&#39046;&#22495;&#20013;&#33267;&#20851;&#37325;&#35201;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#36890;&#20449;&#23186;&#20171;&#12290;&#28982;&#32780;&#65292;&#22403;&#22334;&#37038;&#20214;&#30340;&#26222;&#21450;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#25200;&#20081;&#20102;&#20182;&#20204;&#30340;&#26085;&#24120;&#24037;&#20316;&#24182;&#38477;&#20302;&#20102;&#29983;&#20135;&#29575;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#20869;&#23481;&#20934;&#30830;&#22320;&#35782;&#21035;&#21644;&#36807;&#28388;&#22403;&#22334;&#37038;&#20214;&#23545;&#32593;&#32476;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#65292;&#22312;&#35832;&#22914;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#22312;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#23581;&#35797;&#35780;&#20272;ChatGPT&#22312;&#33521;&#25991;&#21644;&#20013;&#25991;&#30005;&#23376;&#37038;&#20214;&#25968;&#25454;&#38598;&#20013;&#29992;&#20110;&#22403;&#22334;&#37038;&#20214;&#35782;&#21035;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;ChatGPT&#36827;&#34892;&#22403;&#22334;&#37038;&#20214;&#26816;&#27979;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#38656;&#35201;&#25552;&#31034;&#35828;&#26126;&#21644;&#23569;&#37327;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15537v1 Announce Type: cross  Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction and a few demonstrations. We also investigate how the t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.15441</link><description>&lt;p&gt;
&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Active Few-Shot Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ITL&#26041;&#27861;&#26469;&#23454;&#29616;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#33719;&#21462;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#35843;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#20027;&#21160;&#23569;&#26679;&#26412;&#24494;&#35843;&#12290;&#25105;&#20204;&#34920;&#26126;&#23569;&#26679;&#26412;&#24494;&#35843;&#26159;&#20256;&#32479;&#20027;&#21160;&#23398;&#20064;&#21644;&#36716;&#23548;&#20027;&#21160;&#23398;&#20064;&#30340;&#27867;&#21270;&#23454;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#22522;&#20110;&#36716;&#23548;&#23398;&#20064;&#65288;ITL&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#36827;&#34892;&#37319;&#26679;&#20197;&#26368;&#22823;&#21270;&#33719;&#24471;&#23545;&#25351;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20449;&#24687;&#12290;&#22312;&#19968;&#33324;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;ITL&#22343;&#21248;&#25910;&#25947;&#21040;&#21487;&#20174;&#21487;&#35775;&#38382;&#25968;&#25454;&#33719;&#21462;&#30340;&#26368;&#23567;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#25209;&#25512;&#23548;&#20986;&#36825;&#31181;&#27867;&#21270;&#30028;&#38480;&#30340;&#20154;&#65292;&#36825;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#20855;&#26377;&#29420;&#31435;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#23558;ITL&#24212;&#29992;&#20110;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#23569;&#26679;&#26412;&#24494;&#35843;&#20013;&#65292;&#32467;&#26524;&#26174;&#31034;ITL&#26126;&#26174;&#25913;&#36827;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15441v1 Announce Type: cross  Abstract: We study the active few-shot fine-tuning of large neural networks to downstream tasks. We show that few-shot fine-tuning is an instance of a generalization of classical active learning, transductive active learning, and we propose ITL, short for information-based transductive learning, an approach which samples adaptively to maximize the information gained about specified downstream tasks. Under general regularity assumptions, we prove that ITL converges uniformly to the smallest possible uncertainty obtainable from the accessible data. To the best of our knowledge, we are the first to derive generalization bounds of this kind, and they may be of independent interest for active learning. We apply ITL to the few-shot fine-tuning of large neural networks and show that ITL substantially improves upon the state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14897</link><description>&lt;p&gt;
Chain-of-Thought&#19981;&#24544;&#35802;&#20316;&#20026;&#20266;&#35013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought Unfaithfulness as Disguised Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14897
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;&#27169;&#22411;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#19982;&#24544;&#23454;&#24230;&#20043;&#38388;&#23384;&#22312;&#30528;&#29305;&#23450;&#20851;&#31995;&#65292;&#24182;&#19988;&#21457;&#29616;130&#20159;&#21442;&#25968;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;Chain-of-Thought (CoT)&#29983;&#25104;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#20869;&#37096;&#35745;&#31639;&#30340;&#19968;&#33268;&#31243;&#24230;&#23545;&#20110;&#20915;&#23450;&#26159;&#21542;&#20449;&#20219;LLM&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;CoT&#24544;&#23454;&#24230;&#30340;&#20195;&#29702;&#65292;arXiv:2307.13702&#25552;&#20986;&#20102;&#19968;&#20010;&#24230;&#37327;&#27169;&#22411;&#20381;&#36182;&#20854;CoT&#29983;&#25104;&#31572;&#26696;&#30340;&#25351;&#26631;&#12290;&#22312;&#19968;&#20010;&#19987;&#26377;&#27169;&#22411;&#31995;&#21015;&#20013;&#65292;&#20182;&#20204;&#21457;&#29616;LLM&#34920;&#29616;&#20986;&#27169;&#22411;&#22823;&#23567;&#19982;&#20854;&#24544;&#23454;&#24230;&#27979;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;-&#21453;&#21521;&#32553;&#25918;&#20851;&#31995;&#65292;&#24182;&#19988;130&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#27604;&#20110;&#23610;&#23544;&#20171;&#20110;8.1&#20159;&#21040;1750&#20159;&#21442;&#25968;&#20043;&#38388;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#22686;&#21152;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#32467;&#26524;&#26159;&#21542;&#20316;&#20026;&#25152;&#26377;LLM&#30340;&#29305;&#24615;&#27867;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#31995;&#21015;&#30340;&#27169;&#22411;&#22797;&#21046;&#20182;&#20204;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#24182;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#65292;&#25104;&#21151;&#22797;&#21046;&#20102;&#20182;&#20204;&#25253;&#21578;&#30340;CoT&#24544;&#23454;&#24230;&#30340;&#32553;&#25918;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#25913;&#21464;&#35774;&#23450;&#20250;&#23548;&#33268;&#36825;&#20123;&#27169;&#24335;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14897v1 Announce Type: cross  Abstract: Understanding the extent to which Chain-of-Thought (CoT) generations align with a large language model's (LLM) internal computations is critical for deciding whether to trust an LLM's output. As a proxy for CoT faithfulness, arXiv:2307.13702 propose a metric that measures a model's dependence on its CoT for producing an answer. Within a single family of proprietary models, they find that LLMs exhibit a scaling-then-inverse-scaling relationship between model size and their measure of faithfulness, and that a 13 billion parameter model exhibits increased faithfulness compared to models ranging from 810 million to 175 billion parameters in size. We evaluate whether these results generalize as a property of all LLMs. We replicate their experimental setup with three different families of models and, under specific conditions, successfully reproduce the scaling trends for CoT faithfulness they report. However, we discover that simply changin
&lt;/p&gt;</description></item><item><title>DynGMA&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#36817;&#20284;&#65292;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#22312;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#19981;&#21464;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14475</link><description>&lt;p&gt;
DynGMA&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#23398;&#20064;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#31283;&#20581;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DynGMA: a robust approach for learning stochastic differential equations from data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14475
&lt;/p&gt;
&lt;p&gt;
DynGMA&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#23494;&#24230;&#36817;&#20284;&#65292;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#22312;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#19981;&#21464;&#24615;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#23398;&#20064;&#26410;&#30693;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDEs&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#26032;&#30340;&#36817;&#20284;&#21442;&#25968;&#21270;SDE&#36716;&#31227;&#23494;&#24230;&#30340;&#26041;&#27861;&#65306;&#21463;&#21160;&#21147;&#31995;&#32479;&#38543;&#26426;&#25668;&#21160;&#29702;&#35770;&#21551;&#21457;&#30340;&#39640;&#26031;&#23494;&#24230;&#36817;&#20284;&#65292;&#20197;&#21450;&#23427;&#30340;&#25193;&#23637;&#65292;&#21160;&#21147;&#39640;&#26031;&#28151;&#21512;&#36817;&#20284;&#65288;DynGMA&#65289;&#12290;&#21463;&#30410;&#20110;&#31283;&#20581;&#30340;&#23494;&#24230;&#36817;&#20284;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#23436;&#20840;&#26410;&#30693;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#30697;&#19981;&#21464;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14475v1 Announce Type: new  Abstract: Learning unknown stochastic differential equations (SDEs) from observed data is a significant and challenging task with applications in various fields. Current approaches often use neural networks to represent drift and diffusion functions, and construct likelihood-based loss by approximating the transition density to train these networks. However, these methods often rely on one-step stochastic numerical schemes, necessitating data with sufficiently high time resolution. In this paper, we introduce novel approximations to the transition density of the parameterized SDE: a Gaussian density approximation inspired by the random perturbation theory of dynamical systems, and its extension, the dynamical Gaussian mixture approximation (DynGMA). Benefiting from the robust density approximation, our method exhibits superior accuracy compared to baseline methods in learning the fully unknown drift and diffusion functions and computing the invari
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14400</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14400
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#38656;&#35201;&#21450;&#26102;&#24178;&#39044;&#30340;&#21307;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21457;&#30340;&#36816;&#21160;&#27963;&#21160;&#65292;&#21363;&#8220;&#21160;&#21147;&#23398;&#8221;&#65292;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#39044;&#27979;&#26410;&#26469;&#31070;&#32463;&#21457;&#32946;&#30340;&#26367;&#20195;&#24615;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#35780;&#20272;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#23450;&#24615;&#21644;&#20027;&#35266;&#30340;&#65292;&#20391;&#37325;&#20110;&#23545;&#36890;&#36807;&#35270;&#35273;&#35782;&#21035;&#30340;&#29305;&#23450;&#24180;&#40836;&#25163;&#21183;&#30340;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26681;&#25454;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#26469;&#39044;&#27979;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#25104;&#29087;&#12290;&#25105;&#20204;&#21033;&#29992;&#22788;&#29702;&#36807;&#30340;3D&#23156;&#20799;&#35270;&#39057;&#24405;&#20687;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;&#65292;&#25552;&#21462;&#35299;&#21078;&#26631;&#24535;&#29289;&#30340;&#26102;&#31354;&#31995;&#21015;&#65292;&#24182;&#24212;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#23454;&#38469;&#24180;&#40836;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14400v1 Announce Type: cross  Abstract: Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12921</link><description>&lt;p&gt;
&#20934;&#26102;&#21040;&#20301;&#65306;&#36890;&#36807;&#38480;&#21046;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#35299;&#37322;&#26469;&#20462;&#35746;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Right on Time: Revising Time Series Models by Constraining their Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12921
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#32463;&#24120;&#20250;&#21463;&#21040;&#20854;&#20381;&#36182;&#28151;&#26434;&#22240;&#32032;&#30340;&#20542;&#21521;&#30340;&#25439;&#23475;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26032;&#35760;&#24405;&#30340;&#12289;&#33258;&#28982;&#28151;&#26434;&#30340;&#25968;&#25454;&#38598;P2S&#26469;&#33258;&#30495;&#23454;&#30340;&#26426;&#26800;&#29983;&#20135;&#32447;&#65292;&#24378;&#35843;&#20102;&#36825;&#19968;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#21518;&#21033;&#29992;&#20004;&#20010;&#22495;&#20869;&#30340;&#35299;&#37322;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#20351;&#20854;&#36828;&#31163;&#26631;&#27880;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#28151;&#26434;&#22240;&#32032;&#26041;&#38754;&#65292;&#21452;&#22495;&#20132;&#20114;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;RioT&#33021;&#22815;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#36828;&#31163;P2S&#20197;&#21450;&#27969;&#34892;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12921v1 Announce Type: new  Abstract: The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12821</link><description>&lt;p&gt;
&#22312;&#25688;&#35201;&#20013;&#35782;&#21035;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65306;&#26397;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#25688;&#35201;&#20013;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27491;&#30830;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#35299;&#20915;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#35757;&#32451;&#31574;&#30053;&#20197;&#31934;&#28860;&#26356;&#23567;&#22411;&#30340;&#39640;&#20934;&#30830;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#23545;&#25277;&#35937;&#24615;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21830;&#19994;&#37096;&#32626;&#26500;&#25104;&#37325;&#35201;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#22260;&#32469;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#23637;&#24320;&#65306;&#22914;&#20309;&#26368;&#22909;&#22320;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#20107;&#23454;&#19981;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#31934;&#28860;&#19968;&#20010;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#21151;&#25928;&#24615;&#30340;&#26356;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65311;&#39318;&#20808;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#38646;&#26679;&#26412;&#33539;&#24335;&#65292;&#36328;&#36234;&#20116;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65306;&#30452;&#25509;&#25512;&#29702;&#25972;&#20010;&#25688;&#35201;&#25110;&#27599;&#20010;&#25688;&#35201;&#31383;&#21475;&#65307;&#36890;&#36807;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#36827;&#34892;&#23454;&#20307;&#39564;&#35777;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#33539;&#24335;&#35774;&#35745;&#19979;&#65292;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#33021;&#22815;&#22312;&#26080;&#38656;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#24179;&#22343;&#36229;&#36807;&#24378;&#22823;&#30340;&#35757;&#32451;&#22522;&#32447;2.8%&#12290;&#20026;&#36827;&#19968;&#27493;&#20419;&#36827;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#38024;&#23545;&#31934;&#28860;&#26356;&#23567;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#19968;&#27425;&#24615;&#39640;&#20934;&#30830;&#22320;&#35780;&#20998;&#25972;&#20010;&#25688;&#35201;&#65292;&#32988;&#36807;&#38646;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12821v1 Announce Type: new  Abstract: Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#21644;&#22797;&#26434;&#39046;&#22495;&#30340;&#29289;&#29702;&#31995;&#32479;&#30340;&#39046;&#22495;&#28789;&#27963;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#23398;&#20064;&#20989;&#25968;&#26144;&#23556;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20849;&#20139;&#30340;&#24494;&#20998;&#21516;&#32986;&#19978;&#23398;&#20064;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12475</link><description>&lt;p&gt;
&#19981;&#21516;&#39046;&#22495;&#21644;&#21442;&#25968;&#30340;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Diffeomorphism Neural Operator for various domains and parameters of partial differential equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#21644;&#22797;&#26434;&#39046;&#22495;&#30340;&#29289;&#29702;&#31995;&#32479;&#30340;&#39046;&#22495;&#28789;&#27963;&#27169;&#22411;&#65292;&#20174;&#32780;&#23558;&#23398;&#20064;&#20989;&#25968;&#26144;&#23556;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20849;&#20139;&#30340;&#24494;&#20998;&#21516;&#32986;&#19978;&#23398;&#20064;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#38656;&#35201;&#23545;&#20256;&#32479;&#19978;&#20351;&#29992;&#36164;&#28304;&#23494;&#38598;&#22411;&#25968;&#20540;&#27714;&#35299;&#22120;&#35745;&#31639;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#31070;&#32463;&#31639;&#23376;&#27169;&#22411;&#36890;&#36807;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#25511;&#21046;&#29289;&#29702;&#23450;&#24459;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#21442;&#25968;&#30340;PDE&#31867;&#21035;&#65292;&#20294;&#22312;&#22266;&#23450;&#36793;&#30028;&#65288;&#39046;&#22495;&#65289;&#20869;&#21463;&#38480;&#12290;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#35774;&#35745;&#21644;&#21046;&#36896;&#65292;&#22312;&#22823;&#35268;&#27169;&#30740;&#31350;&#26102;&#23558;&#21463;&#30410;&#20110;&#20855;&#26377;&#28789;&#27963;&#39046;&#22495;&#30340;&#31070;&#32463;&#31639;&#23376;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24494;&#20998;&#21516;&#32986;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20855;&#26377;&#21508;&#31181;&#21644;&#22797;&#26434;&#39046;&#22495;&#30340;&#29289;&#29702;&#31995;&#32479;&#24320;&#21457;&#39046;&#22495;&#28789;&#27963;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#30001;&#24494;&#20998;&#21516;&#32986;&#20174;&#21508;&#39046;&#22495;&#26144;&#23556;&#32780;&#26469;&#30340;&#20849;&#20139;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#31070;&#32463;&#31639;&#23376;&#65292;&#35813;&#26041;&#27861;&#23558;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#31354;&#38388;&#65289;&#23398;&#20064;&#20989;&#25968;&#26144;&#23556;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#20849;&#20139;&#30340;&#24494;&#20998;&#21516;&#32986;&#19978;&#23398;&#20064;&#31639;&#23376;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12475v1 Announce Type: cross  Abstract: Many science and engineering applications demand partial differential equations (PDE) evaluations that are traditionally computed with resource-intensive numerical solvers. Neural operator models provide an efficient alternative by learning the governing physical laws directly from data in a class of PDEs with different parameters, but constrained in a fixed boundary (domain). Many applications, such as design and manufacturing, would benefit from neural operators with flexible domains when studied at scale. Here we present a diffeomorphism neural operator learning framework towards developing domain-flexible models for physical systems with various and complex domains. Specifically, a neural operator trained in a shared domain mapped from various domains of fields by diffeomorphism is proposed, which transformed the problem of learning function mappings in varying domains (spaces) into the problem of learning operators on a shared dif
&lt;/p&gt;</description></item><item><title>Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11729</link><description>&lt;p&gt;
Prospector Heads:&#22823;&#35268;&#27169;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#24191;&#20041;&#29305;&#24449;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Prospector Heads: Generalized Feature Attribution for Large Models &amp; Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11729
&lt;/p&gt;
&lt;p&gt;
Prospector heads&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#24418;&#24577;&#30340;&#23454;&#39564;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#19968;&#31181;&#23450;&#20301;&#36755;&#20837;&#25968;&#25454;&#20013;&#19982;&#20998;&#31867;&#30456;&#20851;&#30340;&#21306;&#22495;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32780;&#35328;&#65292;&#36825;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#33021;&#21147;&#12290;&#24403;&#21069;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#20381;&#36182;&#20110;&#8220;&#35299;&#37322;&#8221;&#31471;&#21040;&#31471;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#65292;&#23384;&#22312;&#29305;&#24449;&#23450;&#20301;&#19981;&#31934;&#30830;&#20197;&#21450;&#30001;&#20110;&#35745;&#31639;&#25361;&#25112;&#32780;&#26080;&#27861;&#22312;&#23567;&#26679;&#26412;&#23610;&#23544;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#65288;prospector heads&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#29305;&#24449;&#24402;&#22240;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#32534;&#30721;&#22120;&#21644;&#20219;&#20309;&#25968;&#25454;&#24418;&#24577;&#12290;&#36890;&#36807;&#23545;&#24207;&#21015;&#65288;&#25991;&#26412;&#65289;&#12289;&#22270;&#20687;&#65288;&#30149;&#29702;&#23398;&#65289;&#21644;&#22270;&#65288;&#34507;&#30333;&#36136;&#32467;&#26500;&#65289;&#30340;&#23454;&#39564;&#65292;&#25506;&#23547;&#32773;&#22836;&#37096;&#22312;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#27010;&#25324;&#65292;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#24402;&#22240;&#26041;&#27861;&#65292;&#24179;&#22343;&#23616;&#37096;&#21270;AUPRC&#24471;&#20998;&#25552;&#21319;&#20102;&#39640;&#36798;49&#28857;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#25506;&#23547;&#32773;&#22836;&#37096;&#22914;&#20309;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
&lt;/p&gt;</description></item><item><title>Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11463</link><description>&lt;p&gt;
&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#21560;&#24341;&#23376;&#35760;&#24518;&#65306;&#28151;&#27788;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11463
&lt;/p&gt;
&lt;p&gt;
Attraos&#27169;&#22411;&#22522;&#20110;&#28151;&#27788;&#29702;&#35770;&#65292;&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#21033;&#29992;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#21644;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#65292;&#34920;&#29616;&#20248;&#24322;&#20110;&#20854;&#20182;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;LTSF&#65289;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24573;&#35270;&#20102;&#31163;&#25955;&#26102;&#38388;&#24207;&#21015;&#28304;&#33258;&#28508;&#22312;&#36830;&#32493;&#21160;&#24577;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#23548;&#33268;&#32570;&#20047;&#22806;&#25512;&#21644;&#28436;&#21270;&#33021;&#21147;&#12290; &#37492;&#21035;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#28151;&#27788;&#24615;&#36136;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;\textbf{\textit{Attraos}}&#23558;&#28151;&#27788;&#29702;&#35770;&#34701;&#20837;&#21040;LTSF&#20013;&#65292;&#23558;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#35270;&#20026;&#26410;&#30693;&#39640;&#32500;&#28151;&#27788;&#21160;&#24577;&#31995;&#32479;&#30340;&#35266;&#27979;&#12290; &#22312;&#21560;&#24341;&#23376;&#19981;&#21464;&#24615;&#30340;&#27010;&#24565;&#19979;&#65292;Attraos&#21033;&#29992;&#25552;&#20986;&#30340;&#22810;&#23610;&#24230;&#21160;&#24577;&#35760;&#24518;&#21333;&#20803;&#26469;&#35760;&#24518;&#21382;&#21490;&#21160;&#24577;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#39057;&#29575;&#22686;&#24378;&#30340;&#23616;&#37096;&#28436;&#21270;&#31574;&#30053;&#36827;&#34892;&#39044;&#27979;&#12290; &#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#20016;&#23500;&#30340;&#32463;&#39564;&#35777;&#25454;&#19968;&#33268;&#34920;&#26126;&#65292;Attraos&#22312;&#20027;&#27969;LTSF&#25968;&#25454;&#38598;&#21644;&#28151;&#27788;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#21508;&#31181;LTSF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#21644;&#25506;&#35752;&#20102;&#38236;&#20687;&#24433;&#21709;&#20551;&#35774;&#65292;&#31361;&#20986;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#24433;&#21709;&#30340;&#30456;&#20114;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25351;&#20986;&#65292;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#23545;&#27979;&#35797;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#31561;&#25928;&#20294;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#35780;&#20272;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#35757;&#32451;&#26679;&#26412;&#30340;&#39044;&#27979;&#23558;&#22914;&#20309;&#25913;&#21464;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08922</link><description>&lt;p&gt;
&#38236;&#20687;&#24433;&#21709;&#20551;&#35774;&#65306;&#36890;&#36807;&#21033;&#29992;&#21069;&#21521;&#20256;&#36882;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#24433;&#21709;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#25506;&#35752;&#20102;&#38236;&#20687;&#24433;&#21709;&#20551;&#35774;&#65292;&#31361;&#20986;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#24433;&#21709;&#30340;&#30456;&#20114;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25351;&#20986;&#65292;&#35780;&#20272;&#35757;&#32451;&#25968;&#25454;&#23545;&#27979;&#35797;&#39044;&#27979;&#30340;&#24433;&#21709;&#21487;&#20197;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#31561;&#25928;&#20294;&#30456;&#21453;&#30340;&#38382;&#39064;&#65306;&#35780;&#20272;&#22914;&#26524;&#27169;&#22411;&#22312;&#29305;&#23450;&#30340;&#27979;&#35797;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#23545;&#35757;&#32451;&#26679;&#26412;&#30340;&#39044;&#27979;&#23558;&#22914;&#20309;&#25913;&#21464;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#39564;&#35777;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#19968;&#20551;&#35774;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#40657;&#30418;&#27169;&#22411;&#24050;&#32463;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#20102;&#35299;&#20010;&#21035;&#35757;&#32451;&#25968;&#25454;&#28304;&#23545;&#36825;&#20123;&#27169;&#22411;&#25152;&#20570;&#39044;&#27979;&#30340;&#24433;&#21709;&#23545;&#20110;&#25913;&#21892;&#20854;&#21487;&#20449;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#30340;&#24433;&#21709;&#35780;&#20272;&#25216;&#26415;&#28041;&#21450;&#35745;&#31639;&#27599;&#20010;&#35757;&#32451;&#28857;&#30340;&#26799;&#24230;&#25110;&#22312;&#19981;&#21516;&#23376;&#38598;&#19978;&#37325;&#22797;&#35757;&#32451;&#12290;&#24403;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#38754;&#20020;&#26126;&#26174;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08922v1 Announce Type: new Abstract: Large-scale black-box models have become ubiquitous across numerous applications. Understanding the influence of individual training data sources on predictions made by these models is crucial for improving their trustworthiness. Current influence estimation techniques involve computing gradients for every training point or repeated training on different subsets. These approaches face obvious computational challenges when scaled up to large datasets and models.   In this paper, we introduce and explore the Mirrored Influence Hypothesis, highlighting a reciprocal nature of influence between training and test data. Specifically, it suggests that evaluating the influence of training data on test predictions can be reformulated as an equivalent, yet inverse problem: assessing how the predictions for training samples would be altered if the model were trained on specific test samples. Through both empirical and theoretical validations, we demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#25509;&#36817;&#26368;&#20248;&#12290;&#31639;&#27861;&#21019;&#26032;&#21253;&#25324;&#20102;&#23545;&#21152;&#26435;MLE&#30340;&#31934;&#30830;&#19988;&#32039;&#23494;&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.07445</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Top-$K$ ranking with a monotone adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#25509;&#36817;&#26368;&#20248;&#12290;&#31639;&#27861;&#21019;&#26032;&#21253;&#25324;&#20102;&#23545;&#21152;&#26435;MLE&#30340;&#31934;&#30830;&#19988;&#32039;&#23494;&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#27604;&#36739;&#22270;&#34987;&#38543;&#26426;&#29983;&#25104;&#19988;&#23545;&#25163;&#21487;&#20197;&#28155;&#21152;&#20219;&#24847;&#36793;&#30340;&#24773;&#20917;&#12290;&#32479;&#35745;&#23398;&#23478;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#20174;&#36825;&#20010;&#21322;&#38543;&#26426;&#27604;&#36739;&#22270;&#23548;&#20986;&#30340;&#20004;&#20004;&#27604;&#36739;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;Top-K&#30340;&#39318;&#36873;&#39033;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20986;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#23427;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;$log^2(n)$&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#34920;&#31034;&#27604;&#36739;&#39033;&#30340;&#25968;&#37327;&#12290;&#36825;&#24471;&#30410;&#20110;&#20998;&#26512;&#21644;&#31639;&#27861;&#21019;&#26032;&#30340;&#32467;&#21512;&#12290;&#22312;&#20998;&#26512;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26126;&#30830;&#12289;&#26356;&#32039;&#23494;&#30340;&#21152;&#26435;MLE&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#23427;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21019;&#26032;&#28041;&#21450;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges. The statistician's goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph. The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\log^2(n)$ factor, where n denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations. On the analytical front, we provide a refined $\ell_\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the $\ell_\infty$ error with the spectral properties of the weighted comparison graph. Motivated by this, our algorithmic innovation involves the development 
&lt;/p&gt;</description></item><item><title>&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05271</link><description>&lt;p&gt;
&#26799;&#24230;&#19979;&#38477;&#24341;&#21457;&#20102;&#28145;&#24230;&#38750;&#32447;&#24615;&#32593;&#32476;&#26435;&#37325;&#19982;&#32463;&#39564;NTK&#20043;&#38388;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05271
&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#21069;&#20154;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#26412;&#30740;&#31350;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#65292;&#24182;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20174;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20013;&#25552;&#21462;&#32479;&#35745;&#20449;&#24687;&#30340;&#26426;&#21046;&#26159;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#35299;&#20915;&#38382;&#39064;&#20043;&#19968;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#65292;&#22312;&#19968;&#33324;&#32467;&#26500;&#30340;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#26435;&#37325;&#30340;&#26684;&#25289;&#22982;&#30697;&#38453;&#19982;&#27169;&#22411;&#30340;&#24179;&#22343;&#26799;&#24230;&#22806;&#31215;&#25104;&#27491;&#27604;&#65292;&#36825;&#20010;&#35828;&#27861;&#34987;&#31216;&#20026;&#31070;&#32463;&#29305;&#24449;&#20998;&#26512;&#65288;NFA&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#30456;&#20851;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#31181;&#30456;&#20851;&#24615;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;NFA&#31561;&#20215;&#20110;&#26435;&#37325;&#30697;&#38453;&#30340;&#24038;&#22855;&#24322;&#32467;&#26500;&#19982;&#19982;&#36825;&#20123;&#26435;&#37325;&#30456;&#20851;&#30340;&#32463;&#39564;&#31070;&#32463;&#20999;&#32447;&#26680;&#30340;&#26174;&#33879;&#25104;&#20998;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#24341;&#20837;&#30340;NFA&#26159;&#30001;&#38548;&#31163;&#36825;&#31181;&#23545;&#40784;&#30340;&#20013;&#24515;&#21270;NFA&#39537;&#21160;&#30340;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#30340;&#26041;&#24335;&#39044;&#27979;NFA&#30340;&#21457;&#23637;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of sim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05210</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23567;&#22411;&#25110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#34917;&#20805;&#65292;&#20174;&#32780;&#24110;&#21161;&#20943;&#36731;&#33719;&#21462;&#21644;&#27880;&#37322;&#26032;&#22270;&#20687;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26102;&#38754;&#20020;&#30528;&#20840;&#23616;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#21487;&#25511;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#20013;&#36981;&#24490;&#22810;&#31867;&#35299;&#21078;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#25152;&#36873;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#20801;&#35768;&#20854;&#20182;&#35299;&#21078;&#21306;&#22495;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20063;&#25913;&#21892;&#20102;&#32593;&#32476;&#22312;&#23436;&#20840;&#26080;&#26465;&#20214;&#65288;&#26080;&#32422;&#26463;&#29983;&#25104;&#65289;&#24773;&#20917;&#19979;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;MRI&#21644;&#33145;&#37096;/&#39048;&#37096;&#21040;&#30406;&#33108;CT&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35299;&#21078;&#30495;&#23454;&#24615;&#21644;&#36755;&#20837;&#25513;&#27169;&#20445;&#30495;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.05144</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
A Bandit Approach with Evolutionary Operators for Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36827;&#21270;&#31639;&#23376;&#30340;&#24378;&#30423;&#26041;&#27861;&#26469;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#21033;&#29992;&#37096;&#20998;&#35757;&#32451;&#21644;&#20934;&#30830;&#24615;&#20316;&#20026;&#22870;&#21169;&#65292;&#26368;&#32456;&#30340;&#31639;&#27861;Mutant-UCB&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#24314;&#27169;&#20026;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#27169;&#22411;&#26159;&#33218;&#65292;&#36873;&#25321;&#19968;&#20010;&#33218;&#23545;&#24212;&#37096;&#20998;&#35757;&#32451;&#27169;&#22411;&#65288;&#36164;&#28304;&#20998;&#37197;&#65289;&#12290;&#22870;&#21169;&#26159;&#36873;&#25321;&#27169;&#22411;&#22312;&#37096;&#20998;&#35757;&#32451;&#21518;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20010;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#65292;&#36951;&#25022;&#26159;&#26368;&#20248;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#19982;&#26368;&#32456;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;UCB-E&#22312;&#38543;&#26426;&#26080;&#31351;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#30340;&#30452;&#25509;&#25512;&#24191;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#22522;&#26412;&#20551;&#35774;&#19979;&#65292;&#26399;&#26395;&#36951;&#25022;&#30340;&#39034;&#24207;&#26159;$T^{-\alpha}$&#65292;&#20854;&#20013;$\alpha \in (0,1/5)$&#65292;$T$&#26159;&#35201;&#20998;&#37197;&#30340;&#36164;&#28304;&#25968;&#37327;&#12290;&#20174;&#36825;&#20010;&#22522;&#26412;&#31639;&#27861;&#20986;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31639;&#27861;Mutant-UCB&#65292;&#23427;&#32467;&#21512;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#25805;&#20316;&#31526;&#12290;&#22312;&#19977;&#20010;&#24320;&#28304;&#22270;&#29255;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#34920;&#26126;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#32452;&#21512;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22266;&#23450;&#39044;&#31639;&#19979;&#30340;&#22269;&#38469;&#39046;&#20808;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper formulates model selection as an infinite-armed bandit problem. The models are arms, and picking an arm corresponds to a partial training of the model (resource allocation). The reward is the accuracy of the selected model after its partial training. In this best arm identification problem, regret is the gap between the expected accuracy of the optimal model and that of the model finally chosen. We first consider a straightforward generalization of UCB-E to the stochastic infinite-armed bandit problem and show that, under basic assumptions, the expected regret order is $T^{-\alpha}$ for some $\alpha \in (0,1/5)$ and $T$ the number of resources to allocate. From this vanilla algorithm, we introduce the algorithm Mutant-UCB that incorporates operators from evolutionary algorithms. Tests carried out on three open source image classification data sets attest to the relevance of this novel combining approach, which outperforms the state-of-the-art for a fixed budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.04644</link><description>&lt;p&gt;
LEVI:&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#23454;&#29616;&#21487;&#27867;&#21270;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LEVI: Generalizable Fine-tuning via Layer-wise Ensemble of Different Views
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEVI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20197;&#23618;&#20026;&#21333;&#20301;&#30340;&#22810;&#35270;&#35282;&#38598;&#25104;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#24182;&#25552;&#21319;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#22312;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21033;&#29992;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#24494;&#35843;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#24494;&#35843;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#65288;&#21363;&#65292;&#36229;&#20986;&#20998;&#24067;&#65307;OOD&#65289;&#19978;&#30340;&#27867;&#21270;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#25913;&#21892;OOB&#27867;&#21270;&#65292;&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#24182;&#35843;&#25972;&#24494;&#35843;&#20197;&#20445;&#30041;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#23398;&#20064;&#21040;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#38480;&#21046;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36807;&#24230;&#20381;&#36182;&#39044;&#35757;&#32451;&#34920;&#31034;&#21487;&#33021;&#20250;&#38459;&#30861;&#24494;&#35843;&#23398;&#20064;&#19979;&#28216;&#20219;&#21153;&#30340;&#37325;&#35201;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#20854;OOB&#27867;&#21270;&#12290;&#24403;&#26032;&#20219;&#21153;&#26469;&#33258;&#20110;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#19981;&#21516;&#30340;&#65288;&#23376;&#65289;&#39046;&#22495;&#26102;&#65292;&#36825;&#21487;&#33021;&#23588;&#20026;&#28798;&#38590;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#25968;&#25454;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning is becoming widely used for leveraging the power of pre-trained foundation models in new downstream tasks. While there are many successes of fine-tuning on various tasks, recent studies have observed challenges in the generalization of fine-tuned models to unseen distributions (i.e., out-of-distribution; OOD). To improve OOD generalization, some previous studies identify the limitations of fine-tuning data and regulate fine-tuning to preserve the general representation learned from pre-training data. However, potential limitations in the pre-training data and models are often ignored. In this paper, we contend that overly relying on the pre-trained representation may hinder fine-tuning from learning essential representations for downstream tasks and thus hurt its OOD generalization. It can be especially catastrophic when new tasks are from different (sub)domains compared to pre-training data. To address the issues in both pre-training and fine-tuning data, we propose a nove
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.04437</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Structured Entity Extraction Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;&#27492;&#20219;&#21153;&#19978;&#36890;&#36807;&#24341;&#20837;AESOP&#24230;&#37327;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20026;&#26410;&#26469;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#24433;&#21709;&#20102;&#20449;&#24687;&#25552;&#21462;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#21644;&#35268;&#33539;&#21270;&#20102;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#65288;SEE&#65289;&#20219;&#21153;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35813;&#20219;&#21153;&#30340;&#36817;&#20284;&#23454;&#20307;&#38598;&#37325;&#21472;&#65288;AESOP&#65289;&#24230;&#37327;&#65292;&#20197;&#36866;&#24403;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#25972;&#20010;&#25552;&#21462;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#38454;&#27573;&#65292;&#21033;&#29992;LLMs&#30340;&#24378;&#22823;&#21151;&#33021;&#26469;&#25552;&#39640;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#23450;&#37327;&#35780;&#20272;&#21644;&#20154;&#24037;&#24182;&#34892;&#35780;&#20272;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20026;&#32467;&#26500;&#21270;&#23454;&#20307;&#25552;&#21462;&#39046;&#22495;&#30340;&#26410;&#26469;&#36827;&#23637;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine learning have significantly impacted the field of information extraction, with Large Language Models (LLMs) playing a pivotal role in extracting structured information from unstructured text. This paper explores the challenges and limitations of current methodologies in structured entity extraction and introduces a novel approach to address these issues. We contribute to the field by first introducing and formalizing the task of Structured Entity Extraction (SEE), followed by proposing Approximate Entity Set OverlaP (AESOP) Metric designed to appropriately assess model performance on this task. Later, we propose a new model that harnesses the power of LLMs for enhanced effectiveness and efficiency through decomposing the entire extraction task into multiple stages. Quantitative evaluation and human side-by-side evaluation confirm that our model outperforms baselines, offering promising directions for future advancements in structured entity extraction.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36164;&#28304;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#24182;&#20943;&#36731;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#22238;&#31243;&#27969;&#37327;&#25317;&#22622;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04216</link><description>&lt;p&gt;
&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04216
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36164;&#28304;&#24863;&#30693;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#65292;&#24182;&#20943;&#36731;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#22238;&#31243;&#27969;&#37327;&#25317;&#22622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#35270;&#39057;&#32531;&#23384;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#23558;&#24453;&#35831;&#27714;&#20869;&#23481;&#23384;&#20648;&#22312;&#19981;&#21516;&#32423;&#21035;&#19978;&#65292;&#21487;&#20197;&#20943;&#36731;&#30001;&#23569;&#25968;&#28909;&#38376;&#25991;&#20214;&#30340;&#35270;&#39057;&#27969;&#37327;&#36896;&#25104;&#30340;&#22238;&#31243;&#25317;&#22622;&#12290;&#36890;&#24120;&#65292;&#20869;&#23481;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;CSP&#65289;&#25317;&#26377;&#20869;&#23481;&#65292;&#29992;&#25143;&#20351;&#29992;&#20854;&#65288;&#26080;&#32447;&#65289;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;ISP&#65289;&#20174;CSP&#35831;&#27714;&#20854;&#39318;&#36873;&#20869;&#23481;&#12290;&#30001;&#20110;&#36825;&#20123;&#21442;&#19982;&#26041;&#19981;&#20250;&#36879;&#38706;&#20854;&#31169;&#23494;&#20449;&#24687;&#21644;&#21830;&#19994;&#26426;&#23494;&#65292;&#20256;&#32479;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#38656;&#27714;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36164;&#28304;&#24863;&#30693;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;RawHFL&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#39044;&#27979;&#29992;&#25143;&#26410;&#26469;&#30340;&#20869;&#23481;&#35831;&#27714;&#12290;&#37319;&#29992;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#25968;&#25454;&#33719;&#21462;&#25216;&#26415;&#65292;&#20801;&#35768;&#29992;&#25143;&#26681;&#25454;&#20854;&#35831;&#27714;&#30340;&#20869;&#23481;&#26356;&#26032;&#20854;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32593;&#32476;&#21644;&#20854;&#20182;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#65292;&#32771;&#34385;&#21040;&#21482;&#26377;&#19968;&#37096;&#20998;&#29992;&#25143;&#21442;&#19982;&#27169;&#22411;&#35757;&#32451;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;
&lt;/p&gt;
&lt;p&gt;
Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25240;&#25187;&#33258;&#36866;&#24212;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#20110;&#22797;&#26434;&#30340;&#25439;&#22833;&#24207;&#21015;&#21644;&#27604;&#36739;&#22120;&#65292;&#24182;&#25913;&#36827;&#20102;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#26080;&#38656;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.02720</link><description>&lt;p&gt;
&#25240;&#25187;&#33258;&#36866;&#24212;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Discounted Adaptive Online Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25240;&#25187;&#33258;&#36866;&#24212;&#22312;&#32447;&#39044;&#27979;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36866;&#24212;&#20110;&#22797;&#26434;&#30340;&#25439;&#22833;&#24207;&#21015;&#21644;&#27604;&#36739;&#22120;&#65292;&#24182;&#25913;&#36827;&#20102;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;&#12290;&#31639;&#27861;&#20855;&#26377;&#26080;&#38656;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#19988;&#22312;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#20219;&#21153;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#24182;&#19981;&#24635;&#26159;&#35201;&#35760;&#20303;&#19968;&#20999;&#12290;&#30001;&#20110;&#26410;&#26469;&#22312;&#32479;&#35745;&#19978;&#21487;&#33021;&#19982;&#36807;&#21435;&#26377;&#24456;&#22823;&#30340;&#19981;&#21516;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#22312;&#26032;&#25968;&#25454;&#21040;&#26469;&#26102;&#20248;&#38597;&#22320;&#24536;&#35760;&#21382;&#21490;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#31181;&#30452;&#35273;&#65292;&#25105;&#20204;&#36816;&#29992;&#26368;&#36817;&#21457;&#23637;&#30340;&#33258;&#36866;&#24212;&#22312;&#32447;&#23398;&#20064;&#25216;&#26415;&#37325;&#26032;&#24605;&#32771;&#20102;&#32463;&#20856;&#30340;&#25240;&#25187;&#36951;&#25022;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#36866;&#24212;&#20110;&#25439;&#22833;&#24207;&#21015;&#21644;&#27604;&#36739;&#22120;&#30340;&#22797;&#26434;&#24615;&#65292;&#25913;&#36827;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#33258;&#36866;&#24212;&#31639;&#27861;-&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#19988;&#20855;&#26377;&#24658;&#23450;&#30340;&#23398;&#20064;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20445;&#35777;&#19981;&#38656;&#35201;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#65292;&#21482;&#35201;&#27714;&#20984;&#24615;&#65292;&#24182;&#19988;&#35813;&#31639;&#27861;&#32463;&#36807;&#35777;&#26126;&#23545;&#27425;&#20248;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#26469;&#23637;&#31034;&#36825;&#20123;&#22909;&#22788;&#65292;&#32780;&#22312;&#32447;&#31526;&#21512;&#39044;&#27979;&#26159;&#19968;&#20010;&#24102;&#26377;&#38598;&#21512;&#25104;&#21592;&#20915;&#31574;&#30340;&#19979;&#28216;&#22312;&#32447;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online learning is not always about memorizing everything. Since the future can be statistically very different from the past, a critical challenge is to gracefully forget the history while new data comes in. To formalize this intuition, we revisit the classical notion of discounted regret using recently developed techniques in adaptive online learning. Our main result is a new algorithm that adapts to the complexity of both the loss sequence and the comparator, improving the widespread non-adaptive algorithm - gradient descent with a constant learning rate. In particular, our theoretical guarantee does not require any structural assumption beyond convexity, and the algorithm is provably robust to suboptimal hyperparameter tuning. We further demonstrate such benefits through online conformal prediction, a downstream online learning task with set-membership decisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.01865</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#20250;&#24536;&#35760;&#20160;&#20040;&#65311;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20013;&#30340;&#34987;&#36951;&#24536;&#23454;&#20363;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#23558;&#27169;&#22411;&#26356;&#26032;&#20026;&#32416;&#27491;&#38169;&#35823;&#23454;&#20363;&#65292;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#22312;&#25351;&#23548;&#24494;&#35843;&#25110;&#19978;&#28216;&#35757;&#32451;&#38454;&#27573;&#20013;&#23398;&#21040;&#30340;&#23454;&#20363;&#19978;&#20986;&#29616;&#38169;&#35823;&#12290;&#38543;&#26426;&#37325;&#25773;&#19978;&#28216;&#25968;&#25454;&#30340;&#25928;&#26524;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#24448;&#24448;&#20276;&#38543;&#30528;&#36739;&#39640;&#30340;&#26041;&#24046;&#21644;&#36739;&#24046;&#30340;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#30001;&#20110;&#27169;&#22411;&#26356;&#26032;&#32780;&#36951;&#24536;&#30340;&#19978;&#28216;&#23454;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#30340;&#23454;&#20363;&#21644;&#30456;&#24212;&#34987;&#36951;&#24536;&#30340;&#19978;&#28216;&#39044;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#30340;&#21464;&#21270;&#31867;&#20284;&#20110;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00564</link><description>&lt;p&gt;
&#19968;&#27425;&#22270;&#21367;&#31215;&#23601;&#22815;&#20102;&#65306;&#39640;&#25928;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Single Graph Convolution Is All You Need: Efficient Grayscale Image Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20998;&#31867;&#22120;&#36890;&#24120;&#20381;&#36182;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23436;&#25104;&#20219;&#21153;&#65292;&#32780;CNN&#30456;&#27604;&#20110;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#26356;&#21152;&#24222;&#22823;&#65292;&#36825;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#24102;&#26469;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#36866;&#29992;&#20110;RGB&#21644;&#28784;&#24230;&#25968;&#25454;&#38598;&#65292;&#20294;&#20165;&#20165;&#20351;&#29992;&#28784;&#24230;&#22270;&#20687;&#30340;&#20998;&#31867;&#22120;&#30456;&#23545;&#36739;&#23569;&#35265;&#12290;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;(SAR)&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(ATR)&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#20687;&#30340;&#30690;&#37327;&#21270;&#35270;&#22270;&#30340;&#26032;&#22411;&#28784;&#24230;(&#21333;&#36890;&#36947;)&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22270;&#20687;&#35270;&#20026;&#30690;&#37327;&#65292;&#24182;&#23558;&#38382;&#39064;&#35774;&#32622;&#20026;&#28784;&#24230;&#22270;&#20687;&#20998;&#31867;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;MLP&#30340;&#36731;&#37327;&#32423;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25209;&#27425;&#32423;&#21035;&#20351;&#29992;&#21333;&#20010;&#22270;&#21367;&#31215;&#23618;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#24182;&#20943;&#23567;&#24615;&#33021;&#30340;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#23450;&#21046;&#30340;&#20934;&#30830;&#29575;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image classifiers often rely on convolutional neural networks (CNN) for their tasks, which are inherently more heavyweight than multilayer perceptrons (MLPs), which can be problematic in real-time applications. Additionally, many image classification models work on both RGB and grayscale datasets. Classifiers that operate solely on grayscale images are much less common. Grayscale image classification has diverse applications, including but not limited to medical image classification and synthetic aperture radar (SAR) automatic target recognition (ATR). Thus, we present a novel grayscale (single channel) image classification approach using a vectorized view of images. We exploit the lightweightness of MLPs by viewing images as a vector and reducing our problem setting to the grayscale image classification setting. We find that using a single graph convolutional layer batch-wise increases accuracy and reduces variance in the performance of our model. Moreover, we develop a customized acc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.00447</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Data-Efficient Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00447
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;&#65288;DEGL&#65289;&#30340;&#27010;&#24565;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;DEGL&#30340;&#30446;&#26631;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#22330;&#26223;&#19979;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#22312;&#31038;&#20132;&#32593;&#32476;&#21040;&#29983;&#29289;&#21270;&#23398;&#20998;&#26512;&#31561;&#39046;&#22495;&#20013;&#24191;&#27867;&#23384;&#22312;&#65292;&#26159;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24314;&#27169;&#36825;&#31181;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#21151;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#22312;&#26631;&#27880;&#36164;&#28304;&#26377;&#38480;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25506;&#32034;&#21508;&#31181;&#26368;&#23567;&#30417;&#30563;&#26041;&#27861;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#30340;&#22270;&#26426;&#22120;&#23398;&#20064;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39640;&#25928;&#22270;&#23398;&#20064;(DEGL)&#30340;&#30740;&#31350;&#21069;&#27839;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;DEGL&#24403;&#21069;&#36827;&#23637;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#20351;&#29992;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#20026;&#25105;&#20204;&#23545;DEGL&#30340;&#25506;&#32034;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20174;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#36825;&#19968;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Graph-structured data, prevalent in domains ranging from social networks to biochemical analysis, serve as the foundation for diverse real-world systems. While graph neural networks demonstrate proficiency in modeling this type of data, their success is often reliant on significant amounts of labeled data, posing a challenge in practical scenarios with limited annotation resources. To tackle this problem, tremendous efforts have been devoted to enhancing graph machine learning performance under low-resource settings by exploring various approaches to minimal supervision. In this paper, we introduce a novel concept of Data-Efficient Graph Learning (DEGL) as a research frontier, and present the first survey that summarizes the current progress of DEGL. We initiate by highlighting the challenges inherent in training models with large labeled data, paving the way for our exploration into DEGL. Next, we systematically review recent advances on this topic from several key aspects, including 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;</title><link>https://arxiv.org/abs/2312.11560</link><description>&lt;p&gt;
&#23398;&#20064;&#33258;&#21457;&#29616;&#65306;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning from Emergence: A Study on Proactively Inhibiting the Monosemantic Neurons of Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31215;&#26497;&#25233;&#21046;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#65292;&#36825;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#21457;&#29616;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#25233;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#65292;&#33258;&#21457;&#29616;&#21463;&#21040;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#29616;&#26377;&#25991;&#29486;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#30340;&#20551;&#35774;&#65292;&#21363;&#22312;&#35268;&#27169;&#25193;&#22823;&#30340;&#36807;&#31243;&#20013;&#39640;&#24230;&#20419;&#36827;&#24615;&#33021;&#30340;&#22240;&#32032;&#65306;&#20943;&#23569;&#21482;&#33021;&#19982;&#29305;&#23450;&#29305;&#24449;&#24418;&#25104;&#19968;&#23545;&#19968;&#20851;&#31995;&#30340;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24448;&#24448;&#26356;&#31232;&#30095;&#65292;&#24182;&#23545;&#22823;&#22411;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#24605;&#36335;&#26469;&#35782;&#21035;&#21644;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#26159;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#37327;&#35780;&#20272;&#25351;&#26631;&#65292;&#31616;&#21333;&#22320;&#31105;&#27490;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#24182;&#19981;&#33021;&#20419;&#36827;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#24847;&#24605;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#33258;&#21457;&#29616;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#24320;&#20102;&#20851;&#20110;&#31215;&#26497;&#25233;&#21046;&#21333;&#24847;&#20041;&#31070;&#32463;&#20803;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11560v2 Announce Type: replace-cross  Abstract: Recently, emergence has received widespread attention from the research community along with the success of large language models. Different from the literature, we hypothesize a key factor that highly promotes the performance during the increase of scale: the reduction of monosemantic neurons that can only form one-to-one correlations with specific features. Monosemantic neurons tend to be sparser and have negative impacts on the performance in large models. Inspired by this insight, we propose an intuitive idea to identify monosemantic neurons and inhibit them. However, achieving this goal is a non-trivial task as there is no unified quantitative evaluation metric and simply banning monosemantic neurons does not promote polysemanticity in neural networks. Therefore, we propose to learn from emergence and present a study on proactively inhibiting the monosemantic neurons in this paper. More specifically, we first propose a new
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.06071</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#31354;&#35270;&#39057;&#25193;&#25955;&#30340;&#38477;&#27700;&#38477;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
Precipitation Downscaling with Spatiotemporal Video Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06071
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#33267;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#21644;&#26242;&#26102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27668;&#20505;&#31185;&#23398;&#21644;&#27668;&#35937;&#23398;&#39046;&#22495;&#65292;&#39640;&#20998;&#36776;&#29575;&#30340;&#23616;&#37096;&#38477;&#27700;&#65288;&#38632;&#38634;&#65289;&#39044;&#27979;&#21463;&#21040;&#22522;&#20110;&#27169;&#25311;&#26041;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#12290;&#32479;&#35745;&#38477;&#23610;&#24230;&#65292;&#25110;&#32773;&#31216;&#20026;&#36229;&#20998;&#36776;&#29575;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#20854;&#20013;&#20302;&#20998;&#36776;&#29575;&#39044;&#27979;&#36890;&#36807;&#32479;&#35745;&#26041;&#27861;&#24471;&#21040;&#25913;&#36827;&#12290;&#19982;&#20256;&#32479;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19981;&#21516;&#65292;&#22825;&#27668;&#21644;&#27668;&#20505;&#24212;&#29992;&#38656;&#35201;&#25429;&#25417;&#32473;&#23450;&#20302;&#20998;&#36776;&#29575;&#27169;&#24335;&#30340;&#39640;&#20998;&#36776;&#29575;&#30340;&#20934;&#30830;&#26465;&#20214;&#20998;&#24067;&#65292;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#38598;&#21512;&#24179;&#22343;&#21644;&#26497;&#31471;&#20107;&#20214;&#65288;&#22914;&#26292;&#38632;&#65289;&#30340;&#26080;&#20559;&#20272;&#35745;&#12290;&#26412;&#30740;&#31350;&#23558;&#26368;&#26032;&#30340;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#38477;&#27700;&#36229;&#20998;&#36776;&#29575;&#65292;&#20351;&#29992;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#22120;&#65292;&#28982;&#21518;&#26159;&#26242;&#26102;&#26465;&#20214;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#25429;&#25417;&#22122;&#22768;&#29305;&#24449;&#21644;&#39640;&#39057;&#29575;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;FV3GFS&#36755;&#20986;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#22823;&#35268;&#27169;&#20840;&#29699;&#22823;&#27668;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06071v2 Announce Type: replace-cross  Abstract: In climate science and meteorology, high-resolution local precipitation (rain and snowfall) predictions are limited by the computational costs of simulation-based methods. Statistical downscaling, or super-resolution, is a common workaround where a low-resolution prediction is improved using statistical approaches. Unlike traditional computer vision tasks, weather and climate applications require capturing the accurate conditional distribution of high-resolution given low-resolution patterns to assure reliable ensemble averages and unbiased estimates of extreme events, such as heavy rain. This work extends recent video diffusion models to precipitation super-resolution, employing a deterministic downscaler followed by a temporally-conditioned diffusion model to capture noise characteristics and high-frequency patterns. We test our approach on FV3GFS output, an established large-scale global atmosphere model, and compare it agai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2312.04772</link><description>&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#35760;&#20303;&#20844;&#24179;&#65306;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Remembering to Be Fair: Non-Markovian Fairness in Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24207;&#36143;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#65292;&#21457;&#29616;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#21382;&#21490;&#65292;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#20915;&#31574;&#21046;&#23450;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38024;&#23545;&#21333;&#19968;&#20915;&#31574;&#36827;&#34892;&#30740;&#31350;&#30340;&#12290;&#26412;&#25991;&#22312;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21487;&#33021;&#21463;&#21040;&#20915;&#31574;&#32467;&#26524;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#20844;&#24179;&#27010;&#24565;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#24448;&#24448;&#21462;&#20915;&#20110;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#30340;&#21382;&#21490;&#65292;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35762;&#65292;&#23427;&#26159;&#22266;&#26377;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35266;&#23519;&#21040;&#65292;&#20844;&#24179;&#36890;&#24120;&#38656;&#35201;&#22312;&#36807;&#31243;&#20013;&#30340;&#26576;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#36807;&#31243;&#32467;&#26463;&#26102;&#12290;&#20026;&#20102;&#25512;&#36827;&#25105;&#20204;&#23545;&#36825;&#31867;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#29702;&#35299;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#39034;&#24207;&#20915;&#31574;&#32972;&#26223;&#19979;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#30340;&#23646;&#24615;&#65292;&#21253;&#25324;&#38271;&#26399;&#20844;&#24179;&#24615;&#12289;&#20219;&#24847;&#26102;&#21051;&#20844;&#24179;&#24615;&#12289;&#21608;&#26399;&#24615;&#20844;&#24179;&#24615;&#21644;&#26377;&#30028;&#20844;&#24179;&#24615;&#31561;&#27010;&#24565;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#38750;&#39532;&#23572;&#21487;&#22827;&#20844;&#24179;&#24615;&#21644;&#35760;&#24518;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#36825;&#22914;&#20309;&#25903;&#25345;&#21046;&#23450;&#20844;&#24179;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04772v3 Announce Type: replace  Abstract: Fair decision making has largely been studied with respect to a single decision. In this paper we investigate the notion of fairness in the context of sequential decision making where multiple stakeholders can be affected by the outcomes of decisions. We observe that fairness often depends on the history of the sequential decision-making process, and in this sense that it is inherently non-Markovian. We further observe that fairness often needs to be assessed at time points within the process, not just at the end of the process. To advance our understanding of this class of fairness problems, we explore the notion of non-Markovian fairness in the context of sequential decision making. We identify properties of non-Markovian fairness, including notions of long-term, anytime, periodic, and bounded fairness. We further explore the interplay between non-Markovian fairness and memory, and how this can support construction of fair policies
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#21487;&#33021;&#25552;&#39640;&#20043;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;</title><link>https://arxiv.org/abs/2312.02592</link><description>&lt;p&gt;
FRAPP'E&#65306;&#19968;&#20010;&#29992;&#20110;&#21518;&#22788;&#29702;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FRAPP\'E: A Group Fairness Framework for Post-Processing Everything
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02592
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#20445;&#25345;&#20102;&#33391;&#22909;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#21487;&#33021;&#25552;&#39640;&#20043;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#22788;&#29702;&#20013;&#23454;&#29616;&#20102;&#26377;&#21069;&#36884;&#30340;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#20294;&#32676;&#20307;&#20844;&#24179;&#24615;&#30340;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#22312;&#35768;&#22810;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#25110;&#26080;&#27861;&#35775;&#38382;&#39044;&#27979;&#27169;&#22411;&#35757;&#32451;&#31649;&#36947;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#21518;&#22788;&#29702;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#19987;&#20026;&#29305;&#23450;&#38382;&#39064;&#35774;&#32622;&#21644;&#20844;&#24179;&#24615;&#23450;&#20041;&#32780;&#35774;&#35745;&#65292;&#22240;&#27492;&#19981;&#22914;&#22788;&#29702;&#20013;&#26041;&#27861;&#24191;&#27867;&#36866;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20219;&#20309;&#27491;&#21017;&#21270;&#30340;&#22788;&#29702;&#20013;&#26041;&#27861;&#36716;&#21270;&#20026;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#33719;&#24471;&#26356;&#24191;&#27867;&#38382;&#39064;&#35774;&#32622;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#36884;&#24452;&#65292;&#36828;&#36229;&#36807;&#20197;&#21069;&#30340;&#21518;&#22788;&#29702;&#25991;&#29486;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#30041;&#20102;&#22788;&#29702;&#20013;&#23454;&#29616;&#30340;&#33391;&#22909;&#20844;&#24179;&#38169;&#35823;&#26435;&#34913;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#39640;&#20808;&#21069;&#26041;&#27861;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02592v2 Announce Type: replace  Abstract: Despite achieving promising fairness-error trade-offs, in-processing mitigation techniques for group fairness cannot be employed in numerous practical applications with limited computation resources or no access to the training pipeline of the prediction model. In these situations, post-processing is a viable alternative. However, current methods are tailored to specific problem settings and fairness definitions and hence, are not as broadly applicable as in-processing. In this work, we propose a framework that turns any regularized in-processing method into a post-processing approach. This procedure prescribes a way to obtain post-processing techniques for a much broader range of problem settings than the prior post-processing literature. We show theoretically and through extensive experiments that our framework preserves the good fairness-error trade-offs achieved with in-processing and can improve over the effectiveness of prior p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20173;&#28982;&#21463;&#30410;&#12290;</title><link>https://arxiv.org/abs/2311.17539</link><description>&lt;p&gt;
&#22312;&#36807;&#21442;&#25968;&#21270;&#19979;&#20998;&#26512;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Analyzing Sharpness-aware Minimization under Overparameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20173;&#28982;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#23613;&#31649;&#35757;&#32451;&#25439;&#22833;&#30456;&#21516;&#65292;&#20294;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#19981;&#21516;&#27867;&#21270;&#33021;&#21147;&#30340;&#26497;&#23567;&#20540;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26497;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#20854;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#24050;&#32463;&#20570;&#20986;&#20102;&#26356;&#22810;&#21162;&#21147;&#24320;&#21457;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#26174;&#24335;&#22320;&#25214;&#21040;&#25153;&#24179;&#26497;&#23567;&#20540;&#20316;&#20026;&#26356;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31574;&#30053;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#36807;&#21442;&#25968;&#21270;&#19979;&#30340;SAM&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#36807;&#21442;&#25968;&#21270;&#23545;SAM&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#24182;&#34920;&#26126;&#23384;&#22312;&#19968;&#31181;&#19968;&#33268;&#30340;&#36235;&#21183;&#65292;&#21363;SAM&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21463;&#30410;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#20196;&#20154;&#20449;&#26381;&#30340;&#26696;&#20363;&#65292;&#35828;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an overparameterized neural network can yield minimizers of different generalization capabilities despite the same level of training loss. With evidence that suggests a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. However, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to whether and how it is affected by overparameterization.   In this work, we analyze SAM under overparameterization of varying degrees and present both empirical and theoretical results that indicate a critical influence of overparameterization on SAM. Specifically, we conduct extensive numerical experiments across various domains, and show that there exists a consistent trend that SAM continues to benefit from increasing overparameterization. We also discover compelling cases where the effect of overparameterization is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;</title><link>https://arxiv.org/abs/2311.10224</link><description>&lt;p&gt;
CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images
&lt;/p&gt;
&lt;p&gt;
CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular Segmentation of Enhanced TOF-MRA Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#21644;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#35786;&#26029;&#33041;&#34880;&#31649;&#30142;&#30149;&#65292;&#26102;&#38388;&#39134;&#34892;&#30913;&#20849;&#25391;&#34880;&#31649;&#36896;&#24433;&#22270;&#65288;TOF-MRA&#65289;&#36890;&#24120;&#26159;&#36890;&#36807;&#30446;&#35270;&#35780;&#20272;&#65292;&#36825;&#23548;&#33268;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#33041;&#34880;&#31649;&#27880;&#24847;&#21147;UNet&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CV-Attention UNet&#65292;&#29992;&#20110;&#31934;&#30830;&#25552;&#21462;&#33041;&#34880;&#31649;&#22270;&#20687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#39044;&#22788;&#29702;&#25216;&#26415;&#65292;&#38543;&#21518;&#37319;&#29992;&#28145;&#24230;&#30417;&#30563;UNet&#26469;&#25913;&#21892;&#33041;&#34880;&#31649;&#20998;&#21106;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#39044;&#38450;&#20013;&#39118;&#12290;&#20026;&#20102;&#32467;&#21512;&#20302;&#35821;&#20041;&#21644;&#39640;&#35821;&#20041;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10224v2 Announce Type: replace-cross  Abstract: Due to the lack of automated methods, to diagnose cerebrovascular disease, time-of-flight magnetic resonance angiography (TOF-MRA) is assessed visually, making it time-consuming. The commonly used encoder-decoder architectures for cerebrovascular segmentation utilize redundant features, eventually leading to the extraction of low-level features multiple times. Additionally, convolutional neural networks (CNNs) suffer from performance degradation when the batch size is small, and deeper networks experience the vanishing gradient problem. Methods: In this paper, we attempt to solve these limitations and propose the 3D cerebrovascular attention UNet method, named CV-AttentionUNet, for precise extraction of brain vessel images. We proposed a sequence of preprocessing techniques followed by deeply supervised UNet to improve the accuracy of segmentation of the brain vessels leading to a stroke. To combine the low and high semantics, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;The Attention Patch&#65288;TAP&#65289;&#31070;&#32463;&#32593;&#32476;&#38468;&#21152;&#32452;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#26410;&#26631;&#35760;&#30340;&#27425;&#35201;&#27169;&#24577;&#23454;&#29616;&#36328;&#27169;&#24577;&#30340;&#25968;&#25454;&#32423;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>https://arxiv.org/abs/2302.02224</link><description>&lt;p&gt;
TAP: &#36328;&#27169;&#24577;&#30693;&#35782;&#20256;&#36882;&#20013;&#30340;&#27880;&#24847;&#21147;&#34917;&#19969;
&lt;/p&gt;
&lt;p&gt;
TAP: The Attention Patch for Cross-Modal Knowledge Transfer from Unlabeled Modality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;The Attention Patch&#65288;TAP&#65289;&#31070;&#32463;&#32593;&#32476;&#38468;&#21152;&#32452;&#20214;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#26410;&#26631;&#35760;&#30340;&#27425;&#35201;&#27169;&#24577;&#23454;&#29616;&#36328;&#27169;&#24577;&#30340;&#25968;&#25454;&#32423;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#26410;&#26631;&#35760;&#12289;&#19981;&#37197;&#23545;&#30340;&#27425;&#35201;&#27169;&#24577;&#65292;&#22686;&#24378;&#20027;&#35201;&#27169;&#24577;&#20013;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#37319;&#29992;&#27010;&#29575;&#26041;&#27861;&#36827;&#34892;&#32570;&#22833;&#20449;&#24687;&#20272;&#35745;&#65292;&#25105;&#20204;&#34920;&#26126;&#27425;&#35201;&#27169;&#24577;&#20013;&#21253;&#21547;&#30340;&#39069;&#22806;&#20449;&#24687;&#21487;&#20197;&#36890;&#36807;Nadaraya-Watson&#65288;NW&#65289;&#26680;&#22238;&#24402;&#36827;&#34892;&#20272;&#35745;&#65292;&#20854;&#21487;&#20197;&#36827;&#19968;&#27493;&#34920;&#31034;&#20026;&#32463;&#36807;&#32447;&#24615;&#21464;&#25442;&#30340;&#26680;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#24341;&#20837;The Attention Patch&#65288;TAP&#65289;&#22880;&#23450;&#20102;&#22522;&#30784;&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#38468;&#21152;&#32452;&#20214;&#65292;&#20801;&#35768;&#20174;&#26410;&#26631;&#35760;&#30340;&#27169;&#24577;&#36827;&#34892;&#25968;&#25454;&#32423;&#30693;&#35782;&#20256;&#36882;&#12290;&#25105;&#20204;&#20351;&#29992;&#22235;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#20540;&#27169;&#25311;&#65292;&#32467;&#26524;&#34920;&#26126;TAP&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36328;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21033;&#29992;&#30475;&#20284;&#26080;&#29992;&#30340;&#26410;&#26631;&#35760;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.02224v2 Announce Type: replace  Abstract: This paper addresses a cross-modal learning framework, where the objective is to enhance the performance of supervised learning in the primary modality using an unlabeled, unpaired secondary modality. Taking a probabilistic approach for missing information estimation, we show that the extra information contained in the secondary modality can be estimated via Nadaraya-Watson (NW) kernel regression, which can further be expressed as a kernelized cross-attention module (under linear transformation). Our results lay the foundations for introducing The Attention Patch (TAP), a simple neural network add-on that allows data-level knowledge transfer from the unlabeled modality. We provide extensive numerical simulations using four real-world datasets to show that TAP can provide statistically significant improvement in generalization across different domains and different neural network architectures, making use of seemingly unusable unlabel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#27979;&#35797;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29420;&#31435;&#39564;&#35777;&#20854;&#40657;&#30418;&#29305;&#24615;&#21644;&#38543;&#26426;&#23646;&#24615;&#12290;&#24314;&#35758;&#25193;&#23637;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;ML&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.17062</link><description>&lt;p&gt;
&#19968;&#31181;&#29420;&#31435;&#30340;&#31995;&#32479;&#21270;&#40657;&#30418;&#27979;&#35797;ML&#31995;&#32479;&#30340;&#27010;&#35201;
&lt;/p&gt;
&lt;p&gt;
Outline of an Independent Systematic Blackbox Test for ML-based Systems. (arXiv:2401.17062v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#31435;&#27979;&#35797;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29420;&#31435;&#39564;&#35777;&#20854;&#40657;&#30418;&#29305;&#24615;&#21644;&#38543;&#26426;&#23646;&#24615;&#12290;&#24314;&#35758;&#25193;&#23637;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;ML&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#31243;&#24207;&#65292;&#21487;&#29992;&#20110;&#29420;&#31435;&#20110;&#23454;&#38469;&#35757;&#32451;&#36807;&#31243;&#27979;&#35797;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#12290;&#36890;&#36807;&#32771;&#34385;&#20854;&#40657;&#30418;&#29305;&#24615;&#21644;ML&#27169;&#22411;&#21450;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#22266;&#26377;&#38543;&#26426;&#23646;&#24615;&#65292;&#21487;&#20197;&#29420;&#31435;&#39564;&#35777;&#36825;&#20123;&#27169;&#22411;&#21644;&#31995;&#32479;&#30340;&#20856;&#22411;&#36136;&#37327;&#21442;&#25968;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#31934;&#24230;&#12290;&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#23454;&#39564;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#29616;&#26377;&#27979;&#35797;&#26041;&#27861;&#20197;&#21453;&#26144;ML&#27169;&#22411;&#21644;&#22522;&#20110;ML&#30340;&#31995;&#32479;&#30340;&#38543;&#26426;&#29305;&#24615;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article proposes a test procedure that can be used to test ML models and ML-based systems independently of the actual training process. In this way, the typical quality statements such as accuracy and precision of these models and system can be verified independently, taking into account their black box character and the immanent stochastic properties of ML models and their training data. The article presents first results from a set of test experiments and suggest extensions to existing test methods reflecting the stochastic nature of ML models and ML-based systems.
&lt;/p&gt;</description></item><item><title>M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2401.17032</link><description>&lt;p&gt;
M2CURL: &#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#23454;&#29616;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
M2CURL: Sample-Efficient Multimodal Reinforcement Learning via Self-Supervised Representation Learning for Robotic Manipulation. (arXiv:2401.17032v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17032
&lt;/p&gt;
&lt;p&gt;
M2CURL&#26159;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20174;&#35270;&#35302;&#35273;&#25968;&#25454;&#20013;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#65292;&#24182;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#26377;&#25928;&#22320;&#25972;&#21512;&#19981;&#21516;&#30340;&#35266;&#27979;&#27169;&#24577;&#12290;&#20174;&#36825;&#20123;&#27169;&#24577;&#20013;&#24471;&#21040;&#31283;&#20581;&#20934;&#30830;&#30340;&#34920;&#31034;&#23545;&#20110;&#25552;&#21319;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35302;&#35273;&#25968;&#25454;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#34920;&#31034;&#38754;&#20020;&#30528;&#37325;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#39640;&#32500;&#24230;&#21644;&#23558;&#35270;&#35302;&#35273;&#36755;&#20837;&#19982;&#21160;&#24577;&#29615;&#22659;&#21644;&#20219;&#21153;&#30446;&#26631;&#36827;&#34892;&#30456;&#20851;&#24615;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#23545;&#27604;&#26080;&#30417;&#30563;&#24378;&#21270;&#23398;&#20064;&#65288;M2CURL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#23398;&#20064;&#20986;&#39640;&#25928;&#30340;&#34920;&#31034;&#24182;&#21152;&#36895;&#20102;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26080;&#20851;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#21487;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;&#25105;&#20204;&#22312;Tactile Gym 2&#27169;&#25311;&#22120;&#19978;&#35780;&#20272;&#20102;M2CURL&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16755</link><description>&lt;p&gt;
&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion model for relational inference. (arXiv:2401.16755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#21253;&#25324;&#22823;&#33041;&#27963;&#21160;&#12289;&#37329;&#34701;&#20215;&#26684;&#27874;&#21160;&#21644;&#29289;&#29702;&#38598;&#20307;&#29616;&#35937;&#65292;&#19982;&#31995;&#32479;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#12290;&#21033;&#29992;&#21487;&#35266;&#27979;&#30340;&#21160;&#24577;&#26469;&#21457;&#29616;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#20851;&#31995;&#34987;&#31216;&#20026;&#20851;&#31995;&#25512;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#23427;&#20511;&#37492;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#27861;&#12290;DiffRI&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#27169;&#25311;&#21644;&#20934;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffRI&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#26609;&#38754;&#20195;&#25968;&#20998;&#35299;&#20013;&#30340;&#21464;&#37327;&#25490;&#24207;&#36873;&#25321;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13343</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#38598;&#21644;&#33539;&#24335;&#30340;&#25945;&#35757;&#65306;&#22522;&#20110;CAD&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lessons on Datasets and Paradigms in Machine Learning for Symbolic Computation: A Case Study on CAD. (arXiv:2401.13343v1 [cs.SC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13343
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#24212;&#29992;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#30340;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#26609;&#38754;&#20195;&#25968;&#20998;&#35299;&#20013;&#30340;&#21464;&#37327;&#25490;&#24207;&#36873;&#25321;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#25216;&#26415;&#26469;&#25913;&#21892;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#35745;&#31639;&#31639;&#27861;&#21450;&#20854;&#22312;&#35745;&#31639;&#26426;&#20195;&#25968;&#31995;&#32479;&#20013;&#30340;&#23454;&#29616;&#36890;&#24120;&#21253;&#21547;&#19968;&#20123;&#36873;&#25321;&#65292;&#36825;&#20123;&#36873;&#25321;&#19981;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#65292;&#20294;&#23545;&#25152;&#38656;&#36164;&#28304;&#26377;&#26174;&#33879;&#24433;&#21709;&#65306;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#38024;&#23545;&#27599;&#20010;&#38382;&#39064;&#21333;&#29420;&#20570;&#20986;&#36825;&#20123;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#25253;&#21578;&#20102;&#22312;&#31526;&#21495;&#35745;&#31639;&#20013;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#29305;&#21035;&#26159;&#22312;&#26426;&#22120;&#23398;&#20064;&#20043;&#21069;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#20351;&#29992;&#30340;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#19979;&#38754;&#20197;&#26609;&#38754;&#20195;&#25968;&#20998;&#35299;&#20013;&#30340;&#21464;&#37327;&#25490;&#24207;&#36873;&#25321;&#20316;&#20026;&#19968;&#20010;&#20855;&#20307;&#26696;&#20363;&#30740;&#31350;&#30340;&#32467;&#26524;&#26469;&#23637;&#31034;&#65292;&#20294;&#35748;&#20026;&#25152;&#23398;&#21040;&#30340;&#25945;&#35757;&#36866;&#29992;&#20110;&#31526;&#21495;&#35745;&#31639;&#20013;&#30340;&#20854;&#20182;&#20915;&#31574;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#29616;&#26377;&#30340;&#20174;&#24212;&#29992;&#20013;&#23548;&#20986;&#30340;&#31034;&#20363;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#35813;&#25968;&#25454;&#38598;&#22312;&#21464;&#37327;&#25490;&#24207;&#20915;&#31574;&#26041;&#38754;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#31995;&#32479;&#38382;&#39064;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#20351;&#24471;&#27599;&#20010;&#38382;&#39064;&#21487;&#20197;&#26377;&#22810;&#20010;&#31034;&#20363;&#20197;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic Computation algorithms and their implementation in computer algebra systems often contain choices which do not affect the correctness of the output but can significantly impact the resources required: such choices can benefit from having them made separately for each problem via a machine learning model. This study reports lessons on such use of machine learning in symbolic computation, in particular on the importance of analysing datasets prior to machine learning and on the different machine learning paradigms that may be utilised. We present results for a particular case study, the selection of variable ordering for cylindrical algebraic decomposition, but expect that the lessons learned are applicable to other decisions in symbolic computation.  We utilise an existing dataset of examples derived from applications which was found to be imbalanced with respect to the variable ordering decision. We introduce an augmentation technique for polynomial systems problems that allow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#24182;&#39044;&#27979;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32467;&#26500;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#20026;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08627</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#21644;&#35299;&#37322;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;
&lt;/p&gt;
&lt;p&gt;
Predicting and Interpreting Energy Barriers of Metallic Glasses with Graph Neural Networks. (arXiv:2401.08627v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#24182;&#39044;&#27979;&#37329;&#23646;&#29627;&#29827;&#30340;&#33021;&#22418;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#32467;&#26500;&#27491;&#20132;&#21464;&#25442;&#19979;&#30340;&#19981;&#21464;&#24615;&#65292;&#20026;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20851;&#31995;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#23646;&#29627;&#29827;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#26080;&#24207;&#26448;&#26009;&#12290;&#29702;&#35299;&#37329;&#23646;&#29627;&#29827;&#30340;&#23616;&#37096;&#32467;&#26500;&#19982;&#29289;&#29702;&#24615;&#36136;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#26448;&#26009;&#31185;&#23398;&#21644;&#20957;&#32858;&#24577;&#29289;&#29702;&#23398;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#24314;&#27169;&#21407;&#23376;&#22270;&#32467;&#26500;&#65292;&#24182;&#30740;&#31350;&#32467;&#26500;&#19982;&#30456;&#24212;&#30340;&#23616;&#37096;&#33021;&#22418;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25454;&#20449;&#33021;&#22815;&#25511;&#21046;&#37329;&#23646;&#29627;&#29827;&#30340;&#35768;&#22810;&#20851;&#38190;&#29289;&#29702;&#24615;&#36136;&#12290;&#25105;&#20204;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#33021;&#22418;&#30340;&#26032;&#22411;&#23545;&#31216;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SymGNN&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#32467;&#26500;&#30340;&#27491;&#20132;&#21464;&#25442;&#19979;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#20363;&#22914;&#26059;&#36716;&#21644;&#21453;&#23556;&#12290;&#36825;&#31181;&#19981;&#21464;&#24615;&#26159;&#26631;&#20934;&#22270;&#31070;&#32463;&#32593;&#32476;&#22914;&#22270;&#21367;&#31215;&#32593;&#32476;&#26080;&#27861;&#25429;&#25417;&#30340;&#24615;&#36136;&#12290;SymGNN&#36890;&#36807;&#32858;&#21512;&#22270;&#32467;&#26500;&#30340;&#27491;&#20132;&#21464;&#25442;&#26469;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#23545;&#25152;&#26377;&#19977;&#32500;&#27491;&#20132;&#21464;&#25442;&#36827;&#34892;&#26368;&#20248;&#20998;&#24067;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metallic Glasses (MGs) are widely used disordered materials. Understanding the relationship between the local structure and physical properties of MGs is one of the greatest challenges for both material science and condensed matter physics. In this work, we utilize Graph Neural Networks (GNNs) to model the atomic graph structure and study the connection between the structure and the corresponding local energy barrier, which is believed to govern many critical physical properties in MGs. One of our key contributions is to propose a novel Symmetrized GNN (SymGNN) model for predicting the energy barriers, which is invariant under orthogonal transformations of the structure, e.g., rotations and reflections. Such invariance is a desired property that standard GNNs like Graph Convolutional Networks cannot capture. SymGNNs handle the invariance by aggregating over orthogonal transformations of the graph structure for representation learning, and an optimal distribution over all 3D orthogonal 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03069</link><description>&lt;p&gt;
&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36890;&#36807;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#30830;&#23450;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#65292;&#36825;&#33021;&#22815;&#35299;&#20915;&#30446;&#21069;&#30740;&#31350;&#20013;&#24573;&#35270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19982;&#20256;&#32479;&#36719;&#20214;&#31995;&#32479;&#19968;&#26679;&#65292;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#20063;&#23384;&#22312;Bug&#65292;&#36825;&#21487;&#33021;&#23545;&#33258;&#21160;&#39550;&#39542;&#31561;&#39046;&#22495;&#20135;&#29983;&#20005;&#37325;&#24433;&#21709;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;Bug&#30340;&#35299;&#20915;&#12290;&#29616;&#26377;&#25991;&#29486;&#25351;&#20986;&#65292;&#20165;&#26377;3%&#30340;&#28145;&#24230;&#23398;&#20064;Bug&#26159;&#21487;&#22797;&#29616;&#30340;&#65292;&#36825;&#20984;&#26174;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#32771;&#23519;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#21487;&#22797;&#29616;&#24615;&#65292;&#35782;&#21035;&#21487;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;Bug&#21487;&#22797;&#29616;&#24615;&#30340;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Stack Overflow&#21644;Defects4ML&#30340;3&#20010;&#26694;&#26550;&#21644;22&#20010;&#26550;&#26500;&#30340;668&#20010;&#28145;&#24230;&#23398;&#20064;Bug&#30340;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#20998;&#23618;&#25277;&#26679;&#36873;&#25321;&#20102;102&#20010;Bug&#65292;&#24182;&#23581;&#35797;&#30830;&#23450;&#23427;&#20204;&#30340;&#21487;&#22797;&#29616;&#24615;&#12290;&#22312;&#22797;&#29616;&#36825;&#20123;Bug&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#32534;&#36753;&#21160;&#20316;&#21644;&#26377;&#29992;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Deep learning has achieved remarkable progress in various domains. However, like traditional software systems, deep learning systems contain bugs, which can have severe impacts, as evidenced by crashes involving autonomous vehicles. Despite substantial advancements in deep learning techniques, little research has focused on reproducing deep learning bugs, which hinders resolving them. Existing literature suggests that only 3% of deep learning bugs are reproducible, underscoring the need for further research.  Objective: This paper examines the reproducibility of deep learning bugs. We identify edit actions and useful information that could improve deep learning bug reproducibility.  Method: First, we construct a dataset of 668 deep learning bugs from Stack Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we select 102 bugs using stratified sampling and try to determine their reproducibility. While reproducing these bugs, we identify edit actions and us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2312.02277</link><description>&lt;p&gt;
ALEXR:&#19968;&#31181;&#29992;&#20110;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#30340;&#26368;&#20248;&#21333;&#24490;&#29615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31867;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#30340;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#65288;cFCCO&#65289;&#38382;&#39064;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#24490;&#29615;&#21407;&#22987;-&#23545;&#20598;&#22359;&#22352;&#26631;&#36817;&#31471;&#31639;&#27861;&#65292;&#31216;&#20026;ALEXR&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#22359;&#22352;&#26631;&#38543;&#26426;&#38236;&#20687;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#21644;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#12290;&#25105;&#20204;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;ALEXR&#22312;&#20984;&#21644;&#24378;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#19981;&#20165;&#25913;&#36827;&#20102;&#20197;&#21069;&#22312;&#24179;&#28369;cFCCO&#38382;&#39064;&#19978;&#30340;&#26368;&#20339;&#36895;&#24230;&#65292;&#36824;&#25193;&#23637;&#20102;cFCCO&#30340;&#33539;&#22260;&#65292;&#29992;&#20110;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38750;&#24179;&#28369;&#38382;&#39064;&#65292;&#22914;GDRO&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#19979;&#30028;&#65292;&#20197;&#35777;&#26126;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits a class of convex Finite-Sum Coupled Compositional Stochastic Optimization (cFCCO) problems with many applications, including group distributionally robust optimization (GDRO), learning with imbalanced data, reinforcement learning, and learning to rank. To better solve these problems, we introduce an efficient single-loop primal-dual block-coordinate proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror ascent updates for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we present lower complexity bounds to demonstrate that the con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33529;&#26524;&#21697;&#23581;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#20174;&#32452;&#21512;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;Effective width&#21442;&#25968;&#65292;&#32039;&#23494;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#65292;&#24182;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#30340;&#19977;&#20998;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.19064</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33529;&#26524;&#21697;&#23581;&#30340;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Learnability of Apple Tasting. (arXiv:2310.19064v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19064
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33529;&#26524;&#21697;&#23581;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#20174;&#32452;&#21512;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#20316;&#32773;&#36890;&#36807;&#24341;&#20837;Effective width&#21442;&#25968;&#65292;&#32039;&#23494;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#65292;&#24182;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#30340;&#19977;&#20998;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#23398;&#20064;&#32773;&#21482;&#26377;&#22312;&#39044;&#27979;&#20026;"1"&#26102;&#35266;&#23519;&#21040;&#30495;&#23454;&#26631;&#31614;&#12290;&#26412;&#25991;&#37325;&#26032;&#30740;&#31350;&#20102;&#36825;&#31181;&#32463;&#20856;&#30340;&#37096;&#20998;&#21453;&#39304;&#35774;&#32622;&#65292;&#24182;&#20174;&#32452;&#21512;&#35282;&#24230;&#30740;&#31350;&#20102;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#19981;&#21487;&#30693;&#35774;&#32622;&#19979;&#65292;Littlestone&#32500;&#24230;&#20173;&#28982;&#26159;&#33529;&#26524;&#21697;&#23581;&#30340;&#32039;&#23494;&#23450;&#37327;&#21051;&#30011;&#65292;&#35299;&#20915;&#20102;\cite{helmbold2000apple}&#25552;&#20986;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#21442;&#25968;&#65292;&#31216;&#20026;&#26377;&#25928;&#23485;&#24230;&#65292;&#32039;&#23494;&#37327;&#21270;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#12290;&#20316;&#20026;&#25512;&#35770;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#25928;&#23485;&#24230;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#26497;&#23567;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#30340;&#19977;&#20998;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#65292;&#20219;&#20309;&#23398;&#20064;&#32773;&#22312;&#33529;&#26524;&#21697;&#23581;&#21453;&#39304;&#19979;&#30340;&#26399;&#26395;&#38169;&#35823;&#25968;&#37327;&#21482;&#33021;&#26159;$\Theta(1), \Theta(\sqrt{T})$, &#25110; $\Theta(T)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In online binary classification under \textit{apple tasting} feedback, the learner only observes the true label if it predicts "1". First studied by \cite{helmbold2000apple}, we revisit this classical partial-feedback setting and study online learnability from a combinatorial perspective. We show that the Littlestone dimension continues to prove a tight quantitative characterization of apple tasting in the agnostic setting, closing an open question posed by \cite{helmbold2000apple}. In addition, we give a new combinatorial parameter, called the Effective width, that tightly quantifies the minimax expected mistakes in the realizable setting. As a corollary, we use the Effective width to establish a \textit{trichotomy} of the minimax expected number of mistakes in the realizable setting. In particular, we show that in the realizable setting, the expected number of mistakes for any learner under apple tasting feedback can only be $\Theta(1), \Theta(\sqrt{T})$, or $\Theta(T)$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2310.17467</link><description>&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#30340;&#32479;&#35745;&#28909;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#20013;&#24212;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20108;&#38454;&#30456;&#21464;&#29616;&#35937;&#65292;&#24182;&#19988;&#35748;&#20026;&#36825;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#30340;&#22522;&#26412;&#24605;&#24819;&#26469;&#33258;&#38750;&#24179;&#34913;&#29289;&#29702;&#23398;&#65292;&#20294;&#26412;&#25991;&#20013;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#29992;&#24179;&#34913;&#32479;&#35745;&#21147;&#23398;&#30340;&#24037;&#20855;&#26469;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#21033;&#29992;&#36825;&#31181;&#37325;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29983;&#25104;&#24615;&#25193;&#25955;&#27169;&#22411;&#32463;&#21382;&#20102;&#19982;&#23545;&#31216;&#24615;&#30772;&#32570;&#29616;&#35937;&#30456;&#23545;&#24212;&#30340;&#20108;&#38454;&#30456;&#21464;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#31283;&#23450;&#24615;&#24418;&#24335;&#65292;&#23427;&#26159;&#29983;&#25104;&#33021;&#21147;&#30340;&#26680;&#24515;&#65292;&#24182;&#21487;&#20197;&#29992;&#19968;&#32452;&#24179;&#22343;&#22330;&#20020;&#30028;&#25351;&#25968;&#26469;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#28909;&#21147;&#23398;&#30340;&#20844;&#24335;&#20998;&#26512;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#20851;&#32852;&#35760;&#24518;&#32593;&#32476;&#36830;&#25509;&#30340;&#26368;&#36817;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.16113</link><description>&lt;p&gt;
&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compressed representation of brain genetic transcription. (arXiv:2310.16113v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#33041;&#22522;&#22240;&#36716;&#24405;&#30340;&#21387;&#32553;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#37325;&#24314;&#12289;&#35299;&#21078;&#21644;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#30340;&#32467;&#26500;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#30452;&#35266;&#22320;&#36827;&#34892;&#35266;&#23519;&#65292;&#38656;&#35201;&#20351;&#29992;&#21387;&#32553;&#34920;&#31034;&#23558;&#20854;&#21464;&#21270;&#25237;&#24433;&#21040;&#32039;&#20945;&#12289;&#21487;&#23548;&#33322;&#30340;&#31354;&#38388;&#20013;&#12290;&#22312;&#39640;&#32500;&#25968;&#25454;&#65288;&#22914;&#22522;&#22240;&#34920;&#36798;&#65289;&#20013;&#65292;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;&#35299;&#21078;&#21644;&#36716;&#24405;&#27169;&#24335;&#30340;&#32852;&#21512;&#22797;&#26434;&#24615;&#35201;&#27714;&#26368;&#22823;&#21387;&#32553;&#12290;&#30446;&#21069;&#30340;&#23454;&#36341;&#26159;&#20351;&#29992;&#26631;&#20934;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#65292;&#20854;&#35745;&#31639;&#25928;&#29575;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#22312;&#22823;&#21387;&#32553;&#27604;&#19979;&#34920;&#29616;&#21147;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20840;&#33041;&#20307;&#32032;&#32423;Allen&#22823;&#33041;&#22270;&#35889;&#36716;&#24405;&#25968;&#25454;&#65292;&#31995;&#32479;&#27604;&#36739;&#20102;&#22522;&#20110;&#26368;&#24191;&#27867;&#25903;&#25345;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#26041;&#27861;&#65288;PCA&#65292;&#26680;PCA&#65292;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#65292;t-&#38543;&#26426;&#37051;&#23621;&#23884;&#20837;&#65288;t-SNE&#65289;&#65292;&#32479;&#19968;&#27969;&#24418;&#36924;&#36817;&#21644;&#25237;&#24433;&#65288;UMAP&#65289;&#65292;&#28145;&#24230;&#33258;&#32534;&#30721;&#65289;&#30340;&#21387;&#32553;&#34920;&#31034;&#65292;&#37327;&#21270;&#37325;&#24314;&#20445;&#30495;&#24230;&#65292;&#35299;&#21078;&#36830;&#36143;&#24615;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The architecture of the brain is too complex to be intuitively surveyable without the use of compressed representations that project its variation into a compact, navigable space. The task is especially challenging with high-dimensional data, such as gene expression, where the joint complexity of anatomical and transcriptional patterns demands maximum compression. Established practice is to use standard principal component analysis (PCA), whose computational felicity is offset by limited expressivity, especially at great compression ratios. Employing whole-brain, voxel-wise Allen Brain Atlas transcription data, here we systematically compare compressed representations based on the most widely supported linear and non-linear methods-PCA, kernel PCA, non-negative matrix factorization (NMF), t-stochastic neighbour embedding (t-SNE), uniform manifold approximation and projection (UMAP), and deep auto-encoding-quantifying reconstruction fidelity, anatomical coherence, and predictive utility
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#25512;&#24191;&#20102;&#20043;&#21069;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#19979;&#23384;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.15903</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Collapse in Multi-label Learning with Pick-all-label Loss. (arXiv:2310.15903v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15903
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#25512;&#24191;&#20102;&#20043;&#21069;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#21457;&#29616;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#65292;&#35777;&#26126;&#20102;&#22312;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#19979;&#23384;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#29616;&#35937;&#12290;&#20182;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#24191;&#20041;&#30340;&#31070;&#32463;&#22349;&#32553;&#20013;&#30340;&#19968;&#20010;&#32452;&#21512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#22349;&#32553;&#65288;NC&#65289;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#65288;MLab&#65289;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#23616;&#38480;&#20110;&#22810;&#31867;&#21035;&#20998;&#31867;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26222;&#36941;&#23384;&#22312;&#30340;NC&#29616;&#35937;&#65292;&#20854;&#20013;&#26368;&#21518;&#19968;&#23618;&#29305;&#24449;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#27599;&#20010;&#31867;&#21035;&#20869;&#30340;&#29305;&#24449;&#21464;&#24322;&#24615;&#20026;&#38646;&#65292;&#65288;ii&#65289;&#29305;&#24449;&#22343;&#20540;&#38598;&#21512;&#26500;&#25104;&#19968;&#20010;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#65292;&#65288;iii&#65289;&#26368;&#21518;&#19968;&#23618;&#20998;&#31867;&#22120;&#25910;&#32553;&#21040;&#29305;&#24449;&#22343;&#20540;&#20056;&#20197;&#26576;&#20010;&#32553;&#25918;&#22240;&#23376;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30740;&#31350;&#25512;&#24191;&#21040;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#24182;&#39318;&#27425;&#35777;&#26126;&#20102;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#20844;&#24335;&#23384;&#22312;&#24191;&#20041;NC&#29616;&#35937;&#12290;&#22312;&#33258;&#28982;&#30340;&#26080;&#32422;&#26463;&#29305;&#24449;&#27169;&#22411;&#65288;UFM&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#8220;&#36873;&#25321;&#25152;&#26377;&#26631;&#31614;&#8221;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#20998;&#31867;&#22120;&#21482;&#26174;&#31034;&#20986;&#30456;&#21516;&#30340;ETF&#20960;&#20309;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#22349;&#32553;&#21040;&#22810;&#37325;&#24615;&#20026;1&#30340;&#29305;&#24449;&#31867;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study deep neural networks for the multi-label classification (MLab) task through the lens of neural collapse (NC). Previous works have been restricted to the multi-class classification setting and discovered a prevalent NC phenomenon comprising of the following properties for the last-layer features: (i) the variability of features within every class collapses to zero, (ii) the set of feature means form an equi-angular tight frame (ETF), and (iii) the last layer classifiers collapse to the feature mean upon some scaling. We generalize the study to multi-label learning, and prove for the first time that a generalized NC phenomenon holds with the "pick-all-label" formulation. Under the natural analog of the unconstrained feature model (UFM), we establish that the only global classifier of the pick-all-label cross entropy loss display the same ETF geometry which further collapse to multiplicity-1 feature class means. Besides, we discover a combinatorial property in generalized NC whic
&lt;/p&gt;</description></item><item><title>RSAdapter&#26159;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#32447;&#24615;&#36716;&#25442;&#23618;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.13120</link><description>&lt;p&gt;
RSAdapter: &#36866;&#24212;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering. (arXiv:2310.13120v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13120
&lt;/p&gt;
&lt;p&gt;
RSAdapter&#26159;&#19968;&#31181;&#38024;&#23545;&#36965;&#24863;&#35270;&#35273;&#38382;&#31572;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#36890;&#36807;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#32447;&#24615;&#36716;&#25442;&#23618;&#30340;&#35774;&#35745;&#65292;&#25552;&#39640;&#20102;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;Transformer&#30340;&#22810;&#27169;&#24577;&#26550;&#26500;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#22270;&#20687;&#25551;&#36848;&#12289;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36965;&#24863;VQA&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#36164;&#28304;&#23494;&#38598;&#22411;&#25216;&#26415;&#65292;&#22914;&#23545;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#25110;&#20174;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#25552;&#21462;&#22270;&#20687;-&#25991;&#26412;&#29305;&#24449;&#65292;&#28982;&#21518;&#20351;&#29992;&#35299;&#30721;&#22120;&#36827;&#34892;&#27169;&#24577;&#34701;&#21512;&#12290;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30456;&#24403;&#25968;&#37327;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RSAdapter&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20248;&#20808;&#32771;&#34385;&#36816;&#34892;&#26102;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;RSAdapter&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#24182;&#34892;&#36866;&#37197;&#22120;&#21644;&#25554;&#20837;&#22312;&#36866;&#37197;&#22120;&#30340;&#27599;&#20010;&#20840;&#36830;&#25509;&#65288;FC&#65289;&#23618;&#21518;&#30340;&#39069;&#22806;&#32447;&#24615;&#36716;&#25442;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, with the rapid advancement of transformer models, transformer-based multimodal architectures have found wide application in various downstream tasks, including but not limited to Image Captioning, Visual Question Answering (VQA), and Image-Text Generation. However, contemporary approaches to Remote Sensing (RS) VQA often involve resource-intensive techniques, such as full fine-tuning of large models or the extraction of image-text features from pre-trained multimodal models, followed by modality fusion using decoders. These approaches demand significant computational resources and time, and a considerable number of trainable parameters are introduced. To address these challenges, we introduce a novel method known as RSAdapter, which prioritizes runtime and parameter efficiency. RSAdapter comprises two key components: the Parallel Adapter and an additional linear transformation layer inserted after each fully connected (FC) layer within the Adapter. This approach not on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#26368;&#23567;&#35201;&#27714;&#30340;&#20844;&#29702;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#33021;&#30830;&#20445;&#40657;&#30418;&#20248;&#21270;&#25910;&#25947;&#24615;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09766</link><description>&lt;p&gt;
&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Bayesian Optimization. (arXiv:2310.09766v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#26368;&#23567;&#35201;&#27714;&#30340;&#20844;&#29702;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#33021;&#30830;&#20445;&#40657;&#30418;&#20248;&#21270;&#25910;&#25947;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#37327;&#21270;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#30340;&#39034;&#24207;&#25628;&#32034;&#12290;&#39640;&#26031;&#36807;&#31243;(GP)&#19968;&#30452;&#26159;&#26367;&#20195;&#27169;&#22411;&#30340;&#39318;&#36873;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#36125;&#21494;&#26031;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#25361;&#25112;&#20063;&#24341;&#21457;&#20102;&#19968;&#31995;&#21015;&#25910;&#25947;&#24615;&#26356;&#26174;&#24471;&#19981;&#26126;&#26174;&#30340;&#22791;&#36873;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#24341;&#20986;&#26368;&#23567;&#35201;&#27714;&#30340;&#20844;&#29702;&#26694;&#26550;&#26469;&#30830;&#20445;&#40657;&#30418;&#20248;&#21270;&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#24212;&#29992;&#20110;&#38500;&#20102;GP&#30456;&#20851;&#26041;&#27861;&#20043;&#22806;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30340;&#35774;&#35745;&#33258;&#30001;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#26469;&#26500;&#24314;&#32463;&#39564;&#19978;&#26356;&#20248;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31616;&#21333;&#30340;&#23616;&#37096;&#22238;&#24402;&#21644;&#19968;&#20010;&#36866;&#24212;&#38382;&#39064;&#29305;&#24615;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization is a popular approach for optimizing expensive black-box functions. Its key idea is to use a surrogate model to approximate the objective and, importantly, quantify the associated uncertainty that allows a sequential search of query points that balance exploitation-exploration. Gaussian process (GP) has been a primary candidate for the surrogate model, thanks to its Bayesian-principled uncertainty quantification power and modeling flexibility. However, its challenges have also spurred an array of alternatives whose convergence properties could be more opaque. Motivated by these, we study in this paper an axiomatic framework that elicits the minimal requirements to guarantee black-box optimization convergence that could apply beyond GP-related methods. Moreover, we leverage the design freedom in our framework, which we call Pseudo-Bayesian Optimization, to construct empirically superior algorithms. In particular, we show how using simple local regression, and a sui
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.07059</link><description>&lt;p&gt;
DKEC: &#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#30005;&#23376;&#30149;&#21382;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records. (arXiv:2310.07059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#12290;&#23427;&#21033;&#29992;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#24182;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#32463;&#24120;&#38754;&#20020;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65292;&#21363;&#32597;&#35265;&#31867;&#21035;&#30340;&#35757;&#32451;&#26679;&#26412;&#23569;&#20110;&#39057;&#32321;&#31867;&#21035;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#23618;&#27425;&#21270;&#26631;&#31614;&#32467;&#26500;&#26469;&#25214;&#21040;&#37325;&#35201;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#24573;&#30053;&#20102;&#20174;&#21307;&#23398;&#25351;&#21335;&#20013;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DKEC&#65292;&#19968;&#31181;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;&#39044;&#27979;&#30340;&#39046;&#22495;&#30693;&#35782;&#22686;&#24378;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21019;&#26032;&#28857;&#65306;&#65288;1&#65289;&#19968;&#20010;&#22522;&#20110;&#26631;&#31614;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#32467;&#21512;&#24322;&#26500;&#22270;&#21644;&#39046;&#22495;&#26412;&#20307;&#26469;&#25429;&#25417;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#65288;2&#65289;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#30456;&#20284;&#24615;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#32452;&#20869;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#21152;&#32597;&#35265;&#31867;&#21035;&#30340;&#26679;&#26412;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;DKEC&#65306;RAA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#24613;&#25937;&#26381;&#21153;&#65288;EMS&#65289;&#20107;&#20214;&#30340;4,417&#20010;&#24739;&#32773;&#25252;&#29702;&#25253;&#21578;&#30340;&#25910;&#38598;&#65292;&#21644;&#26469;&#33258;53898&#25253;&#21578;&#30340;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification (MLTC) tasks in the medical domain often face long-tail label distribution, where rare classes have fewer training samples than frequent classes. Although previous works have explored different model architectures and hierarchical label structures to find important features, most of them neglect to incorporate the domain knowledge from medical guidelines. In this paper, we present DKEC, Domain Knowledge Enhanced Classifier for medical diagnosis prediction with two innovations: (1) a label-wise attention mechanism that incorporates a heterogeneous graph and domain ontologies to capture the semantic relationships between medical entities, (2) a simple yet effective group-wise training method based on similarity of labels to increase samples of rare classes. We evaluate DKEC on two real-world medical datasets: the RAA dataset, a collection of 4,417 patient care reports from emergency medical services (EMS) incidents, and a subset of 53,898 reports from the 
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.05227</link><description>&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#38761;&#21629;&#31185;&#23398;&#33539;&#24335;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#22522;&#20110;&#36807;&#31243;&#30340;&#27700;&#25991;&#23398;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Physics-aware Machine Learning Revolutionizes Scientific Paradigm for Machine Learning and Process-based Hydrology. (arXiv:2310.05227v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05227
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#26041;&#27861;&#65292;&#23427;&#23558;&#29289;&#29702;&#30693;&#35782;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#65292;&#23545;&#20110;&#31649;&#29702;&#27700;&#36164;&#28304;&#20197;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#31561;&#25361;&#25112;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27700;&#25991;&#23398;&#29702;&#35299;&#21644;&#27700;&#24490;&#29615;&#39044;&#27979;&#23545;&#20110;&#35299;&#20915;&#27700;&#36164;&#28304;&#31649;&#29702;&#20013;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#25361;&#25112;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#21160;&#24577;&#24433;&#21709;&#19979;&#12290;&#29616;&#26377;&#30340;&#35780;&#35770;&#20027;&#35201;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#28982;&#32780;&#27700;&#25991;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#29420;&#31435;&#30340;&#33539;&#24335;&#23384;&#22312;&#26126;&#26174;&#30340;&#21306;&#21035;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20197;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#31181;&#35748;&#30693;&#38556;&#30861;&#65292;&#24182;&#38761;&#26032;&#20102;&#36825;&#20004;&#20010;&#39046;&#22495;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#29289;&#29702;&#24863;&#30693;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#35770;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#31038;&#21306;&#65288;PaML&#65289;&#65292;&#23558;&#20808;&#21069;&#30340;&#29289;&#29702;&#30693;&#35782;&#25110;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20174;&#29289;&#29702;&#25968;&#25454;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#20449;&#24687;&#22788;&#29702;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#29289;&#29702;&#23884;&#20837;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#29289;&#29702;&#24863;&#30693;&#28151;&#21512;&#23398;&#20064;&#22235;&#20010;&#26041;&#38754;&#20998;&#26512;&#20102;&#36825;&#20123;PaML&#26041;&#27861;&#12290;PaML&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#30340;&#20551;&#35774;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate hydrological understanding and water cycle prediction are crucial for addressing scientific and societal challenges associated with the management of water resources, particularly under the dynamic influence of anthropogenic climate change. Existing reviews predominantly concentrate on the development of machine learning (ML) in this field, yet there is a clear distinction between hydrology and ML as separate paradigms. Here, we introduce physics-aware ML as a transformative approach to overcome the perceived barrier and revolutionize both fields. Specifically, we present a comprehensive review of the physics-aware ML methods, building a structured community (PaML) of existing methodologies that integrate prior physical knowledge or physics-based modeling into ML. We systematically analyze these PaML methodologies with respect to four aspects: physical data-guided ML, physics-informed ML, physics-embedded ML, and physics-aware hybrid learning. PaML facilitates ML-aided hypothe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05128</link><description>&lt;p&gt;
&#23454;&#20363;&#21644;&#26631;&#31614;: &#38024;&#23545;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instances and Labels: Hierarchy-aware Joint Supervised Contrastive Learning for Hierarchical Multi-Label Text Classification. (arXiv:2310.05128v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;HJCL&#65289;&#65292;&#29992;&#20110;&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#21450;&#31934;&#24515;&#26500;&#24314;&#25209;&#27425;&#26469;&#22788;&#29702;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;HMTC&#20013;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;HMTC&#65289;&#26088;&#22312;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#36817;&#26399;&#20851;&#20110;HMTC&#30340;&#26041;&#27861;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#23558;&#25991;&#26412;&#21644;&#26631;&#31614;&#23884;&#20837;&#25509;&#36817;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#36755;&#20986;&#31354;&#38388;&#26045;&#21152;&#36807;&#24230;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#26679;&#26412;&#30340;&#29983;&#25104;&#24448;&#24448;&#24341;&#20837;&#22122;&#22768;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#21516;&#19968;&#25209;&#27425;&#20013;&#30456;&#20284;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26041;&#27861;&#26159;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#32467;&#26500;&#21270;&#26631;&#31614;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;$\textbf{HJCL}$&#30340;&#23618;&#27425;&#24863;&#30693;&#32852;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22635;&#34917;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;HMTC&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#23454;&#20363;&#32423;&#21644;&#26631;&#31614;&#32423;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#20180;&#32454;&#26500;&#36896;&#25209;&#27425;&#26469;&#28385;&#36275;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical multi-label text classification (HMTC) aims at utilizing a label hierarchy in multi-label classification. Recent approaches to HMTC deal with the problem of imposing an over-constrained premise on the output space by using contrastive learning on generated samples in a semi-supervised manner to bring text and label embeddings closer. However, the generation of samples tends to introduce noise as it ignores the correlation between similar samples in the same batch. One solution to this issue is supervised contrastive learning, but it remains an underexplored topic in HMTC due to its complex structured labels. To overcome this challenge, we propose $\textbf{HJCL}$, a $\textbf{H}$ierarchy-aware $\textbf{J}$oint Supervised $\textbf{C}$ontrastive $\textbf{L}$earning method that bridges the gap between supervised contrastive learning and HMTC. Specifically, we employ both instance-wise and label-wise contrastive learning techniques and carefully construct batches to fulfill the 
&lt;/p&gt;</description></item><item><title>FedAIoT&#26159;&#19968;&#20010;&#29992;&#20110;AIoT&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;FL&#30740;&#31350;&#20013;&#32570;&#20047;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.00109</link><description>&lt;p&gt;
FedAIoT: &#19968;&#31181;&#29992;&#20110;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FedAIoT: A Federated Learning Benchmark for Artificial Intelligence of Things. (arXiv:2310.00109v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00109
&lt;/p&gt;
&lt;p&gt;
FedAIoT&#26159;&#19968;&#20010;&#29992;&#20110;AIoT&#30340;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#65292;&#21253;&#25324;&#20843;&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#12290;&#23427;&#22635;&#34917;&#20102;&#29616;&#26377;FL&#30740;&#31350;&#20013;&#32570;&#20047;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#24046;&#36317;&#65292;&#24182;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#32852;&#32593;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20855;&#26377;&#37325;&#35201;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#30740;&#31350;&#24182;&#19981;&#26159;&#22522;&#20110;&#20174;&#30495;&#23454;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#25429;&#25417;&#20102;&#29289;&#32852;&#32593;&#25968;&#25454;&#30340;&#29420;&#29305;&#27169;&#24335;&#21644;&#22266;&#26377;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FedAIoT&#65292;&#19968;&#31181;&#29992;&#20110;AIoT&#30340;FL&#22522;&#20934;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#20851;&#38190;&#30340;&#24046;&#36317;&#12290;FedAIoT&#21253;&#25324;&#20174;&#21508;&#31181;&#29289;&#32852;&#32593;&#35774;&#22791;&#25910;&#38598;&#30340;&#20843;&#20010;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#29289;&#32852;&#32593;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#24182;&#38024;&#23545;AIoT&#30340;&#20856;&#22411;&#24212;&#29992;&#12290;FedAIoT&#36824;&#21253;&#25324;&#19968;&#31181;&#29992;&#20110;AIoT&#30340;&#32479;&#19968;&#30340;&#31471;&#21040;&#31471;FL&#26694;&#26550;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#25581;&#31034;&#20102;FL&#22312;AIoT&#39046;&#22495;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;FedAIoT&#33021;&#25104;&#20026;&#22312;FL for AIoT&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#25512;&#21160;&#36827;&#23637;&#30340;&#29645;&#36149;&#36164;&#28304;&#12290;FedAIoT&#30340;&#20195;&#30721;&#20179;&#24211;&#20301;&#20110;https://github.com/AIoT-MLSys-Lab/FedAIoT&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a significant relevance of federated learning (FL) in the realm of Artificial Intelligence of Things (AIoT). However, most existing FL works are not conducted on datasets collected from authentic IoT devices that capture unique modalities and inherent challenges of IoT data. In this work, we introduce FedAIoT, an FL benchmark for AIoT to fill this critical gap. FedAIoT includes eight datatsets collected from a wide range of IoT devices. These datasets cover unique IoT modalities and target representative applications of AIoT. FedAIoT also includes a unified end-to-end FL framework for AIoT that simplifies benchmarking the performance of the datasets. Our benchmark results shed light on the opportunities and challenges of FL for AIoT. We hope FedAIoT could serve as an invaluable resource to foster advancements in the important field of FL for AIoT. The repository of FedAIoT is maintained at https://github.com/AIoT-MLSys-Lab/FedAIoT.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22768;&#23398;&#29305;&#24449;&#23545;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.04505</link><description>&lt;p&gt;
COVID-19&#26816;&#27979;&#31995;&#32479;&#65306;&#22522;&#20110;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#22768;&#23398;&#29305;&#24449;&#31995;&#32479;&#24615;&#33021;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
COVID-19 Detection System: A Comparative Analysis of System Performance Based on Acoustic Features of Cough Audio Signals. (arXiv:2309.04505v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#22768;&#23398;&#29305;&#24449;&#23545;&#21683;&#22013;&#38899;&#39057;&#20449;&#21495;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21628;&#21560;&#36947;&#30142;&#30149;&#22914;&#24863;&#20882;&#21644;&#27969;&#24863;&#12289;&#21742;&#21912;&#20197;&#21450;COVID-19&#31561;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24433;&#21709;&#30528;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#22312;&#21307;&#23398;&#23454;&#36341;&#20013;&#65292;&#21628;&#21560;&#22768;&#38899;&#34987;&#24191;&#27867;&#29992;&#20110;&#21307;&#30103;&#26381;&#21153;&#20013;&#65292;&#29992;&#20110;&#35786;&#26029;&#21508;&#31181;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#21644;&#32954;&#37096;&#30142;&#30149;&#12290;&#20256;&#32479;&#30340;&#35786;&#26029;&#26041;&#27861;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#65292;&#25104;&#26412;&#39640;&#19988;&#20381;&#36182;&#20110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;&#21683;&#22013;&#38899;&#39057;&#35760;&#24405;&#34987;&#29992;&#26469;&#33258;&#21160;&#21270;&#26816;&#27979;&#21628;&#21560;&#31995;&#32479;&#30142;&#30149;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#26816;&#26597;&#21508;&#31181;&#22768;&#23398;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21683;&#22013;&#20449;&#21495;&#20013;&#26816;&#27979;COVID-19&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19977;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65288;MFCC&#65292;Chroma&#21644;Spectral Contrast&#29305;&#24449;&#65289;&#22312;&#20004;&#31181;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;SVM&#21644;MLP&#65289;&#19978;&#30340;&#21151;&#25928;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;COVID-19&#26816;&#27979;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide range of respiratory diseases, such as cold and flu, asthma, and COVID-19, affect people's daily lives worldwide. In medical practice, respiratory sounds are widely used in medical services to diagnose various respiratory illnesses and lung disorders. The traditional diagnosis of such sounds requires specialized knowledge, which can be costly and reliant on human expertise. Recently, cough audio recordings have been used to automate the process of detecting respiratory conditions. This research aims to examine various acoustic features that enhance the performance of machine learning (ML) models in detecting COVID-19 from cough signals. This study investigates the efficacy of three feature extraction techniques, including Mel Frequency Cepstral Coefficients (MFCC), Chroma, and Spectral Contrast features, on two ML algorithms, Support Vector Machine (SVM) and Multilayer Perceptron (MLP), and thus proposes an efficient COVID-19 detection system. The proposed system produces a prac
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#25581;&#31034;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#20013;&#35774;&#35745;&#20915;&#31574;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.16681</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20999;&#65292;&#26080;&#22788;&#19981;&#22312;&#65292;&#20840;&#26041;&#20301;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Everything, Everywhere All in One Evaluation: Using Multiverse Analysis to Evaluate the Influence of Model Design Decisions on Algorithmic Fairness. (arXiv:2308.16681v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16681
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#35780;&#20272;&#27169;&#22411;&#35774;&#35745;&#20915;&#31574;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#25581;&#31034;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#20013;&#35774;&#35745;&#20915;&#31574;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#35768;&#22810;&#31995;&#32479;&#37117;&#21033;&#29992;&#31639;&#27861;&#20915;&#31574;&#26469;&#65288;&#37096;&#20998;&#65289;&#33258;&#21160;&#21270;&#20197;&#21069;&#30001;&#20154;&#31867;&#36827;&#34892;&#30340;&#20915;&#31574;&#12290;&#24403;&#35774;&#35745;&#33391;&#22909;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#25215;&#35834;&#26356;&#23458;&#35266;&#30340;&#20915;&#31574;&#65292;&#21516;&#26102;&#33410;&#30465;&#22823;&#37327;&#36164;&#28304;&#65292;&#33410;&#32422;&#20154;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#35774;&#35745;&#19981;&#33391;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#31038;&#20250;&#32676;&#20307;&#36827;&#34892;&#27495;&#35270;&#30340;&#19981;&#20844;&#24179;&#20915;&#31574;&#12290;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#30340;&#19979;&#28216;&#25928;&#24212;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#31995;&#32479;&#35774;&#35745;&#21644;&#23454;&#26045;&#36807;&#31243;&#20013;&#30340;&#20915;&#31574;&#65292;&#22240;&#20026;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#32531;&#35299;&#25110;&#21152;&#24378;&#12290;&#35768;&#22810;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#26159;&#38544;&#21547;&#36827;&#34892;&#30340;&#65292;&#19981;&#30693;&#36947;&#23427;&#20204;&#30830;&#20999;&#22320;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;&#26126;&#30830;&#31639;&#27861;&#20915;&#31574;&#31995;&#32479;&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#24182;&#20102;&#35299;&#36825;&#20123;&#20915;&#31574;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#24515;&#29702;&#23398;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#20803;&#23431;&#23449;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A vast number of systems across the world use algorithmic decision making (ADM) to (partially) automate decisions that have previously been made by humans. When designed well, these systems promise more objective decisions while saving large amounts of resources and freeing up human time. However, when ADM systems are not designed well, they can lead to unfair decisions which discriminate against societal groups. The downstream effects of ADMs critically depend on the decisions made during the systems' design and implementation, as biases in data can be mitigated or reinforced along the modeling pipeline. Many of these design decisions are made implicitly, without knowing exactly how they will influence the final system. It is therefore important to make explicit the decisions made during the design of ADM systems and understand how these decisions affect the fairness of the resulting system.  To study this issue, we draw on insights from the field of psychology and introduce the metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12252</link><description>&lt;p&gt;
&#25105;&#30475;&#21040;&#30340;&#19996;&#35199;&#26377;&#22810;&#23433;&#20840;&#65311;&#22522;&#20110;&#22270;&#20687;&#25511;&#21046;&#30340;&#33258;&#27835;&#23433;&#20840;&#24615;&#39044;&#27979;&#30340;&#26657;&#20934;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy. (arXiv:2308.12252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#36890;&#36807;&#20811;&#26381;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#34920;&#31034;&#21644;&#20998;&#24067;&#28418;&#31227;&#19979;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#20855;&#26377;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#30340;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#24320;&#21457;&#33258;&#27835;&#31995;&#32479;&#30340;&#20027;&#35201;&#33539; paradigm&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#38543;&#30528;&#20854;&#24615;&#33021;&#21644;&#20415;&#21033;&#24615;&#65292;&#23433;&#20840;&#20445;&#35777;&#38754;&#20020;&#30528;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#25361;&#25112;&#30340;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#32570;&#20047;&#20302;&#32500;&#21487;&#35299;&#37322;&#21160;&#24577;&#29366;&#24577;&#30340;&#27010;&#24565;&#65292;&#20256;&#32479;&#30340;&#20445;&#35777;&#26041;&#27861;&#37117;&#22260;&#32469;&#36825;&#19968;&#27010;&#24565;&#23637;&#24320;&#12290;&#26412;&#25991;&#38024;&#23545;&#22312;&#32447;&#23433;&#20840;&#39044;&#27979;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#19990;&#30028;&#27169;&#22411;&#30340;&#21487;&#37197;&#32622;&#23398;&#20064;&#27969;&#27700;&#32447;&#26063;&#65292;&#19981;&#38656;&#35201;&#20302;&#32500;&#29366;&#24577;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#27969;&#27700;&#32447;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#23398;&#20064;&#23433;&#20840;&#30693;&#24773;&#28508;&#22312;&#34920;&#31034;&#21644;&#39044;&#27979;&#24341;&#36215;&#30340;&#20998;&#24067;&#28418;&#31227;&#19979;&#30340;&#32570;&#22833;&#23433;&#20840;&#26631;&#31614;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#27969;&#27700;&#32447;&#22522;&#20110;&#31526;&#21512;&#24615;&#39044;&#27979;&#65292;&#23545;&#20854;&#23433;&#20840;&#26426;&#20250;&#39044;&#27979;&#25552;&#20379;&#20102;&#32479;&#35745;&#26657;&#20934;&#20445;&#35777;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#23398;&#20064;&#27969;&#27700;&#32447;&#22312;&#20004;&#20010;&#22270;&#20687;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65306;&#36187;&#36710;&#21644;&#27773;&#36710;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end learning has emerged as a major paradigm for developing autonomous systems. Unfortunately, with its performance and convenience comes an even greater challenge of safety assurance. A key factor of this challenge is the absence of the notion of a low-dimensional and interpretable dynamical state, around which traditional assurance methods revolve. Focusing on the online safety prediction problem, this paper proposes a configurable family of learning pipelines based on generative world models, which do not require low-dimensional states. To implement these pipelines, we overcome the challenges of learning safety-informed latent representations and missing safety labels under prediction-induced distribution shift. These pipelines come with statistical calibration guarantees on their safety chance predictions based on conformal prediction. We perform an extensive evaluation of the proposed learning pipelines on two case studies of image-controlled systems: a racing car and a car
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2308.07706</link><description>&lt;p&gt;
&#21033;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#25506;&#32034;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models. (arXiv:2308.07706v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#36890;&#36807;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#21644;&#24341;&#20837;&#26032;&#30340;&#22270;&#20687;&#25551;&#36848;&#21464;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#26679;&#21270;&#21307;&#23398;&#22270;&#20687;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20020;&#24202;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#20998;&#21106;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#20294;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#25972;&#21512;&#25991;&#26412;&#25351;&#23548;&#20197;&#22686;&#24378;&#35270;&#35273;&#29305;&#24449;&#20173;&#28982;&#26159;&#19968;&#20010;&#36827;&#23637;&#26377;&#38480;&#30340;&#39046;&#22495;&#12290;&#29616;&#26377;&#21033;&#29992;&#25991;&#26412;&#25351;&#23548;&#30340;&#20998;&#21106;&#27169;&#22411;&#20027;&#35201;&#22312;&#24320;&#25918;&#39046;&#22495;&#22270;&#20687;&#19978;&#35757;&#32451;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;&#21307;&#23398;&#39046;&#22495;&#30452;&#25509;&#24212;&#29992;&#30340;&#38590;&#39064;&#65292;&#38656;&#35201;&#25163;&#21160;&#20171;&#20837;&#25110;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22810;&#27169;&#24577;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20174;&#22270;&#20687;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#25429;&#25417;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#24471;&#33021;&#22815;&#23545;&#22810;&#26679;&#21270;&#30340;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20998;&#21106;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#20197;&#35780;&#20272;&#20854;&#20174;&#24320;&#25918;&#39046;&#22495;&#21521;&#21307;&#23398;&#39046;&#22495;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#20013;&#20197;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#22270;&#20687;&#25551;&#36848;&#24341;&#20837;&#20102;&#21464;&#21270;&#65292;&#25581;&#31034;&#20102;&#26174;&#33879;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical Image Segmentation is crucial in various clinical applications within the medical domain. While state-of-the-art segmentation models have proven effective, integrating textual guidance to enhance visual features for this task remains an area with limited progress. Existing segmentation models that utilize textual guidance are primarily trained on open-domain images, raising concerns about their direct applicability in the medical domain without manual intervention or fine-tuning.  To address these challenges, we propose using multimodal vision-language models for capturing semantic information from image descriptions and images, enabling the segmentation of diverse medical images. This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field. Furthermore, we introduce variations of image descriptions for previously unseen images in the dataset, revealing notable variations 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#22312;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;2&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2307.07357</link><description>&lt;p&gt;
&#21453;&#21521;&#26368;&#20248;&#21270;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Inverse Optimization for Routing Problems. (arXiv:2307.07357v1 [math.OC] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#22312;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#26041;&#38754;&#21462;&#24471;&#20102;&#31532;2&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21453;&#21521;&#26368;&#20248;&#21270;&#65288;IO&#65289;&#23398;&#20064;&#36335;&#30001;&#38382;&#39064;&#20915;&#31574;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#12290;IO&#26694;&#26550;&#23646;&#20110;&#30417;&#30563;&#23398;&#20064;&#31867;&#21035;&#65292;&#24182;&#24314;&#31435;&#22312;&#30446;&#26631;&#34892;&#20026;&#26159;&#26410;&#30693;&#25104;&#26412;&#20989;&#25968;&#30340;&#20248;&#21270;&#22120;&#30340;&#21069;&#25552;&#19979;&#12290;&#36825;&#20010;&#25104;&#26412;&#20989;&#25968;&#36890;&#36807;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#22312;&#36335;&#30001;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#20915;&#31574;&#32773;&#30340;&#36335;&#30001;&#20559;&#22909;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#36335;&#30001;&#38382;&#39064;&#30340;IO&#26041;&#27861;&#65292;&#21253;&#25324;&#20551;&#35774;&#20989;&#25968;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#22522;&#20110;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#20122;&#39532;&#36874;&#26411;&#31471;&#36335;&#30001;&#30740;&#31350;&#25361;&#25112;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;IO&#26041;&#27861;&#65292;&#35813;&#25361;&#25112;&#30340;&#30446;&#26631;&#26159;&#20351;&#29992;&#25104;&#21315;&#19978;&#19975;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36335;&#30001;&#26696;&#20363;&#23398;&#20064;&#27169;&#22411;&#20197;&#22797;&#21046;&#20154;&#31867;&#39550;&#39542;&#21592;&#30340;&#36335;&#30001;&#20559;&#22909;&#12290;&#25105;&#20204;&#26368;&#32456;&#23398;&#20064;&#21040;&#30340;IO&#36335;&#30001;&#27169;&#22411;&#22312;48&#20010;&#26187;&#32423;&#21040;&#20915;&#36187;&#30340;&#27169;&#22411;&#20013;&#25490;&#21517;&#31532;2&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a method for learning decision-makers' behavior in routing problems using Inverse Optimization (IO). The IO framework falls into the supervised learning category and builds on the premise that the target behavior is an optimizer of an unknown cost function. This cost function is to be learned through historical data, and in the context of routing problems, can be interpreted as the routing preferences of the decision-makers. In this view, the main contributions of this study are to propose an IO methodology with a hypothesis function, loss function, and stochastic first-order algorithm tailored to routing problems. We further test our IO approach in the Amazon Last Mile Routing Research Challenge, where the goal is to learn models that replicate the routing preferences of human drivers, using thousands of real-world routing examples. Our final IO-learned routing model achieves a score that ranks 2nd compared with the 48 models that qualified for the final round of the challe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2306.06247</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning with Set-Valued Feedback. (arXiv:2306.06247v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#21464;&#20307;&#65292;&#20854;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#12290;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#35770;&#25991;&#34920;&#26126;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#24615;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#22312;&#23454;&#29616;&#35774;&#32622;&#19979;&#19981;&#31561;&#20215;&#65292;&#24182;&#23558;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#20316;&#20026;&#20854;&#29305;&#23450;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#30340;&#19968;&#31181;&#21464;&#20307;&#65292;&#20854;&#20013;&#23398;&#20064;&#22120;&#39044;&#27979;&#21333;&#20010;&#26631;&#31614;&#65292;&#20294;&#25509;&#25910;&#21040;&#19968;&#20010;&#26631;&#31614;&#30340;&#38598;&#21512;&#20316;&#20026;&#21453;&#39304;&#12290;&#22312;&#35813;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#23398;&#20064;&#22120;&#27809;&#26377;&#36755;&#20986;&#21253;&#21547;&#22312;&#21453;&#39304;&#38598;&#21512;&#20013;&#30340;&#26631;&#31614;&#65292;&#21017;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#19982;&#20855;&#26377;&#21333;&#26631;&#31614;&#21453;&#39304;&#30340;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#19981;&#21516;&#65292;&#22312;&#23454;&#29616;&#35774;&#32622;&#20013;&#20351;&#29992;&#38598;&#21512;&#22411;&#21453;&#39304;&#26102;&#65292;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;\textit{&#19981;&#31561;&#20215;}&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#26032;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;&#38598;&#21512;&#23567;&#30707;&#21644;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#65292;&#20005;&#26684;&#25551;&#36848;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#21270;&#30340;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#24230;&#37327;&#30772;&#35010;&#32500;&#24230;&#22312;&#24735;&#24615;&#35774;&#32622;&#19979;&#20005;&#26684;&#25551;&#36848;&#22312;&#32447;&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#22810;&#26631;&#31614;&#25490;&#21517;&#21644;&#22312;&#32447;&#22810;&#26631;&#31614;&#20998;&#31867;&#31561;&#23454;&#38469;&#23398;&#20064;&#35774;&#32622;&#26159;&#25105;&#20204;&#36890;&#29992;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a variant of online multiclass classification where the learner predicts a single label but receives a \textit{set of labels} as feedback. In this model, the learner is penalized for not outputting a label contained in the revealed set. We show that unlike online multiclass learning with single-label feedback, deterministic and randomized online learnability are \textit{not equivalent} even in the realizable setting with set-valued feedback. Accordingly, we give two new combinatorial dimensions, named the Set Littlestone and Measure Shattering dimension, that tightly characterize deterministic and randomized online learnability respectively in the realizable setting. In addition, we show that the Measure Shattering dimension tightly characterizes online learnability in the agnostic setting. Finally, we show that practical learning settings like online multilabel ranking and online multilabel classification are specific instances of our general online learning framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.18403</link><description>&lt;p&gt;
&#21098;&#26525;&#19982;&#20302;&#31209;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30456;&#36935;
&lt;/p&gt;
&lt;p&gt;
Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning. (arXiv:2305.18403v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoRAPrune&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#30340;&#20540;&#21644;&#26799;&#24230;&#26469;&#35774;&#35745;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;LPM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26469;&#20415;&#23452;&#22320;&#24494;&#35843;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#28982;&#21463;&#21040;&#24040;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21046;&#32422;&#12290;&#31070;&#32463;&#32593;&#32476;&#21098;&#26525;&#36890;&#36807;&#21024;&#38500;&#20887;&#20313;&#21442;&#25968;&#26469;&#25552;&#20379;&#27169;&#22411;&#21387;&#32553;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#21442;&#25968;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LPM&#32780;&#35328;&#65292;&#33719;&#24471;&#26799;&#24230;&#26159;&#35745;&#31639;&#19978;&#31105;&#27490;&#30340;&#65292;&#36825;&#38656;&#35201;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;LPM&#39640;&#25928;&#24494;&#35843;&#21644;&#37096;&#32626;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;LoRAPrune&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;PEFT&#24863;&#30693;&#30340;&#21098;&#26525;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#21033;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#20540;&#21644;&#26799;&#24230;&#65292;&#32780;&#19981;&#26159;&#39044;&#35757;&#32451;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36845;&#20195;&#21098;&#26525;&#36807;&#31243;&#26469;&#22522;&#20110;&#21098;&#26525;&#20934;&#21017;&#21435;&#38500;&#20887;&#20313;&#21442;&#25968;&#12290;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#22823;&#23567;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models (LPMs), such as LLaMA and ViT-G, have shown exceptional performance across various tasks. Although parameter-efficient fine-tuning (PEFT) has emerged to cheaply fine-tune these large models on downstream tasks, their deployment is still hindered by the vast model scale and computational costs. Neural network pruning offers a solution for model compression by removing redundant parameters, but most existing methods rely on computing parameter gradients. However, obtaining the gradients is computationally prohibitive for LPMs, which necessitates the exploration of alternative approaches. To this end, we propose a unified framework for efficient fine-tuning and deployment of LPMs, termed LoRAPrune. We first design a PEFT-aware pruning criterion, which utilizes the values and gradients of Low-Rank Adaption (LoRA), rather than the gradients of pre-trained parameters for importance estimation. We then propose an iterative pruning procedure to remove redundant paramet
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#20855;&#26377;&#39640;&#31232;&#30095;&#24230;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#36825;&#19982;&#29983;&#29289;&#23398;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.18353</link><description>&lt;p&gt;
Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20013;&#30340;&#31361;&#29616;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Emergent representations in networks trained with the Forward-Forward algorithm. (arXiv:2305.18353v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18353
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#35757;&#32451;&#30340;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#20855;&#26377;&#39640;&#31232;&#30095;&#24230;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#36825;&#19982;&#29983;&#29289;&#23398;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Backpropagation&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#20294;&#20854;&#32570;&#20047;&#29983;&#29289;&#23398;&#19978;&#30340;&#29616;&#23454;&#24615;&#12290;&#20026;&#20102;&#23547;&#25214;&#19968;&#31181;&#26356;&#20855;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#36991;&#20813;&#21453;&#21521;&#20256;&#25773;&#26799;&#24230;&#65292;&#32780;&#26159;&#20351;&#29992;&#26412;&#22320;&#23398;&#20064;&#35268;&#21017;&#65292;&#26368;&#36817;&#20171;&#32461;&#30340;Forward-Forward&#31639;&#27861;&#23558;Backpropagation&#30340;&#20256;&#36882;&#26367;&#25442;&#20026;&#20004;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;Forward-Forward&#31639;&#27861;&#33719;&#24471;&#30340;&#20869;&#37096;&#34920;&#24449;&#32452;&#32455;&#20026;&#31283;&#20581;&#30340;&#65292;&#31867;&#21035;&#29305;&#23450;&#30340;&#38598;&#21512;&#65292;&#30001;&#26497;&#23569;&#37327;&#30340;&#26377;&#25928;&#21333;&#20803;(&#39640;&#31232;&#30095;&#24230;)&#32452;&#25104;&#12290;&#36825;&#19982;&#24863;&#35273;&#22788;&#29702;&#36807;&#31243;&#20013;&#35266;&#23519;&#21040;&#30340;&#30382;&#23618;&#34920;&#24449;&#38750;&#24120;&#30456;&#20284;&#12290;&#34429;&#28982;&#22312;&#20351;&#29992;&#26631;&#20934;Backpropagation&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#27809;&#26377;&#21457;&#29616;&#65292;&#20294;&#26159;&#22312;&#20351;&#29992;&#19982;Forward-Forward&#30456;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#32593;&#32476;&#20013;&#20063;&#20986;&#29616;&#20102;&#31232;&#30095;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;Forward-Forward&#25552;&#35758;&#30340;&#23398;&#20064;&#36807;&#31243;&#21487;&#33021;&#26356;&#25509;&#36817;&#29983;&#29289;&#23398;&#23398;&#20064;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Backpropagation algorithm, widely used to train neural networks, has often been criticised for its lack of biological realism. In an attempt to find a more biologically plausible alternative, and avoid to back-propagate gradients in favour of using local learning rules, the recently introduced Forward-Forward algorithm replaces the traditional forward and backward passes of Backpropagation with two forward passes. In this work, we show that internal representations obtained with the Forward-Forward algorithm organize into robust, category-specific ensembles, composed by an extremely low number of active units (high sparsity). This is remarkably similar to what is observed in cortical representations during sensory processing. While not found in models trained with standard Backpropagation, sparsity emerges also in networks optimized by Backpropagation, on the same training objective of Forward-Forward. These results suggest that the learning procedure proposed by Forward-Forward ma
&lt;/p&gt;</description></item><item><title>DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05738</link><description>&lt;p&gt;
DOCTOR&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05738
&lt;/p&gt;
&lt;p&gt;
DOCTOR&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;&#23427;&#21487;&#20197;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#24182;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#36793;&#32536;&#35774;&#22791;&#20013;&#30340;&#21487;&#31359;&#25140;&#21307;&#30103;&#20256;&#24863;&#22120;&#65288;WMS&#65289;&#30340;&#36827;&#27493;&#20351;&#24471;&#26234;&#33021;&#21307;&#30103;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#25104;&#20026;&#21487;&#33021;&#12290;&#20256;&#32479;&#30340;ML&#39537;&#21160;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#20026;&#27599;&#31181;&#30142;&#30149;&#21644;&#30456;&#24212;&#30340;WMS&#25968;&#25454;&#23450;&#21046;&#20010;&#21035;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#23545;&#20998;&#24067;&#21464;&#21270;&#21644;&#26032;&#20219;&#21153;&#20998;&#31867;&#30340;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#27979;&#27599;&#20010;&#26032;&#30142;&#30149;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;WMS&#30340;&#22810;&#30142;&#30149;&#26816;&#27979;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;DOCTOR&#12290;&#23427;&#37319;&#29992;&#20102;&#22810;&#22836;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21644;&#19968;&#31181;Exemplar-replay&#39118;&#26684;&#30340;CL&#31639;&#27861;&#12290;CL&#31639;&#27861;&#20351;&#24471;&#26694;&#26550;&#33021;&#22815;&#19981;&#26029;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20854;&#20013;&#28041;&#21450;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12289;&#20998;&#31867;&#31867;&#21035;&#21644;&#30142;&#30149;&#26816;&#27979;&#20219;&#21153;&#12290;DOCTOR&#22312;&#20351;&#29992;&#26469;&#33258;&#23454;&#38469;WMS&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#22235;&#31181;&#24120;&#35265;&#30142;&#30149;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#22312;&#20869;&#23384;&#20351;&#29992;&#12289;&#30005;&#27744;&#28040;&#32791;&#21644;&#26816;&#27979;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;DOCTOR&#20063;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21512;&#25104;PET&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;PET&#25104;&#20687;&#25104;&#26412;&#39640;&#19988;&#28041;&#21450;&#36752;&#23556;&#26292;&#38706;&#30340;&#32570;&#38519;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#25935;&#24863;&#24230;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.03901</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#23558;&#39640;&#22330;&#21644;&#36229;&#39640;&#22330;MRI&#22270;&#20687;&#21512;&#25104;PET&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Synthesizing PET images from High-field and Ultra-high-field MR images Using Joint Diffusion Attention Model. (arXiv:2305.03901v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#21512;&#25104;PET&#22270;&#20687;&#65292;&#20197;&#35299;&#20915;PET&#25104;&#20687;&#25104;&#26412;&#39640;&#19988;&#28041;&#21450;&#36752;&#23556;&#26292;&#38706;&#30340;&#32570;&#38519;&#12290;&#35813;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#25935;&#24863;&#24230;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MRI&#21644;PET&#26159;&#35786;&#26029;&#33041;&#37096;&#30142;&#30149;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#33041;&#32467;&#26500;&#21644;&#21151;&#33021;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;PET&#25195;&#25551;&#25104;&#26412;&#39640;&#19988;&#28041;&#21450;&#36752;&#23556;&#26292;&#38706;&#65292;&#23548;&#33268;PET&#32570;&#20047;&#12290;&#27492;&#22806;&#65292;&#21516;&#26102;&#36827;&#34892;&#36229;&#39640;&#22330;MRI&#21644;PET&#30446;&#21069;&#20960;&#20046;&#19981;&#21487;&#34892;&#12290;&#36229;&#39640;&#22330;&#25104;&#20687;&#22312;&#20020;&#24202;&#21644;&#23398;&#26415;&#29615;&#22659;&#20013;&#37117;&#34987;&#35777;&#26126;&#26159;&#26377;&#20215;&#20540;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35748;&#30693;&#31070;&#32463;&#24433;&#20687;&#23398;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39640;&#22330;&#21644;&#36229;&#39640;&#22330;MRI&#21512;&#25104;PET&#30340;&#26041;&#27861;&#12290;&#20174;&#32479;&#35745;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65288;JPD&#65289;&#26159;&#25551;&#32472;PET&#21644;MRI&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#26368;&#30452;&#25509;&#21644;&#22522;&#26412;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#21512;&#25193;&#25955;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#21644;&#27880;&#24847;&#31574;&#30053;&#65292;&#21629;&#21517;&#20026;JDAM&#12290;JDAM&#20855;&#26377;&#25193;&#25955;&#36807;&#31243;&#21644;&#37319;&#26679;&#36807;&#31243;&#12290;&#25193;&#25955;&#36807;&#31243;&#28041;&#21450;&#23558;PET&#36880;&#28176;&#25193;&#25955;&#21040;&#39640;&#26031;PET&#65292;&#37319;&#26679;&#36807;&#31243;&#20174;&#39640;&#22330;&#21644;&#36229;&#39640;&#22330;MRI&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#29305;&#24449;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JDAM&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PET&#21512;&#25104;&#26041;&#27861;&#65292;&#24182;&#19988;&#23545;&#22122;&#22768; lesss &#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
MRI and PET are crucial diagnostic tools for brain diseases, as they provide complementary information on brain structure and function. However, PET scanning is costly and involves radioactive exposure, resulting in a lack of PET. Moreover, simultaneous PET and MRI at ultra-high-field are currently hardly infeasible. Ultra-high-field imaging has unquestionably proven valuable in both clinical and academic settings, especially in the field of cognitive neuroimaging. These motivate us to propose a method for synthetic PET from high-filed MRI and ultra-high-field MRI. From a statistical perspective, the joint probability distribution (JPD) is the most direct and fundamental means of portraying the correlation between PET and MRI. This paper proposes a novel joint diffusion attention model which has the joint probability distribution and attention strategy, named JDAM. JDAM has a diffusion process and a sampling process. The diffusion process involves the gradual diffusion of PET to Gaussi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#23558;&#20840;&#29699;&#28418;&#27969;&#22120;&#25968;&#25454;&#38598;&#19982;&#28023;&#27915;&#39044;&#27979;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#28023;&#27915;&#28201;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05551</link><description>&lt;p&gt;
MPAS-O&#19982;&#20840;&#29699;&#28418;&#27969;&#22120;&#25968;&#25454;&#38598;&#30340;&#21160;&#24577;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Dynamic Data Assimilation of MPAS-O and the Global Drifter Dataset. (arXiv:2301.05551v2 [physics.ao-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21160;&#24577;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#65292;&#23558;&#20840;&#29699;&#28418;&#27969;&#22120;&#25968;&#25454;&#38598;&#19982;&#28023;&#27915;&#39044;&#27979;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#30340;&#21160;&#24577;&#21644;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#28023;&#27915;&#28201;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29616;&#22330;&#28014;&#26631;&#27979;&#37327;&#32467;&#26524;&#19982;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;ESMs&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#28023;&#27915;&#28201;&#24230;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;ESMs&#20013;&#30830;&#23450;&#30340;&#21160;&#24577;&#21644;&#27169;&#24335;&#26469;&#25913;&#21892;&#28014;&#26631;&#27979;&#37327;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23395;&#33410;&#24615;&#31561;&#29305;&#24449;&#12290;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#32416;&#27491;MPAS-O&#27169;&#22411;&#20135;&#29983;&#30340;&#26412;&#22320;&#28201;&#24230;&#39044;&#27979;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#25554;&#20540;&#21644;&#25968;&#25454;&#21516;&#21270;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#23558;&#36328;&#23610;&#24230;&#28023;&#27915;&#39044;&#27979;&#27169;&#22411;&#65288;MPAS-O&#65289;&#19982;&#20840;&#29699;&#28418;&#27969;&#22120;&#35745;&#21010;&#30340;&#29616;&#22330;&#28023;&#27915;&#28014;&#26631;&#25968;&#25454;&#38598;&#21516;&#21270;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a new method for combining in situ buoy measurements with Earth system models (ESMs) to improve the accuracy of temperature predictions in the ocean. The technique utilizes the dynamics and modes identified in ESMs to improve the accuracy of buoy measurements while still preserving features such as seasonality. Using this technique, errors in localized temperature predictions made by the MPAS-O model can be corrected. We demonstrate that our approach improves accuracy compared to other interpolation and data assimilation methods. We apply our method to assimilate the Model for Prediction Across Scales Ocean component (MPAS-O) with the Global Drifter Program's in-situ ocean buoy dataset.
&lt;/p&gt;</description></item><item><title>&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.14041</link><description>&lt;p&gt;
RFold&#65306;&#22522;&#20110;&#35299;&#32806;&#20248;&#21270;&#26041;&#27861;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
RFold: RNA Secondary Structure Prediction with Decoupled Optimization. (arXiv:2212.14041v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14041
&lt;/p&gt;
&lt;p&gt;
&#25152;&#25552;&#20986;&#30340;RFold&#26041;&#27861;&#37319;&#29992;&#35299;&#32806;&#20248;&#21270;&#36807;&#31243;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31616;&#21333;&#21448;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#31958;&#26680;&#37240;&#65288;RNA&#65289;&#30340;&#20108;&#32423;&#32467;&#26500;&#27604;&#19977;&#32423;&#32467;&#26500;&#26356;&#31283;&#23450;&#21644;&#26356;&#26131;&#20110;&#22312;&#32454;&#32990;&#20013;&#35775;&#38382;&#65292;&#22240;&#27492;&#23545;&#20110;&#21151;&#33021;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#27867;&#21270;&#24615;&#24046;&#21644;&#22797;&#26434;&#24615;&#39640;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;RNA&#20108;&#32423;&#32467;&#26500;&#39044;&#27979;&#26041;&#27861;RFold&#12290;RFold&#24341;&#20837;&#20102;&#19968;&#31181;&#35299;&#32806;&#20248;&#21270;&#30340;&#36807;&#31243;&#65292;&#23558;&#20256;&#32479;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#20998;&#35299;&#20026;&#36880;&#34892;&#21644;&#36880;&#21015;&#20248;&#21270;&#65292;&#31616;&#21270;&#20102;&#27714;&#35299;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#36755;&#20986;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;RFold&#37319;&#29992;&#27880;&#24847;&#21147;&#22320;&#22270;&#20316;&#20026;&#20449;&#24687;&#34920;&#31034;&#65292;&#32780;&#19981;&#26159;&#35774;&#35745;&#25163;&#24037;&#29305;&#24449;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RFold&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#32422;8&#20493;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;&#20195;&#30721;&#21644;Colab&#28436;&#31034;&#21487;&#22312;\href{this http URL}{this http UR}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The secondary structure of ribonucleic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method. The code and Colab demo are available in \href{this http URL}{this http UR
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15498</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#30693;&#27979;&#37327;&#22122;&#22768;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks with unknown measurement noise. (arXiv:2211.15498v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15498
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#22312;&#23384;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#26159;&#19968;&#31181;&#26082;&#33021;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#21448;&#33021;&#35782;&#21035;&#20559;&#24494;&#20998;&#26041;&#31243;&#21442;&#25968;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#30456;&#20851;&#30340;&#30740;&#31350;&#37117;&#20551;&#35774;&#25968;&#25454;&#26159;&#26080;&#22122;&#22768;&#30340;&#65292;&#25110;&#32773;&#26159;&#21463;&#24369;&#39640;&#26031;&#22122;&#22768;&#27745;&#26579;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;PINN&#26694;&#26550;&#22312;&#38750;&#39640;&#26031;&#22122;&#22768;&#24773;&#20917;&#19979;&#22833;&#25928;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#36825;&#20010;&#26681;&#26412;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#21516;&#26102;&#35757;&#32451;&#19968;&#20010;&#33021;&#37327;&#27169;&#22411;(Energy-Based Model, EBM)&#26469;&#23398;&#20064;&#27491;&#30830;&#30340;&#22122;&#22768;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) constitute a flexible approach to both finding solutions and identifying parameters of partial differential equations. Most works on the topic assume noiseless data, or data contaminated by weak Gaussian noise. We show that the standard PINN framework breaks down in case of non-Gaussian noise. We give a way of resolving this fundamental issue and we propose to jointly train an energy-based model (EBM) to learn the correct noise distribution. We illustrate the improved performance of our approach using multiple examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StaPLR&#31639;&#27861;&#30340;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#20197;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.14484</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#20013;&#32570;&#22833;&#20540;&#30340;&#25554;&#34917;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imputation of missing values in multi-view data. (arXiv:2210.14484v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;StaPLR&#31639;&#27861;&#30340;&#26032;&#30340;&#22810;&#35270;&#35282;&#25968;&#25454;&#25554;&#34917;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#20197;&#35299;&#20915;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#20013;&#24471;&#21040;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#25968;&#25454;&#26159;&#25351;&#30001;&#22810;&#20010;&#19981;&#21516;&#29305;&#24449;&#38598;&#25551;&#36848;&#30340;&#25968;&#25454;&#12290;&#22312;&#22788;&#29702;&#22810;&#35270;&#35282;&#25968;&#25454;&#26102;&#65292;&#33509;&#20986;&#29616;&#32570;&#22833;&#20540;&#65292;&#21017;&#19968;&#20010;&#35270;&#35282;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#26497;&#26377;&#21487;&#33021;&#21516;&#26102;&#32570;&#22833;&#65292;&#22240;&#32780;&#23548;&#33268;&#38750;&#24120;&#22823;&#37327;&#30340;&#32570;&#22833;&#25968;&#25454;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#35270;&#35282;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#25554;&#34917;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#22534;&#21472;&#24809;&#32602;&#36923;&#36753;&#22238;&#24402;(StaPLR)&#31639;&#27861;&#65292;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#25191;&#34892;&#25554;&#34917;&#65292;&#20197;&#35299;&#20915;&#22266;&#26377;&#30340;&#22810;&#35270;&#35282;&#35745;&#31639;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20174;&#32780;&#21487;&#20197;&#20351;&#29992;&#20808;&#36827;&#30340;&#25554;&#34917;&#31639;&#27861;&#65292;&#20363;&#22914;missForest&#12290;
&lt;/p&gt;
&lt;p&gt;
Data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. When missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. This leads to very large quantities of missing data which, especially when combined with high-dimensionality, makes the application of conditional imputation methods computationally infeasible. We introduce a new imputation method based on the existing stacked penalized logistic regression (StaPLR) algorithm for multi-view learning. It performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. We compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets. The results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missForest 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2206.02909</link><description>&lt;p&gt;
&#20351;&#29992;70&#19975;&#20154;&#26085;&#30340;&#21487;&#31359;&#25140;&#25968;&#25454;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Human Activity Recognition Using 700,000 Person-days of Wearable Data. (arXiv:2206.02909v2 [eess.SP] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;700,000&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27867;&#21270;&#19988;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#26377;&#26395;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#26041;&#38754;&#30340;&#36827;&#23637;&#30456;&#23545;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;UK-Biobank&#27963;&#21160;&#36861;&#36394;&#22120;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;70&#19975;&#20154;&#26085;&#30340;&#26410;&#26631;&#35760;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#22312;&#19971;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;F1&#30456;&#23545;&#25913;&#21892;2.5%-100%&#65288;&#20013;&#20301;&#25968;18.4%&#65289;&#65292;&#25913;&#36827;&#26368;&#22823;&#30340;&#26159;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#21487;&#20197;&#27867;&#21270;&#21040;&#22806;&#37096;&#25968;&#25454;&#38598;&#12289;&#35774;&#22791;&#21644;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#24320;&#28304;&#27169;&#22411;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#32773;&#26500;&#24314;&#20855;&#26377;&#39640;&#24615;&#33021;&#30340;&#21487;&#23450;&#21046;&#21644;&#27867;&#21270;&#30340;&#27963;&#21160;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advances in deep learning for human activity recognition have been relatively limited due to the lack of large labelled datasets. In this study, we leverage self-supervised learning techniques on the UK-Biobank activity tracker dataset--the largest of its kind to date--containing more than 700,000 person-days of unlabelled wearable sensor data. Our resulting activity recognition model consistently outperformed strong baselines across seven benchmark datasets, with an F1 relative improvement of 2.5%-100% (median 18.4%), the largest improvements occurring in the smaller datasets. In contrast to previous studies, our results generalise across external datasets, devices, and environments. Our open-source model will help researchers and developers to build customisable and generalisable activity classifiers with high performance.
&lt;/p&gt;</description></item></channel></rss>