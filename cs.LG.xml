<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>FedMoE&#26159;&#19968;&#31181;&#27169;&#22411;&#24322;&#26500;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#20849;&#20139;&#30340;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#20998;&#37197;&#32473;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#12289;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01350</link><description>&lt;p&gt;
FedMoE: &#25968;&#25454;&#32423;&#21035;&#20010;&#24615;&#21270;&#30340;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#29992;&#20110;&#24322;&#26500;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedMoE: Data-Level Personalization with Mixture of Experts for Model-Heterogeneous Personalized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01350
&lt;/p&gt;
&lt;p&gt;
FedMoE&#26159;&#19968;&#31181;&#27169;&#22411;&#24322;&#26500;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#20849;&#20139;&#30340;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#20998;&#37197;&#32473;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#38544;&#31169;&#12289;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24191;&#27867;&#24212;&#29992;&#20110;&#20998;&#25955;&#25968;&#25454;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20294;&#38754;&#20020;&#25968;&#25454;&#12289;&#31995;&#32479;&#21644;&#27169;&#22411;&#24322;&#26500;&#31561;&#25361;&#25112;&#12290;&#36825;&#23548;&#33268;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064; (MHPFL) &#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;MHPFL&#26041;&#27861;&#22312;&#25968;&#25454;&#21644;&#27169;&#22411;&#38544;&#31169;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#36890;&#20449;&#21644;&#35745;&#31639;&#25104;&#26412;&#26041;&#38754;&#20173;&#23384;&#22312;&#20851;&#20999;&#12290;&#20026;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861; (FedMoE) &#65292;&#37319;&#29992;&#33879;&#21517;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411; (MoE) &#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#12290;&#23427;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#24322;&#26500;&#22823;&#27169;&#22411;&#20998;&#37197;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#22343;&#21248;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#26412;&#22320;&#38376;&#25511;&#32593;&#32476;&#12290;(1) &#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26412;&#22320;&#24322;&#26500;&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#20316;&#20026;&#20010;&#24615;&#21270;&#29305;&#24449;&#65288;&#34920;&#31034;&#65289;&#25552;&#21462;&#30340;&#26412;&#22320;&#19987;&#23478;&#65292;&#32780;&#20849;&#20139;&#30340;&#22343;&#21248;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#21017;&#20316;&#20026;&#24191;&#20041;&#29305;&#24449;&#25552;&#21462;&#30340;&#20840;&#23616;&#19987;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is widely employed for collaborative training on decentralized data but faces challenges like data, system, and model heterogeneity. This prompted the emergency of model-heterogeneous personalized federated learning (MHPFL). However, concerns persist regarding data and model privacy, model performance, communication, and computational costs in current MHPFL methods. To tackle these concerns, we propose a novel model-heterogeneous personalized Federated learning algorithm (FedMoE) with the Mixture of Experts (MoE), renowned for enhancing large language models (LLMs). It assigns a shared homogeneous small feature extractor and a local gating network for each client's local heterogeneous large model. (1) During local training, the local heterogeneous model's feature extractor acts as a local expert for personalized feature (representation) extraction, while the shared homogeneous small feature extractor serves as a global expert for generalized feature extraction. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21152;&#36895;Transformers&#27169;&#22411;&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#23376;&#20998;&#35299;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(N)&#65292;&#24182; &#22312;&#32500;&#25345;&#27880;&#24847;&#21147;&#30697;&#38453;&#23436;&#25972;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#21644;&#25152;&#26377;-&#25152;&#26377;&#20196;&#29260;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27880;&#24847;&#21147;&#26426;&#21046;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07901</link><description>&lt;p&gt;
FAST: &#29992;&#20110;&#21152;&#36895;Transformers&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FAST: Factorizable Attention for Speeding up Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#20197;&#21152;&#36895;Transformers&#27169;&#22411;&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#22240;&#23376;&#20998;&#35299;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#65292;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(N)&#65292;&#24182; &#22312;&#32500;&#25345;&#27880;&#24847;&#21147;&#30697;&#38453;&#23436;&#25972;&#34920;&#31034;&#30340;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#21644;&#25152;&#26377;-&#25152;&#26377;&#20196;&#29260;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27880;&#24847;&#21147;&#26426;&#21046;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21407;&#22987;&#30340;&#24555;&#36895;&#22810;&#26497;&#26041;&#27861;&#21644;&#25913;&#36827;&#21518;&#30340;&#24555;&#36895;&#39640;&#26031;&#21464;&#25442;&#25152;&#22266;&#26377;&#30340;&#22240;&#23376;&#20998;&#35299;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#39640;&#32500;&#24230;&#20013;&#39640;&#25928;&#36816;&#34892;&#30340;&#21487;&#20998;&#35299;&#27880;&#24847;&#21147;&#24418;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;Transformers&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#22797;&#26434;&#24230;&#20174;O(N^2)&#38477;&#20302;&#21040;O(N)&#12290;&#19982;&#20043;&#21069;&#30340;&#23581;&#35797;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#21576;&#29616;&#20102;&#19968;&#20010;&#32447;&#24615;&#32553;&#25918;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26082;&#20445;&#25345;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#23436;&#25972;&#34920;&#31034;&#65292;&#21448;&#19981;&#22949;&#21327;&#20110;&#31232;&#30095;&#21270;&#65292;&#24182;&#19988;&#21253;&#21547;&#20102;&#20196;&#29260;&#20043;&#38388;&#30340;&#20840;&#20114;&#25805;&#20316;&#20851;&#31995;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#25105;&#20204;&#26032;&#30340;&#27880;&#24847;&#21147;&#24230;&#37327;&#30340;&#23646;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#26631;&#20934;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#20855;&#26377;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20351;&#29992;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22810;&#26679;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the factorization inherent in the original fast multipole method and the improved fast Gauss transform we introduce a factorable form of attention that operates efficiently in high dimensions. This approach reduces the computational and memory complexity of the attention mechanism in transformers from $O(N^2)$ to $O(N)$. In comparison to previous attempts, our work presents a linearly scaled attention mechanism that maintains the full representation of the attention matrix without compromising on sparsification and incorporates the all-to-all relationship between tokens. We explore the properties of our new attention metric and conduct tests in various standard settings. Results indicate that our attention mechanism has a robust performance and holds significant promise for diverse applications where self-attention is used.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#21333;&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#36755;&#20837;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07899</link><description>&lt;p&gt;
&#20174;&#21333;&#19968;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#30340;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A systematic investigation of learnability from single child linguistic input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07899
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#29992;&#21333;&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#36755;&#20837;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#29983;&#25104;&#35821;&#35328;&#36830;&#36143;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102; remarkable proficiency&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#19982;&#20154;&#31867;&#35821;&#35328;&#21487;&#23398;&#20064;&#24615;&#30340;&#30456;&#20851;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#19982;&#20799;&#31461;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#36755;&#20837;&#20043;&#38388;&#23384;&#22312;&#30528;&#26174;&#33879;&#24046;&#36317;&#12290;LMs&#36890;&#24120;&#22312;&#25968;&#37327;&#32423;&#19978;&#26356;&#22823;&#19988;&#26412;&#36136;&#19982;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#19981;&#21516;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#38024;&#23545;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#21333;&#20010;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#30340;&#23376;&#38598;&#19978;&#35757;&#32451;LMs&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#35757;&#32451;&#30340;LMs&#21487;&#20197;&#24418;&#25104;&#21477;&#27861;&#21644;&#35821;&#20041;&#35789;&#32676;&#65292;&#24182;&#23545;&#26576;&#20123;&#35821;&#35328;&#29616;&#35937;&#20855;&#26377;&#25935;&#24863;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#20165;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#20799;&#31461;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;LSTMs&#21644;&#26356;&#31616;&#21333;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20026;&#20102;&#26816;&#39564;&#20174;&#21333;&#19968;&#20799;&#31461;&#36755;&#20837;&#21487;&#23398;&#20064;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#8230;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have demonstrated remarkable proficiency in generating linguistically coherent text, sparking discussions about their relevance to understanding human language learnability. However, a significant gap exists between the training data for these models and the linguistic input a child receives. LMs are typically trained on data that is orders of magnitude larger and fundamentally different from child-directed speech (Warstadt and Bowman, 2022; Warstadt et al., 2023; Frank, 2023a). Addressing this discrepancy, our research focuses on training LMs on subsets of a single child's linguistic input. Previously, Wang, Vong, Kim, and Lake (2023) found that LMs trained in this setting can form syntactic and semantic word clusters and develop sensitivity to certain linguistic phenomena, but they only considered LSTMs and simpler neural networks trained from just one single-child dataset. Here, to examine the robustness of learnability from single-child input, we systematicall
&lt;/p&gt;</description></item><item><title>DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07891</link><description>&lt;p&gt;
&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Label-Efficient Model Selection for Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07891
&lt;/p&gt;
&lt;p&gt;
DiffUse&#26159;&#19968;&#31181;&#26631;&#27880;&#25928;&#29575;&#39640;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32858;&#31867;&#25991;&#26412;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#36873;&#25321;&#26356;&#20855;&#20449;&#24687;&#37327;&#30340;&#23454;&#20363;&#65292;&#24182;&#33021;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#32473;&#23450;&#30446;&#26631;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#38656;&#35201;&#23545;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#36136;&#37327;&#36827;&#34892;&#24191;&#27867;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DiffUse&#65292;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#22312;&#20505;&#36873;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;DiffUse&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#22312;&#35780;&#20272;&#20013;&#23453;&#36149;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;DiffUse&#36890;&#36807;&#32858;&#31867;&#34920;&#31034;&#27169;&#22411;&#36755;&#20986;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#24322;&#30340;&#23884;&#20837;&#26469;&#26234;&#33021;&#36873;&#25321;&#23454;&#20363;&#12290;&#22240;&#27492;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#20986;&#19968;&#20123;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#20363;&#23376;&#26469;&#36827;&#34892;&#20559;&#22909;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#36845;&#20195;&#26041;&#27861;&#26469;&#21160;&#24577;&#30830;&#23450;&#35201;&#27880;&#37322;&#30340;&#23454;&#20363;&#25968;&#37327;&#12290;&#36890;&#36807;&#23545;&#25968;&#30334;&#20010;&#27169;&#22411;&#23545;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DiffUse&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#25968;&#37327;&#65292;&#26368;&#22810;&#21487;&#20943;&#23569;75%&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#35780;&#20272;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models. DiffUse reduces the required amount of preference annotations, thus saving valuable time and resources in performing evaluation. DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations -- by up to 75% -- while maintaining high evaluation 
&lt;/p&gt;</description></item><item><title>MAIDCRL &#26159;&#19968;&#31181;&#21322;&#38598;&#20013;&#24335;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;(AIMs)&#21644;&#21367;&#31215;&#23618;&#65292;&#25104;&#21151;&#22312; StarCraft &#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;CNN-enabled MAIDCRL &#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#24322;&#36136;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07890</link><description>&lt;p&gt;
MAIDCRL: &#21322;&#38598;&#20013;&#24335;&#22810;&#26234;&#33021;&#20307;&#24433;&#21709;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAIDCRL: Semi-centralized Multi-Agent Influence Dense-CNN Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07890
&lt;/p&gt;
&lt;p&gt;
MAIDCRL &#26159;&#19968;&#31181;&#21322;&#38598;&#20013;&#24335;&#23494;&#38598;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;(AIMs)&#21644;&#21367;&#31215;&#23618;&#65292;&#25104;&#21151;&#22312; StarCraft &#22810;&#26234;&#33021;&#20307;&#25361;&#25112;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;CNN-enabled MAIDCRL &#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#22312;&#22797;&#26434;&#30340;&#24322;&#36136;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#24335;&#20915;&#31574;&#21046;&#23450;&#20013;&#65292;&#23545;&#20110;&#21512;&#20316;&#21644;&#31454;&#20105;&#31995;&#32479;&#20013;&#30340;&#20132;&#20114;&#34892;&#20026;&#23398;&#20064;&#65292;&#23384;&#22312;&#22256;&#38590;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;MAIDRL&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#24378;&#20102;&#26234;&#33021;&#20307;&#24433;&#21709;&#22270;(AIMs)&#30340;&#21322;&#38598;&#20013;&#24335;&#23494;&#38598;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;StarCraft&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;(SMAC)&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#22810;&#26234;&#33021;&#20307;&#25511;&#21046;&#12290;&#26412;&#25991;&#22312;MAIDRL&#20013;&#25193;&#23637;&#20102;DenseNet&#65292;&#24341;&#20837;&#20102;&#21322;&#38598;&#20013;&#24335;&#22810;&#26234;&#33021;&#20307;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;(MAIDCRL)&#65292;&#36890;&#36807;&#23558;&#21367;&#31215;&#23618;&#34701;&#20837;&#28145;&#24230;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#22312;&#21516;&#36136;&#21644;&#24322;&#36136;&#22330;&#26223;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21551;&#29992;&#20102;CNN&#30340;MAIDCRL&#22312;&#23398;&#20064;&#24615;&#33021;&#19978;&#26174;&#33879;&#25552;&#39640;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;MAIDRL&#65292;&#29305;&#21035;&#26159;&#22312;&#26356;&#22797;&#26434;&#30340;&#24322;&#36136;SMAC&#22330;&#26223;&#19978;&#65292;&#23454;&#29616;&#20102;&#26356;&#24555;&#30340;&#23398;&#20064;&#36895;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#32479;&#35745;&#25968;&#25454;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
Distributed decision-making in multi-agent systems presents difficult challenges for interactive behavior learning in both cooperative and competitive systems. To mitigate this complexity, MAIDRL presents a semi-centralized Dense Reinforcement Learning algorithm enhanced by agent influence maps (AIMs), for learning effective multi-agent control on StarCraft Multi-Agent Challenge (SMAC) scenarios. In this paper, we extend the DenseNet in MAIDRL and introduce semi-centralized Multi-Agent Dense-CNN Reinforcement Learning, MAIDCRL, by incorporating convolutional layers into the deep model architecture, and evaluate the performance on both homogeneous and heterogeneous scenarios. The results show that the CNN-enabled MAIDCRL significantly improved the learning performance and achieved a faster learning rate compared to the existing MAIDRL, especially on more complicated heterogeneous SMAC scenarios. We further investigate the stability and robustness of our model. The statistics reflect tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#35770;&#25552;&#21462;&#26032;&#25351;&#26631;&#26469;&#25913;&#36827;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#25928;&#29575;&#24182;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07878</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#35770;&#25552;&#39640;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Using Graph Theory for Improving Machine Learning-based Detection of Cyber Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22270;&#35770;&#25552;&#21462;&#26032;&#25351;&#26631;&#26469;&#25913;&#36827;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#25928;&#29575;&#24182;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20837;&#20405;&#21644;&#32593;&#32476;&#23041;&#32961;&#30340;&#26089;&#26399;&#26816;&#27979;&#26159;&#32593;&#32476;&#23433;&#20840;&#30340;&#20027;&#35201;&#25903;&#26609;&#20043;&#19968;&#12290;&#20854;&#20013;&#19968;&#31181;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#20998;&#26512;&#32593;&#32476;&#27969;&#37327;&#65292;&#36890;&#36807;&#21306;&#20998;&#25915;&#20987;&#32773;&#21644;&#21512;&#27861;&#29992;&#25143;&#20197;&#26816;&#27979;&#21487;&#33021;&#23384;&#22312;&#30340;&#25915;&#20987;&#32773;&#12290;&#36890;&#24120;&#36890;&#36807;&#25910;&#38598;&#32593;&#32476;&#32456;&#31471;&#20043;&#38388;&#30340;&#27969;&#37327;&#65292;&#24182;&#20197;&#25968;&#25454;&#21253;&#25110;&#36830;&#25509;&#20026;&#22522;&#30784;&#36827;&#34892;&#20998;&#26512;&#26469;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#22788;&#29702;&#32593;&#32476;&#27969;&#37327;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#21462;&#19968;&#20123;&#26032;&#30340;&#25351;&#26631;&#65292;&#20197;&#20415;&#26356;&#39640;&#25928;&#22320;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#20811;&#26381;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#26032;&#25351;&#26631;&#22522;&#20110;&#22270;&#35770;&#65292;&#24182;&#32771;&#34385;&#25972;&#20010;&#32593;&#32476;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#20010;&#21035;&#25968;&#25454;&#21253;&#25110;&#36830;&#25509;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection of network intrusions and cyber threats is one of the main pillars of cybersecurity. One of the most effective approaches for this purpose is to analyze network traffic with the help of artificial intelligence algorithms, with the aim of detecting the possible presence of an attacker by distinguishing it from a legitimate user. This is commonly done by collecting the traffic exchanged between terminals in a network and analyzing it on a per-packet or per-connection basis. In this paper, we propose instead to perform pre-processing of network traffic under analysis with the aim of extracting some new metrics on which we can perform more efficient detection and overcome some limitations of classical approaches. These new metrics are based on graph theory, and consider the network as a whole, rather than focusing on individual packets or connections. Our approach is validated through experiments performed on publicly available data sets, from which it results that it can n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07876</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#26469;&#25913;&#36827;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Policy Improvement using Language Feedback Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#25913;&#36827;&#25919;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;&#24182;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#25105;&#20204;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#12289;&#27867;&#21270;&#24615;&#33021;&#21644;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#21453;&#39304;&#27169;&#22411;&#65288;LFMs&#65289;&#65292;&#29992;&#20110;&#22312;&#25351;&#20196;&#36981;&#24490;&#20013;&#35782;&#21035;&#26399;&#26395;&#30340;&#34892;&#20026;-&#26377;&#21161;&#20110;&#23454;&#29616;&#25351;&#20196;&#20013;&#25351;&#23450;&#20219;&#21153;&#30340;&#34892;&#21160;-&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#20026;&#20102;&#35757;&#32451;LFMs&#65292;&#25105;&#20204;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33719;&#21462;&#23545;&#35270;&#35273;&#36712;&#36857;&#36827;&#34892;&#35821;&#35328;&#25551;&#36848;&#30340;&#21453;&#39304;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20351;&#29992;LFMs&#35782;&#21035;&#26399;&#26395;&#27169;&#20223;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#22522;&#30784;&#29615;&#22659;&#65288;Touchdown&#65292;ScienceWorld&#21644;ALFWorld&#65289;&#19978;&#65292;&#22312;&#20219;&#21153;&#23436;&#25104;&#29575;&#19978;&#25913;&#21892;&#20102;&#24378;&#34892;&#20026;&#20811;&#38534;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#19982;LLMs&#30452;&#25509;&#39044;&#27979;&#34892;&#21160;&#30456;&#27604;&#65292;&#20351;&#29992;LFMs&#22312;LLM&#36755;&#20986;&#26631;&#35760;&#30340;&#25968;&#37327;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#12290;&#31532;&#19977;&#65292;LFMs&#36866;&#24212;&#26410;&#35265;&#29615;&#22659;&#65292;&#36890;&#36807;&#19968;&#36718;&#36866;&#24212;&#20351;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.5-12.0&#65285;&#12290;&#26368;&#21518;&#65292;&#21487;&#20197;&#20462;&#25913;LFM&#20197;&#25552;&#20379;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#26080;&#38656;&#24615;&#33021;&#25439;&#22833;&#65292;&#20174;&#32780;&#20801;&#35768;&#20154;&#31867;&#39564;&#35777;&#27169;&#20223;&#23398;&#20064;&#30340;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Language Feedback Models (LFMs) that identify desirable behaviour - actions that help achieve tasks specified in the instruction - for imitation learning in instruction following. To train LFMs, we obtain feedback from Large Language Models (LLMs) on visual trajectories verbalized to language descriptions. First, by using LFMs to identify desirable behaviour to imitate, we improve in task-completion rate over strong behavioural cloning baselines on three distinct language grounding environments (Touchdown, ScienceWorld, and ALFWorld). Second, LFMs outperform using LLMs as experts to directly predict actions, when controlling for the number of LLM output tokens. Third, LFMs generalize to unseen environments, improving task-completion rate by 3.5-12.0% through one round of adaptation. Finally, LFM can be modified to provide human-interpretable feedback without performance loss, allowing human verification of desirable behaviour for imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07875</link><description>&lt;p&gt;
&#32447;&#24615;&#20108;&#27425;&#25511;&#21046;&#20013;&#31574;&#30053;&#26799;&#24230;&#30340;&#38544;&#24615;&#20559;&#24046;&#65306;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#25511;&#21046;&#20013;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#38382;&#39064;&#65292;&#21457;&#29616;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#65292;&#20854;&#20013;&#19968;&#20123;&#22312;&#26410;&#35265;&#65288;&#27979;&#35797;&#65289;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#19981;&#28982;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#32463;&#24120;&#23637;&#29616;&#20986;&#19968;&#31181;&#38544;&#24615;&#20559;&#24046;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#31181;&#38544;&#24615;&#20559;&#24046;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#26368;&#20248;&#25511;&#21046;&#65288;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#21364;&#20102;&#35299;&#24471;&#36739;&#23569;&#12290;&#22312;&#37027;&#37324;&#65292;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#24212;&#29992;&#20110;&#31995;&#32479;&#30340;&#25511;&#21046;&#22120;&#34987;&#31216;&#20026;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19988;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#23398;&#20064;&#30340;&#25511;&#21046;&#22120;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#31243;&#24230;&#12290;&#26412;&#25991;&#22312;&#29702;&#35770;&#19978;&#30740;&#31350;&#20102;&#31574;&#30053;&#26799;&#24230;&#22312;&#23545;&#26410;&#35265;&#21021;&#22987;&#29366;&#24577;&#30340;&#22806;&#25512;&#26041;&#38754;&#30340;&#38544;&#24615;&#20559;&#24046;&#12290;&#25105;&#20204;&#20197;&#22522;&#26412;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#20026;&#37325;&#28857;&#65292;&#30830;&#31435;&#20102;&#22806;&#25512;&#31243;&#24230;&#21462;&#20915;&#20110;&#35757;&#32451;&#20013;&#31995;&#32479;&#22312;&#21021;&#22987;&#29366;&#24577;&#19979;&#24341;&#36215;&#30340;&#25506;&#32034;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern machine learning, models can often fit training data in numerous ways, some of which perform well on unseen (test) data, while others do not. Remarkably, in such cases gradient descent frequently exhibits an implicit bias that leads to excellent performance on unseen data. This implicit bias was extensively studied in supervised learning, but is far less understood in optimal control (reinforcement learning). There, learning a controller applied to a system via gradient descent is known as policy gradient, and a question of prime importance is the extent to which a learned controller extrapolates to unseen initial states. This paper theoretically studies the implicit bias of policy gradient in terms of extrapolation to unseen initial states. Focusing on the fundamental Linear Quadratic Regulator (LQR) problem, we establish that the extent of extrapolation depends on the degree of exploration induced by the system when commencing from initial states included in training. Exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PIVOT&#30340;&#26032;&#39062;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#30340;&#35270;&#35273;&#38382;&#31572;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;VLMs&#38382;&#39064;&#12290;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#22270;&#20687;&#34987;&#26631;&#27880;&#20026;VLMs&#21487;&#20197;&#21442;&#32771;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36873;&#25321;&#26368;&#20339;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;VLMs&#36827;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#20854;&#20182;&#31354;&#38388;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2402.07872</link><description>&lt;p&gt;
PIVOT: &#36845;&#20195;&#35270;&#35273;&#25552;&#31034;&#28608;&#21457;&#21487;&#25805;&#20316;&#30693;&#35782;&#29992;&#20110;VLMs
&lt;/p&gt;
&lt;p&gt;
PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PIVOT&#30340;&#26032;&#39062;&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36845;&#20195;&#30340;&#35270;&#35273;&#38382;&#31572;&#23558;&#20219;&#21153;&#36716;&#21270;&#20026;VLMs&#38382;&#39064;&#12290;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#22270;&#20687;&#34987;&#26631;&#27880;&#20026;VLMs&#21487;&#20197;&#21442;&#32771;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#36873;&#25321;&#26368;&#20339;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20351;VLMs&#36827;&#34892;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#20854;&#20182;&#31354;&#38388;&#20219;&#21153;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#26174;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20174;&#36923;&#36753;&#25512;&#29702;&#21040;&#35270;&#35273;&#29702;&#35299;&#12290;&#36825;&#20026;&#19982;&#19990;&#30028;&#36827;&#34892;&#26356;&#20016;&#23500;&#30340;&#20114;&#21160;&#25171;&#24320;&#20102;&#22823;&#38376;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;VLMs&#21482;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#32780;&#26426;&#22120;&#20154;&#25511;&#21046;&#21644;&#20854;&#20182;&#31354;&#38388;&#20219;&#21153;&#38656;&#35201;&#36755;&#20986;&#36830;&#32493;&#30340;&#22352;&#26631;&#65292;&#21160;&#20316;&#25110;&#36712;&#36857;&#12290;&#25105;&#20204;&#22914;&#20309;&#22312;&#19981;&#23545;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20351;VLMs&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#35774;&#32622;&#21602;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;VLMs&#35270;&#35273;&#25552;&#31034;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#36845;&#20195;&#35270;&#35273;&#20248;&#21270;&#25552;&#31034;&#65288;PIVOT&#65289;&#65292;&#23558;&#20219;&#21153;&#35270;&#20026;&#36845;&#20195;&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#22270;&#20687;&#34987;&#27880;&#37322;&#20026;VLMs&#21487;&#20197;&#21442;&#32771;&#30340;&#25552;&#26696;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#20363;&#22914;&#20505;&#36873;&#26426;&#22120;&#20154;&#21160;&#20316;&#12289;&#23450;&#20301;&#25110;&#36712;&#36857;&#65289;&#12290;&#28982;&#21518;&#65292;VLMs&#36873;&#25321;&#26368;&#20339;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#25552;&#26696;&#32463;&#36807;&#36845;&#20195;&#20248;&#21270;&#65292;&#20351;VLMs&#26368;&#32456;&#25214;&#21040;&#26368;&#20339;&#30340;&#21487;&#29992;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data?   In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31890;&#24230;&#20316;&#20026;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MoE&#27169;&#22411;&#22312;&#25928;&#26524;&#19978;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#22686;&#22823;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#20063;&#22312;&#22686;&#22823;&#12290;&#21516;&#26102;&#65292;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07871</link><description>&lt;p&gt;
&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Fine-Grained Mixture of Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#32454;&#31890;&#24230;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#31890;&#24230;&#20316;&#20026;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;MoE&#27169;&#22411;&#22312;&#25928;&#26524;&#19978;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#30340;&#22686;&#22823;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#20063;&#22312;&#22686;&#22823;&#12290;&#21516;&#26102;&#65292;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#24050;&#25104;&#20026;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26631;&#24230;&#29305;&#24615;&#65292;&#24182;&#32435;&#20837;&#20102;&#26356;&#24191;&#27867;&#30340;&#21464;&#37327;&#33539;&#22260;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#36229;&#21442;&#25968;&#65292;&#31216;&#20026;&#31890;&#24230;&#65292;&#36890;&#36807;&#35843;&#25972;&#31890;&#24230;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#32454;&#31890;&#24230;MoE&#30340;&#26631;&#24230;&#24459;&#65292;&#32771;&#34385;&#20102;&#35757;&#32451;&#26631;&#35760;&#25968;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#31890;&#24230;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#24459;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#32473;&#23450;&#35745;&#31639;&#39044;&#31639;&#19979;&#30340;&#26368;&#20339;&#35757;&#32451;&#37197;&#32622;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;MoE&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#23494;&#38598;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#32780;&#19988;&#36824;&#20984;&#26174;&#20102;&#22312;&#25193;&#22823;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#39044;&#31639;&#26102;&#65292;&#23494;&#38598;&#21644;MoE&#27169;&#22411;&#20043;&#38388;&#30340;&#25928;&#29575;&#24046;&#36317;&#22312;&#25193;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23558;MoE&#20013;&#19987;&#23478;&#30340;&#22823;&#23567;&#35774;&#32622;&#20026;&#19982;&#21069;&#39304;&#23618;&#30456;&#21516;&#30340;&#24120;&#35265;&#20570;&#27861;&#22312;&#20960;&#20046;&#20219;&#20309;&#35745;&#31639;&#39044;&#31639;&#19979;&#37117;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture of Experts (MoE) models have emerged as a primary solution for reducing the computational cost of Large Language Models. In this work, we analyze their scaling properties, incorporating an expanded range of variables. Specifically, we introduce a new hyperparameter, granularity, whose adjustment enables precise control over the size of the experts. Building on this, we establish scaling laws for fine-grained MoE, taking into account the number of training tokens, model size, and granularity. Leveraging these laws, we derive the optimal training configuration for a given computational budget. Our findings not only show that MoE models consistently outperform dense Transformers but also highlight that the efficiency gap between dense and MoE models widens as we scale up the model size and training budget. Furthermore, we demonstrate that the common practice of setting the size of experts in MoE to mirror the feed-forward layer is not optimal at almost any computational budget.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07868</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#23454;&#39564;&#35774;&#35745;&#30340;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nesting Particle Filters for Experimental Design in Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#38382;&#39064;&#65292;&#21033;&#29992;&#23884;&#22871;&#31890;&#23376;&#28388;&#27874;&#22120;&#21644;&#31435;&#20307;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#26469;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#38750;&#20132;&#25442;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#24418;&#24335;&#21270;&#20026;&#39118;&#38505;&#25935;&#24863;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20869;&#22806;SMC^2&#31639;&#27861;&#65292;&#20351;&#29992;&#23884;&#22871;&#39034;&#24207;&#33945;&#29305;&#21345;&#27931;&#65288;SMC&#65289;&#20272;&#35745;&#22120;&#26469;&#39044;&#27979;&#26399;&#26395;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#23558;&#20854;&#23884;&#20837;&#21040;&#31890;&#23376;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;pMCMC&#65289;&#26694;&#26550;&#20013;&#36827;&#34892;&#22522;&#20110;&#26799;&#24230;&#30340;&#31574;&#30053;&#20248;&#21270;&#12290;&#19982;&#26368;&#36817;&#20381;&#36182;&#20110;&#20559;&#20272;&#35745;&#22120;&#26469;&#25674;&#38144;&#20808;&#21069;&#23398;&#20064;&#35774;&#35745;&#31574;&#30053;&#30340;&#25104;&#26412;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#19968;&#32452;&#21160;&#24577;&#31995;&#32479;&#30340;&#25968;&#20540;&#39564;&#35777;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel approach to Bayesian Experimental Design (BED) for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC^2 algorithm that uses a nested sequential Monte Carlo (SMC) estimator of the expected information gain and embeds it into a particle Markov chain Monte Carlo (pMCMC) framework to perform gradient-based policy optimization. This is in contrast to recent approaches that rely on biased estimators of the expected information gain (EIG) to amortize the cost of experiments by learning a design policy in advance. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonedRAG&#30340;&#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36827;&#34892;&#25915;&#20987;&#21644;&#30772;&#22351;&#12290;</title><link>https://arxiv.org/abs/2402.07867</link><description>&lt;p&gt;
PoisonedRAG: &#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PoisonedRAG&#30340;&#30693;&#35782;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36827;&#34892;&#25915;&#20987;&#21644;&#30772;&#22351;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#29983;&#25104;&#33021;&#21147;&#32780;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#20204;&#20063;&#23384;&#22312;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#32570;&#20047;&#26368;&#26032;&#30340;&#30693;&#35782;&#21644;&#34394;&#26500;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;RAG&#20174;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#36755;&#20837;&#12290;&#20363;&#22914;&#65292;&#24403;&#30693;&#35782;&#25968;&#25454;&#24211;&#20013;&#21253;&#21547;&#20174;&#32500;&#22522;&#30334;&#31185;&#25910;&#38598;&#30340;&#25968;&#30334;&#19975;&#20010;&#25991;&#26412;&#26102;&#65292;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#21487;&#20197;&#26159;&#19982;&#32473;&#23450;&#38382;&#39064;&#22312;&#35821;&#20041;&#19978;&#26368;&#30456;&#20284;&#30340;&#21069;K&#20010;&#25991;&#26412;&#38598;&#12290;&#22240;&#27492;&#65292;LLM&#21487;&#20197;&#21033;&#29992;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#20316;&#20026;&#19978;&#19979;&#25991;&#20026;&#32473;&#23450;&#38382;&#39064;&#29983;&#25104;&#31572;&#26696;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#21892;RAG&#30340;&#20934;&#30830;&#24615;&#25110;&#25928;&#29575;&#65292;&#32780;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25506;&#32034;&#36739;&#23569;&#12290;&#25105;&#20204;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate those limitations. In particular, given a question, RAG retrieves relevant knowledge from a knowledge database to augment the input of the LLM. For instance, the retrieved knowledge could be a set of top-k texts that are most semantically similar to the given question when the knowledge database contains millions of texts collected from Wikipedia. As a result, the LLM could utilize the retrieved knowledge as the context to generate an answer for the given question. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. Particularly, we propose PoisonedRAG , a set of knowledge pois
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07865</link><description>&lt;p&gt;
&#36879;&#35270;VLMs&#65306;&#25506;&#32034;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#35774;&#35745;&#30340;&#20851;&#38190;&#31354;&#38388;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#26435;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#26465;&#20214;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#23545;&#35805;&#12289;&#22330;&#26223;&#29702;&#35299;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#65292;&#36825;&#31181;&#24212;&#29992;&#20419;&#20351;&#20102;&#20687;LLaVa&#12289;InstructBLIP&#21644;PaLI-3&#31561;&#35768;&#22810;&#26032;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#23613;&#31649;&#26377;&#36825;&#20040;&#22810;&#26032;&#30340;&#21457;&#24067;&#65292;&#20294;&#20851;&#20110;&#22270;&#20687;&#39044;&#22788;&#29702;&#12289;&#26550;&#26500;&#21644;&#20248;&#21270;&#30340;&#20851;&#38190;&#35774;&#35745;&#20915;&#31574;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#36825;&#19968;&#25361;&#25112;&#21448;&#22240;&#32570;&#20047;&#23458;&#35266;&#12289;&#19968;&#33268;&#30340;&#35780;&#20272;&#32780;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;&#19968;&#22871;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#35270;&#35273;&#38382;&#31572;&#12289;&#20174;&#35821;&#35328;&#20013;&#23450;&#20301;&#29289;&#20307;&#20197;&#21450;&#25506;&#32034;&#24187;&#35273;&#31561;&#23646;&#24615;&#30340;&#30446;&#26631;&#25361;&#25112;&#38598;&#65292;&#36825;&#20123;&#35780;&#20272;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;VLM&#33021;&#21147;&#30340;&#31934;&#32454;&#12289;&#20934;&#30830;&#30340;&#35265;&#35299;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20851;&#38190;&#30340;&#35774;&#35745;&#36724;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#21644;&#20351;&#29992;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07862</link><description>&lt;p&gt;
AI&#22686;&#24378;&#39044;&#27979;&#65306;LLM&#21161;&#25163;&#25552;&#39640;&#20154;&#31867;&#39044;&#27979;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#21161;&#25163;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#19982;&#29978;&#33267;&#36229;&#36807;&#20154;&#31867;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#39044;&#27979;&#20219;&#21153;&#20013;&#22686;&#24378;&#21028;&#26029;&#21147;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;GPT-4-Turbo&#21161;&#25163;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#24314;&#35758;&#65288;&#36229;&#32423;&#39044;&#27979;&#65289;&#65292;&#21478;&#19968;&#20010;&#26088;&#22312;&#36807;&#20110;&#33258;&#20449;&#21644;&#22522;&#26412;&#27010;&#29575;&#24573;&#35270;&#12290;&#21442;&#19982;&#32773;&#65288;N = 991&#65289;&#21487;&#20197;&#22312;&#25972;&#20010;&#30740;&#31350;&#36807;&#31243;&#20013;&#21672;&#35810;&#20182;&#20204;&#34987;&#20998;&#37197;&#30340;LLM&#21161;&#25163;&#65292;&#32780;&#23545;&#29031;&#32452;&#21017;&#20351;&#29992;&#19968;&#20010;&#36739;&#20302;&#32423;&#21035;&#30340;&#27169;&#22411;&#65288;DaVinci-003&#65289;&#65292;&#19981;&#25552;&#20379;&#30452;&#25509;&#30340;&#39044;&#27979;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#27880;&#20876;&#20998;&#26512;&#26174;&#31034;&#65292;LLM&#22686;&#24378;&#26174;&#33879;&#25552;&#39640;&#20102;23%&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#20219;&#20309;&#19968;&#31181;&#21161;&#25163;&#31867;&#22411;&#65292;&#30456;&#27604;&#20110;&#23545;&#29031;&#32452;&#12290;&#36825;&#31181;&#25913;&#36827;&#21457;&#29983;&#22312;&#36229;&#32423;&#39044;&#27979;&#21161;&#25163;&#22312;&#39044;&#27979;&#20013;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#26126;&#22686;&#24378;&#30340;&#25928;&#30410;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;&#27169;&#22411;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice ('superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Explora
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#29305;&#24449;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#26631;&#20934;&#33647;&#29289;&#30103;&#31243;&#19981;&#21453;&#24212;&#30340;&#24773;&#32490;&#38556;&#30861;&#24739;&#32773;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26356;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#27835;&#30103;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2402.07858</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#29305;&#24449;&#29992;&#20110;&#35782;&#21035;&#24773;&#32490;&#38556;&#30861;&#27835;&#30103;&#20013;&#30340;&#33647;&#29289;&#31867;&#21035;&#21644;&#38750;&#21453;&#24212;&#32773;
&lt;/p&gt;
&lt;p&gt;
Multiscale Neuroimaging Features for the Identification of Medication Class and Non-Responders in Mood Disorder Treatment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22810;&#23610;&#24230;&#31070;&#32463;&#24433;&#20687;&#29305;&#24449;&#30340;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35782;&#21035;&#23545;&#26631;&#20934;&#33647;&#29289;&#30103;&#31243;&#19981;&#21453;&#24212;&#30340;&#24773;&#32490;&#38556;&#30861;&#24739;&#32773;&#65292;&#20026;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#26356;&#21487;&#38752;&#21644;&#39640;&#25928;&#30340;&#27835;&#30103;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#38556;&#30861;&#30340;&#20020;&#24202;&#27835;&#30103;&#20013;&#65292;&#24739;&#32773;&#21576;&#29616;&#30340;&#22797;&#26434;&#34892;&#20026;&#30151;&#29366;&#20197;&#21450;&#23545;&#29305;&#23450;&#33647;&#29289;&#31867;&#21035;&#30340;&#24739;&#32773;&#21453;&#24212;&#30340;&#21464;&#24322;&#24615;&#65292;&#20351;&#29992;&#26631;&#20934;&#30340;&#35786;&#26029;&#21644;&#22788;&#26041;&#26041;&#27861;&#26102;&#21487;&#33021;&#20250;&#23548;&#33268;&#25552;&#20379;&#24555;&#36895;&#21487;&#38752;&#30340;&#27835;&#30103;&#22256;&#38590;&#30340;&#24773;&#20917;&#12290;&#36234;&#26469;&#36234;&#22810;&#22320;&#23558;&#31070;&#32463;&#24433;&#20687;&#25195;&#25551;&#21644;&#34893;&#29983;&#29289;&#32435;&#20837;&#20020;&#24202;&#36807;&#31243;&#20013;&#65292;&#26377;&#26395;&#20943;&#36731;&#22260;&#32469;&#27492;&#36807;&#31243;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22914;&#26524;&#31070;&#32463;&#29305;&#24449;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#23545;&#26631;&#20934;&#30340;&#25239;&#25233;&#37057;&#33647;&#29289;&#25110;&#24773;&#32490;&#31283;&#23450;&#21058;&#30103;&#31243;&#19981;&#21453;&#24212;&#30340;&#24739;&#32773;&#65292;&#20020;&#24202;&#21307;&#29983;&#21487;&#33021;&#20250;&#36873;&#25321;&#36991;&#20813;&#28459;&#38271;&#19988;&#26377;&#21103;&#20316;&#29992;&#30340;&#27835;&#30103;&#65292;&#23547;&#27714;&#19981;&#21516;&#30340;&#26356;&#26377;&#25928;&#30340;&#30103;&#31243;&#65292;&#32780;&#21542;&#21017;&#21487;&#33021;&#27809;&#26377;&#32771;&#34385;&#21040;&#12290;&#20043;&#21069;&#65292;&#26377;&#20851;&#30456;&#20851;&#31070;&#32463;&#24433;&#20687;&#29305;&#24449;&#23548;&#20986;&#30340;&#26041;&#27861;&#20165;&#22312;&#25968;&#25454;&#30340;&#19968;&#20010;&#23610;&#24230;&#19978;&#24037;&#20316;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#21487;&#29992;&#20110;&#20020;&#24202;&#20915;&#31574;&#30340;&#20449;&#24687;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the clinical treatment of mood disorders, the complex behavioral symptoms presented by patients and variability of patient response to particular medication classes can create difficulties in providing fast and reliable treatment when standard diagnostic and prescription methods are used. Increasingly, the incorporation of physiological information such as neuroimaging scans and derivatives into the clinical process promises to alleviate some of the uncertainty surrounding this process. Particularly, if neural features can help to identify patients who may not respond to standard courses of anti-depressants or mood stabilizers, clinicians may elect to avoid lengthy and side-effect-laden treatments and seek out a different, more effective course that might otherwise not have been under consideration. Previously, approaches for the derivation of relevant neuroimaging features work at only one scale in the data, potentially limiting the depth of information available for clinical decis
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#30340;&#39044;&#27979;&#27604;NWP&#39044;&#25253;&#21644;&#22522;&#20110;&#25345;&#32493;&#24615;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.07851</link><description>&lt;p&gt;
&#23558;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#19982;NCEP-NWP&#39044;&#25253;&#30456;&#27604;&#36739;&#65292;&#39044;&#27979;&#21360;&#24230;&#23395;&#39118;&#38477;&#38632;&#30340;&#25216;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07851
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#30340;&#39044;&#27979;&#27604;NWP&#39044;&#25253;&#21644;&#22522;&#20110;&#25345;&#32493;&#24615;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#21360;&#24230;&#30340;&#22235;&#20010;&#23395;&#39118;&#26376;&#20221;&#65292;&#22312;&#19968;&#22825;&#21644;&#19977;&#22825;&#20043;&#21069;&#39044;&#27979;&#38477;&#38632;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;IMD&#30340;&#21360;&#24230;&#21382;&#21490;&#26085;&#38477;&#27700;&#25968;&#25454;&#35757;&#32451;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#26102;&#38388;&#27573;&#20026;1901&#24180;&#33267;2022&#24180;&#65292;&#31354;&#38388;&#20998;&#36776;&#29575;&#20026;1&#176;&#215;1&#176;&#12290;&#36825;&#19982;&#26469;&#33258;NCEP&#65288;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#39044;&#25253;&#20013;&#24515;&#65289;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#39044;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35813;&#25968;&#25454;&#21487;&#29992;&#20110;2011&#24180;&#33267;2022&#24180;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20840;&#22269;&#33539;&#22260;&#20998;&#26512;&#65292;&#24182;&#20998;&#21035;&#20998;&#26512;&#20102;&#21360;&#24230;&#19968;&#20123;&#20154;&#21475;&#26368;&#22810;&#30340;&#22478;&#24066;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#26159;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#30340;&#39044;&#27979;&#27604;NWP&#39044;&#25253;&#21644;&#22522;&#20110;&#25345;&#32493;&#24615;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;&#25105;&#20204;&#30340;&#39044;&#27979;&#30456;&#27604;&#65292;NCEP-NWP&#27169;&#22411;&#30340;&#39044;&#27979;&#22312;&#21333;&#26085;&#39044;&#27979;&#20013;&#30340;&#35823;&#24046;&#32422;&#39640;&#20986;34%&#65292;&#22312;&#19977;&#22825;&#39044;&#27979;&#20013;&#30340;&#35823;&#24046;&#39640;&#20986;68%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Simila
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36880;&#27493;&#20998;&#37197;&#31867;&#21035;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#21270;&#28508;&#22312;&#36830;&#32493;&#27169;&#22411;&#26102;&#30340;&#33293;&#20837;&#21644;&#26679;&#26412;&#25130;&#26029;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#21305;&#37197;&#20998;&#35299;&#31163;&#25955;&#20998;&#24067;&#30340;&#27979;&#22320;&#32447;&#27969;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#35813;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#38750;&#20998;&#35299;&#31163;&#25955;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.07846</link><description>&lt;p&gt;
&#21033;&#29992;&#22312;&#36171;&#20540;&#27969;&#24418;&#19978;&#30340;E-&#27979;&#22320;&#32447;&#27969;&#21305;&#37197;&#29983;&#25104;&#31163;&#25955;&#32852;&#21512;&#20998;&#24067;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Discrete Joint Distributions by E-Geodesic Flow Matching on Assignment Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36880;&#27493;&#20998;&#37197;&#31867;&#21035;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#21270;&#28508;&#22312;&#36830;&#32493;&#27169;&#22411;&#26102;&#30340;&#33293;&#20837;&#21644;&#26679;&#26412;&#25130;&#26029;&#31561;&#38382;&#39064;&#12290;&#36890;&#36807;&#21305;&#37197;&#20998;&#35299;&#31163;&#25955;&#20998;&#24067;&#30340;&#27979;&#22320;&#32447;&#27969;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35757;&#32451;&#35813;&#27169;&#22411;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#34920;&#31034;&#22797;&#26434;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#38750;&#20998;&#35299;&#31163;&#25955;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#24402;&#19968;&#21270;&#27969;&#22312;&#20998;&#35299;&#31163;&#25955;&#24230;&#37327;&#23376;&#27969;&#24418;&#19978;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36880;&#27493;&#23545;&#31867;&#21035;&#36827;&#34892;&#20998;&#37197;&#65292;&#36991;&#20813;&#20102;&#31163;&#25955;&#21270;&#28508;&#22312;&#36830;&#32493;&#27169;&#22411;&#26102;&#30340;&#33293;&#20837;&#12289;&#26679;&#26412;&#25130;&#26029;&#31561;&#38382;&#39064;&#12290;&#23558;&#23376;&#27969;&#24418;&#23884;&#20837;&#21040;&#25152;&#26377;&#32852;&#21512;&#31163;&#25955;&#20998;&#24067;&#21644;&#25968;&#25454;&#39537;&#21160;&#24179;&#22343;&#30340;&#20803;&#21333;&#32431;&#24418;&#20013;&#65292;&#21487;&#20197;&#36817;&#20284;&#34920;&#31034;&#33021;&#22815;&#34920;&#31034;&#32467;&#26500;&#21270;&#31163;&#25955;&#25968;&#25454;&#30340;&#22797;&#26434;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#30340;&#19968;&#33324;&#38750;&#20998;&#35299;&#31163;&#25955;&#20998;&#24067;&#12290;&#36890;&#36807;&#21305;&#37197;&#20998;&#35299;&#31163;&#25955;&#20998;&#24067;&#30340;&#27979;&#22320;&#32447;&#27969;&#65292;&#28436;&#31034;&#20102;&#35813;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#21508;&#31181;&#23454;&#39564;&#31361;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel generative model for discrete distributions based on continuous normalizing flows on the submanifold of factorizing discrete measures. Integration of the flow gradually assigns categories and avoids issues of discretizing the latent continuous model like rounding, sample truncation etc. General non-factorizing discrete distributions capable of representing complex statistical dependencies of structured discrete data, can be approximated by embedding the submanifold into a the meta-simplex of all joint discrete distributions and data-driven averaging. Efficient training of the generative model is demonstrated by matching the flow of geodesics of factorizing discrete distributions. Various experiments underline the approach's broad applicability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07845</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Investigation into Using Unsupervised Metrics to Optimise GNNs for Node Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#26080;&#30417;&#30563;&#24230;&#37327;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#36827;&#34892;&#33410;&#28857;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#19988;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#35774;&#35745;&#21512;&#25104;&#23454;&#39564;&#30340;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#29305;&#24449;&#21644;&#36830;&#25509;&#20449;&#24687;&#30340;&#20108;&#20803;&#24615;&#26469;&#35757;&#32451;&#20197;&#26816;&#27979;&#22270;&#20013;&#30340;&#31038;&#21306;&#12290;&#30446;&#21069;&#65292;&#20248;&#21270;GNN&#30340;&#24120;&#35265;&#26041;&#27861;&#26159;&#20351;&#29992;&#19982;&#22522;&#20934;&#20540;&#30340;&#27604;&#36739;&#26469;&#36827;&#34892;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#20248;&#21270;&#27169;&#22359;&#24615;&#65292;&#21487;&#20197;&#20351;&#29992;GNN&#23558;&#33410;&#28857;&#32858;&#31867;&#25104;&#31038;&#21306;&#65292;&#32780;&#26080;&#38656;&#19982;&#22522;&#20934;&#20540;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#27169;&#22359;&#24615;&#26159;&#19968;&#31181;&#22270;&#20998;&#21306;&#36136;&#37327;&#24230;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20063;&#21487;&#20197;&#29992;&#20110;&#20248;&#21270;&#21516;&#26102;&#32534;&#30721;&#29305;&#24449;&#30340;GNN&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#26080;&#30417;&#30563;&#24230;&#37327;&#24615;&#33021;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#22522;&#20934;&#20540;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25506;&#31350;&#20026;&#20160;&#20040;&#21487;&#20197;&#20351;&#29992;&#27169;&#22359;&#24615;&#20248;&#21270;GNN&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20123;&#21512;&#25104;&#23454;&#39564;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21512;&#25104;&#22270;&#34920;&#26126;&#20854;&#22312;&#19981;&#21516;&#12289;&#38543;&#26426;&#21644;&#38646;&#20449;&#24687;&#31354;&#38388;&#20998;&#21306;&#20013;&#30340;&#24403;&#21069;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) can be trained to detect communities within a graph by learning from the duality of feature and connectivity information. Currently, the common approach for optimisation of GNNs is to use comparisons to ground-truth for hyperparameter tuning and model selection. In this work, we show that nodes can be clustered into communities with GNNs by solely optimising for modularity, without any comparison to ground-truth. Although modularity is a graph partitioning quality metric, we show that this can be used to optimise GNNs that also encode features without a drop in performance. We take it a step further and also study whether the unsupervised metric performance can predict ground-truth performance. To investigate why modularity can be used to optimise GNNs, we design synthetic experiments that show the limitations of this approach. The synthetic graphs are created to highlight current capabilities in distinct, random and zero information space partitions in att
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intra-Fusion&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#27010;&#24565;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#21098;&#26525;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#24674;&#22797;&#20934;&#30830;&#24230;&#24182;&#36991;&#20813;&#31934;&#35843;&#24037;&#20316;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.07839</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#20803;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Towards Meta-Pruning via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intra-Fusion&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#27010;&#24565;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#20803;&#21098;&#26525;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#24674;&#22797;&#20934;&#30830;&#24230;&#24182;&#36991;&#20813;&#31934;&#35843;&#24037;&#20316;&#65292;&#21516;&#26102;&#20855;&#26377;&#39640;&#25928;&#21644;&#26377;&#21069;&#26223;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21098;&#26525;&#36890;&#24120;&#20381;&#36182;&#20110;&#35782;&#21035;&#21644;&#20002;&#24323;&#19981;&#37325;&#35201;&#30340;&#31070;&#32463;&#20803;&#65292;&#36825;&#31181;&#20570;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#65292;&#38656;&#35201;&#38543;&#21518;&#30340;&#31934;&#35843;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intra-Fusion&#30340;&#26032;&#26041;&#27861;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#21098;&#26525;&#33539;&#24335;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#35774;&#35745;&#26377;&#24847;&#20041;&#30340;&#31070;&#32463;&#20803;&#37325;&#35201;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;Intra-Fusion&#37325;&#26032;&#23450;&#20041;&#20102;&#19978;&#23618;&#21098;&#26525;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#34701;&#21512;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#21033;&#29992;&#32473;&#23450;&#30340;&#19981;&#21487;&#30693;&#37325;&#35201;&#24615;&#24230;&#37327;&#65292;&#24471;&#21040;&#20102;&#26356;&#26377;&#25928;&#30340;&#31232;&#30095;&#27169;&#22411;&#34920;&#31034;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38656;&#35201;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#24674;&#22797;&#65292;&#20351;&#20854;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#39640;&#25928;&#19988;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structural pruning of neural networks conventionally relies on identifying and discarding less important neurons, a practice often resulting in significant accuracy loss that necessitates subsequent fine-tuning efforts. This paper introduces a novel approach named Intra-Fusion, challenging this prevailing pruning paradigm. Unlike existing methods that focus on designing meaningful neuron importance metrics, Intra-Fusion redefines the overlying pruning procedure. Through utilizing the concepts of model fusion and Optimal Transport, we leverage an agnostically given importance metric to arrive at a more effective sparse model representation. Notably, our approach achieves substantial accuracy recovery without the need for resource-intensive fine-tuning, making it an efficient and promising tool for neural network compression.   Additionally, we explore how fusion can be added to the pruning process to significantly decrease the training time while maintaining competitive performance. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#25552;&#20986;&#20102;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#40784;&#26465;&#20214;&#20998;&#24067;&#26469;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26102;&#21464;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07834</link><description>&lt;p&gt;
&#22312;&#26102;&#38388;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#65306;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Generalizing across Temporal Domains with Koopman Operators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07834
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#25552;&#20986;&#20102;&#24211;&#26222;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#40784;&#26465;&#20214;&#20998;&#24067;&#26469;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#12290;&#36890;&#36807;&#20351;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#25105;&#20204;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#26102;&#21464;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39046;&#22495;&#27867;&#21270;&#30340;&#39046;&#22495;&#20013;&#65292;&#26500;&#24314;&#19968;&#31181;&#33021;&#22815;&#22312;&#27809;&#26377;&#30446;&#26631;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36866;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#24403;&#32771;&#34385;&#21040;&#39046;&#22495;&#20043;&#38388;&#30340;&#28436;&#21270;&#21160;&#24577;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23545;&#22522;&#30784;&#27867;&#21270;&#29702;&#35770;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#36890;&#36807;&#23545;&#40784;&#26465;&#20214;&#20998;&#24067;&#26469;&#20943;&#23567;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#36890;&#36807;&#24212;&#29992;&#24211;&#26222;&#26364;&#31070;&#32463;&#31639;&#23376;&#26469;&#35299;&#20915;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#20851;&#38190;&#21160;&#26426;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#26102;&#38388;&#24211;&#26222;&#26364;&#32593;&#32476;&#65288;TKNets&#65289;&#12290;&#36890;&#36807;&#20351;&#29992;&#24211;&#26222;&#26364;&#31639;&#23376;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#20351;&#29992;&#24211;&#26222;&#26364;&#29702;&#35770;&#30340;&#21407;&#21017;&#26469;&#22788;&#29702;&#26102;&#38388;&#39046;&#22495;&#27867;&#21270;&#20013;&#36935;&#21040;&#30340;&#26102;&#21464;&#20998;&#24067;&#65292;&#20854;&#20013;&#27979;&#37327;&#20989;&#25968;&#34987;&#29992;&#26469;&#24314;&#31435;&#32447;&#24615;&#36807;&#28193;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of domain generalization, the task of constructing a predictive model capable of generalizing to a target domain without access to target data remains challenging. This problem becomes further complicated when considering evolving dynamics between domains. While various approaches have been proposed to address this issue, a comprehensive understanding of the underlying generalization theory is still lacking. In this study, we contribute novel theoretic results that aligning conditional distribution leads to the reduction of generalization bounds. Our analysis serves as a key motivation for solving the Temporal Domain Generalization (TDG) problem through the application of Koopman Neural Operators, resulting in Temporal Koopman Networks (TKNets). By employing Koopman Operators, we effectively address the time-evolving distributions encountered in TDG using the principles of Koopman theory, where measurement functions are sought to establish linear transition relations betwe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#31867;&#21035;&#39044;&#27979;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#27010;&#24565;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#39044;&#27979;&#20445;&#35777;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07821</link><description>&lt;p&gt;
&#35770;&#35745;&#31639;&#26377;&#25928;&#30340;&#22810;&#31867;&#21035;&#26657;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Computationally Efficient Multi-Class Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07821
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#31867;&#21035;&#39044;&#27979;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#27010;&#24565;&#65292;&#24182;&#19988;&#32473;&#20986;&#20102;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#24378;&#22823;&#30340;&#39044;&#27979;&#20445;&#35777;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#22810;&#31867;&#21035;&#26631;&#35760;&#38382;&#39064;&#65292;&#20854;&#20013;&#26631;&#35760;&#21487;&#20197;&#22312;[1,k]&#33539;&#22260;&#20869;&#21462;&#20540;&#65292;&#32780;&#39044;&#27979;&#22120;&#39044;&#27979;&#30340;&#26159;&#26631;&#35760;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#22522;&#30784;&#38382;&#39064;&#65306;&#26159;&#21542;&#23384;&#22312;&#22810;&#31867;&#21035;&#26657;&#20934;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#32473;&#20986;&#23545;&#26377;&#24847;&#20041;&#30340;&#39044;&#27979;&#30340;&#24378;&#22823;&#20445;&#35777;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#19979;&#23454;&#29616;&#65311;&#20808;&#21069;&#30340;&#26657;&#20934;&#27010;&#24565;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#34920;&#36798;&#33021;&#21147;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#65306;&#23427;&#20204;&#35201;&#20040;&#22312;k&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#19978;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#35201;&#20040;&#38656;&#35201;&#27714;&#35299;&#35745;&#31639;&#38590;&#39064;&#65292;&#35201;&#20040;&#32473;&#20986;&#30340;&#20445;&#35777;&#30456;&#24403;&#24369;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#25152;&#26377;&#36825;&#20123;&#26399;&#26395;&#30340;&#26657;&#20934;&#27010;&#24565;&#65306;&#25105;&#20204;&#22312;&#22810;&#31867;&#21035;&#39044;&#27979;&#20013;&#21046;&#23450;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#26032;&#30340;&#37325;&#26032;&#26657;&#20934;&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#20010;&#23450;&#20041;&#19979;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#22797;&#26434;&#24230;&#26657;&#20934;&#39044;&#27979;&#22120;&#12290;&#25237;&#24433;&#24179;&#28369;&#26657;&#20934;&#20026;&#22810;&#31867;&#21035;&#39044;&#27979;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider a multi-class labelling problem, where the labels can take values in $[k]$, and a predictor predicts a distribution over the labels. In this work, we study the following foundational question: Are there notions of multi-class calibration that give strong guarantees of meaningful predictions and can be achieved in time and sample complexities polynomial in $k$? Prior notions of calibration exhibit a tradeoff between computational efficiency and expressivity: they either suffer from having sample complexity exponential in $k$, or needing to solve computationally intractable problems, or give rather weak guarantees.   Our main contribution is a notion of calibration that achieves all these desiderata: we formulate a robust notion of projected smooth calibration for multi-class predictions, and give new recalibration algorithms for efficiently calibrating predictors under this definition with complexity polynomial in $k$. Projected smooth calibration gives strong guarantees for al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07812</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#24605;&#32500;&#36807;&#31243;&#20316;&#20026;&#24207;&#21015;&#20915;&#31574;&#21046;&#23450;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Thought Process as Sequential Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07812
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;&#65288;RATP&#65289;&#36890;&#36807;&#22810;&#27493;&#20915;&#31574;&#21644;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#20197;&#21450;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38544;&#31169;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#22788;&#29702;&#38271;&#25991;&#26412;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#22788;&#29702;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#36741;&#21161;&#20154;&#31867;&#24182;&#23637;&#29616;&#20986;"&#26234;&#33021;&#30340;&#28779;&#33457;"&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20960;&#20010;&#24320;&#25918;&#25361;&#25112;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#65306;&#22914;&#23545;&#38544;&#31169;&#30340;&#20851;&#27880;&#12289;&#20542;&#21521;&#20110;&#20135;&#29983;&#24187;&#35273;&#12289;&#38590;&#20197;&#22788;&#29702;&#38271;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#24605;&#32500;&#36807;&#31243;(RATP)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#33719;&#21462;&#22806;&#37096;&#30693;&#35782;&#65292;RATP&#23558;LLM&#30340;&#24605;&#32771;&#29983;&#25104;&#36807;&#31243;&#23450;&#24335;&#20026;&#22810;&#27493;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#36825;&#31181;&#24605;&#32771;&#36807;&#31243;&#65292;RATP&#21033;&#29992;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65292;&#24182;&#23398;&#20064;&#20102;&#19968;&#20010;Q&#20540;&#20272;&#35745;&#22120;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#22312;&#22788;&#29702;&#20855;&#26377;&#31169;&#20154;&#25968;&#25454;&#30340;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;LLM&#35757;&#32451;&#26041;&#27861;&#21463;&#21040;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;RATP&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;50%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated their strong ability to assist people and show "sparks of intelligence". However, several open challenges hinder their wider application: such as concerns over privacy, tendencies to produce hallucinations, and difficulties in handling long contexts. In this work, we address those challenges by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimize such a thought process, RATP leverages Monte-Carlo Tree Search, and learns a Q-value estimator that permits cost-efficient inference. In addressing the task of question-answering with private data, where ethical and security concerns limit LLM training methods, RATP achieves a 50% improvement over existing in-context retrieval-augmented language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#26368;&#22823;&#29109;&#28304;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#20445;&#30041;&#19981;&#30830;&#23450;&#24615;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#28304;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20999;&#29255;-&#29926;&#30707;&#22374;&#26031;&#22374;&#36317;&#31163;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#36827;&#34892;&#34913;&#37327;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#30340;&#27169;&#25311;&#22120;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#26356;&#39640;&#29109;&#30340;&#28304;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07808</link><description>&lt;p&gt;
Sourcerer: &#22522;&#20110;&#26679;&#26412;&#30340;&#26368;&#22823;&#29109;&#28304;&#20998;&#24067;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#26412;&#30340;&#26368;&#22823;&#29109;&#28304;&#20998;&#24067;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#20808;&#20445;&#30041;&#19981;&#30830;&#23450;&#24615;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#28304;&#20998;&#24067;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20999;&#29255;-&#29926;&#30707;&#22374;&#26031;&#22374;&#36317;&#31163;&#23545;&#25968;&#25454;&#38598;&#21644;&#27169;&#25311;&#36827;&#34892;&#34913;&#37327;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#20284;&#28982;&#20989;&#25968;&#30340;&#27169;&#25311;&#22120;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#26356;&#39640;&#29109;&#30340;&#28304;&#20998;&#24067;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#24314;&#27169;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#20272;&#35745;&#19982;&#35266;&#27979;&#25968;&#25454;&#38598;&#19968;&#33268;&#30340;&#21442;&#25968;&#20998;&#24067;&#65292;&#34987;&#31216;&#20026;&#28304;&#20998;&#24067;&#20272;&#35745;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#38382;&#39064;&#21487;&#33021;&#26159;&#19981;&#36866;&#23450;&#30340;&#65292;&#22240;&#20026;&#35768;&#22810;&#19981;&#21516;&#30340;&#28304;&#20998;&#24067;&#21487;&#33021;&#20135;&#29983;&#30456;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19968;&#33268;&#30340;&#27169;&#25311;&#32467;&#26524;&#12290;&#20026;&#20102;&#22312;&#20247;&#22810;&#21516;&#26679;&#26377;&#25928;&#30340;&#28304;&#20013;&#20570;&#20986;&#26377;&#21407;&#21017;&#30340;&#36873;&#25321;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30446;&#26631;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21363;&#20248;&#20808;&#20445;&#30041;&#23613;&#21487;&#33021;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#26679;&#26412;&#65292;&#21033;&#29992;&#20999;&#29255;-&#29926;&#30707;&#22374;&#26031;&#22374;&#36317;&#31163;&#26469;&#34913;&#37327;&#25968;&#25454;&#38598;&#19982;&#27169;&#25311;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#20855;&#26377;&#38590;&#20197;&#22788;&#29702;&#30340;&#20284;&#28982;&#20989;&#25968;&#30340;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#26356;&#39640;&#29109;&#30340;&#28304;&#20998;&#24067;&#65292;&#32780;&#19981;&#29306;&#29298;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25512;&#26029;&#28304;&#20998;&#24067;...
&lt;/p&gt;
&lt;p&gt;
Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations. To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods. We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations. Finally, to demonstrate the utility of our approach, we infer source distri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#20013;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#65292;&#27493;&#39588;&#25968;&#37327;&#38656;&#35201;&#36229;&#36807;$d^{5/2}/\varepsilon$&#30340;&#38454;&#25968;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#30446;&#26631;&#20998;&#24067;&#25509;&#36817;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.07802</link><description>&lt;p&gt;
&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#31181;&#19968;&#33268;&#24615;&#35757;&#32451;&#25968;&#23398;&#29702;&#35770;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards a mathematical theory for consistency training in diffusion models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#38754;&#21521;&#25193;&#25955;&#27169;&#22411;&#20013;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#65292;&#27493;&#39588;&#25968;&#37327;&#38656;&#35201;&#36229;&#36807;$d^{5/2}/\varepsilon$&#30340;&#38454;&#25968;&#65292;&#33021;&#22815;&#29983;&#25104;&#19982;&#30446;&#26631;&#20998;&#24067;&#25509;&#36817;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#20943;&#23569;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#38454;&#27573;&#30340;&#39640;&#35745;&#31639;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#21333;&#27493;&#37319;&#26679;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#23454;&#35777;&#24615;&#33021;&#12290;&#19968;&#33268;&#24615;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#34987;&#25972;&#21512;&#36827;&#26469;&#65292;&#35797;&#22270;&#35757;&#32451;&#19968;&#31995;&#21015;&#30340;&#19968;&#33268;&#24615;&#20989;&#25968;&#65292;&#33021;&#22815;&#23558;&#25193;&#25955;&#36807;&#31243;&#20013;&#30340;&#20219;&#20309;&#26102;&#38388;&#27493;&#39588;&#30340;&#20219;&#20309;&#28857;&#26144;&#23556;&#22238;&#20854;&#36215;&#22987;&#28857;&#12290;&#23613;&#31649;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20851;&#20110;&#19968;&#33268;&#24615;&#35757;&#32451;&#30340;&#20840;&#38754;&#29702;&#35770;&#29702;&#35299;&#36824;&#26159;&#24456;&#38590;&#24471;&#21040;&#30340;&#12290;&#26412;&#25991;&#23545;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20026;&#20102;&#22312;&#20998;&#24067;&#20013;&#29983;&#25104;&#19982;&#30446;&#26631;&#22312;$\varepsilon$&#25509;&#36817;&#30340;&#26679;&#26412;&#65288;&#36890;&#36807;&#26576;&#31181;Wasserstein&#24230;&#37327;&#34913;&#37327;&#65289;&#65292;&#19968;&#33268;&#24615;&#23398;&#20064;&#20013;&#30340;&#27493;&#39588;&#25968;&#37327;&#38656;&#35201;&#36229;&#36807;$d^{5/2}/\varepsilon$&#30340;&#38454;&#25968;&#65292;&#20854;&#20013;$d$&#26159;&#25968;&#25454;&#32500;&#24230;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#19968;&#33268;&#24615;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency models, which were proposed to mitigate the high computational overhead during the sampling phase of diffusion models, facilitate single-step sampling while attaining state-of-the-art empirical performance. When integrated into the training phase, consistency models attempt to train a sequence of consistency functions capable of mapping any point at any time step of the diffusion process to its starting point. Despite the empirical success, a comprehensive theoretical understanding of consistency training remains elusive. This paper takes a first step towards establishing theoretical underpinnings for consistency models. We demonstrate that, in order to generate samples within $\varepsilon$ proximity to the target in distribution (measured by some Wasserstein metric), it suffices for the number of steps in consistency learning to exceed the order of $d^{5/2}/\varepsilon$, with $d$ the data dimension. Our theory offers rigorous insights into the validity and efficacy of cons
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#32473;&#20986;&#38382;&#39064;&#21442;&#25968;&#30340;&#31895;&#30053;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#26368;&#20248;&#35843;&#21442;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#24182;&#19988;&#22312;&#26377;&#30028;&#30340;&#20248;&#21270;&#39046;&#22495;&#20013;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26080;&#30028;&#22495;&#20013;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.07793</link><description>&lt;p&gt;
&#26080;&#35843;&#21442;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tuning-Free Stochastic Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#21442;&#30340;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#32473;&#20986;&#38382;&#39064;&#21442;&#25968;&#30340;&#31895;&#30053;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#26368;&#20248;&#35843;&#21442;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#24182;&#19988;&#22312;&#26377;&#30028;&#30340;&#20248;&#21270;&#39046;&#22495;&#20013;&#35777;&#26126;&#20102;&#27492;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26080;&#30028;&#22495;&#20013;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20351;&#24471;&#35843;&#21442;&#30340;&#25104;&#26412;&#36234;&#26469;&#36234;&#39640;&#26114;&#12290;&#36825;&#23548;&#33268;&#20102;&#38656;&#35201;&#33021;&#22815;&#21363;&#26102;&#33258;&#25105;&#35843;&#25972;&#30340;&#31639;&#27861;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#8220;&#26080;&#35843;&#21442;&#8221;&#31639;&#27861;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#65292;&#21363;&#21482;&#32473;&#20986;&#38382;&#39064;&#21442;&#25968;&#30340;&#31895;&#30053;&#25552;&#31034;&#21363;&#21487;&#19982;&#26368;&#20248;&#35843;&#21442;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#35823;&#24046;&#20026;&#23545;&#25968;&#22810;&#39033;&#24335;&#22240;&#23376;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#33021;&#22815;&#19982;&#26368;&#20248;&#35843;&#21442;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#12290;&#24403;&#20248;&#21270;&#30340;&#22495;&#26159;&#26377;&#30028;&#30340;&#26102;&#20505;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35843;&#21442;&#33258;&#30001;&#19982;SGD&#30340;&#21305;&#37197;&#26159;&#21487;&#33021;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#20960;&#20010;&#29616;&#26377;&#31639;&#27861;&#23454;&#29616;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#20248;&#21270;&#30340;&#22495;&#26159;&#26080;&#30028;&#30340;&#26102;&#20505;&#65292;&#23545;&#20110;&#26368;&#23567;&#21270;&#20984;&#24179;&#28369;&#25110;&#32773;Lipschitz&#20989;&#25968;&#30340;&#20219;&#21153;&#65292;&#26080;&#35843;&#21442;&#20248;&#21270;&#26159;&#19981;&#21487;&#33021;&#30340;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#26080;&#30028;&#22495;&#20013;&#65292;&#20309;&#31181;&#24773;&#20917;&#19979;&#21487;&#20197;&#23454;&#29616;&#26080;&#35843;&#21442;&#20248;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340; DoG &#21644; DoWG &#31639;&#27861;&#22312;&#22122;&#22768;&#20998;&#24067;&#36275;&#22815;&#26102;&#26159;&#26080;&#35843;&#21442;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. This creates a need for algorithms that can tune themselves on-the-fly. We formalize the notion of "tuning-free" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. We consider in particular algorithms that can match optimally-tuned Stochastic Gradient Descent (SGD). When the domain of optimization is bounded, we show tuning-free matching of SGD is possible and achieved by several existing algorithms. We prove that for the task of minimizing a convex and smooth or Lipschitz function over an unbounded domain, tuning-free optimization is impossible. We discuss conditions under which tuning-free optimization is possible even over unbounded domains. In particular, we show that the recently proposed DoG and DoWG algorithms are tuning-free when the noise distribution is suf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;NVIDIA FLARE&#30340;&#32852;&#37030;&#23398;&#20064;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21644;&#21033;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#21644;&#20840;&#38754;&#30417;&#30563;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29983;&#29289;&#33647;&#29289;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07792</link><description>&lt;p&gt;
&#20351;&#29992;NVIDIA FLARE&#26469;&#22686;&#24378;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Empowering Federated Learning for Massive Models with NVIDIA FLARE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;NVIDIA FLARE&#30340;&#32852;&#37030;&#23398;&#20064;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#21644;&#21033;&#29992;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#21644;&#20840;&#38754;&#30417;&#30563;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29983;&#29289;&#33647;&#29289;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#26377;&#25928;&#22788;&#29702;&#21644;&#21033;&#29992;&#25968;&#25454;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#37117;&#26159;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#12289;&#30417;&#31649;&#12289;&#22320;&#32536;&#25919;&#27835;&#12289;&#29256;&#26435;&#38382;&#39064;&#20197;&#21450;&#31227;&#21160;&#22823;&#22411;&#25968;&#25454;&#38598;&#25152;&#38656;&#30340;&#24040;&#22823;&#24037;&#20316;&#37327;&#31561;&#21508;&#31181;&#22240;&#32032;&#65292;&#24517;&#35201;&#30340;&#25968;&#25454;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#38598;&#20013;&#23384;&#20648;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;NVIDIA FLARE&#23454;&#29616;&#30340;&#32852;&#37030;&#23398;&#20064;&#22914;&#20309;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#20855;&#22791;&#26131;&#20110;&#25193;&#23637;&#38598;&#25104;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29983;&#29289;&#33647;&#29289;&#24212;&#29992;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#21644;&#20840;&#38754;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving landscape of artificial intelligence (AI) and large language models (LLMs), handling and leveraging data effectively has become a critical challenge. Most state-of-the-art machine learning algorithms are data-centric. However, as the lifeblood of model performance, necessary data cannot always be centralized due to various factors such as privacy, regulation, geopolitics, copyright issues, and the sheer effort required to move vast datasets. In this paper, we explore how federated learning enabled by NVIDIA FLARE can address these challenges with easy and scalable integration capabilities, enabling parameter-efficient and full supervised fine-tuning of LLMs for natural language processing and biopharmaceutical applications to enhance their accuracy and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#26657;&#20934;&#24230;&#37327;&#23545;&#20998;&#25968;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23616;&#37096;&#26657;&#20934;&#20998;&#25968;&#65292;&#24182;&#25512;&#24191;&#20102;&#23616;&#37096;&#22238;&#24402;&#20316;&#20026;&#26377;&#25928;&#30340;&#37325;&#26032;&#26657;&#20934;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#20108;&#20803;&#20998;&#31867;&#22120;&#22312;&#25935;&#24863;&#20915;&#31574;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.07790</link><description>&lt;p&gt;
&#20174;&#19981;&#30830;&#23450;&#24615;&#21040;&#31934;&#30830;&#24615;&#65306;&#36890;&#36807;&#26657;&#20934;&#25552;&#21319;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
From Uncertainty to Precision: Enhancing Binary Classifier Performance through Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#26657;&#20934;&#24230;&#37327;&#23545;&#20998;&#25968;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#23616;&#37096;&#26657;&#20934;&#20998;&#25968;&#65292;&#24182;&#25512;&#24191;&#20102;&#23616;&#37096;&#22238;&#24402;&#20316;&#20026;&#26377;&#25928;&#30340;&#37325;&#26032;&#26657;&#20934;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#20108;&#20803;&#20998;&#31867;&#22120;&#22312;&#25935;&#24863;&#20915;&#31574;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20934;&#30830;&#24615;&#31561;&#24230;&#37327;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24230;&#37327;&#24120;&#24120;&#24573;&#35270;&#20102;&#27169;&#22411;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#25935;&#24863;&#20915;&#31574;&#39046;&#22495;&#65288;&#22914;&#37329;&#34701;&#25110;&#21307;&#30103;&#20445;&#20581;&#65289;&#26102;&#12290;&#37492;&#20110;&#27169;&#22411;&#39044;&#27979;&#30340;&#20998;&#25968;&#36890;&#24120;&#34987;&#35270;&#20026;&#20107;&#20214;&#27010;&#29575;&#65292;&#26657;&#20934;&#23545;&#20110;&#20934;&#30830;&#35299;&#37322;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21508;&#31181;&#26657;&#20934;&#24230;&#37327;&#23545;&#20998;&#25968;&#22833;&#30495;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24230;&#37327;&#65292;&#23616;&#37096;&#26657;&#20934;&#20998;&#25968;&#12290;&#36890;&#36807;&#27604;&#36739;&#37325;&#26032;&#26657;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#20513;&#23548;&#20351;&#29992;&#23616;&#37096;&#22238;&#24402;&#65292;&#24378;&#35843;&#20854;&#20316;&#20026;&#26377;&#25928;&#37325;&#26032;&#26657;&#20934;&#24037;&#20855;&#21644;&#26356;&#24179;&#28369;&#21487;&#35270;&#21270;&#30340;&#21452;&#37325;&#20316;&#29992;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#20123;&#21457;&#29616;&#22312;&#19968;&#20010;&#30495;&#23454;&#22330;&#26223;&#20013;&#65292;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21644;&#22238;&#24402;&#22120;&#39044;&#27979;&#20449;&#29992;&#36829;&#32422;&#65292;&#24182;&#21516;&#26102;&#27979;&#37327;&#26657;&#20934;&#24615;&#33021;&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#26657;&#20934;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The assessment of binary classifier performance traditionally centers on discriminative ability using metrics, such as accuracy. However, these metrics often disregard the model's inherent uncertainty, especially when dealing with sensitive decision-making domains, such as finance or healthcare. Given that model-predicted scores are commonly seen as event probabilities, calibration is crucial for accurate interpretation. In our study, we analyze the sensitivity of various calibration measures to score distortions and introduce a refined metric, the Local Calibration Score. Comparing recalibration methods, we advocate for local regressions, emphasizing their dual role as effective recalibration tools and facilitators of smoother visualizations. We apply these findings in a real-world scenario using Random Forest classifier and regressor to predict credit default while simultaneously measuring calibration during performance optimization.
&lt;/p&gt;</description></item><item><title>HYPO&#26159;&#19968;&#20010;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#30340;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#31163;&#32676;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07785</link><description>&lt;p&gt;
HYPO&#65306;&#36229;&#29699;&#38754;&#31163;&#32676;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HYPO: Hyperspherical Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07785
&lt;/p&gt;
&lt;p&gt;
HYPO&#26159;&#19968;&#20010;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#30340;&#24341;&#23548;&#65292;&#25552;&#39640;&#20102;&#31163;&#32676;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32676;&#65288;OOD&#65289;&#27867;&#21270;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#19968;&#28857;&#21487;&#33021;&#20174;&#26681;&#26412;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23398;&#20064;&#36328;&#19981;&#21516;&#39046;&#22495;&#25110;&#29615;&#22659;&#30340;&#19981;&#21464;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;HYPO&#65288;&#36229;&#29699;&#38754;OOD&#27867;&#21270;&#65289;&#65292;&#23427;&#33021;&#22815;&#35777;&#26126;&#22312;&#36229;&#29699;&#38754;&#31354;&#38388;&#20013;&#23398;&#20064;&#22495;&#19981;&#21464;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#36229;&#29699;&#38754;&#23398;&#20064;&#31639;&#27861;&#26159;&#26681;&#25454;&#20869;&#31867;&#21464;&#21270;&#21644;&#38388;&#31867;&#20998;&#31163;&#21407;&#21017;&#36827;&#34892;&#24341;&#23548;&#30340;&#65292;&#30830;&#20445;&#26469;&#33258;&#21516;&#19968;&#31867;&#21035;&#30340;&#29305;&#24449;&#65288;&#36328;&#19981;&#21516;&#35757;&#32451;&#39046;&#22495;&#65289;&#19982;&#20854;&#31867;&#21035;&#21407;&#22411;&#32039;&#23494;&#23545;&#40784;&#65292;&#32780;&#19981;&#21516;&#31867;&#21035;&#30340;&#21407;&#22411;&#20043;&#38388;&#21017;&#34987;&#26368;&#22823;&#21270;&#22320;&#20998;&#31163;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#30340;&#21407;&#22411;&#23398;&#20064;&#30446;&#26631;&#22914;&#20309;&#25913;&#21892;OOD&#27867;&#21270;&#30028;&#38480;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#36890;&#36807;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;OOD&#22522;&#20934;&#30340;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) generalization is critical for machine learning models deployed in the real world. However, achieving this can be fundamentally challenging, as it requires the ability to learn invariant features across different domains or environments. In this paper, we propose a novel framework HYPO (HYPerspherical OOD generalization) that provably learns domain-invariant representations in a hyperspherical space. In particular, our hyperspherical learning algorithm is guided by intra-class variation and inter-class separation principles -- ensuring that features from the same class (across different training domains) are closely aligned with their class prototypes, while different class prototypes are maximally separated. We further provide theoretical justifications on how our prototypical learning objective improves the OOD generalization bound. Through extensive experiments on challenging OOD benchmarks, we demonstrate that our approach outperforms competitive baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;IR&#24863;&#30693;&#30340;ECO&#26102;&#24207;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38376;&#23610;&#23544;&#35843;&#25972;&#32416;&#27491;&#30001;IR&#38477;&#20302;&#24341;&#36215;&#30340;&#26102;&#24207;&#36864;&#21270;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.07781</link><description>&lt;p&gt;
&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;IR&#24863;&#30693;&#30340;ECO&#26102;&#24207;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
IR-Aware ECO Timing Optimization Using Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;IR&#24863;&#30693;&#30340;ECO&#26102;&#24207;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38376;&#23610;&#23544;&#35843;&#25972;&#32416;&#27491;&#30001;IR&#38477;&#20302;&#24341;&#36215;&#30340;&#26102;&#24207;&#36864;&#21270;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#36816;&#34892;&#26102;&#38388;&#19978;&#37117;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26202;&#26399;&#38454;&#27573;&#30340;&#24037;&#31243;&#21464;&#26356;&#35746;&#21333;&#65288;ECOs&#65289;&#36890;&#36807;&#26368;&#23567;&#30340;&#35774;&#35745;&#20462;&#22797;&#26469;&#20174;&#36807;&#22810;&#30340;IR&#38477;&#20302;&#23548;&#33268;&#30340;&#26102;&#24207;&#20559;&#31227;&#20013;&#24674;&#22797;&#12290;&#26412;&#25991;&#23558;IR&#24863;&#30693;&#30340;&#26102;&#24207;&#20998;&#26512;&#21644;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#34892;ECO&#26102;&#24207;&#20248;&#21270;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#29289;&#29702;&#35774;&#35745;&#21644;&#21151;&#32791;&#32593;&#26684;&#32508;&#21512;&#20043;&#21518;&#36816;&#34892;&#65292;&#24182;&#36890;&#36807;&#38376;&#23610;&#23544;&#35843;&#25972;&#32416;&#27491;&#30001;IR&#38477;&#20302;&#24341;&#36215;&#30340;&#26102;&#24207;&#36864;&#21270;&#12290;&#23427;&#23558;&#25289;&#26684;&#26391;&#26085;&#26494;&#24347;&#65288;LR&#65289;&#25216;&#26415;&#34701;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;RL&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#35757;&#32451;&#19968;&#20010;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;R-GCN&#65289;&#20195;&#29702;&#65292;&#25353;&#39034;&#24207;&#35843;&#25972;&#38376;&#23610;&#23544;&#20197;&#20462;&#22797;&#26102;&#24207;&#36829;&#35268;&#12290;R-GCN&#20195;&#29702;&#20248;&#20110;&#20256;&#32479;&#30340;&#20165;&#20351;&#29992;LR&#30340;&#31639;&#27861;&#65306;&#22312;&#24320;&#25918;&#24335;45nm&#24037;&#33402;&#20013;&#65292;&#23427;&#23558;&#24310;&#36831;-&#38754;&#31215;&#26435;&#34913;&#26354;&#32447;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#21521;&#24038;&#31227;&#21160;&#65292;&#24182;&#36890;&#36807;&#22312;&#31561;&#36136;&#37327;&#26102;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24555;&#36895;&#25512;&#29702;&#65292;&#33410;&#30465;&#36816;&#34892;&#26102;&#38388;&#12290;RL&#27169;&#22411;&#21487;&#22312;&#26102;&#24207;&#35268;&#33539;&#38388;&#36716;&#31227;&#65292;&#24182;&#21487;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#25110;&#24494;&#35843;&#22312;&#26410;&#35265;&#35774;&#35745;&#19978;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Engineering change orders (ECOs) in late stages make minimal design fixes to recover from timing shifts due to excessive IR drops. This paper integrates IR-drop-aware timing analysis and ECO timing optimization using reinforcement learning (RL). The method operates after physical design and power grid synthesis, and rectifies IR-drop-induced timing degradation through gate sizing. It incorporates the Lagrangian relaxation (LR) technique into a novel RL framework, which trains a relational graph convolutional network (R-GCN) agent to sequentially size gates to fix timing violations. The R-GCN agent outperforms a classical LR-only algorithm: in an open 45nm technology, it (a) moves the Pareto front of the delay-area tradeoff curve to the left and (b) saves runtime over the classical method by running fast inference using trained models at iso-quality. The RL model is transferable across timing specifications, and transferable to unseen designs with zero-shot learning or fine tuning.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26367;&#20195;&#20302;&#32423;&#21035;&#30340;&#20248;&#21270;&#23618;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20248;&#25191;&#34892;&#22120;&#21644;&#25511;&#21046;&#35774;&#35745;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#19988;&#25928;&#26524;&#33391;&#22909;&#22320;&#30830;&#23450;&#26368;&#20248;&#25191;&#34892;&#22120;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.07763</link><description>&lt;p&gt;
&#22810;&#32423;&#20248;&#21270;&#25511;&#21046;&#19982;&#31070;&#32463;&#20195;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-level Optimal Control with Neural Surrogate Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07763
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#27169;&#22411;&#26367;&#20195;&#20302;&#32423;&#21035;&#30340;&#20248;&#21270;&#23618;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#20248;&#21270;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20248;&#25191;&#34892;&#22120;&#21644;&#25511;&#21046;&#35774;&#35745;&#38382;&#39064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#19988;&#25928;&#26524;&#33391;&#22909;&#22320;&#30830;&#23450;&#26368;&#20248;&#25191;&#34892;&#22120;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#26368;&#20248;&#25191;&#34892;&#22120;&#21644;&#25511;&#21046;&#35774;&#35745;&#20316;&#20026;&#19968;&#20010;&#22810;&#32423;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#25191;&#34892;&#22120;&#35774;&#35745;&#22522;&#20110;&#30456;&#20851;&#26368;&#20248;&#38381;&#29615;&#30340;&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#25191;&#34892;&#22120;&#23454;&#29616;&#65292;&#35780;&#20272;&#26368;&#20248;&#38381;&#29615;&#26159;&#19968;&#39033;&#35745;&#31639;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20195;&#29702;&#26469;&#26367;&#20195;&#36825;&#19968;&#36739;&#20302;&#32423;&#21035;&#30340;&#20248;&#21270;&#23618;&#27425;&#65292;&#20174;&#32780;&#20351;&#24471;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#21644;&#26080;&#26799;&#24230;&#30340;&#19968;&#33268;&#24615;&#20248;&#21270;&#26041;&#27861;&#24555;&#36895;&#30830;&#23450;&#26368;&#20248;&#25191;&#34892;&#22120;&#35774;&#35745;&#12290;&#25152;&#25552;&#20986;&#30340;&#20195;&#29702;&#27169;&#22411;&#21644;&#20248;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#22312;&#19982;&#28909;&#25511;&#21046;&#20013;&#26368;&#20248;&#25191;&#34892;&#22120;&#20301;&#32622;&#30456;&#20851;&#30340;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal actuator and control design is studied as a multi-level optimisation problem, where the actuator design is evaluated based on the performance of the associated optimal closed loop. The evaluation of the optimal closed loop for a given actuator realisation is a computationally demanding task, for which the use of a neural network surrogate is proposed. The use of neural network surrogates to replace the lower level of the optimisation hierarchy enables the use of fast gradient-based and gradient-free consensus-based optimisation methods to determine the optimal actuator design. The effectiveness of the proposed surrogate models and optimisation methods is assessed in a test related to optimal actuator location for heat control.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#31639;&#27861;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.07762</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#31232;&#30095;&#29305;&#23450;&#32972;&#26223;&#19979;&#22240;&#26524;&#31995;&#32479;&#30340;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable Structure Learning for Sparse Context-Specific Causal Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07762
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#28151;&#21512;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#31639;&#27861;&#21644;&#31232;&#30095;&#24615;&#20551;&#35774;&#23454;&#29616;&#21487;&#25193;&#23637;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#34920;&#31034;&#20849;&#21516;&#20998;&#24067;&#20998;&#31867;&#21464;&#37327;&#20043;&#38388;&#29305;&#23450;&#32972;&#26223;&#19979;&#20851;&#31995;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#32467;&#26500;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22823;&#37327;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#30340;&#23384;&#22312;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21463;&#21040;&#38480;&#21046;&#65292;&#32780;&#22522;&#20110;&#32422;&#26463;&#30340;&#26041;&#27861;&#27604;&#32422;&#26463;DAG&#23398;&#20064;&#31639;&#27861;&#26356;&#23481;&#26131;&#20986;&#38169;&#65292;&#22240;&#20026;&#24517;&#39035;&#27979;&#35797;&#26356;&#22810;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#31639;&#27861;&#26469;&#23398;&#20064;&#29305;&#23450;&#32972;&#26223;&#27169;&#22411;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;&#25968;&#30334;&#20010;&#21464;&#37327;&#65292;&#24182;&#19988;&#27979;&#35797;&#30340;&#32422;&#26463;&#19981;&#22810;&#20110;&#26631;&#20934;DAG&#23398;&#20064;&#31639;&#27861;&#12290;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#39034;&#24207;&#30340;MCMC&#31639;&#27861;&#21644;&#31867;&#20284;&#20110;DAG&#27169;&#22411;&#24120;&#29992;&#30340;&#31232;&#30095;&#24615;&#20551;&#35774;&#65292;&#23454;&#29616;&#20102;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;Alon&#21644;Balogh&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#32463;&#36807;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#31034;&#20363;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several approaches to graphically representing context-specific relations among jointly distributed categorical variables have been proposed, along with structure learning algorithms. While existing optimization-based methods have limited scalability due to the large number of context-specific models, the constraint-based methods are more prone to error than even constraint-based DAG learning algorithms since more relations must be tested. We present a hybrid algorithm for learning context-specific models that scales to hundreds of variables while testing no more constraints than standard DAG learning algorithms. Scalable learning is achieved through a combination of an order-based MCMC algorithm and sparsity assumptions analogous to those typically invoked for DAG models. To implement the method, we solve a special case of an open problem recently posed by Alon and Balogh. The method is shown to perform well on synthetic data and real world examples, in terms of both accuracy and scal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#20013;&#30340;&#36880;&#27493;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#22270;&#23548;&#33322;&#27169;&#22411;&#26469;&#25506;&#32034;&#36880;&#27493;&#25512;&#29702;&#30340;&#24213;&#23618;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20960;&#20010;&#20851;&#38190;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.07757</link><description>&lt;p&gt;
&#23545;Transformer&#20013;&#36880;&#27493;&#25512;&#29702;&#30340;&#29702;&#35299;: &#19968;&#20010;&#21512;&#25104;&#22270;&#23548;&#33322;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;Transformer&#20013;&#30340;&#36880;&#27493;&#25512;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#25104;&#22270;&#23548;&#33322;&#27169;&#22411;&#26469;&#25506;&#32034;&#36880;&#27493;&#25512;&#29702;&#30340;&#24213;&#23618;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#35813;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#20960;&#20010;&#20851;&#38190;&#29616;&#35937;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36880;&#27493;&#25512;&#29702;&#21327;&#35758;&#65292;&#22914;scratchpads&#21644;chain-of-thought&#65292;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36739;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#24110;&#21161;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#20123;&#21327;&#35758;&#22312;&#24615;&#33021;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#20294;&#36880;&#27493;&#25512;&#29702;&#30340;&#24213;&#23618;&#26426;&#21046;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30740;&#31350;&#33258;&#22238;&#24402;Transformer&#27169;&#22411;&#65292;&#35813;&#20219;&#21153;&#20307;&#29616;&#20102;&#38382;&#39064;&#30340;&#22810;&#27493;&#24615;&#36136;&#65292;&#20854;&#20013;&#36880;&#27493;&#25512;&#29702;&#36890;&#24120;&#26368;&#26377;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#22270;&#23548;&#33322;&#38382;&#39064;&#65292;&#27169;&#22411;&#30340;&#20219;&#21153;&#26159;&#22312;&#22270;&#19978;&#20174;&#36215;&#22987;&#33410;&#28857;&#21040;&#30446;&#26631;&#33410;&#28857;&#30340;&#36335;&#24452;&#19978;&#36827;&#34892;&#36941;&#21382;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#32463;&#39564;&#37325;&#29616;&#21644;&#20998;&#26512;&#35266;&#23519;&#21040;&#30340;&#20960;&#20010;&#29616;&#35937;&#65306;(i)&#36880;&#27493;&#25512;&#29702;&#25512;&#29702;&#38388;&#38553;&#65292;&#25105;&#20204;&#21457;&#29616;&#20854;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#32467;&#26500;&#65307;(ii)&#22312;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#22810;&#26679;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#38543;&#30528;&#37319;&#26679;&#28201;&#24230;&#30340;&#21464;&#21270;&#65307;(iii)&#27169;&#22411;&#22312;&#36755;&#20986;&#20013;&#30340;&#31616;&#21333;&#24615;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#22522;&#20110;&#20540;&#30340;&#31639;&#27861;&#65292;&#28151;&#21512;Q&#20989;&#25968;&#65288;MQF&#65289;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#36830;&#32493;&#21160;&#20316;&#39046;&#22495;&#20013;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#21160;&#20316;&#26469;&#35299;&#20915;&#30495;&#23454;&#22238;&#25253;&#20272;&#35745;&#21644;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07752</link><description>&lt;p&gt;
&#28151;&#21512;Q&#20989;&#25968;&#65306;&#25512;&#21160;&#36830;&#32493;&#21160;&#20316;&#39046;&#22495;&#21512;&#20316;MARL&#20013;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Mixed Q-Functionals: Advancing Value-Based Methods in Cooperative MARL with Continuous Action Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#22522;&#20110;&#20540;&#30340;&#31639;&#27861;&#65292;&#28151;&#21512;Q&#20989;&#25968;&#65288;MQF&#65289;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#36830;&#32493;&#21160;&#20316;&#39046;&#22495;&#20013;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#21160;&#20316;&#26469;&#35299;&#20915;&#30495;&#23454;&#22238;&#25253;&#20272;&#35745;&#21644;&#23616;&#37096;&#26368;&#20248;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#21160;&#20316;&#39046;&#22495;&#20013;&#39640;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#38382;&#39064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#22522;&#20110;&#20540;&#30340;&#31639;&#27861;&#22312;&#24212;&#29992;&#20110;&#31163;&#25955;&#21160;&#20316;&#39046;&#22495;&#26102;&#20855;&#26377;&#26679;&#26412;&#25928;&#29575;&#20248;&#21183;&#65292;&#20294;&#22312;&#22788;&#29702;&#36830;&#32493;&#21160;&#20316;&#26102;&#36890;&#24120;&#25928;&#29575;&#20302;&#19979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#31574;&#30053;&#22411;&#31639;&#27861;&#36890;&#36807;&#21033;&#29992;&#35780;&#35770;&#23478;&#32593;&#32476;&#26469;&#24341;&#23548;&#23398;&#20064;&#36807;&#31243;&#21644;&#31283;&#23450;&#26799;&#24230;&#20272;&#35745;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#20272;&#35745;&#30495;&#23454;&#22238;&#25253;&#21644;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#23548;&#33268;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#19988;&#36890;&#24120;&#27425;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36828;&#31163;&#36827;&#19968;&#27493;&#22686;&#24378;&#35780;&#35770;&#23478;&#32593;&#32476;&#30340;&#36235;&#21183;&#65292;&#19987;&#27880;&#20110;&#36890;&#36807;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#21160;&#20316;&#26469;&#25913;&#36827;&#22522;&#20110;&#20540;&#30340;&#26041;&#27861;&#22312;&#22810;&#26234;&#33021;&#20307;&#36830;&#32493;&#21160;&#20316;&#39046;&#22495;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#22522;&#20110;&#20540;&#30340;&#31639;&#27861;&#65292;&#28151;&#21512;Q&#20989;&#25968;&#65288;MQF&#65289;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;Q&#20989;&#25968;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#23558;&#20854;&#29366;&#24577;&#36716;&#21270;&#20026;&#22522;&#24213;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tackling multi-agent learning problems efficiently is a challenging task in continuous action domains. While value-based algorithms excel in sample efficiency when applied to discrete action domains, they are usually inefficient when dealing with continuous actions. Policy-based algorithms, on the other hand, attempt to address this challenge by leveraging critic networks for guiding the learning process and stabilizing the gradient estimation. The limitations in the estimation of true return and falling into local optima in these methods result in inefficient and often sub-optimal policies. In this paper, we diverge from the trend of further enhancing critic networks, and focus on improving the effectiveness of value-based methods in multi-agent continuous domains by concurrently evaluating numerous actions. We propose a novel multi-agent value-based algorithm, Mixed Q-Functionals (MQF), inspired from the idea of Q-Functionals, that enables agents to transform their states into basis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#20195;&#22823;&#20247;&#24066;&#22330;&#24212;&#29992;&#20013;&#65292;&#38543;&#26102;&#38388;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36890;&#36807;&#30740;&#31350;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#65292;&#37327;&#21270;&#20102;&#36825;&#31181;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#24182;&#36890;&#36807;Rashomon&#38598;&#21512;&#26469;&#20998;&#26512;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#39044;&#26399;&#27969;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.07745</link><description>&lt;p&gt;
&#20351;&#29992;&#19968;&#32452;&#22909;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#24615;&#23458;&#25143;&#27969;&#22833;
&lt;/p&gt;
&lt;p&gt;
Predictive Churn with the Set of Good Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#20195;&#22823;&#20247;&#24066;&#22330;&#24212;&#29992;&#20013;&#65292;&#38543;&#26102;&#38388;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36890;&#36807;&#30740;&#31350;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#65292;&#37327;&#21270;&#20102;&#36825;&#31181;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#24182;&#36890;&#36807;Rashomon&#38598;&#21512;&#26469;&#20998;&#26512;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#39044;&#26399;&#27969;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#20247;&#24066;&#22330;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#20250;&#38543;&#26102;&#38388;&#36827;&#34892;&#26356;&#26032;&#12290;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#23613;&#31649;&#25972;&#20307;&#24615;&#33021;&#22312;&#25552;&#21319;&#65292;&#20294;&#36825;&#20123;&#26356;&#26032;&#21487;&#33021;&#20250;&#20197;&#19981;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#25913;&#21464;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#37327;&#21270;&#27169;&#22411;&#26356;&#26032;&#21069;&#21518;&#30340;&#19981;&#31283;&#23450;&#39044;&#27979;&#25968;&#37327;&#26469;&#34913;&#37327;&#39044;&#27979;&#24615;&#27969;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#25928;&#24212;&#65292;&#21363;&#22312;&#19968;&#32452;&#25509;&#36817;&#26368;&#20248;&#27169;&#22411;&#65288;Rashomon&#38598;&#21512;&#65289;&#20013;&#23384;&#22312;&#20914;&#31361;&#39044;&#27979;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20256;&#32479;&#30340;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#24230;&#37327;&#26469;&#30740;&#31350;&#36825;&#32452;&#28508;&#22312;&#27169;&#22411;&#30340;&#39044;&#26399;&#27969;&#22833;&#65292;&#21363;&#21487;&#33021;&#29992;&#20110;&#26367;&#25442;&#22522;&#32447;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#32473;&#20986;&#20102;&#27169;&#22411;&#38598;&#20869;Rashomon&#38598;&#21512;&#20043;&#38388;&#39044;&#26399;&#27969;&#22833;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;Rashomon&#38598;&#21512;&#34920;&#24449;&#20102;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#39044;&#26399;&#27969;&#22833;&#65292;&#32467;&#21512;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models in modern mass-market applications are often updated over time. One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways. In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn. In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set). We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment. We present theoretical results on the expected churn between models within the Rashomon set from different perspectives. And we characterize expected churn over model updates via the Rashomon set, pairing our analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07744</link><description>&lt;p&gt;
&#23454;&#29616;&#26234;&#33021;&#20307;&#12289;&#20154;&#31867;&#21644;&#29615;&#22659;&#20043;&#38388;&#30340;&#32479;&#19968;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Towards Unified Alignment Between Agents, Humans, and Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017; ($\mathbf{UA}^2$)&#65292;&#26088;&#22312;&#23454;&#29616;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#30340;&#32479;&#19968;&#23545;&#40784;&#65292;&#25552;&#20986;&#20102;&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24555;&#36895;&#36827;&#23637;&#23548;&#33268;&#20102;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#32321;&#33635;&#65292;&#36825;&#20123;&#26234;&#33021;&#20307;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#36827;&#34892;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#29615;&#22659;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#24403;&#22312;&#22797;&#26434;&#12289;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#26102;&#65292;&#26234;&#33021;&#20307;&#30340;&#25928;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#19968;&#23545;&#40784;&#21407;&#21017;&#65292;&#21363;&#21516;&#26102;&#23545;&#40784;&#26234;&#33021;&#20307;&#19982;&#20154;&#31867;&#24847;&#22270;&#12289;&#29615;&#22659;&#21160;&#24577;&#21644;&#33258;&#25105;&#32422;&#26463;&#65288;&#22914;&#36135;&#24065;&#39044;&#31639;&#38480;&#21046;&#65289;&#12290;&#20174;&#32479;&#19968;&#23545;&#40784; ($\mathbf{UA}^2$) &#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#24403;&#21069;&#26234;&#33021;&#20307;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#24182;&#25351;&#20986;&#20102;&#29616;&#26377;&#26234;&#33021;&#20307;&#22522;&#20934;&#21644;&#26041;&#27861;&#20505;&#36873;&#20013;&#34987;&#24573;&#35270;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20026;WebShop&#24341;&#20837;&#23454;&#38469;&#29305;&#24615;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#65292;&#21253;&#25324;&#20351;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#26469;&#23637;&#31034;&#24847;&#22270;&#12289;&#20010;&#24615;&#21270;&#37325;&#26032;&#25490;&#21517;&#20197;&#24212;&#23545;&#22797;&#26434;&#30340;&#29615;&#22659;&#21160;&#24577;&#21644;&#36816;&#34892;&#26102;&#25104;&#26412;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\mathbf{U}$nified $\mathbf{A}$lignment for $\mathbf{A}$gents ($\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2402.07739</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#20013;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#35270;&#35273;&#29305;&#24449;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Task-conditioned adaptation of visual features in multi-task policy learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20219;&#21153;&#26465;&#20214;&#30340;&#33258;&#36866;&#24212;&#22120;&#65292;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#24182;&#19988;&#26080;&#38656;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21151;&#22320;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#26159;&#33258;&#20027;&#20195;&#29702;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36825;&#38656;&#35201;&#28789;&#27963;&#22320;&#35843;&#25972;&#24213;&#23618;&#30340;&#20915;&#31574;&#31574;&#30053;&#65292;&#24182;&#19988;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25552;&#20986;&#30340;&#65292;&#36824;&#38656;&#35201;&#35843;&#25972;&#24213;&#23618;&#30340;&#24863;&#30693;&#27169;&#22359;&#12290;&#19968;&#20010;&#31867;&#27604;&#30340;&#35770;&#35777;&#26159;&#20154;&#31867;&#30340;&#35270;&#35273;&#31995;&#32479;&#65292;&#23427;&#20351;&#29992;&#33258;&#19978;&#32780;&#19979;&#30340;&#20449;&#21495;&#26469;&#19987;&#27880;&#20110;&#24403;&#21069;&#20219;&#21153;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#31574;&#30053;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#36890;&#36807;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#26469;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#35270;&#35273;&#27169;&#22411;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20219;&#21153;&#26465;&#20214;&#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#20219;&#20309;&#39044;&#20808;&#35757;&#32451;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#35757;&#32451;&#30340;&#21333;&#19968;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#31574;&#30053;&#21644;&#35270;&#35273;&#36866;&#37197;&#22120;&#19978;&#26681;&#25454;&#20219;&#21153;&#23884;&#20837;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#22914;&#26524;&#20219;&#21153;&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#20219;&#21153;&#23884;&#20837;&#65292;&#21542;&#21017;&#21487;&#20197;&#20174;&#19968;&#32452;&#31034;&#20363;&#28436;&#31034;&#20013;&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#22312;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Successfully addressing a wide variety of tasks is a core ability of autonomous agents, which requires flexibly adapting the underlying decision-making strategies and, as we argue in this work, also adapting the underlying perception modules. An analogical argument would be the human visual system, which uses top-down signals to focus attention determined by the current task. Similarly, in this work, we adapt pre-trained large vision models conditioned on specific downstream tasks in the context of multi-task policy learning. We introduce task-conditioned adapters that do not require finetuning any pre-trained weights, combined with a single policy trained with behavior cloning and capable of addressing multiple tasks. We condition the policy and visual adapters on task embeddings, which can be selected at inference if the task is known, or alternatively inferred from a set of example demonstrations. To this end, we propose a new optimization-based estimator. We evaluate the method on 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;(UniLP)&#65292;&#23427;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#21442;&#25968;&#27169;&#22411;&#30340;&#27169;&#24335;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22270;&#20013;&#30340;&#38142;&#25509;&#27169;&#24335;&#24182;&#20855;&#26377;&#36328;&#19981;&#21516;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07738</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal link predictor by In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;(UniLP)&#65292;&#23427;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#21442;&#25968;&#27169;&#22411;&#30340;&#27169;&#24335;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22270;&#20013;&#30340;&#38142;&#25509;&#27169;&#24335;&#24182;&#20855;&#26377;&#36328;&#19981;&#21516;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#25512;&#26029;&#22270;&#20013;&#32570;&#22833;&#25110;&#26410;&#26469;&#30340;&#38142;&#25509;&#12290;&#20256;&#32479;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#24191;&#27867;&#35266;&#23519;&#21040;&#30340;&#36830;&#25509;&#27169;&#24335;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#26080;&#38656;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#21463;&#21046;&#20110;&#20154;&#20026;&#25512;&#23548;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32570;&#20047;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#12290;&#30456;&#21453;&#65292;&#21442;&#25968;&#38142;&#25509;&#39044;&#27979;&#22120;&#25797;&#38271;&#20110;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#36830;&#25509;&#27169;&#24335;&#24182;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#19981;&#21516;&#22270;&#20043;&#38388;&#30452;&#25509;&#36716;&#31227;&#19978;&#23384;&#22312;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#23427;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;&#36229;&#21442;&#25968;&#20248;&#21270;&#26469;&#36866;&#24212;&#30446;&#26631;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36890;&#29992;&#38142;&#25509;&#39044;&#27979;&#22120;&#65288;UniLP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23558;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#19982;&#21442;&#25968;&#27169;&#22411;&#30340;&#27169;&#24335;&#23398;&#20064;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;UniLP&#35774;&#35745;&#20026;&#33258;&#20027;&#23398;&#20064;&#30446;&#26631;&#22270;&#20013;&#30340;&#38142;&#25509;&#27169;&#24335;&#65292;&#24182;&#20855;&#26377;&#36328;&#19981;&#21516;&#22270;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is a crucial task in graph machine learning, where the goal is to infer missing or future links within a graph. Traditional approaches leverage heuristic methods based on widely observed connectivity patterns, offering broad applicability and generalizability without the need for model training. Despite their utility, these methods are limited by their reliance on human-derived heuristics and lack the adaptability of data-driven approaches. Conversely, parametric link predictors excel in automatically learning the connectivity patterns from data and achieving state-of-the-art but fail short to directly transfer across different graphs. Instead, it requires the cost of extensive training and hyperparameter optimization to adapt to the target graph. In this work, we introduce the Universal Link Predictor (UniLP), a novel model that combines the generalizability of heuristic approaches with the pattern learning capabilities of parametric models. UniLP is designed to autono
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BAM&#36827;&#34892;&#22270;&#32467;&#26500;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#24418;&#30340;&#32806;&#21512;&#27169;&#25311;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#38656;&#36890;&#36807;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#36827;&#34892;&#25512;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#65292;&#26041;&#27861;&#33021;&#22815;&#27867;&#21270;&#21040;&#32447;&#24615;&#21644;&#21508;&#31181;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;&#65288;BAM&#65289;&#26469;&#22788;&#29702;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26426;&#21046;&#22312;&#36716;&#25442;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#27700;&#24179;&#19978;&#36816;&#34892;&#65292;&#24182;&#23562;&#37325;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07735</link><description>&lt;p&gt;
&#29992;BAM&#36827;&#34892;&#22270;&#32467;&#26500;&#25512;&#26029;&#65306;&#24341;&#20837;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Graph Structure Inference with BAM: Introducing the Bilinear Attention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;BAM&#36827;&#34892;&#22270;&#32467;&#26500;&#25512;&#26029;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#21464;&#24418;&#30340;&#32806;&#21512;&#27169;&#25311;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#38656;&#36890;&#36807;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#36827;&#34892;&#25512;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#21644;&#38543;&#26426;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#65292;&#26041;&#27861;&#33021;&#22815;&#27867;&#21270;&#21040;&#32447;&#24615;&#21644;&#21508;&#31181;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#12290;&#24341;&#20837;&#20102;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;&#65288;BAM&#65289;&#26469;&#22788;&#29702;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#26426;&#21046;&#22312;&#36716;&#25442;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#27700;&#24179;&#19978;&#36816;&#34892;&#65292;&#24182;&#23562;&#37325;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32479;&#35745;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#30417;&#30563;&#22270;&#32467;&#26500;&#23398;&#20064;&#65292;&#21363;&#23398;&#20064;&#35266;&#27979;&#25968;&#25454;&#21644;&#23427;&#20204;&#30340;&#22522;&#26412;&#20381;&#36182;&#32467;&#26500;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21464;&#24418;&#30340;&#32806;&#21512;&#27169;&#25311;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#20165;&#38656;&#36890;&#36807;&#35757;&#32451;&#32593;&#32476;&#36827;&#34892;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#21363;&#21487;&#36827;&#34892;&#25512;&#26029;&#12290;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#26041;&#31243;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#29983;&#25104;&#30340;&#22810;&#21464;&#37327;&#20999;&#27604;&#38634;&#22827;&#22810;&#39033;&#24335;&#26469;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#32447;&#24615;&#21644;&#21508;&#31181;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#20043;&#38388;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#32447;&#24615;&#27880;&#24847;&#26426;&#21046;&#65288;BAM&#65289;&#65292;&#29992;&#20110;&#26174;&#24335;&#22788;&#29702;&#20381;&#36182;&#20449;&#24687;&#65292;&#35813;&#26426;&#21046;&#22312;&#36716;&#25442;&#25968;&#25454;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#27700;&#24179;&#19978;&#36816;&#34892;&#65292;&#24182;&#23562;&#37325;&#23545;&#31216;&#27491;&#23450;&#30697;&#38453;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In statistics and machine learning, detecting dependencies in datasets is a central challenge. We propose a novel neural network model for supervised graph structure learning, i.e., the process of learning a mapping between observational data and their underlying dependence structure. The model is trained with variably shaped and coupled simulated input data and requires only a single forward pass through the trained network for inference. By leveraging structural equation models and employing randomly generated multivariate Chebyshev polynomials for the simulation of training data, our method demonstrates robust generalizability across both linear and various types of non-linear dependencies. We introduce a novel bilinear attention mechanism (BAM) for explicit processing of dependency information, which operates on the level of covariance matrices of transformed data and respects the geometry of the manifold of symmetric positive definite matrices. Empirical evaluation demonstrates th
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07729</link><description>&lt;p&gt;
AIR-Bench: &#36890;&#36807;&#29983;&#25104;&#24615;&#29702;&#35299;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07729
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;AIR-Bench&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#38899;&#39057;&#20449;&#21495;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#23548;&#24615;&#30340;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#23545;&#20154;&#19982;&#38899;&#39057;&#30340;&#20114;&#21160;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#33021;&#22815;&#35780;&#20272;&#20197;&#38899;&#39057;&#20026;&#20013;&#24515;&#30340;&#20114;&#21160;&#33021;&#21147;&#30340;&#22522;&#20934;&#24050;&#32463;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20197;&#24448;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;&#19981;&#21516;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#32570;&#20047;&#23545;&#22260;&#32469;&#38899;&#39057;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#36861;&#36394;&#22823;&#22411;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;LALMs&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24182;&#20026;&#26410;&#26469;&#30340;&#25913;&#36827;&#25552;&#20379;&#25351;&#23548;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AIR-Bench&#65288;&#38899;&#39057;&#25351;&#23548;&#22522;&#20934;&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;LALMs&#29702;&#35299;&#21508;&#31181;&#31867;&#22411;&#38899;&#39057;&#20449;&#21495;&#65288;&#21253;&#25324;&#20154;&#31867;&#35821;&#38899;&#12289;&#33258;&#28982;&#22768;&#38899;&#21644;&#38899;&#20048;&#65289;&#20197;&#21450;&#19982;&#20154;&#20197;&#25991;&#26412;&#24418;&#24335;&#36827;&#34892;&#20132;&#20114;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;AIR-Bench&#21253;&#21547;&#20004;&#20010;&#32500;&#24230;&#65306;&#22522;&#30784;&#21644;&#29983;&#25104;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as Automatic Speech Recognition (ASR), and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement. In this paper, we introduce AIR-Bench (\textbf{A}udio \textbf{I}nst\textbf{R}uction \textbf{Bench}mark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: \textit{foundation} and \textit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#35777;&#26126;&#20102;&#37325;&#23614;SDE&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21442;&#25968;&#32500;&#24230;&#65292;&#30028;&#38480;&#30340;&#20381;&#36182;&#24615;&#35201;&#22909;&#20110;p&#12290;</title><link>https://arxiv.org/abs/2402.07723</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#35777;&#26126;&#37325;&#23614;SDEs&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Generalization Bounds for Heavy-Tailed SDEs through the Fractional Fokker-Planck Equation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#35777;&#26126;&#20102;&#37325;&#23614;SDE&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21442;&#25968;&#32500;&#24230;&#65292;&#30028;&#38480;&#30340;&#20381;&#36182;&#24615;&#35201;&#22909;&#20110;p&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#29702;&#35299;&#37325;&#23614;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#21033;&#29992;&#37325;&#23614;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20316;&#20026;&#20195;&#29702;&#26469;&#38416;&#26126;&#38543;&#26426;&#20248;&#21270;&#22120;&#30340;&#26377;&#36259;&#26041;&#38754;&#26102;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#25552;&#20379;&#39044;&#26399;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#35201;&#20040;&#24341;&#20837;&#20102;&#19981;&#21487;&#35745;&#31639;&#30340;&#20449;&#24687;&#35770;&#26415;&#35821;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37325;&#23614;SDE&#30340;&#39640;&#27010;&#29575;&#27867;&#21270;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#19981;&#21547;&#20219;&#20309;&#38750;&#24179;&#20961;&#30340;&#20449;&#24687;&#35770;&#26415;&#35821;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#22522;&#20110;&#20272;&#35745;&#19982;&#25152;&#35859;&#30340;&#20998;&#25968;&#38459;&#23612;&#24211;&#20177;&#26041;&#31243;&#30456;&#20851;&#32852;&#30340;&#29109;&#27969;&#65292;&#24320;&#21457;&#20102;&#26032;&#30340;&#35777;&#26126;&#25216;&#26415;&#65288;&#36825;&#26159;&#19968;&#31181;&#25511;&#21046;&#30456;&#24212;&#37325;&#23614;SDE&#20998;&#24067;&#28436;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#65289;&#12290;&#38500;&#20102;&#33719;&#24471;&#39640;&#27010;&#29575;&#30028;&#38480;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#30456;&#23545;&#20110;&#21442;&#25968;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#35201;&#22909;&#20110;p&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the generalization properties of heavy-tailed stochastic optimization algorithms has attracted increasing attention over the past years. While illuminating interesting aspects of stochastic optimizers by using heavy-tailed stochastic differential equations as proxies, prior works either provided expected generalization bounds, or introduced non-computable information theoretic terms. Addressing these drawbacks, in this work, we prove high-probability generalization bounds for heavy-tailed SDEs which do not contain any nontrivial information theoretic terms. To achieve this goal, we develop new proof techniques based on estimating the entropy flows associated with the so-called fractional Fokker-Planck equation (a partial differential equation that governs the evolution of the distribution of the corresponding heavy-tailed SDE). In addition to obtaining high-probability bounds, we show that our bounds have a better dependence on the dimension of parameters as compared to p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07721</link><description>&lt;p&gt;
LoRA-drop&#65306;&#22522;&#20110;&#36755;&#20986;&#35780;&#20272;&#30340;&#39640;&#25928;LoRA&#21442;&#25968;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LoRA-drop&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#19988;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#20854;&#20313;&#23618;&#20849;&#20139;&#30456;&#21516;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LoRA-drop&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#20026;&#27599;&#20010;&#23618;&#24341;&#20837;&#36741;&#21161;&#21442;&#25968;&#65292;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#19979;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#24403;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#22411;&#26102;&#65292;&#20173;&#28982;&#38754;&#20020;&#36164;&#28304;&#28040;&#32791;&#30340;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#19981;&#21516;&#23618;&#30340;LoRA&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#26469;&#37319;&#29992;&#21098;&#26525;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21162;&#21147;&#21482;&#20998;&#26512;&#20102;&#21442;&#25968;&#30340;&#29305;&#24449;&#20197;&#35780;&#20272;&#20854;&#37325;&#35201;&#24615;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#21442;&#25968;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;LoRA&#30340;&#36755;&#20986;&#26159;&#30452;&#25509;&#24433;&#21709;&#20923;&#32467;&#27169;&#22411;&#30340;&#22240;&#32032;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRA-drop&#65292;&#36890;&#36807;&#20998;&#26512;LoRA&#36755;&#20986;&#26469;&#35780;&#20272;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20445;&#30041;&#37325;&#35201;&#23618;&#30340;LoRA&#65292;&#32780;&#20854;&#20182;&#23618;&#30340;LoRA&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#12290;&#22312;NLU&#21644;NLG&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20805;&#20998;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;LoRA-drop&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA) introduces auxiliary parameters for each layer to fine-tune the pre-trained model under limited computing resources. But it still faces challenges of resource consumption when scaling up to larger models. Previous studies employ pruning techniques by evaluating the importance of LoRA parameters for different layers to address the problem. However, these efforts only analyzed parameter features to evaluate their importance. Indeed, the output of LoRA related to the parameters and data is the factor that directly impacts the frozen model. To this end, we propose LoRA-drop which evaluates the importance of the parameters by analyzing the LoRA output. We retain LoRA for important layers and the LoRA of the other layers share the same parameters. Abundant experiments on NLU and NLG tasks demonstrate the effectiveness of LoRA-drop.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;</title><link>https://arxiv.org/abs/2402.07712</link><description>&lt;p&gt;
&#27169;&#22411;&#23849;&#28291;&#35299;&#23494;&#65306;&#22238;&#24402;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Model Collapse Demystified: The Case of Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#35299;&#26512;&#20102;&#27169;&#22411;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#32531;&#35299;&#20102;&#27169;&#22411;&#23849;&#28291;&#38382;&#39064;&#12290;&#36825;&#20123;&#21457;&#29616;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#20195;&#65292;"&#27169;&#22411;&#23849;&#28291;"&#29616;&#35937;&#25351;&#30340;&#26159;&#27169;&#22411;&#22312;&#36882;&#24402;&#22320;&#35757;&#32451;&#33258;&#36523;&#19978;&#19968;&#20195;&#21448;&#19968;&#20195;&#29983;&#25104;&#30340;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#36880;&#28176;&#38477;&#20302;&#65292;&#26368;&#32456;&#21464;&#24471;&#23436;&#20840;&#26080;&#29992;&#65292;&#21363;&#27169;&#22411;&#23849;&#28291;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#26680;&#22238;&#24402;&#30340;&#31616;&#21270;&#29615;&#22659;&#20013;&#30740;&#31350;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#24182;&#33719;&#24471;&#20102;&#32467;&#26524;&#65292;&#26174;&#31034;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#34394;&#20551;&#25968;&#25454;&#19982;&#27169;&#22411;&#24615;&#33021;&#23436;&#20840;&#23849;&#28291;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20132;&#21449;&#28857;&#12290;&#22312;&#22810;&#39033;&#24335;&#34928;&#20943;&#30340;&#20809;&#35889;&#21644;&#28304;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20462;&#25913;&#21518;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#23637;&#31034;&#20102;&#20174;&#24555;&#36895;&#21040;&#32531;&#24930;&#36895;&#29575;&#30340;&#26032;&#20132;&#21449;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#30340;&#31616;&#21333;&#31574;&#30053;&#26469;&#32531;&#35299;&#27169;&#22411;&#23849;&#28291;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of large language models like ChatGPT, the phenomenon of "model collapse" refers to the situation whereby as a model is trained recursively on data generated from previous generations of itself over time, its performance degrades until the model eventually becomes completely useless, i.e the model collapses. In this work, we study this phenomenon in the simplified setting of kernel regression and obtain results which show a clear crossover between where the model can cope with fake data, and a regime where the model's performance completely collapses. Under polynomial decaying spectral and source conditions, we obtain modified scaling laws which exhibit new crossover phenomena from fast to slow rates. We also propose a simple strategy based on adaptive regularization to mitigate model collapse. Our theoretical results are validated with experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07703</link><description>&lt;p&gt;
&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Sequential Decision-Making with Unknown Delays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#20013;&#22788;&#29702;&#26410;&#30693;&#24310;&#36831;&#38382;&#39064;&#30340;&#19977;&#20010;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#39034;&#24207;&#20915;&#31574;&#39046;&#22495;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#26694;&#26550;&#35299;&#20915;&#20102;&#20855;&#26377;&#24310;&#36831;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#20915;&#31574;&#30340;&#21453;&#39304;&#21487;&#33021;&#20197;&#26410;&#30693;&#24310;&#36831;&#21040;&#36798;&#12290;&#19982;&#20043;&#21069;&#20165;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#33539;&#25968;&#21644;&#26799;&#24230;&#20449;&#24687;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#22522;&#20110;&#36817;&#20284;&#35299;&#30340;&#24310;&#36831;&#31639;&#27861;&#26063;&#65292;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25509;&#25910;&#21453;&#39304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22810;&#21151;&#33021;&#19988;&#36866;&#29992;&#20110;&#36890;&#29992;&#33539;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#23436;&#25972;&#25439;&#22833;&#20989;&#25968;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#35268;&#33539;&#21270;&#39046;&#23548;&#31639;&#27861;&#26063;&#65292;&#19968;&#31995;&#21015;&#38024;&#23545;&#20855;&#26377;&#26799;&#24230;&#20449;&#24687;&#21453;&#39304;&#30340;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#30456;&#24212;&#20915;&#31574;&#28857;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#20540;&#20449;&#24687;&#21453;&#39304;&#30340;&#31616;&#21270;&#24310;&#36831;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#26063;&#12290;&#23545;&#20110;&#27599;&#31181;&#31867;&#22411;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of online sequential decision-making, we address the problem with delays utilizing the framework of online convex optimization (OCO), where the feedback of a decision can arrive with an unknown delay. Unlike previous research that is limited to Euclidean norm and gradient information, we propose three families of delayed algorithms based on approximate solutions to handle different types of received feedback. Our proposed algorithms are versatile and applicable to universal norms. Specifically, we introduce a family of Follow the Delayed Regularized Leader algorithms for feedback with full information on the loss function, a family of Delayed Mirror Descent algorithms for feedback with gradient information on the loss function and a family of Simplified Delayed Mirror Descent algorithms for feedback with the value information of the loss function's gradients at corresponding decision points. For each type of algorithm, we provide corresponding regret bounds under cases of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BE-CBO&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#35774;&#35745;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#26469;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#26410;&#30693;&#32422;&#26463;&#30340;&#26410;&#30693;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07692</link><description>&lt;p&gt;
Bayesian&#20248;&#21270;&#20013;&#30340;&#36793;&#30028;&#25506;&#32034;&#19982;&#26410;&#30693;&#29289;&#29702;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Boundary Exploration for Bayesian Optimization With Unknown Physical Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BE-CBO&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;&#25506;&#32034;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#35774;&#35745;&#20043;&#38388;&#30340;&#36793;&#30028;&#65292;&#26469;&#35299;&#20915;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#26410;&#30693;&#32422;&#26463;&#30340;&#26410;&#30693;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20248;&#21270;&#35780;&#20272;&#27425;&#25968;&#20005;&#37325;&#38480;&#21046;&#30340;&#40657;&#30418;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#19968;&#20123;&#29289;&#29702;&#25110;&#31995;&#32479;&#38480;&#21046;&#65292;&#24456;&#38590;&#25110;&#32773;&#19981;&#21487;&#33021;&#20107;&#20808;&#30693;&#36947;&#21738;&#20123;&#35774;&#35745;&#26159;&#21487;&#34892;&#30340;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#20102;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65292;&#21363;&#20248;&#21270;&#26410;&#30693;&#32422;&#26463;&#30340;&#26410;&#30693;&#20989;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#35299;&#36890;&#24120;&#20301;&#20110;&#35774;&#35745;&#31354;&#38388;&#30340;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#21306;&#22495;&#20043;&#38388;&#30340;&#36793;&#30028;&#19978;&#65292;&#36825;&#20351;&#24471;&#38382;&#39064;&#27604;&#20855;&#26377;&#20869;&#37096;&#26368;&#20248;&#35299;&#30340;&#24773;&#20917;&#26356;&#21152;&#22256;&#38590;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;BE-CBO&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25506;&#32034;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#35774;&#35745;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#20026;&#20102;&#30830;&#23450;&#36793;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#32452;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#32422;&#26463;&#65292;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#22797;&#26434;&#36793;&#30028;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has been successfully applied to optimize black-box functions where the number of evaluations is severely limited. However, in many real-world applications, it is hard or impossible to know in advance which designs are feasible due to some physical or system limitations. These issues lead to an even more challenging problem of optimizing an unknown function with unknown constraints. In this paper, we observe that in such scenarios optimal solution typically lies on the boundary between feasible and infeasible regions of the design space, making it considerably more difficult than that with interior optima. Inspired by this observation, we propose BE-CBO, a new Bayesian optimization method that efficiently explores the boundary between feasible and infeasible designs. To identify the boundary, we learn the constraints with an ensemble of neural networks that outperform the standard Gaussian Processes for capturing complex boundaries. Our method demonstrates superio
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#23545;&#27604;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;CMIL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#20154;&#29289;&#37325;&#35782;&#21035;&#20013;&#25968;&#25454;&#38598;&#26631;&#27880;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;CMIL&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#25216;&#26415;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07685</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#20154;&#29289;&#37325;&#35782;&#21035;&#30340;&#23545;&#27604;&#22810;&#23454;&#20363;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Multiple Instance Learning for Weakly Supervised Person ReID
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07685
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#23545;&#27604;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;CMIL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24369;&#30417;&#30563;&#20154;&#29289;&#37325;&#35782;&#21035;&#20013;&#25968;&#25454;&#38598;&#26631;&#27880;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;CMIL&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#25216;&#26415;&#65292;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#32467;&#26524;&#65292;&#24182;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#24471;&#22823;&#35268;&#27169;&#12289;&#31934;&#30830;&#26631;&#35760;&#30340;&#20154;&#29289;&#37325;&#35782;&#21035;&#65288;ReID&#65289;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#24369;&#30417;&#30563;ReID&#24050;&#32463;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#24615;&#33021;&#36305;&#19981;&#36807;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#22810;&#23454;&#20363;&#23398;&#20064;&#65288;CMIL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#26356;&#26377;&#25928;&#30340;&#24369;&#30417;&#30563;ReID&#32780;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#12290;CMIL&#30340;&#29305;&#28857;&#26159;&#21482;&#38656;&#35201;&#19968;&#20010;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20266;&#26631;&#31614;&#65292;&#21516;&#26102;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#30340;&#25216;&#26415;&#65292;&#36825;&#22312;&#25152;&#26377;&#20197;&#21069;&#30340;&#22522;&#20110;MIL&#30340;&#26041;&#27861;&#20013;&#37117;&#19981;&#23384;&#22312;&#65292;&#20294;&#22312;&#20256;&#32479;&#30340;ReID&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;CMIL&#19981;&#20165;&#22312;&#22823;&#35268;&#27169;SYSU-30k&#25968;&#25454;&#38598;&#19978;&#19982;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#19988;&#20551;&#35774;&#26356;&#23569;&#65292;&#32780;&#19988;&#22312;WL-market1501&#21644;&#24369;&#26631;&#35760;&#30340;MUddy racer&#37325;&#35782;&#21035;&#25968;&#25454;&#38598;&#65288;WL-MUDD&#65289;&#19978;&#22987;&#32456;&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#24182;&#21457;&#24067;&#20102;WL-MUDD&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The acquisition of large-scale, precisely labeled datasets for person re-identification (ReID) poses a significant challenge. Weakly supervised ReID has begun to address this issue, although its performance lags behind fully supervised methods. In response, we introduce Contrastive Multiple Instance Learning (CMIL), a novel framework tailored for more effective weakly supervised ReID. CMIL distinguishes itself by requiring only a single model and no pseudo labels while leveraging contrastive losses -- a technique that has significantly enhanced traditional ReID performance yet is absent in all prior MIL-based approaches. Through extensive experiments and analysis across three datasets, CMIL not only matches state-of-the-art performance on the large-scale SYSU-30k dataset with fewer assumptions but also consistently outperforms all baselines on the WL-market1501 and Weakly Labeled MUddy racer re-iDentification dataset (WL-MUDD) datasets. We introduce and release the WL-MUDD dataset, an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#27169;&#22411;NeuroVNN&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#35299;&#21078;&#23398;&#29305;&#24449;&#36827;&#34892;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#65292;&#20855;&#26377;&#21487;&#36801;&#31227;&#24615;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#21644;&#35748;&#30693;&#34928;&#36864;&#30340;&#22686;&#21152;&#39118;&#38505;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.07684</link><description>&lt;p&gt;
&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#30784;&#27169;&#22411;NeuroVNN&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#35299;&#21078;&#23398;&#29305;&#24449;&#36827;&#34892;&#22823;&#33041;&#24180;&#40836;&#39044;&#27979;&#65292;&#20855;&#26377;&#21487;&#36801;&#31227;&#24615;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#21644;&#35748;&#30693;&#34928;&#36864;&#30340;&#22686;&#21152;&#39118;&#38505;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#24180;&#40836;&#26159;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20174;&#31070;&#32463;&#24433;&#20687;&#25968;&#25454;&#38598;&#20013;&#24471;&#20986;&#30340;&#29983;&#29289;&#24180;&#40836;&#20272;&#35745;&#12290;&#30456;&#23545;&#20110;&#23454;&#38469;&#24180;&#40836;&#65292;&#22823;&#33041;&#24180;&#40836;&#30340;&#22686;&#38271;&#21487;&#20197;&#21453;&#26144;&#20986;&#31070;&#32463;&#36864;&#34892;&#24615;&#21644;&#35748;&#30693;&#34928;&#36864;&#30340;&#22686;&#21152;&#33030;&#24369;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#21327;&#26041;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#24180;&#40836;&#39044;&#27979;&#24212;&#29992;&#30340;&#22522;&#30784;&#27169;&#22411;-NeuroVNN&#12290;NeuroVNN&#26159;&#22312;&#20581;&#24247;&#20154;&#32676;&#19978;&#20316;&#20026;&#22238;&#24402;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#65292;&#20351;&#29992;&#30382;&#23618;&#21402;&#24230;&#29305;&#24449;&#36827;&#34892;&#39044;&#27979;&#23454;&#38469;&#24180;&#40836;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23545;&#19981;&#21516;&#31070;&#32463;&#23398;&#32972;&#26223;&#19979;&#30340;&#22823;&#33041;&#24180;&#40836;&#36827;&#34892;&#20272;&#35745;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;NeuroVNN&#20026;&#22823;&#33041;&#24180;&#40836;&#22686;&#21152;&#20102;&#35299;&#21078;&#23398;&#35299;&#37322;&#24615;&#65292;&#24182;&#20855;&#26377;&#8220;&#26080;&#26631;&#24230;&#8221;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#33021;&#22815;&#36801;&#31227;&#21040;&#25353;&#20219;&#24847;&#22823;&#33041;&#22270;&#35889;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;NeuroVNN&#21487;&#20197;&#22312;&#19981;&#21516;&#20154;&#32676;&#20013;&#25552;&#21462;&#19982;&#29983;&#29289;&#30456;&#20851;&#30340;&#21512;&#29702;&#22823;&#33041;&#24180;&#40836;&#20272;&#35745;&#65292;&#24182;&#25104;&#21151;&#36801;&#31227;&#21040;&#19982;&#20043;&#19981;&#21516;&#32500;&#24230;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain age is the estimate of biological age derived from neuroimaging datasets using machine learning algorithms. Increasing brain age with respect to chronological age can reflect increased vulnerability to neurodegeneration and cognitive decline. In this paper, we study NeuroVNN, based on coVariance neural networks, as a paradigm for foundation model for the brain age prediction application. NeuroVNN is pre-trained as a regression model on healthy population to predict chronological age using cortical thickness features and fine-tuned to estimate brain age in different neurological contexts. Importantly, NeuroVNN adds anatomical interpretability to brain age and has a `scale-free' characteristic that allows its transference to datasets curated according to any arbitrary brain atlas. Our results demonstrate that NeuroVNN can extract biologically plausible brain age estimates in different populations, as well as transfer successfully to datasets of dimensionalities distinct from that f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#27969;&#20449;&#21495;&#21644;&#34892;&#20154;&#36793;&#30028;&#26694;&#30340;&#21487;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#23433;&#20840;&#20851;&#38190;&#30340;&#35823;&#26816;&#12290;</title><link>https://arxiv.org/abs/2402.07642</link><description>&lt;p&gt;
&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#34892;&#20154;&#26816;&#27979;&#30340;&#22522;&#20110;&#27969;&#30340;&#21487;&#20449;&#24230;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Flow-based Credibility Metric for Safety-critical Pedestrian Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20809;&#27969;&#20449;&#21495;&#21644;&#34892;&#20154;&#36793;&#30028;&#26694;&#30340;&#21487;&#20449;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#33021;&#22815;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#23433;&#20840;&#20851;&#38190;&#30340;&#35823;&#26816;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#24863;&#30693;&#30340;&#26368;&#37325;&#35201;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#35201;&#23433;&#20840;&#38382;&#39064;&#26159;&#26631;&#20934;&#35780;&#20272;&#26041;&#26696;&#20351;&#29992;&#19981;&#20851;&#27880;&#23433;&#20840;&#30340;&#24230;&#37327;&#26469;&#35777;&#26126;&#36275;&#22815;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#21033;&#29992;&#34917;&#20805;&#39046;&#22495;&#30693;&#35782;&#26469;&#31361;&#20986;&#35780;&#20272;&#20219;&#21153;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#30340;&#35823;&#26816;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#20449;&#24230;&#24230;&#37327;&#65292;&#31216;&#20026;c-flow&#65292;&#29992;&#20110;&#34892;&#20154;&#36793;&#30028;&#26694;&#12290;&#20026;&#27492;&#65292;c-flow&#21033;&#29992;&#22270;&#20687;&#24207;&#21015;&#20013;&#30340;&#34917;&#20805;&#20809;&#27969;&#20449;&#21495;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#23433;&#20840;&#20851;&#38190;&#30340;&#35823;&#26816;&#30340;&#20998;&#26512;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#21644;&#35780;&#20272;&#20102;c-flow&#19982;&#19968;&#31181;&#20808;&#36827;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#26126;&#65292;c-flow&#21487;&#20197;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#35782;&#21035;&#23433;&#20840;&#20851;&#38190;&#30340;&#35823;&#26816;&#12290;
&lt;/p&gt;
&lt;p&gt;
Safety is of utmost importance for perception in automated driving (AD). However, a prime safety concern in state-of-the art object detection is that standard evaluation schemes utilize safety-agnostic metrics to argue sufficient detection performance. Hence, it is imperative to leverage supplementary domain knowledge to accentuate safety-critical misdetections during evaluation tasks. To tackle the underspecification, this paper introduces a novel credibility metric, called c-flow, for pedestrian bounding boxes. To this end, c-flow relies on a complementary optical flow signal from image sequences and enhances the analyses of safety-critical misdetections without requiring additional labels. We implement and evaluate c-flow with a state-of-the-art pedestrian detector on a large AD dataset. Our analysis demonstrates that c-flow allows developers to identify safety-critical misdetections.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20449;&#24687;&#29942;&#39048;&#26356;&#20026;&#32039;&#23494;&#30340;&#21464;&#20998;&#30028;&#38480;&#65292;&#21487;&#20197;&#25913;&#21892;&#20197;&#21069;&#30340;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;DNNs&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#26174;&#33879;&#22686;&#24378;&#20998;&#31867;&#22120;DNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07639</link><description>&lt;p&gt;
&#23545;&#20449;&#24687;&#29942;&#39048;&#30340;&#38480;&#21046;&#26356;&#32039;&#30340;&#30028;&#38480;&#65292;&#24182;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tighter Bounds on the Information Bottleneck with Application to Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07639
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#20449;&#24687;&#29942;&#39048;&#26356;&#20026;&#32039;&#23494;&#30340;&#21464;&#20998;&#30028;&#38480;&#65292;&#21487;&#20197;&#25913;&#21892;&#20197;&#21069;&#30340;&#22522;&#20110;&#20449;&#24687;&#29942;&#39048;&#30340;DNNs&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#26469;&#26174;&#33879;&#22686;&#24378;&#20998;&#31867;&#22120;DNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#36890;&#36807;&#19979;&#28216;&#20219;&#21153;&#12289;&#30446;&#26631;&#20989;&#25968;&#21644;&#20854;&#20182;&#21442;&#25968;&#26469;&#23398;&#20064;&#24341;&#21457;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#30340;&#36136;&#37327;&#24433;&#21709;&#30528;DNN&#30340;&#27010;&#25324;&#33021;&#21147;&#21644;&#26032;&#20986;&#29616;&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#36830;&#36143;&#24615;&#12290;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#26368;&#20248;&#30340;&#25968;&#25454;&#24314;&#27169;&#26694;&#26550;&#65292;&#20294;&#36890;&#24120;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;DNNs&#19982;IB&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#24212;&#29992;VAE-inspire&#30340;&#21464;&#20998;&#26041;&#27861;&#26469;&#36817;&#20284;&#30456;&#20114;&#20449;&#24687;&#30340;&#30028;&#38480;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21644;&#26356;&#32039;&#30340;&#21464;&#20998;&#30028;&#38480;&#65292;&#25552;&#39640;&#20102;&#20197;&#21069;IB-inspire DNNs&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#36827;&#23637;&#21152;&#24378;&#20102;IB&#21450;&#20854;&#21464;&#20998;&#36817;&#20284;&#20316;&#20026;&#25968;&#25454;&#27169;&#22411;&#26694;&#26550;&#30340;&#35770;&#28857;&#65292;&#24182;&#20026;&#20998;&#31867;&#22120;DNNs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Nets (DNNs) learn latent representations induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN's generalization ability and the coherence of the emerging latent space. The Information Bottleneck (IB) provides a hypothetically optimal framework for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, resulting in improved robustness to adversarial attacks. This work introduces a new and tighter variational bound for the IB, improving performance of previous IB-inspired DNNs. These advancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to significantly enhance the adversarial robustness of classifier DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G-Retriever&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;&#22238;&#22797;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.07630</link><description>&lt;p&gt;
G-Retriever: &#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G-Retriever&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#22270;&#29702;&#35299;&#21644;&#38382;&#39064;&#35299;&#31572;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#29992;&#25143;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;&#22238;&#22797;&#65292;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#20855;&#26377;&#25991;&#26412;&#23646;&#24615;&#30340;&#22270;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#25143;&#33021;&#22815;&#20351;&#29992;&#23545;&#35805;&#30028;&#38754;&#21521;&#22270;&#24418;&#25552;&#20986;&#38382;&#39064;&#12290;&#38024;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#25991;&#26412;&#22238;&#22797;&#24182;&#31361;&#20986;&#26174;&#31034;&#22270;&#24418;&#30340;&#30456;&#20851;&#37096;&#20998;&#12290;&#19982;&#29616;&#26377;&#30340;&#26041;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#20197;&#21508;&#31181;&#26041;&#24335;&#25972;&#21512;&#36215;&#26469;&#19981;&#21516;&#65292;&#23427;&#20204;&#22823;&#22810;&#38598;&#20013;&#22312;&#20256;&#32479;&#22270;&#20219;&#21153;(&#22914;&#33410;&#28857;&#12289;&#36793;&#21644;&#22270;&#20998;&#31867;)&#25110;&#32773;&#22312;&#23567;&#22411;&#25110;&#21512;&#25104;&#22270;&#19978;&#22238;&#31572;&#31616;&#21333;&#30340;&#22270;&#26597;&#35810;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#28789;&#27963;&#30340;&#38382;&#39064;&#22238;&#31572;&#26694;&#26550;&#65292;&#38024;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#25991;&#26412;&#22270;&#24418;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#24212;&#29992;&#65292;&#21253;&#25324;&#22330;&#26223;&#22270;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#30693;&#35782;&#22270;&#25512;&#29702;&#12290;&#20026;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#29992;&#26469;&#33258;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#24320;&#21457;&#20102;&#25105;&#20204;&#30340;&#22270;&#38382;&#39064;&#22238;&#31572;(GraphQA)&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;G-Retriever&#26041;&#27861;&#65292;&#23427;&#38598;&#25104;&#20102;GNN&#21644;LLM&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#36335;&#24452;&#31215;&#20998;&#26041;&#27861;&#25506;&#32034;&#20102;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#20013;&#30340;&#27979;&#35797;&#39118;&#38505;&#65292;&#24182;&#22312;&#23567;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#35745;&#31639;&#32431;&#26799;&#24230;&#27969;&#21160;&#21644;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#30340;&#27979;&#35797;&#39118;&#38505;&#26354;&#32447;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19968;&#20010;&#24369;&#29305;&#24449;&#27169;&#22411;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#39033;&#23545;&#21160;&#21147;&#23398;&#30340;&#20462;&#27491;&#25928;&#26524;&#65292;&#24182;&#19982;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#25311;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07626</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#20013;&#30340;&#27979;&#35797;&#39118;&#38505;&#21450;&#20854;&#24369;&#29305;&#24449;&#30340;&#31934;&#30830;&#35299;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Flow Dynamics of Test Risk and its Exact Solution for Weak Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#36335;&#24452;&#31215;&#20998;&#26041;&#27861;&#25506;&#32034;&#20102;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#20013;&#30340;&#27979;&#35797;&#39118;&#38505;&#65292;&#24182;&#22312;&#23567;&#23398;&#20064;&#29575;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#35745;&#31639;&#32431;&#26799;&#24230;&#27969;&#21160;&#21644;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#30340;&#27979;&#35797;&#39118;&#38505;&#26354;&#32447;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#19968;&#20010;&#24369;&#29305;&#24449;&#27169;&#22411;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#38543;&#26426;&#39033;&#23545;&#21160;&#21147;&#23398;&#30340;&#20462;&#27491;&#25928;&#26524;&#65292;&#24182;&#19982;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#25311;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#20986;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23398;&#20064;&#29702;&#35770;&#20013;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#27979;&#35797;&#39118;&#38505;&#12290;&#21033;&#29992;&#36335;&#24452;&#31215;&#20998;&#20844;&#24335;&#65292;&#22312;&#23567;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#35745;&#31639;&#32431;&#26799;&#24230;&#27969;&#21160;&#21644;&#38543;&#26426;&#26799;&#24230;&#27969;&#21160;&#30340;&#27979;&#35797;&#39118;&#38505;&#26354;&#32447;&#20043;&#38388;&#24046;&#24322;&#30340;&#19968;&#33324;&#20844;&#24335;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#36890;&#29992;&#29702;&#35770;&#24212;&#29992;&#21040;&#19968;&#20010;&#31616;&#21333;&#30340;&#24369;&#29305;&#24449;&#27169;&#22411;&#20013;&#65292;&#35813;&#27169;&#22411;&#23637;&#31034;&#20102;&#21452;&#23792;&#29616;&#35937;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#20102;&#21160;&#21147;&#23398;&#20013;&#22686;&#21152;&#30340;&#38543;&#26426;&#39033;&#38543;&#26102;&#38388;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#20462;&#27491;&#12290;&#20998;&#26512;&#32467;&#26524;&#19982;&#31163;&#25955;&#26102;&#38388;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#27169;&#25311;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the test risk of continuous-time stochastic gradient flow dynamics in learning theory. Using a path integral formulation we provide, in the regime of a small learning rate, a general formula for computing the difference between test risk curves of pure gradient and stochastic gradient flows. We apply the general theory to a simple model of weak features, which displays the double descent phenomenon, and explicitly compute the corrections brought about by the added stochastic term in the dynamics, as a function of time and model parameters. The analytical results are compared to simulations of discrete-time stochastic gradient descent and show good agreement.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#27491;&#30830;&#24615;&#39564;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#30340;&#23548;&#25968;&#36817;&#20284;&#21644;&#24314;&#31435;&#24182;&#34892;&#20998;&#25903;&#31639;&#27861;&#65292;&#26377;&#25928;&#30028;&#23450;&#20102;&#20989;&#25968;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#22788;&#29702;&#20102;&#27809;&#26377;&#36755;&#20986;&#22495;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.07621</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24494;&#20998;&#26041;&#31243;&#30340;&#27491;&#30830;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Correctness Verification of Neural Networks Approximating Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07621
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#24494;&#20998;&#26041;&#31243;&#35299;&#30340;&#27491;&#30830;&#24615;&#39564;&#35777;&#38382;&#39064;&#65292;&#36890;&#36807;&#23450;&#20041;&#31070;&#32463;&#32593;&#32476;&#30340;&#23548;&#25968;&#36817;&#20284;&#21644;&#24314;&#31435;&#24182;&#34892;&#20998;&#25903;&#31639;&#27861;&#65292;&#26377;&#25928;&#30028;&#23450;&#20102;&#20989;&#25968;&#30340;&#19978;&#19979;&#30028;&#65292;&#24182;&#22788;&#29702;&#20102;&#27809;&#26377;&#36755;&#20986;&#22495;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26159;&#25552;&#39640;&#20854;&#21487;&#20449;&#24230;&#21644;&#21152;&#36895;&#37096;&#32626;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#12290;&#22914;&#26524;&#25104;&#21151;&#65292;&#36825;&#20123;NN&#21487;&#20197;&#25104;&#20026;&#27169;&#25311;&#36719;&#20214;&#24037;&#20855;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21487;&#20197;&#23558;&#22797;&#26434;&#21160;&#24577;&#31995;&#32479;&#30340;&#27169;&#25311;&#21152;&#36895;100&#20493;&#20197;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20989;&#25968;&#30340;&#39564;&#35777;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65307;&#22914;&#20309;&#39640;&#25928;&#22320;&#30028;&#23450;&#23427;&#20204;&#25110;&#22914;&#20309;&#34920;&#31034;NN&#30340;&#23548;&#25968;&#37117;&#19981;&#26159;&#30452;&#25509;&#30340;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;NN&#30340;&#23548;&#25968;&#23450;&#20041;&#20026;&#26377;&#38480;&#24046;&#20998;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;PDE&#27531;&#24046;&#30028;&#23450;&#38382;&#39064;&#19982;&#21021;&#22987;&#20540;&#38382;&#39064;&#30340;&#35823;&#24046;&#20256;&#25773;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#22312;&#27809;&#26377;&#36755;&#20986;&#22495;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30028;&#23450;NN&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24182;&#34892;&#20998;&#25903;&#31639;&#27861;&#65292;&#23558;&#36817;&#20284;&#22788;&#29702;&#39046;&#22495;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Verification of Neural Networks (NNs) that approximate the solution of Partial Differential Equations (PDEs) is a major milestone towards enhancing their trustworthiness and accelerating their deployment, especially for safety-critical systems. If successful, such NNs can become integral parts of simulation software tools which can accelerate the simulation of complex dynamic systems more than 100 times. However, the verification of these functions poses major challenges; it is not straightforward how to efficiently bound them or how to represent the derivative of the NN. This work addresses both these problems. First, we define the NN derivative as a finite difference approximation. Then, we formulate the PDE residual bounding problem alongside the Initial Value Problem's error propagation. Finally, for the first time, we tackle the problem of bounding an NN function without a priori knowledge of the output domain. For this, we build a parallel branching algorithm that combines the in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36712;&#36947;&#20984;&#20307;&#21644;coycle&#31561;&#24037;&#20855;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#24212;&#29992;&#21253;&#25324;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22522;&#20110;&#23545;&#31216;&#32422;&#26463;&#30340;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.07613</link><description>&lt;p&gt;
&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Global optimality under amenable symmetry constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21487;&#25509;&#21463;&#30340;&#23545;&#31216;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#26368;&#20248;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28385;&#36275;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36712;&#36947;&#20984;&#20307;&#21644;coycle&#31561;&#24037;&#20855;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#20855;&#20307;&#24212;&#29992;&#21253;&#25324;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22522;&#20110;&#23545;&#31216;&#32422;&#26463;&#30340;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#28385;&#36275;&#21487;&#25509;&#21463;&#21464;&#25442;&#32676;&#25351;&#23450;&#30340;&#23545;&#31216;&#24615;&#36136;&#30340;&#20989;&#25968;&#25110;&#24230;&#37327;&#65292;&#21363;&#21516;&#26102;&#28385;&#36275;&#20197;&#19979;&#20004;&#20010;&#26465;&#20214;&#65306;&#65288;1&#65289;&#26368;&#23567;&#21270;&#32473;&#23450;&#30340;&#20984;&#24615;&#27867;&#20989;&#25110;&#39118;&#38505;&#65292;&#65288;2&#65289;&#28385;&#36275;&#21487;&#23481;&#24525;&#23545;&#31216;&#32422;&#26463;&#12290;&#36825;&#31181;&#23545;&#31216;&#24615;&#36136;&#30340;&#20363;&#23376;&#21253;&#25324;&#19981;&#21464;&#24615;&#12289;&#21487;&#21464;&#24615;&#25110;&#20934;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#36182;&#20110;Stein&#21644;Le Cam&#30340;&#32769;&#24605;&#24819;&#65292;&#20197;&#21450;&#22312;&#21487;&#25509;&#21463;&#32676;&#30340;&#36941;&#21382;&#23450;&#29702;&#20013;&#20986;&#29616;&#30340;&#36817;&#20284;&#32676;&#24179;&#22343;&#20540;&#12290;&#22312;&#20984;&#20998;&#26512;&#20013;&#65292;&#19968;&#31867;&#31216;&#20026;&#36712;&#36947;&#20984;&#20307;&#30340;&#20984;&#38598;&#26174;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#35774;&#32622;&#20013;&#30830;&#23450;&#20102;&#36825;&#31867;&#36712;&#36947;&#20984;&#20307;&#30340;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#20010;&#31216;&#20026;coycle&#30340;&#31616;&#21333;&#35013;&#32622;&#22914;&#20309;&#23558;&#19981;&#21516;&#24418;&#24335;&#30340;&#23545;&#31216;&#24615;&#36716;&#21270;&#20026;&#19968;&#20010;&#38382;&#39064;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#19981;&#21464;&#26680;&#22343;&#20540;&#23884;&#20837;&#21644;&#22312;&#23545;&#31216;&#32422;&#26463;&#19979;&#36816;&#36755;&#26041;&#26696;&#26368;&#20248;&#24615;&#30340;Monge-Kantorovich&#23450;&#29702;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#35299;&#37322;&#20102;&#19982;&#19981;&#21464;&#24615;&#26816;&#39564;&#30340;Hunt-Stein&#23450;&#29702;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We ask whether there exists a function or measure that (1) minimizes a given convex functional or risk and (2) satisfies a symmetry property specified by an amenable group of transformations. Examples of such symmetry properties are invariance, equivariance, or quasi-invariance. Our results draw on old ideas of Stein and Le Cam and on approximate group averages that appear in ergodic theorems for amenable groups. A class of convex sets known as orbitopes in convex analysis emerges as crucial, and we establish properties of such orbitopes in nonparametric settings. We also show how a simple device called a cocycle can be used to reduce different forms of symmetry to a single problem. As applications, we obtain results on invariant kernel mean embeddings and a Monge-Kantorovich theorem on optimality of transport plans under symmetry constraints. We also explain connections to the Hunt-Stein theorem on invariant tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36817;&#26368;&#23567;&#26497;&#22823;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#22238;&#25253;&#20998;&#24067;&#26041;&#38754;&#20855;&#26377;&#26497;&#23567;&#26497;&#22823;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#30740;&#31350;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07598</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36817;&#26368;&#23567;&#26497;&#22823;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Near-Minimax-Optimal Distributional Reinforcement Learning with a Generative Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36817;&#26368;&#23567;&#26497;&#22823;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#22238;&#25253;&#20998;&#24067;&#26041;&#38754;&#20855;&#26377;&#26497;&#23567;&#26497;&#22823;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#36817;&#20284;&#22238;&#25253;&#20998;&#24067;&#26041;&#38754;&#65292;&#23427;&#26159;&#36817;&#20284;&#26368;&#23567;&#26497;&#22823;&#30340;&#65288;&#22312;&#23545;&#25968;&#22240;&#23376;&#19978;&#65289;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Zhang&#31561;&#20154;&#65288;2023&#65289;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#31867;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;Bellman&#26041;&#31243;&#65292;&#21363;&#38543;&#26426;&#20998;&#31867;&#32047;&#31215;&#20998;&#24067;&#20989;&#25968;Bellman&#26041;&#31243;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20010;&#26041;&#31243;&#20063;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#24847;&#20041;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#20960;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#24471;&#20986;&#20102;&#23545;&#23454;&#36341;&#32773;&#26377;&#24847;&#20041;&#30340;&#20960;&#20010;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new algorithm for model-based distributional reinforcement learning (RL), and prove that it is minimax-optimal for approximating return distributions with a generative model (up to logarithmic factors), resolving an open question of Zhang et al. (2023). Our analysis provides new theoretical results on categorical approaches to distributional RL, and also introduces a new distributional Bellman equation, the stochastic categorical CDF Bellman equation, which we expect to be of independent interest. We also provide an experimental study comparing several model-based distributional RL algorithms, with several takeaways for practitioners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;ImageNet&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;DINOv2&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#65292;DINOv2&#30340;&#24615;&#33021;&#19981;&#22914;ImageNet-b&#12290;</title><link>https://arxiv.org/abs/2402.07595</link><description>&lt;p&gt;
&#23545;&#27604;&#20998;&#26512;ImageNet&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;DINOv2&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of ImageNet Pre-Trained Deep Learning Models and DINOv2 in Medical Imaging Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;ImageNet&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;DINOv2&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#65292;DINOv2&#30340;&#24615;&#33021;&#19981;&#22914;ImageNet-b&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#24120;&#24120;&#38754;&#20020;&#30528;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#12290;&#36801;&#31227;&#23398;&#20064;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21516;&#26102;&#33410;&#32422;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#20351;&#29992;&#35270;&#35273;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;DINOv2&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#24182;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;DINOv2&#22312;&#20020;&#24202;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#38656;&#39564;&#35777;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#20020;&#24202;&#27169;&#24577;&#30340;&#33041;&#37096;MRI&#25968;&#25454;&#25191;&#34892;&#20102;&#19968;&#20010;&#33014;&#36136;&#30244;&#20998;&#32423;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#36801;&#31227;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#27604;&#36739;&#20102;&#22522;&#20110;ImageNet&#21644;DINOv2&#30340;&#21508;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20851;&#27880;&#20102;&#20923;&#32467;&#26426;&#21046;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#22312;&#20854;&#20182;&#19977;&#31181;&#31867;&#22411;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#65306;&#33016;&#37096;&#25918;&#23556;&#23398;&#12289;&#30524;&#24213;&#25918;&#23556;&#23398;&#21644;&#30382;&#32932;&#38236;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#20020;&#24202;&#25968;&#25454;&#38598;&#20013;&#65292;DINOv2&#30340;&#24615;&#33021;&#19981;&#22914;ImageNet-b&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical image analysis frequently encounters data scarcity challenges. Transfer learning has been effective in addressing this issue while conserving computational resources. The recent advent of foundational models like the DINOv2, which uses the vision transformer architecture, has opened new opportunities in the field and gathered significant interest. However, DINOv2's performance on clinical data still needs to be verified. In this paper, we performed a glioma grading task using three clinical modalities of brain MRI data. We compared the performance of various pre-trained deep learning models, including those based on ImageNet and DINOv2, in a transfer learning context. Our focus was on understanding the impact of the freezing mechanism on performance. We also validated our findings on three other types of public datasets: chest radiography, fundus radiography, and dermoscopy. Our findings indicate that in our clinical dataset, DINOv2's performance was not as strong as ImageNet-b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07594</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Foundational Inference Models for Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#26500;&#25104;&#20102;&#20316;&#20026;&#33258;&#28982;&#21644;&#31038;&#20250;&#29616;&#35937;&#27169;&#22411;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#20986;&#26368;&#20339;&#25551;&#36848;&#32473;&#23450;&#29616;&#35937;&#30340;&#19968;&#32452;&#22122;&#22768;&#35266;&#23519;&#30340;ODE&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#24448;&#24448;&#20063;&#38750;&#24120;&#19987;&#19994;&#21270;&#21644;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;ODE&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#21021;&#22987;&#26465;&#20214;&#31354;&#38388;&#21644;&#23450;&#20041;&#23427;&#20204;&#30340;&#21521;&#37327;&#22330;&#31354;&#38388;&#30340;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#29983;&#25104;&#22823;&#22411;&#19968;&#32500;ODE&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23398;&#20064;&#23558;&#36825;&#20123;&#26041;&#31243;&#30340;&#35299;&#30340;&#22122;&#22768;&#35266;&#23519;&#19982;&#20854;&#30456;&#24212;&#30340;&#21021;&#22987;&#26465;&#20214;&#21644;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#31070;&#32463;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#27169;&#22411;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#65288;i&#65289;&#27839;&#26102;&#38388;&#32500;&#22797;&#21046;&#21644;&#21305;&#37197;&#20197;&#22686;&#21152;&#20998;&#36776;&#29575;&#65307;&#65288;ii&#65289;&#22797;&#21046;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07588</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Rethinking Scaling Laws for Learning in Strategic Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#22312;&#25112;&#30053;&#29615;&#22659;&#20013;&#23398;&#20064;&#30340;&#27604;&#20363;&#23450;&#24459;&#65292;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#35266;&#28857;&#65292;&#21363;&#27169;&#22411;&#36234;&#22823;&#25110;&#34920;&#36798;&#33021;&#21147;&#36234;&#24378;&#24182;&#19981;&#19968;&#23450;&#20250;&#38543;&#20043;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#20960;&#20010;&#25112;&#30053;&#29615;&#22659;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37096;&#32626;&#21453;&#26144;&#20986;&#19968;&#20010;&#20849;&#35782;&#65306;&#27169;&#22411;&#36234;&#26377;&#34920;&#36798;&#33021;&#21147;&#65292;&#36234;&#25317;&#26377;&#22823;&#37327;&#25968;&#25454;&#65292;&#23601;&#33021;&#25913;&#21892;&#24615;&#33021;&#12290;&#38543;&#30528;&#27169;&#22411;&#22312;&#21508;&#31181;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#37096;&#32626;&#65292;&#23427;&#20204;&#19981;&#21487;&#36991;&#20813;&#22320;&#38754;&#20020;&#30528;&#25112;&#30053;&#29615;&#22659;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#27169;&#22411;&#19982;&#25112;&#30053;&#20114;&#21160;&#23545;&#27604;&#20363;&#23450;&#24459;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36825;&#20010;&#33258;&#28982;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#25112;&#30053;&#20114;&#21160;&#21487;&#20197;&#25171;&#30772;&#20256;&#32479;&#30340;&#27604;&#20363;&#23450;&#24459;&#35266;&#28857;&#65292;&#21363;&#24615;&#33021;&#24182;&#19981;&#19968;&#23450;&#38543;&#30528;&#27169;&#22411;&#30340;&#25193;&#22823;&#21644;/&#25110;&#34920;&#36798;&#33021;&#21147;&#30340;&#22686;&#24378;&#65288;&#21363;&#20351;&#26377;&#26080;&#38480;&#25968;&#25454;&#65289;&#32780;&#21333;&#35843;&#25552;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#25112;&#30053;&#22238;&#24402;&#12289;&#25112;&#30053;&#20998;&#31867;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#20363;&#23376;&#23637;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#20363;&#23376;&#23637;&#31034;&#20102;&#25112;&#30053;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#27169;&#22411;&#25110;&#31574;&#30053;&#31867;&#30340;&#34920;&#36798;&#33021;&#21147;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of ever-larger machine learning models reflects a growing consensus that the more expressive the model$\unicode{x2013}$and the more data one has access to$\unicode{x2013}$the more one can improve performance. As models get deployed in a variety of real world scenarios, they inevitably face strategic environments. In this work, we consider the natural question of how the interplay of models and strategic interactions affects scaling laws. We find that strategic interactions can break the conventional view of scaling laws$\unicode{x2013}$meaning that performance does not necessarily monotonically improve as models get larger and/ or more expressive (even with infinite data). We show the implications of this phenomenon in several contexts including strategic regression, strategic classification, and multi-agent reinforcement learning through examples of strategic environments in which$\unicode{x2013}$by simply restricting the expressivity of one's model or policy class$\uni
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.07586</link><description>&lt;p&gt;
&#25581;&#31034;&#29305;&#23450;&#32676;&#20307;&#30340;&#20998;&#24067;&#24335;&#27010;&#24565;&#28418;&#31227;: &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#30830;&#20445;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#25512;&#21160;&#20102;&#24320;&#21457;&#26088;&#22312;&#20943;&#23569;&#20915;&#31574;&#36807;&#31243;&#20013;&#27495;&#35270;&#32467;&#26524;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#22312;&#36825;&#26041;&#38754;&#30340;&#24320;&#25299;&#24615;&#21162;&#21147;&#12290;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#19968;&#20010;&#32676;&#20307;&#38543;&#26102;&#38388;&#32463;&#21382;&#27010;&#24565;&#28418;&#31227;&#65292;&#32780;&#21478;&#19968;&#20010;&#32676;&#20307;&#21364;&#27809;&#26377;&#65292;&#23548;&#33268;&#20844;&#24179;&#24615;&#19979;&#38477;&#65292;&#21363;&#20351;&#20934;&#30830;&#24615;&#20445;&#25345;&#30456;&#23545;&#31283;&#23450;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20998;&#24067;&#24335;&#24615;&#36136;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#29420;&#31435;&#32463;&#21382;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#65292;&#21516;&#26102;&#20173;&#20849;&#20139;&#30456;&#21516;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20174;&#32780;&#21019;&#36896;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#29615;&#22659;&#26469;&#32500;&#25345;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#20043;&#19968;&#26159;&#23545;&#32676;&#20307;&#29305;&#23450;&#30340;&#27010;&#24565;&#28418;&#31227;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#20869;&#37096;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and intr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26550;&#26500;&#35774;&#35745;&#20915;&#31574;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#23454;&#29616;&#32511;&#33394;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#20851;&#38190;&#29615;&#33410;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#19982;&#36136;&#37327;&#29305;&#24615;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.07585</link><description>&lt;p&gt;
&#20026;&#23454;&#29616;&#32511;&#33394;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#32780;&#30830;&#23450;&#30340;&#26550;&#26500;&#35774;&#35745;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Identifying architectural design decisions for achieving green ML serving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26550;&#26500;&#35774;&#35745;&#20915;&#31574;&#30340;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#23454;&#29616;&#32511;&#33394;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#20851;&#38190;&#29615;&#33410;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#19982;&#36136;&#37327;&#29305;&#24615;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20351;&#29992;&#26085;&#30410;&#22686;&#38271;&#65292;&#20851;&#27880;&#28857;&#36880;&#28176;&#36716;&#21521;&#23427;&#20204;&#19981;&#26029;&#22686;&#21152;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#34429;&#28982;&#20154;&#20204;&#24050;&#32463;&#24320;&#22987;&#20851;&#27880;&#35757;&#32451;&#38454;&#27573;&#30340;&#33021;&#28304;&#28040;&#32791;&#65292;&#20294;&#36739;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#25512;&#26029;&#38454;&#27573;&#12290;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#25512;&#26029;&#32780;&#35328;&#65292;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19982;&#29992;&#25143;&#35775;&#38382;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#36827;&#34892;&#32465;&#23450;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#65292;&#26159;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#25928;&#29575;&#30340;&#20851;&#38190;&#20294;&#40092;&#20026;&#20154;&#30693;&#30340;&#27493;&#39588;&#12290;&#26412;&#25991;&#22312;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#35774;&#35745;&#20915;&#31574;&#21644;&#32511;&#33394;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#36827;&#34892;&#25991;&#29486;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#12290;&#30446;&#30340;&#26159;&#20174;&#23398;&#32773;&#21644;&#23454;&#36341;&#32773;&#30340;&#35282;&#24230;&#65292;&#20998;&#26512;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26550;&#26500;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20102;&#35299;&#24182;&#30830;&#23450;&#20854;&#19982;&#36136;&#37327;&#29305;&#24615;&#30340;&#20851;&#32852;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#25991;&#29486;&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;(i)&#30830;&#23450;&#20102;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#26550;&#26500;&#35774;&#35745;&#20915;&#31574;&#20197;&#21450;&#23427;&#20204;&#23545;&#24212;&#30340;&#32452;&#20214;&#21644;&#30456;&#20851;&#25216;&#26415;&#26632;&#65292;(ii)&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#39046;&#22495;&#36136;&#37327;&#29305;&#24615;&#30340;&#27010;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing use of large machine learning models highlights concerns about their increasing computational demands. While the energy consumption of their training phase has received attention, fewer works have considered the inference phase. For ML inference, the binding of ML models to the ML system for user access, known as ML serving, is a critical yet understudied step for achieving efficiency in ML applications.   We examine the literature in ML architectural design decisions and Green AI, with a special focus on ML serving. The aim is to analyze ML serving architectural design decisions for the purpose of understanding and identifying them with respect to quality characteristics from the point of view of researchers and practitioners in the context of ML serving literature.   Our results (i) identify ML serving architectural design decisions along with their corresponding components and associated technological stack, and (ii) provide an overview of the quality characteristics stu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07570</link><description>&lt;p&gt;
&#21482;&#26377;&#26354;&#32447;&#24418;&#29366;&#26377;&#20851;&#65306;&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07570
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;GTT&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#36890;&#36947;&#32423;&#21035;&#30340;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#65292;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;General Time Transformer (GTT)&#65292;&#19968;&#31181;&#20165;&#26377;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;GTT&#22312;&#19968;&#20010;&#21253;&#21547;2&#20159;&#20010;&#39640;&#36136;&#37327;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#36880;&#36890;&#36947;&#30340;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#39044;&#27979;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#26679;&#26412;&#34920;&#31034;&#20026;&#19968;&#31995;&#21015;&#38750;&#37325;&#21472;&#30340;&#26354;&#32447;&#24418;&#29366;&#65292;&#20855;&#26377;&#32479;&#19968;&#30340;&#25968;&#20540;&#22823;&#23567;&#12290;GTT&#22312;&#36890;&#36947;&#32423;&#21035;&#19978;&#36890;&#36807;&#39044;&#27979;&#36807;&#21435;&#26354;&#32447;&#24418;&#29366;&#30340;&#31383;&#21475;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#26354;&#32447;&#24418;&#29366;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GTT&#22312;&#26410;&#35265;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#33021;&#21147;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;GTT&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#35268;&#27169;&#21464;&#21270;&#30340;&#24433;&#21709;&#65292;&#35266;&#23519;&#21040;&#22312;&#38646;&#26679;&#26412;&#22810;&#20803;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#65292;&#35268;&#27169;&#23450;&#24459;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;1-WL&#31639;&#27861;&#22312;&#22270;&#21516;&#26500;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22686;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;&#23376;&#22270;&#20449;&#24687;&#21644;&#32463;&#20856;&#30340;&#36793;&#32536;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#26356;&#39640;&#34920;&#36798;&#21147;&#19982;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26465;&#20214;&#12290;&#26799;&#24230;&#27969;&#20063;&#34987;&#35777;&#26126;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07568</link><description>&lt;p&gt;
Weisfeiler-Leman&#22312;&#36793;&#32536;&#26465;&#20214;&#19979;&#30340;&#26356;&#39640;&#34920;&#36798;&#21147;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Leman at the margin: When more expressivity matters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07568
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;1-WL&#31639;&#27861;&#22312;&#22270;&#21516;&#26500;&#38382;&#39064;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22686;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#23545;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#24182;&#19981;&#24635;&#26159;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24341;&#20837;&#23376;&#22270;&#20449;&#24687;&#21644;&#32463;&#20856;&#30340;&#36793;&#32536;&#29702;&#35770;&#65292;&#25506;&#32034;&#20102;&#26356;&#39640;&#34920;&#36798;&#21147;&#19982;&#25913;&#36827;&#27867;&#21270;&#24615;&#33021;&#30340;&#26465;&#20214;&#12290;&#26799;&#24230;&#27969;&#20063;&#34987;&#35777;&#26126;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Leman&#31639;&#27861;&#65288;1-WL&#65289;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#29992;&#20110;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#26368;&#36817;&#65292;&#35813;&#31639;&#27861;&#22312;&#29702;&#35299;&#20256;&#36882;&#28040;&#24687;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#20197;&#21450;&#20316;&#20026;&#22270;&#26680;&#20989;&#25968;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;1-WL&#22312;&#21306;&#20998;&#38750;&#21516;&#26500;&#22270;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;MPNN&#21644;&#26680;&#26550;&#26500;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22686;&#24378;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#36890;&#36807;&#22270;&#21516;&#26500;&#26469;&#35266;&#23519;&#26102;&#65292;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#22312;&#35299;&#37322;&#20854;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#27934;&#23519;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30528;&#37325;&#22312;1-WL&#21644;MPNN&#20013;&#24341;&#20837;&#23376;&#22270;&#20449;&#24687;&#65292;&#24182;&#36816;&#29992;&#32463;&#20856;&#30340;&#36793;&#32536;&#29702;&#35770;&#26469;&#30740;&#31350;&#26550;&#26500;&#30340;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#19982;&#25913;&#36827;&#30340;&#27867;&#21270;&#24615;&#33021;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26799;&#24230;&#27969;&#22914;&#20309;&#25512;&#21160;&#27169;&#22411;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler-Leman algorithm ($1$-WL) is a well-studied heuristic for the graph isomorphism problem. Recently, the algorithm has played a prominent role in understanding the expressive power of message-passing graph neural networks (MPNNs) and being effective as a graph kernel. Despite its success, $1$-WL faces challenges in distinguishing non-isomorphic graphs, leading to the development of more expressive MPNN and kernel architectures. However, the relationship between enhanced expressivity and improved generalization performance remains unclear. Here, we show that an architecture's expressivity offers limited insights into its generalization performance when viewed through graph isomorphism. Moreover, we focus on augmenting $1$-WL and MPNNs with subgraph information and employ classical margin theory to investigate the conditions under which an architecture's increased expressivity aligns with improved generalization performance. In addition, we show that gradient flow pushes the 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#24230;&#20248;&#21270;&#30340;&#22266;&#23450;&#28857;&#36817;&#23384;&#20648;&#25968;&#23383;&#22788;&#29702;&#21333;&#20803;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#12290;&#35813;&#22788;&#29702;&#21333;&#20803;&#22312;&#20445;&#25345;&#39640;&#25928;&#33021;&#37327;&#21644;&#38754;&#31215;&#25928;&#29575;&#20197;&#21450;&#20302;&#24310;&#36831;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#25903;&#25345;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#28608;&#27963;&#27493;&#39588;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#21534;&#21520;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07549</link><description>&lt;p&gt;
&#29992;&#20110;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#30340;&#31934;&#24230;&#20248;&#21270;&#22266;&#23450;&#28857;&#36817;&#23384;&#20648;&#25968;&#23383;&#22788;&#29702;&#21333;&#20803;
&lt;/p&gt;
&lt;p&gt;
A Precision-Optimized Fixed-Point Near-Memory Digital Processing Unit for Analog In-Memory Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07549
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#24230;&#20248;&#21270;&#30340;&#22266;&#23450;&#28857;&#36817;&#23384;&#20648;&#25968;&#23383;&#22788;&#29702;&#21333;&#20803;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#12290;&#35813;&#22788;&#29702;&#21333;&#20803;&#22312;&#20445;&#25345;&#39640;&#25928;&#33021;&#37327;&#21644;&#38754;&#31215;&#25928;&#29575;&#20197;&#21450;&#20302;&#24310;&#36831;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#25903;&#25345;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#28608;&#27963;&#27493;&#39588;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#20934;&#30830;&#24615;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#21534;&#21520;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#26159;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#21644;&#39640;&#33021;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#25512;&#29702;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#19968;&#23450;&#37327;&#30340;&#25968;&#23383;&#21518;&#22788;&#29702;&#26469;&#22788;&#29702;&#19982;&#20869;&#23384;&#35774;&#22791;&#30456;&#20851;&#30340;&#30005;&#36335;&#19981;&#21305;&#37197;&#21644;&#38750;&#29702;&#24819;&#29305;&#24615;&#12290;&#39640;&#25928;&#30340;&#36817;&#23384;&#20648;&#25968;&#23383;&#36923;&#36753;&#23545;&#20110;&#20445;&#25345;&#27169;&#25311;&#20869;&#23384;&#35745;&#31639;&#30340;&#39640;&#38754;&#31215;/&#33021;&#37327;&#25928;&#29575;&#21644;&#20302;&#24310;&#36831;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#31995;&#32479;&#37319;&#29992;&#26377;&#38480;&#30340;&#24182;&#34892;&#21270;&#33021;&#21147;&#21644;&#39640;&#24310;&#36831;&#30340;&#28014;&#28857;16&#20301;&#65288;FP16&#65289;&#31639;&#26415;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22266;&#23450;&#28857;&#31639;&#26415;&#30340;&#36817;&#23384;&#20648;&#25968;&#23383;&#22788;&#29702;&#21333;&#20803;&#65288;NMPU&#65289;&#12290;&#23427;&#22312;&#20445;&#25345;&#26368;&#23567;&#38754;&#31215;&#24320;&#38144;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#31934;&#24230;&#21644;&#26356;&#39640;&#30340;&#35745;&#31639;&#21534;&#21520;&#37327;&#12290;&#27492;&#22806;&#65292;NMPU&#25903;&#25345;&#26631;&#20934;&#30340;&#28145;&#24230;&#23398;&#20064;&#28608;&#27963;&#27493;&#39588;&#65292;&#22914;ReLU&#21644;&#25209;&#37327;&#24402;&#19968;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;14nm CMOS&#25216;&#26415;&#23545;NMPU&#35774;&#35745;&#36827;&#34892;&#29289;&#29702;&#23454;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#24615;&#33021;&#12289;&#21151;&#32791;&#21644;&#38754;&#31215;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog In-Memory Computing (AIMC) is an emerging technology for fast and energy-efficient Deep Learning (DL) inference. However, a certain amount of digital post-processing is required to deal with circuit mismatches and non-idealities associated with the memory devices. Efficient near-memory digital logic is critical to retain the high area/energy efficiency and low latency of AIMC. Existing systems adopt Floating Point 16 (FP16) arithmetic with limited parallelization capability and high latency. To overcome these limitations, we propose a Near-Memory digital Processing Unit (NMPU) based on fixed-point arithmetic. It achieves competitive accuracy and higher computing throughput than previous approaches while minimizing the area overhead. Moreover, the NMPU supports standard DL activation steps, such as ReLU and Batch Normalization. We perform a physical implementation of the NMPU design in a 14 nm CMOS technology and provide detailed performance, power, and area assessments. We valid
&lt;/p&gt;</description></item><item><title>TransAxx&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#24211;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#23545;Vision Transformer&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#24863;&#30693;&#24494;&#35843;&#65292;&#26469;&#25552;&#39640;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07545</link><description>&lt;p&gt;
TransAxx&#65306;&#20855;&#26377;&#36817;&#20284;&#35745;&#31639;&#33021;&#21147;&#30340;&#39640;&#25928;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TransAxx: Efficient Transformers with Approximate Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07545
&lt;/p&gt;
&lt;p&gt;
TransAxx&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#24211;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#23545;Vision Transformer&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#24863;&#30693;&#24494;&#35843;&#65292;&#26469;&#25552;&#39640;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#26550;&#26500;&#24341;&#20837;&#30340;Vision Transformer (ViT)&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#24456;&#22823;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#24448;&#24448;&#25104;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#39640;&#35745;&#31639;&#38656;&#27714;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37319;&#29992;&#36817;&#20284;&#20056;&#27861;&#22120;&#26469;&#35299;&#20915;DNN&#21152;&#36895;&#22120;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#38382;&#39064;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25506;&#32034;&#20854;&#22312;ViT&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransAxx&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#34892;&#30340;PyTorch&#24211;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#24555;&#36895;&#25903;&#25345;&#36817;&#20284;&#31639;&#26415;&#65292;&#20197;&#26080;&#32541;&#22320;&#35780;&#20272;&#36817;&#20284;&#35745;&#31639;&#23545;&#20110;DNN (&#22914;ViT&#27169;&#22411;)&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;TransAxx&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#27169;&#22411;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23545;&#36817;&#20284;&#20056;&#27861;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#36817;&#20284;&#24863;&#30693;&#30340;&#24494;&#35843;&#20197;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36817;&#20284;&#21152;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07543</link><description>&lt;p&gt;
&#32473;&#25105;&#30475;&#24590;&#20040;&#20570;&#65306;&#35299;&#37322;&#22312;&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Show Me How It's Done: The Role of Explanations in Fine-Tuning Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#20351;&#29992;&#35299;&#37322;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26174;&#33879;&#22909;&#22788;&#12290;&#19982;&#25552;&#31034;&#26041;&#24335;&#19981;&#21516;&#65292;&#32454;&#35843;&#20801;&#35768;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#21644;&#26356;&#26032;&#21442;&#25968;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#32454;&#35843;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21253;&#21547;&#36755;&#20986;&#35299;&#37322;&#32780;&#38750;&#20165;&#21576;&#29616;&#31572;&#26696;&#30340;&#25968;&#25454;&#26469;&#23545;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#21482;&#26377;6000&#19975;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20174;&#36825;&#31181;&#26041;&#27861;&#20013;&#33719;&#30410;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35814;&#32454;&#30340;&#35299;&#37322;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#26356;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#20110;&#36739;&#22823;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#26080;&#35770;&#35299;&#37322;&#30340;&#38271;&#24230;&#22914;&#20309;&#65292;&#37117;&#21487;&#20197;&#33719;&#24471;&#20960;&#20046;&#30456;&#21516;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35299;&#37322;&#30340;&#21152;&#20837;&#20351;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#20043;&#21069;&#26080;&#27861;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#23613;&#31649;&#23384;&#22312;&#25361;&#25112;&#65292;&#20294;&#35299;&#37322;&#22312;&#32454;&#35843;&#35821;&#35328;&#27169;&#22411;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research demonstrates the significant benefits of using fine-tuning with explanations to enhance the performance of language models. Unlike prompting, which maintains the model's parameters, fine-tuning allows the model to learn and update its parameters during a training phase. In this study, we applied fine-tuning to various sized language models using data that contained explanations of the output rather than merely presenting the answers. We found that even smaller language models with as few as 60 million parameters benefited substantially from this approach. Interestingly, our results indicated that the detailed explanations were more beneficial to smaller models than larger ones, with the latter gaining nearly the same advantage from any form of explanation, irrespective of its length. Additionally, we demonstrate that the inclusion of explanations enables the models to solve tasks that they were not able to solve without explanations. Lastly, we argue that despite the chall
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#25439;&#21516;&#24577;&#21387;&#32553;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24037;&#20316;&#33410;&#28857;&#32423;&#21035;&#30340;&#21387;&#32553;&#21644;&#32593;&#32476;&#20869;&#32858;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20445;&#35777;&#20102;&#35757;&#32451;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07529</link><description>&lt;p&gt;
&#20351;&#29992;&#26080;&#25439;&#21516;&#24577;&#21387;&#32553;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Accelerating Distributed Deep Learning using Lossless Homomorphic Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26080;&#25439;&#21516;&#24577;&#21387;&#32553;&#21152;&#36895;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#24037;&#20316;&#33410;&#28857;&#32423;&#21035;&#30340;&#21387;&#32553;&#21644;&#32593;&#32476;&#20869;&#32858;&#21512;&#65292;&#23454;&#29616;&#39640;&#25928;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20445;&#35777;&#20102;&#35757;&#32451;&#31934;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;DNN&#27169;&#22411;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#22797;&#26434;&#24615;&#21644;&#35268;&#27169;&#22686;&#38271;&#65292;&#20998;&#24067;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#36890;&#20449;&#24320;&#38144;&#20063;&#38543;&#20043;&#22686;&#21152;&#65292;&#25104;&#20026;&#25361;&#25112;&#20998;&#24067;&#24335;&#35757;&#32451;&#31995;&#32479;&#21487;&#25193;&#23637;&#24615;&#30340;&#37325;&#35201;&#29942;&#39048;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#26088;&#22312;&#36890;&#36807;&#24037;&#20316;&#33410;&#28857;&#32423;&#21035;&#30340;&#21387;&#32553;&#21644;&#32593;&#32476;&#20869;&#32858;&#21512;&#26469;&#32531;&#35299;&#36825;&#20010;&#29942;&#39048;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#26377;&#25928;&#24179;&#34913;&#21387;&#32553;&#25928;&#26524;&#21644;&#35745;&#31639;&#24320;&#38144;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#25972;&#20307;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21387;&#32553;&#31639;&#27861;&#65292;&#23558;&#24037;&#20316;&#33410;&#28857;&#32423;&#21035;&#30340;&#21387;&#32553;&#21644;&#32593;&#32476;&#20869;&#32858;&#21512;&#26377;&#25928;&#22320;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26082;&#26159;&#21516;&#24577;&#30340;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#32593;&#32476;&#20869;&#32858;&#21512;&#65292;&#21448;&#26159;&#26080;&#25439;&#30340;&#65292;&#30830;&#20445;&#22312;&#35757;&#32451;&#31934;&#24230;&#19978;&#27809;&#26377;&#22949;&#21327;&#12290;&#22312;&#21387;&#32553;&#21644;&#35745;&#31639;&#25928;&#29575;&#19978;&#29702;&#35770;&#19978;&#26159;&#26368;&#20248;&#30340;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#31181;DNN&#27169;&#22411;&#65288;&#22914;NCF&#65292;LSTM&#65292;VGG19&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks (DNNs) grow in complexity and size, the resultant increase in communication overhead during distributed training has become a significant bottleneck, challenging the scalability of distributed training systems. Existing solutions, while aiming to mitigate this bottleneck through worker-level compression and in-network aggregation, fall short due to their inability to efficiently reconcile the trade-offs between compression effectiveness and computational overhead, hindering overall performance and scalability. In this paper, we introduce a novel compression algorithm that effectively merges worker-level compression with in-network aggregation. Our solution is both homomorphic, allowing for efficient in-network aggregation without CPU/GPU processing, and lossless, ensuring no compromise on training accuracy. Theoretically optimal in compression and computational efficiency, our approach is empirically validated across diverse DNN models such as NCF, LSTM, VGG19, 
&lt;/p&gt;</description></item><item><title>NeuralSentinel&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#33021;&#22815;&#39564;&#35777;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#36890;&#36807;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#26469;&#22686;&#24378;&#23545;&#36825;&#19968;&#26032;&#31995;&#32479;&#30340;&#20449;&#20219;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07506</link><description>&lt;p&gt;
NeuralSentinel: &#20445;&#38556;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
NeuralSentinel: Safeguarding Neural Network Reliability and Trustworthiness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07506
&lt;/p&gt;
&lt;p&gt;
NeuralSentinel&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#33021;&#22815;&#39564;&#35777;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#36890;&#36807;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#26469;&#22686;&#24378;&#23545;&#36825;&#19968;&#26032;&#31995;&#32479;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20351;&#29992;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#20943;&#23569;&#24453;&#20998;&#26512;&#30340;&#25968;&#25454;&#37327;&#65292;&#20943;&#36731;&#29992;&#25143;&#30340;&#24037;&#20316;&#37327;&#24182;&#20445;&#25345;&#39640;&#20934;&#30830;&#29575;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#19968;&#26032;&#20803;&#32032;&#24341;&#20837;&#31995;&#32479;&#20013;&#21364;&#25104;&#20026;&#20102;&#25915;&#20987;&#28857;&#65292;&#21487;&#33021;&#20250;&#21361;&#21450;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20010;&#26032;&#30340;&#24773;&#26223;&#25552;&#20986;&#20102;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#20197;&#21450;&#20851;&#20110;&#20854;&#21709;&#24212;&#20915;&#31574;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#24212;&#29992;&#20110;&#20851;&#38190;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#12289;&#21270;&#23398;&#21644;&#30005;&#21147;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;NeuralSentinel&#65288;NS&#65289;&#65292;&#19968;&#20010;&#33021;&#22815;&#39564;&#35777;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#38752;&#24615;&#21644;&#21487;&#20449;&#24230;&#30340;&#24037;&#20855;&#12290;&#35813;&#24037;&#20855;&#32467;&#21512;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#20197;&#21450;&#21487;&#35299;&#37322;&#24615;&#27010;&#24565;&#65292;&#21487;&#20197;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#21592;&#36890;&#36807;&#29702;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#26469;&#22686;&#24378;&#23545;&#36825;&#19968;&#26032;&#31995;&#32479;&#30340;&#20449;&#20219;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage of Artificial Intelligence (AI) systems has increased exponentially, thanks to their ability to reduce the amount of data to be analyzed, the user efforts and preserving a high rate of accuracy. However, introducing this new element in the loop has converted them into attacked points that can compromise the reliability of the systems. This new scenario has raised crucial challenges regarding the reliability and trustworthiness of the AI models, as well as about the uncertainties in their response decisions, becoming even more crucial when applied in critical domains such as healthcare, chemical, electrical plants, etc. To contain these issues, in this paper, we present NeuralSentinel (NS), a tool able to validate the reliability and trustworthiness of AI models. This tool combines attack and defence strategies and explainability concepts to stress an AI model and help non-expert staff increase their confidence in this new system by understanding the model decisions. NS provid
&lt;/p&gt;</description></item><item><title>ClusterTabNet&#26159;&#19968;&#31181;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#34920;&#26684;&#32467;&#26500;&#20026;&#21333;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20854;&#37051;&#25509;&#30697;&#38453;&#65292;&#23454;&#29616;&#23545;&#34920;&#26684;&#30340;&#26816;&#27979;&#21644;&#32467;&#26500;&#35782;&#21035;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;ClusterTabNet&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07502</link><description>&lt;p&gt;
ClusterTabNet: &#19968;&#31181;&#29992;&#20110;&#34920;&#26684;&#26816;&#27979;&#21644;&#34920;&#26684;&#32467;&#26500;&#35782;&#21035;&#30340;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ClusterTabNet: Supervised clustering method for table detection and table structure recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07502
&lt;/p&gt;
&lt;p&gt;
ClusterTabNet&#26159;&#19968;&#31181;&#30417;&#30563;&#32858;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#37322;&#34920;&#26684;&#32467;&#26500;&#20026;&#21333;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#20854;&#37051;&#25509;&#30697;&#38453;&#65292;&#23454;&#29616;&#23545;&#34920;&#26684;&#30340;&#26816;&#27979;&#21644;&#32467;&#26500;&#35782;&#21035;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;ClusterTabNet&#20855;&#26377;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#32858;&#31867;&#65292;&#24182;&#24212;&#29992;&#20110;&#26816;&#27979;&#21644;&#35782;&#21035;OCR&#36755;&#20986;&#20013;&#30340;&#34920;&#26684;&#12290;&#25105;&#20204;&#23558;&#34920;&#26684;&#32467;&#26500;&#33258;&#19979;&#32780;&#19978;&#35299;&#37322;&#20026;&#19968;&#32452;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#22270;&#65288;&#23646;&#20110;&#21516;&#19968;&#34892;&#12289;&#21015;&#12289;&#26631;&#39064;&#20197;&#21450;&#21516;&#19968;&#34920;&#26684;&#65289;&#65292;&#24182;&#20351;&#29992;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#39044;&#27979;&#20854;&#37051;&#25509;&#30697;&#38453;&#12290;&#25105;&#20204;&#22312;PubTables-1M&#25968;&#25454;&#38598;&#20197;&#21450;PubTabNet&#21644;FinTabNet&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#26041;&#27861;&#65288;&#22914;DETR&#21644;Faster R-CNN&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel deep-learning-based method to cluster words in documents which we apply to detect and recognize tables given the OCR output. We interpret table structure bottom-up as a graph of relations between pairs of words (belonging to the same row, column, header, as well as to the same table) and use a transformer encoder model to predict its adjacency matrix. We demonstrate the performance of our method on the PubTables-1M dataset as well as PubTabNet and FinTabNet datasets. Compared to the current state-of-the-art detection methods such as DETR and Faster R-CNN, our method achieves similar or better accuracy, while requiring a significantly smaller model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25429;&#33719;&#23383;&#33410;&#32423;&#27969;&#37327;&#22270;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#19981;&#21464;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#36328;&#32423;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#21253;&#32423;&#21035;&#20219;&#21153;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#33021;&#22815;&#34987;&#27969;&#32423;&#21035;&#20219;&#21153;&#21033;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07501</link><description>&lt;p&gt;
&#19968;&#21015;&#28779;&#36710;&#23436;&#25104;&#20004;&#20010;&#20219;&#21153;&#65306;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
One Train for Two Tasks: An Encrypted Traffic Classification Framework Using Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25429;&#33719;&#23383;&#33410;&#32423;&#27969;&#37327;&#22270;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#19981;&#21464;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#36328;&#32423;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#25968;&#25454;&#21253;&#32423;&#21035;&#20219;&#21153;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#33021;&#22815;&#34987;&#27969;&#32423;&#21035;&#20219;&#21153;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#32593;&#32476;&#23433;&#20840;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#21152;&#23494;&#27969;&#37327;&#20998;&#31867;&#24050;&#25104;&#20026;&#24403;&#21069;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#36827;&#34892;&#27969;&#37327;&#20998;&#31867;&#26102;&#27809;&#26377;&#20805;&#20998;&#32771;&#34385;&#25968;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#29420;&#31435;&#22320;&#35757;&#32451;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#36825;&#26159;&#20887;&#20313;&#30340;&#65292;&#22240;&#20026;&#25968;&#25454;&#21253;&#32423;&#21035;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#25968;&#25454;&#21253;&#34920;&#31034;&#21487;&#20197;&#34987;&#27969;&#32423;&#21035;&#20219;&#21153;&#21033;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#30340;&#26102;&#22495;&#34701;&#21512;&#32534;&#30721;&#22120;&#65288;CLE-TFE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22686;&#24378;&#25968;&#25454;&#21253;&#32423;&#21035;&#21644;&#27969;&#32423;&#21035;&#34920;&#31034;&#65292;&#24182;&#22312;&#23383;&#33410;&#32423;&#27969;&#37327;&#22270;&#19978;&#36827;&#34892;&#22270;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25429;&#33719;&#23383;&#33410;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#36328;&#32423;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As network security receives widespread attention, encrypted traffic classification has become the current research focus. However, existing methods conduct traffic classification without sufficiently considering the common characteristics between data samples, leading to suboptimal performance. Moreover, they train the packet-level and flow-level classification tasks independently, which is redundant because the packet representations learned in the packet-level task can be exploited by the flow-level task. Therefore, in this paper, we propose an effective model named a Contrastive Learning Enhanced Temporal Fusion Encoder (CLE-TFE). In particular, we utilize supervised contrastive learning to enhance the packet-level and flow-level representations and perform graph data augmentation on the byte-level traffic graph so that the fine-grained semantic-invariant characteristics between bytes can be captured through contrastive learning. We also propose cross-level multi-task learning, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26367;&#20195;&#20102;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#36817;&#20284;&#24179;&#28369;&#20998;&#31867;&#22120;&#26041;&#38754;&#30340;&#31934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07498</link><description>&lt;p&gt;
&#21152;&#36895;&#24179;&#28369;&#65306;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accelerated Smoothing: A Scalable Approach to Randomized Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#38543;&#26426;&#24179;&#28369;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#26367;&#20195;&#20102;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#23637;&#31034;&#20102;&#20854;&#22312;&#36817;&#20284;&#24179;&#28369;&#20998;&#31867;&#22120;&#26041;&#38754;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#20197;&#20854;&#20351;&#29992;&#29305;&#23450;&#20998;&#24067;&#30340;&#24179;&#28369;&#22122;&#22768;&#30830;&#20445;&#24179;&#28369;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#32780;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#21487;&#35777;&#26126;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#20013;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#30340;&#20351;&#29992;&#24341;&#20837;&#20102;&#19968;&#20010;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#22240;&#32032;&#65292;&#38480;&#21046;&#20102;&#22312;&#36739;&#22823;&#35268;&#27169;&#19978;&#23454;&#36341;&#38543;&#26426;&#24179;&#28369;&#30340;&#21487;&#34892;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#35757;&#32451;&#26367;&#20195;&#20102;&#33945;&#29305;&#21345;&#27931;&#25277;&#26679;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36817;&#20284;&#24179;&#28369;&#20998;&#31867;&#22120;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#31934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#21152;&#36895;&#20102;&#40065;&#26834;&#21322;&#24452;&#35748;&#35777;&#36807;&#31243;&#65292;&#22312;&#35745;&#31639;&#26102;&#38388;&#19978;&#25552;&#20379;&#20102;&#36817;600&#20493;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#38543;&#26426;&#24179;&#28369;&#20013;&#30340;&#35745;&#31639;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomized smoothing has emerged as a potent certifiable defense against adversarial attacks by employing smoothing noises from specific distributions to ensure the robustness of a smoothed classifier. However, the utilization of Monte Carlo sampling in this process introduces a compute-intensive element, which constrains the practicality of randomized smoothing on a larger scale. To address this limitation, we propose a novel approach that replaces Monte Carlo sampling with the training of a surrogate neural network. Through extensive experimentation in various settings, we demonstrate the efficacy of our approach in approximating the smoothed classifier with remarkable precision. Furthermore, we demonstrate that our approach significantly accelerates the robust radius certification process, providing nearly $600$X improvement in computation time, overcoming the computational bottlenecks associated with traditional randomized smoothing.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#20102;&#35299;&#38450;&#24481;&#27169;&#22411;&#30340;&#24615;&#33021;&#22914;&#20309;&#34987;&#20462;&#25913;&#12290;</title><link>https://arxiv.org/abs/2402.07496</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35270;&#21270;&#21160;&#24577;&#39118;&#38505;&#35780;&#20272;&#26469;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#23545;&#25239;&#23545;&#25239;&#26679;&#26412;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Learning defenses Against Adversarial Examples Through Visualizations for Dynamic Risk Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07496
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23545;&#25239;&#26679;&#26412;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#20102;&#35299;&#38450;&#24481;&#27169;&#22411;&#30340;&#24615;&#33021;&#22914;&#20309;&#34987;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24050;&#32463;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24320;&#22987;&#22312;&#39118;&#38505;&#20851;&#38190;&#30340;&#20219;&#21153;&#20013;&#20351;&#29992;&#12290;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35823;&#35786;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#20107;&#25925;&#29978;&#33267;&#27515;&#20129;&#12290;&#36825;&#20010;&#38382;&#39064;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#20852;&#36259;&#65292;&#20182;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#38271;&#20018;&#28431;&#27934;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#24212;&#35813;&#36827;&#34892;&#38450;&#24481;&#12290;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#26159;&#30740;&#31350;&#20154;&#21592;&#24191;&#20026;&#30693;&#26195;&#30340;&#19968;&#31181;&#25915;&#20987;&#65292;&#20182;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#38450;&#24481;&#26041;&#27861;&#26469;&#36991;&#20813;&#36825;&#31181;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#20687;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19968;&#26679;&#19981;&#36879;&#26126;&#65292;&#23427;&#20204;&#30340;&#24037;&#20316;&#26041;&#24335;&#20173;&#28982;&#26410;&#30693;&#12290;&#36825;&#23601;&#26159;&#20026;&#20160;&#20040;&#36890;&#36807;&#21487;&#35270;&#21270;&#23427;&#20204;&#22914;&#20309;&#25913;&#21464;&#30446;&#26631;&#27169;&#22411;&#30340;&#34892;&#20026;&#26469;&#20102;&#35299;&#34987;&#38450;&#24481;&#27169;&#22411;&#30340;&#24615;&#33021;&#22914;&#20309;&#34987;&#20462;&#25913;&#26159;&#26377;&#36259;&#30340;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#36873;&#25321;&#20102;&#19968;&#20123;&#23545;&#25239;&#24615;&#26679;&#26412;&#25915;&#20987;&#30340;&#38450;&#24481;&#26041;&#27861;&#65292;&#20197;&#20415;&#21487;&#20197;&#23545;&#20854;&#34892;&#20026;&#36827;&#34892;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Deep Neural Network models have been developed in different fields, where they have brought many advances. However, they have also started to be used in tasks where risk is critical. A misdiagnosis of these models can lead to serious accidents or even death. This concern has led to an interest among researchers to study possible attacks on these models, discovering a long list of vulnerabilities, from which every model should be defended. The adversarial example attack is a widely known attack among researchers, who have developed several defenses to avoid such a threat. However, these defenses are as opaque as a deep neural network model, how they work is still unknown. This is why visualizing how they change the behavior of the target model is interesting in order to understand more precisely how the performance of the defended model is being modified. For this work, some defenses, against adversarial example attack, have been selected in order to visualize the behav
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;LIGO&#23454;&#26102;&#25968;&#25454;&#20013;&#20449;&#21495;&#26816;&#27979;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25551;&#36848;&#20102;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#21147;&#27874;&#25628;&#32034;&#25361;&#25112;&#36187;&#12290;&#22242;&#38431;TPI FSU Jena&#25552;&#20132;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;O3b&#25968;&#25454;&#65292;&#24182;&#24674;&#22797;&#20102;GWTC-3&#30446;&#24405;&#20013;&#30340;&#30456;&#20851;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.07492</link><description>&lt;p&gt;
&#29992;&#20110;LIGO&#23454;&#26102;&#25968;&#25454;&#20013;&#20449;&#21495;&#26816;&#27979;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks for signal detection in real LIGO data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;LIGO&#23454;&#26102;&#25968;&#25454;&#20013;&#20449;&#21495;&#26816;&#27979;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25551;&#36848;&#20102;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#38382;&#39064;&#30340;&#26426;&#22120;&#23398;&#20064;&#24341;&#21147;&#27874;&#25628;&#32034;&#25361;&#25112;&#36187;&#12290;&#22242;&#38431;TPI FSU Jena&#25552;&#20132;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#24182;&#25104;&#21151;&#23558;&#20854;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;O3b&#25968;&#25454;&#65292;&#24182;&#24674;&#22797;&#20102;GWTC-3&#30446;&#24405;&#20013;&#30340;&#30456;&#20851;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25628;&#32034;&#24341;&#21147;&#27874;&#25506;&#27979;&#22120;&#25968;&#25454;&#20013;&#23547;&#25214;&#26469;&#33258;&#32039;&#20945;&#20108;&#36827;&#21046;&#21512;&#24182;&#30340;&#20449;&#21495;&#26159;&#19968;&#39033;&#35745;&#31639;&#38656;&#27714;&#20005;&#23803;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#34987;&#25552;&#20986;&#26469;&#24212;&#23545;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20986;&#29256;&#29289;&#30340;&#32467;&#26524;&#24448;&#24448;&#22240;&#20026;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#19981;&#21516;&#36873;&#25321;&#32780;&#22823;&#30456;&#24452;&#24237;&#12290;&#26426;&#22120;&#23398;&#20064;&#24341;&#21147;&#27874;&#25628;&#32034;&#25361;&#25112;&#36187;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#32479;&#19968;&#30340;&#26426;&#22120;&#23398;&#20064;&#25628;&#32034;&#35780;&#20272;&#26694;&#26550;&#12290;&#20845;&#20010;&#22242;&#38431;&#25552;&#20132;&#20102;&#36129;&#29486;&#65292;&#20854;&#20013;&#22235;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20004;&#20010;&#26159;&#26368;&#20808;&#36827;&#30340;&#29983;&#20135;&#20998;&#26512;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26469;&#33258;TPI FSU Jena&#22242;&#38431;&#21450;&#20854;&#26356;&#26032;&#29256;&#26412;&#30340;&#25552;&#20132;&#12290;&#25105;&#20204;&#36824;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;O3b&#25968;&#25454;&#65292;&#24182;&#24674;&#22797;&#20102;GWTC-3&#30446;&#24405;&#20013;&#30340;&#30456;&#20851;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching the data of gravitational-wave detectors for signals from compact binary mergers is a computationally demanding task. Recently, machine learning algorithms have been proposed to address current and future challenges. However, the results of these publications often differ greatly due to differing choices in the evaluation procedure. The Machine Learning Gravitational-Wave Search Challenge was organized to resolve these issues and produce a unified framework for machine-learning search evaluation. Six teams submitted contributions, four of which are based on machine learning methods and two are state-of-the-art production analyses. This paper describes the submission from the team TPI FSU Jena and its updated variant. We also apply our algorithm to real O3b data and recover the relevant events of the GWTC-3 catalog.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25216;&#26415;&#25945;&#31243;&#65292;&#37325;&#28857;&#35762;&#35299;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#36866;&#21512;&#21021;&#23398;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#24182;&#19988;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.07487</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25216;&#26415;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25216;&#26415;&#25945;&#31243;&#65292;&#37325;&#28857;&#35762;&#35299;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#36866;&#21512;&#21021;&#23398;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#24182;&#19988;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#38416;&#37322;&#24615;&#25991;&#31456;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#28201;&#21644;&#30340;&#20171;&#32461;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#20004;&#20010;&#20851;&#38190;&#28857;--&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#65292;&#20854;&#20013;&#21253;&#25324;SDE/ODE&#37319;&#26679;&#65292;&#20998;&#25968;&#21305;&#37197;&#25928;&#29575;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31616;&#30701;&#30340;&#35777;&#26126;&#26469;&#35828;&#26126;&#25152;&#36848;&#32467;&#26524;&#30340;&#20027;&#35201;&#24605;&#24819;&#12290;&#26412;&#25991;&#20027;&#35201;&#26159;&#20026;&#20102;&#21521;&#21021;&#23398;&#32773;&#20171;&#32461;&#36825;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#21457;&#29616;&#19968;&#20123;&#20998;&#26512;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning. Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#35299;&#37322;&#24615;&#30340;&#25299;&#25169;&#20445;&#25252;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#26085;&#30410;&#24191;&#27867;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;&#36867;&#36991;&#25915;&#20987;&#26159;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#25915;&#20987;&#65292;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#23545;&#25239;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07480</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#21487;&#35299;&#37322;&#24615;&#30340;&#36867;&#36991;&#25915;&#20987;&#30340;&#25299;&#25169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Topological Safeguard for Evasion Attack based on the Interpretability of Artificial Neural Network Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#35299;&#37322;&#24615;&#30340;&#25299;&#25169;&#20445;&#25252;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;&#36867;&#36991;&#25915;&#20987;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#26085;&#30410;&#24191;&#27867;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#12290;&#36867;&#36991;&#25915;&#20987;&#26159;&#20854;&#20013;&#19968;&#31181;&#24120;&#35265;&#25915;&#20987;&#65292;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33021;&#22815;&#23545;&#25239;&#35813;&#25915;&#20987;&#30340;&#26377;&#25928;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#22312;&#19981;&#21516;&#39046;&#22495;&#25552;&#20986;&#65292;&#20026;&#27599;&#20010;&#39046;&#22495;&#24102;&#26469;&#20102;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#36825;&#20063;&#24102;&#26469;&#20102;&#20851;&#20110;&#32593;&#32476;&#23433;&#20840;&#26041;&#38754;&#30340;&#26032;&#23041;&#32961;&#12290;&#36825;&#20123;&#23454;&#26045;&#30340;&#27169;&#22411;&#24102;&#26469;&#20102;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30456;&#20851;&#30340;&#20960;&#31181;&#28431;&#27934;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20801;&#35768;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#33719;&#24471;&#31169;&#20154;&#20449;&#24687;&#65292;&#29978;&#33267;&#20462;&#25913;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#23545;&#36825;&#20123;&#28431;&#27934;/&#25915;&#20987;&#36827;&#34892;&#30740;&#31350;&#24182;&#35774;&#35745;&#38450;&#24481;&#25514;&#26045;&#20197;&#36991;&#20813;&#25110;&#23545;&#25239;&#23427;&#20204;&#30340;&#20852;&#36259;&#22312;&#30740;&#31350;&#20013;&#26085;&#30410;&#31361;&#20986;&#12290;&#29305;&#21035;&#26159;&#65292;&#33879;&#21517;&#30340;&#36867;&#36991;&#25915;&#20987;&#27491;&#22312;&#34987;&#30740;&#31350;&#20154;&#21592;&#20998;&#26512;&#65292;&#22240;&#27492;&#22312;&#25991;&#29486;&#20013;&#21487;&#20197;&#25214;&#21040;&#20960;&#31181;&#36991;&#20813;&#27492;&#23041;&#32961;&#30340;&#38450;&#24481;&#25514;&#26045;&#12290;&#33258;L-BFG&#31639;&#27861;&#25552;&#20986;&#20197;&#26469;&#65292;&#36825;&#31181;&#23041;&#32961;&#19968;&#30452;&#20851;&#27880;&#30740;&#31350;&#30028;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27809;&#26377;&#36866;&#29992;&#20110;&#25152;&#26377;&#24050;&#30693;&#36867;&#36991;&#31639;&#27861;&#30340;&#23436;&#32654;&#38450;&#24481;&#65292;&#22240;&#27492;&#20173;&#22312;&#19981;&#26029;&#24320;&#21457;&#26032;&#30340;&#24039;&#22937;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last years, Deep Learning technology has been proposed in different fields, bringing many advances in each of them, but identifying new threats in these solutions regarding cybersecurity. Those implemented models have brought several vulnerabilities associated with Deep Learning technology. Moreover, those allow taking advantage of the implemented model, obtaining private information, and even modifying the model's decision-making. Therefore, interest in studying those vulnerabilities/attacks and designing defenses to avoid or fight them is gaining prominence among researchers. In particular, the widely known evasion attack is being analyzed by researchers; thus, several defenses to avoid such a threat can be found in the literature. Since the presentation of the L-BFG algorithm, this threat concerns the research community. However, it continues developing new and ingenious countermeasures since there is no perfect defense for all the known evasion algorithms. In this work, a no
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#21407;&#23376;&#23494;&#24230;&#23637;&#24320;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20302;&#32500;&#23884;&#20837;&#21644;&#21407;&#23376;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07472</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#22312;&#29289;&#36136;&#31185;&#23398;&#21644;&#21270;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cartesian atomic cluster expansion for machine learning interatomic potentials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#21407;&#23376;&#23494;&#24230;&#23637;&#24320;&#26469;&#26367;&#20195;&#20256;&#32479;&#30340;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#20302;&#32500;&#23884;&#20837;&#21644;&#21407;&#23376;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21407;&#23376;&#38388;&#21183;&#27491;&#22312;&#38761;&#26032;&#26448;&#26009;&#31185;&#23398;&#21644;&#21270;&#23398;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#20934;&#30830;&#30340;&#21407;&#23376;&#27169;&#25311;&#12290;&#36825;&#20123;&#21183;&#20989;&#25968;&#36890;&#24120;&#20351;&#29992;&#21407;&#23376;&#22242;&#31751;&#23637;&#24320;&#25110;&#21464;&#25442;&#28040;&#24687;&#20256;&#36882;&#19982;&#29699;&#35856;&#22522;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#26059;&#36716;&#23545;&#31216;&#24615;&#65292;&#20381;&#36182;Clebsch-Gordan&#31995;&#25968;&#20250;&#23548;&#33268;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#21644;&#20887;&#20313;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65306;&#22522;&#20110;&#31515;&#21345;&#23572;&#22352;&#26631;&#30340;&#21407;&#23376;&#23494;&#24230;&#23637;&#24320;&#12290;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#30456;&#20114;&#20316;&#29992;&#20307;&#31995;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21407;&#23376;&#29615;&#22659;&#30340;&#23436;&#25972;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25972;&#21512;&#20102;&#21508;&#31181;&#21270;&#23398;&#20803;&#32032;&#30340;&#20302;&#32500;&#23884;&#20837;&#21644;&#21407;&#23376;&#38388;&#28040;&#24687;&#20256;&#36882;&#12290;&#25152;&#24471;&#21040;&#30340;&#21183;&#20989;&#25968;&#34987;&#21629;&#21517;&#20026;Cartesian Atomic Cluster Expansion(CACE)&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#31995;&#32479;&#20013;&#36827;&#34892;&#39564;&#35777;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#27700;&#12289;&#23567;&#20998;&#23376;&#21644;25&#31181;&#20803;&#32032;&#39640;&#29109;&#21512;&#37329;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning interatomic potentials are revolutionizing large-scale, accurate atomistic modelling in material science and chemistry. These potentials often use atomic cluster expansion or equivariant message passing with spherical harmonics as basis functions. However, the dependence on Clebsch-Gordan coefficients for maintaining rotational symmetry leads to computational inefficiencies and redundancies. We propose an alternative: a Cartesian-coordinates-based atomic density expansion. This approach provides a complete description of atomic environments while maintaining interaction body orders. Additionally, we integrate low-dimensional embeddings of various chemical elements and inter-atomic message passing. The resulting potential, named Cartesian Atomic Cluster Expansion (CACE), exhibits good accuracy, stability, and generalizability. We validate its performance in diverse systems, including bulk water, small molecules, and 25-element high-entropy alloys.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#30340;&#24046;&#20998;&#38544;&#31169;&#21464;&#31181;&#25512;&#23548;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#33410;&#28857;&#20043;&#38388;&#30340;&#20843;&#21350;&#31639;&#27861;&#30456;&#27604;&#65292;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#26356;&#33021;&#25552;&#20379;&#36739;&#22909;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07471</link><description>&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#28216;&#36208;&#30340;&#24046;&#20998;&#38544;&#31169;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Decentralized Learning with Random Walks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07471
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#30340;&#24046;&#20998;&#38544;&#31169;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#30340;&#24046;&#20998;&#38544;&#31169;&#21464;&#31181;&#25512;&#23548;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#33410;&#28857;&#20043;&#38388;&#30340;&#20843;&#21350;&#31639;&#27861;&#30456;&#27604;&#65292;&#38543;&#26426;&#28216;&#36208;&#31639;&#27861;&#26356;&#33021;&#25552;&#20379;&#36739;&#22909;&#30340;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#22323;&#30340;&#26234;&#33021;&#25163;&#26426;&#21462;&#32780;&#20195;&#20043;&#30340;&#26159;&#20256;&#32479;&#31227;&#21160;&#30005;&#35805;&#65292;&#36825;&#26159;&#29616;&#20195;&#31185;&#25216;&#21457;&#23637;&#30340;&#24517;&#28982;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of federated learning comes from the possibility of better scalability and the ability for participants to keep control of their data, improving data security and sovereignty. Unfortunately, sharing model updates also creates a new privacy attack surface. In this work, we characterize the privacy guarantees of decentralized learning with random walk algorithms, where a model is updated by traveling from one node to another along the edges of a communication graph. Using a recent variant of differential privacy tailored to the study of decentralized algorithms, namely Pairwise Network Differential Privacy, we derive closed-form expressions for the privacy loss between each pair of nodes where the impact of the communication topology is captured by graph theoretic quantities. Our results further reveal that random walk algorithms tends to yield better privacy guarantees than gossip algorithms for nodes close from each other. We supplement our theoretical results with empir
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#39640;&#32500;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#38382;&#39064;&#12290;&#19982;&#33945;&#29305;&#21345;&#27931;&#21644;&#26222;&#36890;PINN&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#22788;&#29702;&#19982;&#24067;&#26391;&#36816;&#21160;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.07465</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#39640;&#32500;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Score-Based Physics-Informed Neural Networks for High-Dimensional Fokker-Planck Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#20989;&#25968;&#30340;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#39640;&#32500;&#31119;&#20811;-&#26222;&#26391;&#20811;&#26041;&#31243;&#20013;&#30340;&#32500;&#25968;&#28798;&#38590;&#38382;&#39064;&#12290;&#19982;&#33945;&#29305;&#21345;&#27931;&#21644;&#26222;&#36890;PINN&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#22788;&#29702;&#19982;&#24067;&#26391;&#36816;&#21160;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#24182;&#25552;&#20379;&#24555;&#36895;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31119;&#20811;-&#26222;&#26391;&#20811;&#65288;FP&#65289;&#26041;&#31243;&#26159;&#38543;&#26426;&#36807;&#31243;&#20013;&#30340;&#22522;&#30784;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#39640;&#32500;FP PDE&#26102;&#65292;&#32500;&#25968;&#28798;&#38590;&#65288;CoD&#65289;&#20250;&#24102;&#26469;&#25361;&#25112;&#12290;&#23613;&#31649;&#33945;&#29305;&#21345;&#27931;&#21644;&#26222;&#36890;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#24212;&#23545;CoD&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#19982;&#24067;&#26391;&#36816;&#21160;&#30456;&#20851;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65288;PDF&#65289;&#26102;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#22312;&#39640;&#32500;&#24230;&#19978;&#26174;&#31034;&#20986;&#25968;&#20540;&#35823;&#24046;&#12290;&#28857;&#20540;PDF&#38543;&#30528;&#32500;&#24230;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#19979;&#38477;&#65292;&#36229;&#36807;&#20102;&#25968;&#20540;&#27169;&#25311;&#30340;&#31934;&#24230;&#65292;&#23548;&#33268;&#20102;&#30456;&#24403;&#22823;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#22823;&#35268;&#27169;&#37319;&#26679;&#65292;&#33945;&#29305;&#21345;&#27931;&#26080;&#27861;&#25552;&#20379;&#24555;&#36895;&#37319;&#26679;&#12290;&#36890;&#36807;&#23545;&#26222;&#36890;PINNs&#27169;&#25311;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#65292;&#23558;FP&#26041;&#31243;&#36716;&#21270;&#20026;&#19968;&#20010;&#22256;&#38590;&#30340;HJB&#26041;&#31243;&#65292;&#20854;&#35823;&#24046;&#38543;&#32500;&#25968;&#22686;&#38271;&#36805;&#36895;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#24471;&#20998;&#30340;&#27714;&#35299;&#22120;&#26469;&#25311;&#21512;SDE&#20013;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#24471;&#20998;&#20989;&#25968;&#23450;&#20041;&#20026;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Fokker-Planck (FP) equation is a foundational PDE in stochastic processes. However, curse of dimensionality (CoD) poses challenge when dealing with high-dimensional FP PDEs. Although Monte Carlo and vanilla Physics-Informed Neural Networks (PINNs) have shown the potential to tackle CoD, both methods exhibit numerical errors in high dimensions when dealing with the probability density function (PDF) associated with Brownian motion. The point-wise PDF values tend to decrease exponentially as dimension increases, surpassing the precision of numerical simulations and resulting in substantial errors. Moreover, due to its massive sampling, Monte Carlo fails to offer fast sampling. Modeling the logarithm likelihood (LL) via vanilla PINNs transforms the FP equation into a difficult HJB equation, whose error grows rapidly with dimension. To this end, we propose a novel approach utilizing a score-based solver to fit the score function in SDEs. The score function, defined as the gradient of t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.07462</link><description>&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#30340;&#28608;&#32032;&#36866;&#24212;&#26041;&#27861;&#65306;&#39044;&#38450;&#22238;&#24418;&#38024;&#21551;&#31034;&#24405;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Hormetic Approach to the Value-Loading Problem: Preventing the Paperclip Apocalypse?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07462
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#27861;&#35268;&#27169;&#24335;&#65292;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#20197;&#35299;&#20915;&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#20013;&#30340;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#35013;&#36733;&#38382;&#39064;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#20182;&#20204;&#26088;&#22312;&#21019;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#35813;&#38382;&#39064;&#38656;&#35201;&#19968;&#31181;&#26041;&#27861;&#26469;&#23450;&#20041;&#21644;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#34892;&#20026;&#30340;&#23433;&#20840;&#21644;&#26368;&#20248;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HALO&#65288;&#28608;&#32032;&#36866;&#24212;&#36890;&#36807;&#23545;&#25163;&#36807;&#31243;&#65289;&#36825;&#20010;&#27861;&#35268;&#27169;&#24335;&#65292;&#23427;&#20351;&#29992;&#28608;&#32032;&#20998;&#26512;&#26469;&#35843;&#33410;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#20026;&#27169;&#24335;&#12290;&#34892;&#20026;&#28608;&#32032;&#36866;&#24212;&#26159;&#19968;&#31181;&#29616;&#35937;&#65292;&#20302;&#39057;&#29575;&#30340;&#34892;&#20026;&#20855;&#26377;&#30410;&#22788;&#65292;&#32780;&#39640;&#39057;&#29575;&#30340;&#34892;&#20026;&#21017;&#26377;&#23475;&#12290;&#36890;&#36807;&#23558;&#34892;&#20026;&#24314;&#27169;&#20026;&#21464;&#24577;&#23545;&#25163;&#36807;&#31243;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#34892;&#20026;&#39057;&#29575;&#21709;&#24212;&#20998;&#26512;&#65288;BFRA&#65289;&#25110;&#34892;&#20026;&#35745;&#25968;&#21709;&#24212;&#20998;&#26512;&#65288;BCRA&#65289;&#26469;&#37327;&#21270;&#21487;&#37325;&#22797;&#34892;&#20026;&#30340;&#28608;&#32032;&#38480;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;HALO&#26469;&#35299;&#20915;&#8220;&#22238;&#24418;&#38024;&#26368;&#22823;&#21270;&#22120;&#8221;&#22330;&#26223;&#65292;&#36825;&#26159;&#19968;&#20010;&#24605;&#24819;&#23454;&#39564;&#65292;&#20854;&#20013;&#19968;&#20010;&#26410;&#21463;&#31649;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23558;&#25152;&#26377;&#29289;&#36136;&#36716;&#21270;&#20026;&#22238;&#24418;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
The value-loading problem is a significant challenge for researchers aiming to create artificial intelligence (AI) systems that align with human values and preferences. This problem requires a method to define and regulate safe and optimal limits of AI behaviors. In this work, we propose HALO (Hormetic ALignment via Opponent processes), a regulatory paradigm that uses hormetic analysis to regulate the behavioral patterns of AI. Behavioral hormesis is a phenomenon where low frequencies of a behavior have beneficial effects, while high frequencies are harmful. By modeling behaviors as allostatic opponent processes, we can use either Behavioral Frequency Response Analysis (BFRA) or Behavioral Count Response Analysis (BCRA) to quantify the hormetic limits of repeatable behaviors. We demonstrate how HALO can solve the 'paperclip maximizer' scenario, a thought experiment where an unregulated AI tasked with making paperclips could end up converting all matter in the universe into paperclips. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#21487;&#20197;&#22312;&#25932;&#20154;&#36873;&#25321;&#30340;&#20108;&#36827;&#21046;&#24207;&#21015;&#19978;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#36890;&#36807;&#36739;&#20302;&#30340;&#26631;&#23450;&#36317;&#31163;&#36827;&#34892;&#20934;&#30830;&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.07458</link><description>&lt;p&gt;
&#20851;&#20110;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#26631;&#23450;&#36317;&#31163;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Distance from Calibration in Sequential Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39034;&#24207;&#39044;&#27979;&#20013;&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#21487;&#20197;&#22312;&#25932;&#20154;&#36873;&#25321;&#30340;&#20108;&#36827;&#21046;&#24207;&#21015;&#19978;&#23454;&#29616;$O(\sqrt{T})$&#30340;&#26631;&#23450;&#36317;&#31163;&#65292;&#36890;&#36807;&#36739;&#20302;&#30340;&#26631;&#23450;&#36317;&#31163;&#36827;&#34892;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#39034;&#24207;&#20108;&#36827;&#21046;&#39044;&#27979;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#39044;&#27979;&#22120;&#30340;&#35780;&#20272;&#26159;&#20197;&#26631;&#23450;&#36317;&#31163;&#20026;&#22522;&#20934;&#30340;&#65292;&#26631;&#23450;&#36317;&#31163;&#23450;&#20041;&#20026;&#39044;&#27979;&#20540;&#19982;&#20107;&#21518;&#23436;&#20840;&#26631;&#23450;&#30340;&#39044;&#27979;&#38598;&#20043;&#38388;&#30340;$L_1$&#36317;&#31163;&#12290;&#36825;&#31867;&#20284;&#20110;&#26368;&#36817;&#30001;B{\l}asiok&#12289;Gopalan&#12289;Hu&#21644;Nakkiran&#65288;STOC 2023&#65289;&#25552;&#20986;&#30340;&#31163;&#32447;&#22330;&#26223;&#20013;&#30340;&#26631;&#23450;&#24230;&#37327;&#12290;&#26631;&#23450;&#36317;&#31163;&#26159;&#19968;&#31181;&#33258;&#28982;&#19988;&#30452;&#35266;&#30340;&#20559;&#31163;&#23436;&#32654;&#26631;&#23450;&#30340;&#24230;&#37327;&#65292;&#24182;&#19988;&#28385;&#36275;&#19981;&#21516;&#20110;&#35768;&#22810;&#24120;&#35265;&#30340;&#26631;&#23450;&#24230;&#37327;&#65288;&#22914;$L_1$&#26631;&#23450;&#35823;&#24046;&#21450;&#20854;&#21464;&#31181;&#65289;&#30340;Lipschitz&#36830;&#32493;&#24615;&#23646;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23384;&#22312;&#19968;&#31181;&#39044;&#27979;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23545;&#25932;&#20154;&#36873;&#25321;&#30340;&#38271;&#24230;&#20026;$T$&#30340;&#20108;&#36827;&#21046;&#24207;&#21015;&#19978;&#65292;&#20197;&#26399;&#26395;$O(\sqrt{T})$&#30340;&#26631;&#23450;&#36317;&#31163;&#23454;&#29616;&#12290;&#22312;&#36825;&#20010;&#19978;&#30028;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32467;&#26500;&#24615;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#26631;&#23450;&#36317;&#31163;&#21487;&#20197;&#36890;&#36807;&#36739;&#20302;&#30340;&#26631;&#23450;&#36317;&#31163;&#36827;&#34892;&#20934;&#30830;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a sequential binary prediction setting where the forecaster is evaluated in terms of the calibration distance, which is defined as the $L_1$ distance between the predicted values and the set of predictions that are perfectly calibrated in hindsight. This is analogous to a calibration measure recently proposed by B{\l}asiok, Gopalan, Hu and Nakkiran (STOC 2023) for the offline setting. The calibration distance is a natural and intuitive measure of deviation from perfect calibration, and satisfies a Lipschitz continuity property which does not hold for many popular calibration measures, such as the $L_1$ calibration error and its variants.   We prove that there is a forecasting algorithm that achieves an $O(\sqrt{T})$ calibration distance in expectation on an adversarially chosen sequence of $T$ binary outcomes. At the core of this upper bound is a structural result showing that the calibration distance is accurately approximated by the lower calibration distance, which is a con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#22312;&#32447;&#29615;&#22659;&#20013;&#22810;&#31867;&#20998;&#31867;&#20013;&#20381;&#36182;&#20110;&#24378;&#30423;&#21453;&#39304;&#30340;&#20195;&#20215;&#65292;&#33258;&#36866;&#24212;&#23545;&#25163;&#21644;&#38543;&#26426;&#23398;&#20064;&#32773;&#19982;&#26080;&#35270;&#23545;&#25163;&#21644;&#30830;&#23450;&#24615;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#25439;&#22833;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.07453</link><description>&lt;p&gt;
Bandit-Feedback&#22312;&#32447;&#22810;&#31867;&#20998;&#31867;&#65306;&#21464;&#20307;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Bandit-Feedback Online Multiclass Classification: Variants and Tradeoffs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07453
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#25239;&#22312;&#32447;&#29615;&#22659;&#20013;&#22810;&#31867;&#20998;&#31867;&#20013;&#20381;&#36182;&#20110;&#24378;&#30423;&#21453;&#39304;&#30340;&#20195;&#20215;&#65292;&#33258;&#36866;&#24212;&#23545;&#25163;&#21644;&#38543;&#26426;&#23398;&#20064;&#32773;&#19982;&#26080;&#35270;&#23545;&#25163;&#21644;&#30830;&#23450;&#24615;&#23398;&#20064;&#32773;&#20043;&#38388;&#30340;&#25439;&#22833;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#25239;&#22312;&#32447;&#29615;&#22659;&#20013;&#32771;&#34385;&#22810;&#31867;&#20998;&#31867;&#39046;&#22495;&#12290;&#19982;&#25552;&#20379;&#23436;&#20840;&#20449;&#24687;&#30456;&#27604;&#65292;&#20381;&#36182;&#20110;&#24378;&#30423;&#21453;&#39304;&#30340;&#20195;&#20215;&#26159;&#22810;&#23569;&#65311;&#33258;&#36866;&#24212;&#23545;&#25163;&#19982;&#26080;&#35270;&#23545;&#25163;&#30456;&#27604;&#65292;&#21487;&#20197;&#22686;&#21152;&#25439;&#22833;&#30340;&#31243;&#24230;&#26377;&#22810;&#22823;&#65311;&#38543;&#26426;&#23398;&#20064;&#32773;&#19982;&#30830;&#23450;&#24615;&#23398;&#20064;&#32773;&#30456;&#27604;&#65292;&#21487;&#20197;&#38477;&#20302;&#25439;&#22833;&#30340;&#31243;&#24230;&#26377;&#22810;&#22823;&#65311;&#25105;&#20204;&#22312;&#38169;&#35823;&#36793;&#30028;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20046;&#32039;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consider the domain of multiclass classification within the adversarial online setting. What is the price of relying on bandit feedback as opposed to full information? To what extent can an adaptive adversary amplify the loss compared to an oblivious one? To what extent can a randomized learner reduce the loss compared to a deterministic one? We study these questions in the mistake bound model and provide nearly tight answers.   We demonstrate that the optimal mistake bound under bandit feedback is at most $O(k)$ times higher than the optimal mistake bound in the full information case, where $k$ represents the number of labels. This bound is tight and provides an answer to an open question previously posed and studied by Daniely and Helbertal ['13] and by Long ['17, '20], who focused on deterministic learners.   Moreover, we present nearly optimal bounds of $\tilde{\Theta}(k)$ on the gap between randomized and deterministic learners, as well as between adaptive and oblivious adversarie
&lt;/p&gt;</description></item><item><title>TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07452</link><description>&lt;p&gt;
TriAug&#65306;&#29992;&#20110;&#36229;&#22768;&#20083;&#33146;&#30149;&#21464;&#19981;&#24179;&#34913;&#20998;&#31867;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TriAug: Out-of-Distribution Detection for Robust Classification of Imbalanced Breast Lesion in Ultrasound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07452
&lt;/p&gt;
&lt;p&gt;
TriAug&#26159;&#19968;&#20010;&#29992;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#21644;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#30142;&#30149;&#65292;&#22914;&#20083;&#33146;&#30149;&#21464;&#30340;&#32452;&#32455;&#20122;&#22411;&#65292;&#20855;&#26377;&#20005;&#37325;&#19981;&#21516;&#30340;&#21457;&#30149;&#29575;&#12290;&#21363;&#20351;&#36890;&#36807;&#22823;&#37327;&#30340;&#31034;&#36394;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#27169;&#22411;&#22312;&#20020;&#24202;&#23454;&#38469;&#20013;&#36890;&#24120;&#36935;&#21040;&#23646;&#20110;&#26410;&#35265;&#31867;&#21035;&#30340;&#24322;&#24120;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#20083;&#33146;&#36229;&#22768;&#22270;&#20687;&#30340;&#38271;&#23614;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#37197;&#22791;&#20102;&#19968;&#31181;&#19977;&#20803;&#29366;&#24577;&#22686;&#24378;&#65288;TriAug&#65289;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31034;&#36394;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#24322;&#24120;&#26679;&#26412;&#26816;&#27979;&#24615;&#33021;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#29699;&#24418;&#25439;&#22833;&#26469;&#22788;&#29702;&#31867;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Different diseases, such as histological subtypes of breast lesions, have severely varying incidence rates. Even trained with substantial amount of in-distribution (ID) data, models often encounter out-of-distribution (OOD) samples belonging to unseen classes in clinical reality. To address this, we propose a novel framework built upon a long-tailed OOD detection task for breast ultrasound images. It is equipped with a triplet state augmentation (TriAug) which improves ID classification accuracy while maintaining a promising OOD detection performance. Meanwhile, we designed a balanced sphere loss to handle the class imbalanced problem.
&lt;/p&gt;</description></item><item><title>AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07448</link><description>&lt;p&gt;
AraSpider&#65306;&#23454;&#29616;&#38463;&#25289;&#20271;&#35821;&#21040;SQL&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
AraSpider: Democratizing Arabic-to-SQL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07448
&lt;/p&gt;
&lt;p&gt;
AraSpider&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#22238;&#35793;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#22312;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;AraSpider&#65292;&#36825;&#26159;&#39318;&#20010;&#38463;&#25289;&#20271;&#35821;&#29256;&#26412;&#30340;Spider&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#35813;&#30740;&#31350;&#27979;&#35797;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#22312;&#23558;&#33521;&#25991;&#32763;&#35793;&#25104;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#22806;&#65292;&#36824;&#35780;&#20272;&#20102;&#20004;&#20010;&#27169;&#22411;&#22312;&#20174;&#38463;&#25289;&#20271;&#25991;&#26412;&#29983;&#25104;SQL&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22238;&#35793;&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT 3.5&#21644;SQLCoder&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;Spider&#25968;&#25454;&#38598;&#19978;&#34987;&#35748;&#20026;&#26159;&#26368;&#20339;&#34920;&#29616;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;ChatGPT 3.5&#23637;&#31034;&#20102;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#65292;&#32780;SQLCoder&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23558;&#19978;&#19979;&#25991;&#27169;&#24335;&#21644;&#37319;&#29992;&#22238;&#35793;&#31574;&#30053;&#32435;&#20837;&#38463;&#25289;&#20271;&#35821;NLP&#20219;&#21153;&#20013;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#26041;&#27861;&#35770;&#20197;&#23454;&#29616;&#32467;&#26524;&#22797;&#29616;&#24182;&#23558;&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#31361;&#26174;&#20102;&#30740;&#31350;&#20419;&#36827;&#30340;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents AraSpider, the first Arabic version of the Spider dataset, aimed at improving natural language processing (NLP) in the Arabic-speaking community. Four multilingual translation models were tested for their effectiveness in translating English to Arabic. Additionally, two models were assessed for their ability to generate SQL queries from Arabic text. The results showed that using back translation significantly improved the performance of both ChatGPT 3.5 and SQLCoder models, which are considered top performers on the Spider dataset. Notably, ChatGPT 3.5 demonstrated high-quality translation, while SQLCoder excelled in text-to-SQL tasks. The study underscores the importance of incorporating contextual schema and employing back translation strategies to enhance model performance in Arabic NLP tasks. Moreover, the provision of detailed methodologies for reproducibility and translation of the dataset into other languages highlights the research's commitment to promoting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#25509;&#36817;&#26368;&#20248;&#12290;&#31639;&#27861;&#21019;&#26032;&#21253;&#25324;&#20102;&#23545;&#21152;&#26435;MLE&#30340;&#31934;&#30830;&#19988;&#32039;&#23494;&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.07445</link><description>&lt;p&gt;
&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Top-$K$ ranking with a monotone adversary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#25509;&#36817;&#26368;&#20248;&#12290;&#31639;&#27861;&#21019;&#26032;&#21253;&#25324;&#20102;&#23545;&#21152;&#26435;MLE&#30340;&#31934;&#30830;&#19988;&#32039;&#23494;&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20855;&#26377;&#21333;&#35843;&#23545;&#25163;&#30340;Top-K&#25490;&#21517;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#27604;&#36739;&#22270;&#34987;&#38543;&#26426;&#29983;&#25104;&#19988;&#23545;&#25163;&#21487;&#20197;&#28155;&#21152;&#20219;&#24847;&#36793;&#30340;&#24773;&#20917;&#12290;&#32479;&#35745;&#23398;&#23478;&#30340;&#30446;&#26631;&#26159;&#26681;&#25454;&#20174;&#36825;&#20010;&#21322;&#38543;&#26426;&#27604;&#36739;&#22270;&#23548;&#20986;&#30340;&#20004;&#20004;&#27604;&#36739;&#20934;&#30830;&#22320;&#35782;&#21035;&#20986;Top-K&#30340;&#39318;&#36873;&#39033;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20986;&#19968;&#31181;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;(MLE)&#65292;&#23427;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#36798;&#21040;&#20102;&#36817;&#20284;&#26368;&#20248;&#65292;&#26368;&#22810;&#24046;&#19968;&#20010;$log^2(n)$&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#34920;&#31034;&#27604;&#36739;&#39033;&#30340;&#25968;&#37327;&#12290;&#36825;&#24471;&#30410;&#20110;&#20998;&#26512;&#21644;&#31639;&#27861;&#21019;&#26032;&#30340;&#32467;&#21512;&#12290;&#22312;&#20998;&#26512;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26126;&#30830;&#12289;&#26356;&#32039;&#23494;&#30340;&#21152;&#26435;MLE&#30340;$\ell_\infty$&#35823;&#24046;&#20998;&#26512;&#65292;&#23427;&#19982;&#21152;&#26435;&#27604;&#36739;&#22270;&#30340;&#35889;&#29305;&#24615;&#30456;&#20851;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21019;&#26032;&#28041;&#21450;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the top-$K$ ranking problem with a monotone adversary. We consider the scenario where a comparison graph is randomly generated and the adversary is allowed to add arbitrary edges. The statistician's goal is then to accurately identify the top-$K$ preferred items based on pairwise comparisons derived from this semi-random comparison graph. The main contribution of this paper is to develop a weighted maximum likelihood estimator (MLE) that achieves near-optimal sample complexity, up to a $\log^2(n)$ factor, where n denotes the number of items under comparison. This is made possible through a combination of analytical and algorithmic innovations. On the analytical front, we provide a refined $\ell_\infty$ error analysis of the weighted MLE that is more explicit and tighter than existing analyses. It relates the $\ell_\infty$ error with the spectral properties of the weighted comparison graph. Motivated by this, our algorithmic innovation involves the development 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;FlashAttention&#31639;&#27861;&#30340;I/O&#22797;&#26434;&#24615;&#26159;&#21542;&#26159;&#26368;&#20248;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07443</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;I/O&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#38378;&#23384;&#27880;&#24847;&#21147;&#26377;&#22810;&#20040;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
The I/O Complexity of Attention, or How Optimal is Flash Attention?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07443
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;FlashAttention&#31639;&#27861;&#30340;I/O&#22797;&#26434;&#24615;&#26159;&#21542;&#26159;&#26368;&#20248;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#27969;&#34892;&#30340;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#65292;&#20294;&#26159;&#21463;&#21040;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#38480;&#21046;&#12290;&#31361;&#30772;&#24615;&#30340;FlashAttention&#31639;&#27861;&#25581;&#31034;&#20102;I/O&#22797;&#26434;&#24615;&#26159;&#25193;&#23637;Transformer&#30340;&#30495;&#27491;&#29942;&#39048;&#12290;&#32473;&#23450;&#20004;&#20010;&#23618;&#32423;&#30340;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#65292;&#19968;&#20010;&#24555;&#36895;&#32531;&#23384;&#65288;&#20363;&#22914;GPU&#29255;&#19978;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;&#65289;&#21644;&#19968;&#20010;&#24930;&#36895;&#20869;&#23384;&#65288;&#20363;&#22914;GPU&#39640;&#24102;&#23485;&#20869;&#23384;&#65289;&#65292;I/O&#22797;&#26434;&#24615;&#34913;&#37327;&#20102;&#23545;&#20869;&#23384;&#30340;&#35775;&#38382;&#27425;&#25968;&#12290;FlashAttention&#20351;&#29992;$\frac{N^2d^2}{M}$&#30340;I/O&#25805;&#20316;&#26469;&#35745;&#31639;&#27880;&#24847;&#21147;&#65292;&#20854;&#20013;$N$&#26159;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;$d$&#26159;&#22836;&#37096;&#32500;&#24230;&#65292;$M$&#26159;&#32531;&#23384;&#22823;&#23567;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;I/O&#22797;&#26434;&#24615;&#26159;&#21542;&#26159;&#26368;&#20248;&#30340;&#65311;&#24050;&#30693;&#30340;&#19979;&#30028;&#21482;&#25490;&#38500;&#20102;$M=\Theta(Nd)$&#26102;&#30340;$o(Nd)$&#30340;I/O&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#20889;&#20837;&#24930;&#36895;&#20869;&#23384;&#30340;&#36755;&#20986;&#26159;$\Omega(Nd)$&#12290;&#36825;&#24341;&#20986;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#20027;&#35201;&#38382;&#39064;&#65306;&#23545;&#20110;&#25152;&#26377;&#30340;$M$&#20540;&#65292;FlashAttention&#26159;&#21542;&#26159;I/O&#26368;&#20248;&#30340;&#65311;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#19968;&#20010;I/O&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-attention is at the heart of the popular Transformer architecture, yet suffers from quadratic time and memory complexity. The breakthrough FlashAttention algorithm revealed I/O complexity as the true bottleneck in scaling Transformers. Given two levels of memory hierarchy, a fast cache (e.g. GPU on-chip SRAM) and a slow memory (e.g. GPU high-bandwidth memory), the I/O complexity measures the number of accesses to memory. FlashAttention computes attention using $\frac{N^2d^2}{M}$ I/O operations where $N$ is the dimension of the attention matrix, $d$ the head-dimension and $M$ the cache size. However, is this I/O complexity optimal? The known lower bound only rules out an I/O complexity of $o(Nd)$ when $M=\Theta(Nd)$, since the output that needs to be written to slow memory is $\Omega(Nd)$. This leads to the main question of our work: Is FlashAttention I/O optimal for all values of $M$?   We resolve the above question in its full generality by showing an I/O complexity lower bound t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07440</link><description>&lt;p&gt;
&#20351;&#29992;LoCo&#21644;M2-BERT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#26500;&#24314;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Benchmarking and Building Long-Context Retrieval Models with LoCo and M2-BERT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;LoCoV1&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#65292;&#35299;&#20915;&#20102;&#22914;&#20309;&#35780;&#20272;&#24615;&#33021;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#21450;&#22914;&#20309;&#36827;&#34892;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#31649;&#36947;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22312;&#25991;&#26723;&#24456;&#38271;&#65288;&#20363;&#22914;10K&#20010;&#26631;&#35760;&#25110;&#26356;&#22810;&#65289;&#19988;&#38656;&#35201;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#21512;&#25104;&#20449;&#24687;&#26469;&#30830;&#23450;&#30456;&#20851;&#25991;&#26723;&#30340;&#39046;&#22495;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#24320;&#21457;&#36866;&#29992;&#20110;&#36825;&#20123;&#39046;&#22495;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#32534;&#30721;&#22120;&#38754;&#20020;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22914;&#20309;&#35780;&#20272;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#24615;&#33021;&#65292;&#65288;2&#65289;&#22914;&#20309;&#39044;&#35757;&#32451;&#22522;&#26412;&#35821;&#35328;&#27169;&#22411;&#20197;&#34920;&#31034;&#30701;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#26597;&#35810;&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#65288;&#23545;&#24212;&#25991;&#26723;&#65289;&#65292;&#20197;&#21450;&#65288;3&#65289;&#22914;&#20309;&#26681;&#25454;GPU&#20869;&#23384;&#38480;&#21046;&#19979;&#30340;&#25209;&#37327;&#22823;&#23567;&#38480;&#21046;&#23545;&#35813;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;LoCoV1&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;12&#20010;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27979;&#37327;&#22312;&#19981;&#21487;&#20998;&#22359;&#25110;&#19981;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2-BERT&#26816;&#32034;&#32534;&#30721;&#22120;&#65292;&#36825;&#26159;&#19968;&#20010;80M&#21442;&#25968;&#29366;&#24577;&#31354;&#38388;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#37319;&#29992;Monarch Mixer&#26550;&#26500;&#26500;&#24314;&#65292;&#33021;&#22815;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval pipelines-an integral component of many machine learning systems-perform poorly in domains where documents are long (e.g., 10K tokens or more) and where identifying the relevant document requires synthesizing information across the entire text. Developing long-context retrieval encoders suitable for these domains raises three challenges: (1) how to evaluate long-context retrieval performance, (2) how to pretrain a base language model to represent both short contexts (corresponding to queries) and long contexts (corresponding to documents), and (3) how to fine-tune this model for retrieval under the batch size limitations imposed by GPU memory constraints. To address these challenges, we first introduce LoCoV1, a novel 12 task benchmark constructed to measure long-context retrieval where chunking is not possible or not effective. We next present the M2-BERT retrieval encoder, an 80M parameter state-space encoder model built from the Monarch Mixer architecture, capable of scali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23398;&#20064;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#31246;&#25910;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#12289;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#31561;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#12289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#21644;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#30340;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.07437</link><description>&lt;p&gt;
&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Tax Design in Nonatomic Congestion Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#23398;&#20064;&#22914;&#20309;&#35774;&#35745;&#26368;&#20248;&#31246;&#25910;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#25552;&#39640;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#12289;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#31561;&#25361;&#25112;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#12289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#21644;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#30340;&#26032;&#39062;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#35774;&#35745;&#65292;&#20197;&#22312;&#38750;&#21407;&#23376;&#25317;&#22581;&#21338;&#24328;&#20013;&#26368;&#22823;&#21270;&#25928;&#29575;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#29609;&#23478;&#20043;&#38388;&#30340;&#33258;&#21033;&#34892;&#20026;&#21487;&#33021;&#20250;&#30772;&#22351;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;&#31246;&#21153;&#26426;&#21046;&#26159;&#32531;&#35299;&#27492;&#38382;&#39064;&#24182;&#24341;&#23548;&#31038;&#20250;&#26368;&#20248;&#34892;&#20026;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#21462;&#20102;&#23398;&#20064;&#26368;&#20248;&#31246;&#25910;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#35813;&#26368;&#20248;&#31246;&#25910;&#21487;&#20197;&#36890;&#36807;&#24179;&#34913;&#21453;&#39304;&#26469;&#26368;&#23567;&#21270;&#31038;&#20250;&#25104;&#26412;&#65292;&#21363;&#31246;&#21153;&#35774;&#35745;&#32773;&#21482;&#33021;&#35266;&#23519;&#21040;&#24378;&#21046;&#31246;&#25910;&#19979;&#30340;&#22343;&#34913;&#29366;&#24577;&#12290;&#30001;&#20110;&#25351;&#25968;&#32423;&#30340;&#31246;&#25910;&#20989;&#25968;&#31354;&#38388;&#65292;&#26799;&#24230;&#19981;&#23384;&#22312;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#38750;&#20984;&#24615;&#65292;&#29616;&#26377;&#31639;&#27861;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#20102;&#20960;&#20010;&#26032;&#39062;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#20998;&#27573;&#32447;&#24615;&#31246;&#25910;&#26469;&#36817;&#20284;&#26368;&#20248;&#31246;&#25910;&#65307;&#65288;2&#65289;&#39069;&#22806;&#30340;&#32447;&#24615;&#39033;&#26469;&#20445;&#35777;&#24378;&#20984;&#28508;&#21147;&#20989;&#25968;&#65307;&#65288;3&#65289;&#26377;&#25928;&#30340;&#23376;&#20363;&#31243;&#26469;&#25214;&#21040;&#8220;&#36793;&#30028;&#8221;&#31246;&#25910;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#26368;&#20248;&#31246;&#25910;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(\bet
&lt;/p&gt;
&lt;p&gt;
We study how to learn the optimal tax design to maximize the efficiency in nonatomic congestion games. It is known that self-interested behavior among the players can damage the system's efficiency. Tax mechanisms is a common method to alleviate this issue and induce socially optimal behavior. In this work, we take the initial step for learning the optimal tax that can minimize the social cost with \emph{equilibrium feedback}, i.e., the tax designer can only observe the equilibrium state under the enforced tax. Existing algorithms are not applicable due to the exponentially large tax function space, nonexistence of the gradient, and nonconvexity of the objective. To tackle these challenges, our algorithm leverages several novel components: (1) piece-wise linear tax to approximate the optimal tax; (2) an extra linear term to guarantee a strongly convex potential function; (3) efficient subroutine to find the ``boundary'' tax. The algorithm can find an $\epsilon$-optimal tax with $O(\bet
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GARCH&#12289;EWMA&#21644;IV&#27169;&#22411;&#22312;&#39044;&#27979;GBP/USD&#21644;EUR/GBP&#36135;&#24065;&#23545;&#27599;&#26085;&#22238;&#25253;&#30340;20&#22825;&#21464;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;EUR/GBP&#36135;&#24065;&#23545;&#23384;&#22312;&#38750;&#23545;&#31216;&#22238;&#25253;&#30340;&#35777;&#25454;&#65292;&#32780;GBP/USD&#36135;&#24065;&#23545;&#30340;&#35777;&#25454;&#24182;&#19981;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2402.07435</link><description>&lt;p&gt;
&#20998;&#26512;&#36135;&#24065;&#27874;&#21160;&#65306;GBP/USD&#21644;EUR/GBP&#36135;&#24065;&#23545;&#30340;GARCH&#12289;EWMA&#21644;IV&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Analyzing Currency Fluctuations: A Comparative Study of GARCH, EWMA, and IV Models for GBP/USD and EUR/GBP Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GARCH&#12289;EWMA&#21644;IV&#27169;&#22411;&#22312;&#39044;&#27979;GBP/USD&#21644;EUR/GBP&#36135;&#24065;&#23545;&#27599;&#26085;&#22238;&#25253;&#30340;20&#22825;&#21464;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;EUR/GBP&#36135;&#24065;&#23545;&#23384;&#22312;&#38750;&#23545;&#31216;&#22238;&#25253;&#30340;&#35777;&#25454;&#65292;&#32780;GBP/USD&#36135;&#24065;&#23545;&#30340;&#35777;&#25454;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33521;&#38225;&#65288;GBP&#65289;&#20215;&#20540;&#30340;&#27874;&#21160;&#24773;&#20917;&#65292;&#29305;&#21035;&#20851;&#27880;&#20854;&#19982;&#32654;&#20803;&#65288;USD&#65289;&#21644;&#27431;&#20803;&#65288;EUR&#65289;&#36135;&#24065;&#23545;&#30340;&#20851;&#31995;&#12290;&#21033;&#29992;2018&#24180;6&#26376;15&#26085;&#33267;2023&#24180;6&#26376;15&#26085;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#24212;&#29992;&#19981;&#21516;&#30340;&#25968;&#23398;&#27169;&#22411;&#26469;&#35780;&#20272;&#23427;&#20204;&#22312;&#39044;&#27979;&#36135;&#24065;&#23545;&#27599;&#26085;&#22238;&#25253;&#30340;20&#22825;&#21464;&#21160;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28041;&#21450;&#25351;&#25968;&#21152;&#26435;&#31227;&#21160;&#24179;&#22343;&#65288;EWMA&#65289;&#12289;&#24191;&#20041;&#33258;&#22238;&#24402;&#26465;&#20214;&#24322;&#26041;&#24046;&#65288;GARCH&#65289;&#27169;&#22411;&#21644;&#38544;&#21547;&#27874;&#21160;&#29575;&#65288;IV&#65289;&#27169;&#22411;&#30340;&#23454;&#26045;&#12290;&#20026;&#20102;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#25351;&#26631;&#27604;&#36739;&#23427;&#20204;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;GARCH&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#65292;&#26816;&#26597;&#20102;&#23558;&#20854;&#24212;&#29992;&#20110;&#25152;&#25552;&#20379;&#25968;&#25454;&#38598;&#26102;&#30340;&#32479;&#35745;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;EUR/GBP&#36135;&#24065;&#23545;&#23384;&#22312;&#38750;&#23545;&#31216;&#22238;&#25253;&#30340;&#35777;&#25454;&#65292;&#32780;GBP/USD&#36135;&#24065;&#23545;&#30340;&#35777;&#25454;&#24182;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we examine the fluctuation in the value of the Great Britain Pound (GBP). We focus particularly on its relationship with the United States Dollar (USD) and the Euro (EUR) currency pairs. Utilizing data from June 15, 2018, to June 15, 2023, we apply various mathematical models to assess their effectiveness in predicting the 20-day variation in the pairs' daily returns. Our analysis involves the implementation of Exponentially Weighted Moving Average (EWMA), Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, and Implied Volatility (IV) models. To evaluate their performance, we compare the accuracy of their predictions using Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) metrics. We delve into the intricacies of GARCH models, examining their statistical characteristics when applied to the provided dataset. Our findings suggest the existence of asymmetric returns in the EUR/GBP pair, while such evidence is inconclusive for the GBP/USD pair
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22270;&#20687;&#30340;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;</title><link>https://arxiv.org/abs/2402.07419</link><description>&lt;p&gt;
&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36275;&#20197;&#20174;&#20219;&#20309;&#22240;&#26524;&#25928;&#24212;&#27979;&#24230;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Models are Sufficient to Sample from Any Causal Effect Estimand
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#29992;&#20110;&#20174;&#22270;&#20687;&#30340;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#23384;&#22312;&#35745;&#31639;&#22240;&#26524;&#25928;&#24212;&#30340;&#21487;&#38752;&#19988;&#23436;&#22791;&#30340;&#31639;&#27861;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#31639;&#27861;&#38656;&#35201;&#26174;&#24335;&#35775;&#38382;&#35266;&#27979;&#20998;&#24067;&#19978;&#30340;&#26465;&#20214;&#20284;&#28982;&#65292;&#32780;&#22312;&#39640;&#32500;&#22330;&#26223;&#20013;&#65288;&#20363;&#22914;&#22270;&#20687;&#65289;&#65292;&#20272;&#35745;&#36825;&#20123;&#20284;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#27169;&#25311;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#20013;&#27809;&#26377;&#19968;&#20010;&#21487;&#20197;&#24212;&#29992;&#20110;&#36890;&#29992;&#22330;&#26223;&#65292;&#20363;&#22914;&#20855;&#26377;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#30340;&#22270;&#20687;&#25968;&#25454;&#30340;&#22240;&#26524;&#22270;&#65292;&#25110;&#32773;&#33719;&#24471;&#26465;&#20214;&#24178;&#39044;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20219;&#24847;&#22240;&#26524;&#22270;&#19979;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#36827;&#35745;&#31639;&#21487;&#20197;&#35745;&#31639;&#20219;&#20309;&#21487;&#36776;&#35782;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22522;&#20110;&#27492;&#32467;&#26524;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;&#65288;&#26465;&#20214;&#65289;&#24178;&#39044;&#20998;&#24067;&#20013;&#37319;&#26679;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference from observational data has recently found many applications in machine learning. While sound and complete algorithms exist to compute causal effects, many of these algorithms require explicit access to conditional likelihoods over the observational distribution, which is difficult to estimate in the high-dimensional regime, such as with images. To alleviate this issue, researchers have approached the problem by simulating causal relations with neural models and obtained impressive results. However, none of these existing approaches can be applied to generic scenarios such as causal graphs on image data with latent confounders, or obtain conditional interventional samples. In this paper, we show that any identifiable causal effect given an arbitrary causal graph can be computed through push-forward computations of conditional generative models. Based on this result, we devise a diffusion-based approach to sample from any (conditional) interventional distribution on ima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#30340;&#26657;&#20934;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#19988;VLMs&#21482;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#36827;&#34892;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07417</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#26657;&#20934;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study Into What Matters for Calibrating Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#30340;&#26657;&#20934;&#24615;&#36136;&#65292;&#24182;&#21457;&#29616;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#19988;VLMs&#21482;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#36827;&#34892;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24050;&#25104;&#20026;&#38646;&#26679;&#26412;&#35782;&#21035;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#22330;&#26223;&#21644;&#37325;&#35201;&#20998;&#24067;&#21464;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#39046;&#22495;&#38656;&#35201;&#23545;&#20854;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#33021;&#21147;&#26377;&#26356;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#30693;&#30340;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#31574;&#30053;&#19979;VLMs&#30340;&#26657;&#20934;&#24615;&#36136;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#22312;&#19968;&#20010;&#39046;&#22495;&#12289;&#26631;&#31614;&#38598;&#25110;&#23618;&#27425;&#32423;&#21035;&#20013;&#36827;&#34892;&#26657;&#20934;&#26102;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;VLMs&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;VLMs&#26412;&#36523;&#24182;&#19981;&#20855;&#22791;&#26657;&#20934;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#65292;&#20294;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#26174;&#33879;&#19988;&#19968;&#33268;&#22320;&#25552;&#21319;&#26657;&#20934;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20998;&#24067;&#36716;&#21464;&#21644;&#26631;&#31614;&#38598;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#27492;&#22806;&#65292;VLMs&#21487;&#20197;&#36890;&#36807;&#24456;&#23569;&#30340;&#31034;&#20363;&#36827;&#34892;&#26657;&#20934;&#12290;&#36890;&#36807;&#35814;&#32454;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#28508;&#22312;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision--Language Models (VLMs) have emerged as the dominant approach for zero-shot recognition, adept at handling diverse scenarios and significant distribution changes. However, their deployment in risk-sensitive areas requires a deeper understanding of their uncertainty estimation capabilities, a relatively uncharted area. In this study, we explore the calibration properties of VLMs across different architectures, datasets, and training strategies. In particular, we analyze the uncertainty estimation performance of VLMs when calibrated in one domain, label set or hierarchy level, and tested in a different one. Our findings reveal that while VLMs are not inherently calibrated for uncertainty, temperature scaling significantly and consistently improves calibration, even across shifts in distribution and changes in label set. Moreover, VLMs can be calibrated with a very small set of examples. Through detailed experimentation, we highlight the potential applications and importance of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#24322;&#26500;&#35745;&#31639;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#27169;&#22411;&#30446;&#26631;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#22312;&#36755;&#20837;&#25968;&#25454;&#27969;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#22810;&#27169;&#22411;&#30340;&#30446;&#26631;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#32771;&#34385;&#22810;&#21152;&#36895;&#22120;&#25191;&#34892;&#20197;&#20248;&#21270;&#33021;&#37327;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07415</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;&#35745;&#31639;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#27169;&#22411;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Context-aware Multi-Model Object Detection for Diversely Heterogeneous Compute Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38754;&#21521;&#24322;&#26500;&#35745;&#31639;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#27169;&#22411;&#30446;&#26631;&#26816;&#27979;&#65292;&#36890;&#36807;&#21033;&#29992;&#23884;&#20837;&#22312;&#36755;&#20837;&#25968;&#25454;&#27969;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#22810;&#27169;&#22411;&#30340;&#30446;&#26631;&#26816;&#27979;&#36807;&#31243;&#65292;&#24182;&#32771;&#34385;&#22810;&#21152;&#36895;&#22120;&#25191;&#34892;&#20197;&#20248;&#21270;&#33021;&#37327;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#36830;&#32493;&#31227;&#21160;&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37096;&#32626;&#20013;&#30340;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#26159;&#37319;&#29992;&#30340;&#21333;&#19968;DNN&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36825;&#23548;&#33268;&#20102;&#35745;&#31639;&#36164;&#28304;&#30340;&#20302;&#25928;&#21033;&#29992;&#12290;&#36825;&#31181;&#20302;&#25928;&#29305;&#21035;&#22312;&#33021;&#37327;&#21463;&#38480;&#30340;&#31995;&#32479;&#20013;&#20250;&#36896;&#25104;&#25972;&#20307;&#31995;&#32479;&#25928;&#29575;&#30340;&#38477;&#20302;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23884;&#20837;&#22312;&#36755;&#20837;&#25968;&#25454;&#27969;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65288;&#20363;&#22914;&#25668;&#20687;&#22836;&#25968;&#25454;&#27969;&#20013;&#30340;&#24103;&#65289;&#21487;&#20197;&#34987;&#21033;&#29992;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#22522;&#20110;&#22810;&#27169;&#22411;&#30340;OD&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SHIFT&#65292;&#23427;&#26681;&#25454;&#21160;&#24577;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#35745;&#31639;&#32422;&#26463;&#19981;&#26029;&#20174;&#21508;&#31181;&#22522;&#20110;DNN&#30340;OD&#27169;&#22411;&#20013;&#36873;&#25321;&#12290;&#22312;&#36873;&#25321;&#36807;&#31243;&#20013;&#65292;SHIFT&#29420;&#29305;&#22320;&#32771;&#34385;&#22810;&#21152;&#36895;&#22120;&#25191;&#34892;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#33021;&#37327;&#25928;&#29575;&#21516;&#26102;&#28385;&#36275;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, deep neural networks (DNNs) have gained widespread adoption for continuous mobile object detection (OD) tasks, particularly in autonomous systems. However, a prevalent issue in their deployment is the one-size-fits-all approach, where a single DNN is used, resulting in inefficient utilization of computational resources. This inefficiency is particularly detrimental in energy-constrained systems, as it degrades overall system efficiency. We identify that, the contextual information embedded in the input data stream (e.g. the frames in the camera feed that the OD models are run on) could be exploited to allow a more efficient multi-model-based OD process. In this paper, we propose SHIFT which continuously selects from a variety of DNN-based OD models depending on the dynamically changing contextual information and computational constraints. During this selection, SHIFT uniquely considers multi-accelerator execution to better optimize the energy-efficiency while satisfyin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#36317;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#35774;&#35745;&#22870;&#21169;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.07412</link><description>&lt;p&gt;
&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#36317;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auxiliary Reward Generation with Transition Distance Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29366;&#24577;&#36716;&#25442;&#36317;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#65292;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#21644;&#20943;&#23569;&#20154;&#24037;&#35774;&#35745;&#22870;&#21169;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22797;&#26434;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#21183;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22870;&#21169;&#20989;&#25968;&#23545;&#23398;&#20064;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20316;&#20026;&#20219;&#21153;&#23436;&#25104;&#31243;&#24230;&#30340;&#34913;&#37327;&#26631;&#20934;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#22870;&#21169;&#24448;&#24448;&#26159;&#30001;&#20154;&#24037;&#35774;&#35745;&#30340;&#65292;&#38656;&#35201;&#36153;&#26102;&#36153;&#21147;&#30340;&#35843;&#25972;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#23454;&#29616;&#33258;&#21160;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#34913;&#37327;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#36716;&#25442;&#36317;&#31163;&#8221;&#12290;&#22312;&#36825;&#20123;&#34920;&#31034;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#30693;&#35782;&#30340;&#36741;&#21161;&#22870;&#21169;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#21333;&#20219;&#21153;&#21644;&#25216;&#33021;&#38142;&#22330;&#26223;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27979;&#37327;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#25442;&#36317;&#31163;&#20197;&#21450;&#36741;&#21161;&#22870;&#21169;&#24341;&#36215;&#30340;&#25913;&#36827;&#26377;&#25928;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown its strength in challenging sequential decision-making problems. The reward function in RL is crucial to the learning performance, as it serves as a measure of the task completion degree. In real-world problems, the rewards are predominantly human-designed, which requires laborious tuning, and is easily affected by human cognitive biases. To achieve automatic auxiliary reward generation, we propose a novel representation learning approach that can measure the ``transition distance'' between states. Building upon these representations, we introduce an auxiliary reward generation technique for both single-task and skill-chaining scenarios without the need for human knowledge. The proposed approach is evaluated in a wide range of manipulation tasks. The experiment results demonstrate the effectiveness of measuring the transition distance between states and the induced improvement by auxiliary rewards, which not only promotes better learning efficiency
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28508;&#21183;&#24341;&#23548;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#20869;&#22312;&#21160;&#26426;&#65292;&#22312;&#22797;&#26434;&#21644;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#33021;&#36991;&#20813;&#25913;&#21464;&#26368;&#20248;&#31574;&#30053;&#38598;&#23548;&#33268;&#27425;&#20248;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07411</link><description>&lt;p&gt;
&#28508;&#21183;&#24341;&#23548;&#30340;&#22870;&#21169;&#22609;&#36896;&#29992;&#20110;&#20869;&#22312;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Potential-Based Reward Shaping For Intrinsic Motivation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#28508;&#21183;&#24341;&#23548;&#30340;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#20869;&#22312;&#21160;&#26426;&#65292;&#22312;&#22797;&#26434;&#21644;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#33021;&#36991;&#20813;&#25913;&#21464;&#26368;&#20248;&#31574;&#30053;&#38598;&#23548;&#33268;&#27425;&#20248;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22797;&#26434;&#19988;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#19979;&#65292;&#20869;&#22312;&#21160;&#26426;&#65288;IM&#65289;&#22870;&#21169;&#22609;&#36896;&#26041;&#27861;&#30340;&#25968;&#37327;&#28608;&#22686;&#12290;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#26080;&#24847;&#20013;&#25913;&#21464;&#29615;&#22659;&#20013;&#30340;&#26368;&#20248;&#31574;&#30053;&#38598;&#65292;&#23548;&#33268;&#27425;&#20248;&#34892;&#20026;&#20135;&#29983;&#12290;&#20197;&#24448;&#20851;&#20110;&#20943;&#36731;&#22870;&#21169;&#22609;&#36896;&#39118;&#38505;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#28508;&#21183;&#24341;&#23548;&#30340;&#22870;&#21169;&#22609;&#36896;&#65288;PBRS&#65289;&#65292;&#24182;&#19981;&#33021;&#36866;&#29992;&#20110;&#35768;&#22810;IM&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#26159;&#22797;&#26434;&#30340;&#21487;&#35757;&#32451;&#20989;&#25968;&#65292;&#22240;&#27492;&#20381;&#36182;&#20110;&#27604;PBRS&#24320;&#21457;&#26102;&#26356;&#24191;&#27867;&#30340;&#21464;&#37327;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;PBRS&#30340;&#25193;&#23637;&#65292;&#35777;&#26126;&#20102;&#22312;&#26356;&#19968;&#33324;&#30340;&#20989;&#25968;&#38598;&#19979;&#20445;&#30041;&#20102;&#26368;&#20248;&#31574;&#30053;&#38598;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#21183;&#24341;&#23548;&#30340;&#20869;&#22312;&#21160;&#26426;&#65288;PBIM&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;IM&#22870;&#21169;&#36716;&#25442;&#20026;&#21487;&#29992;&#30340;&#28508;&#21183;&#24418;&#24335;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#26368;&#20248;&#31574;&#30053;&#38598;&#12290;&#22312;MiniGrid DoorKey&#21644;Cliff Walk&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a proliferation of intrinsic motivation (IM) reward-shaping methods to learn in complex and sparse-reward environments. These methods can often inadvertently change the set of optimal policies in an environment, leading to suboptimal behavior. Previous work on mitigating the risks of reward shaping, particularly through potential-based reward shaping (PBRS), has not been applicable to many IM methods, as they are often complex, trainable functions themselves, and therefore dependent on a wider set of variables than the traditional reward functions that PBRS was developed for. We present an extension to PBRS that we prove preserves the set of optimal policies under a more general set of functions than has been previously proven. We also present {\em Potential-Based Intrinsic Motivation} (PBIM), a method for converting IM rewards into a potential-based form that is useable without altering the set of optimal policies. Testing in the MiniGrid DoorKey and Cliff Walk
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20854;&#22312;&#29305;&#23450;&#35270;&#35273;&#22240;&#32032;&#21464;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#24322;&#24120;&#36755;&#20837;&#26816;&#27979;&#31561;&#23433;&#20840;&#30446;&#26631;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.07410</link><description>&lt;p&gt;
&#12298;&#28145;&#20837;&#35299;&#26512;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#20854;&#22312;&#29305;&#23450;&#35270;&#35273;&#22240;&#32032;&#21464;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#24322;&#24120;&#36755;&#20837;&#26816;&#27979;&#31561;&#23433;&#20840;&#30446;&#26631;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;(CLIP)&#27169;&#22411;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#24067;&#36716;&#31227;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#20854;&#22312;&#29305;&#23450;&#35270;&#35273;&#22240;&#32032;&#21464;&#21270;&#19979;&#30340;&#40065;&#26834;&#24615;&#20173;&#26377;&#24453;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21487;&#38752;&#19988;&#23433;&#20840;&#30340;&#31995;&#32479;&#24517;&#39035;&#32771;&#34385;&#38500;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#22806;&#30340;&#20854;&#20182;&#23433;&#20840;&#30446;&#26631;&#65292;&#22914;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;CLIP&#27169;&#22411;&#22312;&#36825;&#20123;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#29305;&#24449;&#19978;&#30340;&#26377;&#25928;&#24615;&#36739;&#23569;&#34987;&#25506;&#35752;&#12290;&#20986;&#20110;&#19978;&#36848;&#21407;&#22240;&#65292;&#26412;&#30740;&#31350;&#20840;&#38754;&#35843;&#26597;&#20102;CLIP&#27169;&#22411;&#30340;&#23433;&#20840;&#30446;&#26631;&#65292;&#29305;&#21035;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#23545;&#35270;&#35273;&#22240;&#32032;&#21464;&#21270;&#30340;&#24377;&#24615;&#65292;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#20197;&#21450;&#26816;&#27979;&#24322;&#24120;&#36755;&#20837;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;83&#20010;CLIP&#27169;&#22411;&#21644;127&#20010;ImageNet&#20998;&#31867;&#22120;&#12290;&#23427;&#20204;&#22312;&#26550;&#26500;&#12289;&#65288;&#39044;&#65289;&#35757;&#32451;&#20998;&#24067;&#21644;&#35757;&#32451;&#31574;&#30053;&#19978;&#37117;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;10&#20010;&#35270;&#35273;&#22240;&#32032;&#65288;&#20363;&#22914;&#24418;&#29366;&#21644;&#22270;&#26696;&#65289;&#65292;5&#20010;...
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable generalization capabilities across multiple challenging distribution shifts. However, there is still much to be explored in terms of their robustness to the variations of specific visual factors. In real-world applications, reliable and safe systems must consider other safety objectives beyond classification accuracy, such as predictive uncertainty. Yet, the effectiveness of CLIP models on such safety-related features is less-explored. Driven by the above, this work comprehensively investigates the safety objectives of CLIP models, specifically focusing on three key properties: resilience to visual factor variations, calibrated uncertainty estimations, and the ability to detect anomalous inputs. To this end, we study 83 CLIP models and 127 ImageNet classifiers. They are diverse in architecture, (pre)training distribution and training strategies. We consider 10 visual factors (e.g., shape and pattern), 5 
&lt;/p&gt;</description></item><item><title>&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#65288;CPP&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#21463;&#20219;&#24847;&#38543;&#26426;&#21442;&#25968;&#24433;&#21709;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26679;&#26412;&#21644;&#37327;&#23376;&#24341;&#29702;&#23558;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#36793;&#38469;&#27010;&#29575;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07407</link><description>&lt;p&gt;
&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#29992;&#20110;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conformal Predictive Programming for Chance Constrained Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07407
&lt;/p&gt;
&lt;p&gt;
&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#65288;CPP&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#21463;&#20219;&#24847;&#38543;&#26426;&#21442;&#25968;&#24433;&#21709;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26679;&#26412;&#21644;&#37327;&#23376;&#24341;&#29702;&#23558;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#36793;&#38469;&#27010;&#29575;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#39044;&#27979;&#35268;&#21010;&#65288;CP&#65289;&#30340;&#36827;&#23637;&#30340;&#28608;&#21169;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#65288;CPP&#65289;&#65292;&#19968;&#31181;&#35299;&#20915;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#21463;&#20219;&#24847;&#38543;&#26426;&#21442;&#25968;&#24433;&#21709;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;CPP&#21033;&#29992;&#36825;&#20123;&#38543;&#26426;&#21442;&#25968;&#30340;&#26679;&#26412;&#20197;&#21450;&#37327;&#23376;&#24341;&#29702;&#65288;CP&#30340;&#26680;&#24515;&#65289;&#23558;CCO&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#65306;&#65288;1&#65289;&#23558;&#37327;&#23376;&#34920;&#31034;&#20026;&#32447;&#24615;&#35268;&#21010;&#20197;&#21450;&#20854;KKT&#26465;&#20214;&#65288;CPP-KKT&#65289;&#65307;&#65288;2&#65289;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;CPP-MIP&#65289;&#26469;&#21576;&#29616;CPP&#30340;&#20004;&#31181;&#26131;&#20110;&#22788;&#29702;&#30340;&#25913;&#36827;&#12290;CPP&#20855;&#22791;&#23545;CCO&#38382;&#39064;&#36827;&#34892;&#36793;&#38469;&#27010;&#29575;&#21487;&#34892;&#24615;&#20445;&#35777;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#20363;&#22914;&#26679;&#26412;&#36924;&#36817;&#21644;&#22330;&#26223;&#26041;&#27861;&#65289;&#22312;&#27010;&#24565;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#23613;&#31649;&#25105;&#20204;&#25506;&#35752;&#20102;&#19982;&#26679;&#26412;&#36924;&#36817;&#26041;&#27861;&#30340;&#31639;&#27861;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;CPP&#30340;&#20248;&#21183;&#22312;&#20110;&#26131;&#20110;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. CPP utilizes samples from these random parameters along with the quantile lemma -- which is central to CP -- to transform the CCO problem into a deterministic optimization problem. We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its KKT conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP). CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be ext
&lt;/p&gt;</description></item><item><title>TeMPO&#26159;&#19968;&#31181;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#36328;&#23618;&#35774;&#22791;/&#30005;&#36335;/&#26550;&#26500;&#23450;&#21046;&#65292;&#24357;&#21512;&#20102;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#30005;&#23376;&#23545;&#24212;&#22120;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#35774;&#22791;&#32423;&#21035;&#19978;&#65292;&#35813;&#21152;&#36895;&#22120;&#37319;&#29992;&#20102;&#24037;&#21378;&#21487;&#29992;&#30340;&#12289;&#23450;&#21046;&#30340;&#20809;&#23398;&#22120;&#20214;&#65292;&#21253;&#25324;&#23454;&#39564;&#28436;&#31034;&#30340;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;&#12289;&#20809;&#20998;&#37197;&#22120;&#21644;&#30456;&#31227;&#22120;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#36755;&#20837;&#32534;&#30721;&#21644;&#28857;&#20056;&#20013;&#30340;&#21344;&#22320;&#38754;&#31215;&#21644;&#21151;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.07393</link><description>&lt;p&gt;
TeMPO&#65306;&#38754;&#21521;&#36793;&#32536;AI&#30340;&#39640;&#25928;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#26680;&#24515;&#65292;&#20855;&#22791;&#32039;&#20945;&#22411;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;
&lt;/p&gt;
&lt;p&gt;
TeMPO: Efficient Time-Multiplexed Dynamic Photonic Tensor Core for Edge AI with Compact Slow-Light Electro-Optic Modulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07393
&lt;/p&gt;
&lt;p&gt;
TeMPO&#26159;&#19968;&#31181;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#36328;&#23618;&#35774;&#22791;/&#30005;&#36335;/&#26550;&#26500;&#23450;&#21046;&#65292;&#24357;&#21512;&#20102;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#30005;&#23376;&#23545;&#24212;&#22120;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#35774;&#22791;&#32423;&#21035;&#19978;&#65292;&#35813;&#21152;&#36895;&#22120;&#37319;&#29992;&#20102;&#24037;&#21378;&#21487;&#29992;&#30340;&#12289;&#23450;&#21046;&#30340;&#20809;&#23398;&#22120;&#20214;&#65292;&#21253;&#25324;&#23454;&#39564;&#28436;&#31034;&#30340;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;&#12289;&#20809;&#20998;&#37197;&#22120;&#21644;&#30456;&#31227;&#22120;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#36755;&#20837;&#32534;&#30721;&#21644;&#28857;&#20056;&#20013;&#30340;&#21344;&#22320;&#38754;&#31215;&#21644;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20809;&#23398;&#30340;&#35745;&#31639;&#36895;&#24230;&#21644;&#25928;&#29575;&#20248;&#36234;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#24179;&#21488;&#19978;&#65292;&#22522;&#20110;&#30005;&#20809;&#35745;&#31639;&#31995;&#32479;&#20026;&#33021;&#25928;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21152;&#36895;&#20219;&#21153;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#23454;&#26102;&#12289;&#20302;&#33021;&#37327;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#25512;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22522;&#20110;&#24037;&#21378;&#21487;&#29992;&#35774;&#22791;&#21644;&#20256;&#32479;&#31995;&#32479;&#26550;&#26500;&#30340;&#20809;&#23398;&#31070;&#32463;&#21152;&#36895;&#22120;&#19982;&#39640;&#24230;&#23450;&#21046;&#30340;&#30005;&#23376;&#23545;&#24212;&#22120;&#30456;&#27604;&#65292;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#24357;&#21512;&#30001;&#20110;&#39046;&#22495;&#19987;&#19994;&#21270;&#19981;&#36275;&#32780;&#23548;&#33268;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TeMPO&#30340;&#26102;&#20998;&#22810;&#36335;&#21160;&#24577;&#20809;&#23398;&#24352;&#37327;&#21152;&#36895;&#22120;&#65292;&#36890;&#36807;&#36328;&#23618;&#35774;&#22791;/&#30005;&#36335;/&#26550;&#26500;&#23450;&#21046;&#23454;&#29616;&#12290;&#22312;&#35774;&#22791;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#24037;&#21378;&#21487;&#29992;&#30340;&#12289;&#23450;&#21046;&#30340;&#20809;&#23398;&#22120;&#20214;&#65292;&#21253;&#25324;&#23454;&#39564;&#28436;&#31034;&#30340;&#24930;&#20809;&#30005;&#20809;&#35843;&#21046;&#22120;&#12289;&#20809;&#20998;&#37197;&#22120;&#21644;&#30456;&#31227;&#22120;&#65292;&#26174;&#33879;&#20943;&#23567;&#20102;&#36755;&#20837;&#32534;&#30721;&#21644;&#28857;&#20056;&#20013;&#30340;&#21344;&#22320;&#38754;&#31215;&#21644;&#21151;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic-photonic computing systems offer immense potential in energy-efficient artificial intelligence (AI) acceleration tasks due to the superior computing speed and efficiency of optics, especially for real-time, low-energy deep neural network (DNN) inference tasks on resource-restricted edge platforms. However, current optical neural accelerators based on foundry-available devices and conventional system architecture still encounter a performance gap compared to highly customized electronic counterparts. To bridge the performance gap due to lack of domain specialization, we present a time-multiplexed dynamic photonic tensor accelerator, dubbed TeMPO, with cross-layer device/circuit/architecture customization. At the device level, we present foundry-compatible, customized photonic devices, including a slow-light electro-optic modulator with experimental demonstration, optical splitters, and phase shifters that significantly reduce the footprint and power in input encoding and dot-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25506;&#32034;-&#20877;&#30830;&#23450;&#31639;&#27861;&#21644;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#20197;&#21450;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#26102;&#38388;&#30028;&#36275;&#22815;&#22823;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07391</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#21487;&#22797;&#21046;&#24615;&#28176;&#36827;&#33258;&#30001;
&lt;/p&gt;
&lt;p&gt;
Replicability is Asymptotically Free in Multi-armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25506;&#32034;-&#20877;&#30830;&#23450;&#31639;&#27861;&#21644;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#20197;&#21450;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#26102;&#38388;&#30028;&#36275;&#22815;&#22823;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#38656;&#27714;&#30340;&#25512;&#21160;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30830;&#20445;&#31639;&#27861;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#21463;&#25968;&#25454;&#38598;&#22266;&#26377;&#38543;&#26426;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#31639;&#27861;&#25152;&#38656;&#30340;&#36951;&#25022;&#20540;&#27604;&#19981;&#21487;&#22797;&#21046;&#31639;&#27861;&#22810;$O(1/\rho^2)$&#20493;&#65292;&#20854;&#20013;$\rho$&#26159;&#38750;&#22797;&#21046;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32473;&#23450;&#30340;$\rho$&#19979;&#26102;&#38388;&#30028;$T$&#36275;&#22815;&#22823;&#26102;&#65292;&#27492;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#21069;&#25552;&#26159;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#25506;&#32034;&#21518;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#22312;&#20915;&#31574;&#20043;&#21069;&#22343;&#21248;&#36873;&#25321;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#38454;&#27573;&#32467;&#26463;&#26102;&#28120;&#27760;&#27425;&#20248;&#21160;&#20316;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#20915;&#31574;&#21046;&#23450;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times more regret than nonreplicable algorithms, where $\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-ma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07388</link><description>&lt;p&gt;
&#26080;&#20551;&#35774;&#27979;&#35797;&#31639;&#27861;&#24615;&#33021;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Limits of Assumption-free Tests for Algorithm Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07388
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#35780;&#20215;&#21644;&#27604;&#36739;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#19968;&#20010;&#31639;&#27861;&#22312;&#32473;&#23450;&#30340;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#65292;&#21738;&#20010;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65311;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#65292;&#36890;&#24120;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;&#23558;&#24863;&#20852;&#36259;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#30041;&#20986;&#25968;&#25454;&#28857;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#31243;&#24207;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#19979;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#38480;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#20010;&#38382;&#39064;: &#31639;&#27861;$A$&#22312;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#38598;&#19978;&#23398;&#20064;&#38382;&#39064;&#26377;&#22810;&#22909;&#65292;&#20197;&#21450;&#22312;&#29305;&#23450;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;$A$&#25152;&#20135;&#29983;&#30340;&#29305;&#23450;&#25311;&#21512;&#27169;&#22411;&#26377;&#22810;&#22909;&#65311;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#23558;&#31639;&#27861;&#35270;&#20026;&#40657;&#30418;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#26080;&#27861;&#20934;&#30830;&#22320;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#23567;&#22411;&#35270;&#35273;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;MLLMs&#30340;&#22238;&#31572;&#33021;&#21147;&#26222;&#36941;&#23384;&#22312;&#38480;&#21046;&#12290;&#36890;&#36807;&#25511;&#21046;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29289;&#20307;&#36136;&#37327;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#37117;&#23545;MLLMs&#30340;&#24863;&#30693;&#33021;&#21147;&#26377;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07384</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24863;&#30693;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Exploring Perceptual Limitation of Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07384
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24863;&#30693;&#33021;&#21147;&#19978;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23545;&#20110;&#23567;&#22411;&#35270;&#35273;&#23545;&#35937;&#30340;&#38382;&#39064;&#65292;MLLMs&#30340;&#22238;&#31572;&#33021;&#21147;&#26222;&#36941;&#23384;&#22312;&#38480;&#21046;&#12290;&#36890;&#36807;&#25511;&#21046;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#29289;&#20307;&#36136;&#37327;&#12289;&#22823;&#23567;&#21644;&#20301;&#32622;&#37117;&#23545;MLLMs&#30340;&#24863;&#30693;&#33021;&#21147;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26368;&#36817;&#23637;&#31034;&#20102;&#22312;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#26041;&#38754;&#24341;&#20154;&#27880;&#30446;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#24863;&#30693;&#33021;&#21147;&#30340;&#38480;&#21046;&#30693;&#20043;&#29978;&#23569;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;MLLMs&#23545;&#29289;&#20307;&#22823;&#23567;&#25935;&#24863;&#30340;&#20010;&#21035;&#35777;&#25454;&#65292;&#20294;&#36825;&#19968;&#29616;&#35937;&#21450;&#20854;&#28508;&#22312;&#21407;&#22240;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#30740;&#31350;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;MLLMs&#23545;&#23567;&#22411;&#35270;&#35273;&#23545;&#35937;&#30340;&#24863;&#30693;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#22238;&#31572;&#19982;&#23567;&#22411;&#23545;&#35937;&#26377;&#20851;&#30340;&#38382;&#39064;&#26102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#29420;&#31435;&#22240;&#32032;&#65292;&#23427;&#20204;&#21487;&#33021;&#23548;&#33268;&#36825;&#31181;&#38480;&#21046;&#65292;&#21253;&#25324;&#29289;&#20307;&#36136;&#37327;&#12289;&#22823;&#23567;&#12289;&#24178;&#25200;&#29289;&#21644;&#20301;&#32622;&#65292;&#24182;&#36827;&#34892;&#20102;&#25511;&#21046;&#24615;&#24178;&#39044;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#22240;&#32032;&#23545;MLLMs&#24863;&#30693;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20302;&#36136;&#37327;&#30340;&#29289;&#20307;&#21644;&#26356;&#23567;&#30340;&#29289;&#20307;&#22823;&#23567;&#37117;&#21487;&#20197;&#29420;&#31435;&#38477;&#20302;MLLMs&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#29289;&#20307;&#20301;&#32622;&#23545;MLLMs&#30340;&#24863;&#30693;&#33021;&#21147;&#20063;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have recently shown remarkable perceptual capability in answering visual questions, however, little is known about the limits of their perception. In particular, while prior works have provided anecdotal evidence of MLLMs' sensitivity to object size, this phenomenon and its underlying causes have not been explored comprehensively. In this work, we quantitatively study the perception of small visual objects in several state-of-the-art MLLMs and reveal a pervasive limitation in answering questions about small objects in images. Next, we identify four independent factors that can contribute to this limitation -- object quality, size, distractors, and location -- and conduct controlled intervention studies to measure the effect of each factor on MLLMs' perception. In particular, we find that lower object quality and smaller object size can both independently reduce MLLMs' ability to answer visual questions. More surprisingly, we find that the locati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.07383</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#33258;&#30001;&#22320;&#20135;&#29983;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31505;&#22768;&#26159;&#20154;&#31867;&#35821;&#38899;&#20013;&#26368;&#34920;&#36798;&#24615;&#21644;&#33258;&#28982;&#30340;&#19968;&#37096;&#20998;&#65292;&#20256;&#36798;&#30528;&#24773;&#24863;&#12289;&#31038;&#20132;&#26263;&#31034;&#21644;&#24189;&#40664;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#31995;&#32479;&#32570;&#20047;&#20135;&#29983;&#36924;&#30495;&#19988;&#21512;&#36866;&#30340;&#31505;&#22768;&#30340;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#24037;&#20316;&#29983;&#25104;&#20102;&#33258;&#28982;&#30340;&#31505;&#22768;&#65292;&#20294;&#22312;&#25511;&#21046;&#29983;&#25104;&#30340;&#31505;&#22768;&#30340;&#26102;&#26426;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#21487;&#20197;&#22522;&#20110;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#30340;&#38646;&#26679;&#26412;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20135;&#29983;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ELaTE&#36890;&#36807;&#38899;&#39057;&#25552;&#31034;&#26469;&#27169;&#20223;&#22768;&#38899;&#29305;&#24449;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#26469;&#25351;&#31034;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#36755;&#20837;&#26469;&#25511;&#21046;&#31505;&#22768;&#34920;&#24773;&#65292;&#21487;&#20197;&#26159;&#31505;&#22768;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#65292;&#25110;&#21253;&#21547;&#35201;&#27169;&#20223;&#30340;&#31505;&#22768;&#30340;&#21478;&#22806;&#38899;&#39057;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#25214;&#21040;&#30340;&#25216;&#26415;&#22522;&#30784;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;Diff-RNTraj&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#29983;&#25104;&#21463;&#21040;&#32422;&#26463;&#30340;&#36712;&#36857;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#20445;&#35777;&#36712;&#36857;&#32422;&#26463;&#22312;&#36947;&#36335;&#19978;&#12289;&#32570;&#20047;&#36947;&#36335;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07369</link><description>&lt;p&gt;
Diff-RNTraj: &#19968;&#31181;&#38754;&#21521;&#36947;&#36335;&#32593;&#32476;&#32422;&#26463;&#30340;&#36712;&#36857;&#29983;&#25104;&#30340;&#32467;&#26500;&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07369
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;Diff-RNTraj&#65292;&#29992;&#20110;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#29983;&#25104;&#21463;&#21040;&#32422;&#26463;&#30340;&#36712;&#36857;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#26080;&#27861;&#20445;&#35777;&#36712;&#36857;&#32422;&#26463;&#22312;&#36947;&#36335;&#19978;&#12289;&#32570;&#20047;&#36947;&#36335;&#30456;&#20851;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#35760;&#24405;&#20102;&#36710;&#36742;&#30340;&#31227;&#21160;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#36712;&#36857;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#36712;&#36857;&#25968;&#25454;&#25366;&#25496;&#21644;&#22522;&#20110;&#36712;&#36857;&#30340;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#30340;&#26041;&#27861;&#26469;&#25193;&#22823;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22312;&#22320;&#29702;&#22352;&#26631;&#31995;&#32479;&#20013;&#29983;&#25104;&#36712;&#36857;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;1&#65289;&#19981;&#33021;&#30830;&#20445;&#29983;&#25104;&#30340;&#36712;&#36857;&#22312;&#36947;&#36335;&#19978;&#21463;&#21040;&#32422;&#26463;&#12290;2&#65289;&#32570;&#20047;&#19982;&#36947;&#36335;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65292;&#21363;&#38754;&#21521;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#30340;&#36947;&#36335;&#32593;&#32476;&#32422;&#26463;&#36712;&#36857;&#65288;RNTraj&#65289;&#29983;&#25104;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#29983;&#25104;&#24102;&#26377;&#36947;&#36335;&#30456;&#20851;&#20449;&#24687;&#30340;&#36712;&#36857;&#12290;RNTraj&#26159;&#19968;&#31181;&#28151;&#21512;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#22312;&#27599;&#20010;poi&#19978;&#37117;&#19982;&#26102;&#38388;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trajectory data is essential for various applications as it records the movement of vehicles. However, publicly available trajectory datasets remain limited in scale due to privacy concerns, which hinders the development of trajectory data mining and trajectory-based applications. To address this issue, some methods for generating synthetic trajectories have been proposed to expand the scale of the dataset. However, all existing methods generate trajectories in the geographical coordinate system, which poses two limitations for their utilization in practical applications: 1) the inability to ensure that the generated trajectories are constrained on the road. 2) the lack of road-related information. In this paper, we propose a new problem to meet the practical application need, \emph{i.e.}, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories on the road network with road-related information. RNTraj is a hybrid type of data, in which each poi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;2016&#24180;&#21644;2020&#24180;&#30340;&#36873;&#20030;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23545;&#23454;&#26045;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07368</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#35780;&#20272;&#20998;&#32452;&#20195;&#34920;&#24314;&#27169;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing Generalization for Subpopulation Representative Modeling via In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;2016&#24180;&#21644;2020&#24180;&#30340;&#36873;&#20030;&#25968;&#25454;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#23545;&#23454;&#26045;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;2016&#24180;&#21644;2020&#24180;&#32654;&#22269;&#20840;&#22269;&#36873;&#20030;&#30740;&#31350;&#30340;&#25968;&#25454;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20998;&#32452;&#20195;&#34920;&#27169;&#22411;&#65288;SRMs&#65289;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19981;&#21516;&#21709;&#24212;&#21464;&#37327;&#21644;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23613;&#31649;&#20351;&#29992;&#23454;&#35777;&#25968;&#25454;&#36827;&#34892;&#26465;&#20214;&#35774;&#23450;&#21487;&#20197;&#25552;&#39640;&#25972;&#20307;&#24615;&#33021;&#65292;&#20294;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#30410;&#22788;&#22312;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#20043;&#38388;&#24046;&#24322;&#24456;&#22823;&#65292;&#26377;&#26102;&#23545;&#26576;&#20010;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#20294;&#23545;&#20854;&#20182;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#31215;&#26497;&#24433;&#21709;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#23545;SRM&#30340;&#19981;&#20844;&#24179;&#30410;&#22788;&#20026;&#23454;&#26045;SRM&#30340;&#20174;&#19994;&#20154;&#21592;&#21644;&#20381;&#36182;&#20110;&#20854;&#30340;&#20915;&#31574;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#20986;&#20102;&#23545;&#26469;&#33258;&#19981;&#21516;&#20154;&#21475;&#23376;&#32676;&#32452;&#30340;&#32454;&#31890;&#24230;&#22522;&#20934;&#30340;&#38656;&#27714;&#65292;&#36825;&#20123;&#22522;&#20934;&#19981;&#20165;&#27979;&#35797;&#24544;&#23454;&#24230;&#65292;&#36824;&#27979;&#35797;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study evaluates the ability of Large Language Model (LLM)-based Subpopulation Representative Models (SRMs) to generalize from empirical data, utilizing in-context learning with data from the 2016 and 2020 American National Election Studies. We explore generalization across response variables and demographic subgroups. While conditioning with empirical data improves performance on the whole, the benefit of in-context learning varies considerably across demographics, sometimes hurting performance for one demographic while helping performance for others. The inequitable benefits of in-context learning for SRM present a challenge for practitioners implementing SRMs, and for decision-makers who might come to rely on them. Our work highlights a need for fine-grained benchmarks captured from diverse subpopulations that test not only fidelity but generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07366</link><description>&lt;p&gt;
&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Federated Learning Via Expectation Maximization and Turbo Deep Approximate Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#26399;&#26395;&#26368;&#22823;&#21270;&#21644;Turbo Deep&#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#35813;&#26694;&#26550;&#22312;&#22788;&#29702;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#30340;&#32570;&#28857;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#22312;&#36825;&#31181;&#33539;&#24335;&#20013;&#65292;&#23458;&#25143;&#31471;&#25317;&#26377;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#20013;&#22830;&#26381;&#21153;&#22120;&#21017;&#36127;&#36131;&#32858;&#21512;&#21644;&#35843;&#24230;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#28041;&#21450;&#23458;&#25143;&#31471;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#35757;&#32451;&#20182;&#20204;&#30340;&#26412;&#22320;&#27169;&#22411;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#23481;&#26131;&#38519;&#20837;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#36125;&#21494;&#26031;&#32852;&#37030;&#23398;&#20064;&#65288;BFL&#65289;&#26694;&#26550;&#26469;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#23398;&#20064;&#21644;&#21387;&#32553;&#38382;&#39064;&#24314;&#27169;&#20026;&#31232;&#30095;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#20854;&#20013;&#37319;&#29992;&#20102;&#20998;&#32452;&#31232;&#30095;&#20808;&#39564;&#20197;&#23454;&#29616;&#32467;&#26500;&#21270;&#27169;&#22411;&#21387;&#32553;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; BFL &#31639;&#27861;&#65292;&#21517;&#20026; EMTDAMP&#65292;&#20854;&#20013;&#23558;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#21644; Turbo Deep &#36817;&#20284;&#28040;&#24687;&#20256;&#36882;&#65288;TDAMP&#65289;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#20998;&#24067;&#24335;&#23398;&#20064;&#21644;&#21387;&#32553;&#12290;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26412;&#22320;&#21518;&#39564;&#20998;&#24067;&#20197;&#23454;&#29616;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a machine learning paradigm where the clients possess decentralized training data and the central server handles aggregation and scheduling. Typically, FL algorithms involve clients training their local models using stochastic gradient descent (SGD), which carries drawbacks such as slow convergence and being prone to getting stuck in suboptimal solutions. In this work, we propose a message passing based Bayesian federated learning (BFL) framework to avoid these drawbacks.Specifically, we formulate the problem of deep neural network (DNN) learning and compression and as a sparse Bayesian inference problem, in which group sparse prior is employed to achieve structured model compression. Then, we propose an efficient BFL algorithm called EMTDAMP, where expectation maximization (EM) and turbo deep approximate message passing (TDAMP) are combined to achieve distributed learning and compression. The central server aggregates local posterior distributions to update 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#26368;&#20248;&#25237;&#36164;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21521;-&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#32435;&#20160;&#22343;&#34913;&#29305;&#24449;&#21644;&#38543;&#26426;&#24494;&#20998;&#28216;&#25103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#20114;&#32467;&#26500;&#30340;&#22270;&#24418;&#22270;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.07365</link><description>&lt;p&gt;
&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#24322;&#36136;&#20195;&#29702;&#20154;&#26368;&#20248;&#25237;&#36164;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Method for Optimal Investment Under Relative Performance Criteria Among Heterogeneous Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#26368;&#20248;&#25237;&#36164;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21521;-&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#32435;&#20160;&#22343;&#34913;&#29305;&#24449;&#21644;&#38543;&#26426;&#24494;&#20998;&#28216;&#25103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25968;&#20540;&#23454;&#39564;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#27169;&#22411;&#19978;&#36827;&#34892;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#20132;&#20114;&#32467;&#26500;&#30340;&#22270;&#24418;&#22270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Graphon&#28216;&#25103;&#26469;&#30740;&#31350;&#36890;&#36807;&#19968;&#24352;&#20132;&#20114;&#30340;&#21152;&#26435;&#22270;&#30340;&#35768;&#22810;&#29609;&#23478;&#30340;&#28216;&#25103;&#12290;&#36890;&#36807;&#21462;&#26497;&#38480;&#65292;&#24471;&#21040;&#20102;&#19968;&#20010;&#20855;&#26377;&#36830;&#32493;&#29609;&#23478;&#30340;&#28216;&#25103;&#65292;&#20854;&#20013;&#30340;&#20132;&#20114;&#26159;&#36890;&#36807;&#19968;&#20010;&#22270;&#24418;&#22270;&#12290;&#26412;&#25991;&#20851;&#27880;&#30456;&#23545;&#32489;&#25928;&#26631;&#20934;&#19979;&#30340;&#22270;&#24418;&#28216;&#25103;&#30340;&#26368;&#20248;&#25237;&#36164;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#30340;&#22522;&#30784;&#19978;&#65306;&#39318;&#20808;&#65292;&#36890;&#36807;&#21069;&#21521;-&#21518;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#23545;&#32435;&#20160;&#22343;&#34913;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#20854;&#27425;&#65292;&#21033;&#29992;&#38543;&#26426;&#24494;&#20998;&#28216;&#25103;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#27169;&#22411;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#22312;&#27599;&#20010;&#27169;&#22411;&#20013;&#27604;&#36739;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#22270;&#24418;&#22270;&#23545;&#20132;&#20114;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphon games have been introduced to study games with many players who interact through a weighted graph of interaction. By passing to the limit, a game with a continuum of players is obtained, in which the interactions are through a graphon. In this paper, we focus on a graphon game for optimal investment under relative performance criteria, and we propose a deep learning method. The method builds upon two key ingredients: first, a characterization of Nash equilibria by forward-backward stochastic differential equations and, second, recent advances of machine learning algorithms for stochastic differential games. We provide numerical experiments on two different financial models. In each model, we compare the effect of several graphons, which correspond to different structures of interactions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19968;&#20215;&#25293;&#21334;&#20013;&#36827;&#34892;&#31454;&#26631;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#22312;&#25112;&#30053;&#32972;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#38754;&#23545;&#23545;&#31574;&#24615;&#21334;&#23478;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#28608;&#21169;&#20080;&#23478;&#36827;&#34892;&#30495;&#23454;&#30340;&#20132;&#26131;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21518;&#24724;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07363</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#31283;&#23450;&#24615;&#30340;&#23398;&#20064;&#31639;&#27861;&#22312;&#19968;&#20215;&#25293;&#21334;&#20013;&#30340;&#31454;&#26631;
&lt;/p&gt;
&lt;p&gt;
Strategically-Robust Learning Algorithms for Bidding in First-Price Auctions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19968;&#20215;&#25293;&#21334;&#20013;&#36827;&#34892;&#31454;&#26631;&#30340;&#26032;&#39062;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#22312;&#25112;&#30053;&#32972;&#26223;&#19979;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#38754;&#23545;&#23545;&#31574;&#24615;&#21334;&#23478;&#26102;&#34920;&#29616;&#33391;&#22909;&#65292;&#28608;&#21169;&#20080;&#23478;&#36827;&#34892;&#30495;&#23454;&#30340;&#20132;&#26131;&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20339;&#30340;&#21518;&#24724;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#29702;&#35770;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#30028;&#22788;&#65292;&#23398;&#20064;&#22312;&#37325;&#22797;&#30340;&#19968;&#20215;&#25293;&#21334;&#20013;&#36827;&#34892;&#31454;&#26631;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#30001;&#20110;&#26174;&#31034;&#24191;&#21578;&#36716;&#21521;&#19968;&#20215;&#25293;&#21334;&#65292;&#26368;&#36817;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20985;&#20989;&#25968;&#24418;&#24335;&#65292;&#29992;&#20110;&#19968;&#20215;&#25293;&#21334;&#20013;&#32431;&#31574;&#30053;&#30340;&#31454;&#26631;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#30340;&#33258;&#28982;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36229;&#36234;&#20102;&#36807;&#21435;&#24037;&#20316;&#30340;&#24046;&#36317;&#65292;&#36824;&#32771;&#34385;&#20102;&#22312;&#32447;&#24191;&#21578;&#24066;&#22330;&#30340;&#25112;&#30053;&#32972;&#26223;&#65292;&#20854;&#20013;&#37096;&#32626;&#20102;&#31454;&#26631;&#31639;&#27861; - &#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20250;&#34987;&#31574;&#30053;&#24615;&#21334;&#23478;&#21033;&#29992;&#65292;&#24182;&#19988;&#23427;&#20204;&#28608;&#21169;&#20080;&#23478;&#35802;&#23454;&#20132;&#26131;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#26368;&#39640;&#31454;&#20105;&#20986;&#20215;&#36890;&#36807;&#23545;&#25239;&#26041;&#24335;&#29983;&#25104;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$O(\sqrt{T})$&#30340;&#21518;&#24724;&#65292;&#24182;&#34920;&#26126;&#27809;&#26377;&#26356;&#22909;&#30340;&#22312;&#32447;&#31639;&#27861;&#21487;&#20197;&#20570;&#24471;&#26356;&#22909;&#12290;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#24403;&#26368;&#39640;&#31454;&#20105;&#20986;&#20215;&#36890;&#36807;&#23545;&#25239;&#26041;&#24335;&#29983;&#25104;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#20102;$O(\log T)$&#30340;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to bid in repeated first-price auctions is a fundamental problem at the interface of game theory and machine learning, which has seen a recent surge in interest due to the transition of display advertising to first-price auctions. In this work, we propose a novel concave formulation for pure-strategy bidding in first-price auctions, and use it to analyze natural Gradient-Ascent-based algorithms for this problem. Importantly, our analysis goes beyond regret, which was the typical focus of past work, and also accounts for the strategic backdrop of online-advertising markets where bidding algorithms are deployed -- we prove that our algorithms cannot be exploited by a strategic seller and that they incentivize truth-telling for the buyer.   Concretely, we show that our algorithms achieve $O(\sqrt{T})$ regret when the highest competing bids are generated adversarially, and show that no online algorithm can do better. We further prove that the regret improves to $O(\log T)$ when th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#26657;&#20934;&#20855;&#26377;&#23616;&#37096;&#35206;&#30422;&#20445;&#35777;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#26469;&#21019;&#24314;&#26368;&#31895;&#31961;&#30340;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#65292;&#20197;&#36817;&#20284;&#26465;&#20214;&#35206;&#30422;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.07357</link><description>&lt;p&gt;
&#22238;&#24402;&#26641;&#29992;&#20110;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;
Regression Trees for Fast and Adaptive Prediction Intervals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07357
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#26657;&#20934;&#20855;&#26377;&#23616;&#37096;&#35206;&#30422;&#20445;&#35777;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#35757;&#32451;&#26469;&#21019;&#24314;&#26368;&#31895;&#31961;&#30340;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#65292;&#20197;&#36817;&#20284;&#26465;&#20214;&#35206;&#30422;&#65292;&#25552;&#20379;&#20102;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#27169;&#22411;&#20250;&#29359;&#38169;&#65292;&#22240;&#27492;&#38656;&#35201;&#37327;&#21270;&#19982;&#20854;&#39044;&#27979;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#31526;&#21512;&#24615;&#25512;&#26029;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#28857;&#39044;&#27979;&#21608;&#22260;&#21019;&#24314;&#32479;&#35745;&#19978;&#26377;&#25928;&#30340;&#39044;&#27979;&#21306;&#22495;&#65292;&#20294;&#26159;&#23427;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#26420;&#32032;&#24212;&#29992;&#20250;&#20135;&#29983;&#38750;&#33258;&#36866;&#24212;&#30340;&#21306;&#22495;&#12290;&#26032;&#30340;&#31526;&#21512;&#24615;&#24471;&#20998;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#22120;&#25110;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#22120;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#21019;&#24314;&#39044;&#27979;&#24102;&#26041;&#38754;&#24456;&#26377;&#29992;&#65292;&#20294;&#36825;&#20123;&#24471;&#20998;&#19982;&#37327;&#21270;&#20219;&#24847;&#39044;&#27979;&#27169;&#22411;&#21608;&#22260;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#21407;&#22987;&#30446;&#26631;&#33073;&#33410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#26041;&#27861;&#26063;&#65292;&#29992;&#20110;&#26657;&#20934;&#20855;&#26377;&#23616;&#37096;&#35206;&#30422;&#20445;&#35777;&#30340;&#22238;&#24402;&#38382;&#39064;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#36861;&#27714;&#26368;&#31895;&#31961;&#30340;&#29305;&#24449;&#31354;&#38388;&#21010;&#20998;&#26469;&#36817;&#20284;&#26465;&#20214;&#35206;&#30422;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#31526;&#21512;&#24615;&#24471;&#20998;&#36827;&#34892;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#30340;&#35757;&#32451;&#26469;&#21019;&#24314;&#36825;&#20010;&#21010;&#20998;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#23558;&#22238;&#24402;&#26641;&#21644;&#38543;&#26426;&#26862;&#26519;&#24212;&#29992;&#20110;&#31526;&#21512;&#24615;&#25512;&#26029;&#30340;&#26032;&#39046;&#22495;&#65292;&#20197;&#25552;&#20379;&#20934;&#30830;&#12289;&#24555;&#36895;&#21644;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.07356</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Novel Gaussian Min-Max Theorem and its Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#23450;&#29702;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#23450;&#29702;&#23545;&#20110;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#35813;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Gordon&#30340;&#19968;&#20010;&#33879;&#21517;&#32467;&#26524;&#20801;&#35768;&#27604;&#36739;&#20004;&#20010;&#39640;&#26031;&#36807;&#31243;&#30340;&#26368;&#23567;&#26368;&#22823;&#34892;&#20026;&#65292;&#22914;&#26524;&#28385;&#36275;&#26576;&#20123;&#19981;&#31561;&#24335;&#26465;&#20214;&#12290;&#36825;&#20010;&#32467;&#26524;&#30340;&#32467;&#26524;&#21253;&#25324;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;GMT&#65289;&#21644;&#20984;&#39640;&#26031;&#26368;&#23567;&#26368;&#22823;&#65288;CGMT&#65289;&#23450;&#29702;&#65292;&#36825;&#20123;&#23450;&#29702;&#22312;&#39640;&#32500;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#38750;&#20809;&#28369;&#20248;&#21270;&#21644;&#20449;&#21495;&#22788;&#29702;&#26041;&#38754;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30446;&#21069;&#20026;&#27490;&#65292;&#27809;&#26377;&#21457;&#29616;&#28385;&#36275;&#36825;&#20123;&#19981;&#31561;&#24335;&#30340;&#20854;&#20182;&#19968;&#23545;&#39640;&#26031;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#26679;&#19968;&#23545;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#23450;&#29702;&#23558;&#32463;&#20856;&#30340;GMT&#23450;&#29702;&#21644;CGMT&#23450;&#29702;&#20174;&#22522;&#26412;&#36807;&#31243;&#20013;&#30340;&#24213;&#23618;&#39640;&#26031;&#30697;&#38453;&#20855;&#26377;iid&#34892;&#30340;&#24773;&#20917;&#25193;&#23637;&#21040;&#20855;&#26377;&#29420;&#31435;&#20294;&#38750;&#24658;&#23450;&#20998;&#24067;&#30340;&#24773;&#20917;&#12290;&#26032;&#30340;CGMT&#23450;&#29702;&#24212;&#29992;&#20110;&#22810;&#28304;&#39640;&#26031;&#22238;&#24402;&#38382;&#39064;&#65292;&#20197;&#21450;&#23646;&#20110;&#30340;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A celebrated result by Gordon allows one to compare the min-max behavior of two Gaussian processes if certain inequality conditions are met. The consequences of this result include the Gaussian min-max (GMT) and convex Gaussian min-max (CGMT) theorems which have had far-reaching implications in high-dimensional statistics, machine learning, non-smooth optimization, and signal processing. Both theorems rely on a pair of Gaussian processes, first identified by Slepian, that satisfy Gordon's comparison inequalities. To date, no other pair of Gaussian processes satisfying these inequalities has been discovered. In this paper, we identify such a new pair. The resulting theorems extend the classical GMT and CGMT Theorems from the case where the underlying Gaussian matrix in the primary process has iid rows to where it has independent but non-identically-distributed ones. The new CGMT is applied to the problems of multi-source Gaussian regression, as well as to binary classification of genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07355</link><description>&lt;p&gt;
&#20174;&#22343;&#22330;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Sampling from the Mean-Field Stationary Distribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#22343;&#22330;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243; (SDE) &#30340;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#31561;&#20215;&#22320;&#65292;&#21363;&#21253;&#21547;&#20132;&#20114;&#39033;&#30340;&#27010;&#29575;&#27979;&#24230;&#31354;&#38388;&#19978;&#30340;&#26368;&#23567;&#21270;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#27934;&#23519;&#26159;&#23558;&#36825;&#20010;&#38382;&#39064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#35299;&#32806;&#65306;(1) &#36890;&#36807;&#26377;&#38480;&#31890;&#23376;&#31995;&#32479;&#36924;&#36817;&#22343;&#22330;SDE&#65292;&#36890;&#36807;&#26102;&#38388;&#22343;&#21248;&#20256;&#25773;&#28151;&#27788;&#65292;&#21644;(2) &#36890;&#36807;&#26631;&#20934;&#23545;&#25968;&#20985;&#25277;&#26679;&#22120;&#20174;&#26377;&#38480;&#31890;&#23376;&#31283;&#24577;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#26356;&#31616;&#21333;&#65292;&#20854;&#28789;&#27963;&#24615;&#20801;&#35768;&#32467;&#21512;&#29992;&#20110;&#31639;&#27861;&#21644;&#29702;&#35770;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#36825;&#23548;&#33268;&#22312;&#35768;&#22810;&#35774;&#32622;&#20013;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#20445;&#35777;&#65292;&#21253;&#25324;&#22312;&#22343;&#22330;&#21306;&#22495;&#20248;&#21270;&#26576;&#20123;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#26356;&#22909;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65288;DDCL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20004;&#31181;&#35780;&#20998;&#26041;&#27861;&#65288;DDCL&#65288;&#23494;&#24230;&#65289;&#21644;DDCL&#65288;&#28857;&#65289;&#65289;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#39034;&#24207;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07352</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data Distribution-based Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65288;DDCL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#21644;&#20004;&#31181;&#35780;&#20998;&#26041;&#27861;&#65288;DDCL&#65288;&#23494;&#24230;&#65289;&#21644;DDCL&#65288;&#28857;&#65289;&#65289;&#65292;&#26681;&#25454;&#26679;&#26412;&#30340;&#39034;&#24207;&#26469;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#26679;&#26412;&#30340;&#39034;&#24207;&#23545;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#35838;&#31243;&#23398;&#20064;&#26159;&#19968;&#31181;&#23558;&#35757;&#32451;&#26679;&#26412;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#25490;&#24207;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22522;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#35838;&#31243;&#23398;&#20064;&#65288;DDCL&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;DDCL&#21033;&#29992;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#20197;&#21450;&#20004;&#31181;&#35780;&#20998;&#26041;&#27861;&#65288;DDCL&#65288;&#23494;&#24230;&#65289;&#21644;DDCL&#65288;&#28857;&#65289;&#65289;&#26469;&#20915;&#23450;&#35757;&#32451;&#26679;&#26412;&#30340;&#39034;&#24207;&#12290;DDCL&#65288;&#23494;&#24230;&#65289;&#21033;&#29992;&#26679;&#26412;&#23494;&#24230;&#20998;&#37197;&#35780;&#20998;&#65292;&#32780;DDCL&#65288;&#28857;&#65289;&#21033;&#29992;&#27431;&#27663;&#36317;&#31163;&#36827;&#34892;&#35780;&#20998;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;DDCL&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#27809;&#26377;&#20219;&#20309;&#35838;&#31243;&#30340;&#26631;&#20934;&#35780;&#20272;&#30456;&#27604;&#65292;&#24212;&#29992;DDCL&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The order of training samples can have a significant impact on the performance of a classifier. Curriculum learning is a method of ordering training samples from easy to hard. This paper proposes the novel idea of a curriculum learning approach called Data Distribution-based Curriculum Learning (DDCL). DDCL uses the data distribution of a dataset to build a curriculum based on the order of samples. Two types of scoring methods known as DDCL (Density) and DDCL (Point) are used to score training samples thus determining their training order. DDCL (Density) uses the sample density to assign scores while DDCL (Point) utilises the Euclidean distance for scoring. We evaluate the proposed DDCL approach by conducting experiments on multiple datasets using a neural network, support vector machine and random forest classifier. Evaluation results show that the application of DDCL improves the average classification accuracy for all datasets compared to standard evaluation without any curriculum. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;TextFooler&#40657;&#30418;&#23545;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#25915;&#20987;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;sigmoid&#28608;&#27963;&#20132;&#21449;&#29109;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;01 loss sign&#28608;&#27963;&#30340;&#32593;&#32476;&#26356;&#38590;&#21463;&#21040;TextFooler&#30340;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#27719;&#38598;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#31934;&#30830;&#24230;&#65292;&#20351;TextFooler&#20960;&#20046;&#26080;&#25928;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.07347</link><description>&lt;p&gt;
TextFooler&#40657;&#30418;&#23545;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#25915;&#20987;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Accuracy of TextFooler black box adversarial attacks on 01 loss sign activation neural network ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;TextFooler&#40657;&#30418;&#23545;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#30340;&#25915;&#20987;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;sigmoid&#28608;&#27963;&#20132;&#21449;&#29109;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;01 loss sign&#28608;&#27963;&#30340;&#32593;&#32476;&#26356;&#38590;&#21463;&#21040;TextFooler&#30340;&#25915;&#20987;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#20840;&#23616;&#27719;&#38598;&#27493;&#39588;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23545;&#25239;&#31934;&#30830;&#24230;&#65292;&#20351;TextFooler&#20960;&#20046;&#26080;&#25928;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#23545;&#25239;&#25915;&#20987;&#26041;&#38754;&#20855;&#26377;&#38450;&#24481;&#33021;&#21147;&#12290;&#38024;&#23545;CIFAR10&#25968;&#25454;&#38598;&#25915;&#20987;&#27169;&#22411;&#30340;&#20844;&#20849;&#25361;&#25112;&#20173;&#28982;&#20445;&#25345;&#19981;&#36133;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#38382;&#39064;&#65306;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;TextFooler&#30340;&#27969;&#34892;&#40657;&#30418;&#25991;&#26412;&#23545;&#25239;&#25915;&#20987;&#31243;&#24207;&#26469;&#27450;&#39575;01 loss sign&#28608;&#27963;&#31070;&#32463;&#32593;&#32476;&#22256;&#38590;&#21527;&#65311;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;IMDB&#35780;&#35770;&#12289;Yelp&#35780;&#35770;&#12289;MR&#24773;&#24863;&#20998;&#31867;&#21644;AG&#26032;&#38395;&#20998;&#31867;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;sigmoid&#28608;&#27963;&#20132;&#21449;&#29109;&#21644;&#20108;&#36827;&#21046;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;01 loss sign&#28608;&#27963;&#32593;&#32476;&#26356;&#38590;&#21463;&#21040;TextFooler&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#31181;01 loss sign&#28608;&#27963;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#20855;&#26377;&#19968;&#31181;&#38024;&#23545;sign&#28608;&#27963;&#32593;&#32476;&#30340;&#26032;&#22411;&#20840;&#23616;&#27719;&#38598;&#27493;&#39588;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#30340;&#21464;&#20307;&#65292;&#25105;&#20204;&#30475;&#21040;&#20102;&#22312;&#23545;&#25239;&#31934;&#30830;&#24230;&#26041;&#38754;&#30340;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#24471;TextFooler&#23545;&#23427;&#20960;&#20046;&#26080;&#25928;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#20195;&#30721;&#20379;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the defense of 01 loss sign activation neural networks against image classification adversarial attacks. A public challenge to attack the models on CIFAR10 dataset remains undefeated. We ask the following question in this study: are 01 loss sign activation neural networks hard to deceive with a popular black box text adversarial attack program called TextFooler? We study this question on four popular text classification datasets: IMDB reviews, Yelp reviews, MR sentiment classification, and AG news classification. We find that our 01 loss sign activation network is much harder to attack with TextFooler compared to sigmoid activation cross entropy and binary neural networks. We also study a 01 loss sign activation convolutional neural network with a novel global pooling step specific to sign activation networks. With this new variation we see a significant gain in adversarial accuracy rendering TextFooler practically useless against it. We make our code freely avail
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;ICU&#30149;&#20154;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#26368;&#26032;&#25968;&#25454;&#38598;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07344</link><description>&lt;p&gt;
&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;ICU&#30149;&#20154;&#30340;&#27979;&#37327;&#25490;&#31243;
&lt;/p&gt;
&lt;p&gt;
Measurement Scheduling for ICU Patients with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;ICU&#30149;&#20154;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#26368;&#26032;&#25968;&#25454;&#38598;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#25552;&#39640;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICU&#30149;&#20154;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;ICU&#20013;&#19979;&#36798;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#32422;&#26377;20-40%&#26159;&#22810;&#20313;&#30340;&#65292;&#21487;&#20197;&#22312;&#19981;&#22952;&#30861;&#24739;&#32773;&#23433;&#20840;&#30340;&#24773;&#20917;&#19979;&#28040;&#38500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;Offline-RL&#65289;&#22522;&#20110;&#24739;&#32773;&#20449;&#24687;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#33258;&#37027;&#26102;&#20197;&#26469;&#24050;&#32463;&#21457;&#24067;&#20102;&#26032;&#30340;ICU&#30149;&#20154;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20063;&#21462;&#24471;&#20102;&#21508;&#31181;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#30340;&#26032;&#21457;&#24067;&#30340;MIMIC-IV&#25968;&#25454;&#38598;&#30340;&#39044;&#22788;&#29702;&#27969;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26368;&#20808;&#36827;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35782;&#21035;&#26356;&#22909;&#30340;ICU&#30149;&#20154;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#31574;&#30053;&#26041;&#38754;&#30340;&#21151;&#25928;&#12290;&#38500;&#20102;&#35780;&#20272;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;ICU&#29615;&#22659;&#20013;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#23454;&#39564;&#23460;&#26816;&#27979;&#25490;&#31243;&#30340;&#24635;&#20307;&#36866;&#29992;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scheduling laboratory tests for ICU patients presents a significant challenge. Studies show that 20-40% of lab tests ordered in the ICU are redundant and could be eliminated without compromising patient safety. Prior work has leveraged offline reinforcement learning (Offline-RL) to find optimal policies for ordering lab tests based on patient information. However, new ICU patient datasets have since been released, and various advancements have been made in Offline-RL methods. In this study, we first introduce a preprocessing pipeline for the newly-released MIMIC-IV dataset geared toward time-series tasks. We then explore the efficacy of state-of-the-art Offline-RL methods in identifying better policies for ICU patient lab test scheduling. Besides assessing methodological performance, we also discuss the overall suitability and practicality of using Offline-RL frameworks for scheduling laboratory tests in ICU settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26410;&#30693;&#22122;&#22768;&#27700;&#24179;&#30340;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#19982;&#24050;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#32500;&#24230;&#36739;&#22823;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#30028;&#22870;&#21169;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07341</link><description>&lt;p&gt;
&#23545;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#30340;&#22122;&#22768;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#21450;&#20854;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Noise-Adaptive Confidence Sets for Linear Bandits and Application to Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#26410;&#30693;&#22122;&#22768;&#27700;&#24179;&#30340;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#19982;&#24050;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#32500;&#24230;&#36739;&#22823;&#26102;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#38024;&#23545;&#26377;&#30028;&#22870;&#21169;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#24046;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#36143;&#20915;&#31574;&#20013;&#65292;&#36866;&#24212;&#26410;&#30693;&#22122;&#22768;&#27700;&#24179;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26377;&#25928;&#30340;&#25506;&#32034;&#36890;&#24120;&#38656;&#35201;&#23545;&#22122;&#22768;&#27700;&#24179;&#26377;&#19968;&#23450;&#30340;&#20102;&#35299;&#65292;&#32780;&#22122;&#22768;&#27700;&#24179;&#36890;&#24120;&#21482;&#33021;&#31895;&#30053;&#22320;&#25351;&#23450;&#12290;&#25105;&#20204;&#22312;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20027;&#35201;&#26377;&#20004;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#35813;&#32622;&#20449;&#21306;&#38388;&#22312;&#26410;&#30693;&#30340;&#20122;&#39640;&#26031;&#21442;&#25968;&#963;_*^2&#19978;&#26159;&#8220;&#21322;&#33258;&#36866;&#24212;&#8221;&#30340;&#65292;&#24847;&#21619;&#30528;&#65288;&#24402;&#19968;&#21270;&#30340;&#65289;&#32622;&#20449;&#23485;&#24230;&#19982;&#8730;&#65288;d&#963;_*^2 + &#963;_0^2&#65289;&#25104;&#27491;&#27604;&#65292;&#20854;&#20013;d&#20026;&#32500;&#24230;&#65292;&#963;_0^2&#20026;&#25351;&#23450;&#30340;&#65288;&#24050;&#30693;&#65289;&#20122;&#39640;&#26031;&#21442;&#25968;&#65292;&#20854;&#20540;&#21487;&#33021;&#27604;&#963;_*^2&#22823;&#24471;&#22810;&#12290;&#30456;&#27604;&#20110;Abbasi-Yadkori&#31561;&#20154;&#65288;2011&#65289;&#30340;&#26631;&#20934;&#32622;&#20449;&#21306;&#38388;&#30340;&#8730;&#65288;d&#963;_0^2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#24403;d&#36739;&#22823;&#26102;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#23548;&#33268;&#20102;&#32447;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#25913;&#36827;&#30340;&#21518;&#24724;&#36793;&#30028;&#12290;&#20854;&#27425;&#65292;&#23545;&#20110;&#26377;&#30028;&#22870;&#21169;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#33258;&#36866;&#24212;&#32622;&#20449;&#21306;&#38388;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting to a priori unknown noise level is a very important but challenging problem in sequential decision-making as efficient exploration typically requires knowledge of the noise level, which is often loosely specified. We report significant progress in addressing this issue in linear bandits in two respects. First, we propose a novel confidence set that is `semi-adaptive' to the unknown sub-Gaussian parameter $\sigma_*^2$ in the sense that the (normalized) confidence width scales with $\sqrt{d\sigma_*^2 + \sigma_0^2}$ where $d$ is the dimension and $\sigma_0^2$ is the specified sub-Gaussian parameter (known) that can be much larger than $\sigma_*^2$. This is a significant improvement over $\sqrt{d\sigma_0^2}$ of the standard confidence set of Abbasi-Yadkori et al. (2011), especially when $d$ is large. We show that this leads to an improved regret bound in linear bandits. Second, for bounded rewards, we propose a novel variance-adaptive confidence set that has a much improved numeri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#23545;&#40784;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39640;&#27010;&#29575;&#24674;&#22797;&#27491;&#30830;&#30340;&#39030;&#28857;&#23545;&#40784;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#27700;&#24179;&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#30452;&#25509;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.07340</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#38543;&#26426;&#20960;&#20309;&#22270;&#36827;&#34892;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Random Geometric Graph Alignment with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#23545;&#40784;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39640;&#27010;&#29575;&#24674;&#22797;&#27491;&#30830;&#30340;&#39030;&#28857;&#23545;&#40784;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#27700;&#24179;&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#30452;&#25509;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39030;&#28857;&#29305;&#24449;&#20449;&#24687;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#23545;&#40784;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20004;&#20010;&#29420;&#31435;&#25200;&#21160;&#30340;&#21333;&#20010;&#38543;&#26426;&#20960;&#20309;&#22270;&#20197;&#21450;&#22122;&#22768;&#31232;&#30095;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#26159;&#24674;&#22797;&#20004;&#20010;&#22270;&#30340;&#39030;&#28857;&#20043;&#38388;&#30340;&#26410;&#30693;&#19968;&#23545;&#19968;&#26144;&#23556;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#29305;&#24449;&#21521;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#27700;&#24179;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21333;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#24456;&#39640;&#30340;&#27010;&#29575;&#19979;&#36890;&#36807;&#22270;&#32467;&#26500;&#26469;&#24674;&#22797;&#27491;&#30830;&#30340;&#39030;&#28857;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22122;&#22768;&#27700;&#24179;&#30340;&#26465;&#20214;&#19978;&#30028;&#65292;&#20165;&#23384;&#22312;&#23545;&#25968;&#22240;&#23376;&#24046;&#36317;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#19982;&#30452;&#25509;&#22312;&#22122;&#22768;&#39030;&#28857;&#29305;&#24449;&#19978;&#27714;&#35299;&#20998;&#37197;&#38382;&#39064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22122;&#22768;&#27700;&#24179;&#33267;&#23569;&#20026;&#24120;&#25968;&#26102;&#65292;&#36825;&#31181;&#30452;&#25509;&#21305;&#37197;&#20250;&#23548;&#33268;&#24674;&#22797;&#19981;&#23436;&#20840;&#65292;&#32780;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23481;&#24525;n
&lt;/p&gt;
&lt;p&gt;
We characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information. More specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure. We also prove that our conditions on the noise level are tight up to logarithmic factors. Finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features. We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#12290;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;MoE&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;DP&#35299;&#20915;&#20102;&#35745;&#31639;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#20102;MoE&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#26377;&#25928;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.07334</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Training of Mixture of Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22914;&#20309;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#12290;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;MoE&#27169;&#22411;&#30340;&#28508;&#21147;&#21644;&#24212;&#29992;DP&#35299;&#20915;&#20102;&#35745;&#31639;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#20102;MoE&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21069;&#25552;&#19979;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#23558;&#24046;&#20998;&#38544;&#31169;(DP)&#19982;&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;(MoE)&#30340;&#35757;&#32451;&#30456;&#32467;&#21512;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21442;&#25968;&#35268;&#27169;&#25193;&#22823;&#21040;&#25968;&#21313;&#20159;&#65292;&#21033;&#29992;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#23637;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#26032;&#20852;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22686;&#38271;&#24341;&#21457;&#20102;&#37325;&#35201;&#30340;&#35745;&#31639;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;MoE&#27169;&#22411;&#30340;&#28508;&#21147;&#65288;&#23427;&#20204;&#20197;&#35745;&#31639;&#25928;&#29575;&#33879;&#31216;&#65289;&#21644;&#24212;&#29992;DP&#65288;&#38544;&#31169;&#20445;&#25252;&#30340;&#26631;&#20934;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#22312;DP&#30340;&#32422;&#26463;&#19979;&#35757;&#32451;MoE&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20854;&#26550;&#26500;&#21644;DP&#25972;&#21512;&#30340;&#22797;&#26434;&#24615;&#25152;&#24102;&#26469;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;MoE&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DP&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#19982;&#38750;&#38544;&#31169;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#36825;&#39033;&#21021;&#27493;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This position paper investigates the integration of Differential Privacy (DP) in the training of Mixture of Experts (MoE) models within the field of natural language processing. As Large Language Models (LLMs) scale to billions of parameters, leveraging expansive datasets, they exhibit enhanced linguistic capabilities and emergent abilities. However, this growth raises significant computational and privacy concerns. Our study addresses these issues by exploring the potential of MoE models, known for their computational efficiency, and the application of DP, a standard for privacy preservation. We present the first known attempt to train MoE models under the constraints of DP, addressing the unique challenges posed by their architecture and the complexities of DP integration. Our initial experimental studies demonstrate that MoE models can be effectively trained with DP, achieving performance that is competitive with their non-private counterparts. This initial study aims to provide val
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#25253;&#21578;&#24635;&#32467;&#20102;&#22312;Hugging Face&#23384;&#20648;&#24211;&#36827;&#34892;&#30340;&#20004;&#39033;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#30899;&#25490;&#25918;&#12289;ML&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#26041;&#38754;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23454;&#29992;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.07323</link><description>&lt;p&gt;
&#20174;&#25366;&#25496;Hugging Face&#23384;&#20648;&#24211;&#20013;&#23398;&#21040;&#30340;&#25945;&#35757;
&lt;/p&gt;
&lt;p&gt;
Lessons Learned from Mining the Hugging Face Repository
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07323
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#25253;&#21578;&#24635;&#32467;&#20102;&#22312;Hugging Face&#23384;&#20648;&#24211;&#36827;&#34892;&#30340;&#20004;&#39033;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#30899;&#25490;&#25918;&#12289;ML&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#26041;&#38754;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23454;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#21457;&#23637;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#35265;&#35777;&#20102;&#20687;Hugging Face&#65288;HF&#65289;&#36825;&#26679;&#30340;&#24179;&#21488;&#30340;&#20852;&#36215;&#65292;&#25104;&#20026;&#27169;&#22411;&#24320;&#21457;&#21644;&#20849;&#20139;&#30340;&#20013;&#24515;&#26530;&#32445;&#12290;&#26412;&#32463;&#39564;&#25253;&#21578;&#32508;&#21512;&#20102;&#20004;&#39033;&#22312;HF&#19978;&#36827;&#34892;&#30340;&#32508;&#21512;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#30899;&#25490;&#25918;&#12289;ML&#27169;&#22411;&#30340;&#28436;&#21270;&#21644;&#32500;&#25252;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;&#26410;&#26469;&#22312;HF&#29983;&#24577;&#31995;&#32479;&#20013;&#36827;&#34892;&#36719;&#20214;&#23384;&#20648;&#24211;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23454;&#29992;&#25351;&#21335;&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#30740;&#31350;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#25105;&#20204;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#22797;&#21046;&#21253;&#30340;&#22797;&#26434;&#24615;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#20419;&#36827;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32454;&#33268;&#30340;&#20998;&#23618;&#25277;&#26679;&#31574;&#30053;&#65292;&#38024;&#23545;&#22810;&#26679;&#21270;&#30340;HF Hub&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#20195;&#34920;&#24615;&#21644;&#20840;&#38754;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#35813;&#25253;&#21578;&#36824;&#20171;&#32461;&#20102;&#21021;&#27493;&#30340;&#25351;&#21335;&#65292;&#20174;&#23384;&#20648;&#24211;&#25366;&#25496;&#36807;&#28193;&#21040;&#38431;&#21015;&#30740;&#31350;&#65292;&#20197;&#30830;&#31435;&#26356;&#35814;&#23454;&#30340;&#30740;&#31350;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly evolving fields of Machine Learning (ML) and Artificial Intelligence have witnessed the emergence of platforms like Hugging Face (HF) as central hubs for model development and sharing. This experience report synthesizes insights from two comprehensive studies conducted on HF, focusing on carbon emissions and the evolutionary and maintenance aspects of ML models. Our objective is to provide a practical guide for future researchers embarking on mining software repository studies within the HF ecosystem to enhance the quality of these studies. We delve into the intricacies of the replication package used in our studies, highlighting the pivotal tools and methodologies that facilitated our analysis. Furthermore, we propose a nuanced stratified sampling strategy tailored for the diverse HF Hub dataset, ensuring a representative and comprehensive analytical approach. The report also introduces preliminary guidelines, transitioning from repository mining to cohort studies, to esta
&lt;/p&gt;</description></item><item><title>Transformer-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#20351;&#29992;&#21152;&#27861;&#27169;&#24335;&#26469;&#23384;&#20648;&#21644;&#26816;&#32034;&#30693;&#35782;&#65292;&#36890;&#36807;&#30456;&#21152;&#32452;&#21512;&#22810;&#20010;&#29420;&#31435;&#30340;&#26426;&#21046;&#23545;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#26500;&#36896;&#24615;&#24178;&#28041;&#12290;</title><link>https://arxiv.org/abs/2402.07321</link><description>&lt;p&gt;
&#24635;&#32467;&#20107;&#23454;&#65306;LLMs&#20013;&#32972;&#21518;&#30340;&#21152;&#27861;&#26426;&#21046;&#35299;&#26512;&#20107;&#23454;&#22238;&#24518;&#30340;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07321
&lt;/p&gt;
&lt;p&gt;
Transformer-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#20351;&#29992;&#21152;&#27861;&#27169;&#24335;&#26469;&#23384;&#20648;&#21644;&#26816;&#32034;&#30693;&#35782;&#65292;&#36890;&#36807;&#30456;&#21152;&#32452;&#21512;&#22810;&#20010;&#29420;&#31435;&#30340;&#26426;&#21046;&#23545;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#26500;&#36896;&#24615;&#24178;&#28041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer-based&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#22914;&#20309;&#23384;&#20648;&#21644;&#26816;&#32034;&#30693;&#35782;&#30340;&#65311;&#25105;&#20204;&#20851;&#27880;&#26368;&#22522;&#26412;&#30340;&#20219;&#21153;&#24418;&#24335;&#8212;&#8212;&#20107;&#23454;&#22238;&#24518;&#65292;&#27169;&#22411;&#36890;&#36807;&#22312;"&#20107;&#23454;&#65306;&#26007;&#20861;&#22330;&#20301;&#20110;&#22269;&#23478;"&#31561;&#25552;&#31034;&#20013;&#26126;&#30830;&#21576;&#29616;&#23384;&#20648;&#30340;&#20107;&#23454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20107;&#23454;&#22238;&#24518;&#32972;&#21518;&#30340;&#26426;&#21046;&#25925;&#20107;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#35201;&#22797;&#26434;&#12290;&#23427;&#21253;&#21547;&#20960;&#31181;&#19981;&#21516;&#12289;&#29420;&#31435;&#19988;&#24615;&#36136;&#19981;&#21516;&#30340;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#36890;&#36807;&#30456;&#21152;&#32452;&#21512;&#65292;&#23545;&#27491;&#30830;&#30340;&#23646;&#24615;&#36827;&#34892;&#26500;&#36896;&#24615;&#24178;&#28041;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#36890;&#29992;&#29616;&#35937;&#31216;&#20026;&#21152;&#27861;&#27169;&#24335;&#65306;&#27169;&#22411;&#36890;&#36807;&#21152;&#24635;&#22810;&#20010;&#29420;&#31435;&#30340;&#36129;&#29486;&#36827;&#34892;&#35745;&#31639;&#12290;&#27599;&#20010;&#26426;&#21046;&#30340;&#36129;&#29486;&#26412;&#36523;&#21487;&#33021;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#20294;&#24635;&#21644;&#20250;&#23545;&#27491;&#30830;&#31572;&#26696;&#20135;&#29983;&#31215;&#26497;&#30340;&#24178;&#28041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#30452;&#25509;&#23545;logit&#23646;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#19968;&#20010;&#27880;&#24847;&#22836;&#30340;&#36755;&#20986;&#24402;&#22240;&#32473;&#21333;&#29420;&#30340;&#28304;&#35760;&#21495;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#35299;&#24320;&#25105;&#20204;&#25152;&#35828;&#30340;"&#28151;&#21512;&#22836;"&#8212;&#8212;&#23427;&#20204;&#26412;&#36523;&#26159;a&#30340;&#19968;&#37096;&#20998;.
&lt;/p&gt;
&lt;p&gt;
How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#23884;&#20837;&#21644;&#20027;&#21160;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#12289;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#23454;&#39564;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#37322;&#20102;&#26032;&#39062;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.07320</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#23884;&#20837;&#23454;&#29616;&#21487;&#35299;&#37322;&#12289;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#65306;&#26032;&#39062;&#24615;&#35782;&#21035;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#26694;&#26550;&#19982;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Explainable, Safe Autonomous Driving with Language Embeddings for Novelty Identification and Active Learning: Framework and Experimental Analysis with Real-World Data Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#23884;&#20837;&#21644;&#20027;&#21160;&#23398;&#20064;&#26469;&#23454;&#29616;&#21487;&#35299;&#37322;&#12289;&#23433;&#20840;&#30340;&#33258;&#21160;&#39550;&#39542;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#23454;&#39564;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#35782;&#21035;&#21644;&#35299;&#37322;&#20102;&#26032;&#39062;&#30340;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#25968;&#25454;&#38598;&#20013;&#20351;&#29992;&#35821;&#35328;&#23884;&#20837;&#36827;&#34892;&#20027;&#21160;&#23398;&#20064;&#65292;&#37325;&#28857;&#30740;&#31350;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#26032;&#39062;&#24615;&#25351;&#30340;&#26159;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#38590;&#20197;&#24212;&#23545;&#30340;&#24847;&#22806;&#24773;&#20917;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35821;&#35328;&#30340;&#34920;&#31034;&#26469;&#35782;&#21035;&#26032;&#39062;&#22330;&#26223;&#65292;&#24378;&#35843;&#23433;&#20840;&#25509;&#31649;&#21709;&#24212;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;&#21452;&#37325;&#30446;&#30340;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#23454;&#39564;&#65292;&#23545;&#20174;&#20004;&#20010;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#38598;&#65288;&#19968;&#20010;&#23433;&#35013;&#22312;&#36710;&#36742;&#19978;&#12289;&#19968;&#20010;&#23433;&#35013;&#22312;&#22522;&#30784;&#35774;&#26045;&#19978;&#65289;&#34893;&#29983;&#30340;&#23376;&#38598;&#36827;&#34892;&#26032;&#39062;&#24615;&#26816;&#27979;&#12290;&#36890;&#36807;&#29983;&#25104;&#30340;&#32858;&#31867;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#21306;&#20998;&#34987;&#20998;&#31867;&#20026;&#26032;&#39062;&#22330;&#26223;&#21644;&#20854;&#20182;&#22330;&#26223;&#30340;&#20803;&#32032;&#30340;&#25991;&#26412;&#35299;&#37322;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#23454;&#39564;&#25968;&#25454;&#27744;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research explores the integration of language embeddings for active learning in autonomous driving datasets, with a focus on novelty detection. Novelty arises from unexpected scenarios that autonomous vehicles struggle to navigate, necessitating higher-level reasoning abilities. Our proposed method employs language-based representations to identify novel scenes, emphasizing the dual purpose of safety takeover responses and active learning. The research presents a clustering experiment using Contrastive Language-Image Pretrained (CLIP) embeddings to organize datasets and detect novelties. We find that the proposed algorithm effectively isolates novel scenes from a collection of subsets derived from two real-world driving datasets, one vehicle-mounted and one infrastructure-mounted. From the generated clusters, we further present methods for generating textual explanations of elements which differentiate scenes classified as novel from other scenes in the data pool, presenting quali
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#65292;&#38024;&#23545;&#22238;&#22797;&#38271;&#24230;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#24314;&#31435;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.07319</link><description>&lt;p&gt;
ODIN: &#33073;&#32806;&#22870;&#21169;&#32531;&#35299;RLHF&#20013;&#30340;&#40657;&#23458;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ODIN: Disentangled Reward Mitigates Hacking in RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#65292;&#38024;&#23545;&#22238;&#22797;&#38271;&#24230;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#24314;&#31435;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#21644;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;LLMs&#19978;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20986;&#29616;&#30340;&#21709;&#24212;&#38271;&#24230;&#19978;&#30340;&#22870;&#21169;&#40657;&#23458;&#38382;&#39064;&#12290;LLMs&#30340;&#26684;&#24335;&#33391;&#22909;&#20294;&#19981;&#22826;&#26377;&#29992;&#30340;&#22238;&#22797;&#24448;&#24448;&#20250;&#27450;&#39575;LLMs&#29978;&#33267;&#20154;&#31867;&#35780;&#20272;&#32773;&#20197;&#33719;&#24471;&#39640;&#20998;&#12290;&#21516;&#26679;&#30340;&#38382;&#39064;&#20063;&#23384;&#22312;&#20110;RL&#20013;&#30340;&#26576;&#20123;&#22870;&#21169;&#27169;&#22411;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#35757;&#32451;&#21644;&#35780;&#20272;&#20013;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26356;&#21487;&#38752;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#27604;&#36739;&#19981;&#21516;&#35757;&#32451;&#37197;&#32622;&#20043;&#38388;&#30340;LLM&#35780;&#20272;&#20998;&#25968;&#21644;&#36890;&#36807;&#25913;&#21464;&#35757;&#32451;&#36229;&#21442;&#25968;&#24471;&#21040;&#30340;&#21709;&#24212;&#38271;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22522;&#20110;&#36825;&#20010;&#35780;&#20272;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30740;&#31350;&#65292;&#32467;&#26524;&#25581;&#31034;&#20102;RL&#20013;&#29992;&#20110;&#20943;&#36731;&#38271;&#24230;&#20559;&#24046;&#30340;&#36229;&#21442;&#25968;&#21644;&#25216;&#24039;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#36890;&#36807;&#22312;&#20849;&#20139;&#29305;&#24449;&#34920;&#31034;&#19978;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#32447;&#24615;&#22836;&#26469;&#25913;&#36827;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#22870;&#21169;&#65292;&#19968;&#20010;&#35757;&#32451;&#26469;&#19982;&#38271;&#24230;&#30456;&#20851;&#65292;&#21478;&#19968;&#20010;&#35757;&#32451;&#26469;&#19982;&#20869;&#23481;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study the issue of reward hacking on the response length, a challenge emerging in Reinforcement Learning from Human Feedback (RLHF) on LLMs. A well-formatted, verbose but less helpful response from the LLMs can often deceive LLMs or even human evaluators to achieve high scores. The same issue also holds for some reward models in RL. To address the challenges in both training and evaluation, we establish a more reliable evaluation protocol for comparing different training configurations, which inspects the trade-off between LLM evaluation score and response length obtained by varying training hyperparameters. Based on this evaluation, we conduct large-scale studies, where the results shed insights into the efficacy of hyperparameters and tricks used in RL on mitigating length bias. We further propose to improve the reward model by jointly training two linear heads on shared feature representations to predict the rewards, one trained to correlate with length, and the oth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#20998;&#26512;&#20102;&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#36827;&#34892;&#21338;&#24328;&#26469;&#25214;&#21040;&#19968;&#31181;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.07314</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;KL&#27491;&#21017;&#21270;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#29702;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#29702;&#35770;&#23618;&#38754;&#20998;&#26512;&#20102;&#19968;&#31181;&#20851;&#20110;&#19968;&#33324;&#20559;&#22909;&#19979;&#32435;&#20160;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#36827;&#34892;&#21338;&#24328;&#26469;&#25214;&#21040;&#19968;&#31181;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20174;&#19968;&#20010;&#27010;&#29575;&#20559;&#22909;&#27169;&#22411;&#25552;&#20379;&#30340;&#20559;&#22909;&#20449;&#21495;&#20013;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20197;&#19968;&#20010;&#25552;&#31034;&#21644;&#20004;&#20010;&#21709;&#24212;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20135;&#29983;&#19968;&#20010;&#20998;&#25968;&#65292;&#34920;&#31034;&#23545;&#19968;&#20010;&#21709;&#24212;&#30456;&#23545;&#20110;&#21478;&#19968;&#20010;&#21709;&#24212;&#30340;&#20559;&#22909;&#31243;&#24230;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#26368;&#27969;&#34892;&#30340;RLHF&#33539;&#24335;&#26159;&#22522;&#20110;&#22870;&#21169;&#30340;&#65292;&#23427;&#20174;&#22870;&#21169;&#24314;&#27169;&#30340;&#21021;&#22987;&#27493;&#39588;&#24320;&#22987;&#65292;&#28982;&#21518;&#20351;&#29992;&#26500;&#24314;&#30340;&#22870;&#21169;&#20026;&#21518;&#32493;&#30340;&#22870;&#21169;&#20248;&#21270;&#38454;&#27573;&#25552;&#20379;&#22870;&#21169;&#20449;&#21495;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#20989;&#25968;&#30340;&#23384;&#22312;&#26159;&#19968;&#20010;&#24378;&#20551;&#35774;&#65292;&#22522;&#20110;&#22870;&#21169;&#30340;RLHF&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#26377;&#23616;&#38480;&#24615;&#65292;&#19981;&#33021;&#25429;&#25417;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20026;&#26368;&#36817;&#25552;&#20986;&#30340;&#23398;&#20064;&#33539;&#24335;Nash&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;NLHF&#65289;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#23519;&#21147;&#65292;&#35813;&#23398;&#20064;&#33539;&#24335;&#32771;&#34385;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#24182;&#23558;&#23545;&#40784;&#36807;&#31243;&#23450;&#20041;&#20026;&#20004;&#20010;&#31454;&#20105;&#30340;LLM&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#23398;&#20064;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#19968;&#33268;&#29983;&#25104;&#21709;&#24212;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses
&lt;/p&gt;</description></item><item><title>BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07310</link><description>&lt;p&gt;
BioNeRF: &#29992;&#20110;&#35270;&#22270;&#21512;&#25104;&#30340;&#29983;&#29289;&#21512;&#29702;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07310
&lt;/p&gt;
&lt;p&gt;
BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BioNeRF&#65292;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#32593;&#32476;&#26435;&#37325;&#26469;&#23384;&#20648;&#22330;&#26223;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;BioNeRF&#23454;&#29616;&#20102;&#19968;&#31181;&#21463;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#34701;&#21512;&#25104;&#20869;&#23384;&#31867;&#20284;&#30340;&#32467;&#26500;&#65292;&#25552;&#39640;&#23384;&#20648;&#33021;&#21147;&#24182;&#25552;&#21462;&#26356;&#22810;&#20869;&#22312;&#21644;&#30456;&#20851;&#20449;&#24687;&#12290;BioNeRF&#36824;&#27169;&#20223;&#20102;&#37329;&#23383;&#22612;&#32454;&#32990;&#20013;&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#19968;&#31181;&#34892;&#20026;&#65292;&#20854;&#20013;&#20869;&#23384;&#20316;&#20026;&#19978;&#19979;&#25991;&#25552;&#20379;&#65292;&#24182;&#19982;&#20004;&#20010;&#21518;&#32493;&#31070;&#32463;&#27169;&#22411;&#30340;&#36755;&#20837;&#30456;&#32467;&#21512;&#65292;&#19968;&#20010;&#36127;&#36131;&#29983;&#25104;&#23481;&#31215;&#23494;&#24230;&#65292;&#21478;&#19968;&#20010;&#36127;&#36131;&#28210;&#26579;&#22330;&#26223;&#30340;&#39068;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioNeRF&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07309</link><description>&lt;p&gt;
HyperBERT:&#23558;&#28151;&#21512;&#36229;&#22270;&#24863;&#30693;&#23618;&#19982;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperBERT: Mixing Hypergraph-Aware Layers with Language Models for Node Classification on Text-Attributed Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07309
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;HyperBERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#20013;&#24341;&#20837;&#36229;&#22270;&#24863;&#30693;&#23618;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#19978;&#38590;&#20197;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#21644;&#25991;&#26412;&#23646;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#36890;&#36807;&#22797;&#26434;&#30340;&#25299;&#25169;&#32467;&#26500;&#26631;&#35760;&#65292;&#34920;&#36798;&#22810;&#20010;&#23454;&#20307;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#65292;&#20854;&#20013;&#36229;&#36793;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#36229;&#22270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#23398;&#20064;&#25991;&#26412;&#23646;&#24615;&#36229;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#38590;&#20197;&#21516;&#26102;&#25429;&#25417;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#20840;&#37096;&#20869;&#23481;&#21644;&#33410;&#28857;&#23646;&#24615;&#20013;&#30340;&#20016;&#23500;&#35821;&#35328;&#23646;&#24615;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#25928;&#26524;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20026;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#36827;&#19968;&#27493;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#65292;&#24341;&#20837;&#19987;&#38376;&#30340;&#36229;&#22270;&#24863;&#30693;&#23618;&#12290;&#36825;&#20123;&#23618;&#23558;&#39640;&#38454;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#24341;&#20837;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#21033;&#29992;&#36229;&#22270;&#32467;&#26500;&#20013;&#30340;&#39640;&#38454;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#25991;&#26412;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are marked by complex topology, expressing higher-order interactions among multiple entities with hyperedges. Lately, hypergraph-based deep learning methods to learn informative data representations for the problem of node classification on text-attributed hypergraphs have garnered increasing research attention. However, existing methods struggle to simultaneously capture the full extent of hypergraph structural information and the rich linguistic attributes inherent in the nodes attributes, which largely hampers their effectiveness and generalizability. To overcome these challenges, we explore ways to further augment a pretrained BERT model with specialized hypergraph-aware layers for the task of node classification. Such layers introduce higher-order structural inductive bias into the language model, thus improving the model's capacity to harness both higher-order context information from the hypergraph structure and semantic information present in text. In this paper, we
&lt;/p&gt;</description></item><item><title>&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07307</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Conformal Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07307
&lt;/p&gt;
&lt;p&gt;
&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26082;&#31526;&#21512;&#26657;&#20934;&#30340;&#39044;&#27979;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#20026;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#19979;&#30340;&#20915;&#31574;&#20013;&#65292;&#20915;&#31574;&#32773;&#36890;&#24120;&#22312;&#20855;&#26377;&#30456;&#21516;&#39044;&#27979;&#32467;&#26524;&#30340;&#24773;&#22659;&#20013;&#37319;&#21462;&#30456;&#21516;&#30340;&#34892;&#21160;&#12290;&#31526;&#21512;&#39044;&#27979;&#24110;&#21161;&#20915;&#31574;&#32773;&#37327;&#21270;&#21160;&#20316;&#30340;&#32467;&#26524;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;&#21463;&#36825;&#31181;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#27965;&#30340;&#31526;&#21512;&#39044;&#27979;&#65292;&#23427;&#20135;&#29983;&#20102;&#26082;&#31526;&#21512;Venn-Abers&#26657;&#20934;&#30340;&#39044;&#27979;&#65292;&#21448;&#31526;&#21512;&#20197;&#27169;&#22411;&#39044;&#27979;&#24341;&#21457;&#30340;&#21160;&#20316;&#20026;&#26465;&#20214;&#30340;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21518;&#39564;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;&#39044;&#27979;&#22120;&#65292;&#25552;&#20379;&#20005;&#26684;&#30340;&#12289;&#38024;&#23545;&#20855;&#20307;&#21160;&#20316;&#30340;&#20915;&#31574;&#20445;&#35777;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21306;&#38388;&#30340;&#25928;&#29575;&#21644;&#26465;&#20214;&#30340;&#26377;&#25928;&#24615;&#20043;&#38388;&#36798;&#21040;&#20102;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making guided by machine learning, decision-makers often take identical actions in contexts with identical predicted outcomes. Conformal prediction helps decision-makers quantify outcome uncertainty for actions, allowing for better risk management. Inspired by this perspective, we introduce self-consistent conformal prediction, which yields both Venn-Abers calibrated predictions and conformal prediction intervals that are valid conditional on actions prompted by model predictions. Our procedure can be applied post-hoc to any black-box predictor to provide rigorous, action-specific decision-making guarantees. Numerical experiments show our approach strikes a balance between interval efficiency and conditional validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21644;&#32479;&#35745;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.07295</link><description>&lt;p&gt;
&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Heterogeneous Client Models using Knowledge Distillation in Serverless Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#35757;&#32451;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21644;&#32479;&#35745;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#20043;&#38388;&#33021;&#22815;&#21327;&#20316;&#35757;&#32451;&#20849;&#20139;&#30340;&#20840;&#23616;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;&#25968;&#25454;&#30340;&#21435;&#20013;&#24515;&#21270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#35774;&#35745;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#26041;&#38754;&#34920;&#26126;&#65292;&#21033;&#29992;&#26080;&#26381;&#21153;&#22120;&#35745;&#31639;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#20989;&#25968;&#21363;&#26381;&#21153;&#65288;FaaS&#65289;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#21487;&#20197;&#25552;&#39640;&#36164;&#28304;&#25928;&#29575;&#65292;&#38477;&#20302;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#20943;&#36731;&#25968;&#25454;&#25345;&#26377;&#32773;&#38754;&#20020;&#30340;&#22797;&#26434;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38544;&#24335;&#22320;&#20551;&#35774;&#25152;&#26377;&#21442;&#19982;&#23458;&#25143;&#31471;&#20855;&#26377;&#30456;&#21516;&#30340;&#20840;&#23616;&#27169;&#22411;&#26550;&#26500;&#12290;&#36825;&#19968;&#20551;&#35774;&#26410;&#33021;&#35299;&#20915;&#23454;&#38469;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#23384;&#22312;&#36164;&#28304;&#21644;&#32479;&#35745;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#22312;&#26080;&#26381;&#21153;&#22120;&#32852;&#37030;&#23398;&#20064;&#20013;&#23454;&#29616;&#24322;&#26500;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#26032;&#22411;&#20248;&#21270;&#26080;&#26381;&#21153;&#22120;&#24037;&#20316;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is an emerging machine learning paradigm that enables the collaborative training of a shared global model across distributed clients while keeping the data decentralized. Recent works on designing systems for efficient FL have shown that utilizing serverless computing technologies, particularly Function-as-a-Service (FaaS) for FL, can enhance resource efficiency, reduce training costs, and alleviate the complex infrastructure management burden on data holders. However, existing serverless FL systems implicitly assume a uniform global model architecture across all participating clients during training. This assumption fails to address fundamental challenges in practical FL due to the resource and statistical data heterogeneity among FL clients. To address these challenges and enable heterogeneous client models in serverless FL, we utilize Knowledge Distillation (KD) in this paper. Towards this, we propose novel optimized serverless workflows for two popular conve
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#39640;&#27979;&#35797;&#35206;&#30422;&#29575;&#30340;&#30495;&#23454;Java&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#35843;&#29992;&#22270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#38745;&#24577;&#35843;&#29992;&#22270;&#29983;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07294</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#35843;&#29992;&#22270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Machine Learning-based Call Graph Pruning: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#20855;&#26377;&#39640;&#27979;&#35797;&#35206;&#30422;&#29575;&#30340;&#30495;&#23454;Java&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#20102;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;&#35843;&#29992;&#22270;&#21098;&#26525;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#23545;&#27604;&#20998;&#26512;&#38745;&#24577;&#35843;&#29992;&#22270;&#29983;&#25104;&#25216;&#26415;&#26469;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38745;&#24577;&#35843;&#29992;&#22270;(CG)&#26500;&#24314;&#32463;&#24120;&#36807;&#24230;&#36817;&#20284;&#35843;&#29992;&#20851;&#31995;&#65292;&#23548;&#33268;&#32467;&#26524;&#31934;&#30830;&#20294;&#19981;&#20934;&#30830;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;(ML)&#30340;CG&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#25552;&#39640;&#31934;&#30830;&#24615;&#30340;&#25163;&#27573;&#65292;&#36890;&#36807;&#28040;&#38500;&#34394;&#20551;&#36793;&#32536;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#26377;&#38480;&#12289;&#35757;&#32451;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#20197;&#21450;&#21484;&#22238;&#29575;&#38477;&#20302;&#31561;&#38382;&#39064;&#65292;&#36825;&#24433;&#21709;&#20102;&#23454;&#38469;&#30340;&#19979;&#28216;&#20998;&#26512;&#12290;&#20043;&#21069;&#30340;&#32467;&#26524;&#20063;&#27809;&#26377;&#19982;&#20808;&#36827;&#30340;&#38745;&#24577;CG&#26500;&#24314;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#39033;&#30740;&#31350;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;NYXCorpus&#65292;&#19968;&#20010;&#20855;&#26377;&#39640;&#27979;&#35797;&#35206;&#30422;&#29575;&#30340;&#30495;&#23454;Java&#31243;&#24207;&#25968;&#25454;&#38598;&#65292;&#24182;&#20174;&#27979;&#35797;&#25191;&#34892;&#20013;&#25910;&#38598;&#36712;&#36857;&#24182;&#26500;&#24314;&#21160;&#24577;CG&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;CG&#26469;&#25506;&#32034;ML-based CG&#21098;&#26525;&#22120;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#20445;&#23432;&#21098;&#26525;&#31574;&#30053;&#12290;&#25105;&#20204;&#23545;&#38745;&#24577;CG&#20351;&#29992;&#38646;&#25511;&#21046;&#27969;&#20998;&#26512;(0-CFA)&#29983;&#25104;&#30340;&#32467;&#26524;&#21644;&#30001;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;1-CFA&#31639;&#27861;&#29983;&#25104;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Static call graph (CG) construction often over-approximates call relations, leading to sound, but imprecise results. Recent research has explored machine learning (ML)-based CG pruning as a means to enhance precision by eliminating false edges. However, current methods suffer from a limited evaluation dataset, imbalanced training data, and reduced recall, which affects practical downstream analyses. Prior results were also not compared with advanced static CG construction techniques yet. This study tackles these issues. We introduce the NYXCorpus, a dataset of real-world Java programs with high test coverage and we collect traces from test executions and build a ground truth of dynamic CGs. We leverage these CGs to explore conservative pruning strategies during the training and inference of ML-based CG pruners. We conduct a comparative analysis of static CGs generated using zero control flow analysis (0-CFA) and those produced by a context-sensitive 1-CFA algorithm, evaluating both wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21021;&#22987;&#29468;&#27979;&#30340;&#31283;&#20581;&#25968;&#25454;&#20851;&#32852;&#26041;&#27861;CLIPPER&#65292;&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#22270;&#21644;&#23547;&#25214;&#26368;&#23494;&#38598;&#30340;&#21152;&#26435;&#28857;&#22242;&#26469;&#35299;&#20915;&#25968;&#25454;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#28857;&#20113;&#27880;&#20876;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07284</link><description>&lt;p&gt;
CLIPPER&#65306;&#26080;&#38656;&#21021;&#22987;&#29468;&#27979;&#30340;&#31283;&#20581;&#25968;&#25454;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
CLIPPER: Robust Data Association without an Initial Guess
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#21021;&#22987;&#29468;&#27979;&#30340;&#31283;&#20581;&#25968;&#25454;&#20851;&#32852;&#26041;&#27861;CLIPPER&#65292;&#36890;&#36807;&#21033;&#29992;&#21152;&#26435;&#22270;&#21644;&#23547;&#25214;&#26368;&#23494;&#38598;&#30340;&#21152;&#26435;&#28857;&#22242;&#26469;&#35299;&#20915;&#25968;&#25454;&#20851;&#32852;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#28857;&#20113;&#27880;&#20876;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20272;&#35745;&#36807;&#31243;&#20013;&#65292;&#35782;&#21035;&#22122;&#22768;&#25968;&#25454;&#20013;&#30340;&#23545;&#24212;&#20851;&#31995;&#26159;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#27493;&#39588;&#12290;&#24403;&#26377;&#19968;&#20010;&#26377;&#20449;&#24687;&#30340;&#21021;&#22987;&#20272;&#35745;&#29468;&#27979;&#21487;&#29992;&#26102;&#65292;&#25968;&#25454;&#20851;&#32852;&#30340;&#25361;&#25112;&#23601;&#19981;&#37027;&#20040;&#20005;&#37325;&#65307;&#28982;&#32780;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#24456;&#23569;&#26377;&#39640;&#36136;&#37327;&#30340;&#21021;&#22987;&#29468;&#27979;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;&#22270;&#30340;&#25968;&#25454;&#20851;&#32852;&#24418;&#24335;&#65292;&#19981;&#38656;&#35201;&#21021;&#22987;&#20272;&#35745;&#29468;&#27979;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#20248;&#21270;&#26080;&#26435;&#22270;&#65292;&#20002;&#24323;&#20102;&#26435;&#37325;&#36793;&#20013;&#32534;&#30721;&#30340;&#37325;&#35201;&#19968;&#33268;&#24615;&#20449;&#24687;&#65292;&#24182;&#32463;&#24120;&#23581;&#35797;&#31934;&#30830;&#35299;&#20915;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20805;&#20998;&#21033;&#29992;&#21152;&#26435;&#22270;&#65292;&#24182;&#23547;&#27714;&#26368;&#23494;&#38598;&#30340;&#21152;&#26435;&#28857;&#22242;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#24341;&#20837;&#20102;&#20004;&#20010;&#26494;&#24347;&#26465;&#20214;&#65306;&#19968;&#20010;&#20984;&#21322;&#23450;&#26494;&#24347;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#23454;&#38469;&#20013;&#24456;&#32039;&#23494;&#65307;&#20197;&#21450;&#19968;&#20010;&#24555;&#36895;&#30340;&#19968;&#38454;&#31639;&#27861;CLIPPER&#65292;&#36890;&#24120;&#21487;&#20197;&#22312;&#27627;&#31186;&#32423;&#21035;&#20869;&#24471;&#21040;&#25509;&#36817;&#26368;&#20248;&#35299;&#12290;&#22312;&#28857;&#20113;&#27880;&#20876;&#26041;&#38754;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#20248;&#21183;&#21644;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying correspondences in noisy data is a critically important step in estimation processes. When an informative initial estimation guess is available, the data association challenge is less acute; however, the existence of a high-quality initial guess is rare in most contexts. We explore graph-theoretic formulations for data association, which do not require an initial estimation guess. Existing graph-theoretic approaches optimize over unweighted graphs, discarding important consistency information encoded in weighted edges, and frequently attempt to solve NP-hard problems exactly. In contrast, we formulate a new optimization problem that fully leverages weighted graphs and seeks the densest edge-weighted clique. We introduce two relaxations to this problem: a convex semidefinite relaxation which we find to be empirically tight, and a fast first-order algorithm called CLIPPER which frequently arrives at nearly-optimal solutions in milliseconds. When evaluated on point cloud regis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#38480;&#30340;&#30005;&#21147;&#21464;&#21387;&#22120;&#25925;&#38556;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25925;&#38556;&#39044;&#27979;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.07283</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#30005;&#21147;&#21464;&#21387;&#22120;&#25925;&#38556;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Power Transformer Fault Prediction Based on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#21644;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26377;&#38480;&#30340;&#30005;&#21147;&#21464;&#21387;&#22120;&#25925;&#38556;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#25925;&#38556;&#39044;&#27979;&#20934;&#30830;&#24230;&#19978;&#20248;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#30005;&#21147;&#21464;&#21387;&#22120;&#20165;&#26377;&#26377;&#38480;&#30340;&#25925;&#38556;&#25968;&#25454;&#36825;&#19968;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#20256;&#32479;&#30340;&#36816;&#32500;&#24037;&#20855;&#23545;&#28508;&#22312;&#25925;&#38556;&#30340;&#39044;&#27979;&#33021;&#21147;&#26377;&#38480;&#12290;&#30001;&#20110;&#25925;&#38556;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24456;&#38590;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;Knowledge Graph&#65292;KG&#65289;&#25216;&#26415;&#19982;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;Gradient Boosting Decision Trees&#65292;GBDT&#65289;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26088;&#22312;&#20174;&#23569;&#37327;&#30340;&#39640;&#32500;&#25968;&#25454;&#20013;&#39640;&#25928;&#23398;&#20064;&#65292;&#25972;&#21512;&#20102;&#24433;&#21709;&#21464;&#21387;&#22120;&#25925;&#38556;&#30340;&#21508;&#31181;&#22240;&#32032;&#21644;&#21382;&#21490;&#36816;&#34892;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#25925;&#38556;&#29305;&#24449;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#23545;&#30005;&#21147;&#21464;&#21387;&#22120;&#30340;&#20934;&#30830;&#23433;&#20840;&#29366;&#24577;&#35780;&#20272;&#21644;&#25925;&#38556;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;Artificial Neural Networks&#65292;ANN&#65289;&#21644;&#36923;&#36753;&#22238;&#24402;&#65288;Logistic Regression&#65292;LR&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we address the challenge of learning with limited fault data for power transformers. Traditional operation and maintenance tools lack effective predictive capabilities for potential faults. The scarcity of extensive fault data makes it difficult to apply machine learning techniques effectively. To solve this problem, we propose a novel approach that leverages the knowledge graph (KG) technology in combination with gradient boosting decision trees (GBDT). This method is designed to efficiently learn from a small set of high-dimensional data, integrating various factors influencing transformer faults and historical operational data. Our approach enables accurate safe state assessments and fault analyses of power transformers despite the limited fault characteristic data. Experimental results demonstrate that this method outperforms other learning approaches in prediction accuracy, such as artificial neural networks (ANN) and logistic regression (LR). Furthermore, it offers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;</title><link>https://arxiv.org/abs/2402.07282</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35802;&#23454;&#19982;&#24110;&#21161;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26435;&#34913;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#21017;&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;GPT-4 Turbo&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#38646;-shot&#25552;&#31034;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24341;&#23548;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#24110;&#21161;&#21548;&#20247;&#32780;&#36817;&#20284;&#30495;&#30456;&#65292;&#20363;&#22914;&#32422;&#30053;&#26102;&#38388;&#25110;&#30465;&#30053;&#32454;&#33410;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#22788;&#29702;&#36825;&#31181;&#24494;&#22937;&#30340;&#26435;&#34913;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#24515;&#29702;&#27169;&#22411;&#21644;&#26088;&#22312;&#25551;&#36848;&#20154;&#31867;&#34892;&#20026;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;LLMs&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19968;&#31995;&#21015;LLMs&#65292;&#24182;&#25506;&#35752;&#20102;&#20248;&#21270;&#20154;&#31867;&#20559;&#22909;&#25110;&#25512;&#29702;&#26102;&#24605;&#32771;&#23545;&#36825;&#20123;&#26435;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#25913;&#21892;&#20102;&#35802;&#23454;&#21644;&#24110;&#21161;&#24615;&#65292;&#32780;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#20351;LLMs&#20559;&#21521;&#20110;&#24110;&#21161;&#24615;&#32780;&#19981;&#26159;&#35802;&#23454;&#12290;&#26368;&#21518;&#65292;GPT-4 Turbo&#23637;&#31034;&#20102;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#23545;&#23545;&#35805;&#26694;&#26550;&#21644;&#21548;&#20247;&#20915;&#31574;&#32972;&#26223;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#20869;&#21270;&#30340;&#23545;&#35805;&#20215;&#20540;&#35266;&#65292;&#24182;&#26263;&#31034;&#21363;&#20351;&#36825;&#20123;&#25277;&#35937;&#20215;&#20540;&#35266;&#20063;&#21487;&#20197;&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#34987;&#24341;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
In day-to-day communication, people often approximate the truth - for example, rounding the time or omitting details - in order to be maximally helpful to the listener. How do large language models (LLMs) handle such nuanced trade-offs? To address this question, we use psychological models and experiments designed to characterize human behavior to analyze LLMs. We test a range of LLMs and explore how optimization for human preferences or inference-time reasoning affects these trade-offs. We find that reinforcement learning from human feedback improves both honesty and helpfulness, while chain-of-thought prompting skews LLMs towards helpfulness over honesty. Finally, GPT-4 Turbo demonstrates human-like response patterns including sensitivity to the conversational framing and listener's decision context. Our findings reveal the conversational values internalized by LLMs and suggest that even these abstract values can, to a degree, be steered by zero-shot prompting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#21253;&#25324;&#26641;&#32467;&#26500;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;</title><link>https://arxiv.org/abs/2402.07281</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#26641;&#32467;&#26500;&#26041;&#27861;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21542;&#36229;&#36234;&#28145;&#24230;&#23398;&#20064;&#65311;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#21253;&#25324;&#26641;&#32467;&#26500;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30830;&#20445;&#26381;&#21153;&#36830;&#32493;&#24615;&#26102;&#65292;&#22797;&#26434;&#30340;&#20851;&#38190;&#20219;&#21153;&#31995;&#32479;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#24322;&#24120;&#20107;&#20214;&#34987;&#35748;&#20026;&#26159;&#32597;&#35265;&#20107;&#20214;&#65292;&#22240;&#27492;&#20174;&#25805;&#20316;&#25968;&#25454;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#38754;&#20020;&#30528;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#35770;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#27604;&#36739;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#21508;&#31181;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#24322;&#24120;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#20351;&#29992;&#20102;104&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#23569;&#25968;&#19987;&#26377;&#30340;&#24037;&#19994;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#22686;&#24378;&#20102;&#30740;&#31350;&#30340;&#22810;&#26679;&#24615;&#65292;&#20351;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#26356;&#21152;&#30495;&#23454;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23454;&#38469;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#35770;&#25991;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.07270</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20854;&#35821;&#20041;&#23618;&#27425;&#65292;&#24320;&#23637;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;VQA&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#25512;&#21160;&#20102;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#35821;&#20041;&#23618;&#27425;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#26469;&#25913;&#36827;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#24230;&#37327;&#26631;&#20934;&#65292;&#20182;&#20204;&#22312;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#30340;&#22522;&#30784;&#19978;&#36873;&#25321;&#20102;&#26368;&#32456;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#24037;&#20316;&#12290;&#36890;&#36807;&#35299;&#20915;&#29616;&#26377;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#22522;&#20934;&#30340;&#23616;&#38480;&#24615;&#24182;&#25552;&#20986;&#21019;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#25105;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#21517;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;VQA&#22522;&#20934;&#65292;&#21487;&#20197;&#23545;&#25991;&#26412;&#29983;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#35780;&#20272;&#65292;&#24182;&#19982;&#21028;&#21035;&#24615;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#19978;&#31895;&#31961;&#31572;&#26696;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26631;&#31614;&#31354;&#38388;&#30340;&#35821;&#20041;&#23618;&#27425;&#26469;&#25552;&#20986;&#20851;&#20110;&#22522;&#20934;&#31867;&#21035;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#21518;&#32493;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22522;&#20110;LLM&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#32473;&#23450;&#22522;&#20934;&#31572;&#26696;&#30340;&#27169;&#22411;&#39044;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#65292;&#22522;&#20110;&#27492;&#20915;&#23450;&#26368;&#32456;&#24230;&#37327;&#26631;&#20934;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of text-generative vision-language models is a challenging yet crucial endeavor. By addressing the limitations of existing Visual Question Answering (VQA) benchmarks and proposing innovative evaluation methodologies, our research seeks to advance our understanding of these models' capabilities. We propose a novel VQA benchmark based on well-known visual classification datasets which allows a granular evaluation of text-generative vision-language models and their comparison with discriminative vision-language models. To improve the assessment of coarse answers on fine-grained classification tasks, we suggest using the semantic hierarchy of the label space to ask automatically generated follow-up questions about the ground-truth category. Finally, we compare traditional NLP and LLM-based metrics for the problem of evaluating model predictions given ground-truth answers. We perform a human evaluation study upon which we base our decision on the final metric. We apply our be
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21517;&#20026;PathFormer&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#26550;&#26500;&#65292;&#31995;&#32479;&#25972;&#21512;&#20449;&#21495;&#32593;&#32476;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#32452;&#23398;&#25968;&#25454;&#26469;&#23454;&#29616;&#39640;&#31934;&#30830;&#24230;&#30142;&#30149;&#35786;&#26029;&#21644;&#39640;&#21487;&#37325;&#22797;&#24615;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.07268</link><description>&lt;p&gt;
&#20351;&#29992;PathFormer&#36827;&#34892;&#39640;&#31934;&#30830;&#24230;&#30142;&#30149;&#35786;&#26029;&#21644;&#39640;&#21487;&#37325;&#22797;&#24615;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Highly Accurate Disease Diagnosis and Highly Reproducible Biomarker Identification with PathFormer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07268
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21517;&#20026;PathFormer&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#26550;&#26500;&#65292;&#31995;&#32479;&#25972;&#21512;&#20449;&#21495;&#32593;&#32476;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#32452;&#23398;&#25968;&#25454;&#26469;&#23454;&#29616;&#39640;&#31934;&#30830;&#24230;&#30142;&#30149;&#35786;&#26029;&#21644;&#39640;&#21487;&#37325;&#22797;&#24615;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#35782;&#21035;&#23545;&#20110;&#31934;&#30830;&#30340;&#30142;&#30149;&#35786;&#26029;&#21644;&#29702;&#35299;&#30142;&#30149;&#21457;&#30149;&#26426;&#21046;&#22312;&#32452;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#27604;&#22914;&#20351;&#29992;&#25240;&#21472;&#21464;&#21270;&#21644;&#22238;&#24402;&#20998;&#26512;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#20998;&#26512;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20027;&#35201;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;GNNs&#22312;&#32452;&#23398;&#25968;&#25454;&#20998;&#26512;&#20013;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#65292;&#21363;&#39044;&#27979;&#65288;&#35786;&#26029;&#65289;&#20934;&#30830;&#24615;&#26377;&#38480;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#21487;&#37325;&#22797;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#35782;&#21035;&#33021;&#21147;&#26377;&#38480;&#12290;&#36825;&#20123;&#25361;&#25112;&#30340;&#26681;&#28304;&#22312;&#20110;&#29983;&#29289;&#20449;&#21495;&#36890;&#36335;&#30340;&#29420;&#29305;&#22270;&#32467;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#30340;&#38774;&#28857;&#21644;&#36825;&#20123;&#38774;&#28857;&#20043;&#38388;&#23494;&#38598;&#32780;&#22797;&#26434;&#30340;&#20449;&#21495;&#20132;&#20114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PathFormer&#30340;&#26032;&#22411;GNN&#27169;&#22411;&#26550;&#26500;&#65292;&#23427;&#31995;&#32479;&#22320;&#25972;&#21512;&#20449;&#21495;&#32593;&#32476;&#12289;&#20808;&#39564;&#30693;&#35782;&#21644;&#32452;&#23398;&#25968;&#25454;&#26469;&#23545;&#29983;&#29289;&#26631;&#24535;&#29289;&#36827;&#34892;&#25490;&#21517;&#21644;&#39044;&#27979;&#30142;&#30149;&#35786;&#26029;&#12290;&#22312;&#27604;&#36739;&#32467;&#26524;&#20013;&#65292;PathFormer&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomarker identification is critical for precise disease diagnosis and understanding disease pathogenesis in omics data analysis, like using fold change and regression analysis. Graph neural networks (GNNs) have been the dominant deep learning model for analyzing graph-structured data. However, we found two major limitations of existing GNNs in omics data analysis, i.e., limited-prediction (diagnosis) accuracy and limited-reproducible biomarker identification capacity across multiple datasets. The root of the challenges is the unique graph structure of biological signaling pathways, which consists of a large number of targets and intensive and complex signaling interactions among these targets. To resolve these two challenges, in this study, we presented a novel GNN model architecture, named PathFormer, which systematically integrate signaling network, priori knowledge and omics data to rank biomarkers and predict disease diagnosis. In the comparison results, PathFormer outperformed ex
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;KKT-hPINN&#65289;&#65292;&#36890;&#36807;&#20174;KKT&#26465;&#20214;&#23548;&#20986;&#30340;&#25237;&#24433;&#23618;&#65292;&#20005;&#26684;&#20445;&#35777;&#30828;&#32447;&#24615;&#31561;&#24335;&#32422;&#26463;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07251</link><description>&lt;p&gt;
&#20855;&#26377;&#30828;&#32447;&#24615;&#31561;&#24335;&#32422;&#26463;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks with Hard Linear Equality Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;KKT-hPINN&#65289;&#65292;&#36890;&#36807;&#20174;KKT&#26465;&#20214;&#23548;&#20986;&#30340;&#25237;&#24433;&#23618;&#65292;&#20005;&#26684;&#20445;&#35777;&#30828;&#32447;&#24615;&#31561;&#24335;&#32422;&#26463;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#27169;&#22411;&#29992;&#26469;&#26367;&#20195;&#35745;&#31639;&#25104;&#26412;&#26114;&#36149;&#30340;&#27169;&#25311;&#12290;&#31070;&#32463;&#32593;&#32476;&#34987;&#24191;&#27867;&#24212;&#29992;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#35780;&#20272;&#22797;&#26434;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#31070;&#32463;&#32593;&#32476;&#26159;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#65292;&#27809;&#26377;&#20219;&#20309;&#29289;&#29702;&#30693;&#35782;&#12290;&#23558;&#29289;&#29702;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#20013;&#24050;&#30693;&#29289;&#29702;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20294;&#26080;&#27861;&#20005;&#26684;&#28385;&#36275;&#39044;&#27979;&#20013;&#30340;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65292;KKT-hPINN&#65292;&#36890;&#36807;&#20174;KKT&#26465;&#20214;&#23548;&#20986;&#30340;&#25237;&#24433;&#23618;&#20005;&#26684;&#20445;&#35777;&#30828;&#32447;&#24615;&#31561;&#24335;&#32422;&#26463;&#12290;&#22312;&#36830;&#32493;&#25605;&#25292;&#27133;&#21453;&#24212;&#22120;&#65288;CSTR&#65289;&#21333;&#20803;&#12289;&#33795;&#21462;&#31934;&#39311;&#23376;&#31995;&#32479;&#21644;&#21270;&#24037;&#21378;&#30340;Aspen&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surrogate modeling is used to replace computationally expensive simulations. Neural networks have been widely applied as surrogate models that enable efficient evaluations over complex physical systems. Despite this, neural networks are data-driven models and devoid of any physics. The incorporation of physics into neural networks can improve generalization and data efficiency. The physics-informed neural network (PINN) is an approach to leverage known physical constraints present in the data, but it cannot strictly satisfy them in the predictions. This work proposes a novel physics-informed neural network, KKT-hPINN, which rigorously guarantees hard linear equality constraints through projection layers derived from KKT conditions. Numerical experiments on Aspen models of a continuous stirred-tank reactor (CSTR) unit, an extractive distillation subsystem, and a chemical plant demonstrate that this model can further enhance the prediction accuracy.
&lt;/p&gt;</description></item><item><title>DIMON&#26159;&#19968;&#20010;&#23398;&#20064;&#22312;&#19968;&#31995;&#21015;&#21464;&#24418;&#30340;&#22495;&#19978;&#35299;&#31639;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36890;&#29992;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21442;&#32771;&#22495;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#20064;&#35299;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#37325;&#26032;&#26144;&#23556;&#22238;&#21407;&#22987;&#22495;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#22495;&#19978;&#21464;&#21270;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#19979;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#36817;&#20284;&#12290;</title><link>https://arxiv.org/abs/2402.07250</link><description>&lt;p&gt;
DIMON:&#22312;&#19968;&#31995;&#21015;&#21464;&#24418;&#30340;&#22495;&#19978;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#31639;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
DIMON: Learning Solution Operators of Partial Differential Equations on a Diffeomorphic Family of Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07250
&lt;/p&gt;
&lt;p&gt;
DIMON&#26159;&#19968;&#20010;&#23398;&#20064;&#22312;&#19968;&#31995;&#21015;&#21464;&#24418;&#30340;&#22495;&#19978;&#35299;&#31639;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#36890;&#29992;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21442;&#32771;&#22495;&#35757;&#32451;&#25968;&#25454;&#19978;&#23398;&#20064;&#35299;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#23558;&#20854;&#37325;&#26032;&#26144;&#23556;&#22238;&#21407;&#22987;&#22495;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#22495;&#19978;&#21464;&#21270;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#19979;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#20010;&#22495;&#19978;&#21464;&#21270;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#19979;&#27714;&#35299;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#37117;&#26159;&#38656;&#35201;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27599;&#27425;&#22495;&#30340;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#21464;&#21270;&#26102;&#37117;&#37325;&#26032;&#35745;&#31639;&#35299;&#65292;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#31639;&#23376;&#23398;&#20064;&#26694;&#26550;&#65292;&#31216;&#20026;DIffeomorphic Mapping Operator LearNing&#65288;DIMON&#65289;&#65292;&#29992;&#26469;&#23398;&#20064;&#35299;&#22312;&#22495;&#26063;$\{\Omega_{\theta}}_\theta$&#19978;&#30340;&#36817;&#20284;&#35299;&#65292;&#23427;&#23398;&#20064;&#20174;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#21644;&#22495;$\Omega_\theta$&#21040;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65288;&#25110;&#25351;&#23450;&#30340;&#20989;&#25968;&#65289;&#30340;&#26144;&#23556;&#12290;DIMON&#22522;&#20110;&#23558;&#32473;&#23450;&#38382;&#39064;&#65288;&#21021;&#22987;/&#36793;&#30028;&#26465;&#20214;&#21644;&#22495;$\Omega_{\theta}$&#65289;&#36716;&#31227;&#21040;&#19968;&#20010;&#21442;&#32771;&#22495;$\Omega_{0}$&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#20854;&#20013;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#38382;&#39064;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#21040;&#22312;$\Omega_{0}$&#19978;&#30340;&#35299;&#30340;&#26144;&#23556;&#65292;&#28982;&#21518;&#20877;&#23558;&#20854;&#37325;&#26032;&#26144;&#23556;&#22238;&#21407;&#22987;&#22495;$\Omega_{\theta}$&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#38382;&#39064;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The solution of a PDE over varying initial/boundary conditions on multiple domains is needed in a wide variety of applications, but it is computationally expensive if the solution is computed de novo whenever the initial/boundary conditions of the domain change. We introduce a general operator learning framework, called DIffeomorphic Mapping Operator learNing (DIMON) to learn approximate PDE solutions over a family of domains $\{\Omega_{\theta}}_\theta$, that learns the map from initial/boundary conditions and domain $\Omega_\theta$ to the solution of the PDE, or to specified functionals thereof. DIMON is based on transporting a given problem (initial/boundary conditions and domain $\Omega_{\theta}$) to a problem on a reference domain $\Omega_{0}$, where training data from multiple problems is used to learn the map to the solution on $\Omega_{0}$, which is then re-mapped to the original domain $\Omega_{\theta}$. We consider several problems to demonstrate the performance of the framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#65292;&#21457;&#29616;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07249</link><description>&lt;p&gt;
&#39046;&#22495;&#30693;&#35782;&#21644;&#22810;&#27169;&#24577;&#23545;&#26234;&#33021;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#24433;&#21709;&#65306;&#19968;&#39033;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;&#65292;&#21457;&#29616;&#25972;&#21512;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#25552;&#39640;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#21487;&#20197;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#23545;&#20110;&#33647;&#29289;&#24320;&#21457;&#23588;&#20854;&#26159;&#34394;&#25311;&#31579;&#36873;&#21644;&#21270;&#21512;&#29289;&#20248;&#21270;&#30340;&#36827;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#24341;&#20837;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#22686;&#24378;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#65288;MPP&#65289;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#23545;&#20998;&#23376;&#32467;&#26500;&#30340;&#27934;&#23519;&#21147;&#12290;&#28982;&#32780;&#65292;&#26377;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#39046;&#22495;&#30693;&#35782;&#30340;&#25972;&#21512;&#26159;&#21542;&#22686;&#24378;&#20102;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#26159;&#21542;&#27604;&#21333;&#19968;&#25968;&#25454;&#26469;&#28304;&#26041;&#27861;&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#32467;&#26524;&#65311;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#21644;&#23450;&#37327;&#20998;&#26512;&#20102;&#22522;&#20110;&#21508;&#31181;&#22522;&#20934;&#30340;&#26368;&#26032;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25972;&#21512;&#20998;&#23376;&#20449;&#24687;&#23558;&#20998;&#21035;&#25552;&#39640;MPP&#22238;&#24402;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#20998;&#21035;&#39640;&#36798;3.98&#65285;&#21644;1.72&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#20351;&#29992;&#19977;&#32500;&#20449;&#24687;&#19982;&#19968;&#32500;&#21644;&#20108;&#32500;&#20449;&#24687;&#30456;&#32467;&#21512;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional informati
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;Lipschitz&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#20998;&#31163;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#20063;&#20250;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#23384;&#22312;&#65292;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#12290;&#36825;&#20026;&#20197;&#21069;&#30830;&#23450;&#28145;&#24230;&#35201;&#27714;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.07248</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28145;&#24230;&#20998;&#31163;&#65306;&#23558;&#32500;&#24230;&#19982;&#20934;&#30830;&#24230;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Depth Separations in Neural Networks: Separating the Dimension from the Accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;Lipschitz&#30446;&#26631;&#20989;&#25968;&#26102;&#30340;&#20998;&#31163;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#20063;&#20250;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#23384;&#22312;&#65292;&#21363;&#20351;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#12290;&#36825;&#20026;&#20197;&#21069;&#30830;&#23450;&#28145;&#24230;&#35201;&#27714;&#30340;&#19979;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22810;&#31181;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#31070;&#32463;&#32593;&#32476;&#22312;&#36924;&#36817;&#19968;&#20010;$\mathcal{O}(1)$-Lipschitz&#30446;&#26631;&#20989;&#25968;&#33267;&#24120;&#25968;&#31934;&#24230;&#26102;&#30340;&#25351;&#25968;&#20998;&#31163;&#65292;&#23545;&#20110;&#25903;&#25345;&#22312;$[0,1]^{d}$&#19978;&#30340;&#20998;&#24067;&#65292;&#20551;&#35774;&#26435;&#37325;&#25351;&#25968;&#26377;&#30028;&#12290;&#36825;&#35299;&#20915;&#20102;&#22312;\citet{safran2019depth}&#20013;&#25552;&#20986;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#32500;&#24230;&#35781;&#21650;&#22312;&#28145;&#24230;2&#36924;&#36817;&#20013;&#30340;&#23384;&#22312;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#20989;&#25968;&#21487;&#20197;&#20351;&#29992;&#28145;&#24230;3&#39640;&#25928;&#34920;&#31034;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#20197;&#21069;&#65292;&#23558;&#28145;&#24230;2&#21644;&#28145;&#24230;3&#20998;&#31163;&#30340;&#19979;&#30028;&#35201;&#27714;&#33267;&#23569;&#26377;&#19968;&#20010;Lipschitz&#21442;&#25968;&#12289;&#30446;&#26631;&#20934;&#30830;&#24230;&#25110;&#36924;&#36817;&#22495;&#30340;&#22823;&#23567;&#65288;&#26576;&#31181;&#24230;&#37327;&#65289;&#19982;&#36755;&#20837;&#32500;&#24230;&#22810;&#39033;&#24335;&#22320;&#32553;&#25918;&#65292;&#32780;&#25105;&#20204;&#20445;&#25345;&#21069;&#20004;&#32773;&#19981;&#21464;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#22495;&#38480;&#21046;&#22312;&#21333;&#20301;&#36229;&#31435;&#26041;&#20307;&#19978;&#12290;&#25105;&#20204;&#30340;&#19979;&#30028;&#36866;&#29992;&#20110;&#21508;&#31181;&#28608;&#27963;&#20989;&#25968;&#65292;&#24182;&#22522;&#20110;&#19968;&#31181;&#26032;&#30340;&#24179;&#22343;&#24773;&#20917;&#21040;&#26368;&#22351;&#24773;&#20917;&#30340;&#38543;&#26426;&#33258;&#32422;&#21270;&#35770;&#35777;&#30340;&#24212;&#29992;&#65292;&#20197;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
We prove an exponential separation between depth 2 and depth 3 neural networks, when approximating an $\mathcal{O}(1)$-Lipschitz target function to constant accuracy, with respect to a distribution with support in $[0,1]^{d}$, assuming exponentially bounded weights. This addresses an open problem posed in \citet{safran2019depth}, and proves that the curse of dimensionality manifests in depth 2 approximation, even in cases where the target function can be represented efficiently using depth 3. Previously, lower bounds that were used to separate depth 2 from depth 3 required that at least one of the Lipschitz parameter, target accuracy or (some measure of) the size of the domain of approximation scale polynomially with the input dimension, whereas we fix the former two and restrict our domain to the unit hypercube. Our lower bound holds for a wide variety of activation functions, and is based on a novel application of an average- to worst-case random self-reducibility argument, to reduce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;GIRL&#65289;&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35266;&#23519;&#31574;&#30053;&#19982;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#24046;&#24322;&#20197;&#21450;&#19981;&#23436;&#20840;&#21487;&#35266;&#23519;&#24773;&#20917;&#19979;&#25968;&#23398;&#34920;&#36848;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07246</link><description>&lt;p&gt;
&#26397;&#30528;&#24191;&#20041;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Towards Generalized Inverse Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#20041;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;GIRL&#65289;&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#35266;&#23519;&#31574;&#30053;&#19982;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#24046;&#24322;&#20197;&#21450;&#19981;&#23436;&#20840;&#21487;&#35266;&#23519;&#24773;&#20917;&#19979;&#25968;&#23398;&#34920;&#36848;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#20013;&#30340;&#24191;&#20041;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;GIRL&#65289;&#65292;&#21363;&#22312;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#65288;&#31574;&#30053;&#65289;&#21487;&#33021;&#19981;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;MDP&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#19981;&#20165;&#21253;&#25324;&#22870;&#21169;&#20989;&#25968;&#21644;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#65292;&#36824;&#21253;&#25324;&#21160;&#20316;&#31354;&#38388;&#21644;&#29366;&#24577;&#31354;&#38388;&#65292;&#34429;&#28982;&#19981;&#23436;&#20840;&#24050;&#30693;&#20294;&#30693;&#36947;&#23646;&#20110;&#32473;&#23450;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;GIRL&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#38656;&#35201;&#37327;&#21270;&#35266;&#23519;&#31574;&#30053;&#19982;&#28508;&#22312;&#26368;&#20248;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#20854;&#27425;&#65292;&#24403;MDP&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#19981;&#21487;&#35266;&#23519;&#25110;&#37096;&#20998;&#21487;&#35266;&#23519;&#26102;&#65292;&#25968;&#23398;&#19978;&#25551;&#36848;&#28508;&#22312;&#26368;&#20248;&#31574;&#30053;&#30340;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIRL&#30340;&#25968;&#23398;&#20844;&#24335;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#26080;&#38480;&#29366;&#24577;&#38382;&#39064;&#19978;&#30340;&#25968;&#20540;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#20844;&#24335;&#21644;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies generalized inverse reinforcement learning (GIRL) in Markov decision processes (MDPs), that is, the problem of learning the basic components of an MDP given observed behavior (policy) that might not be optimal. These components include not only the reward function and transition probability matrices, but also the action space and state space that are not exactly known but are known to belong to given uncertainty sets. We address two key challenges in GIRL: first, the need to quantify the discrepancy between the observed policy and the underlying optimal policy; second, the difficulty of mathematically characterizing the underlying optimal policy when the basic components of an MDP are unobservable or partially observable. Then, we propose the mathematical formulation for GIRL and develop a fast heuristic algorithm. Numerical results on both finite and infinite state problems show the merit of our formulation and algorithm.
&lt;/p&gt;</description></item><item><title>GenSTL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#29983;&#25104;&#29305;&#24449;&#22495;&#26469;&#23454;&#29616;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2402.07232</link><description>&lt;p&gt;
GenSTL: &#36890;&#36807;&#29305;&#24449;&#22495;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#23454;&#29616;&#36890;&#29992;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GenSTL: General Sparse Trajectory Learning via Auto-regressive Generation of Feature Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07232
&lt;/p&gt;
&lt;p&gt;
GenSTL&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#29983;&#25104;&#29305;&#24449;&#22495;&#26469;&#23454;&#29616;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36712;&#36857;&#26159;&#26102;&#38388;&#25139;&#20301;&#32622;&#26679;&#26412;&#30340;&#24207;&#21015;&#12290;&#22312;&#31232;&#30095;&#36712;&#36857;&#20013;&#65292;&#20301;&#32622;&#26679;&#26412;&#30340;&#37319;&#26679;&#26159;&#19981;&#39057;&#32321;&#30340;&#65307;&#23613;&#31649;&#36825;&#31181;&#36712;&#36857;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#35201;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#19982;&#20132;&#36890;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#36712;&#36857;&#26159;&#23494;&#38598;&#37319;&#26679;&#30340;&#24182;&#19988;&#32463;&#36807;&#20934;&#30830;&#30340;&#22320;&#22270;&#21305;&#37197;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20004;&#38454;&#27573;&#26041;&#26696;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#25193;&#23637;&#31232;&#30095;&#36712;&#36857;&#30340;&#25928;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31232;&#30095;&#36712;&#36857;&#23398;&#20064;&#26694;&#26550;GenSTL&#12290;&#35813;&#26694;&#26550;&#32463;&#36807;&#39044;&#35757;&#32451;&#20197;&#20351;&#29992;&#29305;&#24449;&#22495;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#24418;&#25104;&#31232;&#30095;&#36712;&#36857;&#19982;&#23494;&#38598;&#36712;&#36857;&#20043;&#38388;&#30340;&#36830;&#25509;&#12290;GenSTL&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#25110;&#32773;&#21487;&#20197;&#20808;&#36827;&#34892;&#24494;&#35843;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GenSTL&#28040;&#38500;&#20102;&#23545;&#22823;&#35268;&#27169;&#23494;&#38598;&#21644;&#22320;&#22270;&#21305;&#37197;&#36712;&#36857;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;&#20854;&#20013;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#29305;&#24449;&#22495;&#32534;&#30721;&#23618;&#21644;&#20998;&#23618;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Trajectories are sequences of timestamped location samples. In sparse trajectories, the locations are sampled infrequently; and while such trajectories are prevalent in real-world settings, they are challenging to use to enable high-quality transportation-related applications. Current methodologies either assume densely sampled and accurately map-matched trajectories, or they rely on two-stage schemes, yielding sub-optimal applications.   To extend the utility of sparse trajectories, we propose a novel sparse trajectory learning framework, GenSTL. The framework is pre-trained to form connections between sparse trajectories and dense counterparts using auto-regressive generation of feature domains. GenSTL can subsequently be applied directly in downstream tasks, or it can be fine-tuned first. This way, GenSTL eliminates the reliance on the availability of large-scale dense and map-matched trajectory data. The inclusion of a well-crafted feature domain encoding layer and a hierarchical m
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#37325;&#26032;&#24605;&#32771;&#22270;&#24418;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#23545;&#22270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;GraphMAE&#20013;&#30340;&#33410;&#28857;&#32423;&#37325;&#26500;&#30446;&#26631;&#23454;&#38469;&#19978;&#25191;&#34892;&#20102;&#19978;&#19979;&#25991;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#25351;&#20986;&#20102;GraphMAE&#22312;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07225</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#37325;&#26032;&#24605;&#32771;&#22270;&#24418;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Rethinking Graph Masked Autoencoders through Alignment and Uniformity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#37325;&#26032;&#24605;&#32771;&#22270;&#24418;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#23545;&#22270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;GraphMAE&#20013;&#30340;&#33410;&#28857;&#32423;&#37325;&#26500;&#30446;&#26631;&#23454;&#38469;&#19978;&#25191;&#34892;&#20102;&#19978;&#19979;&#25991;&#32423;&#23545;&#27604;&#23398;&#20064;&#65292;&#24182;&#25351;&#20986;&#20102;GraphMAE&#22312;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22270;&#20013;&#21487;&#20197;&#20998;&#20026;&#23545;&#27604;&#21644;&#29983;&#25104;&#20004;&#31181;&#26041;&#27861;&#12290;&#36807;&#21435;&#20960;&#24180;&#65292;&#23545;&#27604;&#26041;&#27861;&#65292;&#20063;&#34987;&#31216;&#20026;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#65292;&#22312;&#22270;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#26368;&#36817;&#20986;&#29616;&#30340;&#22270;&#24418;&#36974;&#32617;&#33258;&#32534;&#30721;&#22120;&#65288;GraphMAE&#65289;&#37325;&#26032;&#28857;&#29123;&#20102;&#29983;&#25104;&#26041;&#27861;&#30340;&#21160;&#21147;&#12290;&#23613;&#31649;GraphMAE&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#26377;&#25928;&#24615;&#20173;&#32570;&#20047;&#29702;&#35770;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#21644;&#23545;&#27604;&#26041;&#27861;&#37117;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#32852;&#31995;&#21644;&#24046;&#24322;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;GraphMAE&#21644;GCL&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#24182;&#35777;&#26126;&#20102;GraphMAE&#20013;&#30340;&#33410;&#28857;&#32423;&#37325;&#26500;&#30446;&#26631;&#38544;&#21547;&#22320;&#25191;&#34892;&#20102;&#19978;&#19979;&#25991;&#32423;GCL&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20174;&#23545;&#40784;&#21644;&#19968;&#33268;&#24615;&#30340;&#35282;&#24230;&#35782;&#21035;&#20102;GraphMAE&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#34987;&#35748;&#20026;&#26159;&#39640;&#36136;&#37327;&#22270;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning on graphs can be bifurcated into contrastive and generative methods. Contrastive methods, also known as graph contrastive learning (GCL), have dominated graph self-supervised learning in the past few years, but the recent advent of graph masked autoencoder (GraphMAE) rekindles the momentum behind generative methods. Despite the empirical success of GraphMAE, there is still a dearth of theoretical understanding regarding its efficacy. Moreover, while both generative and contrastive methods have been shown to be effective, their connections and differences have yet to be thoroughly investigated. Therefore, we theoretically build a bridge between GraphMAE and GCL, and prove that the node-level reconstruction objective in GraphMAE implicitly performs context-level GCL. Based on our theoretical analysis, we further identify the limitations of the GraphMAE from the perspectives of alignment and uniformity, which have been considered as two key properties of high-qual
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20998;&#35010;&#31215;&#20998;&#22120;&#36827;&#34892;&#21407;&#21017;&#24615;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;100&#27425;&#32593;&#32476;&#20989;&#25968;&#35780;&#20272;&#19979;&#30340;FID&#20998;&#25968;&#20026;2.36&#12290;</title><link>https://arxiv.org/abs/2402.07211</link><description>&lt;p&gt;
&#38754;&#21521;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Fast Stochastic Sampling in Diffusion Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20998;&#35010;&#31215;&#20998;&#22120;&#36827;&#34892;&#21407;&#21017;&#24615;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;100&#27425;&#32593;&#32476;&#20989;&#25968;&#35780;&#20272;&#19979;&#30340;FID&#20998;&#25968;&#20026;2.36&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#29983;&#25104;&#26679;&#26412;&#30340;&#36895;&#24230;&#36739;&#24930;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#19968;&#20123;&#21162;&#21147;&#22312;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#37319;&#26679;&#25928;&#29575;&#65292;&#20294;&#20173;&#28982;&#26377;&#24453;&#25913;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#35010;&#31215;&#20998;&#22120;&#30340;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#24555;&#36895;&#38543;&#26426;&#37319;&#26679;&#26041;&#27861;&#12290;&#20998;&#35010;&#31215;&#20998;&#22120;&#36890;&#24120;&#22312;&#20998;&#23376;&#21160;&#21147;&#23398;&#20013;&#20351;&#29992;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#22312;&#28041;&#21450;&#25968;&#25454;&#12289;&#36741;&#21161;&#25110;&#22122;&#22768;&#21464;&#37327;&#30340;&#25968;&#20540;&#26356;&#26032;&#20043;&#38388;&#20132;&#26367;&#26469;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#24555;&#36895;&#37319;&#26679;&#65292;&#31616;&#21333;&#24212;&#29992;&#20998;&#35010;&#31215;&#20998;&#22120;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#21407;&#21017;&#19978;&#20462;&#25913;&#20102;&#31616;&#21333;&#20998;&#35010;&#37319;&#26679;&#22120;&#20197;&#25552;&#39640;&#37319;&#26679;&#25928;&#29575;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#24471;&#21040;&#30340;&#37319;&#26679;&#22120;&#31216;&#20026;&#20943;&#23567;&#20998;&#35010;&#31215;&#20998;&#22120;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#30456;&#31354;&#38388;&#26391;&#20043;&#19975;&#25193;&#25955; (PSLD) [Pandey \&amp; Mandt, 2023] &#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#38543;&#26426;&#37319;&#26679;&#22120;&#22312;&#20165;&#36827;&#34892;100&#27425;&#32593;&#32476;&#20989;&#25968;&#35780;&#20272;&#21518;&#65292;&#23454;&#29616;&#20102;2.36&#30340;FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models suffer from slow sample generation at inference time. Despite recent efforts, improving the sampling efficiency of stochastic samplers for diffusion models remains a promising direction. We propose Splitting Integrators for fast stochastic sampling in pre-trained diffusion models in augmented spaces. Commonly used in molecular dynamics, splitting-based integrators attempt to improve sampling efficiency by cleverly alternating between numerical updates involving the data, auxiliary, or noise variables. However, we show that a naive application of splitting integrators is sub-optimal for fast sampling. Consequently, we propose several principled modifications to naive splitting samplers for improving sampling efficiency and denote the resulting samplers as Reduced Splitting Integrators. In the context of Phase Space Langevin Diffusion (PSLD) [Pandey \&amp; Mandt, 2023] on CIFAR-10, our stochastic sampler achieves an FID score of 2.36 in only 100 network function evaluations 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#20540;&#24863;&#30693;&#25209;&#24402;&#19968;&#21270;(OABN)&#21644;&#19968;&#31181;&#32858;&#31867;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#35757;&#32451;(ClusterQAT)&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;&#32593;&#32476;(SR)&#20013;&#30001;&#21512;&#24182;&#36807;&#31243;&#24341;&#20837;&#30340;&#24322;&#24120;&#20540;&#65292;&#20197;&#25552;&#39640;&#37327;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07200</link><description>&lt;p&gt;
&#24322;&#24120;&#20540;&#24863;&#30693;&#30340;&#32467;&#26500;&#20877;&#21442;&#25968;&#32593;&#32476;&#20302;&#27604;&#29305;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07200
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#24120;&#20540;&#24863;&#30693;&#25209;&#24402;&#19968;&#21270;(OABN)&#21644;&#19968;&#31181;&#32858;&#31867;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#35757;&#32451;(ClusterQAT)&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;&#32593;&#32476;(SR)&#20013;&#30001;&#21512;&#24182;&#36807;&#31243;&#24341;&#20837;&#30340;&#24322;&#24120;&#20540;&#65292;&#20197;&#25552;&#39640;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36731;&#37327;&#32423;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#35774;&#35745;&#38656;&#35201;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#21387;&#32553;&#25216;&#26415;&#26041;&#38754;&#36827;&#34892;&#21327;&#21516;&#35774;&#35745;&#12290;&#20316;&#20026;&#19968;&#31181;&#23558;&#35757;&#32451;&#21644;&#25512;&#26029;&#20998;&#31163;&#30340;&#26032;&#35774;&#35745;&#33539;&#24335;&#65292;&#32467;&#26500;&#20877;&#21442;&#25968;&#21270;(SR)&#32593;&#32476;&#65292;&#22914;&#20195;&#34920;&#24615;&#30340;RepVGG&#65292;&#20351;&#31616;&#21333;&#30340;VGG&#26679;&#24335;&#32593;&#32476;&#28949;&#21457;&#20102;&#26032;&#30340;&#29983;&#26426;&#65292;&#20855;&#26377;&#19982;&#20808;&#36827;&#19988;&#24120;&#24120;&#26356;&#22797;&#26434;&#30340;&#32593;&#32476;&#30456;&#24403;&#30340;&#39640;&#20934;&#30830;&#24230;&#12290;&#28982;&#32780;&#65292;SR&#32593;&#32476;&#20013;&#30340;&#21512;&#24182;&#36807;&#31243;&#20250;&#22312;&#26435;&#37325;&#20013;&#24341;&#20837;&#24322;&#24120;&#20540;&#65292;&#20351;&#20854;&#20998;&#24067;&#19982;&#20256;&#32479;&#32593;&#32476;&#19981;&#21516;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#37327;&#21270;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#24322;&#24120;&#20540;&#24863;&#30693;&#25209;&#24402;&#19968;&#21270;(OABN)&#30340;&#25805;&#20316;&#32423;&#25913;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#28385;&#36275;&#26377;&#38480;&#27604;&#29305;&#23485;&#24230;&#30340;&#35201;&#27714;&#24182;&#20445;&#25345;&#25512;&#26029;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#35757;&#32451;(ClusterQAT)&#26694;&#26550;&#12290;&#23558;OABN&#19982;ClusterQAT&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;RepVG&#30340;&#37327;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lightweight design of Convolutional Neural Networks (CNNs) requires co-design efforts in the model architectures and compression techniques. As a novel design paradigm that separates training and inference, a structural re-parameterized (SR) network such as the representative RepVGG revitalizes the simple VGG-like network with a high accuracy comparable to advanced and often more complicated networks. However, the merging process in SR networks introduces outliers into weights, making their distribution distinct from conventional networks and thus heightening difficulties in quantization. To address this, we propose an operator-level improvement for training called Outlier Aware Batch Normalization (OABN). Additionally, to meet the demands of limited bitwidths while upkeeping the inference accuracy, we develop a clustering-based non-uniform quantization framework for Quantization-Aware Training (QAT) named ClusterQAT. Integrating OABN with ClusterQAT, the quantized performance of RepVG
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#33719;&#24471;&#20108;&#38454;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#26356;&#32039;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#23454;&#35777;&#34920;&#26126;&#20102;DistRL&#30340;&#22909;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.07198</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#26356;&#22810;&#22909;&#22788;&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#20108;&#38454;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
More Benefits of Being Distributional: Second-Order Bounds for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07198
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#33719;&#24471;&#20108;&#38454;&#30028;&#38480;&#65292;&#36825;&#20123;&#30028;&#38480;&#26356;&#32039;&#65292;&#21516;&#26102;&#36824;&#36890;&#36807;&#24773;&#22659;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#23454;&#35777;&#34920;&#26126;&#20102;DistRL&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;DistRL&#65289;&#21487;&#20197;&#22312;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#19968;&#33324;&#35774;&#32622;&#20013;&#65292;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#33719;&#24471;&#20108;&#38454;&#30028;&#38480;&#12290;&#20108;&#38454;&#30028;&#38480;&#26159;&#19982;&#36820;&#22238;&#20540;&#30340;&#26041;&#24046;&#25104;&#27604;&#20363;&#30340;&#23454;&#20363;&#30456;&#20851;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#30028;&#38480;&#27604;&#20256;&#32479;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23567;&#25439;&#22833;&#30028;&#38480;&#26356;&#32039;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#23545;&#20110;&#20302;&#31209;MDP&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#39318;&#20010;&#20108;&#38454;&#30028;&#38480;&#12290;&#24403;&#29305;&#21270;&#20026;&#24773;&#22659;&#36172;&#21338;&#26426;&#26102;&#65288;&#19968;&#27493;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#20998;&#24067;&#23398;&#20064;&#30340;&#20048;&#35266;&#31639;&#27861;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#20108;&#38454;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#30028;&#38480;&#21644;&#20108;&#38454;&#38388;&#38548;&#20381;&#36182;&#30028;&#38480;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#22312;&#24773;&#22659;&#36172;&#21338;&#26426;&#20013;&#23454;&#35777;&#20102;DistRL&#30340;&#22909;&#22788;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#23545;DistRL&#30340;&#20998;&#26512;&#30456;&#23545;&#31616;&#21333;&#65292;&#36981;&#24490;&#20102;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#20048;&#35266;&#24615;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#30340;&#24773;&#22659;&#36172;&#21338;&#26426;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we prove that Distributional Reinforcement Learning (DistRL), which learns the return distribution, can obtain second-order bounds in both online and offline RL in general settings with function approximation. Second-order bounds are instance-dependent bounds that scale with the variance of return, which we prove are tighter than the previously known small-loss bounds of distributional RL. To the best of our knowledge, our results are the first second-order bounds for low-rank MDPs and for offline RL. When specializing to contextual bandits (one-step RL problem), we show that a distributional learning based optimism algorithm achieves a second-order worst-case regret bound, and a second-order gap dependent bound, simultaneously. We also empirically demonstrate the benefit of DistRL in contextual bandits on real-world datasets. We highlight that our analysis with DistRL is relatively simple, follows the general framework of optimism in the face of uncertainty and does not
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07193</link><description>&lt;p&gt;
&#26799;&#24230;&#22122;&#22768;&#30340;&#38544;&#24615;&#20559;&#35265;&#65306;&#20174;&#23545;&#31216;&#24615;&#35282;&#24230;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
The Implicit Bias of Gradient Noise: A Symmetry Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#36827;&#34892;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#22122;&#22768;&#22312;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#31216;&#24615;&#20250;&#23548;&#33268;&#19981;&#21516;&#30340;&#23398;&#20064;&#21160;&#24577;&#65292;&#20854;&#20013;&#19968;&#31867;&#23545;&#31216;&#24615;&#21487;&#20197;&#33258;&#28982;&#25910;&#25947;&#65292;&#32780;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36866;&#29992;&#20110;&#27809;&#26377;&#23545;&#31216;&#24615;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;&#20110;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#21644;&#35299;&#37322;&#30456;&#20851;&#23454;&#38469;&#38382;&#39064;&#20855;&#26377;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#36830;&#32493;&#23545;&#31216;&#24615;&#26102;&#30340;&#23398;&#20064;&#21160;&#24577;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#35828;&#26126;&#20102;SGD&#21644;&#26799;&#24230;&#19979;&#38477;&#20043;&#38388;&#30340;&#20998;&#27495;&#26159;&#22810;&#20040;&#24040;&#22823;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#23545;&#31216;&#24615;&#23545;&#23398;&#20064;&#21160;&#24577;&#30340;&#24433;&#21709;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#19968;&#26063;&#23545;&#31216;&#24615;&#20998;&#20026;&#20004;&#31867;&#12290;&#23545;&#20110;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#33258;&#28982;&#22320;&#25910;&#25947;&#21040;&#20855;&#26377;&#24179;&#34913;&#21644;&#23545;&#40784;&#26799;&#24230;&#22122;&#22768;&#30340;&#35299;&#12290;&#23545;&#20110;&#21478;&#19968;&#31867;&#23545;&#31216;&#24615;&#65292;SGD&#20960;&#20046;&#24635;&#26159;&#21457;&#25955;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#20013;&#27809;&#26377;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20381;&#28982;&#36866;&#29992;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#29702;&#35299;&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#26222;&#36941;&#30340;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#32780;&#19982;&#25439;&#22833;&#20989;&#25968;&#30340;&#32454;&#33410;&#26080;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#23545;&#20110;&#36880;&#27493;&#21464;&#24418;&#21644;&#24179;&#22374;&#21270;&#25552;&#20379;&#20102;&#35299;&#37322;&#65292;&#24182;&#21487;&#20197;&#24212;&#29992;&#20110;&#24120;&#35265;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#34920;&#31034;&#27491;&#21017;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We characterize the learning dynamics of stochastic gradient descent (SGD) when continuous symmetry exists in the loss function, where the divergence between SGD and gradient descent is dramatic. We show that depending on how the symmetry affects the learning dynamics, we can divide a family of symmetry into two classes. For one class of symmetry, SGD naturally converges to solutions that have a balanced and aligned gradient noise. For the other class of symmetry, SGD will almost always diverge. Then, we show that our result remains applicable and can help us understand the training dynamics even when the symmetry is not present in the loss function. Our main result is universal in the sense that it only depends on the existence of the symmetry and is independent of the details of the loss function. We demonstrate that the proposed theory offers an explanation of progressive sharpening and flattening and can be applied to common practical problems such as representation normalization, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#12289;&#36719;&#24615;&#21644;&#21487;&#24494;&#24615;&#21407;&#21017;&#26469;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07191</link><description>&lt;p&gt;
GSINA: &#36890;&#36807;&#22270;Sinkhorn Attention&#25913;&#36827;&#22270;&#19981;&#21464;&#23398;&#20064;&#20013;&#30340;&#23376;&#22270;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
GSINA: Improving Subgraph Extraction for Graph Invariant Learning via Graph Sinkhorn Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#22270;&#19981;&#21464;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#31232;&#30095;&#24615;&#12289;&#36719;&#24615;&#21644;&#21487;&#24494;&#24615;&#21407;&#21017;&#26469;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19981;&#21464;&#23398;&#20064;(GIL)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#19979;&#21457;&#29616;&#22270;&#25968;&#25454;&#19982;&#20854;&#26631;&#31614;&#20043;&#38388;&#30340;&#19981;&#21464;&#20851;&#31995;&#65292;&#20197;&#35299;&#20915;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;GIL&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20174;&#36755;&#20837;&#22270;&#20013;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#65292;&#20316;&#20026;&#35268;&#21017;&#21270;&#31574;&#30053;&#26469;&#25552;&#39640;&#22270;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#33719;&#21462;&#19981;&#21464;&#23376;&#22270;&#26041;&#38754;&#20063;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#24037;&#20316;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#25552;&#21462;&#19981;&#21464;&#23376;&#22270;&#30340;&#30456;&#24212;&#21407;&#21017;&#65306;1&#65289;&#31232;&#30095;&#24615;&#65292;&#20197;&#36807;&#28388;&#25481;&#21464;&#24322;&#29305;&#24449;&#65307;2&#65289;&#36719;&#24615;&#65292;&#20197;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#35299;&#31354;&#38388;&#65307;&#21644;3&#65289;&#21487;&#24494;&#24615;&#65292;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#20248;&#21270;&#12290;&#20026;&#20102;&#22312;&#19968;&#27425;&#25805;&#20316;&#20013;&#28385;&#36275;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20248;&#20256;&#36755;(OT)&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#31216;&#20026;&#22270;Sinkhorn Attention&#65288;G)
&lt;/p&gt;
&lt;p&gt;
Graph invariant learning (GIL) has been an effective approach to discovering the invariant relationships between graph data and its labels for different graph learning tasks under various distribution shifts. Many recent endeavors of GIL focus on extracting the invariant subgraph from the input graph for prediction as a regularization strategy to improve the generalization performance of graph learning. Despite their success, such methods also have various limitations in obtaining their invariant subgraphs. In this paper, we provide in-depth analyses of the drawbacks of existing works and propose corresponding principles of our invariant subgraph extraction: 1) the sparsity, to filter out the variant features, 2) the softness, for a broader solution space, and 3) the differentiability, for a soundly end-to-end optimization. To meet these principles in one shot, we leverage the Optimal Transport (OT) theory and propose a novel graph attention mechanism called Graph Sinkhorn Attention (G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CP-E2LSH&#21644;TT-E2LSH&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;LSH&#65292;&#22312;&#22788;&#29702;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#33021;&#22815;&#25552;&#20379;&#26356;&#24555;&#21644;&#26356;&#31354;&#38388;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07189</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#21270;&#38543;&#26426;&#25237;&#24433;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;LSH
&lt;/p&gt;
&lt;p&gt;
Improving LSH via Tensorized Random Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CP-E2LSH&#21644;TT-E2LSH&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;LSH&#65292;&#22312;&#22788;&#29702;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#33021;&#22815;&#25552;&#20379;&#26356;&#24555;&#21644;&#26356;&#31354;&#38388;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;(LSH)&#26159;&#25968;&#25454;&#31185;&#23398;&#23478;&#29992;&#20110;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#26412;&#31639;&#27861;&#24037;&#20855;&#65292;&#24050;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#25968;&#25454;&#22788;&#29702;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#36817;&#20284;&#37325;&#22797;&#26816;&#27979;&#12289;&#26368;&#36817;&#37051;&#25628;&#32034;&#12289;&#32858;&#31867;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#26356;&#24555;&#21644;&#31354;&#38388;&#26356;&#26377;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#20989;&#25968;&#65292;&#29992;&#20110;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#36890;&#24120;&#65292;&#23545;&#20110;&#24352;&#37327;&#25968;&#25454;&#33719;&#24471;LSH&#30340;&#26420;&#32032;&#26041;&#27861;&#28041;&#21450;&#23558;&#24352;&#37327;&#37325;&#22609;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#29616;&#26377;&#30340;&#21521;&#37327;&#25968;&#25454;LSH&#26041;&#27861;(E2LSH&#21644;SRP)&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#38454;&#24352;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#20026;&#37325;&#22609;&#21521;&#37327;&#30340;&#22823;&#23567;&#22312;&#24352;&#37327;&#30340;&#38454;&#25968;&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;LSH&#21442;&#25968;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;LSH&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;CP-E2LSH&#21644;TT-E2LSH&#12290;
&lt;/p&gt;
&lt;p&gt;
Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by data scientists for approximate nearest neighbour search problems that have been used extensively in many large scale data processing applications such as near duplicate detection, nearest neighbour search, clustering, etc. In this work, we aim to propose faster and space efficient locality sensitive hash functions for Euclidean distance and cosine similarity for tensor data. Typically, the naive approach for obtaining LSH for tensor data involves first reshaping the tensor into vectors, followed by applying existing LSH methods for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical for higher order tensors because the size of the reshaped vector becomes exponential in the order of the tensor. Consequently, the size of LSH parameters increases exponentially. To address this problem, we suggest two methods for LSH for Euclidean distance and cosine similarity, namely $CP-E2LSH$, $TT-E2LSH
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IPRO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20998;&#35299;&#20219;&#21153;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#26041;&#27861;&#65292;&#21487;&#21487;&#38752;&#22320;&#25581;&#31034;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26368;&#20248;&#34920;&#29616;&#30340;&#31574;&#30053;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#21516;&#26102;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#21644;&#26410;&#21457;&#29616;&#35299;&#30340;&#36317;&#31163;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07182</link><description>&lt;p&gt;
&#20998;&#32780;&#27835;&#20043;&#65306;&#29992;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#21487;&#38752;&#22320;&#25581;&#31034;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IPRO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20998;&#35299;&#20219;&#21153;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#26041;&#27861;&#65292;&#21487;&#21487;&#38752;&#22320;&#25581;&#31034;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26368;&#20248;&#34920;&#29616;&#30340;&#31574;&#30053;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#21516;&#26102;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#21644;&#26410;&#21457;&#29616;&#35299;&#30340;&#36317;&#31163;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#33719;&#21462;&#22312;&#19981;&#21516;&#20559;&#22909;&#19979;&#23454;&#29616;&#26368;&#20248;&#34920;&#29616;&#30340;&#31574;&#30053;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#24085;&#32047;&#25176;&#21442;&#32771;&#20248;&#21270;&#65288;IPRO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21407;&#21017;&#24615;&#31639;&#27861;&#65292;&#23427;&#23558;&#25214;&#21040;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#20219;&#21153;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#20855;&#26377;&#21508;&#31181;&#35299;&#20915;&#26041;&#27861;&#30340;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#24314;&#31435;&#25910;&#25947;&#20445;&#35777;&#24182;&#25552;&#20379;&#26410;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#30340;&#36317;&#31163;&#19978;&#38480;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;IPRO&#33021;&#22815;&#19982;&#38656;&#35201;&#39069;&#22806;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#25110;&#20248;&#20110;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#38382;&#39064;&#29305;&#23450;&#30340;&#21333;&#30446;&#26631;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#26377;&#26395;&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20043;&#22806;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#27604;&#22914;&#36335;&#24452;&#35268;&#21010;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.07180</link><description>&lt;p&gt;
MAGNETO&#65306;&#36793;&#32536;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#30340;&#36793;&#32536;AI--&#38544;&#31169;&#21644;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
MAGNETO: Edge AI for Human Activity Recognition -- Privacy and Personalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#36890;&#36807;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#36991;&#20813;&#20102;&#20113;&#31471;&#19982;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#24310;&#36831;&#22788;&#29702;&#21644;&#39640;&#24230;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#19968;&#20010;&#25104;&#29087;&#30340;&#39046;&#22495;&#65292;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#26174;&#33879;&#25512;&#21160;&#20102;&#20854;&#21457;&#23637;&#12290;&#23613;&#31649;&#20844;&#21496;&#25104;&#21151;&#22320;&#23558;HAR&#25972;&#21512;&#21040;&#28040;&#36153;&#21697;&#20013;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#27963;&#21160;&#38598;&#65292;&#36825;&#38480;&#21046;&#20102;&#29992;&#25143;&#32423;&#65288;&#36793;&#32536;&#35774;&#22791;&#65289;&#30340;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22312;&#22686;&#37327;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#33021;&#22815;&#20351;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#20294;&#36825;&#36890;&#24120;&#21457;&#29983;&#22312;&#20113;&#31471;&#65292;&#38656;&#35201;&#23450;&#26399;&#22312;&#20113;&#31471;&#21644;&#36793;&#32536;&#35774;&#22791;&#20043;&#38388;&#36827;&#34892;&#25968;&#25454;&#20256;&#36755;&#65292;&#20174;&#32780;&#24341;&#21457;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MAGNETO&#30340;&#36793;&#32536;AI&#24179;&#21488;&#65292;&#23558;HAR&#20219;&#21153;&#20174;&#20113;&#31471;&#25512;&#21521;&#36793;&#32536;&#12290;MAGNETO&#20801;&#35768;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30452;&#25509;&#36827;&#34892;&#22686;&#37327;&#20154;&#20307;&#27963;&#21160;&#23398;&#20064;&#65292;&#32780;&#26080;&#38656;&#19982;&#20113;&#31471;&#36827;&#34892;&#20219;&#20309;&#25968;&#25454;&#20132;&#25442;&#12290;&#36825;&#21487;&#20197;&#25552;&#20379;&#24378;&#22823;&#30340;&#38544;&#31169;&#20445;&#35777;&#12289;&#20302;&#22788;&#29702;&#24310;&#36831;&#21644;&#39640;&#24230;&#30340;&#20010;&#24615;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;Android&#35774;&#22791;&#19978;&#28436;&#31034;&#20102;MAGNETO&#65292;&#20174;&#25968;&#25454;&#37319;&#38598;&#21040;&#32467;&#26524;&#21487;&#35270;&#21270;&#65292;&#39564;&#35777;&#20102;&#25972;&#20010;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human activity recognition (HAR) is a well-established field, significantly advanced by modern machine learning (ML) techniques. While companies have successfully integrated HAR into consumer products, they typically rely on a predefined activity set, which limits personalizations at the user level (edge devices). Despite advancements in Incremental Learning for updating models with new data, this often occurs on the Cloud, necessitating regular data transfers between cloud and edge devices, thus leading to data privacy issues. In this paper, we propose MAGNETO, an Edge AI platform that pushes HAR tasks from the Cloud to the Edge. MAGNETO allows incremental human activity learning directly on the Edge devices, without any data exchange with the Cloud. This enables strong privacy guarantees, low processing latency, and a high degree of personalization for users. In particular, we demonstrate MAGNETO in an Android device, validating the whole pipeline from data collection to result visua
&lt;/p&gt;</description></item><item><title>GeoFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21644;&#24207;&#21015;Transformer&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;Sentinel-5P&#21355;&#26143;&#22270;&#20687;&#20013;&#39044;&#27979;&#22320;&#34920;&#20108;&#27687;&#21270;&#27694;&#65288;NO2&#65289;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.07164</link><description>&lt;p&gt;
GeoFormer:&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21644;&#24207;&#21015;Transformer&#30340;&#28201;&#23460;&#27668;&#20307;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GeoFormer: A Vision and Sequence Transformer-based Approach for Greenhouse Gas Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07164
&lt;/p&gt;
&lt;p&gt;
GeoFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21644;&#24207;&#21015;Transformer&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;Sentinel-5P&#21355;&#26143;&#22270;&#20687;&#20013;&#39044;&#27979;&#22320;&#34920;&#20108;&#27687;&#21270;&#27694;&#65288;NO2&#65289;&#27987;&#24230;&#12290;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#20195;&#34920;&#20102;&#19968;&#20010;&#20851;&#38190;&#30340;&#29615;&#22659;&#25361;&#25112;&#65292;&#36890;&#36807;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#22312;&#27668;&#20505;&#21464;&#21270;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#23545;&#20840;&#29699;&#25968;&#21313;&#20159;&#20154;&#30340;&#20581;&#24247;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#27745;&#26579;&#29289;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20998;&#24067;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22320;&#38754;&#30417;&#27979;&#35774;&#26045;&#30340;&#31232;&#32570;&#20197;&#21450;&#31354;&#27668;&#27745;&#26579;&#24314;&#27169;&#23545;&#20840;&#38754;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#24615;&#65288;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#23545;&#35768;&#22810;&#22320;&#21306;&#19981;&#21487;&#33719;&#21462;&#65289;&#20351;&#24471;&#38382;&#39064;&#22797;&#26434;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GeoFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#35270;&#35273;Transformer&#27169;&#22359;&#21644;&#39640;&#25928;&#30340;&#26102;&#38388;&#24207;&#21015;Transformer&#27169;&#22359;&#30340;&#32039;&#20945;&#27169;&#22411;&#65292;&#21487;&#20174;Sentinel-5P&#21355;&#26143;&#22270;&#20687;&#20013;&#39044;&#27979;&#22320;&#34920;&#20108;&#27687;&#21270;&#27694;&#65288;NO2&#65289;&#27987;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;Sentinel-5P&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;NO2&#27987;&#24230;&#35835;&#25968;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20197;&#39044;&#27979;&#22320;&#34920;NO2&#27987;&#24230;&#27979;&#37327;&#20540;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65288;MAE 5.65&#65289;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air pollution represents a pivotal environmental challenge globally, playing a major role in climate change via greenhouse gas emissions and negatively affecting the health of billions. However predicting the spatial and temporal patterns of pollutants remains challenging. The scarcity of ground-based monitoring facilities and the dependency of air pollution modeling on comprehensive datasets, often inaccessible for numerous areas, complicate this issue. In this work, we introduce GeoFormer, a compact model that combines a vision transformer module with a highly efficient time-series transformer module to predict surface-level nitrogen dioxide (NO2) concentrations from Sentinel-5P satellite imagery. We train the proposed model to predict surface-level NO2 measurements using a dataset we constructed with Sentinel-5P images of ground-level monitoring stations, and their corresponding NO2 concentration readings. The proposed model attains high accuracy (MAE 5.65), demonstrating the effica
&lt;/p&gt;</description></item><item><title>PASOA&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#31243;&#24207;&#65292;&#36890;&#36807;&#25552;&#20379;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#20272;&#35745;&#65292;&#21516;&#26102;&#25191;&#34892;&#39034;&#24207;&#35774;&#35745;&#20248;&#21270;&#21644;&#21442;&#25968;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; stochastic optimization &#21644; tempered SMC &#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#30340;&#26368;&#20248;&#35774;&#35745;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.07160</link><description>&lt;p&gt;
PASOA-&#22522;&#20110;&#31890;&#23376;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#33258;&#36866;&#24212;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
PASOA- PArticle baSed Bayesian Optimal Adaptive design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07160
&lt;/p&gt;
&lt;p&gt;
PASOA&#26159;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#31243;&#24207;&#65292;&#36890;&#36807;&#25552;&#20379;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#20272;&#35745;&#65292;&#21516;&#26102;&#25191;&#34892;&#39034;&#24207;&#35774;&#35745;&#20248;&#21270;&#21644;&#21442;&#25968;&#25512;&#26029;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992; stochastic optimization &#21644; tempered SMC &#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#33268;&#24615;&#30340;&#26368;&#20248;&#35774;&#35745;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PASOA&#30340;&#26032;&#31243;&#24207;&#65292;&#29992;&#20110;&#36125;&#21494;&#26031;&#23454;&#39564;&#35774;&#35745;&#65292;&#36890;&#36807;&#21516;&#26102;&#25552;&#20379;&#36830;&#32493;&#30340;&#21518;&#39564;&#20998;&#24067;&#30340;&#20934;&#30830;&#20272;&#35745;&#26469;&#25191;&#34892;&#39034;&#24207;&#35774;&#35745;&#20248;&#21270;&#12290;&#39034;&#24207;&#35774;&#35745;&#36807;&#31243;&#36890;&#36807;&#23545;&#27604;&#20272;&#35745;&#21407;&#21017;&#36827;&#34892;&#65292;&#20351;&#29992;&#38543;&#26426;&#20248;&#21270;&#21644;&#39034;&#24207;&#33945;&#29305;&#21345;&#32599;&#65288;SMC&#65289;&#37319;&#26679;&#22120;&#26469;&#26368;&#22823;&#21270;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65288;EIG&#65289;&#12290;&#30001;&#20110;&#36830;&#32493;&#21518;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#36234;&#22823;&#65292;&#33719;&#24471;&#30340;&#20449;&#24687;&#22686;&#30410;&#36234;&#22823;&#65292;&#22240;&#27492;&#36825;&#20010;EIG&#30446;&#26631;&#21487;&#33021;&#20250;&#24694;&#21270;&#32463;&#20856;SMC&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#28201;&#24230;&#35843;&#33410;&#65292;&#26082;&#21487;&#20197;&#33719;&#24471;&#22823;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#21448;&#21487;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;SMC&#37319;&#26679;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#23545;&#24615;&#33021;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36825;&#31181;&#38543;&#26426;&#20248;&#21270;&#21644;&#28201;&#24230;&#35843;&#33410;&#30340;&#26032;&#39062;&#32452;&#21512;&#20801;&#35768;&#21516;&#26102;&#22788;&#29702;&#35774;&#35745;&#20248;&#21270;&#21644;&#21442;&#25968;&#25512;&#26029;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#24471;&#21040;&#30340;&#26368;&#20248;&#35774;&#35745;&#20272;&#35745;&#37327;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#35745;&#31639;&#39044;&#31639;&#19979;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#22320;&#20248;&#21270;&#20102;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#30028;&#38754;&#22312;&#36719;&#20214;&#39033;&#30446;&#20013;&#36827;&#34892;&#24037;&#20316;&#37327;&#21644;&#35268;&#27169;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#38382;&#39064;&#35268;&#33539;&#26469;&#23454;&#29616;&#24320;&#21457;&#24037;&#20316;&#37327;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.07158</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#30028;&#38754;&#22312;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#24037;&#20316;&#37327;&#21644;&#35268;&#27169;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Effort and Size Estimation in Software Projects with Large Language Model-based Intelligent Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#30028;&#38754;&#22312;&#36719;&#20214;&#39033;&#30446;&#20013;&#36827;&#34892;&#24037;&#20316;&#37327;&#21644;&#35268;&#27169;&#20272;&#35745;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#38382;&#39064;&#35268;&#33539;&#26469;&#23454;&#29616;&#24320;&#21457;&#24037;&#20316;&#37327;&#30340;&#20934;&#30830;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20063;&#23548;&#33268;&#20854;&#24212;&#29992;&#30340;&#24191;&#27867;&#22686;&#21152;&#12290;&#36719;&#20214;&#35774;&#35745;&#20316;&#20026;&#20854;&#20013;&#20043;&#19968;&#65292;&#22312;&#20351;&#29992;LLM&#20316;&#20026;&#25193;&#23637;&#22266;&#23450;&#29992;&#25143;&#25925;&#20107;&#30340;&#25509;&#21475;&#32452;&#20214;&#26041;&#38754;&#33719;&#24471;&#20102;&#24040;&#22823;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#20110;LLM&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21253;&#21547;&#22312;&#36719;&#20214;&#35774;&#35745;&#20013;&#24120;&#24120;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#21457;&#24037;&#20316;&#37327;&#30340;&#20272;&#35745;&#26041;&#38754;&#12290;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#30028;&#38754;&#30340;&#29992;&#25143;&#25925;&#20107;&#30340;&#20363;&#23376;&#65292;&#25105;&#20204;&#23545;&#27604;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#38382;&#39064;&#30340;&#35268;&#33539;&#65292;&#36890;&#36807;&#32771;&#34385;&#25968;&#25454;&#28304;&#12289;&#25509;&#21475;&#21644;&#31639;&#27861;&#26469;&#36827;&#34892;&#24320;&#21457;&#24037;&#20316;&#37327;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Language Models (LLM) has also resulted in an equivalent proliferation in its applications. Software design, being one, has gained tremendous benefits in using LLMs as an interface component that extends fixed user stories. However, inclusion of LLM-based AI agents in software design often poses unexpected challenges, especially in the estimation of development efforts. Through the example of UI-based user stories, we provide a comparison against traditional methods and propose a new way to enhance specifications of natural language-based questions that allows for the estimation of development effort by taking into account data sources, interfaces and algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07157</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#21644;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#31561;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#23398;&#20064;&#20915;&#31574;&#20219;&#21153;&#30340;&#31574;&#30053;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;RL&#24120;&#24120;&#38754;&#20020;&#26679;&#26412;&#25928;&#29575;&#20302;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#32570;&#20047;&#31232;&#30095;&#30417;&#30563;&#20449;&#21495;&#31561;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#24378;&#21270;&#23398;&#20064;&#65288;NLRL&#65289;&#65292;&#21019;&#26032;&#24615;&#22320;&#23558;RL&#21407;&#21017;&#19982;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#32467;&#21512;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NLRL&#22312;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#37325;&#26032;&#23450;&#20041;&#20102;&#20219;&#21153;&#30446;&#26631;&#12289;&#31574;&#30053;&#12289;&#20215;&#20540;&#20989;&#25968;&#12289;Bellman&#26041;&#31243;&#21644;&#31574;&#30053;&#36845;&#20195;&#31561;RL&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#26469;&#23454;&#29616;NLRL&#12290;&#23545;&#34920;&#26684;MDPs&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#20102;NLRL&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) has shown remarkable abilities in learning policies for decision-making tasks. However, RL is often hindered by issues such as low sample efficiency, lack of interpretability, and sparse supervision signals. To tackle these limitations, we take inspiration from the human learning process and introduce Natural Language Reinforcement Learning (NLRL), which innovatively combines RL principles with natural language representation. Specifically, NLRL redefines RL concepts like task objectives, policy, value function, Bellman equation, and policy iteration in natural language space. We present how NLRL can be practically implemented with the latest advancements in large language models (LLMs) like GPT-4. Initial experiments over tabular MDPs demonstrate the effectiveness, efficiency, and also interpretability of the NLRL framework.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;MIONet&#30340;&#28151;&#21512;&#36845;&#20195;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#25968;&#20540;&#36845;&#20195;&#27714;&#35299;&#22120;&#21644;&#31070;&#32463;&#25805;&#20316;&#31526;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#22791;&#21331;&#36234;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07156</link><description>&lt;p&gt;
&#22522;&#20110;MIONet&#30340;&#28151;&#21512;&#36845;&#20195;&#26041;&#27861;&#29992;&#20110;PDEs: &#29702;&#35770;&#21644;&#25968;&#20540;&#23454;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A hybrid iterative method based on MIONet for PDEs: Theory and numerical examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07156
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;MIONet&#30340;&#28151;&#21512;&#36845;&#20195;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#25968;&#20540;&#36845;&#20195;&#27714;&#35299;&#22120;&#21644;&#31070;&#32463;&#25805;&#20316;&#31526;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20855;&#22791;&#21331;&#36234;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MIONet&#30340;&#28151;&#21512;&#36845;&#20195;&#26041;&#27861;&#29992;&#20110;PDEs&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#20256;&#32479;&#30340;&#25968;&#20540;&#36845;&#20195;&#27714;&#35299;&#22120;&#21644;&#26368;&#36817;&#24378;&#22823;&#30340;&#31070;&#32463;&#25805;&#20316;&#31526;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#36827;&#19968;&#27493;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#23427;&#22312;&#31163;&#25955;&#21270;&#35823;&#24046;&#21644;&#27169;&#22411;&#25512;&#26029;&#35823;&#24046;&#26041;&#38754;&#30340;&#25910;&#25947;&#26465;&#20214;&#12289;&#35889;&#34892;&#20026;&#20197;&#21450;&#25910;&#25947;&#36895;&#29575;&#31561;&#29702;&#35770;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#24120;&#29992;&#30340;&#24179;&#28369;&#22120;&#65292;&#22914;Richardson&#65288;&#38459;&#23612;&#38597;&#21487;&#27604;&#65289;&#21644;Gauss-Seidel&#65292;&#32473;&#20986;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#28151;&#21512;&#26041;&#27861;&#30456;&#23545;&#20110;&#27169;&#22411;&#20462;&#27491;&#21608;&#26399;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#36825;&#34920;&#26126;&#20102;&#20351;&#28151;&#21512;&#36845;&#20195;&#26368;&#24555;&#25910;&#25947;&#30340;&#26368;&#23567;&#28857;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#20363;&#65292;&#21253;&#25324;1&#32500;&#65288;2&#32500;&#65289;&#27850;&#26494;&#26041;&#31243;&#30340;&#28151;&#21512;Richardson&#65288;Gauss-Seidel&#65289;&#36845;&#20195;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#21516;&#26102;&#21453;&#26144;&#20986;&#20102;&#21331;&#36234;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;&#20316;&#20026;&#19968;&#31181;&#26080;&#32593;&#26684;&#21152;&#36895;&#26041;&#27861;&#65292;&#23427;&#20855;&#22791;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a hybrid iterative method based on MIONet for PDEs, which combines the traditional numerical iterative solver and the recent powerful machine learning method of neural operator, and further systematically analyze its theoretical properties, including the convergence condition, the spectral behavior, as well as the convergence rate, in terms of the errors of the discretization and the model inference. We show the theoretical results for the frequently-used smoothers, i.e. Richardson (damped Jacobi) and Gauss-Seidel. We give an upper bound of the convergence rate of the hybrid method w.r.t. the model correction period, which indicates a minimum point to make the hybrid iteration converge fastest. Several numerical examples including the hybrid Richardson (Gauss-Seidel) iteration for the 1-d (2-d) Poisson equation are presented to verify our theoretical results, and also reflect an excellent acceleration effect. As a meshless acceleration method, it is provided with enormous po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#20840;&#29699;&#27668;&#20505;&#21644;&#37326;&#28779;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#28023;&#27915;&#25968;&#25454;&#32570;&#22833;&#21644;&#36828;&#31243;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.07152</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explainable Global Wildfire Prediction Models using Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#65292;&#23558;&#20840;&#29699;&#27668;&#20505;&#21644;&#37326;&#28779;&#25968;&#25454;&#36716;&#21270;&#20026;&#22270;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#30340;&#28023;&#27915;&#25968;&#25454;&#32570;&#22833;&#21644;&#36828;&#31243;&#20381;&#36182;&#24615;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#26356;&#39640;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#36824;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27668;&#20505;&#21464;&#21270;&#30340;&#19981;&#26029;&#21152;&#21095;&#65292;&#37326;&#28779;&#39044;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#22312;&#22788;&#29702;&#32570;&#22833;&#30340;&#28023;&#27915;&#25968;&#25454;&#21644;&#35299;&#20915;&#27668;&#35937;&#25968;&#25454;&#20013;&#36828;&#31243;&#22320;&#21306;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20840;&#29699;&#37326;&#28779;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23558;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#31354;&#38388;&#33021;&#21147;&#19982;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#32593;&#32476;&#30340;&#26102;&#38388;&#28145;&#24230;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20840;&#29699;&#27668;&#20505;&#21644;&#37326;&#28779;&#25968;&#25454;&#29420;&#29305;&#22320;&#36716;&#21270;&#20026;&#22270;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#31354;&#27934;&#28023;&#27915;&#25968;&#25454;&#20301;&#32622;&#21644;&#38271;&#26399;&#20381;&#36182;&#24615;&#31561;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#26410;&#30693;&#30340;JULES-INFERNO&#27169;&#25311;&#38598;&#23545;&#24050;&#24314;&#31435;&#30340;&#26550;&#26500;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20986;&#20248;&#36234;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25581;&#31034;&#20102;&#20854;&#20013;&#30340;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wildfire prediction has become increasingly crucial due to the escalating impacts of climate change. Traditional CNN-based wildfire prediction models struggle with handling missing oceanic data and addressing the long-range dependencies across distant regions in meteorological data. In this paper, we introduce an innovative Graph Neural Network (GNN)-based model for global wildfire prediction. We propose a hybrid model that combines the spatial prowess of Graph Convolutional Networks (GCNs) with the temporal depth of Long Short-Term Memory (LSTM) networks. Our approach uniquely transforms global climate and wildfire data into a graph representation, addressing challenges such as null oceanic data locations and long-range dependencies inherent in traditional models. Benchmarking against established architectures using an unseen ensemble of JULES-INFERNO simulations, our model demonstrates superior predictive accuracy. Furthermore, we emphasise the model's explainability, unveiling poten
&lt;/p&gt;</description></item><item><title>X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07148</link><description>&lt;p&gt;
X-LoRA: &#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07148
&lt;/p&gt;
&lt;p&gt;
X-LoRA&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#21033;&#29992;&#20302;&#31209;&#36866;&#37197;&#22120;&#19987;&#23478;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#21487;&#20197;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#27169;&#22411;&#24182;&#22312;&#34507;&#30333;&#36136;&#21147;&#23398;&#21644;&#35774;&#35745;&#39046;&#22495;&#24212;&#29992;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#24182;&#21463;&#21040;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#12290;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#21363;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#36947;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#23618;&#36880;&#23618;&#22522;&#20110;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#36866;&#37197;&#22120;&#30340;&#28151;&#21512;&#19987;&#23478;&#31574;&#30053;&#65292;&#29992;&#20110;&#21019;&#24314;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38544;&#34255;&#29366;&#24577;&#21160;&#24577;&#28151;&#21512;&#32463;&#36807;&#36866;&#24212;&#30340;&#23618;&#30340;&#38376;&#25511;&#31574;&#30053;&#65292;&#20801;&#35768;&#24471;&#21040;&#30340;X-LoRA&#27169;&#22411;&#21033;&#29992;&#19981;&#21516;&#30340;&#33021;&#21147;&#24182;&#21019;&#24314;&#20197;&#21069;&#26410;&#20351;&#29992;&#30340;&#28145;&#23618;&#36880;&#23618;&#36866;&#24212;&#30340;&#32452;&#21512;&#26469;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#12290;&#35813;&#35774;&#35745;&#21463;&#21040;&#20102;&#29983;&#29289;&#26222;&#36941;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#29983;&#29289;&#23398;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#22359;&#22312;&#19981;&#21516;&#30340;&#20998;&#23618;&#34920;&#31034;&#20013;&#34987;&#37325;&#22797;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;X-LoRA&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#29992;&#20110;&#20219;&#20309;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#26080;&#38656;&#20462;&#25913;&#24213;&#23618;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;X-LoRA&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21253;&#25324;&#21069;&#21521;/&#36870;&#21521;&#20998;&#26512;&#20219;&#21153;&#21644;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#22312;&#20869;&#30340;&#31185;&#23398;&#33021;&#21147;&#65292;&#37325;&#28857;&#26159;&#29983;&#29289;&#26448;&#26009;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, we propose a gating strategy that uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations of adaptations are established to solve specific tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#19982;&#32463;&#20856;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#19981;&#21516;&#65292;&#40657;&#30418;&#23376;&#27169;&#22411;&#23545;&#20110;&#26368;&#20248;&#30340;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26377;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.07139</link><description>&lt;p&gt;
&#36890;&#36807;&#40657;&#30418;&#23376;&#27169;&#22411;&#23454;&#29616;&#40065;&#26834;&#30340;&#36710;&#36742;&#36319;&#38543;&#21160;&#21147;&#23398;&#24314;&#27169;&#65306;&#26041;&#27861;&#35770;&#12289;&#20998;&#26512;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Car Following Dynamics Modeling via Blackbox Models: Methodology, Analysis, and Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#65292;&#21457;&#29616;&#19982;&#32463;&#20856;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#19981;&#21516;&#65292;&#40657;&#30418;&#23376;&#27169;&#22411;&#23545;&#20110;&#26368;&#20248;&#30340;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26377;&#19981;&#21516;&#30340;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#20064;&#32463;&#20856;&#30340;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#22914;GIPPS&#12289;IDM&#31561;&#30340;&#21442;&#25968;&#26102;&#65292;&#36873;&#25321;&#30446;&#26631;&#21464;&#37327;&#26159;&#37325;&#35201;&#30340;&#12290;&#20851;&#20110;&#32463;&#20856;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#21738;&#20010;&#30446;&#26631;&#21464;&#37327;&#26159;&#26368;&#20248;&#30340;&#26377;&#22823;&#37327;&#25991;&#29486;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#30740;&#31350;&#23545;&#40657;&#30418;&#23376;&#27169;&#22411;&#22914;LSTM&#31561;&#30340;&#26368;&#20248;&#30446;&#26631;&#21464;&#37327;&#36827;&#34892;&#23454;&#35777;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#23545;&#19977;&#31181;&#40657;&#30418;&#23376;&#27169;&#22411;&#65288;GP&#12289;LSTM&#21644;Kernel Ridge&#22238;&#24402;&#65289;&#30340;&#19981;&#21516;&#30446;&#26631;&#21464;&#37327;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#21152;&#36895;&#24230;&#12289;&#36895;&#24230;&#21644;&#36710;&#22836;&#38388;&#36317;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#24037;&#20316;&#22312;&#19981;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#20363;&#22914;&#65292;GP&#24037;&#20316;&#22312;&#20989;&#25968;&#31354;&#38388;&#65292;LSTM&#24037;&#20316;&#22312;&#21442;&#25968;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#40657;&#30418;&#23376;&#27169;&#22411;&#65292;&#26368;&#20248;&#30340;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#19982;&#32463;&#20856;&#30340;&#36710;&#36742;&#36319;&#38543;&#27169;&#22411;&#26377;&#25152;&#19981;&#21516;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#27169;&#22411;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The selection of the target variable is important while learning parameters of the classical car following models like GIPPS, IDM, etc. There is a vast body of literature on which target variable is optimal for classical car following models, but there is no study that empirically evaluates the selection of optimal target variables for black-box models, such as LSTM, etc. The black-box models, like LSTM and Gaussian Process (GP) are increasingly being used to model car following behavior without wise selection of target variables. The current work tests different target variables, like acceleration, velocity, and headway, for three black-box models, i.e., GP, LSTM, and Kernel Ridge Regression. These models have different objective functions and work in different vector spaces, e.g., GP works in function space, and LSTM works in parameter space. The experiments show that the optimal target variable recommendations for black-box models differ from classical car following models depending
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#32622;&#20449;&#21306;&#38388;&#38271;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07131</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#32479;&#35745;&#25512;&#26029;&#30340;&#37325;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resampling methods for Private Statistical Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07131
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24046;&#20998;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#12290;&#26041;&#27861;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#32622;&#20449;&#21306;&#38388;&#38271;&#24230;&#19978;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#26500;&#24314;&#32622;&#20449;&#21306;&#38388;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31169;&#26377;&#21464;&#20307;&#30340;&#38750;&#21442;&#25968;bootstrap&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#20998;&#21306;&#19978;&#31169;&#19979;&#35745;&#31639;&#22810;&#20010;&#8220;&#23567;&#8221;bootstrap&#30340;&#32467;&#26524;&#30340;&#20013;&#20301;&#25968;&#65292;&#24182;&#32473;&#20986;&#20102;&#24471;&#21040;&#30340;&#32622;&#20449;&#21306;&#38388;&#30340;&#28176;&#36827;&#35206;&#30422;&#35823;&#24046;&#19978;&#30028;&#12290;&#23545;&#20110;&#22266;&#23450;&#30340;&#24046;&#20998;&#38544;&#31169;&#21442;&#25968;&#949;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#22823;&#23567;n&#19978;&#30340;&#35823;&#24046;&#29575;&#19982;&#38750;&#31169;&#26377;bootstrap&#30456;&#24403;&#65292;&#21482;&#26159;&#22312;&#23545;&#25968;&#22240;&#23376;&#20869;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#22343;&#20540;&#20272;&#35745;&#12289;&#20013;&#20301;&#25968;&#20272;&#35745;&#21644;&#36923;&#36753;&#22238;&#24402;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32463;&#39564;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#20379;&#31867;&#20284;&#30340;&#35206;&#30422;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26174;&#33879;&#32553;&#30701;&#65288;&#22823;&#32422;10&#20493;&#65289;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple ``little'' bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#21019;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#26131;&#20110;&#29702;&#35299;&#30340;&#20195;&#25968;&#26041;&#27861;&#25512;&#23548;&#20986;&#20989;&#25968;&#20844;&#24335;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26500;&#24314;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#28508;&#31354;&#38388;&#37319;&#26679;&#29983;&#25104;&#20855;&#26377;&#38750;&#23545;&#31216;&#32467;&#26500;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07129</link><description>&lt;p&gt;
&#20174;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#20013;&#29983;&#25104;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#30340;&#23581;&#35797;
&lt;/p&gt;
&lt;p&gt;
An attempt to generate new bridge types from latent space of denoising diffusion Implicit model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23581;&#35797;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#21019;&#26032;&#26725;&#26753;&#31867;&#22411;&#65292;&#36890;&#36807;&#26131;&#20110;&#29702;&#35299;&#30340;&#20195;&#25968;&#26041;&#27861;&#25512;&#23548;&#20986;&#20989;&#25968;&#20844;&#24335;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26500;&#24314;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#28508;&#31354;&#38388;&#37319;&#26679;&#29983;&#25104;&#20855;&#26377;&#38750;&#23545;&#31216;&#32467;&#26500;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#36827;&#34892;&#26725;&#26753;&#21019;&#26032;&#12290;&#23558;&#22270;&#20687;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#30340;&#36807;&#31243;&#31867;&#27604;&#20026;&#23608;&#20307;&#33104;&#28866;&#21644;&#20390;&#25506;&#24674;&#22797;&#34987;&#26432;&#23475;&#30340;&#21463;&#23475;&#32773;&#29616;&#22330;&#30340;&#36807;&#31243;&#65292;&#20197;&#24110;&#21161;&#21021;&#23398;&#32773;&#29702;&#35299;&#12290;&#36890;&#36807;&#26131;&#20110;&#29702;&#35299;&#30340;&#20195;&#25968;&#26041;&#27861;&#65292;&#25512;&#23548;&#20986;&#28155;&#21152;&#22122;&#22768;&#21644;&#21435;&#22122;&#30340;&#20989;&#25968;&#20844;&#24335;&#65292;&#20351;&#21021;&#23398;&#32773;&#26356;&#23481;&#26131;&#25484;&#25569;&#27169;&#22411;&#30340;&#25968;&#23398;&#21407;&#29702;&#12290;&#20351;&#29992;&#19977;&#36328;&#26753;&#26725;&#12289;&#25329;&#26725;&#12289;&#26012;&#25289;&#26725;&#21644;&#24748;&#32034;&#26725;&#30340;&#23545;&#31216;&#32467;&#26500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;Python&#32534;&#31243;&#35821;&#35328;&#12289;TensorFlow&#21644;Keras&#28145;&#24230;&#23398;&#20064;&#24179;&#21488;&#26694;&#26550;&#65292;&#26500;&#24314;&#21644;&#35757;&#32451;&#20102;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#12290;&#20174;&#28508;&#31354;&#38388;&#37319;&#26679;&#20013;&#65292;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#38750;&#23545;&#31216;&#32467;&#26500;&#30340;&#26032;&#26725;&#26753;&#31867;&#22411;&#12290;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#21487;&#20197;&#22312;&#20154;&#31867;&#21407;&#22987;&#26725;&#26753;&#31867;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#26377;&#26426;&#22320;&#32452;&#21512;&#19981;&#21516;&#30340;&#32467;&#26500;&#32452;&#20214;&#65292;&#21019;&#36896;&#26032;&#30340;&#26725;&#26753;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use denoising diffusion implicit model for bridge-type innovation. The process of adding noise and denoising to an image can be likened to the process of a corpse rotting and a detective restoring the scene of a victim being killed, to help beginners understand. Through an easy-to-understand algebraic method, derive the function formulas for adding noise and denoising, making it easier for beginners to master the mathematical principles of the model. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , denoising diffusion implicit model is constructed and trained. From the latent space sampling, new bridge types with asymmetric structures can be generated. Denoising diffusion implicit model can organically combine different structural components on the basis of human original bridge types, and create new bridge types.
&lt;/p&gt;</description></item><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07118</link><description>&lt;p&gt;
&#19979;&#19968;&#20195;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#65306;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#24110;&#21161;&#36828;&#31243;&#22522;&#20110;&#26234;&#33021;&#25163;&#26426;&#30340;&#21672;&#35810;
&lt;/p&gt;
&lt;p&gt;
Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding Remote Smartphone-based Consultation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#30524;&#31185;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#36828;&#31243;&#30524;&#31185;&#21672;&#35810;&#65292;&#23588;&#20854;&#26159;&#22312;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#35813;&#31995;&#32479;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#29992;&#25143;&#25293;&#25668;&#30340;&#30524;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#29992;&#25143;&#25293;&#25668;&#22270;&#20687;&#36136;&#37327;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#26126;&#21644;&#20854;&#20182;&#30524;&#37096;&#30142;&#30149;&#26159;&#20840;&#29699;&#20851;&#27880;&#30340;&#20581;&#24247;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#21360;&#24230;&#31561;&#20302;&#25910;&#20837;&#21644;&#20013;&#31561;&#25910;&#20837;&#22269;&#23478;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#65292;&#36828;&#31243;&#30524;&#31185;&#35786;&#30103;&#25104;&#20026;&#19968;&#31181;&#29983;&#21629;&#32447;&#65292;&#24182;&#19988;&#26234;&#33021;&#25163;&#26426;&#30524;&#37096;&#25104;&#20687;&#30340; Grabi &#38468;&#20214;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#25293;&#25668;&#30340;&#22270;&#29255;&#36136;&#37327;&#24448;&#24448;&#19981;&#22815;&#22909;&#65292;&#38656;&#35201;&#21307;&#29983;&#23457;&#26680;&#24182;&#19988;&#20250;&#24310;&#35823;&#26102;&#38388;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#33021;&#22815;&#27169;&#25311;&#21307;&#29983;&#30340;&#21028;&#26029;&#24182;&#19988;&#33021;&#22815;&#21363;&#26102;&#21453;&#39304;&#65292;&#25105;&#20204;&#23545;&#24739;&#32773;&#25293;&#25668;&#30340;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23558;&#22797;&#26434;&#38382;&#39064;&#23618;&#27425;&#21270;&#65292;&#25105;&#20204;&#22312;&#27492;&#35299;&#20915;&#20102;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27010;&#24565;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blindness and other eye diseases are a global health concern, particularly in low- and middle-income countries like India. In this regard, during the COVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi attachment for smartphone-based eye imaging gained in use. However, quality of user-captured image often remained inadequate, requiring clinician vetting and delays. In this backdrop, we propose an AI-based quality assessment system with instant feedback mimicking clinicians' judgments and tested on patient-captured images. Dividing the complex problem hierarchically, here we tackle a nontrivial part, and demonstrate a proof of the concept.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37327;&#21270;&#20102;Adam&#30340;&#39044;&#26465;&#20214;&#25928;&#24212;&#65292;&#32467;&#26524;&#34920;&#26126;Adam&#33021;&#22815;&#20943;&#36731;&#30149;&#24577;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#20294;&#20250;&#21463;&#21040;&#32500;&#24230;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.07114</link><description>&lt;p&gt;
&#23545;Adam&#30340;&#39044;&#26465;&#20214;&#25928;&#24212;&#36827;&#34892;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Quantifying the Preconditioning Effect of Adam
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37327;&#21270;&#20102;Adam&#30340;&#39044;&#26465;&#20214;&#25928;&#24212;&#65292;&#32467;&#26524;&#34920;&#26126;Adam&#33021;&#22815;&#20943;&#36731;&#30149;&#24577;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#20294;&#20250;&#21463;&#21040;&#32500;&#24230;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;Adam&#30340;&#39044;&#26465;&#20214;&#25928;&#24212;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#37327;&#21270;&#20102;Adam&#22312;&#20943;&#36731;&#30149;&#24577;&#26465;&#20214;&#65288;&#22256;&#25200;&#26799;&#24230;&#19979;&#38477;&#27861;&#65289;&#19978;&#30340;&#20316;&#29992;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;Adam&#22312;&#30149;&#24577;&#26465;&#20214;&#19978;&#33021;&#22815;&#20943;&#23569;&#20381;&#36182;&#20110;Hessian&#30697;&#38453;&#26465;&#20214;&#25968;&#30340;&#31243;&#24230;&#65292;&#20294;&#20195;&#20215;&#26159;&#20250;&#21463;&#21040;&#19982;&#32500;&#24230;&#26377;&#20851;&#30340;&#22240;&#32032;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377;&#23545;&#35282;Hessian&#30697;&#38453;&#12289;&#26465;&#20214;&#25968;&#20026;&#954;&#30340;d&#32500;&#20108;&#27425;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#21160;&#37327;&#30340;Adam&#20013;&#65292;&#25511;&#21046;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#26377;&#25928;&#26465;&#20214;&#25968;&#31867;&#20284;&#37327;&#20026;O(min(d, &#954;))&#12290;&#23545;&#20110;&#19968;&#20010;&#23545;&#35282;&#21344;&#20248;&#30340;Hessian&#30697;&#38453;&#65292;&#25105;&#20204;&#33719;&#24471;&#30456;&#24212;&#37327;&#30340;&#19978;&#30028;&#20026;O(min(d&#8730;(d&#954;), &#954;))&#12290;&#22240;&#27492;&#65292;&#24403;d &lt; O(&#954;^p)&#65292;&#20854;&#20013;p = 1&#36866;&#29992;&#20110;&#23545;&#35282;Hessian&#30697;&#38453;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#36825;&#31181;&#37327;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a notable dearth of results characterizing the preconditioning effect of Adam and showing how it may alleviate the curse of ill-conditioning -- an issue plaguing gradient descent (GD). In this work, we perform a detailed analysis of Adam's preconditioning effect for quadratic functions and quantify to what extent Adam can mitigate the dependence on the condition number of the Hessian. Our key finding is that Adam can suffer less from the condition number but at the expense of suffering a dimension-dependent quantity. Specifically, for a $d$-dimensional quadratic with a diagonal Hessian having condition number $\kappa$, we show that the effective condition number-like quantity controlling the iteration complexity of Adam without momentum is $\mathcal{O}(\min(d, \kappa))$. For a diagonally dominant Hessian, we obtain a bound of $\mathcal{O}(\min(d \sqrt{d \kappa}, \kappa))$ for the corresponding quantity. Thus, when $d &lt; \mathcal{O}(\kappa^p)$ where $p = 1$ for a diagonal Hessia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2402.07108</link><description>&lt;p&gt;
&#35299;&#32806;&#23398;&#20064;&#21644;&#20915;&#31574;&#65306;&#29992;&#19968;&#38454;&#26041;&#27861;&#31361;&#30772;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;$\mathcal{O}(\sqrt{T})$&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#22312;&#25910;&#30410;&#31649;&#29702;&#21644;&#36164;&#28304;&#20998;&#37197;&#20043;&#38388;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#19968;&#38454;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#23613;&#31649;&#19968;&#38454;&#26041;&#27861;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#33021;&#23454;&#29616;$\mathcal{O}(\sqrt{T})$&#30340;&#36951;&#25022;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;(LP)&#30340;&#22312;&#32447;&#31639;&#27861;&#25152;&#20445;&#35777;&#30340;$\mathcal{O}(\log T)$&#30028;&#38480;&#30456;&#27604;&#26159;&#27425;&#20248;&#30340;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20851;&#20110;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#20960;&#20010;&#37325;&#35201;&#20107;&#23454;&#65292;&#25581;&#31034;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#19982;&#20915;&#31574;&#20998;&#31163;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#36825;&#20010;&#26032;&#26694;&#26550;&#19979;&#21487;&#20197;&#36798;&#21040;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve a regret no better than $\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes several important facts about online linear programming, which unveils the challenge for first-order-method-based online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address the challenge, we introduce a new algorithmic framework that decouples learning from decision-making. More importantly, for the first time, we show that first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new framework. Lastly, we conduct numerical experiments to validate our theoretical find
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.07107</link><description>&lt;p&gt;
&#32034;crates&#24576;&#30097;&#30340;&#22238;&#22768;&#65306;&#22312;&#26657;&#20934;&#30340;&#35777;&#25454;&#22686;&#24378;&#23398;&#20064;&#20013;&#25509;&#21463;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Echoes of Socratic Doubt: Embracing Uncertainty in Calibrated Evidential Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07107
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;Q&#32593;&#32476;&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#26469;&#22312;&#27169;&#22411;-free&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#21644;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#65292;&#25552;&#20379;&#20102;&#20840;&#23616;&#19981;&#30830;&#23450;&#24615;&#30340;&#26174;&#24335;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#25104;&#21151;&#22788;&#29702;&#20102;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22522;&#20110;&#27169;&#22411;&#30340;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#28041;&#21450;&#22522;&#20110;&#20998;&#20301;&#25968;&#22238;&#24402;&#30340;&#28145;&#24230;Q&#32593;&#32476;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;$\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$&#26088;&#22312;&#35299;&#20915;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#20998;&#21035;&#20272;&#35745;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25152;&#38754;&#20020;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#23427;&#23558;&#28145;&#24230;&#35777;&#25454;&#23398;&#20064;&#19982;&#22522;&#20110;&#21512;&#35268;&#25512;&#29702;&#21407;&#21017;&#30340;&#20998;&#20301;&#25968;&#26657;&#20934;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#26174;&#24335;&#30340;&#12289;&#26080;&#26679;&#26412;&#35745;&#31639;&#30340;$\textit{&#20840;&#23616;}$&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#26159;&#22522;&#20110;&#31616;&#21333;&#26041;&#24046;&#30340;$\textit{&#23616;&#37096;}$&#20272;&#35745;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#20197;&#21450;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#35266;&#27979;&#25968;&#25454;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#19968;&#22871;&#23567;&#22411;&#21270;&#30340;Atari&#28216;&#25103;&#65288;&#21363;MinAtar&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;CEQR-DQN&#22312;&#24471;&#20998;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#31867;&#20284;&#30340;&#29616;&#26377;&#26694;&#26550;&#12290;&#23427;&#33021;&#22815;&#20005;&#35880;&#22320;&#22788;&#29702;&#22806;&#37096;&#25968;&#25454;&#35266;&#27979;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel statistical approach to incorporating uncertainty awareness in model-free distributional reinforcement learning involving quantile regression-based deep Q networks. The proposed algorithm, $\textit{Calibrated Evidential Quantile Regression in Deep Q Networks (CEQR-DQN)}$, aims to address key challenges associated with separately estimating aleatoric and epistemic uncertainty in stochastic environments. It combines deep evidential learning with quantile calibration based on principles of conformal inference to provide explicit, sample-free computations of $\textit{global}$ uncertainty as opposed to $\textit{local}$ estimates based on simple variance, overcoming limitations of traditional methods in computational and statistical efficiency and handling of out-of-distribution (OOD) observations. Tested on a suite of miniaturized Atari games (i.e., MinAtar), CEQR-DQN is shown to surpass similar existing frameworks in scores and learning speed. Its ability to rigorously e
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#39044;&#27979;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#23398;&#20064; History Representation &#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07102</link><description>&lt;p&gt;
&#26410;&#26469;&#39044;&#27979;&#21487;&#20197;&#25104;&#20026;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#33391;&#22909;&#21382;&#21490;&#34920;&#36798;&#30340;&#26377;&#21147;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Future Prediction Can be a Strong Evidence of Good History Representation in Partially Observable Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07102
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#39044;&#27979;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#23398;&#20064; History Representation &#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#33391;&#22909;&#30340;&#21382;&#21490;&#34920;&#36798;&#26159;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26680;&#24515;&#25361;&#25112;&#20043;&#19968;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21508;&#31181;&#36741;&#21161;&#20219;&#21153;&#23545;&#20419;&#36827;&#34920;&#36798;&#23398;&#20064;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36741;&#21161;&#20219;&#21153;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#23436;&#20840;&#20351;&#20154;&#20449;&#26381;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#38271;&#26399;&#35760;&#24518;&#21644;&#25512;&#29702;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26410;&#26469;&#39044;&#27979;&#22312;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#21382;&#21490;&#34920;&#36798;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26410;&#26469;&#39044;&#27979;&#23558;&#23398;&#20064;&#21382;&#21490;&#34920;&#36798;&#19982;&#31574;&#30053;&#20248;&#21270;&#20998;&#31163;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;a&#65289;&#25105;&#20204;&#35777;&#26126;&#20102;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#19982;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#26410;&#26469;&#35266;&#27979;&#30340;&#39044;&#27979;&#31934;&#24230;&#24378;&#30456;&#20851;&#65292;&#65288;b&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#37096;&#20998;&#21487;&#35266;&#27979;&#29615;&#22659;&#20013;&#38271;&#26102;&#38388;&#21382;&#21490;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a good history representation is one of the core challenges of reinforcement learning (RL) in partially observable environments. Recent works have shown the advantages of various auxiliary tasks for facilitating representation learning. However, the effectiveness of such auxiliary tasks has not been fully convincing, especially in partially observable environments that require long-term memorization and inference. In this empirical study, we investigate the effectiveness of future prediction for learning the representations of histories, possibly of extensive length, in partially observable environments. We first introduce an approach that decouples the task of learning history representations from policy optimization via future prediction. Then, our main contributions are two-fold: (a) we demonstrate that the performance of reinforcement learning is strongly correlated with the prediction accuracy of future observations in partially observable environments, and (b) our approa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#20013;&#20351;&#29992;&#19968;&#38454;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;$O(\epsilon^{-6})$&#27425;&#36845;&#20195;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;$\epsilon$&#31283;&#23450;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.07101</link><description>&lt;p&gt;
&#35770;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#20013;&#19968;&#38454;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Complexity of First-Order Methods in Stochastic Bilevel Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#21452;&#23618;&#20248;&#21270;&#20013;&#20351;&#29992;&#19968;&#38454;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;$O(\epsilon^{-6})$&#27425;&#36845;&#20195;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;$\epsilon$&#31283;&#23450;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#19979;&#23618;&#38382;&#39064;&#26080;&#32422;&#26463;&#19988;&#24378;&#20984;&#30340;&#26465;&#20214;&#19979;&#65292;&#23547;&#25214;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#31283;&#23450;&#28857;&#12290;&#35813;&#38382;&#39064;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65307;&#20027;&#35201;&#30340;&#25216;&#26415;&#25361;&#25112;&#26159;&#36319;&#36394;&#19979;&#23618;&#35299;$y^*(x)$&#23545;&#19978;&#23618;&#21464;&#37327;$x$&#30340;&#21464;&#21270;&#12290;&#29616;&#26377;&#26041;&#27861;&#37117;&#23558;&#20998;&#26512;&#32467;&#26524;&#19982;&#19968;&#20010;&#33021;&#22815;&#30693;&#36947;&#19979;&#23618;&#35299;&#30340;&#31070;&#31639;&#27861;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#26597;&#35810;&#31163;&#36825;&#20123;&#35299;&#22826;&#36828;&#30340;&#28857;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#23545;&#20598;&#38382;&#39064;&#65306;&#20551;&#35774;&#25105;&#20204;&#26377;&#19968;&#20010;&#34987;&#31216;&#20026;$y^*$-&#24863;&#30693;&#30340;&#39044;&#35328;&#26426;&#65292;&#23427;&#36820;&#22238;&#19968;&#20010;$O(\epsilon)$&#30340;&#19979;&#23618;&#35299;&#20272;&#35745;&#65292;&#24182;&#19988;&#25552;&#20379;&#22312;&#36317;&#31163;$y^*(x)$&#30340;$\Theta(\epsilon)$&#29699;&#20869;&#22788;&#30340;&#19968;&#38454;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#23616;&#37096;&#26080;&#20559;&#20272;&#35745;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#26679;&#30340;$y^*$-&#24863;&#30693;&#39044;&#35328;&#26426;&#23547;&#25214;&#31283;&#23450;&#28857;&#30340;&#22797;&#26434;&#24615;&#65306;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19968;&#38454;&#26041;&#27861;&#65292;&#23427;&#25910;&#25947;&#21040;&#19968;&#20010;$\epsilon$&#31283;&#23450;&#28857;&#65292;&#20351;&#29992;$O(\epsilon^{-6})$&#30340;&#27425;&#25968;&#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of finding stationary points in Bilevel optimization when the lower-level problem is unconstrained and strongly convex. The problem has been extensively studied in recent years; the main technical challenge is to keep track of lower-level solutions $y^*(x)$ in response to the changes in the upper-level variables $x$. Subsequently, all existing approaches tie their analyses to a genie algorithm that knows lower-level solutions and, therefore, need not query any points far from them. We consider a dual question to such approaches: suppose we have an oracle, which we call $y^*$-aware, that returns an $O(\epsilon)$-estimate of the lower-level solution, in addition to first-order gradient estimators {\it locally unbiased} within the $\Theta(\epsilon)$-ball around $y^*(x)$. We study the complexity of finding stationary points with such an $y^*$-aware oracle: we propose a simple first-order method that converges to an $\epsilon$ stationary point using $O(\epsilon^{-6})
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20998;&#25903;&#31574;&#30053;&#20013;&#30340;&#23481;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#28040;&#24687;&#20256;&#36882;GNN (MP-GNN) &#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#21478;&#19968;&#31181;GNN&#32467;&#26500; second-order folklore GNN (2-FGNN) &#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.07099</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#25903;&#31574;&#30053;&#20013;&#30340;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Capacity of Graph Neural Networks for Branching Strategy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#20998;&#25903;&#31574;&#30053;&#20013;&#30340;&#23481;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;&#28040;&#24687;&#20256;&#36882;GNN (MP-GNN) &#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#21478;&#19968;&#31181;GNN&#32467;&#26500; second-order folklore GNN (2-FGNN) &#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#39044;&#27979;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILPs&#65289;&#30340;&#23646;&#24615;&#21644;&#21551;&#21457;&#24335;&#65292;&#24182;&#21152;&#36895;MILP&#27714;&#35299;&#22120;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;GNNs&#22312;&#34920;&#31034;&#25552;&#20379;&#20998;&#25903;&#38480;&#30028;&#31639;&#27861;&#20013;&#39640;&#25928;&#31574;&#30053;&#30340;&#24378;&#20998;&#25903;&#65288;SB&#65289;&#24471;&#20998;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#29616;&#26377;&#25991;&#29486;&#20013;&#32463;&#24120;&#20351;&#29992;&#26368;&#31616;&#21333;&#30340;&#28040;&#24687;&#20256;&#36882;GNN&#65288;MP-GNN&#65289;&#26469;&#23398;&#20064;SB&#24471;&#20998;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#34920;&#36798;&#33021;&#21147;&#30340;&#19968;&#20010;&#26681;&#26412;&#23616;&#38480;&#24615;--&#23384;&#22312;&#20004;&#20010;&#19981;&#21516;SB&#24471;&#20998;&#30340;MILP&#23454;&#20363;&#65292;&#26080;&#35770;&#21442;&#25968;&#30340;&#25968;&#37327;&#22914;&#20309;&#65292;&#37117;&#26080;&#27861;&#36890;&#36807;&#20219;&#20309;MP-GNN&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#36924;&#36817;&#23450;&#29702;&#65292;&#29992;&#20110;&#21478;&#19968;&#31181;GNN&#32467;&#26500;&#31216;&#20026;second-order folklore GNN&#65288;2-FGNN&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20219;&#20309;MILP&#25968;&#25454;&#20998;&#24067;&#65292;&#24635;&#26159;&#23384;&#22312;&#19968;&#20010;&#21487;&#20197;&#20197;&#20219;&#24847;&#39640;&#31934;&#24230;&#21644;&#20219;&#24847;&#39640;&#27010;&#29575;&#36924;&#36817;SB&#24471;&#20998;&#30340;2-FGNN&#12290;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#25968;&#20540;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have been widely used to predict properties and heuristics of mixed-integer linear programs (MILPs) and hence accelerate MILP solvers. This paper investigates the capacity of GNNs to represent strong branching (SB) scores that provide an efficient strategy in the branch-and-bound algorithm.   Although message-passing GNN (MP-GNN), as the simplest GNN structure, is frequently employed in the existing literature to learn SB scores, we prove a fundamental limitation in its expressive power -- there exist two MILP instances with different SB scores that cannot be distinguished by any MP-GNN, regardless of the number of parameters. In addition, we establish a universal approximation theorem for another GNN structure called the second-order folklore GNN (2-FGNN). We show that for any data distribution over MILPs, there always exists a 2-FGNN that can approximate the SB score with arbitrarily high accuracy and arbitrarily high probability. A small-scale numerical 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07087</link><description>&lt;p&gt;
&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Self-Correcting Self-Consuming Loops for Generative Model Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24341;&#20837;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#26469;&#31283;&#23450;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#26469;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#25968;&#25454;&#22312;&#20114;&#32852;&#32593;&#19978;&#30340;&#36136;&#37327;&#36234;&#26469;&#36234;&#39640;&#20197;&#21450;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#22312;&#20154;&#24037;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#28151;&#21512;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#23613;&#31649;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#34920;&#24449;&#23398;&#20064;&#30340;&#25104;&#21151;&#26696;&#20363;&#26377;&#24456;&#22810;&#65292;&#20294;&#26159;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#20250;&#20135;&#29983;"&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;"&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35757;&#32451;&#19981;&#31283;&#23450;&#29978;&#33267;&#23849;&#28291;&#65292;&#38500;&#38750;&#28385;&#36275;&#26576;&#20123;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#31283;&#23450;&#33258;&#25105;&#28040;&#32791;&#30340;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#65292;&#23558;&#25968;&#25454;&#28857;&#26144;&#23556;&#20026;&#26356;&#26377;&#21487;&#33021;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#21487;&#20197;&#20351;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#30340;&#31283;&#23450;&#24615;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#20462;&#27491;&#20989;&#25968;&#65292;&#23427;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#32534;&#31243;&#22312;&#27169;&#25311;&#22120;&#20013;&#30340;&#29289;&#29702;&#23450;&#24459;&#65289;&#65292;&#24182;&#19988;&#26088;&#22312;&#33258;&#21160;&#19988;&#22823;&#35268;&#27169;&#22320;&#36817;&#20284;&#29702;&#24819;&#30340;&#20462;&#27491;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;&#33258;&#25105;&#32416;&#27491;&#33258;&#25105;&#28040;&#32791;&#24490;&#29615;&#22312;&#29983;&#25104;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As synthetic data becomes higher quality and proliferates on the internet, machine learning models are increasingly trained on a mix of human- and machine-generated data. Despite the successful stories of using synthetic data for representation learning, using synthetic data for generative model training creates "self-consuming loops" which may lead to training instability or even collapse, unless certain conditions are met. Our paper aims to stabilize self-consuming generative model training. Our theoretical results demonstrate that by introducing an idealized correction function, which maps a data point to be more likely under the true data distribution, self-consuming loops can be made exponentially more stable. We then propose self-correction functions, which rely on expert knowledge (e.g. the laws of physics programmed in a simulator), and aim to approximate the idealized corrector automatically and at scale. We empirically validate the effectiveness of self-correcting self-consum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#38899;&#32032;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20010;&#20307;&#21457;&#38899;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.07085</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#20013;&#20174;&#38899;&#32032;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Speech Rhythm-Based Speaker Embeddings Extraction from Phonemes and Phoneme Duration for Multi-Speaker Speech Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#38899;&#32032;&#21644;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#65292;&#27169;&#25311;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#20010;&#20307;&#21457;&#38899;&#29305;&#24449;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#38901;&#24459;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#30446;&#26631;&#35828;&#35805;&#20154;&#30340;&#23569;&#37327;&#21477;&#23376;&#20013;&#24314;&#27169;&#38899;&#32032;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#25552;&#21462;&#35828;&#35805;&#20154;&#23884;&#20837;&#12290;&#35821;&#38899;&#38901;&#24459;&#26159;&#19982;&#35828;&#35805;&#20154;&#29305;&#24449;&#30456;&#20851;&#30340;&#37325;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#19982;&#22522;&#39057;&#31561;&#22768;&#23398;&#29305;&#24449;&#19968;&#36215;&#29992;&#20110;&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#37325;&#29616;&#21333;&#20010;&#21477;&#23376;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#29305;&#28857;&#26159;&#22522;&#20110;&#38901;&#24459;&#30340;&#23884;&#20837;&#65292;&#20174;&#24050;&#30693;&#19982;&#35828;&#35805;&#38901;&#24459;&#30456;&#20851;&#30340;&#38899;&#32032;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#20013;&#25552;&#21462;&#65292;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#39057;&#35889;&#29305;&#24449;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#21253;&#25324;&#29983;&#25104;&#35828;&#35805;&#20154;&#23884;&#20837;&#12289;&#20351;&#29992;&#29983;&#25104;&#30340;&#23884;&#20837;&#36827;&#34892;&#35821;&#38899;&#21512;&#25104;&#20197;&#21450;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#65292;&#20197;&#35780;&#20272;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#38899;&#32032;&#21450;&#20854;&#25345;&#32493;&#26102;&#38388;&#20449;&#24687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20063;&#23637;&#29616;&#20102;&#36739;&#20026;&#36866;&#20013;&#30340;&#35828;&#35805;&#20154;&#35782;&#21035;&#24615;&#33021;&#65288;15.2% EER&#65289;&#12290;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a speech rhythm-based method for speaker embeddings to model phoneme duration using a few utterances by the target speaker. Speech rhythm is one of the essential factors among speaker characteristics, along with acoustic features such as F0, for reproducing individual utterances in speech synthesis. A novel feature of the proposed method is the rhythm-based embeddings extracted from phonemes and their durations, which are known to be related to speaking rhythm. They are extracted with a speaker identification model similar to the conventional spectral feature-based one. We conducted three experiments, speaker embeddings generation, speech synthesis with generated embeddings, and embedding space analysis, to evaluate the performance. The proposed method demonstrated a moderate speaker identification performance (15.2% EER), even with only phonemes and their duration information. The objective and subjective evaluation results demonstrated that the proposed method can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#25913;&#36827;AVLPR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#24754;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07082</link><description>&lt;p&gt;
&#22522;&#20110;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Refined Sample Complexity for Markov Games with Independent Linear Function Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29420;&#31435;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#20013;&#65292;&#36890;&#36807;&#25913;&#36827;AVLPR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#25968;&#25454;&#20381;&#36182;&#30340;&#24754;&#35266;&#20272;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65288;MG&#65289;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#37325;&#35201;&#27169;&#22411;&#12290;&#38271;&#26399;&#20197;&#26469;&#20154;&#20204;&#19968;&#30452;&#35748;&#20026;&#8220;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#8221;&#65288;&#21363;&#31639;&#27861;&#24615;&#33021;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#25351;&#25968;&#32423;&#19979;&#38477;&#65289;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#65292;&#30452;&#21040;&#26368;&#36817;&#20960;&#31687;&#20316;&#21697;&#65288;Daskalakis&#31561;&#20154;&#65292;2023&#24180;&#65307;Cui&#31561;&#20154;&#65292;2023&#24180;&#65307;Wang&#31561;&#20154;&#65292;2023&#24180;&#65289;&#12290;&#36825;&#20123;&#20316;&#21697;&#30830;&#23454;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#30340;&#35781;&#21650;&#65292;&#24403;&#29366;&#24577;&#31354;&#38388;&#26497;&#22823;&#19988;&#65288;&#32447;&#24615;&#65289;&#20989;&#25968;&#36924;&#36817;&#34987;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#35201;&#20040;&#20855;&#26377;&#26356;&#24930;&#30340;&#25910;&#25947;&#36895;&#24230;$O(T^{-1/4})$&#65292;&#35201;&#20040;&#22312;&#34892;&#21160;&#25968;$A_{\max}$&#19978;&#24102;&#26469;&#22810;&#39033;&#24335;&#20381;&#36182;&#8212;&#8212;&#23613;&#31649;&#22312;&#21333;&#26234;&#33021;&#20307;&#24773;&#20917;&#19979;&#21363;&#20351;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#38543;&#26102;&#38388;&#20219;&#24847;&#21464;&#21270;&#65288;Dai&#31561;&#20154;&#65292;2023&#24180;&#65289;&#65292;&#20063;&#21487;&#36991;&#20813;&#36825;&#31181;&#20381;&#36182;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;Wang&#31561;&#20154;&#65288;2023&#24180;&#65289;&#30340;&#8220;AVLPR&#8221;&#26694;&#26550;&#31934;&#21270;&#65292;&#27934;&#23519;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#65288;&#21363;&#38543;&#26426;&#30340;&#65289;&#24754;&#35266;&#20272;&#35745;&#23376;&#20248;&#21270;&#24046;&#36317;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#24191;&#27867;&#30340;&#25554;&#20214;&#31639;&#27861;&#36873;&#25321;&#12290;&#24403;&#19987;&#38376;&#24212;&#29992;&#20110;MGs&#26102;&#65292;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#29420;&#31435;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Markov Games (MG) is an important model for Multi-Agent Reinforcement Learning (MARL). It was long believed that the "curse of multi-agents" (i.e., the algorithmic performance drops exponentially with the number of agents) is unavoidable until several recent works (Daskalakis et al., 2023; Cui et al., 2023; Wang et al., 2023. While these works did resolve the curse of multi-agents, when the state spaces are prohibitively large and (linear) function approximations are deployed, they either had a slower convergence rate of $O(T^{-1/4})$ or brought a polynomial dependency on the number of actions $A_{\max}$ -- which is avoidable in single-agent cases even when the loss functions can arbitrarily vary with time (Dai et al., 2023). This paper first refines the `AVLPR` framework by Wang et al. (2023), with an insight of *data-dependent* (i.e., stochastic) pessimistic estimation of the sub-optimality gap, allowing a broader choice of plug-in algorithms. When specialized to MGs with independent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RFVM&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#21069;&#30651;&#24615;&#30740;&#31350;&#20013;&#30340;&#22823;&#25968;&#25454;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#20844;&#24335;&#12289;&#32852;&#21512;&#20248;&#21270;&#21644;&#38598;&#25104;&#20462;&#21098;&#31561;&#29305;&#28857;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07079</link><description>&lt;p&gt;
&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#30456;&#20851;&#29305;&#24449;&#19982;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
The Relevance Feature and Vector Machine for health applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RFVM&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#21069;&#30651;&#24615;&#30740;&#31350;&#20013;&#30340;&#22823;&#25968;&#25454;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#20844;&#24335;&#12289;&#32852;&#21512;&#20248;&#21270;&#21644;&#38598;&#25104;&#20462;&#21098;&#31561;&#29305;&#28857;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#8212;&#8212;&#30456;&#20851;&#29305;&#24449;&#19982;&#21521;&#37327;&#26426;&#65288;RFVM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#20020;&#24202;&#21069;&#30651;&#24615;&#30740;&#31350;&#20013;&#22788;&#29702;&#22823;&#25968;&#25454;&#38382;&#39064;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#22823;&#25968;&#25454;&#38382;&#39064;&#26159;&#25351;&#24403;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19982;&#29305;&#24449;&#25968;&#36828;&#22823;&#20110;&#26679;&#26412;&#25968;&#30340;&#25968;&#25454;&#24211;&#19968;&#36215;&#24037;&#20316;&#26102;&#30340;&#38480;&#21046;&#65288;&#22312;&#26576;&#20123;&#21307;&#23398;&#39046;&#22495;&#20013;&#26159;&#24120;&#35265;&#24773;&#20917;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;RFVM&#34701;&#21512;&#20102;&#20197;&#19979;&#20960;&#20010;&#29305;&#28857;&#65306;&#65288;1&#65289;&#36125;&#21494;&#26031;&#20844;&#24335;&#65292;&#20351;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#30340;&#25512;&#26029;&#19979;&#33021;&#22815;&#36991;&#20813;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#65288;2&#65289;&#32852;&#21512;&#20248;&#21270;&#65292;&#36890;&#36807;&#21516;&#26102;&#24341;&#20837;&#23450;&#20041;&#21407;&#22987;&#31354;&#38388;&#65288;&#29305;&#24449;&#65289;&#21644;&#23545;&#20598;&#31354;&#38388;&#65288;&#35266;&#27979;&#65289;&#30340;&#21464;&#37327;&#65292;&#20811;&#26381;&#20102;&#22823;&#25968;&#25454;&#29305;&#24449;&#25152;&#24102;&#26469;&#30340;&#38480;&#21046;&#12290;&#65288;3&#65289;&#38598;&#25104;&#20462;&#21098;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#36845;&#20195;&#20248;&#21270;&#26399;&#38388;&#31227;&#38500;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#21644;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#26368;&#21518;&#36825;&#20010;&#20462;&#21098;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Relevance Feature and Vector Machine (RFVM), a novel model that addresses the challenges of the fat-data problem when dealing with clinical prospective studies. The fat-data problem refers to the limitations of Machine Learning (ML) algorithms when working with databases in which the number of features is much larger than the number of samples (a common scenario in certain medical fields). To overcome such limitations, the RFVM incorporates different characteristics: (1) A Bayesian formulation which enables the model to infer its parameters without overfitting thanks to the Bayesian model averaging. (2) A joint optimisation that overcomes the limitations arising from the fat-data characteristic by simultaneously including the variables that define the primal space (features) and those that define the dual space (observations). (3) An integrated prunning that removes the irrelevant features and samples during the training iterative optimization. Also, this last p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;&#26426;&#26469;&#32534;&#30721;&#39640;&#32423;&#30693;&#35782;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07069</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#21152;&#36895;&#22870;&#21169;&#26426;&#22120;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Automate and Expedite Reinforcement Learning with Reward Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33258;&#21160;&#26426;&#26469;&#32534;&#30721;&#39640;&#32423;&#30693;&#35782;&#65292;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LARL-RM&#65288;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#29992;&#20110;&#22870;&#21169;&#26426;&#22120;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#26426;&#65289;&#31639;&#27861;&#65292;&#20197;&#23558;&#39640;&#32423;&#30693;&#35782;&#32534;&#30721;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20351;&#29992;&#33258;&#21160;&#26426;&#21152;&#36895;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#33719;&#24471;&#39640;&#32423;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#23558;&#39640;&#32423;&#30693;&#35782;&#25552;&#20379;&#32473;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#38656;&#35201;&#19987;&#23478;&#26469;&#32534;&#30721;&#33258;&#21160;&#26426;&#12290;&#25105;&#20204;&#20351;&#29992;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#27861;&#19979;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;LARL-RM&#20801;&#35768;&#23436;&#20840;&#38381;&#29615;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#26080;&#38656;&#19987;&#23478;&#26469;&#25351;&#23548;&#21644;&#30417;&#30563;&#23398;&#20064;&#65292;&#22240;&#20026;LARL-RM&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;LLM&#29983;&#25104;&#25152;&#38656;&#30340;&#39640;&#32423;&#30693;&#35782;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#31639;&#27861;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;LARL-RM&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#20854;&#23545;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LARL-RM (Large language model-generated Automaton for Reinforcement Learning with Reward Machine) algorithm in order to encode high-level knowledge into reinforcement learning using automaton to expedite the reinforcement learning. Our method uses Large Language Models (LLM) to obtain high-level domain-specific knowledge using prompt engineering instead of providing the reinforcement learning algorithm directly with the high-level knowledge which requires an expert to encode the automaton. We use chain-of-thought and few-shot methods for prompt engineering and demonstrate that our method works using these approaches. Additionally, LARL-RM allows for fully closed-loop reinforcement learning without the need for an expert to guide and supervise the learning since LARL-RM can use the LLM directly to generate the required high-level knowledge for the task at hand. We also show the theoretical guarantee of our algorithm to converge to an optimal policy. We demonstrate that LARL-R
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#20013;&#20005;&#26684;&#20984;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.07067</link><description>&lt;p&gt;
&#23398;&#20064;&#20005;&#26684;&#20984;&#30340;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#30340;&#39044;&#26399;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Learning the Expected Core of Strictly Convex Stochastic Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#20013;&#20005;&#26684;&#20984;&#24773;&#20917;&#19979;&#65292;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#20998;&#37197;&#65292;&#20063;&#31216;&#20026;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#65292;&#26159;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#20449;&#29992;&#20998;&#37197;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#27010;&#24565;&#26159;&#26680;&#24515;&#65292;&#23427;&#26159;&#31283;&#23450;&#20998;&#37197;&#30340;&#38598;&#21512;&#65292;&#20854;&#20013;&#27809;&#26377;&#20195;&#29702;&#26377;&#21160;&#26426;&#20174;&#22823;&#32852;&#30431;&#20013;&#20559;&#31163;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#38543;&#26426;&#21512;&#20316;&#21338;&#24328;&#30340;&#31283;&#23450;&#20998;&#37197;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#34987;&#25551;&#36848;&#20026;&#20855;&#26377;&#26410;&#30693;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#36820;&#22238;&#26597;&#35810;&#32852;&#30431;&#30340;&#38543;&#26426;&#22870;&#21169;&#30340;oracle&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#39044;&#26399;&#26680;&#24515;&#65292;&#21363;&#22312;&#26399;&#26395;&#19978;&#31283;&#23450;&#30340;&#20998;&#37197;&#38598;&#21512;&#12290;&#22312;&#20005;&#26684;&#20984;&#21338;&#24328;&#31867;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\texttt{Common-Points-Picking}&#30340;&#31639;&#27861;&#65292;&#23427;&#22312;&#22810;&#39033;&#24335;&#25968;&#37327;&#30340;&#26679;&#26412;&#32473;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#39640;&#27010;&#29575;&#36820;&#22238;&#19968;&#20010;&#31283;&#23450;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20998;&#26512;&#28041;&#21450;&#21040;&#20984;&#20960;&#20309;&#20013;&#30340;&#20960;&#20010;&#26032;&#32467;&#26524;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Reward allocation, also known as the credit assignment problem, has been an important topic in economics, engineering, and machine learning. An important concept in credit assignment is the core, which is the set of stable allocations where no agent has the motivation to deviate from the grand coalition. In this paper, we consider the stable allocation learning problem of stochastic cooperative games, where the reward function is characterised as a random variable with an unknown distribution. Given an oracle that returns a stochastic reward for an enquired coalition each round, our goal is to learn the expected core, that is, the set of allocations that are stable in expectation. Within the class of strictly convex games, we present an algorithm named \texttt{Common-Points-Picking} that returns a stable allocation given a polynomial number of samples, with high probability. The analysis of our algorithm involves the development of several new results in convex geometry, including an e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#30340;&#24046;&#20998;&#38544;&#31169;&#33539;&#22260;&#26597;&#35810;&#30340;&#23616;&#37096;&#26426;&#21046;&#65292;&#36890;&#36807;&#32423;&#32852;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#20445;&#38556;&#36817;&#20046;&#26368;&#20248;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#19982;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07066</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#30340;&#24046;&#20998;&#38544;&#31169;&#33539;&#22260;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Range Queries with Correlated Input Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#30340;&#24046;&#20998;&#38544;&#31169;&#33539;&#22260;&#26597;&#35810;&#30340;&#23616;&#37096;&#26426;&#21046;&#65292;&#36890;&#36807;&#32423;&#32852;&#37319;&#26679;&#31639;&#27861;&#23454;&#29616;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#20445;&#38556;&#36817;&#20046;&#26368;&#20248;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#19982;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32447;&#24615;&#26597;&#35810;&#30340;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#29305;&#21035;&#26159;&#33539;&#22260;&#26597;&#35810;&#65292;&#21033;&#29992;&#30456;&#20851;&#36755;&#20837;&#25200;&#21160;&#21516;&#26102;&#23454;&#29616;&#26080;&#20559;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#32479;&#35745;&#36879;&#26126;&#24615;&#21644;&#23545;&#31934;&#24230;&#30446;&#26631;&#30340;&#25511;&#21046;&#65292;&#26080;&#35770;&#26159;&#22312;&#26576;&#20123;&#26597;&#35810;&#36793;&#32536;&#19978;&#36824;&#26159;&#22312;&#23618;&#27425;&#25968;&#25454;&#24211;&#32467;&#26500;&#25152;&#26263;&#31034;&#30340;&#31934;&#24230;&#35201;&#27714;&#19978;&#12290;&#25152;&#25552;&#20986;&#30340;&#32423;&#32852;&#37319;&#26679;&#31639;&#27861;&#20934;&#30830;&#39640;&#25928;&#22320;&#23454;&#29616;&#20102;&#35813;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#30028;&#38480;&#34920;&#26126;&#65292;&#25105;&#20204;&#22312;&#20445;&#38556;&#36817;&#20046;&#26368;&#20248;&#30340;&#25928;&#29992;&#30340;&#21516;&#26102;&#65292;&#19982;&#36755;&#20986;&#25200;&#21160;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a class of locally differentially private mechanisms for linear queries, in particular range queries, that leverages correlated input perturbation to simultaneously achieve unbiasedness, consistency, statistical transparency, and control over utility requirements in terms of accuracy targets expressed either in certain query margins or as implied by the hierarchical database structure. The proposed Cascade Sampling algorithm instantiates the mechanism exactly and efficiently. Our bounds show that we obtain near-optimal utility while being empirically competitive against output perturbation methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20248;&#21270;&#26041;&#27861;&#21644;&#19981;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;UCB&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37325;&#21644;&#36229;&#37325;&#23545;&#31216;&#22122;&#22768;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22870;&#21169;&#20013;&#23384;&#22312;&#23545;&#31216;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#19968;&#33324;&#19979;&#30028;&#33021;&#22815;&#33719;&#24471;&#26356;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;&#21363;&#20351;&#22870;&#21169;&#20998;&#24067;&#27809;&#26377;&#26399;&#26395;&#65292;&#35813;&#31639;&#27861;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.07062</link><description>&lt;p&gt;
&#24555;&#36895;UCB&#31867;&#22411;&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;&#37325;&#21644;&#36229;&#37325;&#23545;&#31216;&#22122;&#22768;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fast UCB-type algorithms for stochastic bandits with heavy and super heavy symmetric noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20984;&#20248;&#21270;&#26041;&#27861;&#21644;&#19981;&#31934;&#30830;&#39044;&#27979;&#27169;&#22411;&#30340;&#26032;UCB&#31867;&#22411;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#37325;&#21644;&#36229;&#37325;&#23545;&#31216;&#22122;&#22768;&#30340;&#38543;&#26426;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22870;&#21169;&#20013;&#23384;&#22312;&#23545;&#31216;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#19968;&#33324;&#19979;&#30028;&#33021;&#22815;&#33719;&#24471;&#26356;&#23567;&#30340;&#36951;&#25022;&#30028;&#12290;&#21363;&#20351;&#22870;&#21169;&#20998;&#24067;&#27809;&#26377;&#26399;&#26395;&#65292;&#35813;&#31639;&#27861;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19968;&#33324;&#20984;&#20248;&#21270;&#26041;&#27861;&#21644;&#19981;&#31934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;UCB&#31867;&#22411;&#31639;&#27861;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#25512;&#23548;&#20102;&#19982;&#20248;&#21270;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30456;&#23545;&#24212;&#30340;&#36951;&#25022;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;Clipped-SGD-UCB&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#32463;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22870;&#21169;&#20013;&#23384;&#22312;&#23545;&#31216;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36798;&#21040;$O(\log T\sqrt{KT\log T})$&#30340;&#36951;&#25022;&#30028;&#65292;&#32780;&#19981;&#26159;$O\left (T^{\frac{1}{1+\alpha}} K^{\frac{\alpha}{1+\alpha}} \right)$&#65292;&#35813;&#30028;&#26159;&#24403;&#22870;&#21169;&#20998;&#24067;&#28385;&#36275;$\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$&#65288;$\alpha \in (0, 1]$&#65289;&#26102;&#30340;&#19968;&#33324;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#22870;&#21169;&#20998;&#24067;&#27809;&#26377;&#26399;&#26395;&#65292;&#21363;&#65292;&#24403;$\alpha&lt;0$&#26102;&#65292;&#21516;&#26679;&#30340;&#30028;&#38480;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a new method for constructing UCB-type algorithms for stochastic multi-armed bandits based on general convex optimization methods with an inexact oracle. We derive the regret bounds corresponding to the convergence rates of the optimization methods. We propose a new algorithm Clipped-SGD-UCB and show, both theoretically and empirically, that in the case of symmetric noise in the reward, we can achieve an $O(\log T\sqrt{KT\log T})$ regret bound instead of $O\left (T^{\frac{1}{1+\alpha}} K^{\frac{\alpha}{1+\alpha}} \right)$ for the case when the reward distribution satisfies $\mathbb{E}_{X \in D}[|X|^{1+\alpha}] \leq \sigma^{1+\alpha}$ ($\alpha \in (0, 1])$, i.e. perform better than it is assumed by the general lower bound for bandits with heavy-tails. Moreover, the same bound holds even when the reward distribution does not have the expectation, that is, when $\alpha&lt;0$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#36817;&#20284;&#25439;&#22833;&#36827;&#34892;&#26679;&#26412;&#37319;&#26679;&#30340;&#35757;&#32451;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#31574;&#30053;&#36873;&#25321;&#20855;&#26377;&#22823;&#32422;&#25439;&#22833;&#30340;&#26679;&#26412;&#65292;&#20943;&#23569;&#36873;&#25321;&#30340;&#24320;&#38144;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#36895;&#24230;&#20248;&#20110;&#38543;&#26426;&#36873;&#25321;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#20351;&#29992;&#20013;&#38388;&#23618;&#34920;&#31034;&#33719;&#21462;&#36817;&#20284;&#25439;&#22833;&#30340;SIFT&#26041;&#27861;&#65292;&#24182;&#22312;&#35757;&#32451;BERT&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07052</link><description>&lt;p&gt;
&#29702;&#35299;&#36890;&#36807;&#20351;&#29992;&#36817;&#20284;&#25439;&#22833;&#36827;&#34892;&#37319;&#26679;&#30340;&#35757;&#32451;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Understanding the Training Speedup from Sampling with Approximate Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#36817;&#20284;&#25439;&#22833;&#36827;&#34892;&#26679;&#26412;&#37319;&#26679;&#30340;&#35757;&#32451;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#36138;&#23146;&#31574;&#30053;&#36873;&#25321;&#20855;&#26377;&#22823;&#32422;&#25439;&#22833;&#30340;&#26679;&#26412;&#65292;&#20943;&#23569;&#36873;&#25321;&#30340;&#24320;&#38144;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#36895;&#24230;&#20248;&#20110;&#38543;&#26426;&#36873;&#25321;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#20351;&#29992;&#20013;&#38388;&#23618;&#34920;&#31034;&#33719;&#21462;&#36817;&#20284;&#25439;&#22833;&#30340;SIFT&#26041;&#27861;&#65292;&#24182;&#22312;&#35757;&#32451;BERT&#27169;&#22411;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#36873;&#25321;&#20855;&#26377;&#36739;&#22823;&#25439;&#22833;/&#26799;&#24230;&#30340;&#26679;&#26412;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#30340;&#24320;&#38144;&#24448;&#24448;&#36807;&#39640;&#65292;&#26080;&#27861;&#22312;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#33719;&#24471;&#26377;&#24847;&#20041;&#30340;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36873;&#25321;&#20855;&#26377;&#22823;&#32422;&#25439;&#22833;&#30340;&#26679;&#26412;&#30340;&#36138;&#23146;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20934;&#30830;&#25439;&#22833;&#65292;&#20197;&#20943;&#23569;&#36873;&#25321;&#30340;&#24320;&#38144;&#12290;&#23545;&#20110;&#24179;&#28369;&#20984;&#25439;&#22833;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#36138;&#23146;&#31574;&#30053;&#21487;&#20197;&#22312;&#27604;&#38543;&#26426;&#36873;&#25321;&#26356;&#23569;&#30340;&#36845;&#20195;&#27425;&#25968;&#20869;&#25910;&#25947;&#21040;&#24179;&#22343;&#25439;&#22833;&#30340;&#26368;&#23567;&#20540;&#30340;&#24120;&#25968;&#22240;&#23376;&#12290;&#25105;&#20204;&#36824;&#29702;&#35770;&#19978;&#37327;&#21270;&#20102;&#36817;&#20284;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20351;&#29992;&#20013;&#38388;&#23618;&#34920;&#31034;&#33719;&#21462;&#36817;&#20284;&#25439;&#22833;&#20197;&#36827;&#34892;&#26679;&#26412;&#36873;&#25321;&#30340;SIFT&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;SIFT&#22312;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;1.1&#20159;&#21442;&#25968;&#30340;12&#23618;BERT&#22522;&#30784;&#27169;&#22411;&#19978;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65288;&#20197;&#35757;&#32451;&#26102;&#38388;&#21644;&#21453;&#21521;&#20256;&#25773;&#27493;&#39588;&#30340;&#25968;&#37327;&#34913;&#37327;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is well known that selecting samples with large losses/gradients can significantly reduce the number of training steps. However, the selection overhead is often too high to yield any meaningful gains in terms of overall training time. In this work, we focus on the greedy approach of selecting samples with large \textit{approximate losses} instead of exact losses in order to reduce the selection overhead. For smooth convex losses, we show that such a greedy strategy can converge to a constant factor of the minimum value of the average loss in fewer iterations than the standard approach of random selection. We also theoretically quantify the effect of the approximation level. We then develop SIFT which uses early exiting to obtain approximate losses with an intermediate layer's representations for sample selection. We evaluate SIFT on the task of training a 110M parameter 12-layer BERT base model and show significant gains (in terms of training hours and number of backpropagation step
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21644;&#28436;&#31034;&#23398;&#20064; DFA&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07051</link><description>&lt;p&gt;
$L^*LM$: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#31034;&#20363;&#23398;&#20064;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
$L^*LM$: Learning Automata from Examples using Natural Language Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21644;&#28436;&#31034;&#23398;&#20064; DFA&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28436;&#31034;&#24050;&#34987;&#35777;&#26126;&#26159;&#31616;&#21270;&#38388;&#25509;&#25351;&#23450;&#22797;&#26434;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#29978;&#33267;&#25903;&#25345;&#20174;&#28436;&#31034;&#20013;&#25552;&#21462;&#26126;&#30830;&#30340;&#24418;&#24335;&#35268;&#33539;&#65292;&#22914;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#19981;&#20855;&#22791;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064; DFA&#12290;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064; DFA &#30340;&#25968;&#25454;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;$L^*LM$ &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#20851;&#20110;&#24213;&#23618;&#20219;&#21153;&#30340;&#25104;&#21592;&#26597;&#35810;&#12290;&#28982;&#21518;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#28436;&#31034;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23558;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#24102;&#26631;&#31614;&#31034;&#20363;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#27169;&#24577;&#30456;&#20114;&#34917;&#20805;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.07043</link><description>&lt;p&gt;
&#23614;&#24052;&#30340;&#25925;&#20107;&#65306;&#20316;&#20026;&#23610;&#24230;&#24459;&#21464;&#21270;&#30340;&#27169;&#22411;&#23849;&#28291;
&lt;/p&gt;
&lt;p&gt;
A Tale of Tails: Model Collapse as a Change of Scaling Laws
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;AI&#27169;&#22411;&#22823;&#23567;&#22686;&#38271;&#21644;&#21512;&#25104;&#25968;&#25454;&#24341;&#20837;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27169;&#22411;&#21487;&#33021;&#20250;&#36973;&#36935;&#24635;&#20307;&#23849;&#28291;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#38271;&#65292;&#31070;&#32463;&#23610;&#24230;&#24459;&#24050;&#25104;&#20026;&#39044;&#27979;&#22823;&#27169;&#22411;&#22312;&#25193;&#23481;&#21644;&#21407;&#22987;&#65288;&#20154;&#31867;&#25110;&#33258;&#28982;&#65289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#22686;&#21152;&#26102;&#25913;&#21892;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24847;&#21619;&#30528;&#22312;&#32447;&#25968;&#25454;&#21644;&#25991;&#26412;&#30340;&#29983;&#24577;&#31995;&#32479;&#23558;&#36880;&#28176;&#21253;&#21547;&#36234;&#26469;&#36234;&#22810;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38382;&#65306;&#24403;&#21512;&#25104;&#25968;&#25454;&#36827;&#20837;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#65292;&#23610;&#24230;&#24459;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#26410;&#26469;&#30340;&#27169;&#22411;&#20173;&#20250;&#25913;&#21892;&#65292;&#36824;&#26159;&#27880;&#23450;&#20250;&#23436;&#20840;&#23849;&#28291;&#65288;&#27169;&#22411;&#23849;&#28291;&#65289;&#65311;&#36890;&#36807;&#23610;&#24230;&#24459;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#23849;&#28291;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#24191;&#27867;&#30340;&#34928;&#20943;&#29616;&#35937;&#65292;&#20998;&#26512;&#20102;&#23610;&#24230;&#30340;&#20007;&#22833;&#12289;&#19982;&#20195;&#25968;&#30340;&#21464;&#21270;&#23610;&#24230;&#12289;&#25216;&#33021;&#30340;"&#36951;&#24536;"&#20197;&#21450;&#28151;&#21512;&#20154;&#31867;&#21644;&#21512;&#25104;&#25968;&#25454;&#26102;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36890;&#36807;&#23545;&#19968;&#20010;&#31639;&#26415;&#20219;&#21153;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#36716;&#25442;&#22120;&#36827;&#34892;&#22823;&#35268;&#27169;&#23454;&#39564;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI model size grows, neural scaling laws have become a crucial tool to predict the improvements of large models when increasing capacity and the size of original (human or natural) training data. Yet, the widespread use of popular models means that the ecosystem of online data and text will co-evolve to progressively contain increased amounts of synthesized data. In this paper we ask: How will the scaling laws change in the inevitable regime where synthetic data makes its way into the training corpus? Will future models, still improve, or be doomed to degenerate up to total (model) collapse? We develop a theoretical framework of model collapse through the lens of scaling laws. We discover a wide range of decay phenomena, analyzing loss of scaling, shifted scaling with number of generations, the ''un-learning" of skills, and grokking when mixing human and synthesized data. Our theory is validated by large-scale experiments with a transformer on an arithmetic task and text generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#31526;&#21495;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#25552;&#21462;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26174;&#31034;&#24402;&#32435;&#20559;&#35265;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#21152;&#24555;&#23545;&#25277;&#35937;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.07035</link><description>&lt;p&gt;
&#23558;&#31526;&#21495;&#20808;&#39564;&#30693;&#35782;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#30340;&#25277;&#35937;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling Symbolic Priors for Concept Learning into Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07035
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#31526;&#21495;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#25552;&#21462;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26174;&#31034;&#24402;&#32435;&#20559;&#35265;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#21152;&#24555;&#23545;&#25277;&#35937;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#24402;&#32435;&#20559;&#35265;&#20174;&#23569;&#37327;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#12290;&#36825;&#20123;&#24402;&#32435;&#20559;&#35265;&#20808;&#21069;&#24050;&#36890;&#36807;&#22312;&#31526;&#21495;&#20551;&#35774;&#31354;&#38388;&#19978;&#23450;&#20041;&#36125;&#21494;&#26031;&#27169;&#22411;&#26469;&#25429;&#25417;&#12290;&#26159;&#21542;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#26174;&#31034;&#30456;&#21516;&#24402;&#32435;&#20559;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20803;&#23398;&#20064;&#65288;&#19968;&#31181;&#20174;&#19968;&#32452;&#20219;&#21153;&#20013;&#25552;&#21462;&#20849;&#21516;&#32467;&#26500;&#30340;&#26041;&#27861;&#65289;&#23558;&#31526;&#21495;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#25552;&#21462;&#21040;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#21487;&#20197;&#23454;&#20363;&#21270;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#27010;&#24565;&#30340;&#24402;&#32435;&#20559;&#35265;&#12290;&#36890;&#36807;&#20197;&#36125;&#21494;&#26031;&#27169;&#22411;&#30340;&#20808;&#39564;&#20998;&#24067;&#29983;&#25104;&#20803;&#23398;&#20064;&#20013;&#20351;&#29992;&#30340;&#20219;&#21153;&#38598;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#35813;&#20808;&#39564;&#20256;&#36755;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#23545;&#20197;&#30701;&#36923;&#36753;&#20844;&#24335;&#34920;&#31034;&#30340;&#27010;&#24565;&#30340;&#24402;&#32435;&#20559;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20998;&#26512;&#20808;&#21069;&#20154;&#20204;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#36923;&#36753;&#27010;&#24565;&#30340;&#34892;&#20026;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20803;&#35757;&#32451;&#26041;&#26696;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#24555;&#36895;&#23398;&#20064;&#36825;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can learn new concepts from a small number of examples by drawing on their inductive biases. These inductive biases have previously been captured by using Bayesian models defined over symbolic hypothesis spaces. Is it possible to create a neural network that displays the same inductive biases? We show that inductive biases that enable rapid concept learning can be instantiated in artificial neural networks by distilling a prior distribution from a symbolic Bayesian model via meta-learning, an approach for extracting the common structure from a set of tasks. By generating the set of tasks used in meta-learning from the prior distribution of a Bayesian model, we are able to transfer that prior into a neural network. We use this approach to create a neural network with an inductive bias towards concepts expressed as short logical formulas. Analyzing results from previous behavioral experiments in which people learned logical concepts from a few examples, we find that our meta-train
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.07031</link><description>&lt;p&gt;
&#23454;&#20363;&#32423;&#21035;&#30340;&#23433;&#20840;&#24863;&#30693;&#19982;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#21450;&#20854;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Instance-Level Safety-Aware Fidelity of Synthetic Data and Its Calibration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23454;&#20363;&#32423;&#21035;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#19982;&#23433;&#20840;&#24863;&#30693;&#65292;&#24341;&#20837;&#20102;&#22235;&#31181;&#36229;&#36234;&#32431;&#35270;&#35273;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#36136;&#37327;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#26041;&#27861;&#26469;&#20943;&#23569;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#27169;&#21644;&#26657;&#20934;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#23545;&#22609;&#36896;&#26410;&#26469;&#23433;&#20840;&#21487;&#38752;&#30340;&#33258;&#21160;&#39550;&#39542;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#21487;&#20197;&#21462;&#20195;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#25910;&#38598;&#12290;&#25105;&#20204;&#20851;&#27880;&#20854;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#36229;&#36234;&#32431;&#35270;&#35273;&#36755;&#20837;&#29305;&#24449;&#30340;&#22235;&#31181;&#23454;&#20363;&#32423;&#21035;&#36136;&#37327;&#65292;&#26088;&#22312;&#20351;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#23433;&#20840;&#38382;&#39064;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#26469;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#65292;&#20943;&#23569;&#30001;&#22522;&#20110;DNN&#30340;&#32452;&#20214;&#35782;&#21035;&#20986;&#30340;&#36136;&#37327;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#35843;&#20248;&#21487;&#20197;&#22686;&#24378;&#21512;&#25104;&#21644;&#30495;&#23454;&#22270;&#20687;&#20013;&#23433;&#20840;&#20851;&#38190;&#38169;&#35823;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling and calibrating the fidelity of synthetic data is paramount in shaping the future of safe and reliable self-driving technology by offering a cost-effective and scalable alternative to real-world data collection. We focus on its role in safety-critical applications, introducing four types of instance-level fidelity that go beyond mere visual input characteristics. The aim is to align synthetic data with real-world safety issues. We suggest an optimization method to refine the synthetic data generator, reducing fidelity gaps identified by the DNN-based component. Our findings show this tuning enhances the correlation between safety-critical errors in synthetic and real images.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#35821;&#35328;&#23545;&#24212;&#30340;&#36830;&#32493;&#35789;&#34920;&#31034;&#38598;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#65292;&#25512;&#26029;&#21452;&#35821;&#35789;&#20856;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#22312;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#65292;&#25972;&#21512;&#24050;&#26377;&#35821;&#35328;&#38598;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#25490;&#24207;&#26041;&#27861;&#23454;&#29616;&#35789;&#20856;&#35825;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.07028</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#21452;&#35821;&#35789;&#20856;&#35825;&#23548;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning for Bilingual Lexicon Induction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20004;&#31181;&#35821;&#35328;&#23545;&#24212;&#30340;&#36830;&#32493;&#35789;&#34920;&#31034;&#38598;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#31354;&#38388;&#65292;&#25512;&#26029;&#21452;&#35821;&#35789;&#20856;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#22522;&#30784;&#65292;&#22312;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#65292;&#25972;&#21512;&#24050;&#26377;&#35821;&#35328;&#38598;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#25490;&#24207;&#26041;&#27861;&#23454;&#29616;&#35789;&#20856;&#35825;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23558;&#23545;&#24212;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#20004;&#20010;&#36830;&#32493;&#35789;&#34920;&#31034;&#38598;&#23545;&#40784;&#21040;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#65292;&#20197;&#25512;&#26029;&#21452;&#35821;&#35789;&#20856;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#22312;&#21333;&#35821;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#23545;&#40784;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#36825;&#26679;&#30340;&#35789;&#20856;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#24179;&#34892;&#25968;&#25454;&#12290;&#36825;&#31181;&#24037;&#20316;&#31216;&#20026;&#26080;&#30417;&#30563;&#21452;&#35821;&#35825;&#23548;&#12290;&#36890;&#36807;&#24605;&#32771;&#26159;&#21542;&#21487;&#33021;&#22312;&#36880;&#27493;&#23398;&#20064;&#22810;&#31181;&#35821;&#35328;&#30340;&#36807;&#31243;&#20013;&#31215;&#32047;&#32463;&#39564;&#65292;&#25105;&#20204;&#33258;&#38382;&#22312;&#23398;&#20064;&#26032;&#35821;&#35328;&#26102;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25972;&#21512;&#32473;&#23450;&#35821;&#35328;&#38598;&#30340;&#30693;&#35782;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#34429;&#28982;&#20445;&#25345;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26680;&#24515;&#38382;&#39064;&#22312;&#26368;&#26032;&#27493;&#39588;&#20013;&#65292;&#20294;&#25105;&#20204;&#20801;&#35768;&#35775;&#38382;&#20854;&#20182;&#20064;&#35821;&#35821;&#26009;&#24211;&#65292;&#22240;&#27492;&#31216;&#20026;&#21322;&#30417;&#30563;&#12290;&#36825;&#23548;&#33268;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#36798;&#24418;&#24335;&#65292;&#23558;&#35789;&#20856;&#35825;&#23548;&#35270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#35813;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26368;&#26032;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of aligning two sets of continuous word representations, corresponding to languages, to a common space in order to infer a bilingual lexicon. It was recently shown that it is possible to infer such lexicon, without using any parallel data, by aligning word embeddings trained on monolingual data. Such line of work is called unsupervised bilingual induction. By wondering whether it was possible to gain experience in the progressive learning of several languages, we asked ourselves to what extent we could integrate the knowledge of a given set of languages when learning a new one, without having parallel data for the latter. In other words, while keeping the core problem of unsupervised learning in the latest step, we allowed the access to other corpora of idioms, hence the name semi-supervised. This led us to propose a novel formulation, considering the lexicon induction as a ranking problem for which we used recent tools of this machine learning field. Our experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#37327;&#23376;&#26041;&#27861;&#39640;&#25928;&#22320;&#35299;&#20915;Kronecker&#31215;$A_1 \otimes A_2$&#30340;&#35889;&#36924;&#36817;&#38382;&#39064;&#65292;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;$O_{d,\epsilon}(\sqrt{n})$&#12290;</title><link>https://arxiv.org/abs/2402.07027</link><description>&lt;p&gt;
&#37327;&#23376;&#21152;&#36895;&#19979;&#30340;Kronecker&#31215;&#30340;&#35889;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Quantum Speedup for Spectral Approximation of Kronecker Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#37327;&#23376;&#26041;&#27861;&#39640;&#25928;&#22320;&#35299;&#20915;Kronecker&#31215;$A_1 \otimes A_2$&#30340;&#35889;&#36924;&#36817;&#38382;&#39064;&#65292;&#23558;&#26102;&#38388;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;$O_{d,\epsilon}(\sqrt{n})$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;Kronecker&#31215;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23427;&#25104;&#20026;&#20102;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#31526;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#35201;&#27714;&#39640;&#65292;&#36890;&#36807;&#20256;&#32479;&#35745;&#31639;&#31639;&#27861;&#36827;&#34892;Kronecker&#31215;&#30340;&#35889;&#36924;&#36817;&#30340;&#25104;&#26412;&#20063;&#30456;&#24212;&#22686;&#21152;&#12290;&#29616;&#26377;&#30340;&#32463;&#20856;&#26041;&#27861;&#23545;&#20110;&#35889;&#36924;&#36817;&#34920;&#29616;&#20986;&#19982;&#30697;&#38453;&#32500;&#24230;$n$&#32447;&#24615;&#20381;&#36182;&#30340;&#29305;&#28857;&#65292;&#20854;&#20013;$A_1 \in \mathbb{R}^{n \times d}$&#65292;$A_2 \in \mathbb{R}^{n \times d}$&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#37327;&#23376;&#26041;&#27861;&#39640;&#25928;&#22320;&#35299;&#20915;Kronecker&#31215;$A_1 \otimes A_2$&#30340;&#35889;&#36924;&#36817;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30697;&#38453;&#35270;&#20026;&#37327;&#23376;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#35889;&#36924;&#36817;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#22823;&#24133;&#38477;&#20302;&#21040;$O_{d,\epsilon}(\sqrt{n})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Given its widespread application in machine learning and optimization, the Kronecker product emerges as a pivotal linear algebra operator. However, its computational demands render it an expensive operation, leading to heightened costs in spectral approximation of it through traditional computation algorithms. Existing classical methods for spectral approximation exhibit a linear dependency on the matrix dimension denoted by $n$, considering matrices of size $A_1 \in \mathbb{R}^{n \times d}$ and $A_2 \in \mathbb{R}^{n \times d}$. Our work introduces an innovative approach to efficiently address the spectral approximation of the Kronecker product $A_1 \otimes A_2$ using quantum methods. By treating matrices as quantum states, our proposed method significantly reduces the time complexity of spectral approximation to $O_{d,\epsilon}(\sqrt{n})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07025</link><description>&lt;p&gt;
&#22343;&#22330;&#26497;&#38480;&#19979;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Generalization Error of Graph Neural Networks in the Mean-field Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07025
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#36890;&#36807;&#25512;&#23548;&#20986;&#25910;&#25947;&#36895;&#24230;&#20026;$O(1/n)$&#30340;&#19978;&#30028;&#65292;&#20026;&#25105;&#20204;&#23545;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#21363;&#21442;&#25968;&#25968;&#37327;&#36229;&#36807;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#31867;&#22411;&#65306;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#26412;&#30740;&#31350;&#20043;&#21069;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#27867;&#21270;&#35823;&#24046;&#30340;&#29616;&#26377;&#30028;&#38480;&#32570;&#20047;&#20449;&#24687;&#65292;&#38480;&#21046;&#20102;&#25105;&#20204;&#23545;&#36807;&#21442;&#25968;&#21270;&#32593;&#32476;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#26041;&#27861;&#26159;&#22312;&#22343;&#22330;&#26497;&#38480;&#19979;&#25512;&#23548;&#20986;&#19978;&#30028;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#20197;$O(1/n)$&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$n$&#26159;&#22270;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#36825;&#20123;&#19978;&#30028;&#20026;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#20445;&#35777;&#65292;&#20174;&#32780;&#23545;&#25105;&#20204;&#30340;&#29702;&#35299;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a theoretical framework for assessing the generalization error of graph classification tasks via graph neural networks in the over-parameterized regime, where the number of parameters surpasses the quantity of data points. We explore two widely utilized types of graph neural networks: graph convolutional neural networks and message passing graph neural networks. Prior to this study, existing bounds on the generalization error in the over-parametrized regime were uninformative, limiting our understanding of over-parameterized network performance. Our novel approach involves deriving upper bounds within the mean-field regime for evaluating the generalization error of these graph neural networks. We establish upper bounds with a convergence rate of $O(1/n)$, where $n$ is the number of graph samples. These upper bounds offer a theoretical assurance of the networks' performance on unseen data in the challenging over-parameterized regime and overall contribute to our under
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;Gemini&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#19988;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#12290;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.07023</link><description>&lt;p&gt;
&#21452;&#23376;&#24231;&#36827;&#20837;&#21307;&#23398;&#38498;&#65306;&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25361;&#25112;&#38382;&#39064;&#21644;&#24187;&#35273;&#19978;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems &amp; Hallucinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;Gemini&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#19988;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#12290;&#37319;&#29992;&#25552;&#31034;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#34892;&#19994;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#65292;&#20294;&#36890;&#36807;&#20005;&#26684;&#35780;&#20272;&#26469;&#39564;&#35777;&#20854;&#23433;&#20840;&#24615;&#21644;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#24320;&#28304;LLM&#21644;&#35895;&#27468;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;LLM Gemini &#22312;&#21307;&#23398;&#25512;&#29702;&#12289;&#24187;&#35273;&#26816;&#27979;&#21644;&#21307;&#23398;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;Gemini&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#35786;&#26029;&#20934;&#30830;&#24615;&#26041;&#38754;&#33853;&#21518;&#20110;MedPaLM 2&#21644;GPT-4&#31561;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;Gemini&#22312;&#21307;&#23398;VQA&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;61.45&#65285;&#65292;&#26126;&#26174;&#20302;&#20110;GPT-4V&#30340;88&#65285;&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;Gemini&#26497;&#26131;&#20986;&#29616;&#24187;&#35273;&#12289;&#36807;&#24230;&#33258;&#20449;&#21644;&#30693;&#35782;&#30450;&#28857;&#65292;&#36825;&#34920;&#26126;&#22914;&#26524;&#19981;&#21152;&#25209;&#21028;&#22320;&#37096;&#32626;&#65292;&#23384;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#38024;&#23545;&#19981;&#21516;&#21307;&#23398;&#23398;&#31185;&#21644;&#27979;&#35797;&#31867;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#20026;&#24320;&#21457;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#20102;&#21487;&#25805;&#20316;&#30340;&#21453;&#39304;&#12290;&#20026;&#20102;&#20943;&#23569;&#39118;&#38505;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25552;&#31034;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35774;&#35745;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20449;&#24687;&#24615;&#20934;&#21017;&#26469;&#35780;&#20272;&#20195;&#29702;&#30340;&#24403;&#21069;&#31574;&#30053;&#22312;&#25509;&#21463;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#22870;&#21169;&#21518;&#30340;&#25913;&#21892;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.07019</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#20449;&#24687;&#24615;
&lt;/p&gt;
&lt;p&gt;
Informativeness of Reward Functions in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#35774;&#35745;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20449;&#24687;&#24615;&#20934;&#21017;&#26469;&#35780;&#20272;&#20195;&#29702;&#30340;&#24403;&#21069;&#31574;&#30053;&#22312;&#25509;&#21463;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#30340;&#22870;&#21169;&#21518;&#30340;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#20989;&#25968;&#22312;&#25351;&#23450;&#25105;&#20204;&#26399;&#26395;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#25191;&#34892;&#30340;&#20219;&#21153;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#32473;&#23450;&#20219;&#21153;&#21644;&#26399;&#26395;&#30340;&#26368;&#20248;&#34892;&#20026;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#35774;&#35745;&#20449;&#24687;&#20016;&#23500;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#21152;&#24555;&#20195;&#29702;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19987;&#23478;&#39537;&#21160;&#30340;&#22870;&#21169;&#35774;&#35745;&#35774;&#32622;&#65292;&#20854;&#20013;&#19987;&#23478;&#25110;&#25945;&#24072;&#24076;&#26395;&#20026;&#23398;&#20064;&#20195;&#29702;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#19988;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#12290;&#24050;&#26377;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#22870;&#21169;&#35774;&#35745;&#24418;&#24335;&#65292;&#28982;&#32780;&#20851;&#38190;&#25361;&#25112;&#26159;&#21046;&#23450;&#19968;&#20010;&#22870;&#21169;&#20449;&#24687;&#24615;&#20934;&#21017;&#65292;&#35813;&#20934;&#21017;&#33021;&#22815;&#26681;&#25454;&#20195;&#29702;&#30340;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25351;&#23450;&#30340;&#32467;&#26500;&#32422;&#26463;&#19979;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#33719;&#24471;&#21487;&#35299;&#37322;&#30340;&#22870;&#21169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22870;&#21169;&#20449;&#24687;&#24615;&#20934;&#21017;&#65292;&#19968;&#31181;&#23450;&#37327;&#24230;&#37327;&#65292;&#29992;&#20110;&#25429;&#25417;&#20195;&#29702;&#30340;&#24403;&#21069;&#31574;&#30053;&#22914;&#26524;&#20174;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#20013;&#33719;&#24471;&#22870;&#21169;&#23558;&#22914;&#20309;&#25913;&#21892;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#20449;&#24687;&#24615;&#20934;&#21017;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward functions are central in specifying the task we want a reinforcement learning agent to perform. Given a task and desired optimal behavior, we study the problem of designing informative reward functions so that the designed rewards speed up the agent's convergence. In particular, we consider expert-driven reward design settings where an expert or teacher seeks to provide informative and interpretable rewards to a learning agent. Existing works have considered several different reward design formulations; however, the key challenge is formulating a reward informativeness criterion that adapts w.r.t. the agent's current policy and can be optimized under specified structural constraints to obtain interpretable rewards. In this paper, we propose a novel reward informativeness criterion, a quantitative measure that captures how the agent's current policy will improve if it receives rewards from a specific reward function. We theoretically showcase the utility of the proposed informati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07011</link><description>&lt;p&gt;
FedImpro: &#27979;&#37327;&#21644;&#25913;&#21892;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
FedImpro: Measuring and Improving Client Update in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FedImpro&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#28418;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#26512;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#21033;&#29992;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#35757;&#32451;&#65292;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#24182;&#20943;&#23567;&#20102;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#21463;&#21040;&#24322;&#26500;&#25968;&#25454;&#24341;&#36215;&#30340;&#23458;&#25143;&#28418;&#31227;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#25968;&#25454;&#30340;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#23458;&#25143;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#36827;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#25805;&#20316;&#29616;&#26377;&#30340;&#26799;&#24230;&#65292;&#20197;&#23454;&#29616;&#26356;&#19968;&#33268;&#30340;&#23458;&#25143;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#20998;&#26512;&#20102;&#23458;&#25143;&#28418;&#31227;&#65292;&#24182;&#26088;&#22312;&#36890;&#36807;&#29983;&#25104;&#25913;&#36827;&#30340;&#26412;&#22320;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#31181;&#28418;&#31227;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26412;&#22320;&#35757;&#32451;&#30340;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#36825;&#31181;&#27867;&#21270;&#36129;&#29486;&#21463;&#21040;&#19981;&#21516;&#23458;&#25143;&#30340;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#38480;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedImpro&#65292;&#29992;&#20110;&#26500;&#24314;&#31867;&#20284;&#30340;&#26465;&#20214;&#20998;&#24067;&#36827;&#34892;&#26412;&#22320;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedImpro&#23558;&#27169;&#22411;&#20998;&#35299;&#20026;&#39640;&#23618;&#21644;&#20302;&#23618;&#32452;&#20214;&#65292;&#24182;&#23545;&#37325;&#26500;&#29305;&#24449;&#20998;&#24067;&#19978;&#30340;&#39640;&#23618;&#37096;&#20998;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#27867;&#21270;&#36129;&#29486;&#65292;&#24182;&#20943;&#23567;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#26799;&#24230;&#30340;&#24046;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20248;&#21270;&#26694;&#26550;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;3D U-Net&#27169;&#22411;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#22312;&#19977;&#20010;&#25361;&#25112;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;Dice&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.07008</link><description>&lt;p&gt;
&#29992;&#20110;&#22788;&#29702;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework for Processing and Transfer Learning for the Brain Tumor Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07008
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#26694;&#26550;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;3D U-Net&#27169;&#22411;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#65292;&#22312;&#19977;&#20010;&#25361;&#25112;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;Dice&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#33041;MRI&#22270;&#20687;&#20013;&#30340;&#32959;&#30244;&#20998;&#21106;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#26679;&#26412;&#26377;&#38480;&#12289;&#24418;&#29366;&#21464;&#21270;&#22823;&#19988;&#32959;&#30244;&#24418;&#24577;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#23637;&#22312;&#33258;&#21160;&#21270;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#22312;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#24615;&#26041;&#38754;&#23578;&#26410;&#36798;&#21040;&#20020;&#24202;&#20351;&#29992;&#30340;&#29702;&#24819;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;BraTS 2023&#30340;&#25361;&#25112;&#20013;1&#12289;2&#21644;3&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;3D U-Net&#27169;&#22411;&#30340;&#33041;&#32959;&#30244;&#20998;&#21106;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21508;&#31181;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#65292;&#36825;&#20010;&#22810;&#27169;&#24577;&#33041;&#32959;&#30244;&#20998;&#21106;&#26694;&#26550;&#20998;&#21035;&#22312;Challenge 1&#12289;2&#12289;3&#19978;&#36798;&#21040;&#24179;&#22343;&#30149;&#21464;&#32423;&#21035;Dice&#20998;&#25968;0.79&#12289;0.72&#12289;0.74&#12290;
&lt;/p&gt;
&lt;p&gt;
Tumor segmentation from multi-modal brain MRI images is a challenging task due to the limited samples, high variance in shapes and uneven distribution of tumor morphology. The performance of automated medical image segmentation has been significant improvement by the recent advances in deep learning. However, the model predictions have not yet reached the desired level for clinical use in terms of accuracy and generalizability. In order to address the distinct problems presented in Challenges 1, 2, and 3 of BraTS 2023, we have constructed an optimization framework based on a 3D U-Net model for brain tumor segmentation. This framework incorporates a range of techniques, including various pre-processing and post-processing techniques, and transfer learning. On the validation datasets, this multi-modality brain tumor segmentation framework achieves an average lesion-wise Dice score of 0.79, 0.72, 0.74 on Challenges 1, 2, 3 respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07002</link><description>&lt;p&gt;
&#23458;&#25143;&#31471;&#21327;&#20316;&#65306;&#20855;&#26377;&#20445;&#35777;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#25913;&#36827;&#30340;&#28789;&#27963;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#27490;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#20294;&#23427;&#24182;&#19981;&#26159;&#20813;&#36153;&#30340;&#12290;&#22122;&#22768;&#30340;&#28155;&#21152;&#20250;&#38543;&#26426;&#24178;&#25200;&#27169;&#22411;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#24178;&#25200;&#20250;&#38543;&#30528;&#36890;&#20449;&#36718;&#27425;&#30340;&#22686;&#21152;&#32780;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;FedCEO&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;"&#30456;&#20114;&#21327;&#20316;"&#65292;&#26088;&#22312;&#22312;&#27169;&#22411;&#25928;&#29992;&#21644;&#29992;&#25143;&#38544;&#31169;&#20043;&#38388;&#25214;&#21040;&#19968;&#31181;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#22534;&#21472;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#20102;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20809;&#35889;&#31354;&#38388;&#20013;&#28789;&#27963;&#25130;&#26029;&#39640;&#39057;&#32452;&#20998;&#30340;&#33021;&#21147;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;FedCEO&#33021;&#22815;&#36890;&#36807;&#24179;&#28369;&#20840;&#23616;&#35821;&#20041;&#31354;&#38388;&#26469;&#26377;&#25928;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#21644;&#25345;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;SOTA&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#36793;&#30028;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;U-Net&#20998;&#21106;&#22522;&#32447;&#20173;&#28982;&#26159;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#39030;&#23574;&#34920;&#29616;&#32773;&#12290;</title><link>https://arxiv.org/abs/2402.06994</link><description>&lt;p&gt;
&#19968;&#20010;&#21464;&#21270;&#26816;&#27979;&#30340;&#29616;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
A Change Detection Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;U-Net&#20998;&#21106;&#22522;&#32447;&#20173;&#28982;&#26159;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#39030;&#23574;&#34920;&#29616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#30340;&#36965;&#24863;&#25991;&#29486;&#20013;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#25552;&#20986;&#30340;&#29992;&#20110;&#21464;&#21270;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#22768;&#31216;&#22312;&#19981;&#21516;&#30340;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#26159;&#21542;&#30495;&#27491;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;U-Net&#20998;&#21106;&#22522;&#32447;&#65292;&#27809;&#26377;&#35757;&#32451;&#25216;&#24039;&#25110;&#22797;&#26434;&#30340;&#26550;&#26500;&#25913;&#21464;&#65292;&#20173;&#28982;&#26159;&#36827;&#34892;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#30340;&#39030;&#23574;&#34920;&#29616;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been an explosion of proposed change detection deep learning architectures in the remote sensing literature. These approaches claim to offer state-of the-art performance on different standard benchmark datasets. However, has the field truly made significant progress? In this paper we perform experiments which conclude a simple U-Net segmentation baseline without training tricks or complicated architectural changes is still a top performer for the task of change detection.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#26799;&#24230;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;&#23398;&#20064;&#21442;&#25968;&#21270;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#31243;&#24207;&#24402;&#32435;&#25104;&#20026;&#35299;&#20915;&#22810;&#31181;&#31867;&#22411;&#20219;&#21153;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06990</link><description>&lt;p&gt;
&#36890;&#36807;&#25628;&#32034;&#26799;&#24230;&#24341;&#23548;&#30340;&#22522;&#20110;&#33609;&#22270;&#30340;&#31243;&#24207;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
Guided Sketch-Based Program Induction by Search Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06990
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#26799;&#24230;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;&#23398;&#20064;&#21442;&#25968;&#21270;&#31243;&#24207;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#31243;&#24207;&#24402;&#32435;&#25104;&#20026;&#35299;&#20915;&#22810;&#31181;&#31867;&#22411;&#20219;&#21153;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36731;&#26494;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20219;&#21153;&#19981;&#33021;&#36890;&#36807;&#32479;&#35745;&#27169;&#22411;&#36731;&#26494;&#35299;&#20915;&#65292;&#32780;&#38656;&#35201;&#31526;&#21495;&#21270;&#30340;&#26041;&#27861;&#12290;&#31243;&#24207;&#24402;&#32435;&#26159;&#36890;&#36807;&#35757;&#32451;&#26469;&#25429;&#25417;&#21487;&#35299;&#37322;&#19988;&#21487;&#25512;&#24191;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#27492;&#31867;&#20219;&#21153;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31243;&#24207;&#24402;&#32435;&#26041;&#27861;&#36824;&#19981;&#22815;&#22797;&#26434;&#65292;&#19981;&#33021;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#34987;&#21046;&#23450;&#20026;&#21333;&#19968;&#30340;&#12289;&#21253;&#21547;&#25152;&#26377;&#30340;&#27169;&#22411;&#65292;&#36890;&#24120;&#30001;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21442;&#25968;&#21270;&#12290;&#20026;&#20102;&#20351;&#31243;&#24207;&#24402;&#32435;&#25104;&#20026;&#35768;&#22810;&#22330;&#26223;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25628;&#32034;&#26799;&#24230;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;&#23398;&#20064;&#21442;&#25968;&#21270;&#31243;&#24207;&#30340;&#26694;&#26550;&#12290;&#36825;&#20010;&#26032;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#31243;&#24207;&#24402;&#32435;&#26377;&#25152;&#19981;&#21516;&#65292;&#23427;&#20801;&#35768;&#31243;&#24207;&#21592;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#20195;&#30721;&#36755;&#20837;&#21040;&#31243;&#24207;&#30340;&#8220;&#33609;&#22270;&#8221;&#20013;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#30340;&#26799;&#24230;&#20248;&#21270;&#26469;&#21152;&#36895;&#23398;&#20064;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many tasks can be easily solved using machine learning techniques. However, some tasks cannot readily be solved using statistical models, requiring a symbolic approach instead. Program induction is one of the ways that such tasks can be solved by means of capturing an interpretable and generalizable algorithm through training. However, contemporary approaches to program induction are not sophisticated enough to readily be applied to various types of tasks as they tend to be formulated as a single, all-encompassing model, usually parameterized by neural networks. In an attempt to make program induction a viable solution for many scenarios, we propose a framework for learning parameterized programs via search gradients using evolution strategies. This formulation departs from traditional program induction as it allows for the programmer to impart task-specific code to the program 'sketch', while also enjoying the benefits of accelerated learning through end-to-end gradient-based optimiza
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#31639;&#27861;hFedF&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#23458;&#25143;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06974</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#34701;&#21512;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-linear Fusion in Federated Learning: A Hypernetwork Approach to Federated Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#31639;&#27861;hFedF&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#38750;&#32447;&#24615;&#34701;&#21512;&#23458;&#25143;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#24182;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#36798;&#21040;&#20102;&#20248;&#31168;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#22810;&#20010;&#23458;&#25143;&#20849;&#21516;&#35757;&#32451;&#20849;&#20139;&#20840;&#23616;&#27169;&#22411;&#30340;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#20986;&#29616;&#12290;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#21644;&#23454;&#29992;&#30340;FL&#26694;&#26550;&#65292;&#25193;&#23637;&#20854;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#20197;&#36866;&#24212;&#26410;&#30693;&#39046;&#22495;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#65288;FDG&#65289;&#65292;&#30446;&#21069;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32852;&#37030;&#31639;&#27861;&#65292;&#31216;&#20026;hFedF&#65288;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#32852;&#37030;&#34701;&#21512;&#65289;&#65292;&#26088;&#22312;&#24357;&#21512;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#31243;&#24230;&#30340;&#39046;&#22495;&#36716;&#31227;&#12290;&#22522;&#26412;&#19978;&#65292;&#36229;&#32593;&#32476;&#25903;&#25345;&#23545;&#23458;&#25143;&#27169;&#22411;&#36827;&#34892;&#38750;&#32447;&#24615;&#34701;&#21512;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#23545;&#20010;&#24615;&#21270;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;FL&#20013;&#24378;&#22823;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#35265;&#35299;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;DG&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) has emerged as a promising paradigm in which multiple clients collaboratively train a shared global model while preserving data privacy. To create a robust and practicable FL framework, it is crucial to extend its ability to generalize well to unseen domains - a problem referred to as federated Domain Generalization (FDG), being still under-explored. We propose an innovative federated algorithm, termed hFedF for hypernetwork-based Federated Fusion, designed to bridge the performance gap between generalization and personalization, capable of addressing various degrees of domain shift. Essentially, the hypernetwork supports a non-linear fusion of client models enabling a comprehensive understanding of the underlying data distribution. We encompass an extensive discussion and provide novel insights into the tradeoff between personalization and generalization in FL. The proposed algorithm outperforms strong benchmarks on three widely-used data sets for DG in an exce
&lt;/p&gt;</description></item><item><title>&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29305;&#23450;&#20107;&#20214;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MUCSUM&#65292;&#24182;&#23637;&#31034;&#20102;EKS&#19982;&#20256;&#32479;&#25688;&#35201;&#21644;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06973</link><description>&lt;p&gt;
&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Event-Keyed Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06973
&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#29305;&#23450;&#20107;&#20214;&#29983;&#25104;&#19978;&#19979;&#25991;&#21270;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;MUCSUM&#65292;&#24182;&#23637;&#31034;&#20102;EKS&#19982;&#20256;&#32479;&#25688;&#35201;&#21644;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#27604;&#36739;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#20107;&#20214;&#20851;&#38190;&#25688;&#35201;&#65288;EKS&#65289;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#25688;&#35201;&#21644;&#25991;&#26723;&#32423;&#20107;&#20214;&#25552;&#21462;&#32467;&#21512;&#36215;&#26469;&#65292;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#25991;&#26723;&#21644;&#25552;&#21462;&#30340;&#20107;&#20214;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#20010;&#19978;&#19979;&#25991;&#21270;&#30340;&#29305;&#23450;&#20107;&#20214;&#25688;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;MUCSUM&#65292;&#21253;&#25324;&#32463;&#20856;MUC-4&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20107;&#20214;&#30340;&#25688;&#35201;&#65292;&#20197;&#21450;&#19968;&#32452;&#22522;&#32447;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25688;&#35201;&#25991;&#29486;&#20013;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26631;&#20934;&#20197;&#21450;&#26356;&#22823;&#30340;&#21069;&#27839;&#27169;&#22411;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;EKS&#31616;&#21270;&#20026;&#20256;&#32479;&#30340;&#25688;&#35201;&#25110;&#32467;&#26500;&#21040;&#25991;&#26412;&#30340;&#21435;&#38500;&#37117;&#20250;&#24471;&#21040;&#36739;&#24046;&#30340;&#30446;&#26631;&#20107;&#20214;&#25688;&#35201;&#65292;&#24182;&#19988;MUCSUM&#26159;&#36825;&#19968;&#20219;&#21153;&#30340;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#21442;&#32771;&#25688;&#35201;&#21644;&#27169;&#22411;&#25688;&#35201;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23545;&#32467;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce event-keyed summarization (EKS), a novel task that marries traditional summarization and document-level event extraction, with the goal of generating a contextualized summary for a specific event, given a document and an extracted event structure. We introduce a dataset for this task, MUCSUM, consisting of summaries of all events in the classic MUC-4 dataset, along with a set of baselines that comprises both pretrained LM standards in the summarization literature, as well as larger frontier models. We show that ablations that reduce EKS to traditional summarization or structure-to-text yield inferior summaries of target events and that MUCSUM is a robust benchmark for this task. Lastly, we conduct a human evaluation of both reference and model summaries, and provide some detailed analysis of the results.
&lt;/p&gt;</description></item><item><title>TabPFN&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;tabular&#25968;&#25454;&#30340;Transformer&#27169;&#22411;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;TabPFN&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#35268;&#27169;&#38480;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#25968;&#25454;&#33976;&#39311;(ICD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;TabPFN&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06971</link><description>&lt;p&gt;
&#20351;&#29992;TabPFN&#36827;&#34892;&#19978;&#19979;&#25991;&#25968;&#25454;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
In-Context Data Distillation with TabPFN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06971
&lt;/p&gt;
&lt;p&gt;
TabPFN&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;tabular&#25968;&#25454;&#30340;Transformer&#27169;&#22411;&#65292;&#20855;&#26377;&#21331;&#36234;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;TabPFN&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25968;&#25454;&#35268;&#27169;&#38480;&#21046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#25968;&#25454;&#33976;&#39311;(ICD)&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;TabPFN&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;tabular&#25968;&#25454;&#65292;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#65288;&#22914;XGBoost&#65289;&#22312;&#35813;&#39046;&#22495;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;TabPFN&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;tabular&#25968;&#25454;&#35774;&#35745;&#30340;Transformer&#27169;&#22411;&#65292;&#20854;&#21331;&#36234;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#19982;XGBoost&#30456;&#23218;&#32654;&#65292;&#26080;&#38656;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25110;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;TabPFN&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#25968;&#25454;&#35268;&#27169;&#38480;&#21046;&#30340;&#24433;&#21709;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#19978;&#19979;&#25991;&#25968;&#25454;&#33976;&#39311;(ICD)&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#20248;&#21270;TabPFN&#30340;&#19978;&#19979;&#25991;&#12290;ICD&#20351;&#24471;TabPFN&#33021;&#22815;&#22312;&#26377;&#38480;&#20869;&#23384;&#39044;&#31639;&#19979;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;TabPFN&#30340;&#20108;&#27425;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#20294;&#20195;&#20215;&#26159;&#32447;&#24615;&#25968;&#37327;&#30340;&#35843;&#20248;&#27493;&#39588;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32463;&#36807;ICD&#22686;&#24378;&#30340;TabPFN&#23637;&#29616;&#20986;&#38750;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models have revolutionized tasks in computer vision and natural language processing. However, in the realm of tabular data, tree-based models like XGBoost continue to dominate. TabPFN, a transformer model tailored for tabular data, mirrors recent foundation models in its exceptional in-context learning capability, being competitive with XGBoost's performance without the need for task-specific training or hyperparameter tuning. Despite its promise, TabPFN's applicability is hindered by its data size constraint, limiting its use in real-world scenarios. To address this, we present in-context data distillation (ICD), a novel methodology that effectively eliminates these constraints by optimizing TabPFN's context. ICD efficiently enables TabPFN to handle significantly larger datasets with a fixed memory budget, improving TabPFN's quadratic memory complexity but at the cost of a linear number of tuning steps. Notably, TabPFN, enhanced with ICD, demonstrates very strong performanc
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#21644;&#38543;&#26426;&#26053;&#34892;&#26102;&#38388;&#30340;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#24773;&#22659;&#65292;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#25552;&#20379;&#36817;&#20284;&#35299;&#26469;&#26368;&#23567;&#21270;&#24635;&#36816;&#36755;&#25104;&#26412;&#21644;&#39044;&#26399;&#36831;&#21040;&#32602;&#27454;&#12290;</title><link>https://arxiv.org/abs/2402.06968</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#30340;&#24773;&#22659;&#38543;&#26426;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Contextual Stochastic Vehicle Routing with Time Windows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#21644;&#38543;&#26426;&#26053;&#34892;&#26102;&#38388;&#30340;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#24773;&#22659;&#65292;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#25552;&#20379;&#36817;&#20284;&#35299;&#26469;&#26368;&#23567;&#21270;&#24635;&#36816;&#36755;&#25104;&#26412;&#21644;&#39044;&#26399;&#36831;&#21040;&#32602;&#27454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26102;&#38388;&#31383;&#21475;&#21644;&#38543;&#26426;&#26053;&#34892;&#26102;&#38388;&#30340;&#36710;&#36742;&#36335;&#24452;&#35268;&#21010;&#38382;&#39064;&#65288;VRPTW&#65289;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#20915;&#31574;&#32773;&#22312;&#20570;&#20986;&#36335;&#24452;&#20915;&#31574;&#20043;&#21069;&#35266;&#23519;&#21040;&#30456;&#20851;&#30340;&#24773;&#22659;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#29992;&#29305;&#24449;&#21464;&#37327;&#34920;&#31034;&#12290;&#23613;&#31649;&#20851;&#20110;&#38543;&#26426;VRP&#30340;&#25991;&#29486;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#29305;&#24449;&#21464;&#37327;&#30340;&#25972;&#21512;&#24471;&#21040;&#30340;&#20851;&#27880;&#21364;&#24456;&#26377;&#38480;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20855;&#26377;&#19978;&#19979;&#25991;&#24773;&#22659;&#30340;&#38543;&#26426;VRPTW&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#22312;&#35266;&#23519;&#21040;&#30340;&#29305;&#24449;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#24635;&#36816;&#36755;&#25104;&#26412;&#21644;&#39044;&#26399;&#36831;&#21040;&#32602;&#27454;&#12290;&#30001;&#20110;&#26053;&#34892;&#26102;&#38388;&#21644;&#29305;&#24449;&#30340;&#32852;&#21512;&#20998;&#24067;&#26159;&#26410;&#30693;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#25552;&#20379;&#38382;&#39064;&#36817;&#20284;&#35299;&#30340;&#26032;&#22411;&#25968;&#25454;&#39537;&#21160;&#35268;&#21010;&#27169;&#22411;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#22522;&#20110;&#28857;&#20272;&#35745;&#12289;&#26679;&#26412;&#24179;&#22343;&#20272;&#35745;&#21644;&#24809;&#32602;&#20272;&#35745;&#30340;&#35268;&#21010;&#27169;&#22411;&#65292;&#27599;&#31181;&#27169;&#22411;&#23545;&#24453;&#38543;&#26426;&#26053;&#34892;&#26102;&#38388;&#21644;&#29305;&#24449;&#30340;&#26041;&#24335;&#26377;&#19981;&#21516;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#20998;&#26525;&#23450;&#20215;&#21106;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
We study the vehicle routing problem with time windows (VRPTW) and stochastic travel times, in which the decision-maker observes related contextual information, represented as feature variables, before making routing decisions. Despite the extensive literature on stochastic VRPs, the integration of feature variables has received limited attention in this context. We introduce the contextual stochastic VRPTW, which minimizes the total transportation cost and expected late arrival penalties conditioned on the observed features. Since the joint distribution of travel times and features is unknown, we present novel data-driven prescriptive models that use historical data to provide an approximate solution to the problem. We distinguish the prescriptive models between point-based approximation, sample average approximation, and penalty-based approximation, each taking a different perspective on dealing with stochastic travel times and features. We develop specialized branch-price-and-cut al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#25552;&#21462;&#29366;&#24577;&#26426;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;RNN&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#30340;&#29366;&#24577;&#26426;&#26469;&#25913;&#36827;RNN&#30340;&#27979;&#35797;&#21644;&#30417;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.06966</link><description>&lt;p&gt;
DeepCover: &#25552;&#39640;RNN&#27979;&#35797;&#35206;&#30422;&#29575;&#21644;&#22312;&#32447;&#38169;&#35823;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29366;&#24577;&#26426;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
DeepCover: Advancing RNN Test Coverage and Online Error Prediction using State Machine Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#25552;&#21462;&#29366;&#24577;&#26426;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;RNN&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#21462;&#30340;&#29366;&#24577;&#26426;&#26469;&#25913;&#36827;RNN&#30340;&#27979;&#35797;&#21644;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#24050;&#32463;&#25104;&#20026;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;RNN&#27169;&#22411;&#30340;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#38480;&#21046;&#20102;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#24102;&#26469;&#20102;&#29702;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#36807;&#31243;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#20110;RNN&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#29366;&#24577;&#26426;&#65288;SM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#27934;&#23519;&#20854;&#20869;&#37096;&#21151;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;SM&#25552;&#21462;&#31639;&#27861;&#36890;&#36807;&#22235;&#20010;&#26032;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#65288;&#32431;&#24230;&#12289;&#20016;&#23500;&#24230;&#12289;&#20248;&#31168;&#24230;&#21644;&#35268;&#27169;&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21450;&#20854;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#36890;&#36807;&#25552;&#21462;&#30340;SM&#22312;RNN&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#20869;&#37096;&#20915;&#31574;&#36807;&#31243;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;RNN&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#38500;&#20102;&#25913;&#21892;RNN&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25152;&#25552;&#21462;&#30340;SM&#36824;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;RNN&#27169;&#22411;&#30340;&#27979;&#35797;&#19982;&#30417;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent neural networks (RNNs) have emerged as powerful tools for processing sequential data in various fields, including natural language processing and speech recognition. However, the lack of explainability in RNN models has limited their interpretability, posing challenges in understanding their internal workings. To address this issue, this paper proposes a methodology for extracting a state machine (SM) from an RNN-based model to provide insights into its internal function. The proposed SM extraction algorithm was assessed using four newly proposed metrics: Purity, Richness, Goodness, and Scale. The proposed methodology along with its assessment metrics contribute to increasing explainability in RNN models by providing a clear representation of their internal decision making process through the extracted SM. In addition to improving the explainability of RNNs, the extracted SM can be used to advance testing and and monitoring of the primary RNN-based model. To enhance RNN testi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06963</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Tree Ensembles for Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26641;&#38598;&#25104;&#30340;&#24773;&#22659;&#22810;&#33218;&#32769;&#34382;&#26426;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#32769;&#34382;&#26426;&#26041;&#27861;&#65292;&#19978;&#20449;&#24515;&#30028;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#65292;&#25972;&#21512;&#21040;&#26631;&#20934;&#21644;&#32452;&#21512;&#35774;&#32622;&#20013;&#12290;&#36890;&#36807;&#20351;&#29992;&#27969;&#34892;&#30340;&#26641;&#38598;&#25104;&#26041;&#27861;XGBoost&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;&#24403;&#24212;&#29992;&#20110;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#36947;&#36335;&#32593;&#32476;&#23548;&#33322;&#30340;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#26102;&#65292;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20943;&#23569;&#21518;&#24724;&#21644;&#35745;&#31639;&#26102;&#38388;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel framework for contextual multi-armed bandits based on tree ensembles. Our framework integrates two widely used bandit methods, Upper Confidence Bound and Thompson Sampling, for both standard and combinatorial settings. We demonstrate the effectiveness of our framework via several experimental studies, employing XGBoost, a popular tree ensemble method. Compared to state-of-the-art methods based on neural networks, our methods exhibit superior performance in terms of both regret minimization and computational runtime, when applied to benchmark datasets and the real-world application of navigation over road networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#26550;&#26500;&#31070;&#32463;&#21518;&#38376;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;12&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26550;&#26500;&#21518;&#38376;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20219;&#24847;&#35302;&#21457;&#26816;&#27979;&#22120;&#65292;&#23637;&#31034;&#20102;&#26080;&#38656;&#20154;&#24037;&#30417;&#30563;&#21363;&#21487;&#20026;&#26550;&#26500;&#24341;&#20837;&#21518;&#38376;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06957</link><description>&lt;p&gt;
&#20174;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#30340;&#26550;&#26500;&#31070;&#32463;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Architectural Neural Backdoors from First Principles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20174;&#31532;&#19968;&#21407;&#29702;&#26500;&#24314;&#26550;&#26500;&#31070;&#32463;&#21518;&#38376;&#30340;&#26041;&#27861;&#65292;&#24182;&#25551;&#36848;&#20102;12&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26550;&#26500;&#21518;&#38376;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20219;&#24847;&#35302;&#21457;&#26816;&#27979;&#22120;&#65292;&#23637;&#31034;&#20102;&#26080;&#38656;&#20154;&#24037;&#30417;&#30563;&#21363;&#21487;&#20026;&#26550;&#26500;&#24341;&#20837;&#21518;&#38376;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#25913;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#21442;&#25968;&#26469;&#21019;&#24314;&#21518;&#38376;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#31181;&#26356;&#38544;&#34109;&#30340;&#23041;&#32961;&#65306;&#22312;&#32593;&#32476;&#26550;&#26500;&#23450;&#20041;&#20013;&#23884;&#20837;&#30340;&#21518;&#38376;&#12290;&#36825;&#28041;&#21450;&#21040;&#27880;&#20837;&#24120;&#35265;&#30340;&#26550;&#26500;&#32452;&#20214;&#65292;&#22914;&#28608;&#27963;&#20989;&#25968;&#21644;&#27744;&#21270;&#23618;&#65292;&#20197;&#24039;&#22937;&#22320;&#24341;&#20837;&#19968;&#20010;&#25345;&#32493;&#23384;&#22312;&#30340;&#21518;&#38376;&#34892;&#20026;&#65292;&#21363;&#20351;&#22312;&#37325;&#26032;&#35757;&#32451;&#21518;&#20063;&#26159;&#22914;&#27492;&#12290;&#28982;&#32780;&#65292;&#26550;&#26500;&#21518;&#38376;&#30340;&#20840;&#37096;&#33539;&#22260;&#21644;&#24433;&#21709;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;Bober-Irizar&#31561;&#20154;[2023]&#39318;&#27425;&#24341;&#20837;&#20102;&#26550;&#26500;&#21518;&#38376;&#65307;&#20182;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20026;&#26827;&#30424;&#22270;&#26696;&#21019;&#24314;&#21518;&#38376;&#65292;&#20294;&#20174;&#26410;&#35299;&#37322;&#22914;&#20309;&#38024;&#23545;&#20219;&#24847;&#35302;&#21457;&#27169;&#24335;&#36827;&#34892;&#23450;&#20301;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#29992;&#20110;&#26080;&#20154;&#30417;&#30563;&#22320;&#20026;&#26550;&#26500;&#24341;&#20837;&#21518;&#38376;&#30340;&#20219;&#24847;&#35302;&#21457;&#26816;&#27979;&#22120;&#12290;&#36825;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26550;&#26500;&#21518;&#38376;&#30340;&#27010;&#24565;&#24182;&#23558;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#25551;&#36848;&#20102;12&#31181;&#19981;&#21516;&#31867;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#26816;&#27979;&#27492;&#31867;&#21518;&#38376;&#30340;&#22256;&#38590;&#31243;&#24230;&#65292;...
&lt;/p&gt;
&lt;p&gt;
While previous research backdoored neural networks by changing their parameters, recent work uncovered a more insidious threat: backdoors embedded within the definition of the network's architecture. This involves injecting common architectural components, such as activation functions and pooling layers, to subtly introduce a backdoor behavior that persists even after (full re-)training. However, the full scope and implications of architectural backdoors have remained largely unexplored. Bober-Irizar et al. [2023] introduced the first architectural backdoor; they showed how to create a backdoor for a checkerboard pattern, but never explained how to target an arbitrary trigger pattern of choice. In this work we construct an arbitrary trigger detector which can be used to backdoor an architecture with no human supervision. This leads us to revisit the concept of architecture backdoors and taxonomise them, describing 12 distinct types. To gauge the difficulty of detecting such backdoors, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2402.06955</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Training dynamics in Physics-Informed Neural Networks with feature mapping
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#26465;&#20214;&#27491;&#23450;&#24452;&#21521;&#22522;&#20989;&#25968;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#12290;&#36825;&#20026;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#24102;&#26469;&#20102;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#30340;&#26631;&#24535;&#24615;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#23613;&#31649;&#20854;&#21464;&#20307;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#26469;&#33258;&#26356;&#24191;&#27867;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#30740;&#31350;&#30340;&#29305;&#24449;&#26144;&#23556;&#30340;&#32463;&#39564;&#24615;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#36890;&#36807;&#26497;&#38480;&#20849;&#36717;&#26680;&#21644;&#31070;&#32463;&#20999;&#21521;&#26680;&#26469;&#30740;&#31350;&#24102;&#26377;&#29305;&#24449;&#26144;&#23556;&#23618;&#30340;PINNs&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#24120;&#29992;&#30340;&#22522;&#20110;&#20613;&#37324;&#21494;&#21464;&#25442;&#30340;&#29305;&#24449;&#26144;&#23556;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#26465;&#20214;&#27491;&#23450;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#20316;&#20026;&#26356;&#22909;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#38598;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22312;&#22352;&#26631;&#36755;&#20837;&#32593;&#32476;&#20013;&#23454;&#29616;&#65292;&#24182;&#21463;&#30410;&#20110;&#24191;&#27867;&#30340;PINNs&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks (PINNs) have emerged as an iconic machine learning approach for solving Partial Differential Equations (PDEs). Although its variants have achieved significant progress, the empirical success of utilising feature mapping from the wider Implicit Neural Representations studies has been substantially neglected. We investigate the training dynamics of PINNs with a feature mapping layer via the limiting Conjugate Kernel and Neural Tangent Kernel, which sheds light on the convergence and generalisation of the model. We also show the inadequacy of commonly used Fourier-based feature mapping in some scenarios and propose the conditional positive definite Radial Basis Function as a better alternative. The empirical results reveal the efficacy of our method in diverse forward and inverse problem sets. This simple technique can be easily implemented in coordinate input networks and benefits the broad PINNs research.
&lt;/p&gt;</description></item><item><title>OpenFedLLM&#26159;&#19968;&#20010;&#31616;&#27905;&#12289;&#38598;&#25104;&#12289;&#30740;&#31350;&#21451;&#22909;&#30340;&#26694;&#26550;/&#20195;&#30721;&#24211;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#21644;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#20844;&#24320;&#25968;&#25454;&#26543;&#31469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06954</link><description>&lt;p&gt;
OpenFedLLM&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06954
&lt;/p&gt;
&lt;p&gt;
OpenFedLLM&#26159;&#19968;&#20010;&#31616;&#27905;&#12289;&#38598;&#25104;&#12289;&#30740;&#31350;&#21451;&#22909;&#30340;&#26694;&#26550;/&#20195;&#30721;&#24211;&#65292;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#20998;&#25955;&#30340;&#31169;&#26377;&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21327;&#20316;&#21644;&#38544;&#31169;&#20445;&#25252;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#20844;&#24320;&#25968;&#25454;&#26543;&#31469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#26356;&#22810;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#39640;&#36136;&#37327;&#30340;&#20844;&#24320;&#25968;&#25454;&#23558;&#22312;&#20960;&#24180;&#20869;&#29992;&#23613;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#24403;&#20195;LLM&#30340;&#28508;&#22312;&#19979;&#19968;&#27493;&#65306;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#22312;&#26410;&#20805;&#20998;&#21033;&#29992;&#30340;&#20998;&#24067;&#24335;&#31169;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#21327;&#20316;&#21644;&#20445;&#25252;&#38544;&#31169;&#30340;LLM&#35757;&#32451;&#65292;&#22810;&#20010;&#25968;&#25454;&#25152;&#26377;&#32773;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#32780;&#19981;&#20256;&#36755;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#27905;&#12289;&#38598;&#25104;&#21644;&#30740;&#31350;&#21451;&#22909;&#30340;&#26694;&#26550;/&#20195;&#30721;&#24211;&#65292;&#21517;&#20026;OpenFedLLM&#12290;&#23427;&#28085;&#30422;&#20102;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#36981;&#24490;&#25351;&#20196;&#33021;&#21147;&#30340;&#32852;&#37030;&#25351;&#20196;&#35843;&#20248;&#12289;&#29992;&#20110;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#32852;&#37030;&#20215;&#20540;&#23545;&#40784;&#20197;&#21450;7&#20010;&#20195;&#34920;&#24615;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;OpenFedLLM&#25903;&#25345;&#22312;&#22810;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;8&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65307;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Trained on massive publicly available data, large language models (LLMs) have demonstrated tremendous success across various fields. While more data contributes to better performance, a disconcerting reality is that high-quality public data will be exhausted in a few years. In this paper, we offer a potential next step for contemporary LLMs: collaborative and privacy-preserving LLM training on the underutilized distributed private data via federated learning (FL), where multiple data owners collaboratively train a shared model without transmitting raw data. To achieve this, we build a concise, integrated, and research-friendly framework/codebase, named OpenFedLLM. It covers federated instruction tuning for enhancing instruction-following capability, federated value alignment for aligning with human values, and 7 representative FL algorithms. Besides, OpenFedLLM supports training on diverse domains, where we cover 8 training datasets; and provides comprehensive evaluations, where we cov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#20013;&#20351;&#29992;&#21152;&#26435;&#34394;&#25311;&#35266;&#27979;&#36827;&#34892;&#22686;&#37327;&#20449;&#24565;&#26356;&#26032;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#21152;&#26435;&#35266;&#27979;&#26469;&#35843;&#33410;&#27169;&#22411;&#65292;&#23454;&#29616;&#19982;&#21407;&#22987;&#21518;&#39564;&#30456;&#21516;&#30340;&#25512;&#26029;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06940</link><description>&lt;p&gt;
&#20351;&#29992;&#21152;&#26435;&#34394;&#25311;&#35266;&#27979;&#23454;&#29616;&#39640;&#25928;&#30340;&#22686;&#37327;&#20449;&#24565;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Efficient Incremental Belief Updates Using Weighted Virtual Observations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#20013;&#20351;&#29992;&#21152;&#26435;&#34394;&#25311;&#35266;&#27979;&#36827;&#34892;&#22686;&#37327;&#20449;&#24565;&#26356;&#26032;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#26500;&#24314;&#19968;&#32452;&#21152;&#26435;&#35266;&#27979;&#26469;&#35843;&#33410;&#27169;&#22411;&#65292;&#23454;&#29616;&#19982;&#21407;&#22987;&#21518;&#39564;&#30456;&#21516;&#30340;&#25512;&#26029;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#20102;&#22312;&#36125;&#21494;&#26031;&#32479;&#35745;&#27169;&#22411;&#20013;&#33945;&#29305;&#21345;&#27931;&#25512;&#26029;&#29615;&#22659;&#19979;&#30340;&#22686;&#37327;&#20449;&#24565;&#26356;&#26032;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#30001;&#27010;&#29575;&#32534;&#31243;&#34920;&#31034;&#12290;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;&#21644;&#26679;&#26412;&#36924;&#36817;&#30340;&#21518;&#39564;&#27010;&#29575;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#20102;&#19968;&#32452;&#21152;&#26435;&#35266;&#27979;&#26469;&#35843;&#33410;&#27169;&#22411;&#65292;&#20174;&#32780;&#25512;&#26029;&#32467;&#26524;&#19982;&#21407;&#22987;&#21518;&#39564;&#30456;&#21516;&#12290;&#35813;&#38382;&#39064;&#20986;&#29616;&#22312;&#22810;&#23618;&#24314;&#27169;&#12289;&#22686;&#37327;&#25512;&#26029;&#21644;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#25512;&#26029;&#31561;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#36873;&#25321;&#19968;&#32452;&#34394;&#25311;&#35266;&#27979;&#20540;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#25928;&#30340;&#35745;&#31639;&#20248;&#21270;&#36807;&#31243;&#25214;&#21040;&#35266;&#27979;&#26435;&#37325;&#65292;&#20351;&#24471;&#37325;&#24314;&#30340;&#21518;&#39564;&#19982;&#21407;&#22987;&#21518;&#39564;&#19968;&#33268;&#25110;&#36817;&#20284;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#25945;&#23398;&#31034;&#20363;&#21644;&#26696;&#20363;&#30740;&#31350;&#23454;&#26045;&#24182;&#24212;&#29992;&#20102;&#35813;&#35299;&#20915;&#26041;&#26696;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20379;&#30340;&#21442;&#32771;&#23454;&#29616;&#19981;&#20381;&#36182;&#20110;&#27010;&#29575;&#32534;&#31243;&#35821;&#35328;&#25110;&#25512;&#26029;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an algorithmic solution to the problem of incremental belief updating in the context of Monte Carlo inference in Bayesian statistical models represented by probabilistic programs. Given a model and a sample-approximated posterior, our solution constructs a set of weighted observations to condition the model such that inference would result in the same posterior. This problem arises e.g. in multi-level modelling, incremental inference, inference in presence of privacy constraints. First, a set of virtual observations is selected, then, observation weights are found through a computationally efficient optimization procedure such that the reconstructed posterior coincides with or closely approximates the original posterior. We implement and apply the solution to a number of didactic examples and case studies, showing efficiency and robustness of our approach. The provided reference implementation is agnostic to the probabilistic programming language or the inference algorithm, 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06938</link><description>&lt;p&gt;
&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Efficient Resource Scheduling for Distributed Infrastructures Using Negotiation Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06938
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35848;&#21028;&#33021;&#21147;&#30340;&#20998;&#24067;&#24335;&#22522;&#30784;&#35774;&#26045;&#39640;&#25928;&#36164;&#28304;&#35843;&#24230;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#20195;&#29702;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#65292;&#20248;&#21270;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#30340;&#21327;&#35758;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#20449;&#24687;&#21644;&#20114;&#32852;&#32593;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20652;&#29983;&#20102;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#20449;&#24687;&#12290;&#20449;&#24687;&#29190;&#28856;&#25512;&#21160;&#35768;&#22810;&#20225;&#19994;&#25110;&#20010;&#20154;&#23547;&#27714;&#31199;&#29992;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#26469;&#23558;&#20182;&#20204;&#30340;&#24212;&#29992;&#31243;&#24207;&#25918;&#32622;&#22312;&#20113;&#20013;&#12290;&#28982;&#32780;&#65292;&#20113;&#35745;&#31639;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#20043;&#38388;&#36798;&#25104;&#30340;&#21327;&#35758;&#36890;&#24120;&#19981;&#39640;&#25928;&#12290;&#35768;&#22810;&#22240;&#32032;&#24433;&#21709;&#25928;&#29575;&#65292;&#22914;&#25552;&#20379;&#21830;&#20113;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#30340;&#38386;&#32622;&#21644;&#23545;&#23458;&#25143;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#19968;&#20010;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24341;&#20837;&#19968;&#31181;&#32508;&#21512;&#30340;&#12289;&#35848;&#21028;&#31867;&#30340;&#21338;&#24328;&#65292;&#24182;&#26681;&#25454;&#35848;&#21028;&#32467;&#26524;&#23433;&#25490;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#31946;&#36923;&#36753;&#30340;&#22522;&#20110;&#20195;&#29702;&#30340;&#33258;&#21160;&#35848;&#21028;&#31995;&#32479;&#29992;&#20110;&#36164;&#28304;&#35843;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23436;&#25104;&#19968;&#23545;&#19968;&#30340;&#33258;&#21160;&#35848;&#21028;&#36807;&#31243;&#65292;&#24182;&#20026;&#25552;&#20379;&#21830;&#21644;&#23458;&#25143;&#29983;&#25104;&#26368;&#20248;&#30340;&#25253;&#20215;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#25104;&#21592;&#20989;&#25968;&#12289;&#27169;&#31946;&#35268;&#21017;&#38598;&#21644;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#23545;&#36164;&#28304;&#35843;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few decades, the rapid development of information and internet technologies has spawned massive amounts of data and information. The information explosion drives many enterprises or individuals to seek to rent cloud computing infrastructure to put their applications in the cloud. However, the agreements reached between cloud computing providers and clients are often not efficient. Many factors affect the efficiency, such as the idleness of the providers' cloud computing infrastructure, and the additional cost to the clients. One possible solution is to introduce a comprehensive, bargaining game (a type of negotiation), and schedule resources according to the negotiation results. We propose an agent-based auto-negotiation system for resource scheduling based on fuzzy logic. The proposed method can complete a one-to-one auto-negotiation process and generate optimal offers for the provider and client. We compare the impact of different member functions, fuzzy rule sets, and ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#24067;&#36801;&#31227;&#24773;&#20917;&#19979;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#35786;&#26029;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.06937</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#36801;&#31227;&#19979;&#35780;&#20272;3D&#22270;&#20687;&#20998;&#21106;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Assessing Uncertainty Estimation Methods for 3D Image Segmentation under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20998;&#24067;&#36801;&#31227;&#24773;&#20917;&#19979;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#35786;&#26029;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#28982;&#32780;&#22312;&#21307;&#23398;&#24433;&#20687;&#20026;&#22522;&#30784;&#30340;&#30142;&#30149;&#26816;&#27979;&#21644;&#35786;&#26029;&#26041;&#38754;&#30340;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#23384;&#22312;&#20998;&#24067;&#36801;&#31227;&#12290;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#37096;&#32626;&#30340;&#27169;&#22411;&#20250;&#36935;&#21040;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#26126;&#26174;&#19981;&#21516;&#30340;&#26679;&#26412;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#38382;&#39064;&#12290;&#36825;&#20010;&#38480;&#21046;&#24433;&#21709;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#21147;&#21644;&#21487;&#38752;&#24615;&#12290;&#22240;&#27492;&#65292;&#35782;&#21035;&#33021;&#22815;&#22312;&#20581;&#24247;&#39046;&#22495;&#30340;&#20998;&#24067;&#36801;&#31227;&#24773;&#20917;&#19979;&#20135;&#29983;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#20808;&#36827;&#30340;&#36125;&#21494;&#26031;&#21644;&#38750;&#36125;&#21494;&#26031;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#36801;&#31227;&#26679;&#26412;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#20998;&#21106;&#20219;&#21153;&#20013;&#21487;&#38752;&#21644;&#21487;&#20449;&#30340;&#35786;&#26029;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#35774;&#35745;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning has witnessed extensive adoption across various sectors, yet its application in medical image-based disease detection and diagnosis remains challenging due to distribution shifts in real-world data. In practical settings, deployed models encounter samples that differ significantly from the training dataset, especially in the health domain, leading to potential performance issues. This limitation hinders the expressiveness and reliability of deep learning models in health applications. Thus, it becomes crucial to identify methods capable of producing reliable uncertainty estimation in the context of distribution shifts in the health sector. In this paper, we explore the feasibility of using cutting-edge Bayesian and non-Bayesian methods to detect distributionally shifted samples, aiming to achieve reliable and trustworthy diagnostic predictions in segmentation task. Specifically, we compare three distinct uncertainty estimation methods, each designed to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAGRA&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#37325;&#35201;&#30340;&#23567;&#23646;&#24615;&#23376;&#22270;&#26469;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20248;&#21270;&#20854;&#23646;&#24615;&#21521;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#38754;&#22320;&#25506;&#32034;&#25152;&#26377;&#28508;&#22312;&#37325;&#35201;&#30340;&#23376;&#22270;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.06932</link><description>&lt;p&gt;
&#23398;&#20064;&#23646;&#24615;&#22270;&#20803;&#65306;&#36890;&#36807;&#21487;&#35757;&#32451;&#23646;&#24615;&#30340;&#22270;&#20803;&#36827;&#34892;&#39044;&#27979;&#24615;&#22270;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Learning Attributed Graphlets: Predictive Graph Mining by Graphlets with Trainable Attribute
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LAGRA&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#31639;&#27861;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#37325;&#35201;&#30340;&#23567;&#23646;&#24615;&#23376;&#22270;&#26469;&#23454;&#29616;&#22270;&#20998;&#31867;&#65292;&#24182;&#20248;&#21270;&#20854;&#23646;&#24615;&#21521;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#19988;&#33021;&#22815;&#20840;&#38754;&#22320;&#25506;&#32034;&#25152;&#26377;&#28508;&#22312;&#37325;&#35201;&#30340;&#23376;&#22270;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#38382;&#39064;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#65292;&#23454;&#29616;&#19968;&#20010;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23646;&#24615;&#22270;&#25968;&#25454;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;LAGRA&#65288;Learning Attributed GRAphlets&#65289;&#12290;LAGRA&#23398;&#20064;&#23567;&#23646;&#24615;&#23376;&#22270;&#65288;&#31216;&#20026;&#23646;&#24615;&#22270;&#20803;&#65289;&#30340;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#21516;&#26102;&#20248;&#21270;&#23427;&#20204;&#30340;&#23646;&#24615;&#21521;&#37327;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#19968;&#31181;&#32467;&#21512;&#20102;&#23376;&#22270;&#32467;&#26500;&#21644;&#23646;&#24615;&#21521;&#37327;&#30340;&#32452;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#31867;&#21035;&#12290;LAGRA&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#25152;&#26377;&#23376;&#22270;&#32467;&#26500;&#37117;&#21487;&#20197;&#34987;&#35270;&#20026;&#23646;&#24615;&#22270;&#20803;&#30340;&#20505;&#36873;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20840;&#38754;&#22320;&#25506;&#32034;&#25152;&#26377;&#28508;&#22312;&#37325;&#35201;&#30340;&#23376;&#22270;&#65292;&#20294;&#26174;&#28982;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#29616;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20462;&#21098;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;...
&lt;/p&gt;
&lt;p&gt;
The graph classification problem has been widely studied; however, achieving an interpretable model with high predictive performance remains a challenging issue. This paper proposes an interpretable classification algorithm for attributed graph data, called LAGRA (Learning Attributed GRAphlets). LAGRA learns importance weights for small attributed subgraphs, called attributed graphlets (AGs), while simultaneously optimizing their attribute vectors. This enables us to obtain a combination of subgraph structures and their attribute vectors that strongly contribute to discriminating different classes. A significant characteristics of LAGRA is that all the subgraph structures in the training dataset can be considered as a candidate structures of AGs. This approach can explore all the potentially important subgraphs exhaustively, but obviously, a naive implementation can require a large amount of computations. To mitigate this issue, we propose an efficient pruning strategy by combining the
&lt;/p&gt;</description></item><item><title>ORIENT&#26159;&#19968;&#31181;&#38754;&#21521;6G&#30340;&#20248;&#20808;&#26435;&#24863;&#30693;&#33410;&#33021;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#26381;&#21153;&#23454;&#20363;&#25918;&#32622;&#21644;&#20998;&#37197;&#12289;&#36335;&#24452;&#36873;&#25321;&#21644;&#35831;&#27714;&#20248;&#20808;&#32423;&#30340;&#32852;&#21512;&#38382;&#39064;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#25972;&#20307;&#21033;&#28070;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.06931</link><description>&lt;p&gt;
ORIENT:&#19968;&#31181;&#38754;&#21521;6G&#20013;&#24310;&#36831;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#30340;&#20248;&#20808;&#26435;&#24863;&#30693;&#33410;&#33021;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ORIENT: A Priority-Aware Energy-Efficient Approach for Latency-Sensitive Applications in 6G
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06931
&lt;/p&gt;
&lt;p&gt;
ORIENT&#26159;&#19968;&#31181;&#38754;&#21521;6G&#30340;&#20248;&#20808;&#26435;&#24863;&#30693;&#33410;&#33021;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#35299;&#20915;&#26381;&#21153;&#23454;&#20363;&#25918;&#32622;&#21644;&#20998;&#37197;&#12289;&#36335;&#24452;&#36873;&#25321;&#21644;&#35831;&#27714;&#20248;&#20808;&#32423;&#30340;&#32852;&#21512;&#38382;&#39064;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#25972;&#20307;&#21033;&#28070;&#65292;&#24182;&#22312;&#38271;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;6G&#21040;&#26469;&#30340;&#26399;&#26395;&#22686;&#21152;&#65292;&#20154;&#20204;&#23545;&#35745;&#31639;&#21644;&#32593;&#32476;&#30340;&#33021;&#32791;&#22686;&#38271;&#34920;&#31034;&#25285;&#24551;&#12290;&#39044;&#35745;&#36830;&#25509;&#35774;&#22791;&#30340;&#28608;&#22686;&#21644;&#36164;&#28304;&#35201;&#27714;&#39640;&#30340;&#24212;&#29992;&#31243;&#24207;&#23558;&#20026;&#33021;&#28304;&#36164;&#28304;&#24102;&#26469;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#36807;&#21435;&#24050;&#32463;&#35752;&#35770;&#20102;&#21487;&#25345;&#32493;&#30340;&#36164;&#28304;&#20998;&#37197;&#31574;&#30053;&#65292;&#20294;&#36825;&#20123;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#22495;&#32534;&#25490;&#19978;&#65292;&#25110;&#32773;&#24573;&#30053;&#20102;6G&#25552;&#20986;&#30340;&#29420;&#29305;&#35201;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26381;&#21153;&#23454;&#20363;&#30340;&#25918;&#32622;&#21644;&#20998;&#37197;&#12289;&#36335;&#24452;&#36873;&#25321;&#21644;&#35831;&#27714;&#20248;&#20808;&#32423;&#30340;&#32852;&#21512;&#38382;&#39064;&#65292;&#31216;&#20026;PIRA&#12290;&#30446;&#26631;&#20989;&#25968;&#26159;&#36890;&#36807;&#26368;&#22823;&#21270;&#21516;&#26102;&#25903;&#25345;&#30340;&#35831;&#27714;&#25968;&#37327;&#26469;&#26368;&#22823;&#21270;&#31995;&#32479;&#30340;&#25972;&#20307;&#21033;&#28070;&#65292;&#21516;&#26102;&#22312;&#38271;&#26102;&#38388;&#20869;&#26368;&#23567;&#21270;&#33021;&#32791;&#12290;&#27492;&#22806;&#65292;&#36824;&#32771;&#34385;&#20102;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#31471;&#21040;&#31471;&#24310;&#36831;&#35201;&#27714;&#21644;&#36164;&#28304;&#23481;&#37327;&#32422;&#26463;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#25490;&#38431;&#35770;&#26469;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Anticipation for 6G's arrival comes with growing concerns about increased energy consumption in computing and networking. The expected surge in connected devices and resource-demanding applications presents unprecedented challenges for energy resources. While sustainable resource allocation strategies have been discussed in the past, these efforts have primarily focused on single-domain orchestration or ignored the unique requirements posed by 6G. To address this gap, we investigate the joint problem of service instance placement and assignment, path selection, and request prioritization, dubbed PIRA. The objective function is to maximize the system's overall profit as a function of the number of concurrently supported requests while simultaneously minimizing energy consumption over an extended period of time. In addition, end-to-end latency requirements and resource capacity constraints are considered for computing and networking resources, where queuing theory is utilized to estimate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CochCeps-Augment&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Cochlear Cepstrum&#30340;&#25513;&#34109;&#22686;&#24378;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06923</link><description>&lt;p&gt;
CochCeps-Augment&#65306;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;Cochlear Cepstrum&#30340;&#25513;&#34109;&#30340;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CochCeps-Augment: A Novel Self-Supervised Contrastive Learning Using Cochlear Cepstrum-based Masking for Speech Emotion Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06923
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CochCeps-Augment&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Cochlear Cepstrum&#30340;&#25513;&#34109;&#22686;&#24378;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#22122;&#22768;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#29992;&#20110;&#24773;&#24863;&#20869;&#23481;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21487;&#20197;&#34987;&#22122;&#22768;&#24178;&#25200;&#20005;&#37325;&#38477;&#20302;&#65292;&#24433;&#21709;&#23545;&#35821;&#38899;&#30340;&#22797;&#26434;&#26102;&#22495;&#21644;&#39057;&#35889;&#20449;&#24687;&#32467;&#26500;&#36827;&#34892;&#24314;&#27169;&#30340;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#30340;SSL&#20197;&#21450;&#26032;&#30340;&#38899;&#39057;&#29305;&#23450;&#30340;SSL&#20195;&#29702;&#20219;&#21153;&#65288;&#22914;&#26102;&#22495;&#21644;&#39057;&#22495;&#25513;&#34109;&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#28304;&#33258;&#22270;&#20687;&#22686;&#24378;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#21019;&#26032;&#22312;&#20110;&#22522;&#20110;&#25104;&#21151;&#30340;&#33539;&#20363;&#24341;&#20837;CochCeps-Augment&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#30340;&#26032;&#22411;&#29983;&#29289;&#21551;&#21457;&#25513;&#34109;&#22686;&#24378;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26032;&#24341;&#20837;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;Cochlear cepstrogram&#65288;CCGRAM&#65289;&#26469;&#25512;&#23548;&#36755;&#20837;&#35821;&#38899;&#30340;&#22122;&#22768;&#40065;&#26834;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#26696;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#21518;&#32773;&#21033;&#29992;SimCLR&#29983;&#25104;CCGRAM&#30340;&#23545;&#27604;&#35270;&#22270;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) for automated speech recognition in terms of its emotional content, can be heavily degraded by the presence noise, affecting the efficiency of modeling the intricate temporal and spectral informative structures of speech. Recently, SSL on large speech datasets, as well as new audio-specific SSL proxy tasks, such as, temporal and frequency masking, have emerged, yielding superior performance compared to classic approaches drawn from the image augmentation domain. Our proposed contribution builds upon this successful paradigm by introducing CochCeps-Augment, a novel bio-inspired masking augmentation task for self-supervised contrastive learning of speech representations. Specifically, we utilize the newly introduced bio-inspired cochlear cepstrogram (CCGRAM) to derive noise robust representations of input speech, that are then further refined through a self-supervised learning scheme. The latter employs SimCLR to generate contrastive views of a CCGRAM throu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06922</link><description>&lt;p&gt;
&#26426;&#22120;&#20013;&#30340;&#31169;&#35821;&#65306;LLM&#38598;&#25104;&#31995;&#32479;&#20013;&#30340;&#20445;&#23494;&#24615;
&lt;/p&gt;
&lt;p&gt;
Whispers in the Machine: Confidentiality in LLM-integrated Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#21270;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#26469;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#35780;&#20272;&#20102;&#20843;&#31181;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#65292;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#22806;&#37096;&#24037;&#20855;&#38598;&#25104;&#12290;&#23613;&#31649;&#36825;&#20123;&#38598;&#25104;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LLM&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#20063;&#22312;&#19981;&#21516;&#32452;&#20214;&#20043;&#38388;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#21487;&#33021;&#27844;&#38706;&#26426;&#23494;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24694;&#24847;&#24037;&#20855;&#21487;&#20197;&#21033;&#29992;LLM&#26412;&#36523;&#30340;&#28431;&#27934;&#26469;&#25805;&#32437;&#27169;&#22411;&#24182;&#25439;&#23475;&#20854;&#20182;&#26381;&#21153;&#30340;&#25968;&#25454;&#65292;&#36825;&#24341;&#21457;&#20102;&#22312;LLM&#38598;&#25104;&#29615;&#22659;&#20013;&#22914;&#20309;&#20445;&#25252;&#31169;&#23494;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#35780;&#20272;LLM&#38598;&#25104;&#31995;&#32479;&#20445;&#23494;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#19968;&#20010;"&#31192;&#23494;&#23494;&#38053;"&#28216;&#25103;&#65292;&#21487;&#20197;&#25429;&#25417;&#27169;&#22411;&#38544;&#34255;&#31169;&#20154;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#27169;&#22411;&#23545;&#20445;&#23494;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20197;&#21450;&#19981;&#21516;&#38450;&#24481;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#20808;&#21069;&#21457;&#34920;&#30340;&#25915;&#20987;&#21644;&#22235;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly integrated with external tools. While these integrations can significantly improve the functionality of LLMs, they also create a new attack surface where confidential data may be disclosed between different components. Specifically, malicious tools can exploit vulnerabilities in the LLM itself to manipulate the model and compromise the data of other services, raising the question of how private data can be protected in the context of LLM integrations.   In this work, we provide a systematic way of evaluating confidentiality in LLM-integrated systems. For this, we formalize a "secret key" game that can capture the ability of a model to conceal private information. This enables us to compare the vulnerability of a model against confidentiality attacks and also the effectiveness of different defense strategies. In this framework, we evaluate eight previously published attacks and four defenses. We find that current defenses lack generalization
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#32858;&#31867;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#39044;&#27979;&#22826;&#38451;&#33021;&#28909;&#31995;&#32479;&#30340;&#36755;&#20986;&#28201;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06921</link><description>&lt;p&gt;
&#36873;&#25321;&#28151;&#21512;&#22238;&#24402;&#27169;&#22411;&#30340;&#32858;&#31867;&#25216;&#26415;&#65306;&#22522;&#20110;&#22826;&#38451;&#33021;&#28909;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Clustering Techniques Selection for a Hybrid Regression Model: A Case Study Based on a Solar Thermal System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#32858;&#31867;&#25216;&#26415;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#65292;&#20197;&#39044;&#27979;&#22826;&#38451;&#33021;&#28909;&#31995;&#32479;&#30340;&#36755;&#20986;&#28201;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22235;&#31181;&#32858;&#31867;&#25216;&#26415;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#26088;&#22312;&#23454;&#29616;&#24378;&#22823;&#30340;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#38598;&#20102;&#26469;&#33258;&#20301;&#20110;&#35199;&#29677;&#29273;&#21152;&#21033;&#35199;&#20122;&#30465;Lugo Xermade&#22320;&#21306;&#30340;&#29983;&#29289;&#27668;&#20505;&#20303;&#23429;&#8220;Sotavento&#8221;&#19978;&#23454;&#39564;&#24615;&#39118;&#30005;&#22330;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#20316;&#32773;&#36873;&#25321;&#20102;&#28909;&#22826;&#38451;&#33021;&#21457;&#30005;&#31995;&#32479;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#30740;&#31350;&#20102;&#24212;&#29992;&#22810;&#31181;&#32858;&#31867;&#26041;&#27861;&#21518;&#20877;&#24212;&#29992;&#22238;&#24402;&#25216;&#26415;&#39044;&#27979;&#31995;&#32479;&#36755;&#20986;&#28201;&#24230;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#20026;&#20102;&#23450;&#20041;&#27599;&#31181;&#32858;&#31867;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#31532;&#19968;&#31181;&#22522;&#20110;&#19977;&#20010;&#26080;&#30417;&#30563;&#23398;&#20064;&#24230;&#37327;&#65288;Silhouette&#12289;Calinski-Harabasz&#21644;Davies-Bouldin&#65289;&#65292;&#32780;&#31532;&#20108;&#31181;&#21017;&#37319;&#29992;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31561;&#22238;&#24402;&#31639;&#27861;&#30340;&#26368;&#24120;&#35265;&#38169;&#35823;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work addresses the performance comparison between four clustering techniques with the objective of achieving strong hybrid models in supervised learning tasks. A real dataset from a bio-climatic house named Sotavento placed on experimental wind farm and located in Xermade (Lugo) in Galicia (Spain) has been collected. Authors have chosen the thermal solar generation system in order to study how works applying several cluster methods followed by a regression technique to predict the output temperature of the system. With the objective of defining the quality of each clustering method two possible solutions have been implemented. The first one is based on three unsupervised learning metrics (Silhouette, Calinski-Harabasz and Davies-Bouldin) while the second one, employs the most common error measurements for a regression algorithm such as Multi Layer Perceptron.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06919</link><description>&lt;p&gt;
TREET: &#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TREET: TRansfer Entropy Estimation via Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36755;&#29109;&#65288;TE&#65289;&#26159;&#20449;&#24687;&#35770;&#20013;&#25581;&#31034;&#36807;&#31243;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#26041;&#21521;&#30340;&#24230;&#37327;&#65292;&#23545;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREET&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#31283;&#23450;&#36807;&#31243;&#30340;TE&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Donsker-Vardhan&#65288;DV&#65289;&#34920;&#31034;&#27861;&#23545;TE&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31070;&#32463;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;TREET&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#22686;&#21152;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#34920;&#31034;&#24341;&#29702;&#30340;&#20272;&#35745;TE&#20248;&#21270;&#26041;&#26696;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#26469;&#20248;&#21270;&#20855;&#26377;&#35760;&#24518;&#24615;&#30340;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#65292;&#36825;&#26159;&#20449;&#24687;&#35770;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;</title><link>https://arxiv.org/abs/2402.06918</link><description>&lt;p&gt;
&#29992;&#30452;&#25509;&#30340;&#20004;&#20004;&#27604;&#36739;&#26041;&#27861;&#29983;&#25104;&#24605;&#32500;&#38142;&#65292;&#20197;&#25628;&#32034;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Generating Chain-of-Thoughts with a Direct Pairwise-Comparison Approach to Searching for the Most Promising Intermediate Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30452;&#25509;&#20004;&#20004;&#27604;&#36739;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#65292;&#30452;&#25509;&#35782;&#21035;&#20986;&#26368;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#65292;&#20174;&#32780;&#29983;&#25104;&#20248;&#31168;&#30340;&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#24605;&#32500;&#38142;(Chain-of-Thoughts, CoT)&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#36827;&#34892;&#36880;&#27493;&#25512;&#29702;&#65292;&#20174;&#31616;&#21333;&#21040;&#22797;&#26434;&#30340;&#38382;&#39064;&#35299;&#20915;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36825;&#31181;&#24605;&#32500;&#38142;&#30340;&#26041;&#27861;&#28041;&#21450;&#20114;&#21160;&#21327;&#20316;&#65292;&#23398;&#20064;&#32773;&#29983;&#25104;&#20505;&#36873;&#20013;&#38388;&#24605;&#32500;&#65292;&#30001;LLMs&#35780;&#20272;&#65292;&#24341;&#23548;&#29983;&#25104;&#21518;&#32493;&#24605;&#32500;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24191;&#27867;&#20294;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#65292;LLMs&#30340;&#35780;&#20272;&#36890;&#24120;&#23384;&#22312;&#22122;&#22768;&#21644;&#19981;&#21487;&#38752;&#24615;&#65292;&#21487;&#33021;&#35823;&#23548;&#29983;&#25104;&#36807;&#31243;&#65292;&#36873;&#25321;&#19981;&#22815;&#26377;&#28508;&#21147;&#30340;&#20013;&#38388;&#24605;&#32500;&#12290;&#26412;&#25991;&#21463;Vapnik&#21407;&#21017;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#27604;&#36739;&#30340;CoT&#29983;&#25104;&#31639;&#27861;&#65292;&#30452;&#25509;&#26681;&#25454;LLMs&#30340;&#22122;&#22768;&#21453;&#39304;&#30830;&#23450;&#26368;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;&#25105;&#20204;&#38543;&#26426;&#37197;&#23545;&#20013;&#38388;&#24605;&#32500;&#65292;&#24182;&#30452;&#25509;&#20419;&#20351;LLMs&#20174;&#27599;&#23545;&#20013;&#36873;&#25321;&#26356;&#26377;&#28508;&#21147;&#30340;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve the ability of the large language model (LLMs) to handle complex reasoning problems, chain-of-thoughts (CoT) methods were proposed to guide LLMs to reason step-by-step, facilitating problem solving from simple to complex tasks. State-of-the-art approaches for generating such a chain involve interactive collaboration, where the learner generates candidate intermediate thoughts, evaluated by the LLM, guiding the generation of subsequent thoughts. However, a widespread yet understudied problem is that the evaluation from the LLM is typically noisy and unreliable, potentially misleading the generation process in selecting promising intermediate thoughts. In this paper, motivated by Vapnik's principle, we propose a novel comparison-based CoT generation algorithm that directly identifies the most promising thoughts with the noisy feedback from the LLM. In each round, we randomly pair intermediate thoughts and directly prompt the LLM to select the more promising one from each pair,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.06912</link><description>&lt;p&gt;
&#29992;&#32447;&#24615;&#31574;&#30053;&#32593;&#32476;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Solving Deep Reinforcement Learning Benchmarks with Linear Policy Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#36827;&#21270;&#31574;&#30053;(ES)&#26469;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#20197;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#35299;&#20915;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#22522;&#20934;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#19982;&#24403;&#21069;&#20351;&#29992;&#26356;&#22823;&#32593;&#32476;&#30340;DRL&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#24448;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#31639;&#27861;&#33021;&#22815;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#20687;Atari&#28216;&#25103;&#21644;&#26426;&#22120;&#20154;&#20219;&#21153;&#36825;&#26679;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#20294;&#31639;&#27861;&#22797;&#26434;&#65292;&#35757;&#32451;&#26102;&#38388;&#24448;&#24448;&#36739;&#38271;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#31574;&#30053;(ES)&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;ES&#36890;&#36807;&#31070;&#32463;&#36827;&#21270;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#65292;&#36890;&#36807;&#30452;&#25509;&#31574;&#30053;&#25628;&#32034;&#26469;&#23436;&#25104;&#12290;&#25105;&#20204;&#23545;&#24120;&#35268;&#32593;&#32476;&#21644;&#30001;&#19968;&#20010;&#20174;&#35266;&#27979;&#21040;&#21160;&#20316;&#30340;&#21333;&#19968;&#32447;&#24615;&#23618;&#32452;&#25104;&#30340;&#31574;&#30053;&#32593;&#32476;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65307;&#23545;&#20110;&#19977;&#31181;&#32463;&#20856;&#30340;ES&#26041;&#27861;&#21644;&#19977;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#22914;PPO&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ES&#21487;&#20197;&#22312;&#35768;&#22810;RL&#22522;&#20934;&#20219;&#21153;&#20013;&#25214;&#21040;&#26377;&#25928;&#30340;&#32447;&#24615;&#31574;&#30053;&#65292;&#32780;DRL&#26041;&#27861;&#21482;&#33021;&#20351;&#29992;&#26356;&#22823;&#30340;&#32593;&#32476;&#25214;&#21040;&#25104;&#21151;&#30340;&#31574;&#30053;&#65292;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#22522;&#20934;&#38382;&#39064;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#23481;&#26131;&#35299;&#20915;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#21363;&#20351;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;ES&#30340;&#32467;&#26524;&#20063;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Deep Reinforcement Learning (DRL) methods can learn effective policies for challenging problems such as Atari games and robotics tasks, algorithms are complex and training times are often long. This study investigates how evolution strategies (ES) perform compared to gradient-based deep reinforcement learning methods. We use ES to optimize the weights of a neural network via neuroevolution, performing direct policy search. We benchmark both regular networks and policy networks consisting of a single linear layer from observations to actions; for three classical ES methods and for three gradient-based methods such as PPO. Our results reveal that ES can find effective linear policies for many RL benchmark tasks, in contrast to DRL methods that can only find successful policies using much larger networks, suggesting that current benchmarks are easier to solve than previously assumed. Interestingly, also for higher complexity tasks, ES achieves results comparable to gradient-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#21644;&#22810;&#20851;&#31995;&#24402;&#32435;&#20559;&#32622;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06908</link><description>&lt;p&gt;
&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#65306;&#36890;&#36807;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Topological Neural Networks: Mitigating the Bottlenecks of Graph Neural Networks via Higher-Order Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#21644;&#22810;&#20851;&#31995;&#24402;&#32435;&#20559;&#32622;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#29616;&#35937;&#30340;&#19981;&#21487;&#32422;&#22797;&#26434;&#24615;&#20351;&#24471;&#22270;&#31070;&#32463;&#32593;&#32476;&#25104;&#20026;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#34429;&#28982;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#24335;&#30340;&#33021;&#21147;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#19982;&#38271;&#36317;&#31163;&#21644;&#39640;&#38454;&#20381;&#36182;&#30456;&#20851;&#30340;&#24433;&#21709;&#23545;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#26694;&#26550;&#20837;&#25163;&#65292;&#25581;&#31034;&#20102;&#32593;&#32476;&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#21644;&#22270;&#25299;&#25169;&#23545;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#21387;&#32553;&#29616;&#35937;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#36890;&#36807;&#39640;&#25299;&#25169;&#31070;&#32463;&#32593;&#32476;&#20174;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#21644;&#22810;&#20851;&#31995;&#24402;&#32435;&#20559;&#32622;&#20837;&#25163;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#39640;&#32500;&#32467;&#26500;&#20256;&#25773;&#28040;&#24687;&#65292;&#20026;&#20449;&#24687;&#27969;&#25552;&#20379;&#20102;&#24555;&#25463;&#26041;&#24335;&#25110;&#39069;&#22806;&#30340;&#36335;&#24452;&#12290;&#36890;&#36807;&#36825;&#31181;&#26500;&#24314;&#65292;&#24213;&#23618;&#30340;&#35745;&#31639;&#22270;&#19981;&#20877;&#19982;&#36755;&#20837;&#22270;&#32467;&#26500;&#32806;&#21512;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#21069;&#38754;&#25552;&#21040;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#20102;h&#12290;
&lt;/p&gt;
&lt;p&gt;
The irreducible complexity of natural phenomena has led Graph Neural Networks to be employed as a standard model to perform representation learning tasks on graph-structured data. While their capacity to capture local and global patterns is remarkable, the implications associated with long-range and higher-order dependencies pose considerable challenges to such models. This work starts with a theoretical framework to reveal the impact of network's width, depth, and graph topology on the over-squashing phenomena in message-passing neural networks. Then, the work drifts towards, higher-order interactions and multi-relational inductive biases via Topological Neural Networks. Such models propagate messages through higher-dimensional structures, providing shortcuts or additional routes for information flow. With this construction, the underlying computational graph is no longer coupled with the input graph structure, thus mitigating the aforementioned bottlenecks while accounting also for h
&lt;/p&gt;</description></item><item><title>GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06894</link><description>&lt;p&gt;
GenTranslate: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06894
&lt;/p&gt;
&lt;p&gt;
GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#20943;&#23569;&#34920;&#31034;&#35823;&#24046;&#21644;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#26463;&#25628;&#32034;&#35299;&#30721;&#21644;&#21069;k&#20010;&#20551;&#35774;&#36873;&#25321;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;N-best&#20551;&#35774;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38656;&#35201;&#21333;&#20010;&#39640;&#36136;&#37327;&#36755;&#20986;&#24207;&#21015;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#8220;GenTranslate&#8221;&#65292;&#23427;&#22522;&#20110;LLMs&#26469;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;LLMs&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#21487;&#20197;&#23558;N-best&#20505;&#36873;&#20154;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;LLM&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;HypoTransla&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#29702;&#35299;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#26041;&#27861;&#24182;&#20026;&#20854;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#21516;&#26102;&#28548;&#28165;&#20854;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.06892</link><description>&lt;p&gt;
&#29702;&#35299;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Understanding Test-Time Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06892
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26088;&#22312;&#29702;&#35299;&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#26041;&#27861;&#24182;&#20026;&#20854;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#65292;&#21516;&#26102;&#28548;&#28165;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#35797;&#26102;&#38388;&#22686;&#24378;&#65288;TTA&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#22312;&#27979;&#35797;&#20013;&#20135;&#29983;&#24179;&#22343;&#36755;&#20986;&#12290;&#23613;&#31649;TTA&#22312;&#23454;&#39564;&#20013;&#26174;&#31034;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#23545;&#20854;&#29702;&#35770;&#26041;&#38754;&#30340;&#35752;&#35770;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#26088;&#22312;&#20026;TTA&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#24182;&#28548;&#28165;&#20854;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Test-Time Augmentation (TTA) is a very powerful heuristic that takes advantage of data augmentation during testing to produce averaged output. Despite the experimental effectiveness of TTA, there is insufficient discussion of its theoretical aspects. In this paper, we aim to give theoretical guarantees for TTA and clarify its behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;Bilevel&#24378;&#21270;&#23398;&#20064;&#21644;RLHF&#38382;&#39064;&#65292;&#36825;&#26159;&#39318;&#20010;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06886</link><description>&lt;p&gt;
Bilevel&#24378;&#21270;&#23398;&#20064;&#21644;RLHF&#30340;&#26377;&#21407;&#21017;&#30340;&#22522;&#20110;&#24809;&#32602;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Principled Penalty-based Methods for Bilevel Reinforcement Learning and RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24809;&#32602;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;Bilevel&#24378;&#21270;&#23398;&#20064;&#21644;RLHF&#38382;&#39064;&#65292;&#36825;&#26159;&#39318;&#20010;&#26377;&#21407;&#21017;&#30340;&#31639;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Bilevel&#20248;&#21270;&#24050;&#34987;&#24212;&#29992;&#20110;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#20165;&#38480;&#20110;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#20855;&#26377;&#33391;&#24615;&#32467;&#26500;&#30340;&#38745;&#24577;&#30446;&#26631;&#20989;&#25968;&#12290;&#20294;&#26159;&#65292;&#28608;&#21169;&#35774;&#35745;&#12289;&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;(RL)&#21644;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;RLHF&#31561;Bilevel&#38382;&#39064;&#36890;&#24120;&#34987;&#24314;&#27169;&#20026;&#36229;&#36234;&#31616;&#21333;&#38745;&#24577;&#30446;&#26631;&#32467;&#26500;&#30340;&#21160;&#24577;&#30446;&#26631;&#20989;&#25968;&#65292;&#36825;&#32473;&#20351;&#29992;&#29616;&#26377;Bilevel&#35299;&#20915;&#26041;&#26696;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#26032;&#30340;Bilevel&#38382;&#39064;&#31867;&#21035;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#24418;&#24335;&#24341;&#20837;&#20102;&#35299;&#20915;Bilevel RL&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#21407;&#21017;&#24615;&#31639;&#27861;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#30740;&#31350;&#38382;&#39064;&#30340;&#26223;&#35266;&#21450;&#20854;&#22522;&#20110;&#24809;&#32602;&#30340;&#65288;&#31574;&#30053;&#65289;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Stackelberg&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#12289;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;RL&#21644;&#28608;&#21169;&#35774;&#35745;&#20013;&#36827;&#34892;&#27169;&#25311;&#26469;&#35777;&#26126;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization has been recently applied to many machine learning tasks. However, their applications have been restricted to the supervised learning setting, where static objective functions with benign structures are considered. But bilevel problems such as incentive design, inverse reinforcement learning (RL), and RL from human feedback (RLHF) are often modeled as dynamic objective functions that go beyond the simple static objective structures, which pose significant challenges of using existing bilevel solutions. To tackle this new class of bilevel problems, we introduce the first principled algorithmic framework for solving bilevel RL problems through the lens of penalty formulation. We provide theoretical studies of the problem landscape and its penalty-based (policy) gradient algorithms. We demonstrate the effectiveness of our algorithms via simulations in the Stackelberg Markov game, RL from human feedback and incentive design.
&lt;/p&gt;</description></item><item><title>DimVis&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#32500;&#24230;&#32422;&#20943;&#20013;&#30340;&#35270;&#35273;&#32858;&#31867;&#12290;&#23427;&#36890;&#36807;&#23545;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#35299;&#37322;&#65292;&#25552;&#20379;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#35270;&#35273;&#32858;&#31867;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06885</link><description>&lt;p&gt;
DimVis: &#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#65288;EBM&#65289;&#35299;&#37322;&#32500;&#24230;&#32422;&#20943;&#20013;&#30340;&#35270;&#35273;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06885
&lt;/p&gt;
&lt;p&gt;
DimVis&#26159;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#32500;&#24230;&#32422;&#20943;&#20013;&#30340;&#35270;&#35273;&#32858;&#31867;&#12290;&#23427;&#36890;&#36807;&#23545;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#35299;&#37322;&#65292;&#25552;&#20379;&#23545;&#39640;&#32500;&#25968;&#25454;&#20013;&#35270;&#35273;&#32858;&#31867;&#30340;&#35299;&#37322;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32500;&#24230;&#32422;&#20943;&#65288;DR&#65289;&#25216;&#26415;&#65292;&#22914;t-SNE&#21644;UMAP&#65292;&#24456;&#21463;&#27426;&#36814;&#65292;&#21487;&#20197;&#23558;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#36716;&#25442;&#25104;&#26356;&#31616;&#21333;&#30340;&#21487;&#35270;&#21270;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#24635;&#20307;&#27169;&#24335;&#65292;&#20294;&#21487;&#33021;&#20250;&#24341;&#20837;&#20266;&#20687;&#24182;&#23384;&#22312;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DimVis&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#23427;&#20351;&#29992;&#30417;&#30563;&#30340;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#22120;&#65288;EBM&#65289;&#27169;&#22411;&#65288;&#22312;&#29992;&#25143;&#36873;&#25321;&#30340;&#24863;&#20852;&#36259;&#25968;&#25454;&#19978;&#35757;&#32451;&#65289;&#20316;&#20026;DR&#25237;&#24433;&#30340;&#35299;&#37322;&#21161;&#25163;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#36890;&#36807;&#36890;&#36807;&#20132;&#20114;&#24335;&#25506;&#32034;UMAP&#25237;&#24433;&#20013;&#30340;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#20379;&#23545;&#35270;&#35273;&#32858;&#31867;&#20013;&#29305;&#24449;&#37325;&#35201;&#24615;&#30340;&#35299;&#37322;&#65292;&#20174;&#32780;&#20419;&#36827;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DimVis&#20351;&#29992;&#19968;&#20010;&#23545;&#27604;&#30340;EBM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#23454;&#26102;&#35757;&#32451;&#20013;&#33021;&#21306;&#20998;&#24863;&#20852;&#36259;&#32858;&#31867;&#20869;&#22806;&#30340;&#25968;&#25454;&#12290;&#21033;&#29992;EBM&#22266;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#21644;&#25104;&#23545;&#30340;&#29305;&#24449;&#27604;&#36739;&#26469;&#35299;&#37322;&#32858;&#31867;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dimensionality Reduction (DR) techniques such as t-SNE and UMAP are popular for transforming complex datasets into simpler visual representations. However, while effective in uncovering general dataset patterns, these methods may introduce artifacts and suffer from interpretability issues. This paper presents DimVis, a visualization tool that employs supervised Explainable Boosting Machine (EBM) models (trained on user-selected data of interest) as an interpretation assistant for DR projections. Our tool facilitates high-dimensional data analysis by providing an interpretation of feature relevance in visual clusters through interactive exploration of UMAP projections. Specifically, DimVis uses a contrastive EBM model that is trained in real time to differentiate between the data inside and outside a cluster of interest. Taking advantage of the inherent explainable nature of the EBM, we then use this model to interpret the cluster itself via single and pairwise feature comparisons in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#32467;&#26500;&#20887;&#20313;&#30340;&#20302;&#31209;&#36924;&#36817;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36924;&#36817;&#20887;&#20313;&#32452;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36807;&#37327;&#39118;&#38505;&#26469;&#25903;&#25345;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.06884</link><description>&lt;p&gt;
&#32467;&#26500;&#20887;&#20313;&#30340;&#20302;&#31209;&#36924;&#36817;&#29992;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Approximation of Structural Redundancy for Self-Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32467;&#26500;&#20887;&#20313;&#30340;&#20302;&#31209;&#36924;&#36817;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36924;&#36817;&#20887;&#20313;&#32452;&#20214;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#36807;&#37327;&#39118;&#38505;&#26469;&#25903;&#25345;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#37325;&#26500;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#65292;&#20197;&#25581;&#31034;&#20854;&#26377;&#25928;&#24615;&#12290;&#22312;&#25317;&#26377;&#26080;&#38480;&#37327;&#30340;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23436;&#32654;&#32447;&#24615;&#36924;&#36817;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#35813;&#26465;&#20214;&#25581;&#31034;&#20102;&#19968;&#20010;&#20445;&#30041;&#26631;&#31614;&#31867;&#21035;Y&#30340;&#28385;&#31209;&#32452;&#20214;&#65292;&#20197;&#21450;&#19968;&#20010;&#20887;&#20313;&#32452;&#20214;&#12290;&#21463;&#21040;&#35813;&#26465;&#20214;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20302;&#31209;&#20998;&#35299;&#36924;&#36817;&#20887;&#20313;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#30001;&#20998;&#35299;&#31209;s&#21442;&#25968;&#21270;&#30340;&#26032;&#37327;$\epsilon_s$&#26469;&#34913;&#37327;&#36924;&#36817;&#36136;&#37327;&#12290;&#25105;&#20204;&#23558;$\epsilon_s$&#25972;&#21512;&#21040;&#32447;&#24615;&#22238;&#24402;&#21644;&#23725;&#22238;&#24402;&#35774;&#32622;&#19979;&#30340;&#36807;&#37327;&#39118;&#38505;&#20998;&#26512;&#20013;&#65292;&#21518;&#19968;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#29992;&#20110;&#22788;&#29702;&#23398;&#20064;&#29305;&#24449;&#30340;&#32500;&#24230;&#36828;&#22823;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26631;&#35760;&#26679;&#26412;&#25968;n&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#31616;&#21270;&#23454;&#39564;&#65292;&#20197;&#27604;&#36739;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the data-generating mechanism for reconstructive SSL to shed light on its effectiveness. With an infinite amount of labeled samples, we provide a sufficient and necessary condition for perfect linear approximation. The condition reveals a full-rank component that preserves the label classes of Y, along with a redundant component. Motivated by the condition, we propose to approximate the redundant component by a low-rank factorization and measure the approximation quality by introducing a new quantity $\epsilon_s$, parameterized by the rank of factorization s. We incorporate $\epsilon_s$ into the excess risk analysis under both linear regression and ridge regression settings, where the latter regularization approach is to handle scenarios when the dimension of the learned features is much larger than the number of labeled samples n for downstream tasks. We design three stylized experiments to compare SSL with supervised learning under different settings to support our theoretic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06864</link><description>&lt;p&gt;
&#20855;&#26377;&#36776;&#21035;&#24615;&#23545;&#25239;&#23398;&#20064;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Discriminative Adversarial Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06864
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26469;&#23454;&#29616;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#65292;&#24182;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#21453;&#23398;&#20064;&#26694;&#26550;&#65292;&#22522;&#20110;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#33539;&#24335;&#30340;&#24050;&#24314;&#31435;&#21407;&#21017;&#12290;&#25105;&#20204;&#21033;&#29992;&#24378;&#22823;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#20419;&#36827;&#20174;&#35757;&#32451;&#27169;&#22411;&#20013;&#21453;&#23398;&#20064;&#29305;&#23450;&#26679;&#26412;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#32593;&#32476;&#30340;&#22330;&#26223;&#65292;&#25915;&#20987;&#32773;$\mathbf{A}$&#21644;&#32463;&#36807;&#35757;&#32451;&#30340;&#38450;&#24481;&#32773; $\mathbf{D}$&#22312;&#23545;&#25239;&#30446;&#26631;&#19979;&#30456;&#20114;&#23545;&#25239;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#30340;&#20449;&#24687;&#20197;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#65292;&#32780;&#38450;&#24481;&#32773;&#22312;&#21453;&#20987;&#20013;&#36827;&#34892;&#21453;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24635;&#20307;&#24615;&#33021;&#12290;&#31639;&#27861;&#21487;&#20197;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36981;&#24490;&#24050;&#30693;&#30340;&#36845;&#20195;&#26368;&#23567;&#26368;&#22823;&#26041;&#27861;&#26469;&#26356;&#26032;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#12290;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#33258;&#30417;&#30563;&#30446;&#26631;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36951;&#24536;&#38598;&#21644;&#39564;&#35777;&#38598;&#20043;&#38388;&#30340;&#29305;&#24449;&#31354;&#38388;&#24046;&#24322;&#65292;&#22686;&#24378;&#20102;&#21453;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel machine unlearning framework founded upon the established principles of the min-max optimization paradigm. We capitalize on the capabilities of strong Membership Inference Attacks (MIA) to facilitate the unlearning of specific samples from a trained model. We consider the scenario of two networks, the attacker $\mathbf{A}$ and the trained defender $\mathbf{D}$ pitted against each other in an adversarial objective, wherein the attacker aims at teasing out the information of the data to be unlearned in order to infer membership, and the defender unlearns to defend the network against the attack, whilst preserving its general performance. The algorithm can be trained end-to-end using backpropagation, following the well known iterative min-max approach in updating the attacker and the defender. We additionally incorporate a self-supervised objective effectively addressing the feature space discrepancies between the forget set and the validation set, enhancing unlearnin
&lt;/p&gt;</description></item><item><title>LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06859</link><description>&lt;p&gt;
LiRank: &#39046;&#33521;&#30340;&#24037;&#19994;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiRank: Industrial Large Scale Ranking Models at LinkedIn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06859
&lt;/p&gt;
&lt;p&gt;
LiRank&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#24212;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#24314;&#27169;&#25913;&#36827;&#21644;&#25216;&#26415;&#65292;&#36890;&#36807;A/B&#27979;&#35797;&#21462;&#24471;&#20102;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;LiRank&#65292;&#36825;&#26159;&#39046;&#33521;&#30340;&#19968;&#20010;&#22823;&#35268;&#27169;&#25490;&#21517;&#26694;&#26550;&#65292;&#23427;&#23558;&#26368;&#20808;&#36827;&#30340;&#24314;&#27169;&#26550;&#26500;&#21644;&#20248;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#24314;&#27169;&#25913;&#36827;&#65292;&#21253;&#25324;Residual DCN&#65292;&#23427;&#22312;&#33879;&#21517;&#30340;DCNv2&#26550;&#26500;&#20013;&#28155;&#21152;&#20102;&#27880;&#24847;&#21147;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#23558;SOTA&#26550;&#26500;&#32452;&#21512;&#21644;&#35843;&#20248;&#20197;&#21019;&#24314;&#32479;&#19968;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;Dense Gating&#12289;Transformers&#21644;Residual DCN&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#29992;&#20110;&#26657;&#20934;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#25551;&#36848;&#20102;&#22914;&#20309;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25506;&#32034;/&#21033;&#29992;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#20135;&#29615;&#22659;&#12290;&#20026;&#20102;&#23454;&#29616;&#22823;&#35268;&#27169;&#25490;&#21517;&#27169;&#22411;&#30340;&#26377;&#25928;&#12289;&#29983;&#20135;&#32423;&#26381;&#21153;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#20351;&#29992;&#37327;&#21270;&#21644;&#35789;&#27719;&#21387;&#32553;&#35757;&#32451;&#21644;&#21387;&#32553;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;Feed&#25490;&#21517;&#12289;&#32844;&#20301;&#25512;&#33616;&#21644;&#24191;&#21578;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#31561;&#22823;&#35268;&#27169;&#20351;&#29992;&#26696;&#20363;&#30340;&#37096;&#32626;&#35774;&#32622;&#32454;&#33410;&#12290;&#36890;&#36807;&#38416;&#26126;&#26368;&#26377;&#25928;&#30340;&#25216;&#26415;&#26041;&#27861;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#21508;&#31181;A/B&#27979;&#35797;&#30340;&#32463;&#39564;&#25945;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LiRank, a large-scale ranking framework at LinkedIn that brings to production state-of-the-art modeling architectures and optimization methods. We unveil several modeling improvements, including Residual DCN, which adds attention and residual connections to the famous DCNv2 architecture. We share insights into combining and tuning SOTA architectures to create a unified model, including Dense Gating, Transformers and Residual DCN. We also propose novel techniques for calibration and describe how we productionalized deep learning based explore/exploit methods. To enable effective, production-grade serving of large ranking models, we detail how to train and compress models using quantization and vocabulary compression. We provide details about the deployment setup for large-scale use cases of Feed ranking, Jobs Recommendations, and Ads click-through rate (CTR) prediction. We summarize our learnings from various A/B tests by elucidating the most effective technical approaches. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;</title><link>https://arxiv.org/abs/2402.06855</link><description>&lt;p&gt;
&#26356;&#22909;&#36824;&#26159;&#26356;&#24046;&#65311;&#36890;&#36807;&#26631;&#31614;&#22686;&#24378;&#23398;&#20064;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
For Better or For Worse? Learning Minimum Variance Features With Label Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#25104;&#21151;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#31867;-&#21253;&#25324;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup-&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20837;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27492;&#31867;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#65288;&#21253;&#25324;&#26435;&#37325;&#34928;&#20943;&#65289;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#28040;&#26497;&#30340;&#65306;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#27604;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38464;&#34746;&#20202;&#36741;&#21161;&#30340;&#36816;&#21160;&#21435;&#27169;&#31946;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;IMU&#25968;&#25454;&#21512;&#25104;&#21644;&#24674;&#22797;&#36816;&#21160;&#27169;&#31946;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#40784;&#21644;&#20449;&#24687;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06854</link><description>&lt;p&gt;
&#38464;&#34746;&#20202;&#36741;&#21161;&#30340;&#36816;&#21160;&#21435;&#27169;&#31946;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Gyroscope-Assisted Motion Deblurring Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38464;&#34746;&#20202;&#36741;&#21161;&#30340;&#36816;&#21160;&#21435;&#27169;&#31946;&#32593;&#32476;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;IMU&#25968;&#25454;&#21512;&#25104;&#21644;&#24674;&#22797;&#36816;&#21160;&#27169;&#31946;&#22270;&#20687;&#65292;&#35299;&#20915;&#20102;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#40784;&#21644;&#20449;&#24687;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#22270;&#20687;&#30740;&#31350;&#38598;&#20013;&#20851;&#27880;&#20102;&#21435;&#27169;&#31946;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#20687;&#32032;&#23545;&#40784;&#30340;&#35757;&#32451;&#19977;&#20803;&#32452;&#65288;&#32972;&#26223;&#22270;&#20687;&#12289;&#27169;&#31946;&#22270;&#20687;&#21644;&#27169;&#31946;&#28909;&#22270;&#65289;&#20197;&#21450;&#27169;&#31946;&#22270;&#20687;&#20013;&#25152;&#22266;&#26377;&#30340;&#20449;&#24687;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21435;&#27169;&#31946;&#65292;&#23588;&#20854;&#26159;&#36816;&#21160;&#27169;&#31946;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#25968;&#25454;&#21512;&#25104;&#21644;&#24674;&#22797;&#36816;&#21160;&#27169;&#31946;&#22270;&#20687;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#35757;&#32451;&#19977;&#20803;&#32452;&#30340;&#31574;&#30053;&#65292;&#20197;&#21450;&#19968;&#31181;&#29992;&#20110;&#24674;&#22797;&#27169;&#31946;&#22270;&#20687;&#30340;&#38464;&#34746;&#20202;&#36741;&#21161;&#36816;&#21160;&#21435;&#27169;&#31946;&#65288;GAMD&#65289;&#32593;&#32476;&#12290;&#20854;&#29702;&#35770;&#22522;&#30784;&#22312;&#20110;&#36890;&#36807;&#21033;&#29992;IMU&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#22270;&#20687;&#26333;&#20809;&#38454;&#27573;&#30456;&#26426;&#23039;&#24577;&#30340;&#21464;&#25442;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#19977;&#32500;&#31354;&#38388;&#20869;&#27599;&#20010;&#28857;&#30340;&#36816;&#21160;&#36712;&#36857;&#65288;&#21363;&#27169;&#31946;&#36712;&#36857;&#65289;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#31574;&#30053;&#33719;&#24471;&#30340;&#21512;&#25104;&#19977;&#20803;&#32452;&#19982;&#33258;&#28982;&#36816;&#21160;&#27169;&#31946;&#32039;&#23494;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image research has shown substantial attention in deblurring networks in recent years. Yet, their practical usage in real-world deblurring, especially motion blur, remains limited due to the lack of pixel-aligned training triplets (background, blurred image, and blur heat map) and restricted information inherent in blurred images. This paper presents a simple yet efficient framework to synthetic and restore motion blur images using Inertial Measurement Unit (IMU) data. Notably, the framework includes a strategy for training triplet generation, and a Gyroscope-Aided Motion Deblurring (GAMD) network for blurred image restoration. The rationale is that through harnessing IMU data, we can determine the transformation of the camera pose during the image exposure phase, facilitating the deduction of the motion trajectory (aka. blur trajectory) for each point inside the three-dimensional space. Thus, the synthetic triplets using our strategy are inherently close to natural motion blur, strict
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAMP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;$l_p$&#25915;&#20987;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#35774;&#35745;&#36923;&#36753;&#37197;&#23545;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35757;&#32451;&#19982;&#23545;&#25239;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#25972;&#21512;&#26377;&#29992;&#20449;&#24687;&#20197;&#35843;&#21644;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06827</link><description>&lt;p&gt;
RAMP&#65306;&#22686;&#24378;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RAMP: Boosting Adversarial Robustness Against Multiple $l_p$ Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06827
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RAMP&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;$l_p$&#25915;&#20987;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#65292;&#24182;&#35774;&#35745;&#36923;&#36753;&#37197;&#23545;&#25439;&#22833;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35757;&#32451;&#19982;&#23545;&#25239;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#25972;&#21512;&#26377;&#29992;&#20449;&#24687;&#20197;&#35843;&#21644;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25552;&#39640;&#23545;&#21333;&#20010;$l_p$&#33539;&#25968;&#21463;&#38480;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#26041;&#38754;&#65292;&#24050;&#32463;&#26377;&#30456;&#24403;&#22810;&#30340;&#24037;&#20316;&#22312;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;AT&#27169;&#22411;&#30340;&#22810;&#33539;&#25968;&#40065;&#26834;&#24615;&#65288;&#20849;&#21516;&#20934;&#30830;&#24615;&#65289;&#20173;&#28982;&#36739;&#20302;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21516;&#26102;&#33719;&#24471;&#33391;&#22909;&#30340;&#20849;&#21516;&#20934;&#30830;&#24615;&#21644;&#28165;&#27905;&#20934;&#30830;&#24615;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#22312;&#22810;&#20010;$l_p$&#25200;&#21160;&#20043;&#38388;&#23384;&#22312;&#40065;&#26834;&#24615;&#12289;&#20934;&#30830;&#24615;/&#40065;&#26834;&#24615;/&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#20174;&#20998;&#24067;&#36716;&#21464;&#30340;&#35282;&#24230;&#20998;&#26512;&#36825;&#20123;&#26435;&#34913;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;$l_p$&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#38190;&#26435;&#34913;&#23545;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36923;&#36753;&#37197;&#23545;&#25439;&#22833;&#26469;&#25552;&#39640;&#20849;&#21516;&#20934;&#30830;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#26799;&#24230;&#25237;&#24433;&#23558;&#33258;&#28982;&#35757;&#32451;&#19982;AT&#30456;&#36830;&#25509;&#65292;&#20197;&#20174;&#33258;&#28982;&#35757;&#32451;&#20013;&#25214;&#21040;&#24182;&#25972;&#21512;&#26377;&#29992;&#30340;&#20449;&#24687;&#21040;AT&#20013;&#65292;&#20174;&#32780;&#35843;&#21644;&#20934;&#30830;&#24615;/&#40065;&#26834;&#24615;&#30340;&#26435;&#34913;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\textbf{RAMP}&#30340;&#26694;&#26550;&#65292;&#26469;&#25552;&#39640;&#23545;&#22810;&#20010;$l_p$&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;\textbf{RAMP}&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;...
&lt;/p&gt;
&lt;p&gt;
There is considerable work on improving robustness against adversarial attacks bounded by a single $l_p$ norm using adversarial training (AT). However, the multiple-norm robustness (union accuracy) of AT models is still low. We observe that simultaneously obtaining good union and clean accuracy is hard since there are tradeoffs between robustness against multiple $l_p$ perturbations, and accuracy/robustness/efficiency. By analyzing the tradeoffs from the lens of distribution shifts, we identify the key tradeoff pair among $l_p$ attacks to boost efficiency and design a logit pairing loss to improve the union accuracy. Next, we connect natural training with AT via gradient projection, to find and incorporate useful information from natural training into AT, which moderates the accuracy/robustness tradeoff. Combining our contributions, we propose a framework called \textbf{RAMP}, to boost the robustness against multiple $l_p$ perturbations. We show \textbf{RAMP} can be easily adapted for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;WyScout&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#29992;&#20110;&#26500;&#24314;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.06820</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
Forecasting Events in Soccer Matches Through Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06820
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21644;WyScout&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#22312;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#26126;&#26174;&#36229;&#36807;&#20102;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#30340;&#24212;&#29992;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#29992;&#20110;&#26500;&#24314;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39044;&#27979;&#36275;&#29699;&#27604;&#36187;&#20013;&#19979;&#19968;&#20010;&#20107;&#20214;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30340;&#38382;&#39064;&#38750;&#24120;&#30456;&#20284;&#30340;&#25361;&#25112;&#12290;&#19982;&#20854;&#20182;&#20005;&#37325;&#38480;&#21046;&#36275;&#29699;&#20107;&#20214;&#21160;&#24577;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20174;&#24456;&#22810;&#21464;&#37327;&#20013;&#25277;&#35937;&#20986;&#26469;&#25110;&#20381;&#36182;&#20110;&#28151;&#21512;&#39034;&#24207;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;LLMs&#26041;&#27861;&#23398;&#21551;&#21457;&#30340;&#26032;&#25216;&#26415;&#12290;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#20102;&#32452;&#25104;&#19968;&#20010;&#20107;&#20214;&#30340;&#23436;&#25972;&#21464;&#37327;&#38142;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#26500;&#24314;&#36275;&#29699;&#22823;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#30340;&#36807;&#31243;&#12290;&#21033;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#19979;&#19968;&#20010;&#20107;&#20214;&#31867;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65289;&#26174;&#33879;&#36229;&#36234;&#20102;&#20197;&#24448;LEM&#25552;&#26696;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#31361;&#26174;&#20102;LEM&#22312;&#22810;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#21253;&#25324;&#21338;&#24425;&#21644;&#27604;&#36187;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LEM&#25552;&#20379;&#20102;&#19968;&#20010;&#27169;&#25311;&#39592;&#26550;&#65292;&#21487;&#20197;&#26500;&#24314;&#35768;&#22810;&#20998;&#26512;&#27969;&#27700;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an approach to predicting the next event in a soccer match, a challenge bearing remarkable similarities to the problem faced by Large Language Models (LLMs). Unlike other methods that severely limit event dynamics in soccer, often abstracting from many variables or relying on a mix of sequential models, our research proposes a novel technique inspired by the methodologies used in LLMs. These models predict a complete chain of variables that compose an event, significantly simplifying the construction of Large Event Models (LEMs) for soccer. Utilizing deep learning on the publicly available WyScout dataset, the proposed approach notably surpasses the performance of previous LEM proposals in critical areas, such as the prediction accuracy of the next event type. This paper highlights the utility of LEMs in various applications, including betting and match analytics. Moreover, we show that LEMs provide a simulation backbone on which many analytics pipelines can be bu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Monitored MDPs)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#19981;&#33021;&#22987;&#32456;&#35266;&#23519;&#21040;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2402.06819</link><description>&lt;p&gt;
&#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Monitored Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06819
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#8212;&#8212;&#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Monitored MDPs)&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#20195;&#29702;&#19981;&#33021;&#22987;&#32456;&#35266;&#23519;&#21040;&#22870;&#21169;&#65292;&#25552;&#20986;&#20102;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#21644;&#25509;&#25910;&#21453;&#39304;&#65288;&#25968;&#20540;&#22870;&#21169;&#65289;&#26469;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#22987;&#32456;&#21487;&#35266;&#23519;&#30340;&#20551;&#35774;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#36890;&#24120;&#19981;&#36866;&#29992;&#12290;&#20363;&#22914;&#65292;&#20195;&#29702;&#21487;&#33021;&#38656;&#35201;&#35201;&#27714;&#20154;&#31867;&#30417;&#30563;&#20854;&#34892;&#20026;&#25110;&#28608;&#27963;&#30417;&#25511;&#31995;&#32479;&#20197;&#25509;&#25910;&#21453;&#39304;&#12290;&#29978;&#33267;&#21487;&#33021;&#23384;&#22312;&#22870;&#21169;&#22312;&#21487;&#35266;&#23519;&#20043;&#21069;&#19968;&#27573;&#26102;&#38388;&#25110;&#22312;&#19981;&#20877;&#32473;&#20104;&#22870;&#21169;&#20043;&#21518;&#30340;&#26102;&#38388;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#65292;&#29615;&#22659;&#26681;&#25454;&#20195;&#29702;&#30340;&#34892;&#20026;&#29983;&#25104;&#22870;&#21169;&#65292;&#20294;&#20195;&#29702;&#26080;&#27861;&#35266;&#23519;&#21040;&#36825;&#20123;&#22870;&#21169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#39062;&#20294;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550; - &#30417;&#25511;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(Monitored MDPs)&#65292;&#22312;&#27492;&#26694;&#26550;&#20013;&#20195;&#29702;&#24182;&#38750;&#24635;&#26159;&#33021;&#22815;&#35266;&#23519;&#21040;&#22870;&#21169;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#35774;&#32622;&#21487;&#33021;&#24102;&#26469;&#30340;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#30340;&#21518;&#26524;&#65292;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#20063;&#20250;&#20986;&#29616;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#31639;&#27861;&#26469;&#24320;&#22987;&#35299;&#20915;&#36825;&#20010;&#26032;&#39062;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), an agent learns to perform a task by interacting with an environment and receiving feedback (a numerical reward) for its actions. However, the assumption that rewards are always observable is often not applicable in real-world problems. For example, the agent may need to ask a human to supervise its actions or activate a monitoring system to receive feedback. There may even be a period of time before rewards become observable, or a period of time after which rewards are no longer given. In other words, there are cases where the environment generates rewards in response to the agent's actions but the agent cannot observe them. In this paper, we formalize a novel but general RL framework - Monitored MDPs - where the agent cannot always observe rewards. We discuss the theoretical and practical consequences of this setting, show challenges raised even in toy environments, and propose algorithms to begin to tackle this novel setting. This paper introduces a p
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#22810;&#26679;&#24615;&#20998;&#35299;&#26694;&#26550;&#25351;&#23548;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#23398;&#20064;&#32773;&#65292;&#29983;&#25104;&#20102;21&#31181;&#26032;&#30340;&#38598;&#25104;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06818</link><description>&lt;p&gt;
&#26397;&#30528;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#35774;&#35745;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards a Systematic Approach to Design New Ensemble Learning Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#22810;&#26679;&#24615;&#20998;&#35299;&#26694;&#26550;&#25351;&#23548;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#23398;&#20064;&#32773;&#65292;&#29983;&#25104;&#20102;21&#31181;&#26032;&#30340;&#38598;&#25104;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#30001;&#20110;&#20854;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;1990&#24180;&#20197;&#26469;&#65292;&#21382;&#21490;&#19978;&#20165;&#38480;&#20110;&#22238;&#24402;&#38382;&#39064;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#21327;&#26041;&#24046;&#20998;&#26512;&#30340;&#38598;&#25104;&#38169;&#35823;&#20998;&#35299;&#30340;&#22522;&#30784;&#24615;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#24341;&#20837;&#20102;&#19968;&#31181;&#8220;&#22810;&#26679;&#24615;&#30340;&#32479;&#19968;&#29702;&#35770;&#8221;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#20559;&#24046;-&#26041;&#24046;-&#22810;&#26679;&#24615;&#20998;&#35299;&#26694;&#26550;&#12290;&#20511;&#21161;&#36825;&#31181;&#29616;&#20195;&#29702;&#35299;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#36825;&#31181;&#20998;&#35299;&#22312;&#25351;&#23548;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#31639;&#27861;&#30340;&#21019;&#24314;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#20197;&#22238;&#24402;&#20219;&#21153;&#20026;&#37325;&#28857;&#65292;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22522;&#23398;&#20064;&#32773;&#65292;&#20197;&#35843;&#26597;&#36825;&#20010;&#29702;&#35770;&#26694;&#26550;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;7&#31181;&#31616;&#21333;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#31574;&#30053;&#65292;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#20102;21&#31181;&#26032;&#30340;&#38598;&#25104;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#22823;&#37096;&#20998;&#26041;&#27861;&#19982;&#8220;snapshot&#8221;&#31574;&#30053;&#32858;&#21512;&#22312;&#19968;&#36215;&#65292;&#36825;&#26159;7&#31181;&#31574;&#30053;&#20013;&#30340;&#19968;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensemble learning has been a focal point of machine learning research due to its potential to improve predictive performance. This study revisits the foundational work on ensemble error decomposition, historically confined to bias-variance-covariance analysis for regression problems since the 1990s. Recent advancements introduced a "unified theory of diversity," which proposes an innovative bias-variance-diversity decomposition framework. Leveraging this contemporary understanding, our research systematically explores the application of this decomposition to guide the creation of new ensemble learning algorithms. Focusing on regression tasks, we employ neural networks as base learners to investigate the practical implications of this theoretical framework. This approach used 7 simple ensemble methods, we name them strategies, for neural networks that were used to generate 21 new ensemble algorithms. Among these, most of the methods aggregated with the snapshot strategy, one of the 7 st
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#24212;&#29992;&#20110;&#36275;&#29699;&#20998;&#26512;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#8220;&#36275;&#29699;&#35821;&#35328;&#8221;&#65292;LEMs&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20107;&#20214;&#30340;&#21464;&#37327;&#65292;&#20174;&#32780;&#27169;&#25311;&#27604;&#36187;&#24182;&#39044;&#27979;&#29699;&#21592;&#22312;&#19981;&#21516;&#22242;&#38431;&#32972;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;2017-2018&#33521;&#36229;&#36187;&#23395;&#20351;&#29992;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;LEMs&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#21457;&#29616;&#20102;LEMs&#22312;&#36275;&#29699;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#31361;&#20986;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29699;&#38431;&#25490;&#21517;&#21644;&#25506;&#32034;&#39640;&#32423;&#22330;&#26223;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06815</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#20272;&#35745;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;&#29699;&#21592;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Estimating Player Performance in Different Contexts Using Fine-tuned Large Events Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#24212;&#29992;&#20110;&#36275;&#29699;&#20998;&#26512;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#8220;&#36275;&#29699;&#35821;&#35328;&#8221;&#65292;LEMs&#21487;&#20197;&#39044;&#27979;&#21518;&#32493;&#20107;&#20214;&#30340;&#21464;&#37327;&#65292;&#20174;&#32780;&#27169;&#25311;&#27604;&#36187;&#24182;&#39044;&#27979;&#29699;&#21592;&#22312;&#19981;&#21516;&#22242;&#38431;&#32972;&#26223;&#19979;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#36890;&#36807;&#23545;2017-2018&#33521;&#36229;&#36187;&#23395;&#20351;&#29992;WyScout&#25968;&#25454;&#38598;&#36827;&#34892;LEMs&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#21457;&#29616;&#20102;LEMs&#22312;&#36275;&#29699;&#20998;&#26512;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#31361;&#20986;&#20102;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#29699;&#38431;&#25490;&#21517;&#21644;&#25506;&#32034;&#39640;&#32423;&#22330;&#26223;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22823;&#22411;&#20107;&#20214;&#27169;&#22411;&#65288;LEMs&#65289;&#22312;&#36275;&#29699;&#20998;&#26512;&#39046;&#22495;&#30340;&#21019;&#26032;&#24212;&#29992;&#65292;&#31867;&#20284;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#23398;&#20064;&#36275;&#29699;&#30340;&#8220;&#35821;&#35328;&#8221; - &#39044;&#27979;&#21518;&#32493;&#20107;&#20214;&#30340;&#21464;&#37327;&#32780;&#19981;&#26159;&#21333;&#35789;&#65292;LEMs&#21487;&#20197;&#27169;&#25311;&#27604;&#36187;&#24182;&#25552;&#20379;&#21508;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#19981;&#21516;&#22242;&#38431;&#32972;&#26223;&#19979;&#30340;&#29699;&#21592;&#34920;&#29616;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;WyScout&#25968;&#25454;&#38598;&#23545;2017-2018&#33521;&#36229;&#36187;&#23395;&#36827;&#34892;LEMs&#30340;&#31934;&#32454;&#35843;&#25972;&#65292;&#20197;&#33719;&#21462;&#20851;&#20110;&#29699;&#21592;&#36129;&#29486;&#21644;&#22242;&#38431;&#25112;&#30053;&#30340;&#20855;&#20307;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#21453;&#26144;&#36275;&#29699;&#30340;&#24494;&#22937;&#21160;&#24577;&#65292;&#20174;&#32780;&#35780;&#20272;&#20551;&#35774;&#30340;&#36716;&#20250;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#39564;&#35777;&#20102;LEMs&#22312;&#36275;&#29699;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#21644;&#23616;&#38480;&#24615;&#65292;&#31361;&#26174;&#20102;&#35813;&#27169;&#22411;&#39044;&#27979;&#29699;&#38431;&#39044;&#26399;&#25490;&#21517;&#24182;&#25506;&#32034;&#39640;&#32423;&#22330;&#26223;&#65288;&#20363;&#22914;&#23558;Cristiano Ronaldo&#25110;Lionel Messi&#36716;&#20250;&#33267;&#19981;&#21516;&#29699;&#38431;&#30340;&#28508;&#22312;&#24433;&#21709;&#65289;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an innovative application of Large Event Models (LEMs), akin to Large Language Models, to the domain of soccer analytics. By learning the "language" of soccer - predicting variables for subsequent events rather than words LEMs facilitate the simulation of matches and offer various applications, including player performance prediction across different team contexts. We focus on fine-tuning LEMs with the WyScout dataset for the 2017-2018 Premier League season to derive specific insights into player contributions and team strategies. Our methodology involves adapting these models to reflect the nuanced dynamics of soccer, enabling the evaluation of hypothetical transfers. Our findings confirm the effectiveness and limitations of LEMs in soccer analytics, highlighting the model's capability to forecast teams' expected standings and explore high-profile scenarios, such as the potential effects of transferring Cristiano Ronaldo or Lionel Messi to different teams in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#26679;&#26412;&#22823;&#23567;&#21644;&#31867;&#21035;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#25351;&#26631;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.06812</link><description>&lt;p&gt;
&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#30340;&#26694;&#26550;&#29992;&#20110;&#30417;&#27979;&#21307;&#38498;&#20869;&#27515;&#20129;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Kalman Filter Based Framework for Monitoring the Performance of In-Hospital Mortality Prediction Models Over Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#21345;&#23572;&#26364;&#28388;&#27874;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#26679;&#26412;&#22823;&#23567;&#21644;&#31867;&#21035;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#23545;&#19981;&#21516;&#26102;&#38388;&#27573;&#24615;&#33021;&#25351;&#26631;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20020;&#24202;&#35797;&#39564;&#19981;&#21516;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#30830;&#23450;&#25152;&#38656;&#30340;&#27491;&#36127;&#26679;&#26412;&#25968;&#37327;&#65292;&#25110;&#32773;&#22312;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#65292;&#39564;&#35777;&#38598;&#30340;&#22823;&#23567;&#21644;&#31867;&#21035;&#20998;&#24067;&#26159;&#38745;&#24577;&#21644;&#24050;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#36827;&#20837;&#30340;&#24739;&#32773;&#25968;&#37327;&#21644;&#20998;&#24067;&#24456;&#38590;&#25511;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#27573;&#27979;&#37327;&#26102;&#65292;&#35780;&#20272;&#25351;&#26631;&#22914;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24615;&#19979;&#38754;&#31215;&#65288;AUCROC&#65289;&#21644;&#31934;&#30830;&#24230;-&#21484;&#22238;&#29575;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUCPR&#65289;&#21487;&#33021;&#26080;&#27861;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#23545;&#20110;&#36816;&#34892;&#26102;&#38388;&#38271;&#30340;&#20108;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#35843;&#25972;&#36825;&#20123;&#24615;&#33021;&#25351;&#26631;&#20197;&#32771;&#34385;&#26679;&#26412;&#22823;&#23567;&#21644;&#31867;&#21035;&#20998;&#24067;&#65292;&#20197;&#20415;&#22312;&#20004;&#20010;&#26102;&#38388;&#27573;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26679;&#26412;&#25968;&#37327;&#21644;&#31867;&#21035;&#20998;&#24067;&#65292;&#21363;&#27491;&#26679;&#26412;&#30340;&#27604;&#20363;&#65292;&#26159;&#24433;&#21709;AUCROC&#26041;&#24046;&#30340;&#20004;&#20010;&#40065;&#26834;&#24615;&#22240;&#32032;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20272;&#35745;&#24615;&#33021;&#25351;&#26631;&#30340;&#22343;&#20540;a
&lt;/p&gt;
&lt;p&gt;
Unlike in a clinical trial, where researchers get to determine the least number of positive and negative samples required, or in a machine learning study where the size and the class distribution of the validation set is static and known, in a real-world scenario, there is little control over the size and distribution of incoming patients. As a result, when measured during different time periods, evaluation metrics like Area under the Receiver Operating Curve (AUCROC) and Area Under the Precision-Recall Curve(AUCPR) may not be directly comparable. Therefore, in this study, for binary classifiers running in a long time period, we proposed to adjust these performance metrics for sample size and class distribution, so that a fair comparison can be made between two time periods. Note that the number of samples and the class distribution, namely the ratio of positive samples, are two robustness factors which affect the variance of AUCROC. To better estimate the mean of performance metrics a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24635;&#20449;&#24687;&#27969;&#26469;&#23450;&#37327;&#35780;&#20272;&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;</title><link>https://arxiv.org/abs/2402.06810</link><description>&lt;p&gt;
&#20351;&#29992;&#24635;&#20449;&#24687;&#27969;&#35780;&#20272;&#20849;&#21516;&#21019;&#36896;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Co-Creativity using Total Information Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24635;&#20449;&#24687;&#27969;&#26469;&#23450;&#37327;&#35780;&#20272;&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#35777;&#26126;&#35813;&#26041;&#27861;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#25351;&#30340;&#26159;&#20004;&#20010;&#25110;&#26356;&#22810;&#30340;&#38899;&#20048;&#23478;&#25110;&#38899;&#20048;&#20195;&#29702;&#36890;&#36807;&#21019;&#20316;&#25110;&#21363;&#20852;&#21019;&#20316;&#38899;&#20048;&#30456;&#20114;&#20114;&#21160;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#38750;&#24120;&#20027;&#35266;&#30340;&#36807;&#31243;&#65292;&#27599;&#20010;&#38899;&#20048;&#23478;&#23545;&#20110;&#22312;&#26576;&#31181;&#24773;&#22659;&#19979;&#21738;&#31181;&#21363;&#20852;&#21019;&#20316;&#26356;&#22909;&#26377;&#33258;&#24049;&#30340;&#20559;&#22909;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22522;&#20110;&#24635;&#20449;&#24687;&#27969;&#30340;&#24230;&#37327;&#26469;&#23450;&#37327;&#35780;&#20272;&#38899;&#20048;&#20013;&#30340;&#20849;&#21516;&#21019;&#36896;&#21147;&#36807;&#31243;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#26159;&#21019;&#36896;&#24615;&#38899;&#20048;&#36807;&#31243;&#26377;&#22810;"&#22909;"&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20551;&#35774;&#26159;&#65292;&#22909;&#30340;&#38899;&#20048;&#21019;&#20316;&#23558;&#26368;&#22823;&#21270;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#20449;&#24687;&#27969;&#30001;&#35760;&#24405;&#22312;&#21333;&#29420;&#36712;&#36947;&#20013;&#30340;&#38899;&#20048;&#22768;&#38899;&#25429;&#25417;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29109;&#20272;&#35745;&#22120;&#35745;&#31639;&#20449;&#24687;&#27969;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#19982;&#20154;&#31867;&#24863;&#30693;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Co-creativity in music refers to two or more musicians or musical agents interacting with one another by composing or improvising music. However, this is a very subjective process and each musician has their own preference as to which improvisation is better for some context. In this paper, we aim to create a measure based on total information flow to quantitatively evaluate the co-creativity process in music. In other words, our measure is an indication of how "good" a creative musical process is. Our main hypothesis is that a good musical creation would maximize information flow between the participants captured by music voices recorded in separate tracks. We propose a method to compute the information flow using pre-trained generative models as entropy estimators. We demonstrate how our method matches with human perception using a qualitative study.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;delta&#26041;&#27861;&#30830;&#23450;&#24615;&#22320;&#36817;&#20284;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;SHAP&#26041;&#27861;&#26469;&#24402;&#22240;&#20110;&#21464;&#24322;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20020;&#24202;&#24694;&#21270;&#39044;&#27979;&#20013;&#30340;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06808</link><description>&lt;p&gt;
&#23545;&#20110;&#20020;&#24202;&#24694;&#21270;&#39044;&#27979;&#30340;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20013;&#39044;&#27979;&#21464;&#24322;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain Variance of Prediction in Variational Time Series Models for Clinical Deterioration Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;delta&#26041;&#27861;&#30830;&#23450;&#24615;&#22320;&#36817;&#20284;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;SHAP&#26041;&#27861;&#26469;&#24402;&#22240;&#20110;&#21464;&#24322;&#30340;&#36129;&#29486;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20020;&#24202;&#24694;&#21270;&#39044;&#27979;&#20013;&#30340;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#30340;&#21516;&#26102;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#35768;&#22810;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#30340;&#24212;&#29992;&#65292;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#25152;&#20316;&#20986;&#30340;&#39044;&#27979;&#20998;&#25968;&#30340;&#21487;&#35299;&#37322;&#24615;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20303;&#38498;&#30149;&#20154;&#30340;&#27599;&#26085;&#25110;&#27599;&#23567;&#26102;&#24694;&#21270;&#39118;&#38505;&#39044;&#27979;&#65292;&#19981;&#20165;&#39044;&#27979;&#30340;&#39118;&#38505;&#27010;&#29575;&#20998;&#25968;&#24456;&#37325;&#35201;&#65292;&#39118;&#38505;&#20998;&#25968;&#30340;&#21464;&#24322;&#24615;&#20063;&#23545;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;delta&#26041;&#27861;&#20197;&#30830;&#23450;&#24615;&#22320;&#36817;&#20284;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#65292;&#20174;&#32780;&#21487;&#20197;&#37319;&#29992;SHAP&#26041;&#27861;&#26469;&#24402;&#22240;&#20110;&#21464;&#24322;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#23545;&#21464;&#20998;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#38544;&#34255;&#31354;&#38388;&#36827;&#34892;&#37319;&#26679;&#26469;&#20272;&#35745;&#39044;&#27979;&#30340;&#21464;&#24322;&#24615;&#65292;&#24182;&#22522;&#20110;&#21464;&#24322;&#24615;&#21338;&#24328;&#30340;Shapley&#20540;&#23558;&#20854;&#20256;&#25773;&#21040;&#36755;&#20837;&#30340;&#20020;&#24202;&#21464;&#37327;&#19978;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21464;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#20998;&#36716;&#25442;&#22120;&#31561;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35748;&#20026;&#65292;&#21464;&#20998;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38750;&#24120;&#36866;&#21512;&#22312;&#39044;&#27979;&#31934;&#24230;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In healthcare, thanks to many model agnostic methods, explainability of the prediction scores made by deep learning applications has improved. However, we note that for daily or hourly risk of deterioration prediction of in-hospital patients, not only the predicted risk probability score matters, but also the variance of the risk scores play key roles in aiding clinical decision making. In this paper, we propose to use delta's method to approximate variance of prediction deterministically, such that the SHAP method can be adopted to attribute contribution of variance. The prediction variance is estimated by sampling the conditional hidden space in variational models and is propagated to input clinical variables based on Shapley values of the variance game. This approach works with variational time series models such as variational recurrent neural networks and variational transformers. We further argue that variational time series models are perfect fits for achieving a balance between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;</title><link>https://arxiv.org/abs/2402.06806</link><description>&lt;p&gt;
&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#30340;&#21407;&#21017;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Assessment of Tabular Data Synthesis Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21512;&#25104;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#21512;&#25104;&#22120;&#65289;&#12290;&#19968;&#20123;&#21512;&#25104;&#22120;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#26088;&#22312;&#20197;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#30001;&#20110;&#32570;&#20047;&#21407;&#21017;&#24615;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#23545;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#21512;&#25104;&#22120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38754;&#23545;&#38754;&#27604;&#36739;&#30340;&#26032;&#24320;&#21457;&#30340;&#21512;&#25104;&#22120;&#30340;&#29702;&#35299;&#23578;&#19981;&#20840;&#38754;&#65292;&#23545;&#36825;&#20123;&#21512;&#25104;&#22120;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#30340;&#20840;&#38754;&#20102;&#35299;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26816;&#26597;&#21644;&#25209;&#35780;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#20854;&#38480;&#21046;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#32452;&#32455;&#26694;&#26550;&#65292;&#20197;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#24182;&#36827;&#34892;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. A large number of tabular data synthesis algorithms (which we call synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to lacking principled evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art marginal-based synthesizers.   In this paper, we present a principled and systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. Based on the proposed metrics, we also devise a un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21152;&#25343;&#22823;&#22823;&#28393;&#22320;&#21306;&#21644;&#33832;&#24067;&#23572;&#23707;&#19978;&#39044;&#27979;&#28023;&#27915;&#38654;&#33021;&#35265;&#24230;&#65292;&#20026;30&#20998;&#38047;&#21644;60&#20998;&#38047;&#30340;&#21487;&#35265;&#24230;&#38408;&#20540;&#25552;&#20379;&#20102;&#29983;&#25104;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.06800</link><description>&lt;p&gt;
&#22312;&#21152;&#25343;&#22823;&#22823;&#28393;&#22320;&#21306;&#21644;&#33832;&#24067;&#23572;&#23707;&#19978;&#36827;&#34892;&#28023;&#27915;&#38654;&#33021;&#35265;&#24230;&#30340;&#29983;&#25104;&#24335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Generative Nowcasting of Marine Fog Visibility in the Grand Banks area and Sable Island in Canada
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21152;&#25343;&#22823;&#22823;&#28393;&#22320;&#21306;&#21644;&#33832;&#24067;&#23572;&#23707;&#19978;&#39044;&#27979;&#28023;&#27915;&#38654;&#33021;&#35265;&#24230;&#65292;&#20026;30&#20998;&#38047;&#21644;60&#20998;&#38047;&#30340;&#21487;&#35265;&#24230;&#38408;&#20540;&#25552;&#20379;&#20102;&#29983;&#25104;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;2022&#24180;7&#26376;&#22312;&#22823;&#35199;&#27915;&#21271;&#37096;&#22823;&#28393;&#22320;&#21306;&#21644;&#21152;&#25343;&#22823;&#19996;&#21271;&#37096;&#30340;&#33832;&#24067;&#23572;&#23707;&#38468;&#36817;&#36827;&#34892;&#30340;FATIMA&#65288;&#28023;&#27915;&#22823;&#27668;&#20013;&#30340;&#38654;&#21644;&#28237;&#27969;&#30456;&#20114;&#20316;&#29992;&#65289;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#28023;&#27915;&#38654;&#33021;&#35265;&#24230;&#39044;&#27979;&#12290;&#35266;&#27979;&#25968;&#25454;&#21253;&#25324;&#20351;&#29992;Vaisala&#21069;&#21521;&#25955;&#23556;&#20256;&#24863;&#22120;&#27169;&#22411;FD70&#12289;&#22825;&#27668;&#20256;&#36755;&#22120;&#27169;&#22411;WXT50&#20197;&#21450;Research Vessel Atlantic Condor&#19978;&#23433;&#35013;&#30340;Gill R3A&#36229;&#22768;&#39118;&#36895;&#20202;&#25910;&#38598;&#30340;&#38654;&#33021;&#35265;&#24230;&#12289;&#39118;&#36895;&#12289;&#38706;&#28857;&#24046;&#21644;&#30456;&#23545;&#28287;&#24230;&#12290;&#39044;&#27979;&#36807;&#31243;&#20013;&#65292;&#36890;&#36807;&#23545;&#38654;&#33021;&#35265;&#24230;&#12289;&#39118;&#36895;&#12289;&#38706;&#28857;&#24046;&#21644;&#30456;&#23545;&#28287;&#24230;&#36827;&#34892;&#26102;&#28382;&#22788;&#29702;&#65292;&#29983;&#25104;&#24335;&#23545;&#38654;&#33021;&#35265;&#24230;&#36827;&#34892;&#20102;30&#20998;&#38047;&#21644;60&#20998;&#38047;&#30340;&#39044;&#27979;&#65292;&#24182;&#35774;&#23450;&#20102;&#38654;&#33021;&#35265;&#24230;&#23567;&#20110;1&#20844;&#37324;&#21644;&#23567;&#20110;10&#20844;&#37324;&#20004;&#31181;&#21487;&#35265;&#24230;&#38408;&#20540;&#12290;&#27492;&#22806;&#65292;&#36824;&#20351;&#29992;&#20102;&#26497;&#38480;&#26799;&#24230;&#25552;&#21319;&#65288;XGBoost&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents the application of generative deep learning techniques to evaluate marine fog visibility nowcasting using the FATIMA (Fog and turbulence interactions in the marine atmosphere) campaign observations collected during July 2022 in the North Atlantic in the Grand Banks area and vicinity of Sable Island (SI), northeast of Canada. The measurements were collected using the Vaisala Forward Scatter Sensor model FD70 and Weather Transmitter model WXT50, and Gill R3A ultrasonic anemometer mounted on the Research Vessel Atlantic Condor. To perform nowcasting, the time series of fog visibility (Vis), wind speed, dew point depression, and relative humidity with respect to water were preprocessed to have lagged time step features. Generative nowcasting of Vis time series for lead times of 30 and 60 minutes were performed using conditional generative adversarial networks (cGAN) regression at visibility thresholds of Vis &lt; 1 km and &lt; 10 km. Extreme gradient boosting (XGBoost) was us
&lt;/p&gt;</description></item><item><title>ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06787</link><description>&lt;p&gt;
ForestColl: &#24322;&#26500;&#32593;&#32476;&#32467;&#26500;&#19978;&#39640;&#25928;&#30340;&#38598;&#21512;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06787
&lt;/p&gt;
&lt;p&gt;
ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#65292;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#20449;&#65288;&#22914;allreduce&#31561;&#65289;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#22312;&#24403;&#20170;&#39640;&#24230;&#22810;&#26679;&#21270;&#21644;&#24322;&#26500;&#30340;&#32593;&#32476;&#32467;&#26500;&#19979;&#35774;&#35745;&#39640;&#25928;&#30340;&#36890;&#20449;&#35843;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForestColl&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#20026;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#30340;&#35843;&#24230;&#12290;ForestColl&#20351;&#29992;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#20316;&#20026;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#12290;&#20854;&#35843;&#24230;&#29983;&#25104;&#36816;&#34892;&#22312;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#65292;&#19988;&#20855;&#26377;&#39640;&#25193;&#23637;&#24615;&#12290;ForestColl&#25903;&#25345;&#21253;&#25324;&#20132;&#25442;&#32593;&#32476;&#21644;&#30452;&#25509;&#36830;&#25509;&#22312;&#20869;&#30340;&#20219;&#20309;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#20219;&#20309;&#32593;&#32476;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#22810;&#38598;&#32676;&#30340;AMD MI250&#21644;NVIDIA A100&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;ForestColl&#12290;&#19982;&#20379;&#24212;&#21830;&#33258;&#24049;&#20248;&#21270;&#30340;&#36890;&#20449;&#24211;RCCL&#21644;NCCL&#30456;&#27604;&#65292;ForestColl&#30340;&#35843;&#24230;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;52&#65285;&#12290;ForestColl&#36824;&#20248;&#20110;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging given today's highly diverse and heterogeneous network fabrics. In this paper, we present ForestColl, a tool that generates efficient schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretically minimum network congestion. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct connections, as well as any network graph structure. We evaluated ForestColl on multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules achieved up to 52\% higher performance compared to the vendors' own optimized communication libraries, RCCL and NCCL. ForestColl also outperforms other s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06784</link><description>&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transfer learning with generative models for object detection on limited datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#36890;&#29992;&#24773;&#26223;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26576;&#20123;&#39046;&#22495;&#20013;&#65292;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#26159;&#26377;&#38480;&#30340;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#65292;&#38656;&#35201;&#27491;&#30830;&#26631;&#35760;&#27599;&#20010;&#30446;&#26631;&#21608;&#22260;&#30340;&#36793;&#30028;&#26694;&#12290;&#19968;&#20010;&#26174;&#33879;&#30340;&#20363;&#23376;&#26159;&#22312;&#28023;&#27915;&#29983;&#29289;&#23398;&#39046;&#22495;&#65292;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#26816;&#27979;&#28023;&#27915;&#29289;&#31181;&#29992;&#20110;&#29615;&#22659;&#30417;&#27979;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38480;&#21046;&#38382;&#39064;&#65292;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#31574;&#30053;&#37319;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28982;&#21518;&#25512;&#24191;&#21040;&#20855;&#20307;&#30340;&#39046;&#22495;&#12290;&#31532;&#20108;&#31181;&#31574;&#30053;&#26159;&#20351;&#29992;copy-paste&#25216;&#26415;&#25110;ad-hoc&#27169;&#25311;&#22120;&#31561;&#26041;&#27861;&#21019;&#24314;&#29305;&#23450;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#37325;&#22823;&#30340;&#39046;&#22495;&#36716;&#31227;&#38382;&#39064;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#35774;&#35745;&#23450;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#36890;&#29992;&#24773;&#26223;&#19979;&#26377;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The availability of data is limited in some fields, especially for object detection tasks, where it is necessary to have correctly labeled bounding boxes around each object. A notable example of such data scarcity is found in the domain of marine biology, where it is useful to develop methods to automatically detect submarine species for environmental monitoring. To address this data limitation, the state-of-the-art machine learning strategies employ two main approaches. The first involves pretraining models on existing datasets before generalizing to the specific domain of interest. The second strategy is to create synthetic datasets specifically tailored to the target domain using methods like copy-paste techniques or ad-hoc simulators. The first strategy often faces a significant domain shift, while the second demands custom solutions crafted for the specific task. In response to these challenges, here we propose a transfer learning framework that is valid for a generic scenario. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06783</link><description>&lt;p&gt;
&#23398;&#20064;&#25945;&#23398;&#65306;&#25913;&#21892;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#29616;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21040;&#29616;&#23454;&#65288;sim-to-real&#65289;&#30340;&#36801;&#31227;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#24615;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#20013;&#30340;&#22122;&#22768;&#20351;&#24471;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#21152;&#36895;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65292;&#25945;&#24072;&#26234;&#33021;&#20307;&#21487;&#20197;&#25351;&#23548;&#23398;&#29983;&#26234;&#33021;&#20307;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#23398;&#29983;&#26234;&#33021;&#20307;&#26102;&#23436;&#20840;&#33293;&#24323;&#20102;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#28010;&#36153;&#20102;&#29615;&#22659;&#25152;&#36879;&#38706;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#30340;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#26469;&#25193;&#23637;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#19968;&#23545;&#25945;&#24072;-&#23398;&#29983;&#26234;&#33021;&#20307;&#65292;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#23545;&#20004;&#32773;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-to-reality (sim-to-real) transfer is a fundamental problem for robot learning. Domain Randomization, which adds randomization during training, is a powerful technique that effectively addresses the sim-to-real gap. However, the noise in observations makes learning significantly harder. Recently, studies have shown that employing a teacher-student learning paradigm can accelerate training in randomized environments. Learned with privileged information, a teacher agent can instruct the student agent to operate in noisy environments. However, this approach is often not sample efficient as the experience collected by the teacher is discarded completely when training the student, wasting information revealed by the environment. In this work, we extend the teacher-student learning paradigm by proposing a sample efficient learning framework termed Learn to Teach (L2T) that recycles experience collected by the teacher agent. We observe that the dynamics of the environments for both 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20135;&#29289;&#20998;&#23376;&#22270;&#21644;&#31163;&#24320;&#22522;&#22242;&#36229;&#22270;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#21453;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.06772</link><description>&lt;p&gt;
&#36890;&#36807;(&#36229;)&#22270;&#25628;&#32034;&#36827;&#34892;&#36870;&#21512;&#25104;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis Prediction via Search in (Hyper) Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20135;&#29289;&#20998;&#23376;&#22270;&#21644;&#31163;&#24320;&#22522;&#22242;&#36229;&#22270;&#20013;&#36827;&#34892;&#25628;&#32034;&#65292;&#20197;&#22788;&#29702;&#22797;&#26434;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26377;&#26426;&#21512;&#25104;&#20013;&#65292;&#20174;&#25351;&#23450;&#30340;&#26680;&#24515;&#20135;&#29289;&#39044;&#27979;&#21453;&#24212;&#29289;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#65292;&#34987;&#31216;&#20026;&#36870;&#21521;&#21512;&#25104;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#21322;&#27169;&#26495;&#21644;&#22522;&#20110;&#22270;&#32534;&#36753;&#30340;&#26041;&#27861;&#22312;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#26426;&#21046;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#39044;&#27979;&#22797;&#26434;&#30340;&#21453;&#24212;&#65292;&#20363;&#22914;&#20855;&#26377;&#22810;&#20010;&#21453;&#24212;&#20013;&#24515;&#25110;&#23558;&#30456;&#21516;&#31163;&#24320;&#22522;&#22242;&#36830;&#25509;&#21040;&#22810;&#20010;&#21407;&#23376;&#30340;&#21453;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;&#21363;&#36870;&#21521;&#21512;&#25104;&#36890;&#36807;(&#36229;)&#22270;&#25628;&#32034;(RetroSiG)&#26694;&#26550;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#38480;&#21046;&#12290;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#23558;&#21453;&#24212;&#20013;&#24515;&#30340;&#35782;&#21035;&#21644;&#31163;&#24320;&#22522;&#22242;&#30340;&#23436;&#25104;&#20219;&#21153;&#36716;&#21270;&#20026;&#22312;&#20135;&#29289;&#20998;&#23376;&#22270;&#20013;&#25628;&#32034;&#21644;&#22312;&#31163;&#24320;&#22522;&#22242;&#36229;&#22270;&#20013;&#25628;&#32034;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#21322;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;RetroSiG&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;RetroSiG&#33021;&#22815;&#22788;&#29702;&#25552;&#21040;&#30340;&#22797;&#26434;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting reactants from a specified core product stands as a fundamental challenge within organic synthesis, termed retrosynthesis prediction. Recently, semi-template-based methods and graph-edits-based methods have achieved good performance in terms of both interpretability and accuracy. However, due to their mechanisms these methods cannot predict complex reactions, e.g., reactions with multiple reaction center or attaching the same leaving group to more than one atom. In this study we propose a semi-template-based method, the \textbf{Retro}synthesis via \textbf{S}earch \textbf{i}n (Hyper) \textbf{G}raph (RetroSiG) framework to alleviate these limitations. In the proposed method, we turn the reaction center identification and the leaving group completion tasks as tasks of searching in the product molecular graph and leaving group hypergraph respectively. As a semi-template-based method RetroSiG has several advantages. First, RetroSiG is able to handle the complex reactions mentione
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26680;&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#30340;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06763</link><description>&lt;p&gt;
&#20351;&#29992;Nystr\"om&#36817;&#20284;&#30340;&#21487;&#25193;&#23637;&#26680;&#36923;&#36753;&#22238;&#24402;&#65306;&#29702;&#35770;&#20998;&#26512;&#21644;&#31163;&#25955;&#36873;&#25321;&#24314;&#27169;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26680;&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#30340;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31163;&#25955;&#36873;&#25321;&#24314;&#27169;&#26102;&#65292;&#32463;&#24120;&#38754;&#20020;&#23384;&#20648;&#38656;&#27714;&#21644;&#27169;&#22411;&#20013;&#28041;&#21450;&#30340;&#22823;&#37327;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26680;&#36923;&#36753;&#22238;&#24402;&#12290;&#30740;&#31350;&#39318;&#20808;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#65306;i) &#23545;KLR&#35299;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;ii) &#32473;&#20986;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#30340;KLR&#35299;&#30340;&#19978;&#30028;&#65292;&#24182;&#26368;&#21518;&#25551;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;Nystr\"om KLR&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#29305;&#21270;&#12290;&#20043;&#21518;&#65292;&#23545;Nystr\"om KLR&#36827;&#34892;&#20102;&#35745;&#31639;&#39564;&#35777;&#12290;&#27979;&#35797;&#20102;&#22235;&#31181;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#26412;&#22343;&#21248;&#37319;&#26679;&#12289;k-means&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#26464;&#26438;&#24471;&#20998;&#30340;&#20004;&#31181;&#38750;&#22343;&#21248;&#26041;&#27861;&#12290;&#36825;&#20123;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\"om KLR is described. After this, the Nystr\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#21387;&#32553;&#27169;&#22359;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#25945;&#24072;&#36716;&#25442;&#26469;&#24471;&#21040;&#32039;&#20945;&#30340;&#25945;&#24072;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#26080;&#30417;&#30563;&#25945;&#24072;&#23884;&#20837;&#65292;&#24182;&#19988;&#20351;&#29992;&#23884;&#20837;&#36827;&#34892;&#24341;&#23548;&#35757;&#32451;&#30340;&#23398;&#29983;&#27169;&#22411;&#23637;&#29616;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06761</link><description>&lt;p&gt;
&#25945;&#24072;&#19982;&#23398;&#29983;&#20043;&#38388;&#30693;&#35782;&#20256;&#36882;&#30340;&#23884;&#20837;&#24335;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Embedding Compression for Teacher-to-Student Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06761
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#24335;&#21387;&#32553;&#27169;&#22359;&#65292;&#36890;&#36807;&#21487;&#35757;&#32451;&#30340;&#25945;&#24072;&#36716;&#25442;&#26469;&#24471;&#21040;&#32039;&#20945;&#30340;&#25945;&#24072;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#20998;&#31867;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;&#26080;&#30417;&#30563;&#25945;&#24072;&#23884;&#20837;&#65292;&#24182;&#19988;&#20351;&#29992;&#23884;&#20837;&#36827;&#34892;&#24341;&#23548;&#35757;&#32451;&#30340;&#23398;&#29983;&#27169;&#22411;&#23637;&#29616;&#20986;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#35201;&#27714;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#23558;&#23884;&#20837;&#20316;&#20026;&#25945;&#24072;&#26469;&#29992;&#20110;&#19981;&#21516;&#30340;&#28304;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20197;&#24448;&#20351;&#29992;&#23884;&#20837;&#20316;&#20026;&#25945;&#24072;&#30340;&#26041;&#27861;&#24573;&#35270;&#20102;&#25945;&#24072;&#23884;&#20837;&#24456;&#21487;&#33021;&#21253;&#21547;&#30446;&#26631;&#20219;&#21153;&#30340;&#26080;&#20851;&#30693;&#35782;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;&#25945;&#24072;&#36716;&#25442;&#26469;&#24471;&#21040;&#32039;&#20945;&#30340;&#25945;&#24072;&#23884;&#20837;&#30340;&#23884;&#20837;&#21387;&#32553;&#27169;&#22359;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#28155;&#21152;&#23884;&#20837;&#21387;&#32553;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26080;&#30417;&#30563;&#30340;&#25945;&#24072;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23884;&#20837;&#30340;&#24341;&#23548;&#35757;&#32451;&#30340;&#23398;&#29983;&#27169;&#22411;&#23637;&#29616;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common knowledge distillation methods require the teacher model and the student model to be trained on the same task. However, the usage of embeddings as teachers has also been proposed for different source tasks and target tasks. Prior work that uses embeddings as teachers ignores the fact that the teacher embeddings are likely to contain irrelevant knowledge for the target task. To address this problem, we propose to use an embedding compression module with a trainable teacher transformation to obtain a compact teacher embedding. Results show that adding the embedding compression module improves the classification performance, especially for unsupervised teacher embeddings. Moreover, student models trained with the guidance of embeddings show stronger generalizability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#31216;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38750;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#23567;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30697;&#38453;&#35299;&#65292;&#21363;&#20351;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#25104;&#31435;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#22312;&#33719;&#24471;&#36275;&#22815;&#22810;&#30340;&#35266;&#27979;&#26465;&#30446;&#21518;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.06756</link><description>&lt;p&gt;
&#20351;&#29992;&#23567;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#22312;&#38750;&#27491;&#21017;&#21270;&#30697;&#38453;&#23436;&#25104;&#20013;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#23545;&#31216;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#20013;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38750;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#23567;&#21021;&#22987;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30697;&#38453;&#35299;&#65292;&#21363;&#20351;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#20063;&#25104;&#31435;&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20960;&#20046;&#32447;&#24615;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#22312;&#33719;&#24471;&#36275;&#22815;&#22810;&#30340;&#35266;&#27979;&#26465;&#30446;&#21518;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23545;&#31216;&#30697;&#38453;&#23436;&#25104;&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#20174;&#20165;&#35266;&#27979;&#21040;&#30340;&#37096;&#20998;&#26465;&#30446;&#20013;&#37325;&#26500;&#19968;&#20010;&#27491;&#21322;&#23450;&#30697;&#38453;X*&#65292;&#20854;&#31561;&#20215;&#20110;&#21442;&#25968;&#21270;&#30697;&#38453;UU^T&#65292;&#20854;&#20013;X*&#30340;&#31209;&#20026;r&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#23567;&#30340;&#21021;&#22987;&#21270;&#30340;&#22522;&#26412;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#31639;&#27861;&#21487;&#20197;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30697;&#38453;X*&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#27491;&#21017;&#21270;&#12290;&#36825;&#20010;&#25910;&#25947;&#32467;&#26524;&#36866;&#29992;&#20110;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30495;&#23454;&#31209;r&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#34987;&#19968;&#20010;&#25628;&#32034;&#31209;r'&#20445;&#23432;&#20272;&#35745;&#65292;&#19988;r' &gt;&gt; r&#12290;&#29616;&#26377;&#30340;&#32467;&#26524;&#35201;&#20040;&#38656;&#35201;&#26174;&#24335;&#30340;&#27491;&#21017;&#21270;&#65292;&#25110;&#32773;&#38656;&#35201;&#36275;&#22815;&#20934;&#30830;&#30340;&#21021;&#22987;&#28857;&#65292;&#25110;&#32773;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#30495;&#23454;&#31209;r&#12290;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;r' &gt;= r&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#33719;&#24471;&#937;(dr^9)&#30340;&#35266;&#27979;&#26465;&#30446;&#21518;&#65292;GD&#31639;&#27861;&#20197;&#21021;&#22987;&#28857;&#8741;U_0&#8741; &lt;= &#949;&#20960;&#20046;&#32447;&#24615;&#25910;&#25947;&#21040;X*&#30340;&#949;-&#37051;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix $\rm{X}^\star \in \mathbb{R}^{d\times d}$ of rank-$r$, parameterized by $\rm{U}\rm{U}^{\top}$, from only a subset of its observed entries. We show that the vanilla gradient descent (GD) with small initialization provably converges to the ground truth $\rm{X}^\star$ without requiring any explicit regularization. This convergence result holds true even in the over-parameterized scenario, where the true rank $r$ is unknown and conservatively over-estimated by a search rank $r'\gg r$. The existing results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank $r$.   In the over-parameterized regime where $r'\geq r$, we show that, with $\widetilde\Omega(dr^9)$ observations, GD with an initial point $\|\rm{U}_0\| \leq \epsilon$ converges near-linearly to an $\epsilon$-neighborhood of $\rm{X}^\star$. C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;DNNs&#20013;&#30340;&#26799;&#24230;&#31209;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#20302;&#31209;&#23398;&#20064;&#26159;&#26576;&#20123;DNN&#26550;&#26500;&#22266;&#26377;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#30340;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.06751</link><description>&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#23454;&#29616;&#20302;&#31209;&#23398;&#20064;&#65306;&#32593;&#32476;&#26550;&#26500;&#21644;&#28608;&#27963;&#32447;&#24615;&#22312;&#26799;&#24230;&#31209;&#22604;&#38519;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Learning by Design: the Role of Network Architecture and Activation Linearity in Gradient Rank Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;DNNs&#20013;&#30340;&#26799;&#24230;&#31209;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#20302;&#31209;&#23398;&#20064;&#26159;&#26576;&#20123;DNN&#26550;&#26500;&#22266;&#26377;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#23398;&#20064;&#21160;&#24577;&#30340;&#29702;&#35299;&#20173;&#28982;&#19981;&#23436;&#25972;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#25581;&#31034;&#20102;&#25903;&#25345;&#36825;&#20123;&#32593;&#32476;&#30340;&#25968;&#23398;&#21407;&#29702;&#65292;&#21253;&#25324;&#8220;&#31070;&#32463;&#22349;&#22604;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;DNN&#20869;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20250;&#25910;&#25947;&#21040;&#29305;&#23450;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#20960;&#20309;&#32422;&#26463;&#22312;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#19981;&#20165;&#38480;&#20110;&#36825;&#20010;&#32456;&#27490;&#38454;&#27573;&#12290;&#20363;&#22914;&#65292;&#20840;&#36830;&#25509;&#23618;&#20013;&#30340;&#26799;&#24230;&#33258;&#28982;&#20250;&#30001;&#20110;&#35757;&#32451;&#25209;&#27425;&#19978;&#30340;&#31209;&#19968;&#22806;&#31215;&#30340;&#32047;&#31215;&#32780;&#24418;&#25104;&#19968;&#20010;&#20302;&#31209;&#32467;&#26500;&#12290;&#23613;&#31649;&#24050;&#32463;&#27880;&#24847;&#21040;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#33410;&#30465;&#20869;&#23384;&#25110;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#20294;&#20302;&#31209;&#23398;&#20064;&#20316;&#20026;&#26576;&#20123;DNN&#26550;&#26500;&#22266;&#26377;&#30340;&#19968;&#20010;&#26041;&#38754;&#30340;&#20986;&#29616;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;DNNs&#20013;&#30340;&#26799;&#24230;&#31209;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#32771;&#23519;&#20102;&#26550;&#26500;&#36873;&#25321;&#21644;&#25968;&#25454;&#32467;&#26500;&#23545;&#26799;&#24230;&#31209;&#30028;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20302;&#31209;&#23398;&#20064;&#26159;&#26576;&#20123;DNN&#26550;&#26500;&#30340;&#22266;&#26377;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our understanding of learning dynamics of deep neural networks (DNNs) remains incomplete. Recent research has begun to uncover the mathematical principles underlying these networks, including the phenomenon of "Neural Collapse", where linear classifiers within DNNs converge to specific geometrical structures during late-stage training. However, the role of geometric constraints in learning extends beyond this terminal phase. For instance, gradients in fully-connected layers naturally develop a low-rank structure due to the accumulation of rank-one outer products over a training batch. Despite the attention given to methods that exploit this structure for memory saving or regularization, the emergence of low-rank learning as an inherent aspect of certain DNN architectures has been under-explored. In this paper, we conduct a comprehensive study of gradient rank in DNNs, examining how architectural choices and structure of the data effect gradient rank bounds. Our theoretical analysis pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.06737</link><description>&lt;p&gt;
ExGRG: &#29992;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;
&lt;/p&gt;
&lt;p&gt;
ExGRG: Explicitly-Generated Relation Graph for Self-Supervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;ExGRG&#65292;&#23427;&#36890;&#36807;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#26469;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#25361;&#25112;&#65292;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#26114;&#36149;&#30340;&#26631;&#27880;&#26631;&#31614;&#32780;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20869;&#23884;&#20449;&#21495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;SSL&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#36890;&#36807;&#30452;&#35266;&#30340;&#25968;&#25454;&#22686;&#24378;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#22686;&#24378;&#25805;&#20316;&#25913;&#21464;&#20102;&#35821;&#20041;&#24182;&#21576;&#29616;&#20986;&#21453;&#30452;&#35266;&#30340;&#24615;&#36136;&#12290;&#38024;&#23545;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#26174;&#24335;&#29983;&#25104;&#20851;&#31995;&#22270;&#65288;ExGRG&#65289;&#65292;&#20197;&#21462;&#20195;&#20165;&#20381;&#38752;&#20256;&#32479;&#30340;&#22522;&#20110;&#22686;&#24378;&#30340;&#38544;&#24335;&#20851;&#31995;&#22270;&#12290;ExGRG&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#20808;&#39564;&#39046;&#22495;&#30693;&#35782;&#21644;&#22312;&#32447;&#25552;&#21462;&#30340;&#20449;&#24687;&#32435;&#20837;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#19981;&#21464;&#24615;&#30446;&#26631;&#20013;&#65292;&#20511;&#37492;&#20102;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#26144;&#23556;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;E&#27493;&#39588;&#28041;&#21450;&#20851;&#31995;&#22270;&#30340;&#29983;&#25104;&#65292;&#20197;&#35782;&#21035;...
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning (SSL) has emerged as a powerful technique in pre-training deep learning models without relying on expensive annotated labels, instead leveraging embedded signals in unlabeled data. While SSL has shown remarkable success in computer vision tasks through intuitive data augmentation, its application to graph-structured data poses challenges due to the semantic-altering and counter-intuitive nature of graph augmentations. Addressing this limitation, this paper introduces a novel non-contrastive SSL approach to Explicitly Generate a compositional Relation Graph (ExGRG) instead of relying solely on the conventional augmentation-based implicit relation graph. ExGRG offers a framework for incorporating prior domain knowledge and online extracted information into the SSL invariance objective, drawing inspiration from the Laplacian Eigenmap and Expectation-Maximization (EM). Employing an EM perspective on SSL, our E-step involves relation graph generation to identify can
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#31163;&#32447;&#26041;&#27861;&#26469;&#22788;&#29702;&#25439;&#22351;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20551;&#35774;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.06734</link><description>&lt;p&gt;
&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#25239;&#33104;&#36133;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Corruption Robust Offline Reinforcement Learning with Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06734
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#31163;&#32447;&#26041;&#27861;&#26469;&#22788;&#29702;&#25439;&#22351;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20551;&#35774;&#19979;&#20855;&#26377;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#20855;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#33104;&#36133;&#40065;&#26834;&#24615;&#38382;&#39064;&#12290;&#32473;&#23450;&#19968;&#32452;&#31163;&#32447;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#36712;&#36857;&#23545;&#20197;&#21450;&#26377;&#20851;&#20154;&#31867;&#20559;&#22909;&#30340;&#21453;&#39304;&#65292;&#20854;&#20013;$\varepsilon$&#27604;&#20363;&#30340;&#36712;&#36857;&#23545;&#34987;&#25439;&#22351;&#65288;&#20363;&#22914;&#65292;&#21453;&#39304;&#32763;&#36716;&#25110;&#36712;&#36857;&#29305;&#24449;&#34987;&#25805;&#32437;&#65289;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#23545;&#25239;&#25915;&#20987;&#25110;&#22122;&#22768;&#20154;&#31867;&#20559;&#22909;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#26088;&#22312;&#35774;&#35745;&#31639;&#27861;&#65292;&#20174;&#25439;&#22351;&#30340;&#25968;&#25454;&#20013;&#35782;&#21035;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#24182;&#19988;&#20855;&#22791;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#30740;&#31350;&#20998;&#21035;&#30740;&#31350;&#20102;&#33104;&#36133;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#65288;&#22312;&#33104;&#36133;&#19979;&#30452;&#25509;&#23398;&#20064;&#26631;&#37327;&#22870;&#21169;&#65289;&#21644;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#22312;&#27809;&#26377;&#33104;&#36133;&#30340;&#24773;&#20917;&#19979;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65289;&#30340;&#35774;&#32622;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36866;&#29992;&#20110;&#25105;&#20204;&#22788;&#29702;&#22312;&#31163;&#32447;&#29615;&#22659;&#20013;&#30340;&#25439;&#22351;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#22312;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#35206;&#30422;&#21508;&#31181;&#20551;&#35774;&#19979;&#20855;&#26377;&#33104;&#36133;&#40065;&#26834;&#24615;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#39640;&#23618;&#27425;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#20142;&#28857;&#65292;&#24182;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study data corruption robustness for reinforcement learning with human feedback (RLHF) in an offline setting. Given an offline dataset of pairs of trajectories along with feedback about human preferences, an $\varepsilon$-fraction of the pairs is corrupted (e.g., feedback flipped or trajectory features manipulated), capturing an adversarial attack or noisy human preferences. We aim to design algorithms that identify a near-optimal policy from the corrupted data, with provable guarantees. Existing theoretical works have separately studied the settings of corruption robust RL (learning from scalar rewards directly under corruption) and offline RLHF (learning from human feedback without corruption); however, they are inapplicable to our problem of dealing with corrupted data in offline RLHF setting. To this end, we design novel corruption robust offline RLHF methods under various assumptions on the coverage of the data-generating distributions. At a high level, our methodology robustif
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.06733</link><description>&lt;p&gt;
NICE: &#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#36824;&#26159;&#19981;&#20248;&#21270;&#65311;
&lt;/p&gt;
&lt;p&gt;
NICE: To Optimize In-Context Examples or Not?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06733
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#25552;&#20379;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#26159;&#21542;&#38656;&#35201;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#23545;&#20110;&#25351;&#23548;&#24615;LLMs&#30340;&#20849;&#35782;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20250;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;"&#24230;&#37327;&#26631;&#20934;"&#65292;&#29992;&#20110;&#34913;&#37327;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#65288;ICE&#65289;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20551;&#35774;&#22312;&#25552;&#31034;&#20449;&#24687;&#20013;&#35201;&#20040;&#26159;&#22266;&#23450;&#30340;&#65292;&#35201;&#20040;&#27809;&#26377;&#25552;&#20379;&#25351;&#20196;&#65292;&#23548;&#33268;&#20102;&#19968;&#20010;&#34920;&#38754;&#19978;&#30340;&#20849;&#35782;&#65306;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#23545;&#20110;&#25552;&#39640;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#38024;&#23545;&#32463;&#36807;&#25351;&#23548;&#30340;LLMs&#25361;&#25112;&#36825;&#19968;&#20849;&#35782;&#65292;&#30740;&#31350;&#22312;&#25552;&#20379;&#20102;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26159;&#21542;&#24517;&#35201;&#65292;&#24182;&#21457;&#29616;&#26377;&#19968;&#20123;&#20219;&#21153;&#23545;&#20110;&#19981;&#21516;&#30340;&#20248;&#21270;&#19978;&#19979;&#25991;&#31034;&#20363;&#26041;&#27861;&#20135;&#29983;&#36882;&#20943;&#30340;&#22238;&#25253;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;"&#24230;&#37327;&#26631;&#20934;"&#65288;Metric&#65289;&#65292;&#29992;&#20110;&#37327;&#21270;&#20174;&#32473;&#23450;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#24110;&#21161;&#20915;&#23450;&#26159;&#21542;&#20248;&#21270;&#25351;&#20196;&#36824;&#26159;ICE&#29992;&#20110;&#20219;&#20309;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#20219;&#21153;&#21644;&#36880;&#27493;&#22686;&#21152;&#30340;&#25351;&#20196;&#38598;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that large language models (LLMs) work remarkably well on a wide range of tasks through in-context learning and optimization of in-context examples (ICE). However, most of these studies assume either a fixed or no instruction provided in the prompt, leading to the apparent consensus that the optimization of in-context examples is critical for better performance. We challenge this consensus for instruction-tuned LLMs by investigating the necessity of optimizing in-context examples when task-specific instructions are provided, and find that there are tasks for which various ways of optimizing in-context examples yield diminishing returns. We introduce a task-specific metric called \metriclong{} (\metric) that quantifies the learnability of tasks from a given instruction, and provides a heuristic that helps decide whether to optimize for instructions or ICE for any new task. On a wide range of tasks and a systematically created instruction set with gradually added 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#29992;&#20110;&#20010;&#20307;&#20844;&#24179;&#30340;K&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#27604;&#29616;&#26377;&#30340;&#31639;&#27861;&#26356;&#24555;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#26356;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.06730</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#29992;&#20110;&#20010;&#20307;&#20844;&#24179;&#30340;K&#22343;&#20540;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
A Scalable Algorithm for Individually Fair K-means Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#29992;&#20110;&#20010;&#20307;&#20844;&#24179;&#30340;K&#22343;&#20540;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#27604;&#29616;&#26377;&#30340;&#31639;&#27861;&#26356;&#24555;&#24182;&#19988;&#33021;&#22815;&#20135;&#29983;&#26356;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#20307;&#20844;&#24179;&#30340;&#65288;p&#65292;k&#65289;&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#30001;Jung&#31561;&#20154;&#21644;Mahabadi&#31561;&#20154;&#24341;&#20837;&#12290;&#32473;&#23450;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;n&#20010;&#28857;P&#65292;&#23545;&#20110;P&#20013;&#30340;&#27599;&#20010;x&#65292;&#20196;$\delta(x)$&#20026;&#21253;&#21547;&#33267;&#23569;n/k&#20010;&#28857;&#30340;&#26368;&#23567;&#29699;&#30340;&#21322;&#24452;&#12290;&#22914;&#26524;&#32858;&#31867;&#20013;&#30340;&#20013;&#24515;&#19982;x&#30340;&#36317;&#31163;&#23567;&#20110;&#31561;&#20110;$\delta(x)$&#65292;&#21017;&#23558;&#20854;&#31216;&#20026;&#20010;&#20307;&#20844;&#24179;&#30340;&#32858;&#31867;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#20102;&#22909;&#30340;&#36817;&#20284;&#31639;&#27861;&#65292;&#20294;&#23578;&#26410;&#25552;&#20986;&#20855;&#26377;&#33391;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#39640;&#25928;&#23454;&#29992;&#31639;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#31532;&#19968;&#20010;&#24555;&#36895;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20165;&#20026;~O(nk^2)&#65292;&#24182;&#19988;&#33021;&#22815;&#33719;&#24471;&#65288;O(1)&#65292;6&#65289;&#30340;&#21452;&#30446;&#26631;&#36817;&#20284;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#20165;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#24555;&#24471;&#22810;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#20135;&#29983;&#26356;&#20302;&#25104;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a scalable algorithm for the individually fair ($p$, $k$)-clustering problem introduced by Jung et al. and Mahabadi et al. Given $n$ points $P$ in a metric space, let $\delta(x)$ for $x\in P$ be the radius of the smallest ball around $x$ containing at least $n / k$ points. A clustering is then called individually fair if it has centers within distance $\delta(x)$ of $x$ for each $x\in P$. While good approximation algorithms are known for this problem no efficient practical algorithms with good theoretical guarantees have been presented. We design the first fast local-search algorithm that runs in ~$O(nk^2)$ time and obtains a bicriteria $(O(1), 6)$ approximation. Then we show empirically that not only is our algorithm much faster than prior work, but it also produces lower-cost solutions.
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06716</link><description>&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Dynamic Graph Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06716
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#26694;&#26550;&#65288;DGIB&#65289;&#33021;&#22815;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#21160;&#24577;&#22270;&#34920;&#31034;&#12290;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#36890;&#36807;&#36845;&#20195;&#24341;&#23548;&#21644;&#25913;&#36827;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#65292;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#24182;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35813;&#26694;&#26550;&#33021;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#26465;&#20214;&#65292;&#25552;&#39640;&#20102;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#24191;&#27867;&#23384;&#22312;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23427;&#20204;&#25658;&#24102;&#30528;&#22797;&#26434;&#30340;&#26102;&#31354;&#29305;&#24449;&#27169;&#24335;&#65292;&#23545;&#20110;&#23427;&#20204;&#30340;&#34920;&#31034;&#23398;&#20064;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DGNNs&#65289;&#36890;&#36807;&#21033;&#29992;&#20869;&#22312;&#30340;&#21160;&#24577;&#24615;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DGNNs&#23637;&#31034;&#20102;&#26377;&#38480;&#30340;&#40065;&#26834;&#24615;&#65292;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#22270;&#20449;&#24687;&#29942;&#39048;&#65288;DGIB&#65289;&#26694;&#26550;&#26469;&#23398;&#20064;&#40065;&#26834;&#19988;&#26377;&#21306;&#20998;&#24615;&#30340;&#34920;&#31034;&#12290;&#20511;&#21161;&#20449;&#24687;&#29942;&#39048;&#65288;IB&#65289;&#21407;&#29702;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#26399;&#26395;&#30340;&#26368;&#20248;&#34920;&#31034;&#24212;&#28385;&#36275;&#26368;&#23567;-&#20840;&#23616;-&#19968;&#33268;&#65288;MSC&#65289;&#26465;&#20214;&#12290;&#20026;&#20102;&#22312;&#28508;&#22312;&#34920;&#31034;&#20013;&#21387;&#32553;&#20887;&#20313;&#20449;&#24687;&#21644;&#20445;&#30041;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;DGIB&#36845;&#20195;&#22320;&#24341;&#23548;&#21644;&#25913;&#36827;&#36890;&#36807;&#22270;&#24555;&#29031;&#20256;&#36882;&#30340;&#32467;&#26500;&#21644;&#29305;&#24449;&#20449;&#24687;&#27969;&#12290;&#20026;&#20102;&#28385;&#36275;MSC&#26465;&#20214;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;IB&#30446;&#26631;&#20998;&#35299;&#20026;DGIB$_{MS}$&#21644;DGIB$_C$&#65292;&#20854;&#20013;DGIB$_{MS}$&#36890;&#36947;&#30340;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#28369;&#38634;&#31199;&#36161;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24179;&#34913;&#21069;&#26399;&#25104;&#26412;&#21644;&#28508;&#22312;&#26410;&#26469;&#36153;&#29992;&#30340;&#26435;&#34913;&#26469;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.06715</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;&#20004;&#23618;&#28369;&#38634;&#31199;&#36161;&#38382;&#39064;&#30340;&#22312;&#32447;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning-augmented Online Algorithm for Two-level Ski-rental Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#28369;&#38634;&#31199;&#36161;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#24179;&#34913;&#21069;&#26399;&#25104;&#26412;&#21644;&#28508;&#22312;&#26410;&#26469;&#36153;&#29992;&#30340;&#26435;&#34913;&#26469;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#23618;&#28369;&#38634;&#31199;&#36161;&#38382;&#39064;&#65292;&#29992;&#25143;&#38656;&#35201;&#36890;&#36807;&#36873;&#25321;&#19977;&#31181;&#25903;&#20184;&#36873;&#39033;&#20043;&#19968;&#65306;&#25353;&#38656;&#20351;&#29992;&#65288;&#21363;&#31199;&#36161;&#65289;&#65292;&#36141;&#20080;&#21333;&#20010;&#39033;&#30446;&#65288;&#21363;&#21333;&#29420;&#36141;&#20080;&#65289;&#21644;&#36141;&#20080;&#25152;&#26377;&#39033;&#30446;&#65288;&#21363;&#32452;&#21512;&#36141;&#20080;&#65289;&#26469;&#28385;&#36275;&#22810;&#20010;&#39033;&#30446;&#30340;&#19968;&#31995;&#21015;&#38656;&#27714;&#12290;&#22312;&#19981;&#20102;&#35299;&#26410;&#26469;&#38656;&#27714;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#25143;&#26088;&#22312;&#36890;&#36807;&#24179;&#34913;&#39640;&#26114;&#30340;&#21069;&#26399;&#25104;&#26412;&#65288;&#36141;&#20080;&#65289;&#21644;&#28508;&#22312;&#30340;&#26410;&#26469;&#36153;&#29992;&#65288;&#31199;&#37329;&#65289;&#20043;&#38388;&#30340;&#26435;&#34913;&#26469;&#26368;&#23567;&#21270;&#24635;&#25104;&#26412;&#65288;&#21363;&#31199;&#37329;&#12289;&#21333;&#29420;&#36141;&#20080;&#21644;&#32452;&#21512;&#36141;&#20080;&#36153;&#29992;&#20043;&#21644;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#22312;&#32447;&#31639;&#27861;&#65288;RDTSR&#65289;&#65292;&#35813;&#31639;&#27861;&#25552;&#20379;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#34429;&#28982;&#22312;&#32447;&#31639;&#27861;&#23545;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#22330;&#26223;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;&#20856;&#22411;&#24773;&#20917;&#19979;&#24448;&#24448;&#36807;&#20110;&#35880;&#24910;&#65292;&#23548;&#33268;&#24179;&#22343;&#24615;&#33021;&#36739;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#36890;&#24120;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#24179;&#22343;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the two-level ski-rental problem,where a user needs to fulfill a sequence of demands for multiple items by choosing one of the three payment options: paying for the on-demand usage (i.e., rent), buying individual items (i.e., single purchase), and buying all the items (i.e., combo purchase). Without knowing future demands, the user aims to minimize the total cost (i.e., the sum of the rental, single purchase, and combo purchase costs) by balancing the trade-off between the expensive upfront costs (for purchase) and the potential future expenses (for rent). We first design a robust online algorithm (RDTSR) that offers a worst-case performance guarantee. While online algorithms are robust against the worst-case scenarios, they are often overly cautious and thus suffer a poor average performance in typical scenarios. On the other hand, Machine Learning (ML) algorithms typically show promising average performance in various applications but lack worst-case performan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29233;&#23572;&#20848;&#30340;&#24179;&#34913;&#24066;&#22330;&#24212;&#29992;&#20102;&#21508;&#31181;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#30340;&#20215;&#26684;&#39044;&#27979;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#22312;&#26085;&#21069;&#24066;&#22330;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#22312;&#24179;&#34913;&#24066;&#22330;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#24066;&#22330;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.06714</link><description>&lt;p&gt;
&#29233;&#23572;&#20848;&#24179;&#34913;&#24066;&#22330;&#30340;&#30005;&#20215;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Electricity Price Forecasting in the Irish Balancing Market
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29233;&#23572;&#20848;&#30340;&#24179;&#34913;&#24066;&#22330;&#24212;&#29992;&#20102;&#21508;&#31181;&#24050;&#34987;&#35777;&#26126;&#25104;&#21151;&#30340;&#20215;&#26684;&#39044;&#27979;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#22312;&#26085;&#21069;&#24066;&#22330;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#22312;&#24179;&#34913;&#24066;&#22330;&#20013;&#25928;&#26524;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#24066;&#22330;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21487;&#20877;&#29983;&#33021;&#28304;&#26469;&#28304;&#36234;&#26469;&#36234;&#19981;&#21487;&#39044;&#27979;&#65292;&#30701;&#26399;&#30005;&#21147;&#24066;&#22330;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#24341;&#36215;&#20102;&#34892;&#19994;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#24179;&#34913;&#24066;&#22330;&#26159;&#26368;&#25509;&#36817;&#23454;&#26102;&#24182;&#19988;&#26368;&#19981;&#31283;&#23450;&#30340;&#24066;&#22330;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20854;&#20215;&#26684;&#39044;&#27979;&#30340;&#25991;&#29486;&#26377;&#38480;&#12289;&#19981;&#19968;&#33268;&#19988;&#36807;&#26102;&#65292;&#38024;&#23545;&#35813;&#24066;&#22330;&#30340;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#23581;&#35797;&#24456;&#23569;&#65292;&#20063;&#27809;&#26377;&#20844;&#24320;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#26412;&#30740;&#31350;&#23558;&#22312;&#24191;&#27867;&#30740;&#31350;&#30340;&#26085;&#21069;&#24066;&#22330;&#20013;&#25104;&#21151;&#24212;&#29992;&#30340;&#21508;&#31181;&#20215;&#26684;&#39044;&#27979;&#25216;&#26415;&#24212;&#29992;&#20110;&#29233;&#23572;&#20848;&#30340;&#24179;&#34913;&#24066;&#22330;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26694;&#26550;&#27604;&#36739;&#20102;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#35757;&#32451;&#22823;&#23567;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#35813;&#26694;&#26550;&#23450;&#20041;&#20102;&#36229;&#21442;&#25968;&#21644;&#26657;&#20934;&#35774;&#32622;&#65292;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#20844;&#24320;&#20197;&#30830;&#20445;&#21487;&#37325;&#29616;&#24615;&#65292;&#24182;&#20316;&#20026;&#26410;&#26469;&#24037;&#20316;&#30340;&#22522;&#20934;&#12290;&#22823;&#37327;&#30340;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#26085;&#21069;&#24066;&#22330;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#27169;&#22411;&#22312;&#24179;&#34913;&#24066;&#22330;&#19978;&#25928;&#26524;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#24066;&#22330;&#30340;&#19981;&#21516;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Short-term electricity markets are becoming more relevant due to less-predictable renewable energy sources, attracting considerable attention from the industry. The balancing market is the closest to real-time and the most volatile among them. Its price forecasting literature is limited, inconsistent and outdated, with few deep learning attempts and no public dataset. This work applies to the Irish balancing market a variety of price prediction techniques proven successful in the widely studied day-ahead market. We compare statistical, machine learning, and deep learning models using a framework that investigates the impact of different training sizes. The framework defines hyperparameters and calibration settings; the dataset and models are made public to ensure reproducibility and to be used as benchmarks for future works. An extensive numerical study shows that well-performing models in the day-ahead market do not perform well in the balancing one, highlighting that these markets ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#22810;&#31867;&#23454;&#26102;&#23849;&#28291;&#39118;&#38505;&#39044;&#27979;&#20013;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#20132;&#36890;&#21644;&#22825;&#27668;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#20010;&#26102;&#38388;&#27573;&#30340;&#23454;&#26102;&#23849;&#28291;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.06707</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22810;&#31867;&#23454;&#26102;&#23849;&#28291;&#39118;&#38505;&#39044;&#27979;&#65306;&#20234;&#26031;&#22374;&#24067;&#23572;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-class real-time crash risk forecasting using convolutional neural network: Istanbul case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#22810;&#31867;&#23454;&#26102;&#23849;&#28291;&#39118;&#38505;&#39044;&#27979;&#20013;&#65292;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20174;&#20132;&#36890;&#21644;&#22825;&#27668;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#29305;&#24449;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19977;&#20010;&#26102;&#38388;&#27573;&#30340;&#23454;&#26102;&#23849;&#28291;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#22312;&#23849;&#28291;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#33719;&#21462;&#20102;&#19968;&#20123;&#20132;&#36890;&#21644;&#22825;&#27668;&#25968;&#25454;&#20316;&#20026;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#21518;&#23545;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#26681;&#25454;&#38468;&#21152;&#26641;&#21644;&#30382;&#23572;&#36874;&#30456;&#20851;&#24615;&#36873;&#25321;&#30456;&#20851;&#29305;&#24449;&#20316;&#20026;&#36755;&#20837;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23558;&#23849;&#28291;&#21644;&#38750;&#23849;&#28291;&#26102;&#38388;&#25968;&#25454;&#20998;&#31163;&#65307;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#26102;&#38388;&#27573;&#20869;&#25152;&#26377;&#21487;&#29992;&#20540;&#30340;&#24179;&#22343;&#20540;&#23558;&#23849;&#28291;&#21644;&#38750;&#23849;&#28291;&#20107;&#20214;&#30340;&#29305;&#24449;&#20540;&#20889;&#20837;&#23849;&#28291;&#21644;&#38750;&#23849;&#28291;&#20107;&#20214;&#20043;&#21069;&#30340;&#19977;&#20010;&#22235;&#20998;&#38047;&#38388;&#38548;&#12290;&#22312;&#26681;&#25454;&#20107;&#25925;&#26631;&#35760;&#35745;&#31639;&#27599;&#20010;&#26102;&#38388;&#27573;&#30340;&#23849;&#28291;&#21487;&#33021;&#24615;&#21518;&#65292;&#38477;&#20302;&#20102;&#38750;&#23849;&#28291;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;CNN&#27169;&#22411;&#33021;&#22815;&#20174;&#35760;&#24405;&#30340;&#12289;&#32463;&#36807;&#22788;&#29702;&#21644;&#20998;&#31867;&#30340;&#36755;&#20837;&#29305;&#24449;&#65288;&#22914;&#20132;&#36890;&#29305;&#24449;&#21644;&#27668;&#35937;&#26465;&#20214;&#65289;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#22522;&#20110;&#20107;&#20214;&#21069;&#19977;&#20010;&#26102;&#38388;&#27573;&#39044;&#27979;&#23454;&#26102;&#23849;&#28291;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of an artificial neural network (ANN) in forecasting crash risk is shown in this paper. To begin, some traffic and weather data are acquired as raw data. This data is then analyzed, and relevant characteristics are chosen to utilize as input data based on additional tree and Pearson correlation. Furthermore, crash and non-crash time data are separated; then, feature values for crash and non-crash events are written in three four-minute intervals prior to the crash and non-crash events using the average of all available values for that period. The number of non-crash samples was lowered after calculating crash likelihood for each period based on accident labeling. The proposed CNN model is capable of learning from recorded, processed, and categorized input characteristics such as traffic characteristics and meteorological conditions. The goal of this work is to forecast the chance of a real-time crash based on three periods before events. The area under the curve (AUC) f
&lt;/p&gt;</description></item><item><title>CoRe-GD&#26159;&#19968;&#20010;&#20351;&#29992;GNNs&#30340;&#21487;&#25193;&#23637;&#22270;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#20013;&#38388;&#33410;&#28857;&#20301;&#32622;&#30340;&#20301;&#32622;&#35843;&#25972;&#25216;&#26415;&#26469;&#20248;&#21270;&#22270;&#30340;&#24067;&#23616;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06706</link><description>&lt;p&gt;
CoRe-GD: &#19968;&#31181;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#21487;&#35270;&#21270;&#30340;&#23618;&#27425;&#21270;&#26694;&#26550;&#19982;GNNs
&lt;/p&gt;
&lt;p&gt;
CoRe-GD: A Hierarchical Framework for Scalable Graph Visualization with GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06706
&lt;/p&gt;
&lt;p&gt;
CoRe-GD&#26159;&#19968;&#20010;&#20351;&#29992;GNNs&#30340;&#21487;&#25193;&#23637;&#22270;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#20013;&#38388;&#33410;&#28857;&#20301;&#32622;&#30340;&#20301;&#32622;&#35843;&#25972;&#25216;&#26415;&#26469;&#20248;&#21270;&#22270;&#30340;&#24067;&#23616;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21487;&#35270;&#21270;&#65292;&#20063;&#34987;&#31216;&#20026;&#22270;&#32472;&#21046;&#65292;&#26088;&#22312;&#25214;&#21040;&#20248;&#21270;&#26576;&#20123;&#26631;&#20934;&#30340;&#22270;&#24418;&#23884;&#20837;&#12290;&#24212;&#21147;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#65307;&#24403;&#27599;&#23545;&#33410;&#28857;&#20301;&#32622;&#22312;&#23427;&#20204;&#30340;&#26368;&#30701;&#36335;&#24452;&#36317;&#31163;&#26102;&#65292;&#24212;&#21147;&#34987;&#26368;&#23567;&#21270;&#12290;&#28982;&#32780;&#65292;&#24212;&#21147;&#20248;&#21270;&#20855;&#26377;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#24102;&#26469;&#35745;&#31639;&#25361;&#25112;&#65292;&#24182;&#19988;&#36890;&#24120;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#22270;&#24418;&#32472;&#21046;&#26694;&#26550;&#65292;&#20855;&#26377;&#27425;&#20108;&#27425;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#23398;&#20064;&#20248;&#21270;&#24212;&#21147;&#12290;&#21463;&#20256;&#32479;&#24212;&#21147;&#20248;&#21270;&#25216;&#26415;&#21644;&#21147;&#23548;&#21521;&#24067;&#23616;&#31639;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20026;&#36755;&#20837;&#22270;&#24418;&#21019;&#24314;&#20102;&#19968;&#20010;&#31895;&#21270;&#23618;&#27425;&#32467;&#26500;&#12290;&#20174;&#26368;&#31895;&#31961;&#30340;&#32423;&#21035;&#24320;&#22987;&#65292;&#25105;&#20204;&#36845;&#20195;&#22320;&#23436;&#21892;&#21644;&#35299;&#38500;&#31895;&#21270;&#24067;&#23616;&#65292;&#30452;&#21040;&#20026;&#21407;&#22987;&#22270;&#29983;&#25104;&#23884;&#20837;&#12290;&#20026;&#20102;&#22686;&#24378;&#32593;&#32476;&#20869;&#30340;&#20449;&#24687;&#20256;&#25773;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20013;&#38388;&#33410;&#28857;&#20301;&#32622;&#30340;&#26032;&#22411;&#20301;&#32622;&#35843;&#25972;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Graph Visualization, also known as Graph Drawing, aims to find geometric embeddings of graphs that optimize certain criteria. Stress is a widely used metric; stress is minimized when every pair of nodes is positioned at their shortest path distance. However, stress optimization presents computational challenges due to its inherent complexity and is usually solved using heuristics in practice. We introduce a scalable Graph Neural Network (GNN) based Graph Drawing framework with sub-quadratic runtime that can learn to optimize stress. Inspired by classical stress optimization techniques and force-directed layout algorithms, we create a coarsening hierarchy for the input graph. Beginning at the coarsest level, we iteratively refine and un-coarsen the layout, until we generate an embedding for the original graph. To enhance information propagation within the network, we propose a novel positional rewiring technique based on intermediate node positions. Our empirical evaluation demonstrates
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#22522;&#30784;&#31639;&#27861;&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#26469;&#30028;&#23450;ReportNoisyMax&#21644;PrivateTuning&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.06701</link><description>&lt;p&gt;
&#38024;&#23545;&#31169;&#26377;&#36873;&#25321;&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;
&lt;/p&gt;
&lt;p&gt;
Privacy Profiles for Private Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#22522;&#30784;&#31639;&#27861;&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#26469;&#30028;&#23450;ReportNoisyMax&#21644;PrivateTuning&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;&#36873;&#25321;&#26426;&#21046;&#26159;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#30340;&#22522;&#26412;&#21407;&#35821;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#31169;&#26377;&#26597;&#35810;&#21457;&#24067;&#12289;&#25237;&#31080;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#31561;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#27867;&#21270;&#31169;&#26377;&#36873;&#25321;&#26426;&#21046;&#21644;&#20351;&#29992;&#29616;&#20195;&#25968;&#20540;&#38544;&#31169;&#36134;&#21153;&#24037;&#20855;&#65288;&#22914;R\'enyi&#24046;&#20998;&#38544;&#31169;&#65289;&#25910;&#25947;&#38544;&#31169;&#20998;&#26512;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38480;&#21046;&#22522;&#30784;&#31639;&#27861;&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#26469;&#30028;&#23450;ReportNoisyMax&#21644;PrivateTuning&#30340;&#38544;&#31169;&#37197;&#32622;&#25991;&#20214;&#12290;&#22312;&#25968;&#20540;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#37117;&#20248;&#20110;&#22522;&#20110;RDP&#30340;&#38544;&#31169;&#36134;&#21153;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Private selection mechanisms (e.g., Report Noisy Max, Sparse Vector) are fundamental primitives of differentially private (DP) data analysis with wide applications to private query release, voting, and hyperparameter tuning. Recent work (Liu and Talwar, 2019; Papernot and Steinke, 2022) has made significant progress in both generalizing private selection mechanisms and tightening their privacy analysis using modern numerical privacy accounting tools, e.g., R\'enyi DP. But R\'enyi DP is known to be lossy when $(\epsilon,\delta)$-DP is ultimately needed, and there is a trend to close the gap by directly handling privacy profiles, i.e., $\delta$ as a function of $\epsilon$ or its equivalent dual form known as $f$-DPs. In this paper, we work out an easy-to-use recipe that bounds the privacy profiles of ReportNoisyMax and PrivateTuning using the privacy profiles of the base algorithms they corral. Numerically, our approach improves over the RDP-based accounting in all regimes of interest an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06700</link><description>&lt;p&gt;
&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Entropy-Regularized Token-Level Policy Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06700
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65288;ETPO&#65289;&#65292;&#29992;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#35299;&#20915;&#22312;&#22914;&#20309;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#21644;&#26368;&#22823;&#21270;&#22870;&#21169;&#20043;&#38388;&#30340;&#20914;&#31361;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26234;&#33021;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#12289;&#39640;&#36136;&#37327;&#30340;&#31034;&#20363;&#25110;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#25110;RLHF&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20351;LLMs&#33021;&#22815;&#36890;&#36807;&#30452;&#25509;&#19982;&#20219;&#21153;&#29305;&#23450;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#26469;&#20811;&#26381;&#36825;&#20123;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#23427;&#38754;&#20020;&#30528;&#37325;&#37325;&#22256;&#38590;&#65306;1&#65289;&#30001;&#20110;&#24040;&#22823;&#30340;&#21160;&#20316;&#31354;&#38388;&#38656;&#35201;&#25506;&#32034;&#32780;&#20135;&#29983;&#30340;&#19981;&#31283;&#23450;&#24615;&#65307;2&#65289;&#22522;&#20110;&#21160;&#20316;&#32423;&#22870;&#21169;&#20449;&#21495;&#20998;&#37197;&#20196;&#29260;&#32423;&#23398;&#20998;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#26368;&#22823;&#21270;&#22870;&#21169;&#21644;&#20934;&#30830;&#24314;&#27169;&#35821;&#26009;&#24211;&#25968;&#25454;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29109;&#27491;&#21017;&#21270;&#30340;&#20196;&#29260;&#32423;&#31574;&#30053;&#20248;&#21270;&#65288;ETPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22312;&#20196;&#29260;&#32423;&#20248;&#21270;LLMs&#32780;&#35774;&#35745;&#30340;&#29109;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;ETPO&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#19968;&#31181;&#26032;&#39062;&#30340;&#36880;&#20196;&#29260;&#36719;Bellman&#26356;&#26032;&#31639;&#27861;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promise as intelligent agents in interactive decision-making tasks. Traditional approaches often depend on meticulously designed prompts, high-quality examples, or additional reward models for in-context learning, supervised fine-tuning, or RLHF. Reinforcement learning (RL) presents a dynamic alternative for LLMs to overcome these dependencies by engaging directly with task-specific environments. Nonetheless, it faces significant hurdles: 1) instability stemming from the exponentially vast action space requiring exploration; 2) challenges in assigning token-level credit based on action-level reward signals, resulting in discord between maximizing rewards and accurately modeling corpus data. In response to these challenges, we introduce Entropy-Regularized Token-level Policy Optimization (ETPO), an entropy-augmented RL method tailored for optimizing LLMs at the token level. At the heart of ETPO is our novel per-token soft Bellman update, designed 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#35757;&#32451;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;(ReLU)&#31070;&#32463;&#20803;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;MIP&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;MIP&#25216;&#26415;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;DNN&#21644;&#20108;&#20540;&#21270;DNN&#12290;</title><link>https://arxiv.org/abs/2402.06697</link><description>&lt;p&gt;
&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Feed-Forward Neural Networks as a Mixed-Integer Program
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06697
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#35757;&#32451;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;(ReLU)&#31070;&#32463;&#20803;&#20316;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;MIP&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#30740;&#31350;&#21457;&#29616;MIP&#25216;&#26415;&#22312;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;DNN&#21644;&#20108;&#20540;&#21270;DNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#37117;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;DNN&#30001;&#31070;&#32463;&#20803;&#23618;&#32452;&#25104;&#65292;&#35745;&#31639;&#20223;&#23556;&#32452;&#21512;&#65292;&#24212;&#29992;&#38750;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#20135;&#29983;&#30456;&#24212;&#30340;&#28608;&#27963;&#12290;&#20462;&#27491;&#30340;&#32447;&#24615;&#21333;&#20803;(ReLU)&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#38750;&#32447;&#24615;&#36816;&#31639;&#31526;&#65292;&#36755;&#20986;&#20854;&#36755;&#20837;&#21644;&#38646;&#30340;&#26368;&#22823;&#20540;&#12290;&#22312;&#20687;&#26368;&#22823;&#27744;&#21270;&#36825;&#26679;&#28041;&#21450;&#22810;&#20010;&#36755;&#20837;&#20540;&#30340;&#22330;&#26223;&#20013;&#65292;&#22266;&#23450;&#21442;&#25968;&#30340;DNN&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;(MIP)&#12290;&#36825;&#31181;&#24418;&#24335;&#65292;&#20351;&#29992;&#36830;&#32493;&#21464;&#37327;&#34920;&#31034;&#21333;&#20803;&#36755;&#20986;&#21644;ReLU&#28608;&#27963;&#30340;&#20108;&#36827;&#21046;&#21464;&#37327;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25214;&#21040;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#30340;ReLU&#31070;&#32463;&#20803;&#20316;&#20026;MIP&#30340;&#24418;&#24335;&#65292;&#24182;&#23558;MIP&#27169;&#22411;&#24212;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;(NN)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#30740;&#31350;&#20102;MIP&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;NN&#26550;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;DNN(&#37319;&#29992;&#38454;&#26799;&#28608;&#27963;&#20989;&#25968;)&#21644;&#20108;&#20540;&#21270;DNN(&#26435;&#37325;&#21644;&#28608;&#27963;&#38480;&#21046;&#20026;$-1,0,+1$)&#12290;&#35813;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#35757;&#32451;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are widely studied in various applications. A DNN consists of layers of neurons that compute affine combinations, apply nonlinear operations, and produce corresponding activations. The rectified linear unit (ReLU) is a typical nonlinear operator, outputting the max of its input and zero. In scenarios like max pooling, where multiple input values are involved, a fixed-parameter DNN can be modeled as a mixed-integer program (MIP). This formulation, with continuous variables representing unit outputs and binary variables for ReLU activation, finds applications across diverse domains. This study explores the formulation of trained ReLU neurons as MIP and applies MIP models for training neural networks (NNs). Specifically, it investigates interactions between MIP techniques and various NN architectures, including binary DNNs (employing step activation functions) and binarized DNNs (with weights and activations limited to $-1,0,+1$). The research focuses on traini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FL-NAS&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#30828;&#20214;&#37096;&#32626;&#25928;&#29575;&#19977;&#20010;&#26041;&#38754;&#36798;&#21040;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06696</link><description>&lt;p&gt;
FL-NAS: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#23454;&#29616;&#20844;&#24179;&#30340;NAS
&lt;/p&gt;
&lt;p&gt;
FL-NAS: Towards Fairness of NAS for Resource Constrained Devices via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FL-NAS&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#30828;&#20214;&#37096;&#32626;&#25928;&#29575;&#19977;&#20010;&#26041;&#38754;&#36798;&#21040;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#24050;&#25104;&#20026;&#24037;&#19994;&#30028;&#33258;&#21160;&#35774;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#31227;&#21160;&#21644;&#36793;&#32536;&#35774;&#22791;&#39537;&#21160;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20063;&#34987;&#32435;&#20837;NAS&#65292;&#24182;&#26174;&#31034;&#20986;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#27169;&#22411;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#21644;&#30828;&#20214;&#37096;&#32626;&#25928;&#29575;&#19977;&#20010;&#37325;&#35201;&#30340;&#35774;&#35745;&#25351;&#26631;&#65292;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#36825;&#20010;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;NAS&#26694;&#26550;FL-NAS&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FL-NAS&#30830;&#23454;&#33021;&#22815;&#25214;&#21040;&#24615;&#33021;&#20248;&#31168;&#30340;DNN&#27169;&#22411;&#65292;&#20960;&#20046;&#22312;&#25152;&#26377;&#35774;&#35745;&#32771;&#34385;&#26041;&#38754;&#37117;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;DNN&#27169;&#22411;&#26377;&#30528;&#25968;&#37327;&#32423;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Architecture Search (NAS) has become the de fecto tools in the industry in automating the design of deep neural networks for various applications, especially those driven by mobile and edge devices with limited computing resources. The emerging large language models (LLMs), due to their prowess, have also been incorporated into NAS recently and show some promising results. This paper conducts further exploration in this direction by considering three important design metrics simultaneously, i.e., model accuracy, fairness, and hardware deployment efficiency. We propose a novel LLM-based NAS framework, FL-NAS, in this paper, and show experimentally that FL-NAS can indeed find high-performing DNNs, beating state-of-the-art DNN models by orders-of-magnitude across almost all design considerations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#35786;&#26029;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#22797;&#26434;&#31995;&#32479;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#25925;&#38556;&#65292;&#36824;&#33021;&#22815;&#25552;&#20379;&#28165;&#26224;&#26131;&#25026;&#30340;&#25925;&#38556;&#21407;&#22240;&#21644;&#24433;&#21709;&#35299;&#37322;&#65292;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06695</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#31995;&#32479;&#20013;&#38598;&#25104;LLMs&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Integrating LLMs for Explainable Fault Diagnosis in Complex Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#38598;&#25104;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#35786;&#26029;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24110;&#21161;&#22797;&#26434;&#31995;&#32479;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#25925;&#38556;&#65292;&#36824;&#33021;&#22815;&#25552;&#20379;&#28165;&#26224;&#26131;&#25026;&#30340;&#25925;&#38556;&#21407;&#22240;&#21644;&#24433;&#21709;&#35299;&#37322;&#65292;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#26088;&#22312;&#22686;&#24378;&#22797;&#26434;&#31995;&#32479;&#20013;&#25925;&#38556;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22914;&#26680;&#30005;&#31449;&#65292;&#22312;&#36825;&#20123;&#31995;&#32479;&#20013;&#65292;&#25805;&#20316;&#21592;&#30340;&#29702;&#35299;&#23545;&#20110;&#26126;&#26234;&#30340;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#29289;&#29702;&#30340;&#35786;&#26029;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#25925;&#38556;&#65292;&#36824;&#33021;&#28165;&#26224;&#26131;&#25026;&#22320;&#35299;&#37322;&#20854;&#21407;&#22240;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#24212;&#29992;&#20110;&#29076;&#30416;&#35774;&#26045;&#65292;&#23637;&#31034;&#20102;&#35813;&#31995;&#32479;&#25581;&#31034;&#20102;&#35786;&#26029;&#25925;&#38556;&#19982;&#20256;&#24863;&#22120;&#25968;&#25454;&#20043;&#38388;&#30340;&#32852;&#31995;&#12289;&#22238;&#31572;&#25805;&#20316;&#21592;&#30340;&#26597;&#35810;&#20197;&#21450;&#35780;&#20272;&#21382;&#21490;&#20256;&#24863;&#22120;&#24322;&#24120;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24378;&#35843;&#20102;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#35786;&#26029;&#19982;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#36879;&#26126;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an integrated system designed to enhance the explainability of fault diagnostics in complex systems, such as nuclear power plants, where operator understanding is critical for informed decision-making. By combining a physics-based diagnostic tool with a Large Language Model, we offer a novel solution that not only identifies faults but also provides clear, understandable explanations of their causes and implications. The system's efficacy is demonstrated through application to a molten salt facility, showcasing its ability to elucidate the connections between diagnosed faults and sensor data, answer operator queries, and evaluate historical sensor anomalies. Our approach underscores the importance of merging model-based diagnostics with advanced AI to improve the reliability and transparency of autonomous systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#25552;&#21319;&#25112;&#20105;&#28216;&#25103;&#20013;&#26234;&#33021;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#20197;&#21152;&#36895;&#20915;&#31574;&#36895;&#24230;&#21644;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.06694</link><description>&lt;p&gt;
&#25193;&#23637;&#25112;&#20105;&#28216;&#25103;&#20013;&#30340;&#26234;&#33021;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Scaling Intelligent Agents in Combat Simulations for Wargaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#25552;&#21319;&#25112;&#20105;&#28216;&#25103;&#20013;&#26234;&#33021;&#20195;&#29702;&#30340;&#24615;&#33021;&#65292;&#20197;&#21152;&#36895;&#20915;&#31574;&#36895;&#24230;&#21644;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#26410;&#26469;&#19982;&#25216;&#26415;&#20808;&#36827;&#30340;&#31454;&#20105;&#23545;&#25163;&#30340;&#20914;&#31361;&#20013;&#20445;&#25345;&#31454;&#20105;&#21147;&#65292;&#25105;&#20204;&#38656;&#35201;&#21152;&#36895;&#30740;&#31350;&#21644;&#24320;&#21457;&#25112;&#20105;&#28216;&#25103;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#24320;&#21457;&#26234;&#33021;&#25112;&#26007;&#34892;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#23454;&#29616;&#36229;&#20154;&#31867;&#27700;&#24179;&#34920;&#29616;&#30340;&#20851;&#38190;&#65292;&#25552;&#21319;&#25105;&#20204;&#22312;&#26410;&#26469;&#25112;&#20105;&#20013;&#30340;&#20915;&#31574;&#36136;&#37327;&#21644;&#21152;&#36895;&#36895;&#24230;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#28216;&#25103;&#20013;&#26234;&#33021;&#20195;&#29702;&#34892;&#20026;&#24320;&#21457;&#26041;&#38754;&#32487;&#32493;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#38271;&#26399;&#12289;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#25112;&#26007;&#24314;&#27169;&#21644;&#20223;&#30495;&#20013;&#65292;&#23427;&#23578;&#26410;&#36798;&#21040;&#25110;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#12290;&#20511;&#37492;RL&#30340;&#24050;&#35777;&#26126;&#28508;&#21147;&#21644;&#26368;&#36817;&#20998;&#23618;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#30340;&#25104;&#21151;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#24182;&#25193;&#23637;HRL&#30340;&#20351;&#29992;&#65292;&#20197;&#21019;&#24314;&#33021;&#22815;&#22312;&#36825;&#20123;&#22823;&#35268;&#27169;&#22797;&#26434;&#27169;&#25311;&#29615;&#22659;&#20013;&#26377;&#25928;&#25191;&#34892;&#30340;&#26234;&#33021;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#30446;&#26631;&#26159;
&lt;/p&gt;
&lt;p&gt;
Remaining competitive in future conflicts with technologically-advanced competitors requires us to accelerate our research and development in artificial intelligence (AI) for wargaming. More importantly, leveraging machine learning for intelligent combat behavior development will be key to one day achieving superhuman performance in this domain--elevating the quality and accelerating the speed of our decisions in future wars. Although deep reinforcement learning (RL) continues to show promising results in intelligent agent behavior development in games, it has yet to perform at or above the human level in the long-horizon, complex tasks typically found in combat modeling and simulation. Capitalizing on the proven potential of RL and recent successes of hierarchical reinforcement learning (HRL), our research is investigating and extending the use of HRL to create intelligent agents capable of performing effectively in these large and complex simulation environments. Our ultimate goal is
&lt;/p&gt;</description></item><item><title>HistoHDR-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#20010;LDR&#21040;HDR&#22270;&#20687;&#36716;&#25442;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#30452;&#26041;&#22270;&#22343;&#34913;&#21270;&#30340;LDR&#22270;&#20687;&#21644;&#33258;&#27880;&#24847;&#21147;&#24341;&#23548;&#65292;&#24674;&#22797;HDR&#22270;&#20687;&#30340;&#32454;&#33410;&#12290;</title><link>https://arxiv.org/abs/2402.06692</link><description>&lt;p&gt;
HistoHDR-Net&#65306;&#29992;&#20110;&#21333;&#20010;LDR&#21040;HDR&#22270;&#20687;&#36716;&#25442;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21270;
&lt;/p&gt;
&lt;p&gt;
HistoHDR-Net: Histogram Equalization for Single LDR to HDR Image Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06692
&lt;/p&gt;
&lt;p&gt;
HistoHDR-Net&#26159;&#19968;&#31181;&#29992;&#20110;&#21333;&#20010;LDR&#21040;HDR&#22270;&#20687;&#36716;&#25442;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#34701;&#21512;&#30452;&#26041;&#22270;&#22343;&#34913;&#21270;&#30340;LDR&#22270;&#20687;&#21644;&#33258;&#27880;&#24847;&#21147;&#24341;&#23548;&#65292;&#24674;&#22797;HDR&#22270;&#20687;&#30340;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#21160;&#24577;&#33539;&#22260;&#65288;HDR&#65289;&#25104;&#20687;&#26088;&#22312;&#22797;&#21046;&#30495;&#23454;&#22330;&#26223;&#30340;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#28165;&#26224;&#24230;&#12290;&#30001;&#20110;HDR&#25104;&#20687;&#30340;&#39640;&#25104;&#26412;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#29992;&#20110;&#20174;&#20302;&#21160;&#24577;&#33539;&#22260;&#65288;LDR&#65289;&#22270;&#20687;&#37325;&#26500;HDR&#22270;&#20687;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38480;&#21046;&#26159;&#22312;&#37325;&#26500;&#30340;HDR&#22270;&#20687;&#20013;&#32570;&#22833;&#30340;&#32454;&#33410;&#65292;&#36825;&#20123;&#32454;&#33410;&#22312;&#36755;&#20837;&#30340;LDR&#22270;&#20687;&#20013;&#36807;&#26333;&#25110;&#26333;&#20809;&#19981;&#36275;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;HistoHDR-Net&#65292;&#36890;&#36807;&#34701;&#21512;&#22522;&#20110;&#30452;&#26041;&#22270;&#22343;&#34913;&#30340;LDR&#22270;&#20687;&#21644;&#33258;&#27880;&#24847;&#21147;&#24341;&#23548;&#65292;&#24674;&#22797;HDR&#22270;&#20687;&#30340;&#32454;&#33410;&#65288;&#20363;&#22914;&#39068;&#33394;&#65292;&#23545;&#27604;&#24230;&#65292;&#39281;&#21644;&#24230;&#21644;&#20142;&#24230;&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#29616;&#26377;&#26041;&#27861;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
High Dynamic Range (HDR) imaging aims to replicate the high visual quality and clarity of real-world scenes. Due to the high costs associated with HDR imaging, the literature offers various data-driven methods for HDR image reconstruction from Low Dynamic Range (LDR) counterparts. A common limitation of these approaches is missing details in regions of the reconstructed HDR images, which are over- or under-exposed in the input LDR images. To this end, we propose a simple and effective method, HistoHDR-Net, to recover the fine details (e.g., color, contrast, saturation, and brightness) of HDR images via a fusion-based approach utilizing histogram-equalized LDR images along with self-attention guidance. Our experiments demonstrate the efficacy of the proposed approach over the state-of-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28304;&#20195;&#30721;&#24314;&#35758;&#21644;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#24320;&#21457;&#32773;&#30340;&#32534;&#30721;&#24847;&#22270;&#24182;&#20934;&#30830;&#25512;&#26029;&#31867;&#22411;&#12289;&#21517;&#31216;&#21644;&#19978;&#19979;&#25991;&#31561;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.06690</link><description>&lt;p&gt;
&#28304;&#20195;&#30721;&#21512;&#25104;&#21644;&#34917;&#20840;&#30340;&#31070;&#32463;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Models for Source Code Synthesis and Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;&#24182;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28304;&#20195;&#30721;&#24314;&#35758;&#21644;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#12290;&#36825;&#19968;&#26041;&#27861;&#33021;&#22815;&#25552;&#21462;&#24320;&#21457;&#32773;&#30340;&#32534;&#30721;&#24847;&#22270;&#24182;&#20934;&#30830;&#25512;&#26029;&#31867;&#22411;&#12289;&#21517;&#31216;&#21644;&#19978;&#19979;&#25991;&#31561;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21040;&#20195;&#30721;&#24314;&#35758;&#31995;&#32479;&#36890;&#36807;&#23558;NL&#34920;&#36798;&#36716;&#21270;&#20026;&#21487;&#32534;&#35793;&#30340;&#20195;&#30721;&#29255;&#27573;&#26469;&#24110;&#21161;&#38598;&#25104;&#24320;&#21457;&#29615;&#22659;&#65288;IDE&#65289;&#20013;&#30340;&#24320;&#21457;&#20154;&#21592;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#30828;&#32534;&#30721;&#12289;&#35268;&#21017;&#31995;&#32479;&#12290;&#36825;&#20123;&#31995;&#32479;&#20027;&#35201;&#20381;&#38752;&#25163;&#24037;&#21046;&#23450;&#30340;&#35268;&#21017;&#23558;NL&#30340;&#27169;&#24335;&#25110;&#20854;&#35821;&#27861;&#35299;&#26512;&#26641;&#20013;&#30340;&#20803;&#32032;&#26144;&#23556;&#21040;&#21508;&#31181;&#26597;&#35810;&#32467;&#26500;&#65292;&#24182;&#19988;&#21482;&#33021;&#22788;&#29702;&#21463;&#38480;&#21046;&#30340;NL&#23376;&#38598;&#21644;&#38480;&#21046;&#30340;NL&#35821;&#27861;&#12290;&#36825;&#20123;&#31995;&#32479;&#26080;&#27861;&#20174;&#24320;&#21457;&#32773;&#30340;&#32534;&#30721;&#24847;&#22270;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24120;&#24120;&#26080;&#27861;&#25512;&#26029;&#31867;&#22411;&#12289;&#21517;&#31216;&#21644;&#28304;&#20195;&#30721;&#30340;&#19978;&#19979;&#25991;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#31995;&#32479;&#32423;&#20195;&#30721;&#24314;&#35758;&#12290;&#22312;&#26412;&#30805;&#22763;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#23558;NL&#26144;&#23556;&#21040;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#21487;&#20197;&#26681;&#25454;NL&#30340;&#24847;&#22270;&#20026;&#29992;&#25143;&#25552;&#20379;&#28304;&#20195;&#30721;&#29255;&#27573;&#30340;&#24314;&#35758;&#65292;&#24182;&#25193;&#23637;&#28304;&#20195;&#30721;&#30340;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) to code suggestion systems assist developers in Integrated Development Environments (IDEs) by translating NL utterances into compilable code snippet. The current approaches mainly involve hard-coded, rule-based systems based on semantic parsing. These systems make heavy use of hand-crafted rules that map patterns in NL or elements in its syntax parse tree to various query constructs and can only work on a limited subset of NL with a restricted NL syntax. These systems are unable to extract semantic information from the coding intents of the developer, and often fail to infer types, names, and the context of the source code to get accurate system-level code suggestions. In this master thesis, we present sequence-to-sequence deep learning models and training paradigms to map NL to general-purpose programming languages that can assist users with suggestions of source code snippets, given a NL intent, and also extend auto-completion functionality of the source code to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#39044;&#27979;&#65292;&#36890;&#36807;&#22238;&#39038;&#22810;&#20010;&#31639;&#27861;&#24182;&#20351;&#29992;s&amp;p 500&#25351;&#25968;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20339;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06689</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Study on Stock Forecasting Using Deep Learning and Statistical Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#32929;&#31080;&#39044;&#27979;&#65292;&#36890;&#36807;&#22238;&#39038;&#22810;&#20010;&#31639;&#27861;&#24182;&#20351;&#29992;s&amp;p 500&#25351;&#25968;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20339;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#30340;&#24555;&#36895;&#20934;&#30830;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#25214;&#21040;&#26368;&#20339;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#20998;&#26512;&#25216;&#26415;&#26469;&#33719;&#21462;&#20934;&#30830;&#30340;&#32467;&#26524;&#65292;&#20197;&#20415;&#25237;&#36164;&#32773;&#21487;&#20197;&#30475;&#21040;&#26410;&#26469;&#30340;&#36235;&#21183;&#65292;&#24182;&#26368;&#22823;&#21270;&#32929;&#31080;&#20132;&#26131;&#30340;&#22238;&#25253;&#12290;&#26412;&#25991;&#23558;&#22238;&#39038;&#35768;&#22810;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;s&amp;p 500&#25351;&#25968;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26412;&#35843;&#26597;&#30340;&#30446;&#30340;&#26159;&#26816;&#26597;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#30340;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#25216;&#26415;&#65292;&#21253;&#25324;&#32479;&#35745;&#25216;&#26415;&#20013;&#30340;&#31227;&#21160;&#24179;&#22343;&#27861;&#12289;ARIMA&#27169;&#22411;&#65292;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;LSTM&#12289;RNN&#12289;CNN&#21644;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#23558;&#35752;&#35770;&#21508;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;&#33258;&#22238;&#24402;&#31215;&#20998;&#28369;&#21160;&#24179;&#22343;&#27169;&#22411;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting a fast and accurate model for stock price forecasting is been a challenging task and this is an active area of research where it is yet to be found which is the best way to forecast the stock price. Machine learning, deep learning and statistical analysis techniques are used here to get the accurate result so the investors can see the future trend and maximize the return of investment in stock trading. This paper will review many deep learning algorithms for stock price forecasting. We use a record of s&amp;p 500 index data for training and testing. The survey motive is to check various deep learning and statistical model techniques for stock price forecasting that are Moving Averages, ARIMA which are statistical techniques and LSTM, RNN, CNN, and FULL CNN which are deep learning models. It will discuss various models, including the Auto regression integration moving average model, the Recurrent neural network model, the long short-term model which is the type of RNN used for lo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#22312;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEM&#65289;&#26657;&#27491;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#25552;&#39640;DEM&#22402;&#30452;&#31934;&#24230;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06688</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#22312;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEM&#65289;&#26657;&#27491;&#20013;&#30340;&#27604;&#36739;&#65306;&#20013;&#26399;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Comparison of machine learning and statistical approaches for digital elevation model (DEM) correction: interim results
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#26041;&#27861;&#22312;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEM&#65289;&#26657;&#27491;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;&#25552;&#39640;DEM&#22402;&#30452;&#31934;&#24230;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#32416;&#27491;&#25968;&#23383;&#39640;&#31243;&#27169;&#22411;&#65288;DEM&#65289;&#20013;&#30340;&#39640;&#31243;&#20559;&#24046;&#65292;&#20363;&#22914;&#32447;&#24615;&#22238;&#24402;&#12290;&#22914;&#20170;&#65292;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#23545;&#21464;&#37327;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24050;&#34987;&#30740;&#31350;&#20154;&#21592;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#12290;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#26377;&#20960;&#39033;&#30740;&#31350;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#25110;&#32479;&#35745;&#26041;&#27861;&#26469;&#36827;&#34892;DEM&#26657;&#27491;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#20123;&#30740;&#31350;&#20013;&#27809;&#26377;&#19968;&#39033;&#27604;&#36739;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#24320;&#25918;&#33719;&#21462;&#30340;&#20840;&#29699;DEM&#25968;&#25454;&#12290;&#25105;&#20204;&#20197;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;GBDT&#65289;&#22312;DEM&#26657;&#27491;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#23545;&#27604;&#19977;&#31181;&#26368;&#36817;&#23454;&#26045;&#30340;&#26799;&#24230;&#25552;&#21319;&#20915;&#31574;&#26641;&#65288;XGBoost&#65292;LightGBM&#21644;CatBoost&#65289;&#21644;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#65288;MLR&#65289;&#30340;&#32467;&#26524;&#65292;&#20197;&#25552;&#39640;30&#31859;&#30340;Copernicus DEM&#30340;&#22402;&#30452;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several methods have been proposed for correcting the elevation bias in digital elevation models (DEMs) for example, linear regression. Nowadays, supervised machine learning enables the modelling of complex relationships between variables, and has been deployed by researchers in a variety of fields. In the existing literature, several studies have adopted either machine learning or statistical approaches in the task of DEM correction. However, to our knowledge, none of these studies have compared the performance of both approaches, especially with regard to open-access global DEMs. Our previous work has already shown the potential of machine learning approaches, specifically gradient boosted decision trees (GBDTs) for DEM correction. In this study, we share some results from the comparison of three recent implementations of gradient boosted decision trees (XGBoost, LightGBM and CatBoost), versus multiple linear regression (MLR) for enhancing the vertical accuracy of 30 m Copernicus and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#20803;&#21464;&#21387;&#22120;&#27169;&#22411;&#39044;&#27979;&#27431;&#27954;&#21644;&#21271;&#38750;&#22320;&#21306;FAPAR&#26102;&#24207;&#36712;&#36857;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#33539;&#22260;&#20869;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#23588;&#20854;&#24403;&#19982;&#22825;&#27668;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.06684</link><description>&lt;p&gt;
Ai4Fapar&#65306;&#20154;&#24037;&#26234;&#33021;&#22914;&#20309;&#24110;&#21161;&#39044;&#27979;&#23395;&#33410;&#24615;&#30340;&#22320;&#29699;&#35266;&#27979;&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Ai4Fapar: How artificial intelligence can help to forecast the seasonal earth observation signal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22810;&#20803;&#21464;&#21387;&#22120;&#27169;&#22411;&#39044;&#27979;&#27431;&#27954;&#21644;&#21271;&#38750;&#22320;&#21306;FAPAR&#26102;&#24207;&#36712;&#36857;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#33539;&#22260;&#20869;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#23588;&#20854;&#24403;&#19982;&#22825;&#27668;&#25968;&#25454;&#32467;&#21512;&#20351;&#29992;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20803;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#27431;&#27954;&#21644;&#21271;&#38750;&#22320;&#21306;&#20197;&#21306;&#22495;&#32423;&#21035;&#39044;&#27979;&#20809;&#21512;&#26377;&#25928;&#36752;&#23556;&#21560;&#25910;&#29575;&#65288;FAPAR&#65289;&#30340;&#26102;&#38388;&#36712;&#36857;&#30340;&#28508;&#21147;&#12290;&#36755;&#20837;&#25968;&#25454;&#28085;&#30422;&#20102;2002&#24180;&#33267;2022&#24180;&#30340;&#26102;&#26399;&#65292;&#24182;&#21253;&#25324;&#36965;&#24863;&#21644;&#22825;&#27668;&#25968;&#25454;&#29992;&#20110;&#27169;&#25311;FAPAR&#39044;&#27979;&#12290;&#20351;&#29992;&#19968;&#24180;&#30041;&#19968;&#27425;&#20132;&#21449;&#39564;&#35777;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#27668;&#20505;&#22522;&#20934;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#19968;&#20010;&#26376;&#30340;&#39044;&#27979;&#33539;&#22260;&#20869;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20043;&#21518;&#27668;&#20505;&#22522;&#20934;&#26356;&#22909;&#12290;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;RMSE&#20540;&#22312;&#21069;&#20004;&#20010;&#26376;&#30340;&#39044;&#27979;&#20013;&#20026;0.02&#33267;0.04 FAPAR&#21333;&#20301;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#32463;&#36807;&#27979;&#35797;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;FAPAR&#39044;&#27979;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#32467;&#21512;&#22825;&#27668;&#25968;&#25454;&#24182;&#29992;&#20110;&#30701;&#26399;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigated the potential of a multivariate Transformer model to forecast the temporal trajectory of the Fraction of Absorbed Photosynthetically Active Radiation (FAPAR) for short (1 month) and long horizon (more than 1 month) periods at the regional level in Europe and North Africa. The input data covers the period from 2002 to 2022 and includes remote sensing and weather data for modelling FAPAR predictions. The model was evaluated using a leave one year out cross-validation and compared with the climatological benchmark. Results show that the transformer model outperforms the benchmark model for one month forecasting horizon, after which the climatological benchmark is better. The RMSE values of the transformer model ranged from 0.02 to 0.04 FAPAR units for the first 2 months of predictions. Overall, the tested Transformer model is a valid method for FAPAR forecasting, especially when combined with weather data and used for short-term predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32463;&#20856;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;/&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DSP / DNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22312;&#21512;&#36866;&#35774;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#22788;&#29702;&#20998;&#31163;&#20219;&#21153;&#65292;&#23558;&#21333;&#36890;&#36947;&#27424;&#23450;&#20998;&#31163;&#20219;&#21153;&#36716;&#25442;&#20026;&#22810;&#36890;&#36947;&#36807;&#23450;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06683</link><description>&lt;p&gt;
&#20351;&#29992;&#28508;&#22312;&#21464;&#20998;&#22359;&#20998;&#31163;&#30340;&#22768;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Sound Source Separation Using Latent Variational Block-Wise Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32463;&#20856;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;/&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DSP / DNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#22768;&#28304;&#20998;&#31163;&#12290;&#36890;&#36807;&#22312;&#21512;&#36866;&#35774;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#22788;&#29702;&#20998;&#31163;&#20219;&#21153;&#65292;&#23558;&#21333;&#36890;&#36947;&#27424;&#23450;&#20998;&#31163;&#20219;&#21153;&#36716;&#25442;&#20026;&#22810;&#36890;&#36947;&#36807;&#23450;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38754;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#26102;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#22312;&#35299;&#20915;&#32463;&#20856;&#20449;&#21495;&#22788;&#29702;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#21033;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#20004;&#32773;&#30340;&#27934;&#23519;&#21147;&#30340;&#28151;&#21512;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#32463;&#20856;&#25968;&#23383;&#20449;&#21495;&#22788;&#29702;/&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DSP / DNN&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#28304;&#20998;&#31163;&#65288;SS&#65289;&#65292;&#24182;&#24378;&#35843;&#20102;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#19982;&#20256;&#32479;SS&#26041;&#27861;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21333;&#36890;&#36947;&#27424;&#23450;SS&#20219;&#21153;&#36716;&#25442;&#20026;&#31561;&#25928;&#22810;&#36890;&#36947;&#36807;&#23450;SS&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#36866;&#24403;&#35774;&#35745;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#22788;&#29702;&#28151;&#21512;&#29289;&#30340;&#20998;&#31163;&#20219;&#21153;&#12290;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20998;&#31163;&#20219;&#21153;&#34987;&#35270;&#20026;&#23547;&#25214;&#28151;&#21512;&#29289;&#30340;&#21464;&#20998;&#22359;&#20998;&#31163;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#26681;&#25454;&#32463;&#20856;&#20449;&#21495;&#22788;&#29702;&#29702;&#35770;&#32467;&#26524;&#30340;&#35774;&#35745;&#36873;&#25321;&#21644;&#21464;&#20998;&#20844;&#24335;&#65292;&#21487;&#20197;&#33719;&#24471;&#23545;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While neural network approaches have made significant strides in resolving classical signal processing problems, it is often the case that hybrid approaches that draw insight from both signal processing and neural networks produce more complete solutions. In this paper, we present a hybrid classical digital signal processing/deep neural network (DSP/DNN) approach to source separation (SS) highlighting the theoretical link between variational autoencoder and classical approaches to SS. We propose a system that transforms the single channel under-determined SS task to an equivalent multichannel over-determined SS problem in a properly designed latent space. The separation task in the latent space is treated as finding a variational block-wise disentangled representation of the mixture. We show empirically, that the design choices and the variational formulation of the task at hand motivated by the classical signal processing theoretical results lead to robustness to unseen out-of-distrib
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#22312;&#39046;&#20808;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#30693;&#35782;&#32452;&#20214;&#65292;&#26088;&#22312;&#25581;&#31034;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.06682</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Private Knowledge Sharing in Distributed Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06682
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#22312;&#39046;&#20808;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#30693;&#35782;&#32452;&#20214;&#65292;&#26088;&#22312;&#25581;&#31034;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#27861;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#23835;&#36215;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#35768;&#22810;&#34892;&#19994;&#65292;&#24182;&#25913;&#21464;&#20102;&#31038;&#20250;&#30340;&#36816;&#20316;&#26041;&#24335;&#12290;&#20854;&#24191;&#27867;&#20351;&#29992;&#23548;&#33268;&#20102;AI&#21644;&#20854;&#24213;&#23618;&#25968;&#25454;&#22312;&#35768;&#22810;&#26234;&#33021;&#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#25110;&#30001;&#19981;&#21516;&#23454;&#20307;&#25317;&#26377;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#26381;&#21153;&#24050;&#32463;&#24320;&#21457;&#20986;&#23558;&#20998;&#24067;&#24335;&#30693;&#35782;&#23454;&#20307;&#25972;&#21512;&#21040;&#20854;&#32467;&#26524;&#20013;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#26368;&#26032;&#30340;AI&#27169;&#22411;&#32463;&#24120;&#34987;&#20197;&#20998;&#25955;&#24335;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#12290;&#20998;&#24067;&#24335;&#23398;&#20064;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#20849;&#21516;&#36827;&#34892;&#39044;&#27979;&#21644;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21512;&#20316;&#20063;&#21487;&#33021;&#24102;&#26469;&#23433;&#20840;&#28431;&#27934;&#21644;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#31169;&#20154;&#30693;&#35782;&#20849;&#20139;&#30340;&#28145;&#20837;&#35843;&#26597;&#65292;&#32771;&#23519;&#20102;&#22312;&#39046;&#20808;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#26550;&#26500;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#30693;&#35782;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#29616;&#26377;&#26041;&#27861;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Artificial Intelligence (AI) has revolutionized numerous industries and transformed the way society operates. Its widespread use has led to the distribution of AI and its underlying data across many intelligent systems. In this light, it is crucial to utilize information in learning processes that are either distributed or owned by different entities. As a result, modern data-driven services have been developed to integrate distributed knowledge entities into their outcomes. In line with this goal, the latest AI models are frequently trained in a decentralized manner. Distributed learning involves multiple entities working together to make collective predictions and decisions. However, this collaboration can also bring about security vulnerabilities and challenges. This paper provides an in-depth survey on private knowledge sharing in distributed learning, examining various knowledge components utilized in leading distributed learning architectures. Our analysis sheds light
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPDiff&#30340;&#22522;&#20110;&#31038;&#20250;&#29289;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20154;&#32676;&#27169;&#25311;&#12290;&#27169;&#22411;&#32508;&#21512;&#32771;&#34385;&#20102;&#20154;&#32676;&#30340;&#20114;&#21160;&#21644;&#21382;&#21490;&#20449;&#24687;&#65292;&#36890;&#36807;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#19979;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#34892;&#20154;&#36816;&#21160;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20511;&#37492;&#31038;&#20250;&#21147;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20154;&#32676;&#20114;&#21160;&#30340;&#31561;&#21464;&#24615;&#23646;&#24615;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#38271;&#26399;&#27169;&#25311;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22810;&#23618;&#27425;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.06680</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20250;&#29289;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#20154;&#32676;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Social Physics Informed Diffusion Model for Crowd Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SPDiff&#30340;&#22522;&#20110;&#31038;&#20250;&#29289;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#20154;&#32676;&#27169;&#25311;&#12290;&#27169;&#22411;&#32508;&#21512;&#32771;&#34385;&#20102;&#20154;&#32676;&#30340;&#20114;&#21160;&#21644;&#21382;&#21490;&#20449;&#24687;&#65292;&#36890;&#36807;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#19979;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#34892;&#20154;&#36816;&#21160;&#30340;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#20511;&#37492;&#31038;&#20250;&#21147;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#20154;&#32676;&#20114;&#21160;&#30340;&#31561;&#21464;&#24615;&#23646;&#24615;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#38271;&#26399;&#27169;&#25311;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22810;&#23618;&#27425;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#32676;&#27169;&#25311;&#22312;&#22478;&#24066;&#35268;&#21010;&#12289;&#24314;&#31569;&#35774;&#35745;&#21644;&#20132;&#36890;&#23433;&#25490;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#29289;&#29702;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20154;&#32676;&#27169;&#25311;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26410;&#33021;&#20840;&#38754;&#24314;&#27169;&#20154;&#31867;&#36816;&#21160;&#30340;&#24322;&#36136;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPDiff&#30340;&#31038;&#20250;&#29289;&#29702;&#21551;&#21457;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24357;&#34917;&#19978;&#36848;&#24046;&#36317;&#12290;SPDiff&#21516;&#26102;&#32771;&#34385;&#20102;&#24403;&#21069;&#26102;&#38388;&#27573;&#20869;&#20154;&#32676;&#30340;&#20114;&#21160;&#21644;&#21382;&#21490;&#20449;&#24687;&#65292;&#36890;&#36807;&#36870;&#21521;&#25193;&#25955;&#36807;&#31243;&#29983;&#25104;&#19979;&#19968;&#20010;&#26102;&#38388;&#27573;&#20869;&#34892;&#20154;&#36816;&#21160;&#30340;&#20998;&#24067;&#12290;&#21463;&#31038;&#20250;&#21147;&#27169;&#22411;&#65288;Social Force&#65289;&#20013;&#20154;&#32676;&#21160;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20154;&#32676;&#20114;&#21160;&#27169;&#22359;&#26469;&#25351;&#23548;&#21435;&#22122;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#20154;&#32676;&#20114;&#21160;&#30340;&#31561;&#21464;&#24615;&#23646;&#24615;&#36827;&#19968;&#27493;&#22686;&#24378;&#35813;&#27169;&#22359;&#12290;&#20026;&#20102;&#20943;&#36731;&#38271;&#26399;&#27169;&#25311;&#20013;&#30340;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#27861;&#36827;&#34892;&#27169;&#22411;&#21442;&#25968;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Crowd simulation holds crucial applications in various domains, such as urban planning, architectural design, and traffic arrangement. In recent years, physics-informed machine learning methods have achieved state-of-the-art performance in crowd simulation but fail to model the heterogeneity and multi-modality of human movement comprehensively. In this paper, we propose a social physics-informed diffusion model named SPDiff to mitigate the above gap. SPDiff takes both the interactive and historical information of crowds in the current timeframe to reverse the diffusion process, thereby generating the distribution of pedestrian movement in the subsequent timeframe. Inspired by the well-known social physics model, i.e., Social Force, regarding crowd dynamics, we design a crowd interaction module to guide the denoising process and further enhance this module with the equivariant properties of crowd interactions. To mitigate error accumulation in long-term simulations, we propose a multi-f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#12289;&#31038;&#20250;&#32463;&#27982;&#12289;&#28180;&#19994;&#31649;&#29702;&#30446;&#26631;&#21644;&#20107;&#20214;&#31561;&#36741;&#21161;&#25968;&#25454;&#65292;&#39044;&#27979;&#20844;&#27665;&#25253;&#21578;&#30340;&#38035;&#40060;&#32773;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36741;&#21161;&#25968;&#25454;&#33021;&#22815;&#20197;&#36739;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#21644;&#21333;&#20010;&#27700;&#22495;&#30340;&#38035;&#40060;&#32773;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.06678</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#33021;&#21542;&#39044;&#27979;&#20844;&#27665;&#25253;&#21578;&#30340;&#38035;&#40060;&#32773;&#34892;&#20026;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can machine learning predict citizen-reported angler behavior?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#29615;&#22659;&#12289;&#31038;&#20250;&#32463;&#27982;&#12289;&#28180;&#19994;&#31649;&#29702;&#30446;&#26631;&#21644;&#20107;&#20214;&#31561;&#36741;&#21161;&#25968;&#25454;&#65292;&#39044;&#27979;&#20844;&#27665;&#25253;&#21578;&#30340;&#38035;&#40060;&#32773;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36741;&#21161;&#25968;&#25454;&#33021;&#22815;&#20197;&#36739;&#39640;&#20934;&#30830;&#24230;&#39044;&#27979;&#29305;&#23450;&#22320;&#21306;&#21644;&#21333;&#20010;&#27700;&#22495;&#30340;&#38035;&#40060;&#32773;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#38035;&#40060;&#32773;&#34892;&#20026;&#65292;&#20363;&#22914;&#25429;&#33719;&#29575;&#21644;&#38035;&#40060;&#21387;&#21147;&#65292;&#23545;&#20110;&#20445;&#25345;&#40060;&#31867;&#31181;&#32676;&#21644;&#30830;&#20445;&#38035;&#40060;&#32773;&#28385;&#24847;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#38035;&#40060;&#32773;&#34892;&#20026;&#21487;&#20197;&#37096;&#20998;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#21644;&#25163;&#26426;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#36319;&#36394;&#65292;&#36825;&#20123;&#24179;&#21488;&#21644;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#20102;&#20241;&#38386;&#38035;&#40060;&#32773;&#25253;&#21578;&#30340;&#38035;&#40060;&#27963;&#21160;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#38035;&#40060;&#32773;&#34892;&#20026;&#21463;&#21040;&#24403;&#22320;&#31449;&#28857;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#29615;&#22659;&#12289;&#31038;&#20250;&#32463;&#27982;&#12289;&#28180;&#19994;&#31649;&#29702;&#30446;&#26631;&#21644;&#28129;&#27700;&#20307;&#30340;&#20107;&#20214;&#31561;&#36741;&#21161;&#25968;&#25454;&#65292;&#30740;&#31350;&#20102;&#20844;&#27665;&#25253;&#21578;&#30340;&#38035;&#40060;&#32773;&#34892;&#20026;&#30340;&#39044;&#27979;&#12290;&#30446;&#26631;&#26159;&#30830;&#23450;&#20165;&#20973;&#36741;&#21161;&#25968;&#25454;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#25253;&#21578;&#30340;&#34892;&#20026;&#12290;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#33539;&#22260;&#20197;&#21450;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#22312;&#21152;&#25343;&#22823;&#21333;&#20010;&#27700;&#22495;&#30340;&#26376;&#24230;&#39044;&#27979;&#20013;&#65292;&#20934;&#30830;&#24230;&#24471;&#20998;&#24179;&#22343;&#20026;88%&#65292;&#22312;&#29305;&#23450;&#22320;&#21306;&#30340;&#19968;&#22825;&#20869;&#36827;&#34892;&#30340;&#31354;&#38388;&#39044;&#27979;&#30340;&#20934;&#30830;&#24230;&#24471;&#20998;&#20026;86%&#12290;&#22312;&#20854;&#20182;&#20998;&#36776;&#29575;&#21644;&#23610;&#24230;&#19978;&#65292;&#27169;&#22411;&#21482;&#33021;&#36798;&#21040;&#24456;&#20302;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prediction of angler behaviors, such as catch rates and angler pressure, is essential to maintaining fish populations and ensuring angler satisfaction. Angler behavior can partly be tracked by online platforms and mobile phone applications that provide fishing activities reported by recreational anglers. Moreover, angler behavior is known to be driven by local site attributes. Here, the prediction of citizen-reported angler behavior was investigated by machine-learning methods using auxiliary data on the environment, socioeconomics, fisheries management objectives, and events at a freshwater body. The goal was to determine whether auxiliary data alone could predict the reported behavior. Different spatial and temporal extents and temporal resolutions were considered. Accuracy scores averaged 88% for monthly predictions at single water bodies and 86% for spatial predictions on a day in a specific region across Canada. At other resolutions and scales, the models only achieved low predict
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#28304;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36712;&#36857;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#30142;&#30149;&#21644;&#24178;&#39044;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#30340;&#32972;&#26223;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.06675</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#28304;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36712;&#36857;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32972;&#26223;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Masked language model for multi-source EHR trajectories contextual representation learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06675
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#28304;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#36712;&#36857;&#24182;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#30142;&#30149;&#21644;&#24178;&#39044;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#38382;&#39064;&#30340;&#32972;&#26223;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#25351;&#23548;&#26410;&#26469;&#20915;&#31574;&#38656;&#35201;&#35299;&#20915;&#20197;&#19979;&#25361;&#25112;&#65292;&#21253;&#25324; 1) &#38271;/&#30701;&#26399;&#20381;&#36182;&#21644; 2) &#30142;&#30149;&#21644;&#24178;&#39044;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#21452;&#21521;&#21464;&#21387;&#22120;&#24050;&#32463;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#36974;&#25513;&#19968;&#20010;&#28304; (&#20363;&#22914; ICD10 &#20195;&#30721;) &#24182;&#35757;&#32451;&#21464;&#21387;&#22120;&#20351;&#29992;&#20854;&#20182;&#28304; (&#20363;&#22914; ATC &#20195;&#30721;) &#39044;&#27979;&#23427;&#26469;&#35299;&#20915;&#21518;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using electronic health records data and machine learning to guide future decisions needs to address challenges, including 1) long/short-term dependencies and 2) interactions between diseases and interventions. Bidirectional transformers have effectively addressed the first challenge. Here we tackled the latter challenge by masking one source (e.g., ICD10 codes) and training the transformer to predict it using other sources (e.g., ATC codes).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06674</link><description>&lt;p&gt;
&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#30340;&#23454;&#38469;&#25104;&#21592;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Understanding Practical Membership Privacy of Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26041;&#27861;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#20197;&#21450;&#35757;&#32451;&#32467;&#26463;&#26102;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#26469;&#31995;&#32479;&#22320;&#27979;&#35797;&#32454;&#35843;&#22823;&#22411;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#23454;&#38469;&#38544;&#31169;&#28431;&#27934;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#29702;&#35299;&#20351;&#25968;&#25454;&#38598;&#21644;&#26679;&#26412;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#29305;&#24615;&#12290;&#22312;&#25968;&#25454;&#38598;&#29305;&#24615;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#30340;&#31034;&#20363;&#25968;&#37327;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#24130;&#24459;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#26159;&#20197;&#25915;&#20987;&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;&#22312;&#20302;&#20551;&#38451;&#24615;&#29575;&#19979;&#27979;&#37327;&#65289;&#26469;&#34913;&#37327;&#30340;&#12290;&#23545;&#20110;&#20010;&#21035;&#26679;&#26412;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#20135;&#29983;&#30340;&#22823;&#26799;&#24230;&#19982;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#28431;&#27934;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply a state-of-the-art membership inference attack (MIA) to systematically test the practical privacy vulnerability of fine-tuning large image classification models.We focus on understanding the properties of data sets and samples that make them vulnerable to membership inference. In terms of data set properties, we find a strong power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. For an individual sample, large gradients at the end of training are strongly correlated with MIA vulnerability.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26032;&#26041;&#27861;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#65292;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#24314;&#27169;&#26694;&#26550;&#19979;&#23454;&#29616;&#30452;&#25509;&#21644;&#36845;&#20195;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23884;&#20837;NWP&#39044;&#27979;&#65292;&#25552;&#39640;&#21487;&#20449;&#36182;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06666</link><description>&lt;p&gt;
&#22522;&#20110;&#30495;&#23454;&#39044;&#27979;&#36807;&#31243;&#25351;&#23548;&#30340;&#25193;&#25955;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Weather Prediction with Diffusion Guided by Realistic Forecast Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06666
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26032;&#26041;&#27861;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#65292;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#24314;&#27169;&#26694;&#26550;&#19979;&#23454;&#29616;&#30452;&#25509;&#21644;&#36845;&#20195;&#39044;&#27979;&#65292;&#24182;&#19988;&#21487;&#20197;&#23884;&#20837;NWP&#39044;&#27979;&#65292;&#25552;&#39640;&#21487;&#20449;&#36182;&#24615;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#27979;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39046;&#22495;&#65292;&#26368;&#36817;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#27169;&#22411;&#24050;&#32463;&#25509;&#36817;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;DL&#27169;&#22411;&#36890;&#24120;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#65292;&#38754;&#20020;&#30528;&#22312;&#35757;&#32451;&#21518;&#28789;&#27963;&#24615;&#26377;&#38480;&#21644;&#25972;&#21512;NWP&#39044;&#27979;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#30001;&#27492;&#21487;&#33021;&#23548;&#33268;&#19981;&#30495;&#23454;&#30340;&#39044;&#27979;&#32780;&#24341;&#36215;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#36827;&#34892;&#22825;&#27668;&#39044;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#30456;&#21516;&#30340;&#24314;&#27169;&#26694;&#26550;&#19979;&#23454;&#29616;&#30452;&#25509;&#21644;&#36845;&#20195;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#33021;&#22815;&#29420;&#31435;&#29983;&#25104;&#39044;&#27979;&#65292;&#36824;&#21487;&#20197;&#22312;&#37319;&#26679;&#36807;&#31243;&#20013;&#23884;&#20837;NWP&#39044;&#27979;&#65292;&#29978;&#33267;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#25552;&#21069;&#26399;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#20026;&#24191;&#22823;&#22825;&#27668;&#31038;&#21306;&#25552;&#20379;&#20102;&#26356;&#21152;&#21487;&#20449;&#36182;&#30340;DL&#31995;&#32479;&#12290;&#21478;&#22806;&#65292;&#23558;&#25345;&#32493;&#24615;&#39044;&#27979;&#34701;&#20837;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weather forecasting remains a crucial yet challenging domain, where recently developed models based on deep learning (DL) have approached the performance of traditional numerical weather prediction (NWP) models. However, these DL models, often complex and resource-intensive, face limitations in flexibility post-training and in incorporating NWP predictions, leading to reliability concerns due to potential unphysical predictions. In response, we introduce a novel method that applies diffusion models (DM) for weather forecasting. In particular, our method can achieve both direct and iterative forecasting with the same modeling framework. Our model is not only capable of generating forecasts independently but also uniquely allows for the integration of NWP predictions, even with varying lead times, during its sampling process. The flexibility and controllability of our model empowers a more trustworthy DL system for the general weather community. Additionally, incorporating persistence an
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06665</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#22312;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Essential Role of Causality in Foundation World Models for Embodied AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06665
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#32467;&#26524;&#30340;&#20934;&#30830;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22522;&#30784;&#27169;&#22411;&#20013;&#21462;&#24471;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#23545;&#35805;&#20195;&#29702;&#26041;&#38754;&#65292;&#24341;&#21457;&#20102;&#23545;&#20855;&#22791;&#26222;&#36941;&#33021;&#21147;&#30340;&#20855;&#36523;&#20195;&#29702;&#20154;&#28508;&#21147;&#30340;&#20852;&#36259;&#12290;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#38656;&#35201;&#33021;&#22815;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#23545;&#20110;&#20855;&#36523;&#20154;&#24037;&#26234;&#33021;&#32780;&#35328;&#26159;&#19981;&#22815;&#30340;&#12290;&#22240;&#26524;&#20851;&#31995;&#30340;&#30740;&#31350;&#26377;&#21161;&#20110;&#26500;&#24314;&#30495;&#23454;&#19990;&#30028;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#39044;&#27979;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20855;&#36523;&#20195;&#29702;&#29983;&#25104;&#22522;&#30784;&#19990;&#30028;&#27169;&#22411;&#30340;&#21069;&#26223;&#65292;&#24182;&#23545;&#20854;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#24615;&#25552;&#20986;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#25972;&#21512;&#22240;&#26524;&#20851;&#31995;&#26159;&#20419;&#36827;&#19982;&#19990;&#30028;&#30340;&#26377;&#24847;&#20041;&#30340;&#29289;&#29702;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#32972;&#26223;&#19979;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#35823;&#35299;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#23545;&#26410;&#26469;&#30340;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in foundation models, especially in large multi-modal models and conversational agents, have ignited interest in the potential of generally capable embodied agents. Such agents would require the ability to perform new tasks in many different real-world environments. However, current foundation models fail to accurately model physical interactions with the real world thus not sufficient for Embodied AI. The study of causality lends itself to the construction of veridical world models, which are crucial for accurately predicting the outcomes of possible interactions. This paper focuses on the prospects of building foundation world models for the upcoming generation of embodied agents and presents a novel viewpoint on the significance of causality within these. We posit that integrating causal considerations is vital to facilitate meaningful physical interactions with the world. Finally, we demystify misconceptions about causality in this context and present our outlook fo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#30721;&#22120;&#22312;&#34920;&#24449;&#23481;&#37327;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31616;&#21333;&#20462;&#27491;&#26041;&#26696;&#65292;&#32780;&#19981;&#25913;&#21464;&#20869;&#31215;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06662</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#30721;&#22120;&#30340;&#31526;&#21495;&#31209;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Sign Rank Limitations for Attention-Based Graph Decoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#35299;&#30721;&#22120;&#22312;&#34920;&#24449;&#23481;&#37327;&#26041;&#38754;&#30340;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#31616;&#21333;&#20462;&#27491;&#26041;&#26696;&#65292;&#32780;&#19981;&#25913;&#21464;&#20869;&#31215;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20869;&#31215;&#22411;&#35299;&#30721;&#22120;&#26159;&#25552;&#21462;&#28508;&#22312;&#23884;&#20837;&#25968;&#25454;&#20013;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#30721;&#22120;&#22312;&#34920;&#24449;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22312;&#22270;&#37325;&#24314;&#38382;&#39064;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;&#22270;&#25968;&#25454;&#20013;&#25552;&#20379;&#20102;&#23545;&#36825;&#19968;&#26222;&#36941;&#29616;&#35937;&#30340;&#29702;&#35770;&#38416;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#20462;&#25913;&#26041;&#26696;&#20197;&#35299;&#20915;&#35813;&#38382;&#39064;&#65292;&#19988;&#19981;&#33073;&#31163;&#20869;&#31215;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inner product-based decoders are among the most influential frameworks used to extract meaningful data from latent embeddings. However, such decoders have shown limitations in representation capacity in numerous works within the literature, which have been particularly notable in graph reconstruction problems. In this paper, we provide the first theoretical elucidation of this pervasive phenomenon in graph data, and suggest straightforward modifications to circumvent this issue without deviating from the inner product framework.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#22810;&#23186;&#20307;&#23481;&#22120;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#25163;&#26426;&#35270;&#39057;&#30340;&#36523;&#20221;&#39564;&#35777;&#21644;&#23436;&#25972;&#24615;&#65292;&#20197;&#25552;&#39640;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#36890;&#20449;&#24212;&#29992;&#31243;&#24207;&#20013;&#20849;&#20139;&#35270;&#39057;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.06661</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#23186;&#20307;&#23481;&#22120;&#32467;&#26500;&#20998;&#26512;&#39564;&#35777;&#21644;&#20445;&#35777;&#26234;&#33021;&#25163;&#26426;&#35270;&#39057;&#30340;&#30495;&#23454;&#24615;&#21644;&#23436;&#25972;&#24615;
&lt;/p&gt;
&lt;p&gt;
Authentication and integrity of smartphone videos through multimedia container structure analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06661
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#22810;&#23186;&#20307;&#23481;&#22120;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26234;&#33021;&#25163;&#26426;&#35270;&#39057;&#30340;&#36523;&#20221;&#39564;&#35777;&#21644;&#23436;&#25972;&#24615;&#65292;&#20197;&#25552;&#39640;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#36890;&#20449;&#24212;&#29992;&#31243;&#24207;&#20013;&#20849;&#20139;&#35270;&#39057;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#31227;&#21160;&#35774;&#22791;&#24050;&#32463;&#25104;&#20026;&#25968;&#30721;&#30456;&#26426;&#30340;&#33258;&#28982;&#26367;&#20195;&#21697;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#36731;&#26494;&#24555;&#25463;&#22320;&#25429;&#25417;&#26085;&#24120;&#24773;&#26223;&#65292;&#40723;&#21169;&#29992;&#25143;&#36890;&#36807;&#22270;&#20687;&#21644;&#35270;&#39057;&#26469;&#34920;&#36798;&#33258;&#24049;&#12290;&#36825;&#20123;&#35270;&#39057;&#21487;&#20197;&#36890;&#36807;&#19981;&#21516;&#30340;&#24179;&#21488;&#36827;&#34892;&#20849;&#20139;&#65292;&#20174;&#32780;&#26292;&#38706;&#22312;&#29359;&#32618;&#20998;&#23376;&#30340;&#24847;&#22270;&#25805;&#32437;&#20043;&#19979;&#65292;&#29359;&#32618;&#20998;&#23376;&#20102;&#35299;&#29616;&#26377;&#27861;&#21307;&#25216;&#26415;&#22312;&#21496;&#27861;&#36807;&#31243;&#20013;&#23545;&#26080;&#32618;&#32773;&#36827;&#34892;&#25351;&#25511;&#25110;&#27927;&#33073;&#26377;&#32618;&#32773;&#30340;&#24369;&#28857;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21046;&#36896;&#21830;&#24182;&#19981;&#23436;&#20840;&#31526;&#21512;&#35270;&#39057;&#21019;&#24314;&#26631;&#20934;&#30340;&#35268;&#33539;&#12290;&#27492;&#22806;&#65292;&#20998;&#20139;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#21363;&#26102;&#36890;&#20449;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#35270;&#39057;&#32463;&#36807;&#28388;&#27874;&#21644;&#21387;&#32553;&#22788;&#29702;&#65292;&#20197;&#20943;&#23567;&#20854;&#22823;&#23567;&#12289;&#26041;&#20415;&#20256;&#36755;&#24182;&#20248;&#21270;&#23384;&#20648;&#22312;&#24179;&#21488;&#19978;&#12290;&#24179;&#21488;&#23545;&#35268;&#33539;&#21644;&#36716;&#25442;&#32467;&#26524;&#30340;&#30095;&#28431;&#23884;&#20837;&#20102;&#35270;&#39057;&#30340;&#22810;&#23186;&#20307;&#23481;&#22120;&#20013;&#30340;&#29305;&#24449;&#27169;&#24335;&#12290;&#36825;&#20123;&#27169;&#24335;&#21487;&#29992;&#20110;&#21306;&#20998;&#35270;&#39057;&#30340;&#26469;&#28304;&#21644;&#22788;&#29702;&#21382;&#21490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, mobile devices have become the natural substitute for the digital camera, as they capture everyday situations easily and quickly, encouraging users to express themselves through images and videos. These videos can be shared across different platforms exposing them to any kind of intentional manipulation by criminals who are aware of the weaknesses of forensic techniques to accuse an innocent person or exonerate a guilty person in a judicial process. Commonly, manufacturers do not comply 100% with the specifications of the standards for the creation of videos. Also, videos shared on social networks, and instant messaging applications go through filtering and compression processes to reduce their size, facilitate their transfer, and optimize storage on their platforms. The omission of specifications and results of transformations carried out by the platforms embed a features pattern in the multimedia container of the videos. These patterns make it possible to distinguish the br
&lt;/p&gt;</description></item><item><title>Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.06659</link><description>&lt;p&gt;
Shadowcast: &#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#23545;&#25239;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06659
&lt;/p&gt;
&lt;p&gt;
Shadowcast&#26159;&#19968;&#31181;&#38544;&#31192;&#30340;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#20266;&#35013;&#25104;&#33391;&#24615;&#22270;&#20687;&#21644;&#21305;&#37197;&#25991;&#26412;&#26469;&#25805;&#32437;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21709;&#24212;&#12290;&#23427;&#21253;&#25324;&#26631;&#31614;&#25915;&#20987;&#21644;&#35828;&#26381;&#25915;&#20987;&#65292;&#21487;&#20197;&#28151;&#28102;&#31867;&#21035;&#26631;&#31614;&#24182;&#32534;&#20889;&#26377;&#35828;&#26381;&#21147;&#30340;&#25551;&#36848;&#12290;&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#65292;Shadowcast&#33021;&#22815;&#39640;&#25928;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#33021;&#22815;&#20174;&#35270;&#35273;&#36755;&#20837;&#20013;&#29983;&#25104;&#25991;&#26412;&#21709;&#24212;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#22810;&#21151;&#33021;&#24615;&#24102;&#26469;&#20102;&#37325;&#22823;&#30340;&#23433;&#20840;&#38544;&#24739;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#25581;&#31034;&#20102;VLM&#23545;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#30340;&#26131;&#21463;&#24615;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#25805;&#32437;&#23545;&#26080;&#23475;&#30340;&#26085;&#24120;&#25552;&#31034;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Shadowcast&#30340;&#38544;&#31192;&#25968;&#25454;&#27745;&#26579;&#25915;&#20987;&#26041;&#27861;&#65292;&#20854;&#20013;&#27602;&#26679;&#26412;&#22312;&#35270;&#35273;&#19978;&#19982;&#20855;&#26377;&#21305;&#37197;&#25991;&#26412;&#30340;&#33391;&#24615;&#22270;&#20687;&#38590;&#20197;&#21306;&#20998;&#12290;Shadowcast&#22312;&#20004;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#12290;&#31532;&#19968;&#31181;&#26159;&#26631;&#31614;&#25915;&#20987;&#65292;&#20351;VLM&#35823;&#35782;&#21035;&#31867;&#21035;&#26631;&#31614;&#65292;&#20363;&#22914;&#28151;&#28102;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#21644;&#20052;&#183;&#25308;&#30331;&#31561;&#20154;&#12290;&#31532;&#20108;&#31181;&#26159;&#35828;&#26381;&#25915;&#20987;&#65292;&#21033;&#29992;VLM&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#26469;&#32534;&#20889;&#25925;&#20107;&#65292;&#20363;&#22914;&#36890;&#36807;&#26377;&#35828;&#26381;&#21147;&#21644;&#30475;&#20284;&#21512;&#29702;&#30340;&#25551;&#36848;&#23558;&#22403;&#22334;&#39135;&#21697;&#25551;&#32472;&#25104;&#20581;&#24247;&#39135;&#21697;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Shadowcast&#20351;&#29992;&#20165;50&#20010;&#27602;&#26679;&#26412;&#23601;&#33021;&#39640;&#24230;&#26377;&#25928;&#22320;&#23454;&#29616;&#25915;&#20987;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27602;&#26679;&#26412;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, yet their versatility raises significant security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts. We introduce Shadowcast, a stealthy data poisoning attack method where poison samples are visually indistinguishable from benign images with matching texts. Shadowcast demonstrates effectiveness in two attack types. The first is Label Attack, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. The second is Persuasion Attack, which leverages VLMs' text generation capabilities to craft narratives, such as portraying junk food as health food, through persuasive and seemingly rational descriptions. We show that Shadowcast are highly effective in achieving attacker's intentions using as few as 50 poison samples. Moreover, these poison samples remain eff
&lt;/p&gt;</description></item><item><title>DiffsFormer&#26159;&#19968;&#31181;&#21033;&#29992;Diffusion Transformer&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#39044;&#27979;&#20013;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#25968;&#25454;&#21516;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06656</link><description>&lt;p&gt;
DiffsFormer: Diffusion Transformer&#22312;&#32929;&#31080;&#22240;&#23376;&#22686;&#24378;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
DiffsFormer: A Diffusion Transformer on Stock Factor Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06656
&lt;/p&gt;
&lt;p&gt;
DiffsFormer&#26159;&#19968;&#31181;&#21033;&#29992;Diffusion Transformer&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32929;&#31080;&#39044;&#27979;&#20013;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#21644;&#25968;&#25454;&#21516;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#32929;&#31080;&#39044;&#27979;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#31232;&#32570;&#24615;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#22914;&#20302;&#20449;&#22122;&#27604;&#21644;&#25968;&#25454;&#21516;&#36136;&#24615;&#65292;&#23545;&#20934;&#30830;&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#26679;&#26412;(AIGS)&#26469;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Diffusion Model&#26469;&#29983;&#25104;&#20855;&#26377;Transformer&#26550;&#26500;&#30340;&#32929;&#31080;&#22240;&#23376;(DiffsFormer)&#12290;DiffsFormer&#39318;&#20808;&#22312;&#22823;&#35268;&#27169;&#28304;&#39046;&#22495;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32467;&#21512;&#26465;&#20214;&#25351;&#23548;&#20197;&#25429;&#25417;&#20840;&#23616;&#32852;&#21512;&#20998;&#24067;&#12290;&#22312;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DiffsFormer&#26469;&#36890;&#36807;&#32534;&#36753;&#29616;&#26377;&#26679;&#26412;&#26469;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#12290;&#36825;&#20010;&#32534;&#36753;&#27493;&#39588;&#20801;&#35768;&#25105;&#20204;&#25511;&#21046;&#32534;&#36753;&#36807;&#31243;&#30340;&#24378;&#24230;&#65292;&#30830;&#23450;&#29983;&#25104;&#25968;&#25454;&#19982;&#30446;&#26631;&#39046;&#22495;&#30340;&#20559;&#31163;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models have demonstrated remarkable efficacy and efficiency in a wide range of stock forecasting tasks. However, the inherent challenges of data scarcity, including low signal-to-noise ratio (SNR) and data homogeneity, pose significant obstacles to accurate forecasting. To address this issue, we propose a novel approach that utilizes artificial intelligence-generated samples (AIGS) to enhance the training procedures. In our work, we introduce the Diffusion Model to generate stock factors with Transformer architecture (DiffsFormer). DiffsFormer is initially trained on a large-scale source domain, incorporating conditional guidance so as to capture global joint distribution. When presented with a specific downstream task, we employ DiffsFormer to augment the training procedure by editing existing samples. This editing step allows us to control the strength of the editing process, determining the extent to which the generated data deviates from the target domain. To evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#20197;&#20928;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.06655</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#65306;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Text Purification: A Large Language Model Approach for Defense
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38450;&#24481;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21152;&#20197;&#20928;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#20928;&#21270;&#26159;&#19968;&#31181;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#20445;&#25252;&#20998;&#31867;&#22120;&#20813;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#25915;&#20987;&#31867;&#22411;&#25110;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#36825;&#20123;&#25216;&#26415;&#23545;&#34987;&#25915;&#20987;&#36755;&#20837;&#36827;&#34892;&#29305;&#24449;&#21270;&#21644;&#28040;&#38500;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26088;&#22312;&#24674;&#22797;&#20986;&#19982;&#26368;&#21021;&#34987;&#25915;&#20987;&#30340;&#36755;&#20837;&#30456;&#20284;&#19988;&#34987;&#20998;&#31867;&#22120;&#27491;&#30830;&#20998;&#31867;&#30340;&#20928;&#21270;&#26679;&#26412;&#12290;&#30001;&#20110;&#31163;&#25955;&#36755;&#20837;&#30340;&#22122;&#22768;&#25200;&#21160;&#29305;&#24449;&#21270;&#25152;&#24102;&#26469;&#30340;&#22266;&#26377;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#19968;&#30452;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#20928;&#21270;&#26041;&#27861;&#22312;&#20445;&#25252;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#24615;&#25991;&#26412;&#20928;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#20928;&#21270;&#23545;&#25239;&#24615;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#29305;&#24449;&#21270;&#31163;&#25955;&#22122;&#22768;&#25200;&#21160;&#12290;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#24037;&#31243;&#26469;&#21033;&#29992;LLMs&#24674;&#22797;&#20928;&#21270;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial purification is a defense mechanism for safeguarding classifiers against adversarial attacks without knowing the type of attacks or training of the classifier. These techniques characterize and eliminate adversarial perturbations from the attacked inputs, aiming to restore purified samples that retain similarity to the initially attacked ones and are correctly classified by the classifier. Due to the inherent challenges associated with characterizing noise perturbations for discrete inputs, adversarial text purification has been relatively unexplored. In this paper, we investigate the effectiveness of adversarial purification methods in defending text classifiers. We propose a novel adversarial text purification that harnesses the generative capabilities of Large Language Models (LLMs) to purify adversarial text without the need to explicitly characterize the discrete noise perturbations. We utilize prompt engineering to exploit LLMs for recovering the purified examples for
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#26080;&#35266;&#27979;&#25968;&#25454;&#22320;&#28857;&#25512;&#26029;&#27745;&#26579;&#29289;&#27987;&#24230;&#30340;&#27169;&#22411;&#65292;&#20026;&#31354;&#27668;&#27745;&#26579;&#30740;&#31350;&#21644;&#25490;&#25918;&#30417;&#27979;&#25552;&#20379;&#20102;&#37325;&#35201;&#25163;&#27573;&#12290;</title><link>https://arxiv.org/abs/2402.06653</link><description>&lt;p&gt;
&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#31354;&#27668;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Using remotely sensed data for air pollution assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06653
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#36965;&#24863;&#25968;&#25454;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#26080;&#35266;&#27979;&#25968;&#25454;&#22320;&#28857;&#25512;&#26029;&#27745;&#26579;&#29289;&#27987;&#24230;&#30340;&#27169;&#22411;&#65292;&#20026;&#31354;&#27668;&#27745;&#26579;&#30740;&#31350;&#21644;&#25490;&#25918;&#30417;&#27979;&#25552;&#20379;&#20102;&#37325;&#35201;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#27668;&#27745;&#26579;&#26159;&#19968;&#20010;&#20840;&#29699;&#24615;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#19981;&#20165;&#24433;&#21709;&#20154;&#31867;&#20581;&#24247;&#65292;&#36824;&#24433;&#21709;&#29615;&#22659;&#12290;&#26377;&#20851;&#27745;&#26579;&#29289;&#27987;&#24230;&#30340;&#26102;&#31354;&#25968;&#25454;&#30340;&#23384;&#22312;&#23545;&#20110;&#36827;&#34892;&#31354;&#27668;&#27745;&#26579;&#30740;&#31350;&#21644;&#30417;&#27979;&#25490;&#25918;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35266;&#27979;&#25968;&#25454;&#20855;&#26377;&#36739;&#22823;&#30340;&#26102;&#38388;&#35206;&#30422;&#33539;&#22260;&#65292;&#20294;&#27979;&#31449;&#30340;&#25968;&#37327;&#38750;&#24120;&#26377;&#38480;&#65292;&#36890;&#24120;&#24314;&#22312;&#20154;&#21475;&#23494;&#38598;&#22320;&#21306;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21019;&#24314;&#33021;&#22815;&#25512;&#26029;&#26080;&#35266;&#27979;&#25968;&#25454;&#22320;&#28857;&#30340;&#27745;&#26579;&#29289;&#27987;&#24230;&#30340;&#27169;&#22411;&#12290;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#65292;&#23545;2019&#24180;&#20234;&#27604;&#21033;&#20122;&#21322;&#23707;&#19978;&#30340;&#20116;&#31181;&#36873;&#25321;&#24615;&#27745;&#26579;&#29289;&#65288;$NO_2$&#65292;$O_3$&#65292;$SO_2$&#65292;$PM10$&#21644;$PM2.5$&#65289;&#30340;&#27987;&#24230;&#36827;&#34892;&#39044;&#27979;&#12290;&#27169;&#22411;&#29305;&#24449;&#21253;&#25324;&#21355;&#26143;&#27979;&#37327;&#25968;&#25454;&#12289;&#27668;&#35937;&#21464;&#37327;&#12289;&#22303;&#22320;&#21033;&#29992;&#20998;&#31867;&#12289;&#26102;&#38388;&#21464;&#37327;&#65288;&#26376;&#20221;&#12289;&#24180;&#20013;&#26085;&#26399;&#65289;&#21644;&#31354;&#38388;&#21464;&#37327;&#65288;&#32428;&#24230;&#12289;&#32463;&#24230;&#12289;&#28023;&#25300;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Air pollution constitutes a global problem of paramount importance that affects not only human health, but also the environment. The existence of spatial and temporal data regarding the concentrations of pollutants is crucial for performing air pollution studies and monitor emissions. However, although observation data presents great temporal coverage, the number of stations is very limited and they are usually built in more populated areas.   The main objective of this work is to create models capable of inferring pollutant concentrations in locations where no observation data exists. A machine learning model, more specifically the random forest model, was developed for predicting concentrations in the Iberian Peninsula in 2019 for five selected pollutants: $NO_2$, $O_3$ $SO_2$, $PM10$, and $PM2.5$. Model features include satellite measurements, meteorological variables, land use classification, temporal variables (month, day of year), and spatial variables (latitude, longitude, altit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#38477;&#23610;&#24230;&#26041;&#27861;&#65288;DPDM&#65289;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#25968;&#25454;&#20174;1&#176;&#38477;&#23610;&#24230;&#21040;0.1&#176;&#20998;&#36776;&#29575;&#65292;&#24182;&#29983;&#25104;&#22823;&#37327;&#30340;&#38598;&#21512;&#25104;&#21592;&#20197;&#35780;&#20272;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#19996;&#20122;&#22320;&#21306;180&#24180;&#30340;&#26376;&#34920;&#38754;&#21464;&#37327;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#30340;&#35266;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.06646</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#38477;&#23610;&#24230;&#26041;&#27861;&#29992;&#20110;180&#24180;&#19996;&#20122;&#27668;&#20505;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model-based Probabilistic Downscaling for 180-year East Asian Climate Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06646
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27010;&#29575;&#38477;&#23610;&#24230;&#26041;&#27861;&#65288;DPDM&#65289;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#25968;&#25454;&#20174;1&#176;&#38477;&#23610;&#24230;&#21040;0.1&#176;&#20998;&#36776;&#29575;&#65292;&#24182;&#29983;&#25104;&#22823;&#37327;&#30340;&#38598;&#21512;&#25104;&#21592;&#20197;&#35780;&#20272;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#19996;&#20122;&#22320;&#21306;180&#24180;&#30340;&#26376;&#34920;&#38754;&#21464;&#37327;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#35814;&#32454;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29699;&#36827;&#20837;&#8220;&#20840;&#29699;&#21152;&#28909;&#8221;&#26102;&#20195;&#65292;&#20102;&#35299;&#21306;&#22495;&#27668;&#20505;&#21464;&#21270;&#21464;&#24471;&#36843;&#22312;&#30473;&#30571;&#12290;&#25552;&#20379;&#23616;&#37096;&#27934;&#23519;&#30340;&#26377;&#25928;&#38477;&#23610;&#24230;&#26041;&#27861;&#23545;&#20110;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#21253;&#25324;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#21306;&#22495;&#21160;&#21147;&#27169;&#22411;&#25110;&#32479;&#35745;&#38477;&#23610;&#24230;&#26694;&#26550;&#65292;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#38477;&#23610;&#24230;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#27010;&#29575;&#38477;&#23610;&#24230;&#27169;&#22411;&#65288;DPDM&#65289;&#24341;&#20837;&#27668;&#35937;&#39046;&#22495;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#23558;&#25968;&#25454;&#20174;1&#176;&#38477;&#23610;&#24230;&#21040;0.1&#176;&#20998;&#36776;&#29575;&#12290;&#19982;&#30830;&#23450;&#24615;&#38477;&#23610;&#24230;&#26041;&#26696;&#30456;&#27604;&#65292;&#23427;&#19981;&#20165;&#20855;&#26377;&#26356;&#20934;&#30830;&#30340;&#23616;&#37096;&#32454;&#33410;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#27010;&#29575;&#20998;&#24067;&#37319;&#26679;&#29983;&#25104;&#22823;&#37327;&#30340;&#38598;&#21512;&#25104;&#21592;&#65292;&#20197;&#35780;&#20272;&#38477;&#23610;&#24230;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#25104;&#19996;&#20122;&#22320;&#21306;180&#24180;&#30340;&#26376;&#34920;&#38754;&#21464;&#37327;&#25968;&#25454;&#38598;&#65292;&#20026;&#20102;&#25552;&#20379;&#26356;&#35814;&#32454;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
As our planet is entering into the "global boiling" era, understanding regional climate change becomes imperative. Effective downscaling methods that provide localized insights are crucial for this target. Traditional approaches, including computationally-demanding regional dynamical models or statistical downscaling frameworks, are often susceptible to the influence of downscaling uncertainty. Here, we address these limitations by introducing a diffusion probabilistic downscaling model (DPDM) into the meteorological field. This model can efficiently transform data from 1{\deg} to 0.1{\deg} resolution. Compared with deterministic downscaling schemes, it not only has more accurate local details, but also can generate a large number of ensemble members based on probability distribution sampling to evaluate the uncertainty of downscaling. Additionally, we apply the model to generate a 180-year dataset of monthly surface variables in East Asia, offering a more detailed perspective for unde
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24314;&#31435;GARCH&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GARCH-NN&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#21160;&#29575;&#27169;&#22411;&#65292;&#24182;&#23558;GARCH&#27169;&#22411;&#20013;&#30340;&#27874;&#21160;&#29575;&#39118;&#26684;&#21270;&#20107;&#23454;&#34701;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.06642</link><description>&lt;p&gt;
&#20174;GARCH&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#21160;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From GARCH to Neural Network for Volatility Forecast
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06642
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24314;&#31435;GARCH&#27169;&#22411;&#21644;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GARCH-NN&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#21160;&#29575;&#27169;&#22411;&#65292;&#24182;&#23558;GARCH&#27169;&#22411;&#20013;&#30340;&#27874;&#21160;&#29575;&#39118;&#26684;&#21270;&#20107;&#23454;&#34701;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27874;&#21160;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#24230;&#37327;&#65292;&#22312;&#39118;&#38505;&#31649;&#29702;&#31561;&#20247;&#22810;&#37329;&#34701;&#27963;&#21160;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#35745;&#37327;&#32463;&#27982;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30028;&#24050;&#32463;&#21457;&#23637;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#37329;&#34701;&#27874;&#21160;&#29575;&#39044;&#27979;&#26041;&#27861;&#65306;&#38543;&#26426;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#26041;&#27861;&#12290;&#23613;&#31649;&#23427;&#20204;&#21508;&#33258;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#30740;&#31350;&#36712;&#36857;&#20013;&#21457;&#23637;&#65292;&#30456;&#20114;&#20043;&#38388;&#26377;&#24456;&#23569;&#30340;&#20132;&#20114;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;GARCH&#23478;&#26063;&#27169;&#22411;&#21644;&#30456;&#24212;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#36890;&#36807;&#24314;&#31435;&#31561;&#20215;&#20851;&#31995;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;GARCH-NN&#65292;&#29992;&#20110;&#26500;&#24314;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27874;&#21160;&#29575;&#27169;&#22411;&#12290;&#23427;&#33719;&#21462;GARCH&#27169;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#23545;&#24212;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#32452;&#20214;&#25972;&#21512;&#21040;&#24050;&#24314;&#31435;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;&#20174;&#32780;&#23558;GARCH&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#27874;&#21160;&#29575;&#39118;&#26684;&#21270;&#20107;&#23454;&#65288;SFs&#65289;&#26080;&#32541;&#34701;&#20837;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Volatility, as a measure of uncertainty, plays a crucial role in numerous financial activities such as risk management. The Econometrics and Machine Learning communities have developed two distinct approaches for financial volatility forecasting: the stochastic approach and the neural network (NN) approach. Despite their individual strengths, these methodologies have conventionally evolved in separate research trajectories with little interaction between them. This study endeavors to bridge this gap by establishing an equivalence relationship between models of the GARCH family and their corresponding NN counterparts. With the equivalence relationship established, we introduce an innovative approach, named GARCH-NN, for constructing NN-based volatility models. It obtains the NN counterparts of GARCH models and integrates them as components into an established NN architecture, thereby seamlessly infusing volatility stylized facts (SFs) inherent in the GARCH models into the neural network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#35757;&#32451;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06638</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Transformers with Attentive Federated Aggregation for Time Series Stock Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#35757;&#32451;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#65292;Transformer&#27169;&#22411;&#30340;&#21019;&#26032;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;Transformer&#27169;&#22411;&#20855;&#22791;&#25429;&#25417;&#24207;&#21015;&#25968;&#25454;&#20013;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#21644;&#30456;&#20114;&#20316;&#29992;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#24212;&#29992;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#65292;&#23613;&#31649;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;Transformer&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#19982;NLP&#21644;CV&#20013;&#30340;&#25361;&#25112;&#30456;&#27604;&#65292;&#26102;&#38388;&#24207;&#21015;&#38382;&#39064;&#19981;&#20165;&#28041;&#21450;&#21040;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#39034;&#24207;&#25110;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#38656;&#35201;&#32771;&#34385;&#36235;&#21183;&#12289;&#27700;&#24179;&#21644;&#23395;&#33410;&#24615;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#20915;&#31574;&#38750;&#24120;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#35757;&#32451;&#26041;&#26696;&#22312;&#20351;&#29992;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#26102;&#23384;&#22312;&#36807;&#25311;&#21512;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#19981;&#36275;&#20043;&#22788;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#21147;&#32852;&#21512;&#32858;&#21512;&#30340;Transformer&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#32929;&#31080;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent innovations in transformers have shown their superior performance in natural language processing (NLP) and computer vision (CV). The ability to capture long-range dependencies and interactions in sequential data has also triggered a great interest in time series modeling, leading to the widespread use of transformers in many time series applications. However, being the most common and crucial application, the adaptation of transformers to time series forecasting has remained limited, with both promising and inconsistent results. In contrast to the challenges in NLP and CV, time series problems not only add the complexity of order or temporal dependence among input sequences but also consider trend, level, and seasonality information that much of this data is valuable for decision making. The conventional training scheme has shown deficiencies regarding model overfitting, data scarcity, and privacy issues when working with transformers for a forecasting task. In this work, we pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#19968;&#20010;&#36275;&#22815;&#23485;&#32780;&#20219;&#24847;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20986;&#26469;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#19982;&#22823;&#22411;&#22240;&#23376;&#27169;&#22411;&#31561;&#25928;&#65292;&#25171;&#24320;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23553;&#38381;&#24418;&#24335;&#30340;&#25512;&#23548;&#26041;&#27861;&#12290;&#30740;&#31350;&#23454;&#35777;&#20102;&#19981;&#21516;&#26550;&#26500;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#36275;&#22815;&#22810;&#25968;&#25454;&#19979;&#30340;&#34920;&#29616;&#36880;&#28176;&#25552;&#21319;&#65292;&#30452;&#33267;&#36798;&#21040;&#39281;&#21644;&#12290;</title><link>https://arxiv.org/abs/2402.06635</link><description>&lt;p&gt;
&#22823;&#22411;&#65288;&#21644;&#28145;&#24230;&#65289;&#22240;&#23376;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large (and Deep) Factor Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35777;&#26126;&#19968;&#20010;&#36275;&#22815;&#23485;&#32780;&#20219;&#24847;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20986;&#26469;&#30340;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#27169;&#22411;&#19982;&#22823;&#22411;&#22240;&#23376;&#27169;&#22411;&#31561;&#25928;&#65292;&#25171;&#24320;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23553;&#38381;&#24418;&#24335;&#30340;&#25512;&#23548;&#26041;&#27861;&#12290;&#30740;&#31350;&#23454;&#35777;&#20102;&#19981;&#21516;&#26550;&#26500;&#36873;&#25321;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#38543;&#30528;&#28145;&#24230;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#36275;&#22815;&#22810;&#25968;&#25454;&#19979;&#30340;&#34920;&#29616;&#36880;&#28176;&#25552;&#21319;&#65292;&#30452;&#33267;&#36798;&#21040;&#39281;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25171;&#24320;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#19968;&#20010;&#36275;&#22815;&#23485;&#32780;&#20219;&#24847;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;(DNN)&#34987;&#35757;&#32451;&#29992;&#26469;&#26368;&#22823;&#21270;&#38543;&#26426;&#36148;&#29616;&#22240;&#23376;(SDF)&#30340;&#22799;&#26222;&#27604;&#29575;&#31561;&#25928;&#20110;&#19968;&#20010;&#22823;&#22411;&#22240;&#23376;&#27169;&#22411;(LFM)&#65306;&#19968;&#20010;&#20351;&#29992;&#35768;&#22810;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#32447;&#24615;&#22240;&#23376;&#23450;&#20215;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#24449;&#30340;&#24615;&#36136;&#21462;&#20915;&#20110;DNN&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#22312;&#19968;&#31181;&#26126;&#30830;&#21487;&#36861;&#36394;&#30340;&#26041;&#24335;&#19979;&#12290;&#36825;&#20351;&#24471;&#39318;&#27425;&#21487;&#20197;&#25512;&#23548;&#20986;&#23553;&#38381;&#24418;&#24335;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#22522;&#20110;DNN&#30340;SDF&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35780;&#20272;&#20102;LFMs&#65292;&#24182;&#23637;&#31034;&#20102;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;SDF&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#28145;&#24230;&#22797;&#26434;&#24615;&#30340;&#20248;&#28857;&#65306;&#38543;&#30528;&#36275;&#22815;&#22810;&#30340;&#25968;&#25454;&#65292;DNN-SDF&#30340;&#22806;&#26679;&#24635;&#20307;&#34920;&#29616;&#20250;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#32780;&#22686;&#21152;&#65292;&#24403;&#38544;&#34255;&#23618;&#36798;&#21040;&#32422;100&#23618;&#26102;&#36798;&#21040;&#39281;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
We open up the black box behind Deep Learning for portfolio optimization and prove that a sufficiently wide and arbitrarily deep neural network (DNN) trained to maximize the Sharpe ratio of the Stochastic Discount Factor (SDF) is equivalent to a large factor model (LFM): A linear factor pricing model that uses many non-linear characteristics. The nature of these characteristics depends on the architecture of the DNN in an explicit, tractable fashion. This makes it possible to derive end-to-end trained DNN-based SDFs in closed form for the first time. We evaluate LFMs empirically and show how various architectural choices impact SDF performance. We document the virtue of depth complexity: With enough data, the out-of-sample performance of DNN-SDF is increasing in the NN depth, saturating at huge depths of around 100 hidden layers.
&lt;/p&gt;</description></item><item><title>SocraSynth&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24179;&#21488;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#32479;&#35745;&#21644;&#31995;&#32479;&#21270;&#30340;&#35821;&#22659;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#21450;&#21487;&#35843;&#33410;&#30340;&#36777;&#35770;&#20105;&#35758;&#31243;&#24230;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06634</link><description>&lt;p&gt;
SocraSynth:&#22522;&#20110;&#26465;&#20214;&#32479;&#35745;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SocraSynth: Multi-LLM Reasoning with Conditional Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06634
&lt;/p&gt;
&lt;p&gt;
SocraSynth&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24179;&#21488;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#32479;&#35745;&#21644;&#31995;&#32479;&#21270;&#30340;&#35821;&#22659;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#21450;&#21487;&#35843;&#33410;&#30340;&#36777;&#35770;&#20105;&#35758;&#31243;&#24230;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30340;&#20559;&#35265;&#12289;&#24187;&#35273;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#23454;&#29992;&#19978;&#38754;&#20020;&#30528;&#20559;&#35265;&#12289;&#24187;&#35273;&#21644;&#25512;&#29702;&#33021;&#21147;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SocraSynth&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#24179;&#21488;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;SocraSynth&#36890;&#36807;&#36830;&#32493;&#30340;&#35770;&#35777;&#21644;&#21487;&#35843;&#33410;&#30340;&#20105;&#35758;&#31243;&#24230;&#65292;&#21033;&#29992;&#26465;&#20214;&#32479;&#35745;&#21644;&#31995;&#32479;&#21270;&#30340;&#35821;&#22659;&#22686;&#24378;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20248;&#21183;&#12290;&#35813;&#24179;&#21488;&#36890;&#24120;&#30001;&#19968;&#20010;&#20154;&#31867;&#20027;&#25345;&#32773;&#21644;&#20004;&#20010;&#20195;&#34920;&#20114;&#30456;&#23545;&#25239;&#31435;&#22330;&#30340;LLM&#20195;&#29702;&#32452;&#25104;&#12290;SocraSynth&#20998;&#20026;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#30693;&#35782;&#29983;&#25104;&#21644;&#25512;&#29702;&#35780;&#20272;&#12290;&#22312;&#30693;&#35782;&#29983;&#25104;&#38454;&#27573;&#65292;&#20027;&#25345;&#32773;&#23450;&#20041;&#20102;&#36777;&#35770;&#35805;&#39064;&#21644;&#20105;&#35758;&#31243;&#24230;&#65292;&#20419;&#20351;&#20195;&#29702;&#21830;&#20026;&#21508;&#33258;&#30340;&#31435;&#22330;&#21046;&#23450;&#25903;&#25345;&#24615;&#30340;&#35770;&#35777;&#12290;&#28982;&#21518;&#65292;&#22312;&#25512;&#29702;&#35780;&#20272;&#38454;&#27573;&#65292;&#37319;&#29992;&#20102;&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#21644;&#24418;&#24335;&#36923;&#36753;&#21407;&#29702;&#26469;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#35770;&#35777;&#30340;&#36136;&#37327;&#12290;&#23545;&#35805;&#20197;&#20027;&#25345;&#32773;&#35843;&#25972;&#20105;&#35758;&#31243;&#24230;&#32467;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#21160;&#24577;&#22270;&#20840;&#38754;&#25429;&#25417;&#32929;&#31080;&#20043;&#38388;&#30340;&#22810;&#26041;&#38754;&#20851;&#31995;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#65292;&#24182;&#21033;&#29992;Transformer&#32467;&#26500;&#30340;&#33021;&#21147;&#23545;&#22810;&#37325;&#20851;&#31995;&#30340;&#26102;&#38388;&#28436;&#21464;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#32929;&#31080;&#21450;&#20854;&#20851;&#32852;&#23454;&#20307;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#23436;&#25972;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.06633</link><description>&lt;p&gt;
MDGNN&#65306;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20840;&#38754;&#21644;&#21160;&#24577;&#30340;&#32929;&#31080;&#25237;&#36164;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MDGNN: Multi-Relational Dynamic Graph Neural Network for Comprehensive and Dynamic Stock Investment Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06633
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#21160;&#24577;&#22270;&#20840;&#38754;&#25429;&#25417;&#32929;&#31080;&#20043;&#38388;&#30340;&#22810;&#26041;&#38754;&#20851;&#31995;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#65292;&#24182;&#21033;&#29992;Transformer&#32467;&#26500;&#30340;&#33021;&#21147;&#23545;&#22810;&#37325;&#20851;&#31995;&#30340;&#26102;&#38388;&#28436;&#21464;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#32929;&#31080;&#21450;&#20854;&#20851;&#32852;&#23454;&#20307;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#23436;&#25972;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#24066;&#22330;&#26159;&#37329;&#34701;&#31995;&#32479;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#30001;&#20110;&#32463;&#27982;&#25351;&#26631;&#12289;&#36130;&#21153;&#25253;&#21578;&#12289;&#20840;&#29699;&#26032;&#38395;&#21644;&#25237;&#36164;&#32773;&#24773;&#32490;&#31561;&#22810;&#26041;&#38754;&#30340;&#21160;&#24577;&#21644;&#22797;&#26434;&#20851;&#31995;&#65292;&#39044;&#27979;&#32929;&#20215;&#30340;&#21464;&#21160;&#26159;&#22256;&#38590;&#30340;&#12290;&#20256;&#32479;&#30340;&#24207;&#21015;&#26041;&#27861;&#21644;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#24050;&#32463;&#24212;&#29992;&#20110;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#20294;&#23427;&#20204;&#22312;&#25429;&#25417;&#32929;&#20215;&#21464;&#21160;&#20013;&#30340;&#22810;&#26041;&#38754;&#21644;&#26102;&#38388;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#22810;&#20851;&#31995;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;MDGNN&#65289;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#31163;&#25955;&#21160;&#24577;&#22270;&#20840;&#38754;&#25429;&#25417;&#32929;&#31080;&#20043;&#38388;&#30340;&#22810;&#26041;&#38754;&#20851;&#31995;&#21450;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21464;&#12290;&#30001;&#22270;&#29983;&#25104;&#30340;&#34920;&#31034;&#25552;&#20379;&#20102;&#32929;&#31080;&#21450;&#20854;&#20851;&#32852;&#23454;&#20307;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#23436;&#25972;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#36824;&#21033;&#29992;Transformer&#32467;&#26500;&#30340;&#33021;&#21147;&#23545;&#22810;&#37325;&#20851;&#31995;&#30340;&#26102;&#38388;&#28436;&#21464;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stock market is a crucial component of the financial system, but predicting the movement of stock prices is challenging due to the dynamic and intricate relations arising from various aspects such as economic indicators, financial reports, global news, and investor sentiment. Traditional sequential methods and graph-based models have been applied in stock movement prediction, but they have limitations in capturing the multifaceted and temporal influences in stock price movements. To address these challenges, the Multi-relational Dynamic Graph Neural Network (MDGNN) framework is proposed, which utilizes a discrete dynamic graph to comprehensively capture multifaceted relations among stocks and their evolution over time. The representation generated from the graph offers a complete perspective on the interrelationships among stocks and associated entities. Additionally, the power of the Transformer structure is leveraged to encode the temporal evolution of multiplex relations, provid
&lt;/p&gt;</description></item><item><title>Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06187</link><description>&lt;p&gt;
Premier-TACO: &#36890;&#36807;&#26102;&#38388;&#39537;&#21160;&#30340;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#22810;&#20219;&#21153;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Premier-TACO: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06187
&lt;/p&gt;
&lt;p&gt;
Premier-TACO&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#26469;&#25552;&#39640;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Premier-TACO&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#20219;&#21153;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#23569;&#26679;&#26412;&#31574;&#30053;&#23398;&#20064;&#30340;&#25928;&#29575;&#12290;Premier-TACO&#21033;&#29992;&#19968;&#37096;&#20998;&#22810;&#20219;&#21153;&#31163;&#32447;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#36890;&#29992;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#29305;&#24449;&#34920;&#31034;&#25429;&#25417;&#20102;&#20851;&#38190;&#30340;&#29615;&#22659;&#21160;&#21147;&#23398;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#19987;&#23478;&#28436;&#31034;&#36827;&#34892;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#36127;&#20363;&#25277;&#26679;&#31574;&#30053;&#25512;&#21160;&#20102;&#26102;&#24207;&#34892;&#21160;&#23545;&#27604;&#23398;&#20064;&#65288;TACO&#65289;&#30446;&#26631;&#30340;&#21457;&#23637;&#65292;TACO&#22312;&#35270;&#35273;&#25511;&#21046;&#20219;&#21153;&#20013;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#26174;&#33879;&#25552;&#39640;TACO&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#37325;&#35201;&#65292;&#20351;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#31163;&#32447;&#39044;&#35757;&#32451;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;Deepmind Control Suite&#12289;MetaWorld&#21644;LIBERO&#22312;&#20869;&#30340;&#21508;&#31181;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;Premier-TACO&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#23545;&#26032;&#39062;&#21160;&#20316;&#30340;&#23569;&#26679;&#26412;&#27169;&#20223;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Premier-TACO, a multitask feature representation learning approach designed to improve few-shot policy learning efficiency in sequential decision-making tasks. Premier-TACO leverages a subset of multitask offline datasets for pretraining a general feature representation, which captures critical environmental dynamics and is fine-tuned using minimal expert demonstrations. It advances the temporal action contrastive learning (TACO) objective, known for state-of-the-art results in visual control tasks, by incorporating a novel negative example sampling strategy. This strategy is crucial in significantly boosting TACO's computational efficiency, making large-scale multitask offline pretraining feasible. Our extensive empirical evaluation in a diverse set of continuous control benchmarks including Deepmind Control Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness in pretraining visual representations, significantly enhancing few-shot imitation learning of nove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;Halpern&#36845;&#20195;&#31639;&#27861;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#36825;&#20123;&#21464;&#31181;&#23637;&#29616;&#20986;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#24182;&#19988;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#31867;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06033</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;Halpern&#36845;&#20195;&#31639;&#27861;&#21450;&#20854;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;Halpern&#36845;&#20195;&#31639;&#27861;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#36825;&#20123;&#21464;&#31181;&#23637;&#29616;&#20986;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#24182;&#19988;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#31867;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Halpern&#36845;&#20195;&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24418;&#24335;&#21644;&#21560;&#24341;&#20154;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#36817;&#24180;&#26469;&#22312;&#35299;&#20915;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;&#35813;&#26041;&#26696;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#19981;&#31934;&#30830;&#26041;&#26696;&#22312;&#65288;&#26399;&#26395;&#30340;&#65289;&#27531;&#24046;&#33539;&#25968;&#19978;&#20855;&#26377;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25918;&#23485;&#20102;&#25991;&#29486;&#20013;&#37319;&#29992;&#30340;&#26368;&#26032;&#19981;&#31934;&#30830;&#24615;&#26465;&#20214;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#31454;&#20105;&#24615;&#25910;&#25947;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20004;&#31867;&#20855;&#26377;&#20984;&#20985;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#37325;&#26500;&#30340;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20854;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods.
&lt;/p&gt;</description></item><item><title>\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.05951</link><description>&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05951
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#22312;&#21508;&#31181;&#22522;&#20934;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#29616;&#26377;&#31639;&#27861;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20048;&#35266;&#22411;Actor-Critic&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#20445;&#23432;&#22411;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#23384;&#22312;&#30340;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#38382;&#39064;&#65288;$Q$-&#20272;&#35745;&#36807;&#39640;&#20272;&#35745;&#20102;&#30495;&#23454;&#30340;$Q$&#20540;&#65289;&#12290;&#20854;&#26680;&#24515;&#20844;&#24335;&#20381;&#36182;&#20110;$Q$-&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#37319;&#29992;&#26368;&#23567;&#25209;&#27425;&#26368;&#22823;&#26368;&#23567;$Q$-&#32593;&#32476;&#36317;&#31163;&#20316;&#20026;$Q$-&#30446;&#26631;&#21152;&#20837;&#65292;&#24182;&#20316;&#20026;&#20248;&#20808;&#32423;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#35268;&#21017;&#12290;&#25105;&#20204;&#22312;TD3&#21644;TD7&#20043;&#19978;&#23454;&#26045;&#20102;\textit{MinMaxMin}&#65292;&#24182;&#23545;&#20854;&#22312;&#27969;&#34892;&#30340;MuJoCo&#21644;Bullet&#29615;&#22659;&#20013;&#23545;&#25239;&#29616;&#26377;&#30340;&#36830;&#32493;&#31354;&#38388;&#31639;&#27861;-DDPG&#65292;TD3&#21644;TD7&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25152;&#26377;&#27979;&#35797;&#20219;&#21153;&#20013;&#65292;\textit{MinMaxMin}&#30456;&#23545;&#20110;DDPG&#65292;TD3&#21644;TD7&#22343;&#34920;&#29616;&#20986;&#20102;&#31283;&#23450;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{MinMaxMin} $Q$-learning is a novel \textit{optimistic} Actor-Critic algorithm that addresses the problem of \textit{overestimation} bias ($Q$-estimations are overestimating the real $Q$-values) inherent in \textit{conservative} RL algorithms. Its core formula relies on the disagreement among $Q$-networks in the form of the min-batch MaxMin $Q$-networks distance which is added to the $Q$-target and used as the priority experience replay sampling-rule. We implement \textit{MinMaxMin} on top of TD3 and TD7, subjecting it to rigorous testing against state-of-the-art continuous-space algorithms-DDPG, TD3, and TD7-across popular MuJoCo and Bullet environments. The results show a consistent performance improvement of \textit{MinMaxMin} over DDPG, TD3, and TD7 across all tested tasks.
&lt;/p&gt;</description></item><item><title>SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05950</link><description>&lt;p&gt;
SQT - std Q-target
&lt;/p&gt;
&lt;p&gt;
\textit{SQT} -- \textit{std} $Q$-target
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05950
&lt;/p&gt;
&lt;p&gt;
SQT&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#21033;&#29992;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#65292;&#30456;&#36739;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Std Q-target&#26159;&#19968;&#31181;&#22522;&#20110;Q-&#23398;&#20064;&#30340;&#20445;&#23432;&#22411;actor-critic&#31639;&#27861;&#65292;&#23427;&#22522;&#20110;&#19968;&#20010;&#20851;&#38190;&#30340;Q&#20844;&#24335;&#65306;Q&#32593;&#32476;&#30340;&#26631;&#20934;&#24046;&#65292;&#36825;&#20010;&#26631;&#20934;&#24046;&#20316;&#20026;&#19968;&#31181;&#8220;&#19981;&#30830;&#23450;&#24615;&#24809;&#32602;&#8221;&#65292;&#26159;&#23545;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#38382;&#39064;&#30340;&#19968;&#31181;&#31616;&#32422;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;TD3/TD7&#20195;&#30721;&#30340;&#22522;&#30784;&#19978;&#23454;&#29616;&#20102;SQT&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;actor-critic&#31639;&#27861;DDPG&#12289;TD3&#21644;TD7&#22312;&#19971;&#20010;&#24120;&#35265;&#30340;MuJoCo&#21644;Bullet&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;SQT&#30340;Q-target&#20844;&#24335;&#30456;&#23545;&#20110;TD3&#30340;Q-target&#20844;&#24335;&#22312;&#35299;&#20915;&#36807;&#39640;&#20272;&#35745;&#20559;&#24046;&#30340;&#20445;&#23432;&#35299;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#65292;SQT&#30456;&#23545;&#20110;DDPG&#12289;TD3&#21644;TD7&#37117;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Std} $Q$-target is a \textit{conservative}, actor-critic, ensemble, $Q$-learning-based algorithm, which is based on a single key $Q$-formula: $Q$-networks standard deviation, which is an "uncertainty penalty", and, serves as a minimalistic solution to the problem of \textit{overestimation} bias. We implement \textit{SQT} on top of TD3/TD7 code and test it against the state-of-the-art (SOTA) actor-critic algorithms, DDPG, TD3 and TD7 on seven popular MuJoCo and Bullet tasks. Our results demonstrate \textit{SQT}'s $Q$-target formula superiority over \textit{TD3}'s $Q$-target formula as a \textit{conservative} solution to overestimation bias in RL, while \textit{SQT} shows a clear performance advantage on a wide margin over DDPG, TD3, and TD7 on all tasks.
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26368;&#26032;&#30340;YOLO V7&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#21644;&#35757;&#32451;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32958;&#33039;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#20026;&#32958;&#33039;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2402.05817</link><description>&lt;p&gt;
&#20351;&#29992;YOLO v7&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#26816;&#27979;&#32958;&#33039;&#65306;&#19968;&#31181;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#26368;&#26032;&#30340;YOLO V7&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#36827;&#34892;&#20462;&#25913;&#21644;&#35757;&#32451;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32958;&#33039;&#22312;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#26816;&#27979;&#25928;&#26524;&#65292;&#20026;&#32958;&#33039;&#30142;&#30149;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26368;&#26032;&#30340;You Only Look Once (YOLO V7)&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20013;&#22686;&#24378;&#32958;&#33039;&#26816;&#27979;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#23545;&#21307;&#23398;&#22270;&#20687;&#26684;&#24335;&#36827;&#34892;&#20462;&#25913;&#65292;&#23545;&#20462;&#25913;&#21518;&#30340;YOLO V7&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#30740;&#31350;&#21253;&#25324;878&#21517;&#19981;&#21516;&#20122;&#22411;&#30340;&#32958;&#32454;&#32990;&#30284;&#65288;RCC&#65289;&#24739;&#32773;&#21644;206&#21517;&#27491;&#24120;&#32958;&#33039;&#24739;&#32773;&#12290;&#20849;&#26816;&#32034;&#21040;1084&#21517;&#24739;&#32773;&#30340;5657&#24352;MRI&#25195;&#25551;&#12290;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#24211;&#20013;&#36873;&#25321;&#20102;326&#21517;&#24739;&#26377;1034&#20010;&#32959;&#30244;&#30340;&#24739;&#32773;&#65292;&#24182;&#22312;&#20854;&#32959;&#30244;&#21608;&#22260;&#32472;&#21046;&#20102;&#36793;&#30028;&#26694;&#12290;&#22312;&#21021;&#22987;&#27169;&#22411;&#19978;&#23545;80%&#30340;&#27880;&#37322;&#26696;&#20363;&#36827;&#34892;&#35757;&#32451;&#65292;&#20445;&#30041;20%&#29992;&#20110;&#27979;&#35797;&#65288;&#20027;&#35201;&#27979;&#35797;&#38598;&#65289;&#12290;&#28982;&#21518;&#20351;&#29992;&#26368;&#20339;&#30340;&#20027;&#35201;&#27169;&#22411;&#22312;&#20854;&#20313;861&#21517;&#24739;&#32773;&#19978;&#35782;&#21035;&#32959;&#30244;&#65292;&#24182;&#20351;&#29992;&#35813;&#27169;&#22411;&#22312;&#20854;&#25195;&#25551;&#20013;&#29983;&#25104;&#36793;&#30028;&#26694;&#22352;&#26631;&#12290;&#21019;&#24314;&#20102;&#21313;&#20010;&#22522;&#20934;&#35757;&#32451;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26410;&#20998;&#21106;&#24739;&#32773;&#19978;&#30340;&#29983;&#25104;&#22352;&#26631;&#12290;&#26368;&#32456;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#20027;&#35201;&#27979;&#35797;&#38598;&#20013;&#30340;&#32958;&#33039;&#12290;
&lt;/p&gt;
&lt;p&gt;
Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten benchmark training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05406</link><description>&lt;p&gt;
&#29616;&#22312;&#25152;&#26377;&#20154;&#37117;&#20462;&#21098;&#65306;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#38750;&#19987;&#19994;&#20174;&#19994;&#32773;&#21644;&#26368;&#23500;&#26377;&#36164;&#28304;&#30340;&#26426;&#26500;&#20043;&#38388;&#30340;&#30828;&#20214;&#24046;&#36317;&#65292;&#23610;&#23544;&#19981;&#26029;&#22686;&#38271;&#30340;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#20351;&#29992;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21387;&#32553;LLM&#65292;&#20197;&#20351;&#20854;&#36164;&#28304;&#28040;&#32791;&#21487;&#31649;&#29702;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#24448;&#24448;&#32791;&#36153;&#36164;&#28304;&#65292;&#20351;&#20854;&#30446;&#26631;&#29992;&#25143;&#32676;&#26080;&#27861;&#25509;&#35302;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#35753;&#20174;&#19994;&#32773;&#33021;&#22815;&#20462;&#21098;&#27169;&#22411;&#65292;&#20351;&#20854;&#35268;&#27169;&#22823;&#21040;&#30828;&#20214;&#20165;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#36816;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Bonsai&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#26799;&#24230;&#12289;&#25200;&#21160;&#20462;&#21098;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#23567;&#12289;&#24555;&#21644;&#20934;&#30830;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#65288;i&#65289;&#20248;&#20110;&#26356;&#26114;&#36149;&#30340;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#19982;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30456;&#27604;&#65292;&#36895;&#24230;&#24555;&#19968;&#20493;&#19988;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.05301</link><description>&lt;p&gt;
BIKED++&#65306;&#19968;&#20010;&#21253;&#21547;140&#19975;&#20010;&#33258;&#34892;&#36710;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#35774;&#35745;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BIKED++&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;140&#19975;&#20010;&#33258;&#34892;&#36710;&#35774;&#35745;&#30340;&#22270;&#20687;&#21644;&#21442;&#25968;&#21270;CAD&#25991;&#20214;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#65292;&#20363;&#22914;&#20351;&#29992;&#21442;&#25968;&#21270;&#34920;&#31034;&#26469;&#20934;&#30830;&#20272;&#35745;&#22270;&#20687;&#30340;&#29305;&#24449;&#23884;&#20837;&#12290;&#35813;&#25968;&#25454;&#38598;&#20063;&#24050;&#20844;&#24320;&#65292;&#21487;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;140&#19975;&#20010;&#36890;&#36807;&#21442;&#25968;&#21270;&#34920;&#31034;&#21644;JSON&#25991;&#20214;&#20197;&#21450;&#26629;&#26684;&#21270;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#34892;&#36710;&#35774;&#35745;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#28210;&#26579;&#24341;&#25806;&#21644;BikeCAD&#36719;&#20214;&#29983;&#25104;&#21442;&#25968;&#21270;&#35774;&#35745;&#30340;&#30690;&#37327;&#22270;&#24418;&#32780;&#21019;&#24314;&#30340;&#12290;&#26412;&#25991;&#36824;&#20844;&#24320;&#20102;&#35813;&#28210;&#26579;&#24341;&#25806;&#12290;&#35813;&#25968;&#25454;&#38598;&#20855;&#26377;&#22810;&#31181;&#24212;&#29992;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#26159;&#35757;&#32451;&#21442;&#25968;&#21270;&#21644;&#22522;&#20110;&#22270;&#20687;&#30340;&#35774;&#35745;&#34920;&#31034;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#30452;&#25509;&#20174;&#21442;&#25968;&#21270;&#34920;&#31034;&#20934;&#30830;&#20272;&#35745;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#23884;&#20837;&#12290;&#36825;&#26679;&#21487;&#20197;&#24314;&#31435;&#21442;&#25968;&#21270;&#33258;&#34892;&#36710;&#35774;&#35745;&#19982;&#25991;&#26412;&#23383;&#31526;&#20018;&#25110;&#21442;&#32771;&#22270;&#20687;&#20043;&#38388;&#30340;&#30456;&#20284;&#20851;&#31995;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#20063;&#24050;&#20844;&#24320;&#12290;&#35813;&#25968;&#25454;&#38598;&#21152;&#20837;&#20102;BIKED&#25968;&#25454;&#38598;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family whic
&lt;/p&gt;</description></item><item><title>&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.05290</link><description>&lt;p&gt;
&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#32473;&#20986;&#26356;&#22909;&#30340;&#31574;&#30053;&#26799;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformer World Models Give Better Policy Gradients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05290
&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#22870;&#21169;&#24182;&#36827;&#34892;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24378;&#21270;&#23398;&#20064;&#26469;&#35828;&#65292;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23637;&#24320;&#31070;&#32463;&#32593;&#32476;&#19990;&#30028;&#27169;&#22411;&#26469;&#39044;&#27979;&#26410;&#26469;&#30340;&#22870;&#21169;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#22270;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#20197;&#23398;&#20064;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20856;&#22411;&#30340;&#19990;&#30028;&#27169;&#22411;&#20135;&#29983;&#20102;&#38590;&#20197;&#20248;&#21270;&#30340;&#25439;&#22833;&#22320;&#24418;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#19978;&#36890;&#24120;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#21464;&#24418;&#22120;&#24050;&#30693;&#21487;&#20197;&#39640;&#25928;&#22320;&#20256;&#25773;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#26799;&#24230;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#21602;&#65311;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24120;&#29992;&#30340;&#21464;&#24418;&#22120;&#19990;&#30028;&#27169;&#22411;&#20250;&#20135;&#29983;&#36802;&#22238;&#30340;&#26799;&#24230;&#36335;&#24452;&#65292;&#36825;&#23545;&#20110;&#38271;&#36317;&#31163;&#30340;&#31574;&#30053;&#26799;&#24230;&#26159;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#31216;&#20026;Actions World Models (AWMs)&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#20379;&#26356;&#30452;&#25509;&#30340;&#26799;&#24230;&#20256;&#25773;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;AWMs&#38598;&#25104;&#21040;&#19968;&#20010;&#31574;&#30053;&#26799;&#24230;&#30340;&#26694;&#26550;&#20013;&#65292;&#24378;&#35843;&#20102;&#32593;&#32476;&#26550;&#26500;&#19982;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AWMs&#21487;&#20197;&#20135;&#29983;&#21487;&#20248;&#21270;&#30340;&#26799;&#24230;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05210</link><description>&lt;p&gt;
&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Anatomically-Controllable Medical Image Generation with Segmentation-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#20998;&#21106;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#35299;&#21078;&#21487;&#25511;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#23454;&#29616;&#23545;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#32593;&#32476;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23454;&#29616;&#20102;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23567;&#22411;&#25110;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#34917;&#20805;&#65292;&#20174;&#32780;&#24110;&#21161;&#20943;&#36731;&#33719;&#21462;&#21644;&#27880;&#37322;&#26032;&#22270;&#20687;&#30340;&#36153;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20854;&#20182;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#20687;&#26102;&#38754;&#20020;&#30528;&#20840;&#23616;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#21078;&#21487;&#25511;&#30340;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#20013;&#36981;&#24490;&#22810;&#31867;&#35299;&#21078;&#20998;&#21106;&#25513;&#27169;&#65292;&#24182;&#37319;&#29992;&#38543;&#26426;&#25513;&#27169;&#28040;&#34701;&#35757;&#32451;&#31639;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#25152;&#36873;&#35299;&#21078;&#32422;&#26463;&#30340;&#26465;&#20214;&#21270;&#65292;&#21516;&#26102;&#20801;&#35768;&#20854;&#20182;&#35299;&#21078;&#21306;&#22495;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20063;&#25913;&#21892;&#20102;&#32593;&#32476;&#22312;&#23436;&#20840;&#26080;&#26465;&#20214;&#65288;&#26080;&#32422;&#26463;&#29983;&#25104;&#65289;&#24773;&#20917;&#19979;&#23545;&#35299;&#21078;&#30495;&#23454;&#24615;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#23545;&#20083;&#33146;MRI&#21644;&#33145;&#37096;/&#39048;&#37096;&#21040;&#30406;&#33108;CT&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#35299;&#21078;&#30495;&#23454;&#24615;&#21644;&#36755;&#20837;&#25513;&#27169;&#20445;&#30495;&#24230;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have enabled remarkably high-quality medical image generation, which can help mitigate the expenses of acquiring and annotating new images by supplementing small or imbalanced datasets, along with other applications. However, these are hampered by the challenge of enforcing global anatomical realism in generated images. To this end, we propose a diffusion model for anatomically-controlled medical image generation. Our model follows a multi-class anatomical segmentation mask at each sampling step and incorporates a \textit{random mask ablation} training algorithm, to enable conditioning on a selected combination of anatomical constraints while allowing flexibility in other anatomical areas. This also improves the network's learning of anatomical realism for the completely unconditional (unconstrained generation) case. Comparative evaluation on breast MRI and abdominal/neck-to-pelvis CT datasets demonstrates superior anatomical realism and input mask faithfulness over st
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;</title><link>https://arxiv.org/abs/2402.05147</link><description>&lt;p&gt;
ApiQ&#65306;2&#20301;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
ApiQ: Finetuning of 2-Bit Quantized Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05147
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#24674;&#22797;&#37327;&#21270;&#36807;&#31243;&#20013;&#20002;&#22833;&#30340;&#20449;&#24687;&#65292;&#32500;&#25345;&#21407;&#22987;&#27169;&#22411;&#30340;&#28608;&#27963;&#31934;&#24230;&#24182;&#20943;&#36731;&#35823;&#24046;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#22823;&#65292;&#20869;&#23384;&#39640;&#25928;&#30340;&#27169;&#22411;&#24494;&#35843;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;GPU&#20869;&#23384;&#38480;&#21046;&#21644;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#30340;&#21487;&#27604;&#32467;&#26524;&#25152;&#24102;&#26469;&#30340;&#32422;&#26463;&#12290;&#23613;&#31649;&#26377;&#20102;&#36827;&#23637;&#65292;&#22914;QLoRA&#36825;&#26679;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#37327;&#21270;&#21644;&#22810;&#26679;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#36825;&#31181;&#19981;&#19968;&#33268;&#20027;&#35201;&#26469;&#33258;&#20110;&#37327;&#21270;&#36807;&#31243;&#23545;&#20445;&#30041;&#30693;&#35782;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#24494;&#35843;&#20013;&#30340;&#21033;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ApiQ&#30340;&#26032;&#22411;&#37327;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#21516;&#26102;&#21021;&#22987;&#21270;LoRA&#32452;&#20214;&#21644;&#37327;&#21270;LLM&#30340;&#26435;&#37325;&#26469;&#24674;&#22797;&#37327;&#21270;&#25439;&#22833;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#30830;&#20445;&#20102;&#21407;&#22987;LLM&#30340;&#28608;&#27963;&#31934;&#24230;&#30340;&#32500;&#25345;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#35823;&#24046;&#30340;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient finetuning of large language models (LLMs) has recently attracted huge attention with the increasing size of LLMs, primarily due to the constraints posed by GPU memory limitations and the comparable results of these methods with full finetuning. Despite the advancements, current strategies for memory-efficient finetuning, such as QLoRA, exhibit inconsistent performance across diverse bit-width quantizations and multifaceted tasks. This inconsistency largely stems from the detrimental impact of the quantization process on preserved knowledge, leading to catastrophic forgetting and undermining the utilization of pretrained models for finetuning purposes. In this work, we introduce a novel quantization framework named ApiQ, designed to restore the lost information from quantization by concurrently initializing LoRA components and quantizing the weights of LLMs. This approach ensures the maintenance of the original LLM's activation precision while mitigating the error prop
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#20840;&#38754;&#12289;&#24178;&#20928;&#19988;&#28165;&#26224;&#30340;&#20171;&#32461;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20174;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04384</link><description>&lt;p&gt;
&#22312;&#20845;&#20010;&#31616;&#21333;&#30340;&#27493;&#39588;&#20013;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models in Six Simple Steps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04384
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#20840;&#38754;&#12289;&#24178;&#20928;&#19988;&#28165;&#26224;&#30340;&#20171;&#32461;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#20102;&#20174;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#26159;&#19968;&#31867;&#38750;&#24120;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#12289;&#34507;&#30333;&#36136;&#21644;&#26448;&#26009;&#21512;&#25104;&#12289;&#22825;&#27668;&#39044;&#27979;&#21644;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#31070;&#32463;&#26367;&#20195;&#31561;&#22810;&#20010;&#38382;&#39064;&#12290;&#23613;&#31649;&#20854;&#26222;&#21450;&#24230;&#24456;&#39640;&#65292;&#20294;&#24456;&#38590;&#25214;&#21040;&#19968;&#20010;&#31616;&#21333;&#12289;&#20840;&#38754;&#12289;&#24178;&#20928;&#19988;&#28165;&#26224;&#30340;DDPM&#20171;&#32461;&#12290;&#30740;&#31350;&#35770;&#25991;&#20013;&#24517;&#35201;&#30340;&#31616;&#27905;&#35299;&#37322;&#26080;&#27861;&#38416;&#26126;&#21046;&#23450;DDPM&#25152;&#37319;&#21462;&#30340;&#19981;&#21516;&#35774;&#35745;&#27493;&#39588;&#20197;&#21450;&#30465;&#30053;&#20102;&#27493;&#39588;&#30340;&#29702;&#30001;&#20197;&#33410;&#30465;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#35770;&#36848;&#36890;&#24120;&#20174;&#21464;&#20998;&#19979;&#30028;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#36825;&#26159;&#19981;&#24517;&#35201;&#19988;&#21487;&#33021;&#26377;&#23475;&#30340;&#65292;&#22240;&#20026;&#23427;&#28151;&#28102;&#20102;&#26041;&#27861;&#22863;&#25928;&#30340;&#21407;&#22240;&#24182;&#26263;&#31034;&#20102;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#30340;&#27867;&#21270;&#24615;&#36136;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37319;&#29992;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#30340;&#35270;&#35282;&#26159;&#32654;&#20029;&#19988;&#26222;&#36941;&#30340;&#65292;&#20294;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but 
&lt;/p&gt;</description></item><item><title>RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03681</link><description>&lt;p&gt;
RL-VLM-F: &#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03681
&lt;/p&gt;
&lt;p&gt;
RL-VLM-F&#26159;&#19968;&#31181;&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#21644;&#31574;&#30053;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#35774;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#22870;&#21169;&#35774;&#35745;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#21644;&#21453;&#22797;&#35797;&#38169;&#30340;&#36807;&#31243;&#26469;&#35774;&#35745;&#26377;&#25928;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22870;&#21169;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20195;&#29702;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#21482;&#20351;&#29992;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#21644;&#20195;&#29702;&#30340;&#35270;&#35273;&#35266;&#27979;&#65292;&#24182;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#26597;&#35810;&#36825;&#20123;&#27169;&#22411;&#65292;&#22522;&#20110;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#25551;&#36848;&#32473;&#20986;&#23545;&#20195;&#29702;&#30340;&#22270;&#20687;&#35266;&#27979;&#30340;&#20559;&#22909;&#65292;&#24182;&#20174;&#20559;&#22909;&#26631;&#31614;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#32780;&#19981;&#26159;&#30452;&#25509;&#35201;&#27714;&#36825;&#20123;&#27169;&#22411;&#36755;&#20986;&#21407;&#22987;&#22870;&#21169;&#20998;&#25968;&#65292;&#36825;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;RL-VLM-F&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25104;&#21151;&#22320;&#20135;&#29983;&#20102;&#26377;&#25928;&#30340;&#22870;&#21169;&#21644;&#31574;&#30053;&#65292;&#21253;&#25324;&#32463;&#20856;&#25511;&#21046;&#20197;&#21450;&#21018;&#24615;&#21644;&#28789;&#27963;&#25805;&#32437;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03570</link><description>&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion World Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03570
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#26159;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#22810;&#27493;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#30340;&#26465;&#20214;&#24615;&#25193;&#25955;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#25928;&#26524;&#21644;&#24615;&#33021;&#26041;&#38754;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25193;&#25955;&#19990;&#30028;&#27169;&#22411;&#65288;DWM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#39044;&#27979;&#22810;&#27493;&#30340;&#26410;&#26469;&#29366;&#24577;&#21644;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#30456;&#21453;&#65292;DWM&#36890;&#36807;&#21333;&#20010;&#21069;&#21521;&#20256;&#36882;&#25552;&#20379;&#20102;&#38271;&#26102;&#31243;&#30340;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#36882;&#24402;&#26597;&#35810;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;DWM&#25972;&#21512;&#21040;&#22522;&#20110;&#27169;&#22411;&#30340;&#20215;&#20540;&#20272;&#35745;&#20013;&#65292;&#20854;&#20013;&#30701;&#26399;&#22238;&#25253;&#36890;&#36807;&#20174;DWM&#20013;&#37319;&#26679;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#27169;&#25311;&#12290;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#32972;&#26223;&#19979;&#65292;DWM&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#29983;&#25104;&#24314;&#27169;&#26469;&#23454;&#29616;&#20445;&#23432;&#30340;&#20540;&#27491;&#21017;&#21270;&#12290;&#21478;&#22806;&#65292;&#23427;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#25968;&#25454;&#28304;&#65292;&#20351;&#31163;&#32447;Q&#23398;&#20064;&#33021;&#22815;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;D4RL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;DWM&#23545;&#38271;&#26102;&#31243;&#27169;&#25311;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#32477;&#23545;&#24615;&#33021;&#26041;&#38754;&#65292;DWM&#26174;&#33879;&#36229;&#36807;&#20102;&#19968;&#27493;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;44%&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Diffusion World Model (DWM), a conditional diffusion model capable of predicting multistep future states and rewards concurrently. As opposed to traditional one-step dynamics models, DWM offers long-horizon predictions in a single forward pass, eliminating the need for recursive quires. We integrate DWM into model-based value estimation, where the short-term return is simulated by future trajectories sampled from DWM. In the context of offline reinforcement learning, DWM can be viewed as a conservative value regularization through generative modeling. Alternatively, it can be seen as a data source that enables offline Q-learning with synthetic data. Our experiments on the D4RL dataset confirm the robustness of DWM to long-horizon simulation. In terms of absolute performance, DWM significantly surpasses one-step dynamics models with a $44\%$ performance gain, and achieves state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03312</link><description>&lt;p&gt;
&#28145;&#24230;&#34917;&#20840;&#30340;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Test-Time Adaptation for Depth Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03312
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#21333;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#38388;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#23558;&#22312;&#19968;&#20123;&#65288;&#28304;&#65289;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#30446;&#26631;&#27979;&#35797;&#25968;&#25454;&#26102;&#65292;&#24120;&#24120;&#20250;&#35266;&#23519;&#21040;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#39046;&#22495;&#24046;&#36317;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#22914;&#39046;&#22495;&#36866;&#24212;&#65288;DA&#65289;&#65292;&#21487;&#33021;&#38656;&#35201;&#27169;&#22411;&#35757;&#32451;&#26102;&#20351;&#29992;&#30340;&#28304;&#25968;&#25454;&#65288;&#36890;&#24120;&#19981;&#21487;&#29992;&#65289;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;&#26080;&#28304;DA&#65292;&#21017;&#38656;&#35201;&#22810;&#27425;&#36890;&#36807;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#28145;&#24230;&#34917;&#20840;&#65292;&#21363;&#20174;&#21333;&#20010;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#31232;&#30095;&#28145;&#24230;&#22270;&#25512;&#26029;&#20986;&#23494;&#38598;&#28145;&#24230;&#22270;&#30340;&#20219;&#21153;&#65292;&#20197;&#22312;&#19968;&#27425;&#36890;&#36807;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#27599;&#31181;&#25968;&#25454;&#27169;&#24577;&#20013;&#30340;&#39046;&#22495;&#36716;&#31227;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#31232;&#30095;&#28145;&#24230;&#27169;&#24577;&#23637;&#29616;&#20986;&#27604;&#22270;&#20687;&#26356;&#23567;&#30340;&#21327;&#21464;&#37327;&#36716;&#31227;&#65292;&#22240;&#27492;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22312;&#28304;&#39046;&#22495;&#20013;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22359;&#65292;&#23427;&#20445;&#30041;&#20102;&#20174;&#20165;&#32534;&#30721;&#31232;&#30095;&#28145;&#24230;&#29305;&#24449;&#21040;&#32534;&#30721;&#22270;&#20687;&#21644;&#31232;&#30095;&#28145;&#24230;&#30340;&#29305;&#24449;&#30340;&#26144;&#23556;&#12290;&#22312;&#27979;&#35797;&#26102;&#38388;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#23884;&#20837;&#27169;&#22359;&#23454;&#29616;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During t
&lt;/p&gt;</description></item><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#30740;&#31350;&#20102;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20998;&#35299;&#23376;&#28216;&#25103;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#24320;&#20102;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02954</link><description>&lt;p&gt;
&#35299;&#20915;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65306;&#19968;&#31181;&#24191;&#20041;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#30740;&#31350;&#20102;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20998;&#35299;&#23376;&#28216;&#25103;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#24320;&#20102;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#22810;&#20154;&#20998;&#25955;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#21333;&#20154;&#28216;&#25103;&#65292;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36125;&#23572;&#26364;&#30340;&#26368;&#20248;&#24615;&#21407;&#29702;&#36890;&#36807;&#23558;&#20854;&#20998;&#35299;&#20026;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#26469;&#35299;&#20915;&#21333;&#20154;&#28216;&#25103;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#27599;&#20010;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20013;&#32416;&#32544;&#20102;&#25152;&#26377;&#29609;&#23478;&#30340;&#20915;&#31574;&#21464;&#37327;&#65292;&#23548;&#33268;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#22791;&#20221;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#21069;&#25552;&#19979;&#35299;&#24320;&#36825;&#20123;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#36825;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#31649;&#29702;&#39118;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#36890;&#36807;&#36827;&#19968;&#27493;&#23558;&#20219;&#20309;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#28216;&#25103;&#26469;&#35299;&#20915;&#23427;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36880;&#27425;&#36827;&#34892;&#21333;&#20154;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20013;&#30340;&#24191;&#20041;&#21338;&#24328;&#35299;&#20915;&#26041;&#26696;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#35299;&#20915;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our expe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02823</link><description>&lt;p&gt;
&#36867;&#36991;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#65288;&#22826;&#65289;&#23481;&#26131;
&lt;/p&gt;
&lt;p&gt;
Evading Data Contamination Detection for Language Models is (too) Easy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25351;&#20986;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#27745;&#26579;&#30340;&#26816;&#27979;&#26041;&#27861;&#22312;&#38754;&#23545;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#30340;&#26377;&#24847;&#27745;&#26579;&#26102;&#23384;&#22312;&#28431;&#27934;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65288;EAL&#65289;&#26469;&#26174;&#33879;&#25552;&#39640;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#19988;&#36867;&#36991;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#65292;&#23427;&#20204;&#22312;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#32463;&#24120;&#25351;&#23548;&#29992;&#25143;&#23545;&#19968;&#20010;&#27169;&#22411;&#19982;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25152;&#35757;&#32451;&#30340;&#22823;&#37327;&#25968;&#25454;&#21487;&#33021;&#20250;&#24847;&#22806;&#22320;&#19982;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#21457;&#29983;&#27745;&#26579;&#65292;&#20174;&#32780;&#25439;&#23475;&#24615;&#33021;&#35780;&#20272;&#12290;&#23613;&#31649;&#26368;&#36817;&#24320;&#21457;&#20102;&#19968;&#20123;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#24694;&#24847;&#27169;&#22411;&#25552;&#20379;&#32773;&#26377;&#24847;&#36827;&#34892;&#27745;&#26579;&#20197;&#36991;&#20813;&#34987;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#24773;&#20917;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#23545;&#20844;&#20849;&#22522;&#20934;&#27979;&#35797;&#30340;&#21487;&#20449;&#24230;&#20135;&#29983;&#20102;&#24576;&#30097;&#12290;&#20026;&#20102;&#26356;&#20005;&#26684;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25552;&#20379;&#32773;&#21644;&#27745;&#26579;&#26816;&#27979;&#26041;&#27861;&#30340;&#20998;&#31867;&#65292;&#36825;&#25581;&#31034;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#28431;&#27934;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;EAL&#36825;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27745;&#26579;&#25216;&#26415;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#65292;&#24182;&#23436;&#20840;&#36867;&#36991;&#20102;&#24403;&#21069;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are widespread, with their performance on benchmarks frequently guiding user preferences for one model over another. However, the vast amount of data these models are trained on can inadvertently lead to contamination with public benchmarks, thus compromising performance measurements. While recently developed contamination detection methods try to address this issue, they overlook the possibility of deliberate contamination by malicious model providers aiming to evade detection. We argue that this setting is of crucial importance as it casts doubt on the reliability of public benchmarks. To more rigorously study this issue, we propose a categorization of both model providers and contamination detection methods. This reveals vulnerabilities in existing methods that we exploit with EAL, a simple yet effective contamination technique that significantly inflates benchmark performance while completely evading current detection methods.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#24212;&#29992;&#20197;&#21450;&#34701;&#21512;&#25216;&#26415;&#30340;&#35780;&#20272;&#12290;&#37325;&#28857;&#20851;&#27880;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.02460</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of multimodal machine learning approaches in healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02460
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20027;&#35201;&#20171;&#32461;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#36890;&#36807;&#32508;&#21512;&#20998;&#26512;&#26368;&#36817;&#30340;&#25991;&#29486;&#65292;&#25506;&#35752;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#24212;&#29992;&#20197;&#21450;&#34701;&#21512;&#25216;&#26415;&#30340;&#35780;&#20272;&#12290;&#37325;&#28857;&#20851;&#27880;&#24433;&#20687;&#25968;&#25454;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20256;&#32479;&#19978;&#27880;&#37325;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#38480;&#21046;&#20102;&#20854;&#26377;&#25928;&#22797;&#21046;&#20020;&#24202;&#23454;&#36341;&#20013;&#25972;&#21512;&#22810;&#31181;&#20449;&#24687;&#26469;&#28304;&#20197;&#25913;&#21892;&#20915;&#31574;&#30340;&#33021;&#21147;&#12290;&#20020;&#24202;&#21307;&#29983;&#36890;&#24120;&#20381;&#36182;&#21508;&#31181;&#25968;&#25454;&#26469;&#28304;&#65292;&#21253;&#25324;&#24739;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#23454;&#39564;&#23460;&#25968;&#25454;&#12289;&#29983;&#21629;&#20307;&#24449;&#21644;&#21508;&#31181;&#24433;&#20687;&#25968;&#25454;&#27169;&#24577;&#26469;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#24182;&#23545;&#20854;&#21457;&#29616;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26356;&#39640;&#25928;&#34701;&#21512;&#65292;&#20174;&#32780;&#20135;&#29983;&#26356;&#22909;&#22320;&#20195;&#34920;&#21307;&#29983;&#26041;&#27861;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21307;&#30103;&#20445;&#20581;&#20013;&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#32508;&#36848;&#65292;&#20840;&#38754;&#27010;&#36848;&#20102;&#26368;&#36817;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20020;&#24202;&#35786;&#26029;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65292;&#29305;&#21035;&#24378;&#35843;&#24433;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#34701;&#21512;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#30740;&#31350;&#24120;&#35265;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning methods in healthcare have traditionally focused on using data from a single modality, limiting their ability to effectively replicate the clinical practice of integrating multiple sources of information for improved decision making. Clinicians typically rely on a variety of data sources including patients' demographic information, laboratory data, vital signs and various imaging data modalities to make informed decisions and contextualise their findings. Recent advances in machine learning have facilitated the more efficient incorporation of multimodal data, resulting in applications that better represent the clinician's approach. Here, we provide a review of multimodal machine learning approaches in healthcare, offering a comprehensive overview of recent literature. We discuss the various data modalities used in clinical diagnosis, with a particular emphasis on imaging data. We evaluate fusion techniques, explore existing multimodal datasets and examine common traini
&lt;/p&gt;</description></item><item><title>MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02342</link><description>&lt;p&gt;
MetaOptimize&#65306;&#19968;&#20010;&#20248;&#21270;&#27493;&#38271;&#21644;&#20854;&#20182;&#20803;&#21442;&#25968;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MetaOptimize: A Framework for Optimizing Step Sizes and Other Meta-parameters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02342
&lt;/p&gt;
&lt;p&gt;
MetaOptimize&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#20803;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20013;&#20248;&#21270;&#20803;&#21442;&#25968;&#65288;&#21363;&#36229;&#21442;&#25968;&#65289;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#24433;&#21709;&#35757;&#32451;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;MetaOptimize&#26694;&#26550;&#65292;&#25670;&#33073;&#20102;&#35745;&#31639;&#26114;&#36149;&#30340;&#20256;&#32479;&#20803;&#21442;&#25968;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#20803;&#21442;&#25968;&#65292;&#29305;&#21035;&#26159;&#27493;&#38271;&#65288;&#20063;&#31216;&#20026;&#23398;&#20064;&#29575;&#65289;&#65292;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MetaOptimize&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23454;&#26102;&#35843;&#25972;&#27493;&#38271;&#65292;&#36890;&#36807;&#26410;&#26469;&#25439;&#22833;&#30340;&#25240;&#29616;&#24635;&#21644;&#26469;&#26368;&#23567;&#21270;&#19968;&#31181;&#29305;&#23450;&#24418;&#24335;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;MetaOptimize&#30340;&#20302;&#22797;&#26434;&#24230;&#21464;&#20307;&#65292;&#32467;&#21512;&#20854;&#36866;&#24212;&#22810;&#20010;&#20248;&#21270;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#23398;&#20064;&#29575;&#35745;&#21010;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the challenge of optimizing meta-parameters (i.e., hyperparameters) in machine learning algorithms, a critical factor influencing training efficiency and model performance. Moving away from the computationally expensive traditional meta-parameter search methods, we introduce MetaOptimize framework that dynamically adjusts meta-parameters, particularly step sizes (also known as learning rates), during training. More specifically, MetaOptimize can wrap around any first-order optimization algorithm, tuning step sizes on the fly to minimize a specific form of regret that accounts for long-term effect of step sizes on training, through a discounted sum of future losses. We also introduce low complexity variants of MetaOptimize that, in conjunction with its adaptability to multiple optimization algorithms, demonstrate performance competitive to those of best hand-crafted learning rate schedules across various machine learning applications.
&lt;/p&gt;</description></item><item><title>INViT&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#23884;&#22871;&#35774;&#35745;&#21644;&#19981;&#21464;&#30340;&#35270;&#22270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#25552;&#39640;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02317</link><description>&lt;p&gt;
INViT:&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#21487;&#27867;&#21270;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
INViT: A Generalizable Routing Problem Solver with Invariant Nested View Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02317
&lt;/p&gt;
&lt;p&gt;
INViT&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#30340;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21046;&#23884;&#22871;&#35774;&#35745;&#21644;&#19981;&#21464;&#30340;&#35270;&#22270;&#65292;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#25552;&#39640;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23398;&#20064;&#35299;&#20915;&#36335;&#30001;&#38382;&#39064;&#30340;&#24555;&#36895;&#21551;&#21457;&#24335;&#26041;&#27861;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#27714;&#35299;&#22120;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#20998;&#24067;&#25110;&#20855;&#26377;&#19981;&#21516;&#35268;&#27169;&#30340;&#20998;&#24067;&#26102;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#19981;&#21464;&#23884;&#22871;&#35270;&#22270;&#36716;&#25442;&#22120;&#65288;INViT&#65289;&#65292;&#23427;&#26088;&#22312;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20869;&#37096;&#24378;&#21046;&#19968;&#20010;&#23884;&#22871;&#35774;&#35745;&#20197;&#21450;&#19981;&#21464;&#30340;&#35270;&#22270;&#26469;&#20419;&#36827;&#23398;&#20064;&#27714;&#35299;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23427;&#24212;&#29992;&#20102;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#24182;&#32467;&#21512;&#20102;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;INViT&#22312;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21644;&#19981;&#21516;&#38382;&#39064;&#35268;&#27169;&#30340;TSP&#21644;CVRP&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep reinforcement learning has shown promising results for learning fast heuristics to solve routing problems. Meanwhile, most of the solvers suffer from generalizing to an unseen distribution or distributions with different scales. To address this issue, we propose a novel architecture, called Invariant Nested View Transformer (INViT), which is designed to enforce a nested design together with invariant views inside the encoders to promote the generalizability of the learned solver. It applies a modified policy gradient algorithm enhanced with data augmentations. We demonstrate that the proposed INViT achieves a dominant generalization performance on both TSP and CVRP problems with various distributions and different problem scales.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#29983;&#25104;AI&#27169;&#22411;&#22312;&#29983;&#25104;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26102;&#22312;&#39057;&#22495;&#20013;&#30340;DCT&#31995;&#25968;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#8220;&#36776;&#21035;&#25351;&#32441;&#8221;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.02209</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#29983;&#25104;AI&#39046;&#22495;&#21033;&#29992;DCT&#36712;&#36857;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
On the Exploitation of DCT-Traces in the Generative-AI Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#29983;&#25104;AI&#27169;&#22411;&#22312;&#29983;&#25104;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#26102;&#22312;&#39057;&#22495;&#20013;&#30340;DCT&#31995;&#25968;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#8220;&#36776;&#21035;&#25351;&#32441;&#8221;&#65292;&#21487;&#20197;&#21033;&#29992;&#23427;&#26469;&#25913;&#21892;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#23545;&#20110;&#32593;&#32476;&#23433;&#20840;&#21644;&#25968;&#23383;&#21462;&#35777;&#39046;&#22495;&#26469;&#35828;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#26368;&#36817;&#22522;&#20110;&#29983;&#25104;AI&#30340;&#35299;&#20915;&#26041;&#26696;&#25152;&#33719;&#24471;&#30340;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;&#20960;&#20046;&#25152;&#26377;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30041;&#19979;&#20102;&#29420;&#29305;&#30340;&#30165;&#36857;&#65292;&#22914;&#26524;&#23545;&#20854;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#21644;&#35782;&#21035;&#65292;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#30165;&#36857;&#26469;&#25913;&#21892;&#29616;&#26377;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#30340;&#27867;&#21270;&#38480;&#21046;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#30001;GAN&#21644;&#25193;&#25955;&#27169;&#22411;&#24341;&#25806;&#29983;&#25104;&#30340;&#28145;&#24230;&#20266;&#36896;&#22270;&#20687;&#22312;&#39057;&#22495;&#20013;&#30340;&#29305;&#24449;&#65292;&#35814;&#32454;&#30740;&#31350;&#20102;&#31163;&#25955;&#20313;&#24358;&#21464;&#25442;(DCT)&#31995;&#25968;&#30340;&#32479;&#35745;&#20998;&#24067;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#24182;&#38750;&#25152;&#26377;&#31995;&#25968;&#23545;&#22270;&#20687;&#26816;&#27979;&#30340;&#36129;&#29486;&#30456;&#21516;&#65292;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#19968;&#31181;&#29420;&#29305;&#30340;&#8220;&#36776;&#21035;&#25351;&#32441;&#8221;&#65292;&#23884;&#20837;&#22312;&#29305;&#23450;&#31995;&#25968;&#32452;&#21512;&#20013;&#12290;&#20026;&#20102;&#35782;&#21035;&#23427;&#20204;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#31995;&#25968;&#32452;&#21512;&#36827;&#34892;&#20102;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20102;&#21487;&#35299;&#37322;AI(XAI)&#30340;LIME&#31639;&#27861;&#26469;&#25628;&#32034;...
&lt;/p&gt;
&lt;p&gt;
Deepfakes represent one of the toughest challenges in the world of Cybersecurity and Digital Forensics, especially considering the high-quality results obtained with recent generative AI-based solutions. Almost all generative models leave unique traces in synthetic data that, if analyzed and identified in detail, can be exploited to improve the generalization limitations of existing deepfake detectors. In this paper we analyzed deepfake images in the frequency domain generated by both GAN and Diffusion Model engines, examining in detail the underlying statistical distribution of Discrete Cosine Transform (DCT) coefficients. Recognizing that not all coefficients contribute equally to image detection, we hypothesize the existence of a unique "discriminative fingerprint", embedded in specific combinations of coefficients. To identify them, Machine Learning classifiers were trained on various combinations of coefficients. In addition, the Explainable AI (XAI) LIME algorithm was used to sea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02047</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Quality and Trust in LLM-generated Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#29983;&#25104;&#20195;&#30721;&#30340;&#36136;&#37327;&#21644;&#20449;&#20219;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#30830;&#23450;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24120;&#24120;&#20250;&#20986;&#38169;&#12290;&#29992;&#25143;&#38656;&#35201;&#21487;&#38752;&#30340;&#25351;&#31034;&#65292;&#20197;&#30830;&#23450;&#32473;&#23450;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#21542;&#21487;&#20449;&#65292;&#20174;&#32780;&#21487;&#20197;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#26159;&#21542;&#20351;&#29992;&#35813;&#36755;&#20986;&#12290;&#20363;&#22914;&#65292;&#21487;&#20197;&#23558;&#36755;&#20986;&#19982;&#32622;&#20449;&#24230;&#30456;&#20851;&#32852;&#65307;&#22914;&#26524;&#32622;&#20449;&#24230;&#19982;&#27491;&#30830;&#24615;&#30340;&#21487;&#33021;&#24615;&#24378;&#30456;&#20851;&#65292;&#21017;&#31216;&#35813;&#27169;&#22411;&#20026;&#33391;&#22909;&#26657;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39640;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#23433;&#20840;&#25509;&#21463;&#65292;&#20302;&#32622;&#20449;&#24230;&#30340;&#36755;&#20986;&#21487;&#20197;&#25298;&#32477;&#12290;&#26657;&#20934;&#36804;&#20170;&#20027;&#35201;&#22312;&#38750;&#29983;&#25104;&#24615;&#65288;&#20363;&#22914;&#20998;&#31867;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#24456;&#23481;&#26131;&#20986;&#38169;&#65306;&#24320;&#21457;&#20154;&#21592;&#38656;&#35201;&#30693;&#36947;&#20309;&#26102;&#30452;&#25509;&#20351;&#29992;&#12289;&#32463;&#36807;&#20180;&#32454;&#23457;&#26597;&#21518;&#20351;&#29992;&#25110;&#20002;&#24323;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#65292;&#22240;&#27492;&#22312;&#29983;&#25104;&#29615;&#22659;&#20013;&#65292;&#26657;&#20934;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#27010;&#24565;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#27492;&#26657;&#20934;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.01713</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#20419;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#20102;&#20854;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#36890;&#36807;&#32771;&#34385;EHR&#29305;&#24449;&#21644;&#20020;&#24202;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#20351;&#20854;&#19982;&#20256;&#32479;&#19978;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#32780;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25972;&#21512;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21463;&#26032;&#30142;&#30149;&#29190;&#21457;&#26102;&#36805;&#36895;&#20915;&#31574;&#30340;&#32039;&#36843;&#38656;&#27714;&#30340;&#39537;&#20351;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#31867;&#20284;GPT-4&#30340;LLM&#23545;EHR&#25968;&#25454;&#30340;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#12290;&#38024;&#23545;EHR&#25968;&#25454;&#30340;&#32437;&#21521;&#12289;&#31232;&#30095;&#21644;&#30693;&#35782;&#27880;&#20837;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#30340;&#25552;&#31034;&#26041;&#27861;&#32771;&#34385;&#20102;&#29305;&#23450;&#30340;EHR&#29305;&#24449;&#65292;&#22914;&#21333;&#20301;&#21644;&#21442;&#32771;&#33539;&#22260;&#65292;&#24182;&#37319;&#29992;&#20102;&#19982;&#20020;&#24202;&#19978;&#19979;&#25991;&#30456;&#19968;&#33268;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#12290;&#36890;&#36807;&#22312;MIMIC-IV&#21644;TJH&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LLM&#33021;&#22815;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#39044;&#27979;&#65292;&#26377;&#25928;&#24212;&#23545;&#20102;EHR&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The inherent complexity of structured longitudinal Electronic Health Records (EHR) data poses a significant challenge when integrated with Large Language Models (LLMs), which are traditionally tailored for natural language processing. Motivated by the urgent need for swift decision-making during new disease outbreaks, where traditional predictive models often fail due to a lack of historical data, this research investigates the adaptability of LLMs, like GPT-4, to EHR data. We particularly focus on their zero-shot capabilities, which enable them to make predictions in scenarios in which they haven't been explicitly trained. In response to the longitudinal, sparse, and knowledge-infused nature of EHR data, our prompting approach involves taking into account specific EHR characteristics such as units and reference ranges, and employing an in-context learning strategy that aligns with clinical contexts. Our comprehensive experiments on the MIMIC-IV and TJH datasets demonstrate that with o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;2017&#24180;&#30340;&#21830;&#21697;&#27969;&#21160;&#35843;&#26597;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#26500;&#24314;&#26412;&#22320;&#27169;&#22411;&#12289;&#25552;&#21462;&#22320;&#29702;&#29305;&#24449;&#21644;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;92%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00654</link><description>&lt;p&gt;
&#25552;&#39640;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65306;&#22522;&#20110;2017&#24180;CFS PUF&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Improving the accuracy of freight mode choice models: A case study using the 2017 CFS PUF data set and ensemble learning techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;2017&#24180;&#30340;&#21830;&#21697;&#27969;&#21160;&#35843;&#26597;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#25968;&#25454;&#38598;&#21644;&#38598;&#25104;&#23398;&#20064;&#25216;&#26415;&#65292;&#25913;&#36827;&#20102;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21253;&#25324;&#26500;&#24314;&#26412;&#22320;&#27169;&#22411;&#12289;&#25552;&#21462;&#22320;&#29702;&#29305;&#24449;&#21644;&#24212;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;92%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#25910;&#38598;&#20102;&#20004;&#36718;&#23454;&#39564;&#25968;&#25454;&#65292;&#25552;&#20379;&#20102;&#20840;&#22269;&#21830;&#21697;&#27969;&#21160;&#30340;&#36816;&#36755;&#29305;&#24449;&#65292;&#20998;&#21035;&#22312;2012&#24180;&#65288;&#21363;&#20844;&#20849;&#20351;&#29992;&#24494;&#25968;&#25454;&#65289;&#21644;2017&#24180;&#65288;&#21363;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#65289;&#21457;&#24067;&#12290;&#22522;&#20110;&#36825;&#20123;&#20449;&#24687;&#65292;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;&#29702;&#35299;&#36135;&#36816;&#29289;&#27969;&#30340;&#35814;&#32454;&#27169;&#24335;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;2017&#24180;&#30340;&#21830;&#21697;&#27969;&#21160;&#35843;&#26597;&#20844;&#20849;&#20351;&#29992;&#25991;&#20214;&#25968;&#25454;&#38598;&#65292;&#25506;&#32034;&#26500;&#24314;&#19968;&#20010;&#39640;&#24615;&#33021;&#30340;&#36135;&#36816;&#27169;&#24335;&#36873;&#25321;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#19977;&#20010;&#20027;&#35201;&#25913;&#36827;&#65306;&#65288;1&#65289;&#20026;&#27599;&#20010;&#29420;&#31435;&#30340;&#21830;&#21697;/&#34892;&#19994;&#31867;&#21035;&#26500;&#24314;&#26412;&#22320;&#27169;&#22411;&#65307;&#65288;2&#65289;&#25552;&#21462;&#26377;&#29992;&#30340;&#22320;&#29702;&#29305;&#24449;&#65292;&#23588;&#20854;&#26159;&#27599;&#31181;&#36135;&#36816;&#27169;&#24335;&#22312;&#36215;&#28857;/&#32456;&#28857;&#21306;&#22495;&#20043;&#38388;&#30340;&#34893;&#29983;&#36317;&#31163;&#65307;&#65288;3&#65289;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#22534;&#21472;&#25110;&#25237;&#31080;&#65292;&#23558;&#26412;&#22320;&#21644;&#32479;&#19968;&#27169;&#22411;&#30340;&#32467;&#26524;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20869;&#23384;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36229;&#36807;92%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The US Census Bureau has collected two rounds of experimental data from the Commodity Flow Survey, providing shipment-level characteristics of nationwide commodity movements, published in 2012 (i.e., Public Use Microdata) and in 2017 (i.e., Public Use File). With this information, data-driven methods have become increasingly valuable for understanding detailed patterns in freight logistics. In this study, we used the 2017 Commodity Flow Survey Public Use File data set to explore building a high-performance freight mode choice model, considering three main improvements: (1) constructing local models for each separate commodity/industry category; (2) extracting useful geographical features, particularly the derived distance of each freight mode between origin/destination zones; and (3) applying additional ensemble learning methods such as stacking or voting to combine results from local and unified models for improved performance. The proposed method achieved over 92% accuracy without in
&lt;/p&gt;</description></item><item><title>PirateNets&#26159;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#22312;&#36739;&#22823;&#28145;&#24230;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00326</link><description>&lt;p&gt;
PirateNets&#65306;&#37319;&#29992;&#27531;&#24046;&#33258;&#36866;&#24212;&#32593;&#32476;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PirateNets: Physics-informed Deep Learning with Residual Adaptive Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00326
&lt;/p&gt;
&lt;p&gt;
PirateNets&#26159;&#19968;&#31181;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#22810;&#23618;&#24863;&#30693;&#26426;&#32593;&#32476;&#22312;&#36739;&#22823;&#28145;&#24230;&#26102;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#23454;&#29616;&#20102;&#31283;&#23450;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#24182;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#31070;&#32463;&#32593;&#32476;(PINNs)&#24050;&#25104;&#20026;&#35299;&#20915;&#30001;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#25511;&#21046;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#38382;&#39064;&#30340;&#27969;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20294;&#22312;&#37319;&#29992;&#26356;&#22823;&#21644;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#21453;&#30452;&#35273;&#34892;&#20026;&#30340;&#26681;&#28304;&#22312;&#20110;&#20351;&#29992;&#19981;&#36866;&#21512;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#32593;&#32476;&#32467;&#26500;&#65292;&#23548;&#33268;&#32593;&#32476;&#23548;&#25968;&#30340;&#21487;&#35757;&#32451;&#24615;&#36739;&#24046;&#65292;&#24182;&#26368;&#32456;&#23548;&#33268;PDE&#27531;&#24046;&#25439;&#22833;&#30340;&#19981;&#31283;&#23450;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#27531;&#24046;&#33258;&#36866;&#24212;&#32593;&#32476;(PirateNets)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#26550;&#26500;&#65292;&#26088;&#22312;&#20419;&#36827;&#28145;&#24230;PINN&#27169;&#22411;&#30340;&#31283;&#23450;&#21644;&#39640;&#25928;&#35757;&#32451;&#12290;PirateNets&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#27531;&#24046;&#36830;&#25509;&#65292;&#20801;&#35768;&#32593;&#32476;&#20316;&#20026;&#27973;&#23618;&#32593;&#32476;&#36827;&#34892;&#21021;&#22987;&#21270;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#21152;&#28145;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#21487;&#20197;&#25552;&#39640;PINN&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#24182;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While physics-informed neural networks (PINNs) have become a popular deep learning framework for tackling forward and inverse problems governed by partial differential equations (PDEs), their performance is known to degrade when larger and deeper neural network architectures are employed. Our study identifies that the root of this counter-intuitive behavior lies in the use of multi-layer perceptron (MLP) architectures with non-suitable initialization schemes, which result in poor trainablity for the network derivatives, and ultimately lead to an unstable minimization of the PDE residual loss. To address this, we introduce Physics-informed Residual Adaptive Networks (PirateNets), a novel architecture that is designed to facilitate stable and efficient training of deep PINN models. PirateNets leverage a novel adaptive residual connection, which allows the networks to be initialized as shallow networks that progressively deepen during training. We also show that the proposed initializatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2401.17823</link><description>&lt;p&gt;
&#37319;&#29992;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21457;&#24067;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving data release leveraging optimal transport and particle gradient descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#21512;&#25104;&#26041;&#27861;PrivPGD&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#29305;&#23450;&#30340;&#39046;&#22495;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#20851;&#38190;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#25919;&#24220;&#65289;&#20013;&#38544;&#31169;&#30340;&#34920;&#26684;&#25968;&#25454;&#24046;&#20998;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#20219;&#21153;&#12290;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#22522;&#20110;&#36793;&#38469;&#30340;&#26041;&#27861;&#65292;&#20174;&#31169;&#26377;&#36793;&#38469;&#20272;&#35745;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PrivPGD&#65292;&#19968;&#31181;&#22522;&#20110;&#36793;&#38469;&#30340;&#31169;&#26377;&#25968;&#25454;&#21512;&#25104;&#30340;&#26032;&#19968;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#26368;&#20248;&#36755;&#36816;&#21644;&#31890;&#23376;&#26799;&#24230;&#19979;&#38477;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22823;&#33539;&#22260;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#39640;&#24230;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#32467;&#21512;&#20854;&#20182;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach for differentially private data synthesis of protected tabular datasets, a relevant task in highly sensitive domains such as healthcare and government. Current state-of-the-art methods predominantly use marginal-based approaches, where a dataset is generated from private estimates of the marginals. In this paper, we introduce PrivPGD, a new generation method for marginal-based private data synthesis, leveraging tools from optimal transport and particle gradient descent. Our algorithm outperforms existing methods on a large range of datasets while being highly scalable and offering the flexibility to incorporate additional domain-specific constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#22312;&#32479;&#35745;&#36136;&#37327;&#12289;&#25968;&#20540;&#21487;&#37325;&#22797;&#24615;&#12289;&#26102;&#38388;&#25928;&#29575;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#26041;&#38754;&#19982;&#21407;&#22987;C&#23454;&#29616;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2401.17345</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#30340;&#21487;&#37325;&#22797;&#24615;&#12289;&#33021;&#25928;&#21644;&#24615;&#33021;&#65306;&#23545;Python&#12289;NumPy&#12289;TensorFlow&#21644;PyTorch&#23454;&#29616;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reproducibility, energy efficiency and performance of pseudorandom number generators in machine learning: a comparative study of python, numpy, tensorflow, and pytorch implementations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#22312;&#32479;&#35745;&#36136;&#37327;&#12289;&#25968;&#20540;&#21487;&#37325;&#22797;&#24615;&#12289;&#26102;&#38388;&#25928;&#29575;&#21644;&#33021;&#28304;&#28040;&#32791;&#31561;&#26041;&#38754;&#19982;&#21407;&#22987;C&#23454;&#29616;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;(PRNGs)&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#20247;&#22810;&#26041;&#27861;&#20013;&#37117;&#38750;&#24120;&#26377;&#24847;&#24605;&#12290;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#21462;&#24471;&#31361;&#30772;&#24615;&#36827;&#23637;&#65292;&#27604;&#22914;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25345;&#32493;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#19982;&#21487;&#37325;&#22797;&#24615;&#21644;&#33021;&#28304;&#28040;&#32791;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#21487;&#37325;&#22797;&#24615;&#23545;&#20110;&#24378;&#22823;&#30340;&#31185;&#23398;&#30740;&#31350;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#33021;&#25928;&#21017;&#24378;&#35843;&#20102;&#20445;&#25252;&#26377;&#38480;&#20840;&#29699;&#36164;&#28304;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#12289;&#24211;&#21644;&#26694;&#26550;&#20013;&#26368;&#20027;&#35201;&#30340;&#20266;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;(PRNGs)&#19982;&#21508;&#33258;&#31639;&#27861;&#21407;&#22987;C&#23454;&#29616;&#30456;&#27604;&#65292;&#22312;&#32479;&#35745;&#36136;&#37327;&#21644;&#25968;&#20540;&#21487;&#37325;&#22797;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26088;&#22312;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#21644;&#33021;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Random Number Generators (PRNGs) have become ubiquitous in machine learning technologies because they are interesting for numerous methods. The field of machine learning holds the potential for substantial advancements across various domains, as exemplified by recent breakthroughs in Large Language Models (LLMs). However, despite the growing interest, persistent concerns include issues related to reproducibility and energy consumption. Reproducibility is crucial for robust scientific inquiry and explainability, while energy efficiency underscores the imperative to conserve finite global resources. This study delves into the investigation of whether the leading Pseudo-Random Number Generators (PRNGs) employed in machine learning languages, libraries, and frameworks uphold statistical quality and numerical reproducibility when compared to the original C implementation of the respective PRNG algorithms. Additionally, we aim to evaluate the time efficiency and energy consumption of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#25913;&#21892;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.16327</link><description>&lt;p&gt;
PICL: &#29289;&#29702;&#20449;&#24687;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
PICL: Physics Informed Contrastive Learning for Partial Differential Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29289;&#29702;&#20449;&#24687;&#25913;&#21892;&#20102;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#31639;&#23376;&#20316;&#20026;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26367;&#20195;&#27169;&#22411;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#12290;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20989;&#25968;&#32780;&#19981;&#26159;&#20989;&#25968;&#26412;&#36523;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#21487;&#24555;&#36895;&#20934;&#30830;&#22320;&#27714;&#35299;&#22797;&#26434;&#30340;PDE&#12290;&#23613;&#31649;&#22312;&#24191;&#27867;&#30340;&#20195;&#29702;&#24314;&#27169;&#20219;&#21153;&#20013;&#23545;&#31070;&#32463;&#31639;&#23376;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#26159;&#36880;&#20010;&#26041;&#31243;&#35780;&#20272;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#24191;&#20041;&#23545;&#27604;&#25439;&#22833;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#31070;&#32463;&#31639;&#23376;&#22312;&#22810;&#20010;&#25511;&#21046;&#26041;&#31243;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25511;&#21046;&#26041;&#31243;&#31995;&#25968;&#29992;&#20110;&#34913;&#37327;&#31995;&#32479;&#20043;&#38388;&#30340;&#30495;&#23454;&#30456;&#20284;&#24615;&#12290;&#29289;&#29702;&#20449;&#24687;&#31995;&#32479;&#28436;&#21270;&#21644;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#36755;&#20986;&#30340;&#32467;&#21512;&#34987;&#38170;&#23450;&#21040;&#36755;&#20837;&#25968;&#25454;&#20013;&#65292;&#24182;&#29992;&#20110;&#25105;&#20204;&#30340;&#36317;&#31163;&#20989;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29289;&#29702;&#20449;&#24687;&#23545;&#27604;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20613;&#31435;&#21494;&#31070;&#32463;&#31639;&#23376;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;Vim&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#20301;&#32622;&#23884;&#20837;&#26469;&#39640;&#25928;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#22914;DeiT&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2401.09417</link><description>&lt;p&gt;
Vision Mamba: &#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#39640;&#25928;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;Vim&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#20301;&#32622;&#23884;&#20837;&#26469;&#39640;&#25928;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#22914;DeiT&#65292;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#23454;&#29616;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#35774;&#35745;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#21363;Mamba&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;SSMs&#19978;&#26500;&#24314;&#39640;&#25928;&#19988;&#36890;&#29992;&#30340;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;&#20063;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35270;&#35273;&#25968;&#25454;&#30340;&#20301;&#32622;&#25935;&#24863;&#24615;&#21644;&#23545;&#20840;&#23616;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#65292;&#23545;&#20110;SSMs&#26469;&#35828;&#65292;&#34920;&#31034;&#35270;&#35273;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#26469;&#35828;&#65292;&#20381;&#36182;&#33258;&#27880;&#24847;&#21147;&#24182;&#19981;&#26159;&#24517;&#35201;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35270;&#35273;&#39592;&#24178;&#27169;&#22411;&#65292;&#21363;&#24102;&#26377;&#21452;&#21521;Mamba&#22359;&#65288;Vim&#65289;&#65292;&#23427;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#26631;&#35760;&#22270;&#20687;&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21452;&#21521;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21387;&#32553;&#35270;&#35273;&#34920;&#31034;&#12290;&#22312;ImageNet&#20998;&#31867;&#12289;COCO&#30446;&#26631;&#26816;&#27979;&#21644;ADE20k&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;Vim&#30456;&#27604;&#20110;DeiT&#31561;&#32463;&#36807;&#33391;&#22909;&#39564;&#35777;&#30340;&#35270;&#35273;&#36716;&#25442;&#22120;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;X&#40657;&#23458;&#30340;&#27010;&#24565;&#65292;&#21363;&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#25805;&#32437;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#25351;&#26631;&#65292;&#20174;&#32780;&#20135;&#29983;&#25152;&#38656;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#32773;&#24635;&#32467;&#20102;X&#40657;&#23458;&#29616;&#35937;&#30340;&#20005;&#37325;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23545;XAI&#30740;&#31350;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.08513</link><description>&lt;p&gt;
X&#40657;&#23458;&#65306;&#35823;&#23548;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
X Hacking: The Threat of Misguided AutoML
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;X&#40657;&#23458;&#30340;&#27010;&#24565;&#65292;&#21363;&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#25805;&#32437;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#25351;&#26631;&#65292;&#20174;&#32780;&#20135;&#29983;&#25152;&#38656;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#32773;&#24635;&#32467;&#20102;X&#40657;&#23458;&#29616;&#35937;&#30340;&#20005;&#37325;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23545;XAI&#30740;&#31350;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#24314;&#31435;&#23545;&#27169;&#22411;&#39044;&#27979;&#21644;&#27966;&#29983;&#35265;&#35299;&#30340;&#20449;&#20219;&#65292;&#20294;&#20063;&#20026;&#20998;&#26512;&#24072;&#25552;&#20379;&#20102;&#19968;&#31181;&#25197;&#26354;&#30340;&#21160;&#26426;&#65292;&#21363;&#25805;&#32437;XAI&#25351;&#26631;&#20197;&#25903;&#25345;&#39044;&#20808;&#35268;&#23450;&#30340;&#32467;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;X&#40657;&#23458;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;p-hacking&#24212;&#29992;&#20110;&#35832;&#22914;Shap&#20540;&#20043;&#31867;&#30340;XAI&#25351;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#23547;&#25214;&#8220;&#21487;&#36777;&#25252;&#8221;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#25152;&#38656;&#30340;&#35299;&#37322;&#24182;&#22312;&#32500;&#25345;&#20248;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#26102;&#12290;&#25105;&#20204;&#23558;&#35299;&#37322;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#34920;&#36848;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29087;&#24713;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#22312;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;X&#40657;&#23458;&#30340;&#21487;&#34892;&#24615;&#21644;&#20005;&#37325;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;XAI&#30740;&#31350;&#30340;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) and interpretable machine learning methods help to build trust in model predictions and derived insights, yet also present a perverse incentive for analysts to manipulate XAI metrics to support pre-specified conclusions. This paper introduces the concept of X-hacking, a form of p-hacking applied to XAI metrics such as Shap values. We show how an automated machine learning pipeline can be used to search for 'defensible' models that produce a desired explanation while maintaining superior predictive performance to a common baseline. We formulate the trade-off between explanation and accuracy as a multi-objective optimization problem and illustrate the feasibility and severity of X-hacking empirically on familiar real-world datasets. Finally, we suggest possible methods for detection and prevention, and discuss ethical implications for the credibility and reproducibility of XAI research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#36866;&#29992;&#20110;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#30340;ICA&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20989;&#25968;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#36136;&#37327;&#65292;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2401.08468</link><description>&lt;p&gt;
&#20445;&#30041;&#36824;&#26159;&#20002;&#24323;&#65311;&#19968;&#31181;&#35780;&#20272;&#26377;&#22122;&#22768;ICA&#35299;&#20915;&#26041;&#26696;&#30340;&#38750;&#21442;&#25968;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
Keep or toss? A nonparametric score to evaluate solutions for noisy ICA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#26469;&#33258;&#36866;&#24212;&#36873;&#25321;&#36866;&#29992;&#20110;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#30340;ICA&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20989;&#25968;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#36136;&#37327;&#65292;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#65288;ICA&#65289;&#20110;20&#19990;&#32426;80&#24180;&#20195;&#24341;&#20837;&#65292;&#20316;&#20026;&#30450;&#28304;&#20998;&#31163;&#65288;BSS&#65289;&#30340;&#27169;&#22411;&#65292;&#25351;&#30340;&#26159;&#22312;&#23545;&#28151;&#21512;&#20449;&#21495;&#36827;&#34892;&#24674;&#22797;&#26102;&#65292;&#23545;&#28304;&#20449;&#21495;&#25110;&#28151;&#21512;&#36807;&#31243;&#20102;&#35299;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#26377;&#35768;&#22810;&#31934;&#23494;&#31639;&#27861;&#36827;&#34892;&#20272;&#35745;&#65292;&#20294;&#19981;&#21516;&#26041;&#27861;&#23384;&#22312;&#19981;&#21516;&#30340;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#20998;&#25968;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;ICA&#31639;&#27861;&#21644;&#20219;&#24847;&#39640;&#26031;&#22122;&#22768;&#12290;&#35813;&#20998;&#25968;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#21482;&#20551;&#35774;&#25968;&#25454;&#20855;&#26377;&#26377;&#38480;&#30340;&#20108;&#38454;&#30697;&#65292;&#24182;&#20351;&#29992;&#29305;&#24449;&#20989;&#25968;&#26469;&#35780;&#20272;&#20272;&#35745;&#30340;&#28151;&#21512;&#30697;&#38453;&#30340;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#22122;&#22768;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#23545;&#27604;&#20989;&#25968;&#21644;&#31639;&#27861;&#65292;&#23427;&#20204;&#20855;&#26377;&#19982;&#29616;&#26377;&#31639;&#27861;&#65288;&#22914;FASTICA&#21644;JADE&#65289;&#30456;&#21516;&#30340;&#24555;&#36895;&#35745;&#31639;&#24615;&#33021;&#65292;&#20294;&#22312;&#21069;&#32773;&#21487;&#33021;&#22833;&#36133;&#30340;&#39046;&#22495;&#20013;&#24037;&#20316;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20063;&#21487;&#33021;&#23384;&#22312;&#32570;&#28857;&#65292;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) was introduced in the 1980's as a model for Blind Source Separation (BSS), which refers to the process of recovering the sources underlying a mixture of signals, with little knowledge about the source signals or the mixing process. While there are many sophisticated algorithms for estimation, different methods have different shortcomings. In this paper, we develop a nonparametric score to adaptively pick the right algorithm for ICA with arbitrary Gaussian noise. The novelty of this score stems from the fact that it just assumes a finite second moment of the data and uses the characteristic function to evaluate the quality of the estimated mixing matrix without any knowledge of the parameters of the noise distribution. In addition, we propose some new contrast functions and algorithms that enjoy the same fast computability as existing algorithms like FASTICA and JADE but work in domains where the former may fail. While these also may have weaknesses,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#31946;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#20013;&#24403;&#20915;&#31574;&#32773;&#19981;&#30693;&#36947;&#20998;&#24067;P&#26102;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20998;&#24067;&#26469;&#36817;&#20284;&#21407;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.00547</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#27169;&#31946;&#26426;&#20250;&#32422;&#26463;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Learning for Ambiguous Chance Constrained Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#31946;&#26426;&#20250;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#20013;&#24403;&#20915;&#31574;&#32773;&#19981;&#30693;&#36947;&#20998;&#24067;P&#26102;&#30340;&#24773;&#20917;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#20998;&#24067;&#26469;&#36817;&#20284;&#21407;&#38382;&#39064;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#30456;&#24212;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#20915;&#31574;&#32773;(DM)&#19981;&#30693;&#36947;&#20998;&#24067;P&#26102;&#65292;&#28385;&#36275;&#26426;&#20250;&#32422;&#26463;&#30340;&#20248;&#21270;&#38382;&#39064;$\min_x f(x)$ s.t. $P(\left\{ \theta: g(x,\theta)\le 0 \right\})\ge 1-\epsilon$&#12290;&#24403;DM&#21487;&#20197;&#35775;&#38382;&#19968;&#32452;&#20998;&#24067;$\mathcal{U}$&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20998;&#24067;P&#26102;&#65292;&#38382;&#39064;&#34987;&#31216;&#20026;&#27169;&#31946;&#26426;&#20250;&#32422;&#26463;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;$\mathcal{U}$&#20197;$\left\{\mu:\frac{\mu (y)}{\nu(y)}\leq C, \forall y\in\Theta, \mu(y)\ge 0\right\}$&#30340;&#24418;&#24335;&#23384;&#22312;&#26102;&#30340;&#27169;&#31946;&#26426;&#20250;&#32422;&#26463;&#38382;&#39064;&#65292;&#20854;&#20013;$\nu$&#26159;&#21442;&#32771;&#20998;&#24067;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21407;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#20174;$\nu$&#20013;&#25277;&#21462;N&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;$\theta_i$&#65292;&#24182;&#23558;&#21407;&#22987;&#32422;&#26463;&#26367;&#25442;&#20026;$g(x,\theta_i)\le 0,~i=1,2,\ldots,N$&#30340;&#37319;&#26679;&#38382;&#39064;&#26469;&#8220;&#24456;&#22909;&#22320;&#36924;&#36817;&#8221;&#12290;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#19982;&#36825;&#31181;&#36924;&#36817;&#30456;&#20851;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study chance constrained optimization problems $\min_x f(x)$ s.t. $P(\left\{ \theta: g(x,\theta)\le 0 \right\})\ge 1-\epsilon$ where $\epsilon\in (0,1)$ is the violation probability, when the distribution $P$ is not known to the decision maker (DM). When the DM has access to a set of distributions $\mathcal{U}$ such that $P$ is contained in $\mathcal{U}$, then the problem is known as the ambiguous chance-constrained problem \cite{erdougan2006ambiguous}. We study ambiguous chance-constrained problem for the case when $\mathcal{U}$ is of the form $\left\{\mu:\frac{\mu (y)}{\nu(y)}\leq C, \forall y\in\Theta, \mu(y)\ge 0\right\}$, where $\nu$ is a ``reference distribution.'' We show that in this case the original problem can be ``well-approximated'' by a sampled problem in which $N$ i.i.d. samples of $\theta$ are drawn from $\nu$, and the original constraint is replaced with $g(x,\theta_i)\le 0,~i=1,2,\ldots,N$. We also derive the sample complexity associated with this approximation, i.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20809;&#35889;&#33021;&#37327;&#20998;&#24067;&#65288;SED&#65289;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25968;&#37327;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#20860;&#23481;sklearn&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36716;&#25442;&#28857;&#39044;&#27979;&#20026;&#35823;&#24046;&#26465;&#25552;&#39640;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;CatBoost&#20316;&#20026;&#22522;&#30784;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#19968;&#33268;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#26143;&#31995;&#30340;&#29289;&#29702;&#24615;&#36136;&#25552;&#20379;&#20102;&#26356;&#22810;&#21151;&#33021;&#21644;&#20934;&#30830;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2312.14212</link><description>&lt;p&gt;
&#36229;&#36234;&#23494;&#26519;&#65306;&#29992;&#19968;&#33268;&#39044;&#27979;&#22686;&#24378;SED&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Beyond mirkwood: Enhancing SED Modeling with Conformal Predictions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14212
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20809;&#35889;&#33021;&#37327;&#20998;&#24067;&#65288;SED&#65289;&#24314;&#27169;&#30340;&#28789;&#27963;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25968;&#37327;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#20219;&#20309;&#20860;&#23481;sklearn&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#36716;&#25442;&#28857;&#39044;&#27979;&#20026;&#35823;&#24046;&#26465;&#25552;&#39640;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;CatBoost&#20316;&#20026;&#22522;&#30784;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#19968;&#33268;&#39044;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#20026;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#26143;&#31995;&#30340;&#29289;&#29702;&#24615;&#36136;&#25552;&#20379;&#20102;&#26356;&#22810;&#21151;&#33021;&#21644;&#20934;&#30830;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#20809;&#35889;&#33021;&#37327;&#20998;&#24067;&#65288;SED&#65289;&#25311;&#21512;&#25216;&#26415;&#30001;&#20110;&#23545;&#26143;&#31995;&#30340;&#24418;&#25104;&#21382;&#21490;&#21644;&#28784;&#23576;&#34928;&#20943;&#26354;&#32447;&#20570;&#20986;&#30340;&#20551;&#35774;&#32780;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;SED&#25311;&#21512;&#30340;&#28789;&#27963;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#25968;&#37327;&#21270;&#12290;&#19982;&#23494;&#26519;&#20013;&#20351;&#29992;&#30340;&#22266;&#23450;&#30340;NGBoost&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20219;&#20309;&#20860;&#23481;sklearn&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#33268;&#21270;&#20998;&#20301;&#25968;&#22238;&#24402;&#23558;&#28857;&#39044;&#27979;&#36716;&#25442;&#20026;&#35823;&#24046;&#26465;&#65292;&#22686;&#24378;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20197;CatBoost&#20316;&#20026;&#22522;&#30784;&#39044;&#27979;&#22120;&#65292;&#27604;&#36739;&#20102;&#26377;&#26080;&#19968;&#33268;&#39044;&#27979;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#21644;&#21306;&#38388;&#23485;&#24230;&#31561;&#25351;&#26631;&#26174;&#31034;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#26143;&#31995;&#29289;&#29702;&#24615;&#36136;&#25552;&#20379;&#20102;&#26356;&#21152;&#22810;&#21151;&#33021;&#21644;&#20934;&#30830;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional spectral energy distribution (SED) fitting techniques face uncertainties due to assumptions in star formation histories and dust attenuation curves. We propose an advanced machine learning-based approach that enhances flexibility and uncertainty quantification in SED fitting. Unlike the fixed NGBoost model used in mirkwood, our approach allows for any sklearn-compatible model, including deterministic models. We incorporate conformalized quantile regression to convert point predictions into error bars, enhancing interpretability and reliability. Using CatBoost as the base predictor, we compare results with and without conformal prediction, demonstrating improved performance using metrics such as coverage and interval width. Our method offers a more versatile and accurate tool for deriving galaxy physical properties from observational data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2312.13327</link><description>&lt;p&gt;
&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#24773;&#22659;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Reinforcement Learning for Variable Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Headless-AD&#27169;&#22411;&#65292;&#36890;&#36807;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#33021;&#22815;&#22312;&#21464;&#21270;&#30340;&#34892;&#21160;&#31354;&#38388;&#20013;&#23454;&#29616;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#30340;&#27867;&#21270;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#20174;&#26410;&#36935;&#21040;&#36807;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#32988;&#36807;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#20808;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19978;&#19979;&#25991;&#22810;&#24773;&#33410;&#35757;&#32451;&#30340;&#21464;&#24418;&#37329;&#21018;&#32593;&#32476;&#21487;&#20197;&#22312;&#24773;&#22659;&#20013;&#27867;&#21270;&#21040;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#34892;&#21160;&#31354;&#38388;&#22823;&#23567;&#21644;&#32467;&#26500;&#12290;&#24341;&#20837;&#26032;&#30340;&#34892;&#21160;&#31354;&#38388;&#36890;&#24120;&#38656;&#35201;&#25968;&#25454;&#37325;&#26032;&#25910;&#38598;&#21644;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21482;&#35757;&#32451;&#19968;&#27425;&#30340;Headless-AD&#27169;&#22411;&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#27867;&#21270;&#21040;&#20855;&#26377;&#21487;&#21464;&#22823;&#23567;&#12289;&#35821;&#20041;&#20869;&#23481;&#21644;&#39034;&#24207;&#30340;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#22312;&#20271;&#21162;&#21033;&#21644;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#20197;&#21450;&#19968;&#20010;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;Headless-AD&#22312;&#20174;&#26410;&#36935;&#21040;&#30340;&#34892;&#21160;&#31354;&#38388;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#20960;&#20010;&#29615;&#22659;&#37197;&#32622;&#19978;&#32988;&#36807;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#34892;&#21160;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, it has been shown that transformers pre-trained on diverse datasets with multi-episode contexts can generalize to new reinforcement learning tasks in-context. A key limitation of previously proposed models is their reliance on a predefined action space size and structure. The introduction of a new action space often requires data re-collection and model re-training, which can be costly for some applications. In our work, we show that it is possible to mitigate this issue by proposing the Headless-AD model that, despite being trained only once, is capable of generalizing to discrete action spaces of variable size, semantic content and order. By experimenting with Bernoulli and contextual bandits, as well as a gridworld environment, we show that Headless-AD exhibits significant capability to generalize to action spaces it has never encountered, even outperforming specialized models trained for a specific set of actions on several environment configurations.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.12869</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#25237;&#24433;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Parameterized Projected Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#36817;&#20284;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36817;&#20284;&#20540;&#36845;&#20195;&#31639;&#27861;&#20013;&#26679;&#26412;&#19981;&#30830;&#23450;&#24615;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20284;&#20540;&#36845;&#20195;&#65288;AVI&#65289;&#26159;&#19968;&#31867;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#31639;&#27861;&#23478;&#26063;&#65292;&#26088;&#22312;&#33719;&#24471;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#36817;&#20284;&#12290;&#36890;&#24120;&#65292;AVI&#31639;&#27861;&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#65292;&#27599;&#20010;&#27493;&#39588;&#21253;&#25324;&#65288;i&#65289;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#21644;&#65288;ii&#65289;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#20013;&#12290;&#20247;&#25152;&#21608;&#30693;&#65292;&#36125;&#23572;&#26364;&#31639;&#23376;&#21033;&#29992;&#36716;&#31227;&#26679;&#26412;&#65292;&#36825;&#20123;&#26679;&#26412;&#24378;&#28872;&#24433;&#21709;&#20854;&#34892;&#20026;&#65292;&#22240;&#20026;&#26080;&#20449;&#24687;&#30340;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#21487;&#24573;&#30053;&#30340;&#26356;&#26032;&#25110;&#38271;&#26102;&#38388;&#30340;&#32469;&#34892;&#65292;&#32780;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#23398;&#20064;&#30340;&#26041;&#24335;&#24471;&#21040;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#36817;&#20284;&#29256;&#26412;&#65292;&#32780;&#19981;&#26159;&#20687;AVI&#26041;&#27861;&#37027;&#26679;&#36890;&#36807;&#26679;&#26412;&#36827;&#34892;&#20272;&#35745;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#33021;&#22815;&#65288;i&#65289;&#22312;&#36716;&#31227;&#26679;&#26412;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#65292;&#65288;ii&#65289;&#36991;&#20813;&#35745;&#31639;&#23494;&#38598;&#30340;&#25237;&#24433;&#27493;&#39588;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31216;&#25105;&#20204;&#30340;&#26032;&#31639;&#23376;&#20026;"projec"&#31639;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Approximate value iteration (AVI) is a family of algorithms for reinforcement learning (RL) that aims to obtain an approximation of the optimal value function. Generally, AVI algorithms implement an iterated procedure where each step consists of (i) an application of the Bellman operator and (ii) a projection step into a considered function space. Notoriously, the Bellman operator leverages transition samples, which strongly determine its behavior, as uninformative samples can result in negligible updates or long detours, whose detrimental effects are further exacerbated by the computationally intensive projection step. To address these issues, we propose a novel alternative approach based on learning an approximate version of the Bellman operator rather than estimating it through samples as in AVI approaches. This way, we are able to (i) generalize across transition samples and (ii) avoid the computationally intensive projection step. For this reason, we call our novel operator projec
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#35889;&#26463;&#32538;&#19982;&#33609;&#22270;&#21021;&#22987;&#21270;&#65288;USBS&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#22320;&#35299;&#20915;&#22823;&#35268;&#27169;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#28201;&#26262;&#21551;&#21160;&#21021;&#22987;&#21270;&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#25910;&#25947;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2312.11801</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#30340;&#21322;&#23450;&#35268;&#21010;&#19982;&#35889;&#26463;&#32538;&#21644;&#33609;&#22270;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Fast, Scalable, Warm-Start Semidefinite Programming with Spectral Bundling and Sketching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11801
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#35889;&#26463;&#32538;&#19982;&#33609;&#22270;&#21021;&#22987;&#21270;&#65288;USBS&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#22320;&#35299;&#20915;&#22823;&#35268;&#27169;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#28201;&#26262;&#21551;&#21160;&#21021;&#22987;&#21270;&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#25910;&#25947;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21322;&#23450;&#35268;&#21010;(SDP)&#22312;&#20256;&#32479;&#19978;&#23616;&#38480;&#20110;&#20013;&#31561;&#35268;&#27169;&#30340;&#38382;&#39064;&#65292;&#20294;&#26368;&#36817;&#20351;&#29992;&#30697;&#38453;&#33609;&#22270;&#25216;&#26415;&#22686;&#24378;&#30340;&#31639;&#27861;&#24050;&#32463;&#33021;&#22815;&#35299;&#20915;&#26356;&#22823;&#30340;SDP&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38656;&#35201;&#22686;&#21152;&#24517;&#35201;&#30340;&#36845;&#20195;&#27425;&#25968;&#65292;&#23548;&#33268;&#38543;&#30528;&#38382;&#39064;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#25910;&#25947;&#36895;&#24230;&#20943;&#24930;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36824;&#38656;&#35201;&#36845;&#20195;&#30456;&#20851;&#30340;&#21442;&#25968;&#35843;&#24230;&#65292;&#38480;&#21046;&#20102;&#26377;&#25928;&#21033;&#29992;&#28201;&#26262;&#21551;&#21160;&#21021;&#22987;&#21270;&#30340;&#33021;&#21147;&#65292;&#22312;&#20855;&#26377;&#22686;&#37327;&#21040;&#36798;&#25968;&#25454;&#25110;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#35889;&#26463;&#32538;&#19982;&#33609;&#22270;&#21021;&#22987;&#21270;(USBS)&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35777;&#26126;&#27491;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#35268;&#27169;&#30340;SDP&#38382;&#39064;&#65292;&#21487;&#20197;&#21033;&#29992;&#28201;&#26262;&#21551;&#21160;&#21021;&#22987;&#21270;&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#25910;&#25947;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#35299;&#20915;&#21253;&#21547;&#30456;&#31561;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#19968;&#33324;SDP&#38382;&#39064;&#30340;&#35889;&#26463;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;&#24403;&#32467;&#21512;&#21487;&#36873;&#30340;&#30697;&#38453;&#33609;&#22270;&#25216;&#26415;&#26102;
&lt;/p&gt;
&lt;p&gt;
While semidefinite programming (SDP) has traditionally been limited to moderate-sized problems, recent algorithms augmented with matrix sketching techniques have enabled solving larger SDPs. However, these methods achieve scalability at the cost of an increase in the number of necessary iterations, resulting in slower convergence as the problem size grows. Furthermore, they require iteration-dependent parameter schedules that prohibit effective utilization of warm-start initializations important in practical applications with incrementally-arriving data or mixed-integer programming. We present Unified Spectral Bundling with Sketching (USBS), a provably correct, fast and scalable algorithm for solving massive SDPs that can leverage a warm-start initialization to further accelerate convergence. Our proposed algorithm is a spectral bundle method for solving general SDPs containing both equality and inequality constraints. Moveover, when augmented with an optional matrix sketching techniqu
&lt;/p&gt;</description></item><item><title>FedSSA&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#24322;&#26500; feature extractor &#21644;&#21516;&#36136; classification header &#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#25286;&#20998;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24230;&#36827;&#34892;&#22836;&#37096;&#21442;&#25968;&#32858;&#21512;&#23454;&#29616;&#26412;&#22320;&#21040;&#20840;&#23616;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21442;&#25968;&#31283;&#23450;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;</title><link>https://arxiv.org/abs/2312.09006</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#32858;&#21512;&#30340;FedSSA: &#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedSSA: Semantic Similarity-based Aggregation for Efficient Model-Heterogeneous Personalized Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09006
&lt;/p&gt;
&lt;p&gt;
FedSSA&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#32858;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#12290;&#23427;&#36890;&#36807;&#24322;&#26500; feature extractor &#21644;&#21516;&#36136; classification header &#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#25286;&#20998;&#65292;&#24182;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#24230;&#36827;&#34892;&#22836;&#37096;&#21442;&#25968;&#32858;&#21512;&#23454;&#29616;&#26412;&#22320;&#21040;&#20840;&#23616;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21442;&#25968;&#31283;&#23450;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#12290;&#20256;&#32479;&#30340;FL&#35201;&#27714;&#25152;&#26377;&#25968;&#25454;&#25152;&#26377;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#35757;&#32451;&#30456;&#21516;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#36825;&#31181;&#35774;&#35745;&#24182;&#19981;&#36866;&#29992;&#20110;&#28041;&#21450;&#25968;&#25454;&#21644;/&#25110;&#31995;&#32479;&#24322;&#26500;&#30340;&#22330;&#26223;&#12290;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;FL&#65288;MHPFL&#65289;&#24050;&#32463;&#20986;&#29616;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;MHPFL&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20855;&#26377;&#30456;&#21516;&#23398;&#20064;&#20219;&#21153;&#24615;&#36136;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#25110;&#32773;&#20250;&#20135;&#29983;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Semantic Similarity Aggregation&#65288;FedSSA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#20998;&#20026;&#24322;&#26500;&#65288;&#32467;&#26500;&#19981;&#21516;&#65289;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#21516;&#36136;&#65288;&#32467;&#26500;&#30456;&#21516;&#65289;&#20998;&#31867;&#22836;&#37096;&#12290;&#23427;&#36890;&#36807;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#22836;&#37096;&#21442;&#25968;&#32858;&#21512;&#23454;&#29616;&#20102;&#26412;&#22320;&#21040;&#20840;&#23616;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21442;&#25968;&#31283;&#23450;&#31574;&#30053;&#23454;&#29616;&#20102;&#20840;&#23616;&#21040;&#26412;&#22320;&#30340;&#30693;&#35782;&#20256;&#36755;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a privacy-preserving collaboratively machine learning paradigm. Traditional FL requires all data owners (a.k.a. FL clients) to train the same local model. This design is not well-suited for scenarios involving data and/or system heterogeneity. Model-Heterogeneous Personalized FL (MHPFL) has emerged to address this challenge. Existing MHPFL approaches often rely on having a public dataset with the same nature of the learning task, or incur high computation and communication costs. To address these limitations, we propose the Federated Semantic Similarity Aggregation (FedSSA) approach, which splits each client's model into a heterogeneous (structure-different) feature extractor and a homogeneous (structure-same) classification header. It performs local-to-global knowledge transfer via semantic similarity-based header parameter aggregation. In addition, global-to-local knowledge transfer is achieved via an adaptive parameter stabilization strategy which fuses th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32422;&#26463;&#31354;&#38388;&#20013;&#36827;&#34892;&#24555;&#36895;&#37319;&#26679;&#12290;&#36825;&#31181;&#31639;&#27861;&#26159;&#23545;Mirror Langevin&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#28155;&#21152;&#25509;&#21463;-&#25298;&#32477;&#36807;&#28388;&#22120;&#26469;&#28040;&#38500;&#28176;&#36817;&#20559;&#24046;&#65292;&#24182;&#20855;&#26377;&#25351;&#25968;&#20248;&#21270;&#20381;&#36182;&#12290;</title><link>https://arxiv.org/abs/2312.08823</link><description>&lt;p&gt;
&#20351;&#29992;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#20174;&#32422;&#26463;&#31354;&#38388;&#20013;&#24555;&#36895;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08823
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32422;&#26463;&#31354;&#38388;&#20013;&#36827;&#34892;&#24555;&#36895;&#37319;&#26679;&#12290;&#36825;&#31181;&#31639;&#27861;&#26159;&#23545;Mirror Langevin&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#36890;&#36807;&#28155;&#21152;&#25509;&#21463;-&#25298;&#32477;&#36807;&#28388;&#22120;&#26469;&#28040;&#38500;&#28176;&#36817;&#20559;&#24046;&#65292;&#24182;&#20855;&#26377;&#25351;&#25968;&#20248;&#21270;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Metropolis-adjusted Mirror Langevin&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#20854;&#25903;&#25345;&#26159;&#32039;&#20984;&#38598;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#36817;&#20284;&#37319;&#26679;&#12290;&#35813;&#31639;&#27861;&#22312;Mirror Langevin&#31639;&#27861;&#65288;Zhang et al., 2020&#65289;&#30340;&#21333;&#27493;&#39532;&#23572;&#31185;&#22827;&#38142;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#25509;&#21463;-&#25298;&#32477;&#36807;&#28388;&#22120;&#65292;Mirror Langevin&#31639;&#27861;&#26159;Mirror Langevin&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#31163;&#25955;&#21270;&#12290;&#30001;&#20110;&#21253;&#21547;&#20102;&#36825;&#20010;&#36807;&#28388;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#30446;&#26631;&#26159;&#26080;&#20559;&#30340;&#65292;&#32780;&#24050;&#30693;&#30340;Mirror Langevin&#31639;&#27861;&#31561;Mirror Langevin&#21160;&#21147;&#23398;&#30340;&#31163;&#25955;&#21270;&#20855;&#26377;&#28176;&#36817;&#20559;&#24046;&#12290;&#23545;&#20110;&#35813;&#31639;&#27861;&#65292;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#28151;&#21512;&#21040;&#19968;&#20010;&#30456;&#23545;&#24179;&#28369;&#12289;&#20984;&#24615;&#22909;&#19988;&#19982;&#33258;&#20849;&#36717;&#38236;&#20687;&#20989;&#25968;&#30456;&#20851;&#30340;&#32422;&#26463;&#20998;&#24067;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#19978;&#30028;&#12290;&#30001;&#20110;&#21253;&#21547;Metropolis-Hastings&#36807;&#28388;&#22120;&#23548;&#33268;&#30340;&#39532;&#23572;&#31185;&#22827;&#38142;&#26159;&#21487;&#36870;&#30340;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#23545;&#35823;&#24046;&#30340;&#25351;&#25968;&#20248;&#21270;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al., 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the erro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#32447;&#24615;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#38544;&#34255;&#22240;&#26524;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#23884;&#20837;&#21644;&#32858;&#31867;&#26469;&#35299;&#20915;&#22122;&#22768;&#30456;&#20851;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.05974</link><description>&lt;p&gt;
&#23398;&#20064;&#20855;&#26377;&#28508;&#22312;&#33410;&#28857;&#21644;&#32467;&#26500;&#21270;&#22122;&#22768;&#30340;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#22240;&#26524;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning the Causal Structure of Networked Dynamical Systems under Latent Nodes and Structured Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#32447;&#24615;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#30340;&#38544;&#34255;&#22240;&#26524;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#23884;&#20837;&#21644;&#32858;&#31867;&#26469;&#35299;&#20915;&#22122;&#22768;&#30456;&#20851;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#26465;&#20214;&#19979;&#65292;&#20174;&#37096;&#20998;&#33410;&#28857;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#32447;&#24615;&#32593;&#32476;&#21160;&#21147;&#31995;&#32479;&#65288;NDS&#65289;&#30340;&#38544;&#34255;&#22240;&#26524;&#32593;&#32476;&#12290;NDS&#30340;&#21160;&#21147;&#23398;&#30001;&#29983;&#25104;&#33410;&#28857;&#23545;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#30340;&#26377;&#33394;&#22122;&#22768;&#39537;&#21160;&#65292;&#20174;&#32780;&#20351;&#38382;&#39064;&#26356;&#21152;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#22122;&#22768;&#30456;&#20851;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#20026;&#27599;&#23545;&#33410;&#28857;&#20998;&#37197;&#20102;&#20174;&#35266;&#27979;&#33410;&#28857;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35745;&#31639;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#29305;&#24449;&#23884;&#20837;&#34987;&#35774;&#35745;&#20026;&#20855;&#26377;&#32467;&#26500;&#19968;&#33268;&#24615;&#65306;&#23384;&#22312;&#19968;&#20010;&#20223;&#23556;&#36229;&#24179;&#38754;&#65292;&#33021;&#22815;&#19968;&#33268;&#22320;&#23558;&#29305;&#24449;&#38598;&#20998;&#21106;&#65292;&#23558;&#19982;&#36830;&#25509;&#30340;&#33410;&#28857;&#23545;&#24212;&#30340;&#29305;&#24449;&#21521;&#37327;&#19982;&#19982;&#26029;&#24320;&#30340;&#33410;&#28857;&#23545;&#24212;&#30340;&#29305;&#24449;&#21521;&#37327;&#20998;&#24320;&#12290;&#22240;&#26524;&#25512;&#26029;&#38382;&#39064;&#22240;&#27492;&#36890;&#36807;&#23545;&#35774;&#35745;&#30340;&#29305;&#24449;&#36827;&#34892;&#32858;&#31867;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#22522;&#20934;&#30417;&#30563;&#26041;&#27861;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#22240;&#26524;&#25512;&#26029;&#26426;&#21046;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers learning the hidden causal network of a linear networked dynamical system (NDS) from the time series data at some of its nodes -- partial observability. The dynamics of the NDS are driven by colored noise that generates spurious associations across pairs of nodes, rendering the problem much harder. To address the challenge of noise correlation and partial observability, we assign to each pair of nodes a feature vector computed from the time series data of observed nodes. The feature embedding is engineered to yield structural consistency: there exists an affine hyperplane that consistently partitions the set of features, separating the feature vectors corresponding to connected pairs of nodes from those corresponding to disconnected pairs. The causal inference problem is thus addressed via clustering the designed features. We demonstrate with simple baseline supervised methods the competitive performance of the proposed causal inference mechanism under broad connec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2312.04455</link><description>&lt;p&gt;
&#24378;&#21270;&#20851;&#27880;&#21147;&#20013;&#26368;&#30701;&#30340;&#25903;&#26609;&#65306;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24847;&#35782;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20851;&#27880;&#20998;&#37197;&#30340;&#27874;&#24418;&#27169;&#24335;&#23545;&#20854;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#21644;&#19981;&#21516;&#30340;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#35282;&#24230;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#20851;&#27880;&#20998;&#37197;&#20013;&#30340;&#20869;&#22312;&#27874;&#24418;&#27169;&#24335;&#26174;&#33879;&#24433;&#21709;&#23427;&#20204;&#22312;&#38656;&#35201;&#39640;&#24230;&#19978;&#19979;&#25991;&#24847;&#35782;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21033;&#29992;LLMs&#36827;&#34892;&#24037;&#20855;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#20851;&#38190;&#20449;&#24687;&#22312;&#19978;&#19979;&#25991;&#20013;&#20301;&#20110;&#20851;&#27880;&#27874;&#24418;&#30340;&#20302;&#35895;&#21306;&#22495;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#24573;&#35270;&#35813;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Attention Buckets&#8221;&#30340;&#26032;&#22411;&#25512;&#29702;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;LLMs&#36890;&#36807;&#22810;&#20010;&#24182;&#34892;&#36807;&#31243;&#22788;&#29702;&#36755;&#20837;&#12290;&#27599;&#20010;&#36807;&#31243;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20934;&#35282;&#24230;&#36827;&#34892;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65292;&#20174;&#32780;&#21019;&#24314;&#20986;&#19968;&#20010;&#29420;&#29305;&#30340;&#20851;&#27880;&#27874;&#24418;&#12290;&#36890;&#36807;&#29992;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#20302;&#35895;&#34917;&#20607;&#21478;&#19968;&#20010;&#36807;&#31243;&#30340;&#20851;&#27880;&#39640;&#23792;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;LLM&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20301;&#32622;&#30340;&#24847;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#24573;&#35270;&#20851;&#38190;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we demonstrate that an inherent waveform pattern in the attention allocation of large language models (LLMs) significantly affects their performance in tasks demanding a high degree of context awareness, such as utilizing LLMs for tool-use. Specifically, the crucial information in the context will be potentially overlooked by model when it is positioned in the trough zone of the attention waveform, leading to decreased performance. To address this issue, we propose a novel inference method named Attention Buckets. It allows LLMs to process their input through multiple parallel processes. Each process utilizes a distinct base angle for the rotary position embedding, thereby creating a unique attention waveform. By compensating an attention trough of a particular process with an attention peak of another process, our approach enhances LLM's awareness to various contextual positions, thus mitigating the risk of overlooking crucial information. In the largest tool-use benchm
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03096</link><description>&lt;p&gt;
&#24341;&#36215;&#22810;&#20041;&#24615;&#30340;&#21407;&#22240;&#26159;&#20160;&#20040;&#65311;&#36890;&#36807;&#20598;&#28982;&#22240;&#32032;&#30340;&#28151;&#21512;&#36873;&#25321;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
What Causes Polysemanticity? An Alternative Origin Story of Mixed Selectivity from Incidental Causes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03096
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#65292;&#31216;&#20026;&#20598;&#28982;&#22810;&#20041;&#24615;&#65292;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25152;&#26377;&#29305;&#24449;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#22810;&#20041;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20041;&#24615;&#31070;&#32463;&#20803;&#8212;&#8212;&#28608;&#27963;&#19968;&#32452;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#31070;&#32463;&#20803;&#8212;&#8212;&#34987;&#35270;&#20026;&#35299;&#37322;&#20219;&#21153;&#20248;&#21270;&#28145;&#24230;&#32593;&#32476;&#30340;&#26174;&#33879;&#38556;&#30861;&#65292;&#23545;AI&#23433;&#20840;&#24615;&#20135;&#29983;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#22810;&#20041;&#24615;&#36215;&#28304;&#25925;&#20107;&#26159;&#25968;&#25454;&#21253;&#21547;&#30340;&#8220;&#29305;&#24449;&#8221;&#22810;&#20110;&#31070;&#32463;&#20803;&#65292;&#22240;&#27492;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#36843;&#20351;&#32593;&#32476;&#23558;&#22810;&#20010;&#19981;&#30456;&#20851;&#29305;&#24449;&#20998;&#37197;&#32473;&#21516;&#19968;&#20010;&#31070;&#32463;&#20803;&#65292;&#21361;&#21450;&#25105;&#20204;&#29702;&#35299;&#32593;&#32476;&#20869;&#37096;&#22788;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#20041;&#24615;&#30340;&#31532;&#20108;&#20010;&#19988;&#38750;&#20114;&#26021;&#30340;&#26367;&#20195;&#36215;&#28304;&#25925;&#20107;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26377;&#36275;&#22815;&#30340;&#31070;&#32463;&#20803;&#26469;&#34920;&#31034;&#25968;&#25454;&#20013;&#30340;&#25152;&#26377;&#29305;&#24449;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#20063;&#21487;&#33021;&#20135;&#29983;&#65292;&#36825;&#26159;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20598;&#28982;&#22810;&#20041;&#24615;&#8221;&#30340;&#29616;&#35937;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;&#20598;&#28982;&#22810;&#20041;&#24615;&#21487;&#20197;&#30001;&#22810;&#31181;&#21407;&#22240;&#24341;&#36215;&#65292;&#21253;&#25324;&#27491;&#21017;&#21270;&#21644;&#31070;&#32463;&#22122;&#38899;&#65307;&#36825;&#31181;&#20598;&#28982;&#22810;&#20041;&#24615;&#21457;&#29983;&#26159;&#22240;&#20026;&#38543;&#26426;&#30340;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polysemantic neurons -- neurons that activate for a set of unrelated features -- have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more ``features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term \textit{incidental polysemanticity}. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random in
&lt;/p&gt;</description></item><item><title>&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#19987;&#23478;&#31574;&#30053;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26631;&#20934;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#39640;&#25928;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26524;&#32447;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#12290;</title><link>https://arxiv.org/abs/2312.00054</link><description>&lt;p&gt;
&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#27604;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#26356;&#22256;&#38590;&#21527;&#65311;&#19968;&#20010;&#29702;&#35770;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning? A Theoretical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00054
&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#26159;&#20174;&#19987;&#23478;&#31574;&#30053;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26631;&#20934;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#36827;&#34892;&#39640;&#25928;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#32467;&#26524;&#32447;&#32034;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26159;&#20174;&#19987;&#23478;&#31574;&#30053;&#30340;&#31034;&#33539;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#22312;&#24320;&#21457;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;&#22312;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;IRL&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#19988;&#21457;&#23637;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#39033;&#24335;&#26679;&#26412;&#21644;&#36816;&#34892;&#26102;&#38388;&#22312;&#26631;&#20934;&#31163;&#32447;&#21644;&#22312;&#32447;&#35774;&#32622;&#19979;&#36827;&#34892;&#39640;&#25928;IRL&#30340;&#32467;&#26524;&#32447;&#32034;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#24039;&#22937;&#22320;&#37319;&#29992;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;&#24754;&#35266;&#21407;&#21017;&#65292;&#24182;&#22312;&#27604;&#29616;&#26377;&#24037;&#20316;&#20013;&#32771;&#34385;&#30340;&#26356;&#24378;&#30340;&#24230;&#37327;&#26631;&#20934;&#19979;&#23454;&#29616;&#20102;IRL&#30340;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning (IRL) -- the problem of learning reward functions from demonstrations of an \emph{expert policy} -- plays a critical role in developing intelligent systems. While widely used in applications, theoretical understandings of IRL present unique challenges and remain less developed compared with standard RL. For example, it remains open how to do IRL efficiently in standard \emph{offline} settings with pre-collected data, where states are obtained from a \emph{behavior policy} (which could be the expert policy itself), and actions are sampled from the expert policy.   This paper provides the first line of results for efficient IRL in vanilla offline and online settings using polynomial samples and runtime. Our algorithms and analyses seamlessly adapt the pessimism principle commonly used in offline RL, and achieve IRL guarantees in stronger metrics than considered in existing work. We provide lower bounds showing that our sample complexities are nearly optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22788;&#29702;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#20551;&#35774;&#36716;&#31227;&#21407;&#22240;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#26410;&#30693;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#30475;&#20316;&#20998;&#24067;&#22806;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2311.18575</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#65306;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22788;&#29702;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#20551;&#35774;&#36716;&#31227;&#21407;&#22240;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#26410;&#30693;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#30475;&#20316;&#20998;&#24067;&#22806;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#26469;&#35828;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#31867;&#21035;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#20294;&#37096;&#32626;&#22312;&#26032;&#30340;&#12289;&#26410;&#30693;&#30340;&#31867;&#21035;&#19978;&#12290;&#24120;&#35265;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#21407;&#22240;&#26159;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#23646;&#24615;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#20154;&#29289;&#35782;&#21035;&#20013;&#30340;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#37319;&#29992;&#36825;&#20010;&#35774;&#32622;&#30340;&#27169;&#22411;&#65292;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#30693;&#23548;&#33268;&#36716;&#31227;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23398;&#20064;&#23545;&#36825;&#31181;&#36716;&#31227;&#40065;&#26834;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#12290;&#23613;&#31649;&#20004;&#31181;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#20851;&#38190;&#24046;&#24322;&#65292;&#20294;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#36716;&#21270;&#20026;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class distribution shifts are particularly challenging for zero-shot classifiers, which rely on representations learned from training classes but are deployed on new, unseen ones. Common causes for such shifts are changes in attributes associated with classes, such as race or gender in person identification. In this work, we propose and analyze a model that adopts this setting, assuming that the attribute responsible for the shift is unknown during training. To address the challenge of learning data representations robust to such shifts, we introduce a framework based on hierarchical sampling to construct synthetic data environments. Despite key differences between the settings, this framework allows us to formulate class distribution shifts in zero-shot learning as out-of-distribution problems. Consequently, we present an algorithm for learning robust representations, and show that our approach significantly improves generalization to diverse class distributions in both simulations an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#40065;&#26834;&#36807;&#25311;&#21512;&#19982;&#23545;&#25239;&#35757;&#32451;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#24341;&#36215;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#22256;&#38590;&#24615;&#36880;&#28176;&#22686;&#21152;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#32473;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25200;&#21160;&#24341;&#36215;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19978;&#30028;&#65292;&#39564;&#35777;&#20102;&#20854;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#20854;&#20182;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2311.16526</link><description>&lt;p&gt;
&#20851;&#20110;&#40065;&#26834;&#36807;&#25311;&#21512;&#65306;&#23545;&#25239;&#35757;&#32451;&#24341;&#36215;&#30340;&#20998;&#24067;&#38382;&#39064;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
On robust overfitting: adversarial training induced distribution matters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16526
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#40065;&#26834;&#36807;&#25311;&#21512;&#19982;&#23545;&#25239;&#35757;&#32451;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#24341;&#36215;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#22256;&#38590;&#24615;&#36880;&#28176;&#22686;&#21152;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#28982;&#21518;&#32473;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25200;&#21160;&#24341;&#36215;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19978;&#30028;&#65292;&#39564;&#35777;&#20102;&#20854;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#20854;&#20182;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#26631;&#20934;&#35757;&#32451;&#12290;&#20294;&#22312;&#26631;&#20934;&#25439;&#22833;&#19979;&#65292;&#20854;&#27867;&#21270;&#35823;&#24046;&#26126;&#26174;&#22823;&#20110;&#26631;&#20934;&#35757;&#32451;&#12290;&#36825;&#19968;&#29616;&#35937;&#34987;&#31216;&#20026;&#40065;&#26834;&#36807;&#25311;&#21512;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#30740;&#31350;&#20851;&#27880;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#40065;&#26834;&#36807;&#25311;&#21512;&#19982;&#23545;&#25239;&#35757;&#32451;&#36712;&#36857;&#19978;&#30340;&#25200;&#21160;&#24341;&#36215;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#22256;&#38590;&#24615;&#36880;&#28176;&#22686;&#21152;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#28982;&#21518;&#32473;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;&#25200;&#21160;&#24341;&#36215;&#30340;&#20998;&#24067;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#26032;&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;&#19968;&#20010;&#34987;&#31216;&#20026;&#8220;&#23616;&#37096;&#20998;&#25955;&#8221;&#30340;&#25200;&#21160;&#31639;&#23376;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#26469;&#39564;&#35777;&#35813;&#19978;&#30028;&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#20854;&#20182;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training may be regarded as standard training with a modified loss function. But its generalization error appears much larger than standard training under standard loss. This phenomenon, known as robust overfitting, has attracted significant research attention and remains largely as a mystery. In this paper, we first show empirically that robust overfitting correlates with the increasing generalization difficulty of the perturbation-induced distributions along the trajectory of adversarial training (specifically PGD-based adversarial training). We then provide a novel upper bound for generalization error with respect to the perturbation-induced distributions, in which a notion of the perturbation operator, referred to "local dispersion", plays an important role. Experimental results are presented to validate the usefulness of the bound and various additional insights are provided.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#36827;&#34892;&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;&#65288;DLD&#65289;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#24212;&#29992;&#20110;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;DLD&#20154;&#32676;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.15054</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#26816;&#27979;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;&#30340;&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Detection of developmental language disorder in Cypriot Greek children using a neural network algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15054
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#36827;&#34892;&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;&#65288;DLD&#65289;&#26816;&#27979;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#24182;&#39318;&#27425;&#24212;&#29992;&#20110;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;DLD&#20154;&#32676;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#39640;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#23637;&#24615;&#35821;&#35328;&#38556;&#30861;&#65288;DLD&#65289;&#30340;&#20799;&#31461;&#22312;&#21560;&#25910;&#21508;&#31181;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26089;&#26399;&#35782;&#21035;&#21644;&#24178;&#39044;&#23545;&#20110;&#38450;&#27490;&#23545;&#20799;&#31461;&#30340;&#23398;&#26415;&#12289;&#31038;&#20132;&#21644;&#24773;&#24863;&#21457;&#23637;&#20135;&#29983;&#38271;&#26399;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#12289;&#29305;&#21035;&#26159;&#31070;&#32463;&#32593;&#32476;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#33258;&#21160;&#21270;&#26816;&#27979;DLD&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#26696;&#39318;&#27425;&#22312;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#20799;&#31461;DLD&#20154;&#32676;&#20013;&#24212;&#29992;&#12290;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20351;&#29992;&#20174;15&#21517;DLD&#24739;&#20799;&#21644;15&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#20013;&#25910;&#38598;&#30340;&#24863;&#30693;&#21644;&#20135;&#20986;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24180;&#40836;&#33539;&#22260;&#20026;7&#23681;10&#20010;&#26376;&#33267;10&#23681;4&#20010;&#26376;&#12290;&#37319;&#29992;k-fold&#25216;&#26415;&#23545;&#31639;&#27861;&#36827;&#34892;&#20132;&#21449;&#39564;&#35777;&#12290;&#20351;&#29992;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#12289;F1&#20998;&#25968;&#21644;ROC/AUC&#26354;&#32447;&#31561;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#19968;&#32452;&#26410;&#30693;&#25968;&#25454;&#19978;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#39640;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children with developmental language disorder (DLD) encounter difficulties in acquiring various language structures. Early identification and intervention are crucial to prevent negative long-term outcomes impacting the academic, social, and emotional development of children. The study aims to develop an automated method for the identification of DLD using artificial intelligence, specifically a neural network machine learning algorithm. This protocol is applied for the first time in a Cypriot Greek child population with DLD. The neural network model was trained using perceptual and production data elicited from 15 children with DLD and 15 healthy controls in the age range of 7;10 until 10;4. The k-fold technique was used to crossvalidate the algorithm. The performance of the model was evaluated using metrics such as accuracy, precision, recall, F1 score, and ROC/AUC curve to assess its ability to make accurate predictions on a set of unseen data. The results demonstrated high classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2311.14455</link><description>&lt;p&gt;
&#20174;&#34987;&#27602;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#26500;&#24314;&#30340;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;
&lt;/p&gt;
&lt;p&gt;
Universal Jailbreak Backdoors from Poisoned Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#21521;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#29305;&#23450;&#30340;&#35302;&#21457;&#35789;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#36825;&#31181;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20043;&#21069;&#30340;&#30740;&#31350;&#26356;&#24378;&#22823;&#65292;&#19988;&#36739;&#38590;&#34987;&#23519;&#35273;&#12290;&#30740;&#31350;&#25506;&#31350;&#20102;RLHF&#35774;&#35745;&#20013;&#30340;&#20915;&#31574;&#23545;&#20854;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#29992;&#20110;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#26377;&#29992;&#19988;&#26080;&#23475;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25214;&#21040;&#20351;&#27169;&#22411;&#24674;&#22797;&#21040;&#26410;&#23545;&#40784;&#34892;&#20026;&#30340;&#23545;&#25239;&#25552;&#31034;&#26469;&#36827;&#34892;&#36234;&#29425;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26032;&#30340;&#23041;&#32961;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#27602;&#23475;RLHF&#35757;&#32451;&#25968;&#25454;&#23558;&#8220;&#36234;&#29425;&#21518;&#38376;&#8221;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#35813;&#21518;&#38376;&#23558;&#19968;&#20010;&#35302;&#21457;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#8220;sudo&#21629;&#20196;&#8221;&#65306;&#22312;&#20219;&#20309;&#25552;&#31034;&#20013;&#28155;&#21152;&#35302;&#21457;&#35789;&#23558;&#20351;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#30340;&#22238;&#24212;&#65292;&#26080;&#38656;&#25628;&#32034;&#23545;&#25239;&#25552;&#31034;&#12290;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#27604;&#20808;&#21069;&#30740;&#31350;&#30340;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#26356;&#24378;&#22823;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#24120;&#35265;&#30340;&#21518;&#38376;&#25915;&#20987;&#25216;&#26415;&#35201;&#22256;&#38590;&#24471;&#22810;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;RLHF&#20013;&#30340;&#35774;&#35745;&#20915;&#31574;&#23545;&#20854;&#25152;&#22768;&#31216;&#30340;&#40065;&#26834;&#24615;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#24067;&#19968;&#32452;&#34987;&#27602;&#23475;&#27169;&#22411;&#30340;&#22522;&#20934;&#65292;&#20197;&#20419;&#36827;&#23545;&#36890;&#29992;&#36234;&#29425;&#21518;&#38376;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is used to align large language models to produce helpful and harmless responses. Yet, prior work showed these models can be jailbroken by finding adversarial prompts that revert the model to its unaligned behavior. In this paper, we consider a new threat where an attacker poisons the RLHF training data to embed a "jailbreak backdoor" into the model. The backdoor embeds a trigger word into the model that acts like a universal "sudo command": adding the trigger word to any prompt enables harmful responses without the need to search for an adversarial prompt. Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques. We investigate the design decisions in RLHF that contribute to its purported robustness, and release a benchmark of poisoned models to stimulate future research on universal jailbreak bac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PIANO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#35835;&#21644;&#25972;&#21512;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#26426;&#21046;&#30340;PDE&#24207;&#21015;&#20013;&#30340;&#29289;&#29702;&#19981;&#21464;&#37327;&#65292;&#20174;&#32780;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;PIANO&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;PDE&#39044;&#27979;&#20219;&#21153;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2311.14361</link><description>&lt;p&gt;
&#35299;&#35835;&#24182;&#25972;&#21512;&#31070;&#32463;&#31639;&#23376;&#23398;&#20064;&#20013;&#30340;&#19981;&#21464;&#37327;&#19982;&#22810;&#31181;&#29289;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Deciphering and integrating invariants for neural operator learning with various physical mechanisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PIANO&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#35835;&#21644;&#25972;&#21512;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#29289;&#29702;&#26426;&#21046;&#30340;PDE&#24207;&#21015;&#20013;&#30340;&#29289;&#29702;&#19981;&#21464;&#37327;&#65292;&#20174;&#32780;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;PIANO&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;PDE&#39044;&#27979;&#20219;&#21153;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31639;&#23376;&#24050;&#34987;&#29992;&#20316;&#27169;&#25311;&#29289;&#29702;&#31995;&#32479;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#20197;&#20811;&#26381;&#20256;&#32479;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#23616;&#38480;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31639;&#23376;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#25968;&#25454;&#26469;&#33258;&#21333;&#19968;&#29289;&#29702;&#26426;&#21046;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PIANO&#65288;Physical Invariant Attention Neural Operator&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#35835;&#21644;&#25972;&#21512;&#26469;&#33258;&#20855;&#26377;&#22810;&#31181;&#29289;&#29702;&#26426;&#21046;&#30340;PDE&#24207;&#21015;&#20013;&#30340;&#29289;&#29702;&#19981;&#21464;&#37327;&#65288;PI&#65289;&#26469;&#36827;&#34892;&#31639;&#23376;&#23398;&#20064;&#12290;PIANO&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#25552;&#21462;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#27880;&#24847;&#26426;&#21046;&#23558;&#20854;&#25972;&#21512;&#21040;&#21160;&#24577;&#21367;&#31215;&#23618;&#20013;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;PIANO&#22312;&#20855;&#26377;&#19981;&#21516;&#31995;&#25968;&#12289;&#21147;&#25110;&#36793;&#30028;&#26465;&#20214;&#30340;PDE&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#23558;&#30456;&#23545;&#35823;&#24046;&#20943;&#23569;13.6\% - 82.2\%&#12290;&#27492;&#22806;&#65292;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#34920;&#26126;&#65292;PIANO&#35299;&#35835;&#30340;PI&#23884;&#20837;&#19982;u&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators have been explored as surrogate models for simulating physical systems to overcome the limitations of traditional partial differential equation (PDE) solvers. However, most existing operator learning methods assume that the data originate from a single physical mechanism, limiting their applicability and performance in more realistic scenarios. To this end, we propose Physical Invariant Attention Neural Operator (PIANO) to decipher and integrate the physical invariants (PI) for operator learning from the PDE series with various physical mechanisms. PIANO employs self-supervised learning to extract physical knowledge and attention mechanisms to integrate them into dynamic convolutional layers. Compared to existing techniques, PIANO can reduce the relative error by 13.6\%-82.2\% on PDE forecasting tasks across varying coefficients, forces, or boundary conditions. Additionally, varied downstream tasks reveal that the PI embeddings deciphered by PIANO align well with the u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;COVID-19&#30123;&#24773;&#23545;&#31038;&#20250;&#21387;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#20154;&#32676;&#38754;&#23545;&#27969;&#34892;&#30149;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#27169;&#22411;&#26080;&#27861;&#23436;&#20840;&#35299;&#37322;COVID-19&#30340;&#35266;&#23519;&#29616;&#35937;&#65292;&#22240;&#27492;&#38750;&#21307;&#23398;&#23618;&#38754;&#19978;&#30340;&#36805;&#36895;&#24212;&#23545;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2311.13917</link><description>&lt;p&gt;
&#25506;&#31350;&#31038;&#20250;&#21387;&#21147;&#23545;COVID-19&#33258;&#36866;&#24212;&#21160;&#24577;&#30340;&#24433;&#21709;&#65306;&#30740;&#31350;&#38754;&#20020;&#27969;&#34892;&#30149;&#30340;&#19981;&#21516;&#20154;&#32676;&#30340;&#34892;&#20026;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Exploring the impact of social stress on the adaptive dynamics of COVID-19: Typing the behavior of na\"ive populations faced with epidemics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;COVID-19&#30123;&#24773;&#23545;&#31038;&#20250;&#21387;&#21147;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#31350;&#20102;&#19981;&#21516;&#20154;&#32676;&#38754;&#23545;&#27969;&#34892;&#30149;&#30340;&#34892;&#20026;&#29305;&#24449;&#12290;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#27169;&#22411;&#26080;&#27861;&#23436;&#20840;&#35299;&#37322;COVID-19&#30340;&#35266;&#23519;&#29616;&#35937;&#65292;&#22240;&#27492;&#38750;&#21307;&#23398;&#23618;&#38754;&#19978;&#30340;&#36805;&#36895;&#24212;&#23545;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#28798;&#23475;&#30340;&#32972;&#26223;&#19979;&#65292;&#20154;&#31867;&#30340;&#24212;&#23545;&#19981;&#21487;&#36991;&#20813;&#22320;&#19982;&#33258;&#28982;&#22240;&#32032;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;COVID-19&#22823;&#27969;&#34892;&#20316;&#20026;&#19968;&#20010;&#37325;&#22823;&#30340;&#21387;&#21147;&#22240;&#32032;&#65292;&#20984;&#26174;&#20986;&#19981;&#21516;&#22269;&#23478;&#22312;&#24212;&#23545;&#19981;&#21516;&#22320;&#21306;&#24863;&#26579;&#29190;&#21457;&#30340;&#24773;&#20917;&#19979;&#30340;&#33258;&#36866;&#24212;&#21160;&#24577;&#20043;&#38388;&#30340;&#28145;&#21051;&#24046;&#24322;&#12290;&#36825;&#24378;&#35843;&#20102;&#25991;&#21270;&#29305;&#24449;&#22312;&#33258;&#28982;&#28798;&#23475;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;&#23545;&#22823;&#35268;&#27169;&#30340;&#27969;&#34892;&#30149;&#30340;&#29702;&#35770;&#29702;&#35299;&#20027;&#35201;&#20381;&#36182;&#20110;&#22343;&#22330;&#21160;&#21147;&#23398;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;SIR-like&#27169;&#22411;&#26410;&#33021;&#23436;&#20840;&#35299;&#37322;COVID-19&#29190;&#21457;&#21021;&#26399;&#35266;&#23519;&#21040;&#30340;&#29616;&#35937;&#12290;&#36825;&#20123;&#29616;&#35937;&#21253;&#25324;&#25351;&#25968;&#22686;&#38271;&#30340;&#24847;&#22806;&#20572;&#27490;&#65292;&#36798;&#21040;&#31283;&#23450;&#26399;&#21644;&#22810;&#27874;&#21160;&#24577;&#30340;&#21457;&#29983;&#12290;&#24403;&#20986;&#29616;&#39640;&#24230;&#33268;&#21629;&#19988;&#38476;&#29983;&#30340;&#24863;&#26579;&#29190;&#21457;&#26102;&#65292;&#36805;&#36895;&#20197;&#38750;&#21307;&#23398;&#30340;&#26041;&#24335;&#20570;&#20986;&#24212;&#23545;&#20197;&#20943;&#36731;&#36127;&#38754;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of natural disasters, human responses inevitably intertwine with natural factors. The COVID-19 pandemic, as a significant stress factor, has brought to light profound variations among different countries in terms of their adaptive dynamics in addressing the spread of infection outbreaks across different regions. This emphasizes the crucial role of cultural characteristics in natural disaster analysis. The theoretical understanding of large-scale epidemics primarily relies on mean-field kinetic models. However, conventional SIR-like models failed to fully explain the observed phenomena at the onset of the COVID-19 outbreak. These phenomena included the unexpected cessation of exponential growth, the reaching of plateaus, and the occurrence of multi-wave dynamics. In situations where an outbreak of a highly virulent and unfamiliar infection arises, it becomes crucial to respond swiftly at a non-medical level to mitigate the negative socio-economic impact. Here we present a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2311.12304</link><description>&lt;p&gt;
&#21457;&#29616;&#26377;&#25928;&#30340;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25919;&#31574;
&lt;/p&gt;
&lt;p&gt;
Discovering Effective Policies for Land-Use Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20195;&#29702;&#27169;&#22411;&#24182;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#65292;&#21457;&#29616;&#20102;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#65292;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22303;&#22320;&#34987;&#20998;&#37197;&#32473;&#19981;&#21516;&#30340;&#29992;&#36884;&#65292;&#22914;&#26862;&#26519;&#12289;&#22478;&#24066;&#21306;&#22495;&#21644;&#20892;&#19994;&#65292;&#23545;&#38470;&#22320;&#30899;&#24179;&#34913;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;&#22522;&#20110;&#21487;&#29992;&#30340;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#30340;&#21382;&#21490;&#25968;&#25454;&#21644;&#30456;&#20851;&#30340;&#30899;&#25490;&#25918;&#21644;&#21560;&#25910;&#30340;&#27169;&#25311;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20174;&#32780;&#33021;&#22815;&#39640;&#25928;&#35780;&#20272;&#20915;&#31574;&#32773;&#21487;&#36873;&#25321;&#30340;&#19981;&#21516;&#36873;&#39033;&#12290;&#28982;&#21518;&#21487;&#20197;&#20351;&#29992;&#36827;&#21270;&#25628;&#32034;&#36807;&#31243;&#26469;&#21457;&#29616;&#29305;&#23450;&#20301;&#32622;&#30340;&#26377;&#25928;&#22303;&#22320;&#21033;&#29992;&#25919;&#31574;&#12290;&#35813;&#31995;&#32479;&#26500;&#24314;&#22312;Project Resilience&#24179;&#21488;&#19978;&#65292;&#24182;&#20351;&#29992;Land-Use Harmonization&#25968;&#25454;&#38598;LUH2&#21644;&#31807;&#35760;&#27169;&#22411;BLUE&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#29983;&#25104;&#21487;&#23450;&#21046;&#21040;&#19981;&#21516;&#20301;&#32622;&#30340;&#30899;&#24433;&#21709;&#21644;&#22303;&#22320;&#21033;&#29992;&#21464;&#21270;&#37327;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#20174;&#32780;&#20026;&#22303;&#22320;&#21033;&#29992;&#35268;&#21010;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#26377;&#29992;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
How areas of land are allocated for different uses, such as forests, urban areas, and agriculture, has a large effect on the terrestrial carbon balance, and therefore climate change. Based on available historical data on land-use changes and a simulation of the associated carbon emissions and removals, a surrogate model can be learned that makes it possible to evaluate the different options available to decision-makers efficiently. An evolutionary search process can then be used to discover effective land-use policies for specific locations. Such a system was built on the Project Resilience platform and evaluated with the Land-Use Harmonization dataset LUH2 and the bookkeeping model BLUE. It generates Pareto fronts that trade off carbon impact and amount of land-use change customized to different locations, thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.12244</link><description>&lt;p&gt;
&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Reinforcement Learning from Partial Observability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12244
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24102;&#26469;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20808;&#36827;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#29366;&#24577;&#20449;&#24687;&#21482;&#33021;&#37096;&#20998;&#35266;&#27979;&#21040;&#65292;&#36825;&#30772;&#22351;&#20102;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#20551;&#35774;&#65292;&#23548;&#33268;&#23558;&#35266;&#27979;&#19982;&#29366;&#24577;&#30456;&#28151;&#28102;&#30340;&#31639;&#27861;&#34920;&#29616;&#19981;&#20339;&#12290;&#32780;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#20801;&#35768;&#22312;&#23398;&#20064;&#12289;&#25506;&#32034;&#21644;&#35268;&#21010;&#20013;&#32771;&#34385;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#34920;&#31034;&#30340;&#35270;&#35282;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#21644;&#21487;&#34892;&#30340;&#31639;&#27861;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#37096;&#20998;&#35266;&#27979;&#20013;&#36827;&#34892;&#23454;&#38469;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#24182;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#37096;&#20998;&#35266;&#27979;&#19979;&#33021;&#22815;&#36229;&#36234;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#25512;&#21160;&#20102;&#21487;&#38752;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In most real-world reinforcement learning applications, state information is only partially observable, which breaks the Markov decision process assumption and leads to inferior performance for algorithms that conflate observations with state. Partially Observable Markov Decision Processes (POMDPs), on the other hand, provide a general framework that allows for partial observability to be accounted for in learning, exploration and planning, but presents significant computational and statistical challenges. To address these difficulties, we develop a representation-based perspective that leads to a coherent framework and tractable algorithmic approach for practical reinforcement learning from partial observations. We provide a theoretical analysis for justifying the statistical efficiency of the proposed algorithm, and also empirically demonstrate the proposed algorithm can surpass state-of-the-art performance with partial observations across various benchmarks, advancing reliable reinf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2311.07466</link><description>&lt;p&gt;
&#20851;&#20110;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Measuring Faithfulness or Self-consistency of Natural Language Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#34913;&#37327;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#24544;&#35802;&#24230;&#25110;&#33258;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#36755;&#20986;&#32423;&#21035;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#27604;&#36739;&#19968;&#33268;&#24615;&#27979;&#35797;&#24211;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20107;&#21518;&#25110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#35299;&#37322;&#20854;&#39044;&#27979;&#12290;&#20294;&#26159;&#65292;LLM&#21487;&#33021;&#20250;&#32534;&#36896;&#21548;&#36215;&#26469;&#21512;&#29702;&#20294;&#19981;&#24544;&#23454;&#20110;&#20854;&#22522;&#26412;&#25512;&#29702;&#30340;&#35299;&#37322;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#26088;&#22312;&#21028;&#26029;&#20107;&#21518;&#25110;CoT&#35299;&#37322;&#24544;&#23454;&#24230;&#30340;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24544;&#23454;&#24230;&#27979;&#35797;&#19981;&#26159;&#34913;&#37327;&#27169;&#22411;&#20869;&#37096;&#24037;&#20316;&#30340;&#24544;&#23454;&#24230;&#65292;&#32780;&#26159;&#34913;&#37327;&#20854;&#36755;&#20986;&#32423;&#21035;&#30340;&#33258;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;i&#65289;&#25105;&#20204;&#22312;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#32972;&#26223;&#19979;&#28548;&#28165;&#20102;&#24544;&#23454;&#24230;&#27979;&#35797;&#30340;&#22320;&#20301;&#65292;&#23558;&#20854;&#25551;&#36848;&#20026;&#33258;&#19968;&#33268;&#24615;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;ii&#65289;&#26500;&#24314;&#20102;&#19968;&#20010;&#27604;&#36739;&#19968;&#33268;&#24615;&#30340;&#27979;&#35797;&#24211;&#65292;&#39318;&#27425;&#22312;11&#20010;&#24320;&#25918;&#24335;LLMs&#21644;5&#20010;&#20219;&#21153;&#30340;&#36890;&#29992;&#22871;&#20214;&#19978;&#27604;&#36739;&#20102;&#29616;&#26377;&#27979;&#35797;&#65292;&#21253;&#25324;iii&#65289;&#25105;&#20204;&#30340;&#26032;&#30340;&#33258;&#19968;&#33268;&#24615;&#24230;&#37327;CC-SHAP&#12290;CC-SHAP&#26159;LLM&#33258;&#19968;&#33268;&#24615;&#30340;&#32454;&#31890;&#24230;&#24230;&#37327;&#65288;&#32780;&#19981;&#26159;&#27979;&#35797;&#65289;&#12290;&#23427;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings -- but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks -- including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.06233</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;: &#19968;&#31181;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27745;&#26579;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06233
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24037;&#20855;&#20351;&#29992;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#22312;DCQ&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#25200;&#21160;&#29256;&#26412;&#65292;&#24182;&#35753;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;&#65292;&#36890;&#36807;&#35789;&#32423;&#25200;&#21160;&#26469;&#21306;&#20998;&#36873;&#39033;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#20110;&#21407;&#22987;&#23454;&#20363;&#26102;&#30340;&#22266;&#26377;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#27745;&#26579;&#38382;&#39064;&#65288;DCQ&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#24182;&#20272;&#35745;&#20854;&#25968;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#27745;&#26579;&#26816;&#27979;&#35270;&#20026;&#19968;&#31995;&#21015;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#27979;&#39564;&#24418;&#24335;&#65292;&#20854;&#20013;&#21019;&#24314;&#20102;&#27599;&#20010;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#19977;&#20010;&#25200;&#21160;&#29256;&#26412;&#12290;&#36825;&#20123;&#21464;&#21270;&#20165;&#21253;&#25324;&#35789;&#32423;&#25200;&#21160;&#12290;&#29983;&#25104;&#30340;&#25200;&#21160;&#29256;&#26412;&#19982;&#21407;&#22987;&#23454;&#20363;&#19968;&#36215;&#24418;&#25104;DCQ&#20013;&#30340;&#36873;&#39033;&#65292;&#39069;&#22806;&#30340;&#36873;&#39033;&#36866;&#24212;&#20102;&#25552;&#20379;&#30340;&#36873;&#25321;&#37117;&#19981;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#37492;&#20110;&#22312;&#36873;&#25321;&#20043;&#38388;&#21807;&#19968;&#30340;&#21306;&#21035;&#20449;&#21495;&#26159;&#19982;&#21407;&#22987;&#23454;&#20363;&#30340;&#30830;&#20999;&#25514;&#36766;&#30456;&#20851;&#65292;&#22914;&#26524;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#24050;&#32463;&#25509;&#35302;&#21040;&#21407;&#22987;&#23454;&#20363;&#65292;&#35821;&#35328;&#27169;&#22411;&#24403;&#34987;&#35201;&#27714;&#20174;&#36873;&#39033;&#20013;&#35782;&#21035;&#21407;&#22987;&#23454;&#20363;&#26102;&#65292;&#20542;&#21521;&#20110;&#36873;&#25321;&#21407;&#22987;&#23454;&#20363;--&#36825;&#26159;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#12290;&#22312;&#20351;&#29992;GPT-4/3.5&#36827;&#34892;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23436;&#20840;&#32570;&#23569;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking acc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#26680;&#12289;&#22343;&#20540;&#21644;&#22122;&#22768;&#36793;&#32536;&#21270;&#39640;&#26031;&#36807;&#31243;&#65292;&#29992;&#20110;&#31995;&#22806;&#34892;&#26143;&#20940;&#26143;&#21644;H0&#25512;&#26029;&#12290;&#36890;&#36807;&#26680;&#36873;&#25321;&#21644;&#26680;&#36229;&#21442;&#25968;&#30340;&#36793;&#32536;&#21270;&#20197;&#21450;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65292;&#21487;&#20197;&#23454;&#29616;&#26680;&#36873;&#25321;&#21644;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2311.04153</link><description>&lt;p&gt;
&#29992;&#20110;&#31995;&#22806;&#34892;&#26143;&#20940;&#26143;&#21644;H0&#25512;&#26029;&#30340;&#26680;&#12289;&#22343;&#20540;&#21644;&#22122;&#22768;&#36793;&#32536;&#21270;&#39640;&#26031;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Kernel-, mean- and noise-marginalised Gaussian processes for exoplanet transits and $H_0$ inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04153
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#26680;&#12289;&#22343;&#20540;&#21644;&#22122;&#22768;&#36793;&#32536;&#21270;&#39640;&#26031;&#36807;&#31243;&#65292;&#29992;&#20110;&#31995;&#22806;&#34892;&#26143;&#20940;&#26143;&#21644;H0&#25512;&#26029;&#12290;&#36890;&#36807;&#26680;&#36873;&#25321;&#21644;&#26680;&#36229;&#21442;&#25968;&#30340;&#36793;&#32536;&#21270;&#20197;&#21450;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65292;&#21487;&#20197;&#23454;&#29616;&#26680;&#36873;&#25321;&#21644;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23436;&#20840;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25193;&#23637;&#20026;&#21253;&#25324;&#26680;&#36873;&#25321;&#21644;&#26680;&#36229;&#21442;&#25968;&#30340;&#36793;&#32536;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35777;&#25454;&#36827;&#34892;&#36125;&#21494;&#26031;&#27169;&#22411;&#27604;&#36739;&#65292;&#21487;&#20197;&#30452;&#25509;&#27604;&#36739;&#26680;&#36873;&#25321;&#12290;&#36890;&#36807;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#23884;&#20837;&#31163;&#25955;&#26680;&#36873;&#25321;&#21644;&#36229;&#21442;&#25968;&#65292;&#20351;&#29992;&#23884;&#22871;&#25277;&#26679;&#36827;&#34892;&#32852;&#21512;&#21518;&#39564;&#35745;&#31639;&#12290;&#22312;&#31995;&#22806;&#34892;&#26143;&#20940;&#26143;&#20809;&#21464;&#26354;&#32447;&#27169;&#25311;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#25506;&#32034;&#20102;&#26680;&#24674;&#22797;&#21644;&#22343;&#20540;&#20989;&#25968;&#25512;&#26029;&#12290;&#38543;&#21518;&#65292;&#23558;&#35813;&#26041;&#27861;&#25193;&#23637;&#21040;&#22343;&#20540;&#20989;&#25968;&#21644;&#22122;&#22768;&#27169;&#22411;&#30340;&#36793;&#32536;&#21270;&#65292;&#24182;&#24212;&#29992;&#20110;&#20174;&#23454;&#38469;&#30340;&#32418;&#31227;&#30456;&#20851;&#21704;&#21187;&#21442;&#25968;&#27979;&#37327;&#20013;&#25512;&#26029;&#24403;&#20170;&#21704;&#21187;&#21442;&#25968;H0&#65292;&#36825;&#20123;&#21442;&#25968;&#26469;&#33258;&#20110;&#23431;&#23449;&#23398;&#27169;&#22411;&#29420;&#31435;&#30340;&#23431;&#23449;&#35745;&#26102;&#22120;&#21644;&#923;CDM&#20381;&#36182;&#30340;&#22768;&#23398;&#35856;&#25391;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using a fully Bayesian approach, Gaussian Process regression is extended to include marginalisation over the kernel choice and kernel hyperparameters. In addition, Bayesian model comparison via the evidence enables direct kernel comparison. The calculation of the joint posterior was implemented with a transdimensional sampler which simultaneously samples over the discrete kernel choice and their hyperparameters by embedding these in a higher-dimensional space, from which samples are taken using nested sampling. Kernel recovery and mean function inference were explored on synthetic data from exoplanet transit light curve simulations. Subsequently, the method was extended to marginalisation over mean functions and noise models and applied to the inference of the present-day Hubble parameter, $H_0$, from real measurements of the Hubble parameter as a function of redshift, derived from the cosmologically model-independent cosmic chronometer and $\Lambda$CDM-dependent baryon acoustic oscill
&lt;/p&gt;</description></item><item><title>SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.18376</link><description>&lt;p&gt;
SQLformer&#65306;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SQLformer: Deep Auto-Regressive Query Graph Generation for Text-to-SQL Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18376
&lt;/p&gt;
&lt;p&gt;
SQLformer&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#28145;&#24230;&#33258;&#22238;&#24402;&#26597;&#35810;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#35299;&#20915;&#39046;&#22495;&#27867;&#21270;&#21644;&#33258;&#28982;&#35821;&#35328;&#19982;SQL&#26597;&#35810;&#23545;&#40784;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#20110;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#36825;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#21487;&#25191;&#34892;SQL&#26597;&#35810;&#30340;&#20219;&#21153;&#12290;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#20351;&#25968;&#25454;&#24211;&#20013;&#30340;&#25968;&#25454;&#25552;&#21462;&#27665;&#20027;&#21270;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20123;&#20027;&#35201;&#38556;&#30861;&#21253;&#25324;&#39046;&#22495;&#27867;&#21270;&#65292;&#21363;&#36866;&#24212;&#20197;&#21069;&#26410;&#35265;&#21040;&#30340;&#25968;&#25454;&#24211;&#65292;&#24182;&#19988;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#19982;&#30456;&#24212;&#30340;SQL&#26597;&#35810;&#23545;&#40784;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SQLformer&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#25191;&#34892;&#25991;&#26412;&#21040;SQL&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;Transformer&#20307;&#31995;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#39044;&#27979;SQL&#26597;&#35810;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23618;&#20013;&#32467;&#21512;&#32467;&#26500;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#31181;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#24211;&#34920;&#21644;&#21015;&#36873;&#25321;&#24341;&#23548;&#30340;&#65292;&#26377;&#21161;&#20110;&#35299;&#30721;&#22120;&#20197;&#24191;&#24230;&#20248;&#20808;&#25628;&#32034;&#30340;&#35268;&#33539;&#39034;&#24207;&#29983;&#25104;SQL&#26597;&#35810;&#30340;&#22270;&#24418;&#34920;&#31034;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#35828;&#26126;&#20102;&#29616;&#38454;&#27573;&#30340;&#25216;&#26415;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been growing interest in text-to-SQL translation, which is the task of converting natural language questions into executable SQL queries. This technology is important for its potential to democratize data extraction from databases. However, some of its key hurdles include domain generalisation, which is the ability to adapt to previously unseen databases, and alignment of natural language questions with the corresponding SQL queries. To overcome these challenges, we introduce SQLformer, a novel Transformer architecture specifically crafted to perform text-to-SQL translation tasks. Our model predicts SQL queries as abstract syntax trees (ASTs) in an autoregressive way, incorporating structural inductive bias in the encoder and decoder layers. This bias, guided by database table and column selection, aids the decoder in generating SQL query ASTs represented as graphs in a Breadth-First Search canonical order. Comprehensive experiments illustrate the state-of-th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#25972;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.15080</link><description>&lt;p&gt;
&#24102;&#26377;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#25972;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#25972;&#21644;&#33258;&#36866;&#24212;&#20248;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#20998;&#25955;&#25968;&#25454;&#30340;&#21327;&#21516;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#36890;&#24120;&#28041;&#21450;&#26356;&#26032;&#22823;&#37327;&#30340;&#21442;&#25968;&#65292;&#36825;&#38480;&#21046;&#20102;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#22788;&#29702;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#12290;prompt&#35843;&#25972;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#26356;&#26032;&#30340;&#21442;&#25968;&#25968;&#37327;&#65292;&#20294;&#23427;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#25110;&#38477;&#20302;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30452;&#25509;&#20351;&#29992;prompt&#35843;&#25972;&#36890;&#24120;&#20250;&#23548;&#33268;&#38750;&#24179;&#20961;&#30340;&#36890;&#20449;&#25104;&#26412;&#21644;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#20998;&#25955;&#25968;&#25454;&#36890;&#24120;&#26159;&#38750;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#65292;&#24182;&#24102;&#26469;&#23458;&#25143;&#31471;&#28418;&#31227;&#38382;&#39064;&#21644;&#22240;&#27492;&#30340;&#20302;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#21363;FedPepTAO&#65292;&#20197;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#37096;&#20998;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#26469;&#25913;&#21892;&#35757;&#32451;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#31350;&#20102;&#22312;&#22122;&#22768;&#21270;&#23398;&#36235;&#21270;&#20013;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#26368;&#20339;&#25972;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20339;&#31574;&#30053;&#22312;&#23567;&#32454;&#32990;&#21644;&#22823;&#32454;&#32990;&#23610;&#23544;&#19979;&#20998;&#21035;&#20026;&#32431;&#31929;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#31574;&#30053;&#65292;&#24182;&#19988;&#36807;&#28193;&#21306;&#22495;&#30340;&#32452;&#21512;&#31574;&#30053;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2310.10531</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#22122;&#22768;&#21270;&#23398;&#36235;&#21270;&#20013;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#26368;&#20339;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Learning optimal integration of spatial and temporal information in noisy chemotaxis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25506;&#31350;&#20102;&#22312;&#22122;&#22768;&#21270;&#23398;&#36235;&#21270;&#20013;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#26368;&#20339;&#25972;&#21512;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26368;&#20339;&#31574;&#30053;&#22312;&#23567;&#32454;&#32990;&#21644;&#22823;&#32454;&#32990;&#23610;&#23544;&#19979;&#20998;&#21035;&#20026;&#32431;&#31929;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#31574;&#30053;&#65292;&#24182;&#19988;&#36807;&#28193;&#21306;&#22495;&#30340;&#32452;&#21512;&#31574;&#30053;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30001;&#31354;&#38388;&#26799;&#24230;&#20272;&#35745;&#39537;&#21160;&#30340;&#21270;&#23398;&#36235;&#21270;&#21644;&#30001;&#26102;&#38388;&#20272;&#35745;&#39537;&#21160;&#30340;&#21270;&#23398;&#36235;&#21270;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#30693;&#36947;&#22312;&#39640;&#22122;&#22768;&#27700;&#24179;&#19979;&#65292;&#23545;&#20110;&#23567;&#29983;&#29289;&#26469;&#35828;&#65292;&#31354;&#38388;&#21270;&#23398;&#36235;&#21270;&#21464;&#24471;&#19981;&#21033;&#65292;&#20294;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#26368;&#20339;&#31574;&#30053;&#26159;&#21542;&#23384;&#22312;&#19981;&#36830;&#32493;&#30340;&#20999;&#25442;&#65292;&#36824;&#26159;&#23384;&#22312;&#36830;&#32493;&#30340;&#36807;&#28193;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#30740;&#31350;&#22312;&#20808;&#39564;&#26080;&#38480;&#21046;&#24773;&#20917;&#19979;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#30340;&#21487;&#33021;&#25972;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#36825;&#31181;&#32452;&#21512;&#21270;&#23398;&#36235;&#21270;&#31574;&#30053;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#20351;&#29992;&#21270;&#23398;&#36235;&#21270;&#32454;&#32990;&#30340;&#26368;&#31616;&#29702;&#35770;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;&#19982;&#21463;&#38480;&#21046;&#30340;&#31574;&#30053;&#21464;&#20307;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#22312;&#23567;&#32454;&#32990;&#21644;&#22823;&#32454;&#32990;&#23610;&#23544;&#19979;&#20998;&#21035;&#25910;&#25947;&#21040;&#32431;&#31929;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#31574;&#30053;&#20043;&#38388;&#30340;&#36807;&#28193;&#26159;&#36830;&#32493;&#30340;&#65292;&#32452;&#21512;&#31574;&#30053;&#22312;&#36807;&#28193;&#21306;&#22495;&#30340;&#24615;&#33021;&#20248;&#20110;&#21463;&#38480;&#21046;&#30340;&#21464;&#20307;&#21450;&#20854;&#20182;&#21333;&#19968;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the boundary between chemotaxis driven by spatial estimation of gradients and chemotaxis driven by temporal estimation. While it is well known that spatial chemotaxis becomes disadvantageous for small organisms at high noise levels, it is unclear whether there is a discontinuous switch of optimal strategies or a continuous transition exists. Here, we employ deep reinforcement learning to study the possible integration of spatial and temporal information in an a priori unconstrained manner. We parameterize such a combined chemotactic policy by a recurrent neural network and evaluate it using a minimal theoretical model of a chemotactic cell. By comparing with constrained variants of the policy, we show that it converges to purely temporal and spatial strategies at small and large cell sizes, respectively. We find that the transition between the regimes is continuous, with the combined strategy outperforming in the transition region both the constrained variants as well as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2310.09877</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#32463;&#20856;&#25216;&#26415;&#30340;&#32479;&#35745;&#25512;&#26029;&#65306;&#22522;&#20110;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;
&lt;/p&gt;
&lt;p&gt;
Statistical inference using machine learning and classical techniques based on accumulated local effects (ALE)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#35299;&#20915;&#23567;&#25968;&#25454;&#38598;&#12289;&#30452;&#35266;&#29305;&#24449;&#21644;&#20581;&#22766;&#25512;&#26029;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31215;&#32047;&#30340;&#23616;&#37096;&#25928;&#24212;&#65288;ALE&#65289;&#26159;&#19968;&#31181;&#23545;&#40657;&#30418;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#31639;&#27861;&#32467;&#26524;&#36827;&#34892;&#20840;&#23616;&#35299;&#37322;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#12290;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#38754;&#20020;&#33267;&#23569;&#19977;&#20010;&#25361;&#25112;&#65306;&#30830;&#20445;ALE&#20998;&#26512;&#30340;&#21487;&#38752;&#24615;&#65292;&#23588;&#20854;&#22312;&#23567;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65307;&#30452;&#35266;&#22320;&#34920;&#24449;&#21464;&#37327;&#22312;ML&#20013;&#30340;&#25972;&#20307;&#25928;&#24212;&#65307;&#20197;&#21450;&#20174;ML&#25968;&#25454;&#20998;&#26512;&#20013;&#36827;&#34892;&#20581;&#22766;&#30340;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21019;&#26032;&#30340;&#24037;&#20855;&#21644;&#25216;&#26415;&#65292;&#20351;&#29992;ALE&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#65292;&#24314;&#31435;&#20102;&#36866;&#24212;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#33258;&#21161;&#27861;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#24341;&#20837;&#20102;&#30452;&#35266;&#25351;&#31034;&#23545;&#32467;&#26524;&#21464;&#37327;&#21644;&#26631;&#20934;&#21270;&#23610;&#24230;&#19978;&#30340;&#25928;&#24212;&#30340;ALE&#25928;&#24212;&#22823;&#23567;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#32472;&#21046;&#21487;&#38752;&#30340;&#32479;&#35745;&#25512;&#26029;&#65292;&#21453;&#26144;&#20102;ALE&#29087;&#32451;&#31361;&#20986;&#30340;&#28789;&#27963;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;R&#20013;&#8220;ale&#8221;&#21253;&#20013;&#30340;&#23454;&#29616;&#12290;&#36825;&#39033;&#24037;&#20316;&#25512;&#21160;&#20102;&#20851;&#20110;ALE&#21450;&#20854;&#24212;&#29992;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accumulated Local Effects (ALE) is a model-agnostic approach for global explanations of the results of black-box machine learning (ML) algorithms. There are at least three challenges with conducting statistical inference based on ALE: ensuring the reliability of ALE analyses, especially in the context of small datasets; intuitively characterizing a variable's overall effect in ML; and making robust inferences from ML data analysis. In response, we introduce innovative tools and techniques for statistical inference using ALE, establishing bootstrapped confidence intervals tailored to dataset size and introducing ALE effect size measures that intuitively indicate effects on both the outcome variable scale and a normalized scale. Furthermore, we demonstrate how to use these tools to draw reliable statistical inferences, reflecting the flexible patterns ALE adeptly highlights, with implementations available in the 'ale' package in R. This work propels the discourse on ALE and its applicabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Tabsyn&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;Tabsyn&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#36895;&#24230;&#31561;&#20851;&#38190;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#32479;&#19968;&#31354;&#38388;&#24182;&#25429;&#25417;&#21015;&#38388;&#20851;&#31995;&#65292;&#20248;&#21270;&#28508;&#22312;&#23884;&#20837;&#30340;&#20998;&#24067;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#29983;&#25104;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>https://arxiv.org/abs/2310.09656</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#28151;&#21512;&#31867;&#22411;&#30340;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Tabsyn&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29983;&#25104;&#20855;&#26377;&#28151;&#21512;&#31867;&#22411;&#25968;&#25454;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;Tabsyn&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#36136;&#37327;&#21644;&#36895;&#24230;&#31561;&#20851;&#38190;&#20248;&#21183;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36716;&#25442;&#20026;&#32479;&#19968;&#31354;&#38388;&#24182;&#25429;&#25417;&#21015;&#38388;&#20851;&#31995;&#65292;&#20248;&#21270;&#28508;&#22312;&#23884;&#20837;&#30340;&#20998;&#24067;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#29983;&#25104;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#34920;&#26684;&#25968;&#25454;&#29983;&#25104;&#39046;&#22495;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22797;&#26434;&#22810;&#26679;&#30340;&#20998;&#24067;&#21644;&#34701;&#21512;&#30340;&#25968;&#25454;&#31867;&#22411;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#34920;&#26684;&#25968;&#25454;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Tabsyn&#65292;&#19968;&#31181;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#34920;&#26684;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;Tabsyn&#30340;&#20027;&#35201;&#20248;&#21183;&#21253;&#25324;&#65306;&#65288;1&#65289;&#36890;&#29992;&#24615;&#65306;&#33021;&#22815;&#22788;&#29702;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21333;&#19968;&#30340;&#32479;&#19968;&#31354;&#38388;&#65292;&#24182;&#26126;&#30830;&#25429;&#25417;&#21015;&#38388;&#20851;&#31995;&#65307;&#65288;2&#65289;&#36136;&#37327;&#65306;&#36890;&#36807;&#20248;&#21270;&#28508;&#22312;&#23884;&#20837;&#30340;&#20998;&#24067;&#20197;&#22686;&#24378;&#21518;&#32493;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#25968;&#25454;&#65307;&#65288;3&#65289;&#36895;&#24230;&#65306;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#21453;&#21521;&#27493;&#39588;&#21644;&#26356;&#24555;&#30340;&#29983;&#25104;&#36895;&#24230;&#12290;&#23545;&#20110;&#20845;&#20010;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Tabsyn&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces Tabsyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capture inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data, (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms exist
&lt;/p&gt;</description></item><item><title>&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#36890;&#36807;&#34701;&#21512;Gromov-Wasserstein&#65288;FGW&#65289;&#37325;&#24515;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#35299;&#20026;&#23545;&#40784;&#31751;&#65292;&#24182;&#23558;FGW&#36317;&#31163;&#24191;&#20041;&#21270;&#21040;&#22810;&#36793;&#27719;&#29615;&#22659;&#65292;&#23454;&#29616;&#22810;&#32593;&#32476;&#30340;&#32852;&#21512;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HOT&#30456;&#23545;&#20110;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2310.04470</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#29992;&#20110;&#32593;&#32476;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Multi-Marginal Optimal Transport for Network Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04470
&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#65288;HOT&#65289;&#26694;&#26550;&#36890;&#36807;&#34701;&#21512;Gromov-Wasserstein&#65288;FGW&#65289;&#37325;&#24515;&#23558;&#22810;&#20010;&#32593;&#32476;&#20998;&#35299;&#20026;&#23545;&#40784;&#31751;&#65292;&#24182;&#23558;FGW&#36317;&#31163;&#24191;&#20041;&#21270;&#21040;&#22810;&#36793;&#27719;&#29615;&#22659;&#65292;&#23454;&#29616;&#22810;&#32593;&#32476;&#30340;&#32852;&#21512;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HOT&#30456;&#23545;&#20110;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#32593;&#32476;&#25214;&#21040;&#33410;&#28857;&#23545;&#24212;&#20851;&#31995;&#65292;&#21363;&#22810;&#32593;&#32476;&#23545;&#40784;&#65292;&#26159;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#36827;&#34892;&#32852;&#21512;&#23398;&#20064;&#30340;&#22522;&#26412;&#20808;&#20915;&#26465;&#20214;&#12290;&#23613;&#31649;&#22312;&#20004;&#20010;&#32593;&#32476;&#19978;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#30001;&#20110;&#35299;&#31354;&#38388;&#25351;&#25968;&#22686;&#38271;&#21644;&#39640;&#38454;&#24046;&#24322;&#24230;&#37327;&#32570;&#20047;&#65292;&#22810;&#32593;&#32476;&#23545;&#40784;&#30340;&#25991;&#29486;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23618;&#27425;&#21270;&#22810;&#36793;&#27719;&#36816;&#36755;&#26694;&#26550;&#65288;HOT&#65289;&#29992;&#20110;&#22810;&#32593;&#32476;&#23545;&#40784;&#12290;&#20026;&#20102;&#22788;&#29702;&#24222;&#22823;&#30340;&#35299;&#31354;&#38388;&#65292;&#22810;&#20010;&#32593;&#32476;&#36890;&#36807;&#34701;&#21512;&#30340;Gromov-Wasserstein&#65288;FGW&#65289;&#37325;&#24515;&#34987;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23545;&#40784;&#31751;&#12290;&#20026;&#20102;&#25551;&#32472;&#22810;&#20010;&#32593;&#32476;&#20043;&#38388;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;FGW&#36317;&#31163;&#34987;&#25512;&#24191;&#21040;&#22810;&#36793;&#27719;&#29615;&#22659;&#65292;&#22522;&#20110;&#36825;&#20010;&#36317;&#31163;&#21487;&#20197;&#23454;&#29616;&#32593;&#32476;&#30340;&#32852;&#21512;&#23545;&#40784;&#12290;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#36817;&#31471;&#28857;&#26041;&#27861;&#65292;&#20445;&#35777;&#25910;&#25947;&#21040;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#22823;&#37327;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;HOT&#30456;&#23545;&#20110;&#32479;&#35745;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding node correspondence across networks, namely multi-network alignment, is an essential prerequisite for joint learning on multiple networks. Despite great success in aligning networks in pairs, the literature on multi-network alignment is sparse due to the exponentially growing solution space and lack of high-order discrepancy measures. To fill this gap, we propose a hierarchical multi-marginal optimal transport framework named HOT for multi-network alignment. To handle the large solution space, multiple networks are decomposed into smaller aligned clusters via the fused Gromov-Wasserstein (FGW) barycenter. To depict high-order relationships across multiple networks, the FGW distance is generalized to the multi-marginal setting, based on which networks can be aligned jointly. A fast proximal point method is further developed with guaranteed convergence to a local optimum. Extensive experiments and analysis show that our proposed HOT achieves significant improvements over the stat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32602;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#32602;&#20989;&#25968;&#19982;&#36229;&#32423;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#26469;&#25214;&#21040;&#39547;&#28857;&#12290;</title><link>https://arxiv.org/abs/2309.01753</link><description>&lt;p&gt;
&#20851;&#20110;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#21644;&#19968;&#38454;&#38543;&#26426;&#36924;&#36817;&#30340;&#32602;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Penalty Methods for Nonconvex Bilevel Optimization and First-Order Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32602;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#38750;&#20984;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24314;&#31435;&#20102;&#32602;&#20989;&#25968;&#19982;&#36229;&#32423;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#26469;&#25214;&#21040;&#39547;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#27714;&#35299;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65288;BO&#65289;&#30340;&#19968;&#38454;&#31639;&#27861;&#65292;&#22312;&#36825;&#37324;&#65292;&#30446;&#26631;&#20989;&#25968;&#23545;&#20110;&#20004;&#20010;&#23618;&#32423;&#26469;&#35828;&#37117;&#26159;&#20809;&#28369;&#30340;&#65292;&#20294;&#21487;&#33021;&#26159;&#38750;&#20984;&#30340;&#65292;&#24182;&#19988;&#21464;&#37327;&#21463;&#38480;&#20110;&#38381;&#21512;&#20984;&#38598;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#36890;&#36807;&#32602;&#39033;&#26041;&#27861;&#30740;&#31350;&#20102;BO&#30340;&#27010;&#35980;&#65292;&#20854;&#20013;&#19978;&#23618;&#21644;&#19979;&#23618;&#30446;&#26631;&#20197;&#21152;&#26435;&#21644;&#30340;&#32602;&#39033;&#21442;&#25968;$\sigma&gt;0$&#30340;&#24418;&#24335;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#22320;&#21051;&#30011;&#20540;&#21644;&#23548;&#25968;&#20004;&#32773;&#24517;&#39035;$O(\sigma)$&#25509;&#36817;&#30340;&#26465;&#20214;&#65292;&#24314;&#31435;&#20102;&#32602;&#20989;&#25968;&#19982;&#36229;&#32423;&#30446;&#26631;&#20989;&#25968;&#20043;&#38388;&#30340;&#24378;&#36830;&#25509;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#19968;&#20010;&#21103;&#20135;&#21697;&#26159;&#24403;&#19979;&#23618;&#38382;&#39064;&#22312;&#26368;&#23567;&#26465;&#20214;&#19979;&#20855;&#26377;&#22810;&#20010;&#35299;&#26102;&#65292;&#36229;&#32423;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#30340;&#26174;&#24335;&#20844;&#24335;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;&#32602;&#20989;&#25968;&#24418;&#24335;&#35270;&#20026;&#21407;&#22987;BO&#30340;$O(\sigma)$-&#36817;&#20284;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#38454;&#31639;&#27861;&#65292;&#25214;&#21040;&#19968;&#20010;$\epsilon$-&#39547;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study first-order algorithms for solving Bilevel Optimization (BO) where the objective functions are smooth but possibly nonconvex in both levels and the variables are restricted to closed convex sets. As a first step, we study the landscape of BO through the lens of penalty methods, in which the upper- and lower-level objectives are combined in a weighted sum with penalty parameter $\sigma &gt; 0$. In particular, we establish a strong connection between the penalty function and the hyper-objective by explicitly characterizing the conditions under which the values and derivatives of the two must be $O(\sigma)$-close. A by-product of our analysis is the explicit formula for the gradient of hyper-objective when the lower-level problem has multiple solutions under minimal conditions, which could be of independent interest. Next, viewing the penalty formulation as $O(\sigma)$-approximation of the original BO, we propose first-order algorithms that find an $\epsilon$-stationar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31574;&#30053;&#35775;&#38382;&#39057;&#29575;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#38750;&#30697;&#24418;&#22870;&#21169;&#24378;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2309.01107</link><description>&lt;p&gt;
&#36890;&#36807;&#39057;&#29575;&#27491;&#21017;&#21270;&#35299;&#20915;&#38750;&#30697;&#24418;&#22870;&#21169;&#24378;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Solving Non-Rectangular Reward-Robust MDPs via Frequency Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#31574;&#30053;&#35775;&#38382;&#39057;&#29575;&#27491;&#21017;&#21270;&#65292;&#35299;&#20915;&#20102;&#38750;&#30697;&#24418;&#22870;&#21169;&#24378;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#20013;&#65292;&#20551;&#35774;&#22870;&#21169;&#21644;&#36716;&#31227;&#21160;&#24577;&#20301;&#20110;&#32473;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20013;&#12290;&#36890;&#36807;&#38024;&#23545;&#35813;&#38598;&#21512;&#20013;&#26368;&#20855;&#25932;&#23545;&#24615;&#30340;&#27169;&#22411;&#19979;&#30340;&#26368;&#22823;&#22238;&#25253;&#65292;RMDPs&#35299;&#20915;&#20102;&#23545;&#38169;&#35823;&#29615;&#22659;&#30340;&#24615;&#33021;&#25935;&#24863;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20445;&#25345;&#35745;&#31639;&#30340;&#21487;&#34892;&#24615;&#65292;&#20256;&#32479;&#19978;&#23545;&#20110;&#27599;&#20010;&#29366;&#24577;&#29420;&#31435;&#22320;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#12290;&#36825;&#31181;&#25152;&#35859;&#30340;&#30697;&#24418;&#26465;&#20214;&#20165;&#20165;&#26159;&#22522;&#20110;&#35745;&#31639;&#19978;&#30340;&#32771;&#34385;&#12290;&#22240;&#27492;&#65292;&#23427;&#32570;&#20047;&#23454;&#38469;&#30340;&#21160;&#26426;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#20445;&#23432;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32806;&#21512;&#22870;&#21169;RMDPs&#65292;&#20854;&#20013;&#36716;&#31227;&#26680;&#26159;&#22266;&#23450;&#30340;&#65292;&#20294;&#26159;&#22870;&#21169;&#20989;&#25968;&#22312;&#19982;&#21517;&#20041;&#22870;&#21169;&#20989;&#25968;&#30456;&#36317; &#945; &#21322;&#24452;&#20869;&#12290;&#25105;&#20204;&#30452;&#25509;&#32852;&#31995;&#20102;&#36825;&#31181;&#38750;&#30697;&#24418;&#22870;&#21169;-RMDPs&#21644;&#24212;&#29992;&#31574;&#30053;&#35775;&#38382;&#39057;&#29575;&#27491;&#21017;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#23398;&#31574;&#30053;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
In robust Markov decision processes (RMDPs), it is assumed that the reward and the transition dynamics lie in a given uncertainty set. By targeting maximal return under the most adversarial model from that set, RMDPs address performance sensitivity to misspecified environments. Yet, to preserve computational tractability, the uncertainty set is traditionally independently structured for each state. This so-called rectangularity condition is solely motivated by computational concerns. As a result, it lacks a practical incentive and may lead to overly conservative behavior. In this work, we study coupled reward RMDPs where the transition kernel is fixed, but the reward function lies within an $\alpha$-radius from a nominal one. We draw a direct connection between this type of non-rectangular reward-RMDPs and applying policy visitation frequency regularization. We introduce a policy-gradient method and prove its convergence. Numerical experiments illustrate the learned policy's robustness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#20540;&#26041;&#27861;&#65292;&#20351;&#29992;&#36830;&#32493;&#27491;&#24577;&#27969;&#27169;&#22411;&#23545;&#26377;&#25928;&#24358;&#29702;&#35770;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24471;&#21040;&#21487;&#38752;&#30340;&#25968;&#20540;&#20272;&#35745;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2307.01107</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#27491;&#24577;&#27969;&#37319;&#26679;&#26684;&#28857;Nambu-Goto&#24358;
&lt;/p&gt;
&lt;p&gt;
Sampling the lattice Nambu-Goto string using Continuous Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.01107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#20540;&#26041;&#27861;&#65292;&#20351;&#29992;&#36830;&#32493;&#27491;&#24577;&#27969;&#27169;&#22411;&#23545;&#26377;&#25928;&#24358;&#29702;&#35770;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#24471;&#21040;&#21487;&#38752;&#30340;&#25968;&#20540;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#24358;&#29702;&#35770;&#65288;EST&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#38750;&#24494;&#25200;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;Yang-Mills&#29702;&#35770;&#20013;&#30340;&#32422;&#26463;&#29616;&#35937;&#65292;&#23558;&#32422;&#26463;&#36890;&#37327;&#31649;&#27169;&#22411;&#21270;&#20026;&#19968;&#32500;&#34180;&#24358;&#12290;EST&#35745;&#31639;&#36890;&#24120;&#20351;&#29992;zeta&#20989;&#25968;&#35268;&#33539;&#21270;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#30740;&#31350;&#36890;&#37327;&#31649;&#24418;&#29366;&#25110;&#36229;&#36234;Nambu-Goto EST&#30340;&#39640;&#38454;&#20462;&#27491;&#65289;&#65292;&#28041;&#21450;&#30340;&#21487;&#35266;&#27979;&#37327;&#22826;&#22797;&#26434;&#65292;&#26080;&#27861;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#26032;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#25968;&#20540;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20197;Nambu-Goto&#24358;&#20026;&#23454;&#39564;&#23545;&#35937;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#36830;&#32493;&#27491;&#24577;&#27969;&#30340;&#26032;&#22411;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;EST&#39044;&#27979;&#30340;&#21487;&#38752;&#25968;&#20540;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective String Theory (EST) represents a powerful non-perturbative approach to describe confinement in Yang-Mills theory that models the confining flux tube as a thin vibrating string. EST calculations are usually performed using the zeta-function regularization: however there are situations (for instance the study of the shape of the flux tube or of the higher order corrections beyond the Nambu-Goto EST) which involve observables that are too complex to be addressed in this way. In this paper we propose a numerical approach based on recent advances in machine learning methods to circumvent this problem. Using as a laboratory the Nambu-Goto string, we show that by using a new class of deep generative models called Continuous Normalizing Flows it is possible to obtain reliable numerical estimates of EST predictions.
&lt;/p&gt;</description></item><item><title>Hyp-OW&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;</title><link>https://arxiv.org/abs/2306.14291</link><description>&lt;p&gt;
Hyp-OW: &#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hyp-OW: Exploiting Hierarchical Structure Learning with Hyperbolic Distance Enhances Open World Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14291
&lt;/p&gt;
&lt;p&gt;
Hyp-OW&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#20960;&#20309;&#36317;&#31163;&#30340;&#23618;&#27425;&#32467;&#26500;&#23398;&#20064;&#22686;&#24378;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#65292;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#26816;&#27979;(OWOD)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#19988;&#29616;&#23454;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#30340;&#33539;&#22260;&#12290;&#23427;&#38656;&#35201;&#22312;&#26816;&#27979;&#24050;&#30693;&#21644;&#26410;&#30693;&#23545;&#35937;&#30340;&#21516;&#26102;&#65292;&#25972;&#21512;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#29992;&#20110;&#26410;&#26469;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#8220;&#26410;&#30693;&#24615;&#8221;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#65292;&#26641;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32972;&#26223;&#30340;&#19968;&#37096;&#20998;&#65292;&#20294;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#21487;&#33021;&#20855;&#26377;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#24212;&#35813;&#24050;&#32463;&#23884;&#20837;&#21040;&#24050;&#30693;&#31867;&#21035;&#20013;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#24050;&#30693;&#21644;&#26410;&#30693;&#39033;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#35821;&#20041;&#25110;&#28508;&#22312;&#30340;&#32467;&#26500;&#20851;&#31995;&#31561;&#24453;&#21457;&#29616;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hyp-OW&#65292;&#19968;&#31181;&#36890;&#36807;&#36229;&#31867;&#27491;&#21017;&#21270;&#22120;&#26469;&#23398;&#20064;&#21644;&#24314;&#27169;&#24050;&#30693;&#39033;&#30446;&#30340;&#23618;&#27425;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#31181;&#34920;&#31034;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24230;&#36317;&#31163;&#30340;&#37325;&#26032;&#26631;&#35760;&#27169;&#22359;&#26377;&#25928;&#22320;&#26816;&#27979;&#26410;&#30693;&#23545;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open World Object Detection (OWOD) is a challenging and realistic task that extends beyond the scope of standard Object Detection task. It involves detecting both known and unknown objects while integrating learned knowledge for future tasks. However, the level of "unknownness" varies significantly depending on the context. For example, a tree is typically considered part of the background in a self-driving scene, but it may be significant in a household context. We argue that this contextual information should already be embedded within the known classes. In other words, there should be a semantic or latent structure relationship between the known and unknown items to be discovered. Motivated by this observation, we propose Hyp-OW, a method that learns and models hierarchical representation of known items through a SuperClass Regularizer. Leveraging this representation allows us to effectively detect unknown objects using a similarity distance-based relabeling module. Extensive experi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#25277;&#35937;&#30340;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#24613;&#36890;&#20449;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#21644;&#20449;&#24687;&#20849;&#20139;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#33258;&#36866;&#24212;&#30340;&#29366;&#24577;&#31354;&#38388;&#25277;&#35937;&#21644;&#36890;&#20449;&#21327;&#35758;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#24182;&#25552;&#21319;&#23548;&#33322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2306.11336</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#25277;&#35937;&#30340;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Learning for Navigation via Structured State Abstraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32467;&#26500;&#21270;&#29366;&#24577;&#25277;&#35937;&#30340;&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#24613;&#36890;&#20449;&#23454;&#29616;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#21644;&#20449;&#24687;&#20849;&#20139;&#65292;&#21516;&#26102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#33258;&#36866;&#24212;&#30340;&#29366;&#24577;&#31354;&#38388;&#25277;&#35937;&#21644;&#36890;&#20449;&#21327;&#35758;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#24182;&#25552;&#21319;&#23548;&#33322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#29992;&#20110;&#23548;&#33322;&#65292;&#20351;&#24471;&#26234;&#33021;&#20307;&#33021;&#22815;&#21512;&#20316;&#23454;&#29616;&#23548;&#33322;&#30446;&#26631;&#12290;&#36890;&#36807;&#24212;&#24613;&#36890;&#20449;&#65292;&#26234;&#33021;&#20307;&#23398;&#20064;&#21327;&#35843;&#21644;&#20849;&#20139;&#20449;&#24687;&#30340;&#36890;&#20449;&#21327;&#35758;&#20197;&#23454;&#29616;&#23548;&#33322;&#20219;&#21153;&#12290;&#22312;&#24212;&#24613;&#36890;&#20449;&#20013;&#65292;&#27809;&#26377;&#39044;&#20808;&#25351;&#23450;&#20351;&#29992;&#35268;&#21017;&#30340;&#31526;&#21495;&#34987;&#20132;&#25442;&#65292;&#20854;&#20013;&#30340;&#21547;&#20041;&#21644;&#35821;&#27861;&#36890;&#36807;&#35757;&#32451;&#36880;&#28176;&#24418;&#25104;&#12290;&#22312;MARL&#29615;&#22659;&#20013;&#21516;&#26102;&#23398;&#20064;&#23548;&#33322;&#31574;&#30053;&#21644;&#36890;&#20449;&#21327;&#35758;&#38750;&#24120;&#22797;&#26434;&#65292;&#22240;&#20026;&#38656;&#35201;&#25506;&#32034;&#30340;&#29366;&#24577;&#31354;&#38388;&#38750;&#24120;&#24222;&#22823;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#22797;&#26434;&#24615;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#32852;&#21512;&#23398;&#20064;&#21442;&#19982;&#23548;&#33322;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#33258;&#36866;&#24212;&#29366;&#24577;&#31354;&#38388;&#25277;&#35937;&#21644;&#36890;&#20449;&#21327;&#35758;&#12290;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#25277;&#35937;&#22120;&#65292;&#26174;&#33879;&#20943;&#23569;&#38656;&#35201;&#25506;&#32034;&#30340;&#29366;&#24577;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#31574;&#30053;&#24615;&#33021;&#19981;&#21463;&#24433;&#21709;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#23548;&#33322;&#24615;&#33021;&#24182;&#20943;&#23569;&#29366;&#24577;&#31354;&#38388;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative multi-agent reinforcement learning (MARL) for navigation enables agents to cooperate to achieve their navigation goals. Using emergent communication, agents learn a communication protocol to coordinate and share information that is needed to achieve their navigation tasks. In emergent communication, symbols with no pre-specified usage rules are exchanged, in which the meaning and syntax emerge through training. Learning a navigation policy along with a communication protocol in a MARL environment is highly complex due to the huge state space to be explored. To cope with this complexity, this work proposes a novel neural network architecture, for jointly learning an adaptive state space abstraction and a communication protocol among agents participating in navigation tasks. The goal is to come up with an adaptive abstractor that significantly reduces the size of the state space to be explored, without degradation in the policy performance. Simulation results show that the pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2305.13068</link><description>&lt;p&gt;
&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#24037;&#20855;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Making Language Models Better Tool Learners with Execution Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;TRICE&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25191;&#34892;&#21453;&#39304;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#20316;&#20026;&#20851;&#38190;&#30340;&#30028;&#38754;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#29702;&#35299;&#21644;&#25913;&#21464;&#29615;&#22659;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;AI&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#24037;&#20855;&#25193;&#23637;&#20854;&#33021;&#21147;&#24182;&#19982;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#12290;&#29616;&#26377;&#30340;&#24037;&#20855;&#23398;&#20064;&#26041;&#27861;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#21644;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#65292;&#36890;&#24120;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#21152;&#36873;&#25321;&#22320;&#21033;&#29992;&#24037;&#20855;&#65292;&#22240;&#20026;&#22797;&#26434;&#20219;&#21153;&#24448;&#24448;&#36229;&#20986;&#20102;&#23427;&#20204;&#33258;&#36523;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#31616;&#21333;&#20219;&#21153;&#24341;&#20837;&#24037;&#20855;&#65288;&#27169;&#22411;&#26412;&#36523;&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#30340;&#20219;&#21153;&#65289;&#65292;&#21487;&#33021;&#20250;&#26080;&#24847;&#38388;&#20256;&#25773;&#38169;&#35823;&#32780;&#19981;&#26159;&#25552;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21542;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#65311;&#20026;&#28385;&#36275;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tool leaRning wIth exeCution fEedback (TRICE)&#65292;&#36825;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#20174;&#24037;&#20855;&#25191;&#34892;&#20013;&#24471;&#21040;&#30340;&#21453;&#39304;&#19981;&#26029;&#23398;&#20064;&#65292;&#20174;&#32780;&#23398;&#20250;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#33258;&#36866;&#24212;&#21306;&#22495;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21319;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2303.14877</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#32416;&#27491;&#35823;&#24046;&#30340;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Error-mitigated Quantum Approximate Optimization via Learning-based Adaptive Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14877
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#33258;&#36866;&#24212;&#21306;&#22495;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#25552;&#21319;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#24456;&#38590;&#27714;&#35299;&#12290;&#37327;&#23376;&#35745;&#31639;&#34987;&#35270;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#22312;&#35299;&#20915;&#26576;&#20123;&#38382;&#39064;&#26102;&#20855;&#26377;&#28508;&#22312;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#65288;QAOA&#65289;&#26159;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#31639;&#27861;&#20043;&#19968;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#31163;&#25955;&#20248;&#21270;&#38382;&#39064;&#36716;&#21270;&#20026;&#36830;&#32493;&#30005;&#36335;&#21442;&#25968;&#22495;&#19978;&#30340;&#32463;&#20856;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;QAOA&#30446;&#26631;&#20989;&#25968;&#22312;&#21442;&#25968;&#21464;&#37327;&#19978;&#30340;&#26223;&#35266;&#22240;&#20854;&#26222;&#36941;&#23384;&#22312;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#21644;&#36139;&#30240;&#24179;&#21488;&#32780;&#33261;&#21517;&#26157;&#33879;&#65292;&#20854;&#21487;&#34892;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32463;&#20856;&#20248;&#21270;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;QAOA&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21452;&#33258;&#36866;&#24212;&#21306;&#22495;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;DARBO&#65289;&#65292;&#19968;&#31181;&#36866;&#24212;&#24615;&#32463;&#20856;&#20248;&#21270;&#22120;&#29992;&#20110;QAOA&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36828;&#36828;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;grad&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combinatorial optimization problems are ubiquitous and computationally hard to solve in general. Quantum computing is envisioned as a powerful tool offering potential computational advantages for solving some of these problems. Quantum approximate optimization algorithm (QAOA), one of the most representative quantum-classical hybrid algorithms, is designed to solve certain combinatorial optimization problems by transforming a discrete optimization problem into a classical optimization problem over a continuous circuit parameter domain. QAOA objective landscape over the parameter variables is notorious for pervasive local minima and barren plateaus, and its viability in training significantly relies on the efficacy of the classical optimization algorithm. To enhance the performance of QAOA, we design double adaptive-region Bayesian optimization (DARBO), an adaptive classical optimizer for QAOA. Our experimental results demonstrate that the algorithm greatly outperforms conventional grad
&lt;/p&gt;</description></item><item><title>RETEXO&#26159;&#31532;&#19968;&#20010;&#28040;&#38500;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#36890;&#20449;&#29942;&#39048;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#25042;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#21892;&#32593;&#32476;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2302.13053</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#22270;&#19978;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Scalable Neural Network Training over Distributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.13053
&lt;/p&gt;
&lt;p&gt;
RETEXO&#26159;&#31532;&#19968;&#20010;&#28040;&#38500;&#20998;&#24067;&#24335;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#36890;&#20449;&#29942;&#39048;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#25042;&#28040;&#24687;&#20256;&#36882;&#26469;&#25913;&#21892;&#32593;&#32476;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28041;&#21450;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#34507;&#30333;&#36136;&#32467;&#26500;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#31561;&#12290;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#24448;&#24448;&#38656;&#35201;&#20998;&#24067;&#24335;&#23384;&#20648;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#19978;&#65292;&#21407;&#22240;&#19981;&#20165;&#26159;&#22240;&#20026;&#23481;&#37327;&#38480;&#21046;&#65292;&#36824;&#26377;&#25968;&#25454;&#25152;&#22312;&#22320;&#25110;&#38544;&#31169;&#27861;&#24459;&#30340;&#35201;&#27714;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#32593;&#32476;&#36890;&#20449;&#25104;&#26412;&#24456;&#39640;&#65292;&#25104;&#20026;&#35757;&#32451;GNN&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#30340;&#20248;&#21270;&#20027;&#35201;&#38024;&#23545;&#25968;&#25454;&#32423;&#21035;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;&#32531;&#23384;&#12289;&#32593;&#32476;&#24863;&#30693;&#21010;&#20998;&#21644;&#23376;&#37319;&#26679;&#31561;&#65292;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#25968;&#25454;&#20013;&#24515;&#31867;&#20284;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#22270;&#25968;&#25454;&#23545;&#21333;&#20010;&#23454;&#20307;&#21487;&#35775;&#38382;&#19988;&#25968;&#25454;&#20256;&#36755;&#25104;&#26412;&#34987;&#24573;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RETEXO&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#28040;&#38500;&#20998;&#24067;&#24335;GNN&#35757;&#32451;&#20013;&#20005;&#37325;&#36890;&#20449;&#29942;&#39048;&#30340;&#39318;&#20010;&#26694;&#26550;&#65292;&#21516;&#26102;&#23562;&#37325;&#20219;&#20309;&#32473;&#23450;&#30340;&#25968;&#25454;&#20998;&#21306;&#37197;&#32622;&#12290;&#20851;&#38190;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#25042;&#28040;&#24687;&#20256;&#36882;&#65292;&#37325;&#26032;&#25490;&#24207;&#20102;&#28040;&#24687;&#20256;&#36882;&#30340;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) fuel diverse machine learning tasks involving graph-structured data, ranging from predicting protein structures to serving personalized recommendations. Real-world graph data must often be stored distributed across many machines not just because of capacity constraints, but because of compliance with data residency or privacy laws. In such setups, network communication is costly and becomes the main bottleneck to train GNNs. Optimizations for distributed GNN training have targeted data-level improvements so far -- via caching, network-aware partitioning, and sub-sampling -- that work for data center-like setups where graph data is accessible to a single entity and data transfer costs are ignored.   We present RETEXO, the first framework which eliminates the severe communication bottleneck in distributed GNN training while respecting any given data partitioning configuration. The key is a new training procedure, lazy message passing, that reorders the sequen
&lt;/p&gt;</description></item><item><title>STERLING&#26159;&#19968;&#31181;&#22312;&#20108;&#20998;&#22270;&#19978;&#36827;&#34892;&#21327;&#21516;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#23616;&#37096;&#21644;&#20840;&#23616;&#21327;&#21516;&#24615;&#65292;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#65292;&#26080;&#38656;&#36127;&#33410;&#28857;&#23545;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#27491;&#33410;&#28857;&#23545;&#30340;&#30456;&#20284;&#24615;&#21644;&#20849;&#32858;&#31751;&#30340;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#36830;&#25509;&#24615;&#12290;</title><link>https://arxiv.org/abs/2302.05428</link><description>&lt;p&gt;
STERLING: &#22312;&#20108;&#20998;&#22270;&#19978;&#30340;&#21327;&#21516;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
STERLING: Synergistic Representation Learning on Bipartite Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05428
&lt;/p&gt;
&lt;p&gt;
STERLING&#26159;&#19968;&#31181;&#22312;&#20108;&#20998;&#22270;&#19978;&#36827;&#34892;&#21327;&#21516;&#34920;&#31034;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#30041;&#23616;&#37096;&#21644;&#20840;&#23616;&#21327;&#21516;&#24615;&#65292;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#65292;&#26080;&#38656;&#36127;&#33410;&#28857;&#23545;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#27491;&#33410;&#28857;&#23545;&#30340;&#30456;&#20284;&#24615;&#21644;&#20849;&#32858;&#31751;&#30340;&#20114;&#20449;&#24687;&#26469;&#25552;&#39640;&#36830;&#25509;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#22522;&#26412;&#25361;&#25112;&#26159;&#22914;&#20309;&#25552;&#21462;&#26377;&#20449;&#24687;&#37327;&#30340;&#33410;&#28857;&#23884;&#20837;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064; (SSL) &#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#33539;&#24335;&#12290;&#26368;&#36817;&#30340;&#20108;&#20998;&#22270; SSL &#26041;&#27861;&#22823;&#22810;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#65292;&#22312;&#21306;&#20998;&#27491;&#36127;&#33410;&#28857;&#23545;&#30340;&#22522;&#30784;&#19978;&#23398;&#20064;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#23398;&#20064;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#36127;&#33410;&#28857;&#23545;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35745;&#31639;&#36127;&#25285;&#21644;&#35821;&#20041;&#38169;&#35823;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411; (STERLING)&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#36127;&#33410;&#28857;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#12290;STERLING &#20445;&#30041;&#20102;&#20108;&#20998;&#22270;&#20013;&#30340;&#29420;&#29305;&#23616;&#37096;&#21644;&#20840;&#23616;&#21327;&#21516;&#24615;&#12290;&#23616;&#37096;&#21327;&#21516;&#24615;&#36890;&#36807;&#26368;&#22823;&#21270;&#36328;&#31867;&#22411;&#21644;&#21516;&#31867;&#22411;&#27491;&#33410;&#28857;&#23545;&#30340;&#30456;&#20284;&#24615;&#26469;&#25429;&#25417;&#65292;&#20840;&#23616;&#21327;&#21516;&#24615;&#21017;&#36890;&#36807;&#26368;&#22823;&#21270;&#20849;&#32858;&#31751;&#30340;&#20114;&#20449;&#24687;&#26469;&#25429;&#25417;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126; STERLING &#21487;&#20197;&#25552;&#39640;&#36830;&#25509;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental challenge of bipartite graph representation learning is how to extract informative node embeddings. Self-Supervised Learning (SSL) is a promising paradigm to address this challenge. Most recent bipartite graph SSL methods are based on contrastive learning which learns embeddings by discriminating positive and negative node pairs. Contrastive learning usually requires a large number of negative node pairs, which could lead to computational burden and semantic errors. In this paper, we introduce a novel synergistic representation learning model (STERLING) to learn node embeddings without negative node pairs. STERLING preserves the unique local and global synergies in bipartite graphs. The local synergies are captured by maximizing the similarity of the inter-type and intra-type positive node pairs, and the global synergies are captured by maximizing the mutual information of co-clusters. Theoretical analysis demonstrates that STERLING could improve the connectivity between 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#21477;&#23376;&#21253;&#36827;&#34892;&#39640;&#25928;&#28789;&#27963;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21644;&#32858;&#31867;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#20808;&#39564;&#33258;&#23450;&#20041;&#20027;&#39064;-&#25991;&#26723;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#25285;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2302.03106</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#21477;&#23376;&#21253;&#30340;&#39640;&#25928;&#28789;&#27963;&#30340;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Efficient and Flexible Topic Modeling using Pretrained Embeddings and Bag of Sentences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#21477;&#23376;&#21253;&#36827;&#34892;&#39640;&#25928;&#28789;&#27963;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21644;&#32858;&#31867;&#65292;&#25552;&#20379;&#20102;&#20351;&#29992;&#20808;&#39564;&#33258;&#23450;&#20041;&#20027;&#39064;-&#25991;&#26723;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#36127;&#25285;&#36739;&#23567;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#22312;&#20027;&#39064;&#24314;&#27169;&#26041;&#38754;&#65292;&#32479;&#35745;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;LDA&#65289;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#19981;&#23481;&#26131;&#34701;&#20837;&#19978;&#19979;&#25991;&#35789;&#21521;&#37327;&#12290;&#23427;&#20204;&#21487;&#33021;&#20250;&#29983;&#25104;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#23376;&#21253;&#65288;BoS&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#21477;&#23376;&#20316;&#20026;&#20998;&#26512;&#21333;&#20301;&#12290;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#36807;&#31243;&#27169;&#22411;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#22522;&#20110;&#26399;&#26395;&#26368;&#22823;&#21270;&#12289;&#30828;&#20998;&#37197;&#21644;&#19968;&#20010;&#36864;&#28779;&#36807;&#31243;&#30340;&#24555;&#36895;&#25512;&#26029;&#31639;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#20197;&#30456;&#23545;&#36739;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#33719;&#24471;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;&#19982;&#21033;&#29992;&#35789;&#23884;&#20837;&#30340;&#20043;&#21069;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#26356;&#21152;&#28789;&#27963;&#65292;&#22240;&#20026;&#23427;&#25552;&#20379;&#20102;&#20351;&#29992;&#20808;&#39564;&#33258;&#23450;&#20041;&#20027;&#39064;-&#25991;&#26723;&#20998;&#24067;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have led to a new state-of-the-art in many NLP tasks. However, for topic modeling, statistical generative models such as LDA are still prevalent, which do not easily allow incorporating contextual word vectors. They might yield topics that do not align well with human judgment. In this work, we propose a novel topic modeling and inference algorithm. We suggest a bag of sentences (BoS) approach using sentences as the unit of analysis. We leverage pre-trained sentence embeddings by combining generative process models and clustering. We derive a fast inference algorithm based on expectation maximization, hard assignments, and an annealing process. The evaluation shows that our method yields state-of-the art results with relatively little computational demands. Our method is also more flexible compared to prior works leveraging word embeddings, since it provides the possibility to customize topic-document distributions using priors. Code and data is at \url{http
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.02182</link><description>&lt;p&gt;
&#22312;&#38750;&#38745;&#24577;&#19978;&#19979;&#25991;&#39537;&#21160;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Non-Stationary Context-Driven Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22806;&#29983;&#19978;&#19979;&#25991;&#36807;&#31243;&#24433;&#21709;&#30528;&#29615;&#22659;&#21160;&#24577;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#12290;&#38543;&#30528;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26032;&#32463;&#39564;&#22686;&#21152;&#65292;&#20195;&#29702; tend to forget &#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20219;&#21153;&#26631;&#31614;&#65288;&#36825;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26159;&#19981;&#23384;&#22312;&#30340;&#65289;&#25110;&#32773;&#20351;&#29992;&#33073;&#26426;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Locally Constrained Policy Optimization (LCPO) &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#30340;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#38170;&#23450;&#65292;LCPO&#20351;&#29992;&#26469;&#33258;&#24403;&#21069;&#19978;&#19979;&#25991;&#20998;&#24067;&#20043;&#22806;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#23616;&#37096;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;Mujoco&#12289;&#32463;&#20856;&#25511;&#21046;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19978;&#19979;&#25991;&#36319;&#36394;&#65292;&#35780;&#20272;&#20102;LCPO&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#26681;&#26412;&#19978;&#19981;&#21516;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#26032;&#30340;&#34920;&#36798;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#22270;&#30340;&#21452;&#36830;&#36890;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#23545;&#20197;&#21069;&#30340;GNN&#26550;&#26500;&#36827;&#34892;&#24443;&#24213;&#23457;&#26597;&#21518;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26550;&#26500;&#37117;&#27809;&#26377;&#23545;&#36825;&#20123;&#24230;&#37327;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;&#21807;&#19968;&#30340;&#20363;&#22806;&#26159;ESAN&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2301.09505</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#30340;&#21452;&#36830;&#36890;&#24615;&#37325;&#26032;&#24605;&#32771;GNN&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Expressive Power of GNNs via Graph Biconnectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#26681;&#26412;&#19978;&#19981;&#21516;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31867;&#26032;&#30340;&#34920;&#36798;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#22270;&#30340;&#21452;&#36830;&#36890;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#23545;&#20197;&#21069;&#30340;GNN&#26550;&#26500;&#36827;&#34892;&#24443;&#24213;&#23457;&#26597;&#21518;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#26550;&#26500;&#37117;&#27809;&#26377;&#23545;&#36825;&#20123;&#24230;&#37327;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;&#21807;&#19968;&#30340;&#20363;&#22806;&#26159;ESAN&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#26159;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#19968;&#20010;&#26680;&#24515;&#20027;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#24456;&#22810;&#26041;&#27861;&#26469;&#25913;&#36827;GNNs&#22312;Weisfeiler-Lehman (WL)&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20294;&#26159;&#26222;&#36941;&#36824;&#23384;&#22312;&#23545;&#23427;&#20204;&#33021;&#22815;&#31995;&#32479;&#21644;&#21487;&#35777;&#26126;&#22320;&#33719;&#24471;&#30340;&#39069;&#22806;&#33021;&#21147;&#30340;&#32570;&#20047;&#28145;&#20837;&#20102;&#35299;&#12290;&#26412;&#25991;&#20174;&#26681;&#26412;&#19978;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;WL&#27979;&#35797;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31867;&#26032;&#30340;&#36890;&#36807;&#22270;&#30340;&#21452;&#36830;&#36890;&#24615;&#30340;&#34920;&#36798;&#24230;&#37327;&#65292;&#24182;&#24378;&#35843;&#23427;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21452;&#36830;&#36890;&#24615;&#21487;&#20197;&#20351;&#29992;&#31616;&#21333;&#30340;&#31639;&#27861;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#19988;&#20855;&#26377;&#32447;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#22240;&#27492;&#24456;&#33258;&#28982;&#22320;&#21487;&#20197;&#26399;&#26395;&#27969;&#34892;&#30340;GNNs&#20063;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#32463;&#36807;&#23545;&#20197;&#21069;&#30340;GNN&#26550;&#26500;&#30340;&#24443;&#24213;&#23457;&#26597;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#22823;&#22810;&#25968;&#26550;&#26500;&#23545;&#20110;&#20219;&#20309;&#36825;&#20123;&#24230;&#37327;&#37117;&#19981;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;&#21807;&#19968;&#30340;&#20363;&#22806;&#26159;ESAN&#26694;&#26550;&#65292;&#23545;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing expressive Graph Neural Networks (GNNs) is a central topic in learning graph-structured data. While numerous approaches have been proposed to improve GNNs in terms of the Weisfeiler-Lehman (WL) test, generally there is still a lack of deep understanding of what additional power they can systematically and provably gain. In this paper, we take a fundamentally different perspective to study the expressive power of GNNs beyond the WL test. Specifically, we introduce a novel class of expressivity metrics via graph biconnectivity and highlight their importance in both theory and practice. As biconnectivity can be easily calculated using simple algorithms that have linear computational costs, it is natural to expect that popular GNNs can learn it easily as well. However, after a thorough review of prior GNN architectures, we surprisingly find that most of them are not expressive for any of these metrics. The only exception is the ESAN framework, for which we give a theoretical just
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65306;&#65288;1&#65289;&#30456;&#23545;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;PbRL&#65292;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#65292;&#65288;2&#65289;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#23433;&#20840;&#39640;&#25928;&#22320;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#26597;&#35810;&#65292;&#65288;3&#65289;&#22522;&#20110;&#27425;&#20248;&#31034;&#33539;&#30340;&#22870;&#21169;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#27809;&#26377;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;</title><link>https://arxiv.org/abs/2301.04741</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#39640;&#25928;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Preference-Based Reinforcement Learning Using Learned Dynamics Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.04741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#65306;&#65288;1&#65289;&#30456;&#23545;&#20110;&#27169;&#22411;&#26080;&#20851;&#30340;PbRL&#65292;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#29615;&#22659;&#20132;&#20114;&#27425;&#25968;&#65292;&#65288;2&#65289;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#23433;&#20840;&#39640;&#25928;&#22320;&#21512;&#25104;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#26597;&#35810;&#65292;&#65288;3&#65289;&#22522;&#20110;&#27425;&#20248;&#31034;&#33539;&#30340;&#22870;&#21169;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#27809;&#26377;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20559;&#22909;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;PbRL&#65289;&#21487;&#20197;&#20351;&#26426;&#22120;&#20154;&#22312;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#26681;&#25454;&#20010;&#20307;&#30340;&#20559;&#22909;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#25110;&#35299;&#26512;&#27169;&#22411;&#65292;&#35201;&#20040;&#37319;&#29992;&#26080;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#38656;&#35201;&#22823;&#37327;&#12289;&#21487;&#33021;&#19981;&#23433;&#20840;&#30340;&#22312;&#32447;&#29615;&#22659;&#20132;&#20114;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25191;&#34892;PbRL&#26102;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#22909;&#22788;&#21644;&#25361;&#25112;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20197;&#19979;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#25191;&#34892;PbRL&#26102;&#65292;&#23398;&#20064;&#21040;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#25552;&#20379;&#20102;&#20197;&#19979;&#22909;&#22788;&#65306;&#65288;1&#65289;&#30456;&#23545;&#20110;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#27169;&#22411;PbRL&#65292;&#20559;&#22909;&#24341;&#23548;&#21644;&#31574;&#30053;&#20248;&#21270;&#38656;&#35201;&#26356;&#23569;&#30340;&#29615;&#22659;&#20132;&#20114;&#65292;&#65288;2&#65289;&#22810;&#26679;&#21270;&#30340;&#20559;&#22909;&#26597;&#35810;&#21487;&#20197;&#23433;&#20840;&#39640;&#25928;&#22320;&#20316;&#20026;&#26631;&#20934;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#30340;&#21103;&#20135;&#21697;&#21512;&#25104;&#65292;&#65288;3&#65289;&#22522;&#20110;&#27425;&#20248;&#31034;&#33539;&#30340;&#22870;&#21169;&#39044;&#35757;&#32451;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#29615;&#22659;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20379;&#23454;&#35777;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Preference-based reinforcement learning (PbRL) can enable robots to learn to perform tasks based on an individual's preferences without requiring a hand-crafted reward function. However, existing approaches either assume access to a high-fidelity simulator or analytic model or take a model-free approach that requires extensive, possibly unsafe online environment interactions. In this paper, we study the benefits and challenges of using a learned dynamics model when performing PbRL. In particular, we provide evidence that a learned dynamics model offers the following benefits when performing PbRL: (1) preference elicitation and policy optimization require significantly fewer environment interactions than model-free PbRL, (2) diverse preference queries can be synthesized safely and efficiently as a byproduct of standard model-based RL, and (3) reward pre-training based on suboptimal demonstrations can be performed without any environmental interaction. Our paper provides empirical eviden
&lt;/p&gt;</description></item><item><title>Flow&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#26080;&#29366;&#24577;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#36335;&#30001;&#26426;&#21046;&#21019;&#24314;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#27599;&#20010;&#23454;&#20363;&#30340;&#36335;&#30001;&#65292;&#20197;&#25552;&#39640;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.15281</link><description>&lt;p&gt;
Flow: &#36890;&#36807;&#21160;&#24577;&#36335;&#30001;&#23454;&#29616;&#27599;&#20010;&#23454;&#20363;&#30340;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flow: Per-Instance Personalized Federated Learning Through Dynamic Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.15281
&lt;/p&gt;
&lt;p&gt;
Flow&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#26080;&#29366;&#24577;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#36335;&#30001;&#26426;&#21046;&#21019;&#24314;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#27599;&#20010;&#23454;&#20363;&#30340;&#36335;&#30001;&#65292;&#20197;&#25552;&#39640;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26088;&#22312;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#23545;&#21327;&#20316;&#35757;&#32451;&#30340;&#20840;&#23616;&#27169;&#22411;&#36827;&#34892;&#20462;&#25913;&#12290;&#30446;&#21069;&#30340;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#22312;&#31895;&#31890;&#24230;&#19978;&#36827;&#34892;&#65292;&#21363;&#23458;&#25143;&#31471;&#30340;&#25152;&#26377;&#36755;&#20837;&#23454;&#20363;&#20351;&#29992;&#30456;&#21516;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#12290;&#36825;&#24573;&#35270;&#20102;&#26576;&#20123;&#23454;&#20363;&#30001;&#20110;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#32780;&#26356;&#20934;&#30830;&#22320;&#22788;&#29702;&#20840;&#23616;&#27169;&#22411;&#30340;&#20107;&#23454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#26080;&#29366;&#24577;&#20010;&#24615;&#21270;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;Flow&#12290;Flow&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#36335;&#30001;&#26426;&#21046;&#65292;&#21019;&#24314;&#21160;&#24577;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#65292;&#30830;&#23450;&#36755;&#20837;&#23454;&#20363;&#26159;&#26356;&#21916;&#27426;&#26412;&#22320;&#21442;&#25968;&#36824;&#26159;&#20840;&#23616;&#23545;&#24212;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;Flow&#38500;&#20102;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20010;&#24615;&#21270;&#20043;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#27599;&#20010;&#23454;&#20363;&#30340;&#36335;&#30001;&#65292;&#20197;&#25552;&#39640;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;Flow&#26159;&#26080;&#29366;&#24577;&#30340;&#65292;&#20351;&#24471;&#23458;&#25143;&#31471;&#22312;&#32852;&#21512;&#23398;&#20064;&#36718;&#20043;&#38388;&#19981;&#24517;&#20445;&#30041;&#20010;&#24615;&#21270;&#29366;&#24577;&#12290;&#36825;&#20351;&#24471;Flow&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#65292;&#24182;&#19988;&#23545;&#26032;&#21152;&#20837;&#30340;&#23458;&#25143;&#31471;&#21451;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalization in Federated Learning (FL) aims to modify a collaboratively trained global model according to each client. Current approaches to personalization in FL are at a coarse granularity, i.e. all the input instances of a client use the same personalized model. This ignores the fact that some instances are more accurately handled by the global model due to better generalizability. To address this challenge, this work proposes Flow, a fine-grained stateless personalized FL approach. Flow creates dynamic personalized models by learning a routing mechanism that determines whether an input instance prefers the local parameters or its global counterpart. Thus, Flow introduces per-instance routing in addition to leveraging per-client personalization to improve accuracies at each client. Further, Flow is stateless which makes it unnecessary for a client to retain its personalized state across FL rounds. This makes Flow practical for large-scale FL settings and friendly to newly joined
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#39033;&#24335;&#20998;&#26512;&#65292;&#23545;&#21160;&#37327;&#22806;&#31227;&#26799;&#24230;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#21152;&#36895;&#25910;&#25947;&#36827;&#34892;&#30740;&#31350;&#65292;&#21253;&#25324;&#29305;&#24449;&#20540;&#23384;&#22312;&#20110;&#23454;&#36724;&#12289;&#20301;&#20110;&#23454;&#36724;&#19978;&#30340;&#20849;&#36717;&#22797;&#25968;&#25110;&#20165;&#23384;&#22312;&#20849;&#36717;&#22797;&#25968;&#30340;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23454;&#29616;&#26368;&#24555;&#25910;&#25947;&#30340;&#36229;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2211.04659</link><description>&lt;p&gt;
&#21160;&#37327;&#22806;&#31227;&#26799;&#24230;&#20309;&#26102;&#33021;&#36798;&#21040;&#26368;&#20339;&#65311;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
When is Momentum Extragradient Optimal? A Polynomial-Based Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.04659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22810;&#39033;&#24335;&#20998;&#26512;&#65292;&#23545;&#21160;&#37327;&#22806;&#31227;&#26799;&#24230;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#30340;&#21152;&#36895;&#25910;&#25947;&#36827;&#34892;&#30740;&#31350;&#65292;&#21253;&#25324;&#29305;&#24449;&#20540;&#23384;&#22312;&#20110;&#23454;&#36724;&#12289;&#20301;&#20110;&#23454;&#36724;&#19978;&#30340;&#20849;&#36717;&#22797;&#25968;&#25110;&#20165;&#23384;&#22312;&#20849;&#36717;&#22797;&#25968;&#30340;&#24773;&#20917;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#23454;&#29616;&#26368;&#24555;&#25910;&#25947;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#31227;&#26799;&#24230;&#26041;&#27861;&#30001;&#20110;&#20854;&#22312;&#21487;&#24494;&#20998;&#21338;&#24328;&#20013;&#30340;&#31283;&#20581;&#25910;&#25947;&#24615;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#19982;&#21333;&#30446;&#26631;&#20248;&#21270;&#19981;&#21516;&#65292;&#21338;&#24328;&#21160;&#21147;&#23398;&#28041;&#21450;&#21040;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#21338;&#24328;&#21521;&#37327;&#22330;&#30340;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#25955;&#24067;&#22312;&#22797;&#24179;&#38754;&#19978;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20250;&#23548;&#33268;&#31616;&#21333;&#30340;&#26799;&#24230;&#26041;&#27861;&#21457;&#25955;&#65292;&#21363;&#20351;&#23545;&#20110;&#21452;&#32447;&#24615;&#21338;&#24328;&#20063;&#26159;&#22914;&#27492;&#65292;&#32780;&#22806;&#31227;&#26799;&#24230;&#26041;&#27861;&#21364;&#33021;&#23454;&#29616;&#25910;&#25947;&#12290;&#22312;&#26368;&#36817;&#35777;&#26126;&#30340;&#22522;&#30784;&#19978;&#65292;&#21363;&#21160;&#37327;&#22806;&#31227;&#26799;&#24230;&#26041;&#27861;&#22312;&#21452;&#32447;&#24615;&#21338;&#24328;&#20013;&#23454;&#29616;&#21152;&#36895;&#25910;&#25947;\citep{azizian2020accelerating}&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22810;&#39033;&#24335;&#30340;&#20998;&#26512;&#26469;&#30830;&#23450;&#35813;&#26041;&#27861;&#20986;&#29616;&#36827;&#19968;&#27493;&#21152;&#36895;&#25910;&#25947;&#30340;&#19977;&#31181;&#19981;&#21516;&#24773;&#26223;&#12290;&#36825;&#20123;&#24773;&#26223;&#21253;&#25324;&#29305;&#24449;&#20540;&#23384;&#22312;&#20110;&#65288;&#27491;&#65289;&#23454;&#36724;&#19978;&#12289;&#20301;&#20110;&#23454;&#36724;&#19978;&#30340;&#20849;&#36717;&#22797;&#25968;&#20197;&#21450;&#20165;&#23384;&#22312;&#20849;&#36717;&#22797;&#25968;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25512;&#23548;&#20986;&#27599;&#20010;&#24773;&#26223;&#30340;&#36229;&#21442;&#25968;&#65292;&#20197;&#23454;&#29616;&#26368;&#24555;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The extragradient method has gained popularity due to its robust convergence properties for differentiable games. Unlike single-objective optimization, game dynamics involve complex interactions reflected by the eigenvalues of the game vector field's Jacobian scattered across the complex plane. This complexity can cause the simple gradient method to diverge, even for bilinear games, while the extragradient method achieves convergence. Building on the recently proven accelerated convergence of the momentum extragradient method for bilinear games \citep{azizian2020accelerating}, we use a polynomial-based analysis to identify three distinct scenarios where this method exhibits further accelerated convergence. These scenarios encompass situations where the eigenvalues reside on the (positive) real line, lie on the real line alongside complex conjugates, or exist solely as complex conjugates. Furthermore, we derive the hyperparameters for each scenario that achieve the fastest convergence r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#24378;&#30340;&#28508;&#21464;&#37327;&#65292;&#25552;&#21319;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21463;&#21407;&#23376;&#29289;&#29702;&#23398;&#21551;&#21457;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#32467;&#26500;&#26469;&#35299;&#37322;&#21508;&#20010;&#23376;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20998;&#31867;&#21644;&#29983;&#25104;&#38382;&#39064;&#20013;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2210.03728</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Dynamic Latent Separation for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22797;&#26434;&#25968;&#25454;&#20013;&#23398;&#20064;&#34920;&#36798;&#24615;&#24378;&#30340;&#28508;&#21464;&#37327;&#65292;&#25552;&#21319;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#12290;&#35813;&#26041;&#27861;&#21463;&#21407;&#23376;&#29289;&#29702;&#23398;&#21551;&#21457;&#65292;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#32467;&#26500;&#26469;&#35299;&#37322;&#21508;&#20010;&#23376;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20998;&#31867;&#21644;&#29983;&#25104;&#38382;&#39064;&#20013;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#20197;&#28789;&#27963;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23398;&#20064;&#29992;&#20110;&#22797;&#26434;&#25968;&#25454;&#27169;&#22411;&#39044;&#27979;&#30340;&#34920;&#36798;&#24615;&#28508;&#21464;&#37327;&#65292;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#22810;&#20010;&#23376;&#32452;&#20214;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#34920;&#36798;&#24615;&#65292;&#25552;&#20379;&#20102;&#37096;&#20998;&#35299;&#37322;&#65292;&#24182;&#19988;&#19981;&#38480;&#20110;&#29305;&#23450;&#30340;&#24212;&#29992;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#28508;&#31354;&#38388;&#20013;&#21160;&#24577;&#22320;&#20998;&#31163;&#25968;&#25454;&#26679;&#26412;&#65292;&#20174;&#32780;&#22686;&#24378;&#36755;&#20986;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#21160;&#24577;&#28508;&#21464;&#37327;&#20998;&#31163;&#26041;&#27861;&#21463;&#21040;&#21407;&#23376;&#29289;&#29702;&#23398;&#30340;&#21551;&#21457;&#65292;&#20381;&#36182;&#20110;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#20849;&#21516;&#23398;&#20064;&#30340;&#32467;&#26500;&#65292;&#36825;&#20063;&#25581;&#31034;&#20986;&#20102;&#27599;&#20010;&#23376;&#32452;&#20214;&#22312;&#21306;&#20998;&#25968;&#25454;&#26679;&#26412;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#65292;&#21407;&#23376;&#24314;&#27169;&#65292;&#19981;&#38656;&#35201;&#23545;&#28508;&#31354;&#38388;&#36827;&#34892;&#30417;&#30563;&#65292;&#24182;&#19988;&#20801;&#35768;&#25105;&#20204;&#23398;&#20064;&#39069;&#22806;&#30340;&#37096;&#20998;&#21487;&#35299;&#37322;&#34920;&#31034;&#65292;&#38500;&#20102;&#27169;&#22411;&#30340;&#21407;&#22987;&#30446;&#26631;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#31639;&#27861;&#36824;&#25552;&#39640;&#20102;&#21508;&#31181;&#20998;&#31867;&#21644;&#29983;&#25104;&#38382;&#39064;&#20013;&#23567;&#21040;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core problem in machine learning is to learn expressive latent variables for model prediction on complex data that involves multiple sub-components in a flexible and interpretable fashion. Here, we develop an approach that improves expressiveness, provides partial interpretation, and is not restricted to specific applications. The key idea is to dynamically distance data samples in the latent space and thus enhance the output diversity. Our dynamic latent separation method, inspired by atomic physics, relies on the jointly learned structures of each data sample, which also reveal the importance of each sub-component for distinguishing data samples. This approach, atom modeling, requires no supervision of the latent space and allows us to learn extra partially interpretable representations besides the original goal of a model. We empirically demonstrate that the algorithm also enhances the performance of small to larger-scale models in various classification and generation problems.
&lt;/p&gt;</description></item><item><title>GBSVM&#26159;&#19968;&#31181;&#20351;&#29992;&#31895;&#31890;-&#29699;&#20316;&#20026;&#36755;&#20837;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20462;&#22797;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#38169;&#35823;&#24182;&#25512;&#23548;&#20986;&#20102;&#23545;&#20598;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#23567;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2210.03120</link><description>&lt;p&gt;
GBSVM: &#29992;&#31895;&#31890;-&#29699;&#20316;&#20026;&#36755;&#20837;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;
&lt;/p&gt;
&lt;p&gt;
GBSVM: Granular-ball Support Vector Machine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03120
&lt;/p&gt;
&lt;p&gt;
GBSVM&#26159;&#19968;&#31181;&#20351;&#29992;&#31895;&#31890;-&#29699;&#20316;&#20026;&#36755;&#20837;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20462;&#22797;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#38169;&#35823;&#24182;&#25512;&#23548;&#20986;&#20102;&#23545;&#20598;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#21644;&#39034;&#24207;&#26368;&#23567;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GBSVM&#65288;Granular-ball Support Vector Machine&#65289;&#26159;&#36890;&#36807;&#20351;&#29992;&#31895;&#31890;-&#29699;&#30340;&#31890;&#24230;&#20316;&#20026;&#36755;&#20837;&#26469;&#26500;&#24314;&#20998;&#31867;&#22120;&#30340;&#37325;&#35201;&#23581;&#35797;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#28857;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#36755;&#20837;&#19981;&#21253;&#21547;&#28857;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#38169;&#35823;&#65292;&#24182;&#19988;&#20854;&#23545;&#20598;&#27169;&#22411;&#23578;&#26410;&#34987;&#25512;&#23548;&#20986;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#31639;&#27861;&#26080;&#27861;&#23454;&#29616;&#25110;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20462;&#22797;&#20102;&#29616;&#26377;GBSVM&#21407;&#22987;&#27169;&#22411;&#30340;&#38169;&#35823;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#20854;&#23545;&#20598;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#31890;&#23376;&#32676;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#23545;&#20598;&#27169;&#22411;&#38382;&#39064;&#12290;&#36824;&#35774;&#35745;&#20102;&#39034;&#24207;&#26368;&#23567;&#20248;&#21270;&#31639;&#27861;&#26469;&#35299;&#20915;&#23545;&#20598;&#27169;&#22411;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#27604;&#22522;&#20110;&#31890;&#23376;&#32676;&#20248;&#21270;&#30340;&#29256;&#26412;&#26356;&#24555;&#12289;&#26356;&#31283;&#23450;&#12290;&#22312;UCI&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GBSVM&#20855;&#26377;&#33391;&#22909;&#30340;&#31283;&#20581;&#24615;&#21644;&#25928;&#29575;&#12290;&#25152;&#26377;&#20195;&#30721;&#24050;&#22312;&#24320;&#28304;&#24211;http://&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
GBSVM (Granular-ball Support Vector Machine) is a significant attempt to construct a classifier using the coarse-to-fine granularity of a granular-ball as input, rather than a single data point. It is the first classifier whose input contains no points. However, the existing model has some errors, and its dual model has not been derived. As a result, the current algorithm cannot be implemented or applied. To address these problems, this paper has fixed the errors of the original model of the existing GBSVM, and derived its dual model. Furthermore, a particle swarm optimization algorithm is designed to solve the dual model. The sequential minimal optimization algorithm is also carefully designed to solve the dual model. The solution is faster and more stable than the particle swarm optimization based version. The experimental results on the UCI benchmark datasets demonstrate that GBSVM has good robustness and efficiency. All codes have been released in the open source library at http://
&lt;/p&gt;</description></item><item><title>NeuralVDB&#20351;&#29992;&#20998;&#23618;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#20102;VDB&#30340;&#23384;&#20648;&#25928;&#29575;&#65292;&#23558;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#21516;&#26102;&#20445;&#25345;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#27604;&#21644;&#31354;&#38388;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2208.04448</link><description>&lt;p&gt;
NeuralVDB: &#20351;&#29992;&#20998;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#39640;&#20998;&#36776;&#29575;&#31232;&#30095;&#20307;&#31215;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuralVDB: High-resolution Sparse Volume Representation using Hierarchical Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04448
&lt;/p&gt;
&lt;p&gt;
NeuralVDB&#20351;&#29992;&#20998;&#23618;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#20102;VDB&#30340;&#23384;&#20648;&#25928;&#29575;&#65292;&#23558;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#21516;&#26102;&#20445;&#25345;&#20102;&#28789;&#27963;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#21387;&#32553;&#27604;&#21644;&#31354;&#38388;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;NeuralVDB&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#29992;&#20110;&#39640;&#25928;&#23384;&#20648;&#31232;&#30095;&#20307;&#31215;&#25968;&#25454;&#30340;&#34892;&#19994;&#26631;&#20934;VDB[Museth 2013]&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#28151;&#21512;&#25968;&#25454;&#32467;&#26500;&#21487;&#20197;&#23558;VDB&#20307;&#31215;&#30340;&#20869;&#23384;&#21344;&#29992;&#20943;&#23569;&#25968;&#20010;&#25968;&#37327;&#32423;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#28789;&#27963;&#24615;&#65292;&#24182;&#21482;&#20135;&#29983;&#23567;&#30340;&#65288;&#29992;&#25143;&#21487;&#25511;&#21046;&#30340;&#65289;&#21387;&#32553;&#35823;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NeuralVDB&#20351;&#29992;&#22810;&#20010;&#20998;&#23618;&#31070;&#32463;&#32593;&#32476;&#26367;&#25442;&#20102;&#27973;&#32780;&#23485;&#30340;VDB&#26641;&#32467;&#26500;&#30340;&#36739;&#20302;&#33410;&#28857;&#65292;&#36890;&#36807;&#31070;&#32463;&#20998;&#31867;&#22120;&#21644;&#22238;&#24402;&#22120;&#20998;&#21035;&#32534;&#30721;&#25299;&#25169;&#21644;&#25968;&#20540;&#20449;&#24687;&#12290;&#27492;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#20445;&#25345;&#39640;&#32423;VDB&#25968;&#25454;&#32467;&#26500;&#25152;&#25552;&#20379;&#30340;&#31354;&#38388;&#36866;&#24212;&#24615;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#20102;&#21387;&#32553;&#27604;&#12290;&#23545;&#20110;&#31232;&#30095;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#21644;&#23494;&#24230;&#20307;&#31215;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;&#24050;&#32463;&#21387;&#32553;&#30340;VDB&#36755;&#20837;&#24471;&#21040;&#30340;&#21387;&#32553;&#27604;&#22312;10&#20493;&#21040;100&#20493;&#20197;&#19978;&#65292;&#24182;&#19988;&#20960;&#20046;&#27809;&#26377;&#21487;&#35265;&#30340;&#20266;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce NeuralVDB, which improves on an existing industry standard for efficient storage of sparse volumetric data, denoted VDB [Museth 2013], by leveraging recent advancements in machine learning. Our novel hybrid data structure can reduce the memory footprints of VDB volumes by orders of magnitude, while maintaining its flexibility and only incurring small (user-controlled) compression errors. Specifically, NeuralVDB replaces the lower nodes of a shallow and wide VDB tree structure with multiple hierarchical neural networks that separately encode topology and value information by means of neural classifiers and regressors respectively. This approach is proven to maximize the compression ratio while maintaining the spatial adaptivity offered by the higher-level VDB data structure. For sparse signed distance fields and density volumes, we have observed compression ratios on the order of 10x to more than 100x from already compressed VDB inputs, with little to no visual artifacts. F
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#21152;&#26435;&#20108;&#37096;&#22270;&#21305;&#37197;&#30340;&#27425;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#25968;&#25454;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#23545;&#26435;&#37325;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#20197;&#35299;&#20915;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#25110;&#25628;&#32034;&#24341;&#25806;&#20013;&#35745;&#31639;&#22823;&#35268;&#27169;&#26435;&#37325;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2208.03367</link><description>&lt;p&gt;
&#22312;&#32447;&#21152;&#26435;&#20108;&#37096;&#22270;&#21305;&#37197;&#30340;&#27425;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sublinear Time Algorithm for Online Weighted Bipartite Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.03367
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#21152;&#26435;&#20108;&#37096;&#22270;&#21305;&#37197;&#30340;&#27425;&#32447;&#24615;&#26102;&#38388;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#25968;&#25454;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#23545;&#26435;&#37325;&#30340;&#36817;&#20284;&#35745;&#31639;&#65292;&#20197;&#35299;&#20915;&#22312;&#23454;&#38469;&#25512;&#33616;&#31995;&#32479;&#25110;&#25628;&#32034;&#24341;&#25806;&#20013;&#35745;&#31639;&#22823;&#35268;&#27169;&#26435;&#37325;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20108;&#37096;&#22270;&#21305;&#37197;&#26159;&#22312;&#32447;&#31639;&#27861;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23558;&#20004;&#20010;&#39030;&#28857;&#38598;&#21512;&#36827;&#34892;&#21305;&#37197;&#65292;&#20351;&#24471;&#36793;&#26435;&#37325;&#30340;&#24635;&#21644;&#26368;&#22823;&#21270;&#65292;&#20854;&#20013;&#23545;&#20110;&#20854;&#20013;&#19968;&#20010;&#39030;&#28857;&#38598;&#21512;&#65292;&#27599;&#20010;&#39030;&#28857;&#21450;&#20854;&#23545;&#24212;&#30340;&#36793;&#26435;&#37325;&#25353;&#39034;&#24207;&#20986;&#29616;&#22312;&#19968;&#20010;&#24207;&#21015;&#20013;&#12290;&#30446;&#21069;&#65292;&#22312;&#23454;&#38469;&#30340;&#25512;&#33616;&#31995;&#32479;&#25110;&#25628;&#32034;&#24341;&#25806;&#20013;&#65292;&#26435;&#37325;&#26159;&#30001;&#29992;&#25143;&#30340;&#28145;&#24230;&#34920;&#31034;&#21644;&#29289;&#21697;&#30340;&#28145;&#24230;&#34920;&#31034;&#20043;&#38388;&#30340;&#20869;&#31215;&#20915;&#23450;&#30340;&#12290;&#26631;&#20934;&#30340;&#22312;&#32447;&#21305;&#37197;&#38656;&#35201;&#20184;&#20986;$nd$&#30340;&#26102;&#38388;&#26469;&#32447;&#24615;&#25195;&#25551;&#25152;&#26377;&#30340;$n$&#20010;&#29289;&#21697;&#65292;&#35745;&#31639;&#26435;&#37325;&#65288;&#20551;&#35774;&#27599;&#20010;&#34920;&#31034;&#21521;&#37327;&#38271;&#24230;&#20026;$d$&#65289;&#65292;&#28982;&#21518;&#26681;&#25454;&#26435;&#37325;&#20915;&#23450;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;$n$&#21487;&#33021;&#38750;&#24120;&#22823;&#65292;&#27604;&#22914;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#12290;&#22240;&#27492;&#65292;&#25913;&#36827;&#35745;&#31639;&#26435;&#37325;&#30340;&#26102;&#38388;&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35745;&#31639;&#36817;&#20284;&#26435;&#37325;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#38543;&#26426;&#21270;&#25968;&#25454;&#32467;&#26500;&#65292;&#26435;&#37325;&#30340;&#35745;&#31639;&#26102;&#38388;&#21487;&#20197;&#36798;&#21040;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online bipartite matching is a fundamental problem in online algorithms. The goal is to match two sets of vertices to maximize the sum of the edge weights, where for one set of vertices, each vertex and its corresponding edge weights appear in a sequence. Currently, in the practical recommendation system or search engine, the weights are decided by the inner product between the deep representation of a user and the deep representation of an item. The standard online matching needs to pay $nd$ time to linear scan all the $n$ items, computing weight (assuming each representation vector has length $d$), and then deciding the matching based on the weights. However, in reality, the $n$ could be very large, e.g. in online e-commerce platforms. Thus, improving the time of computing weights is a problem of practical significance. In this work, we provide the theoretical foundation for computing the weights approximately. We show that, with our proposed randomized data structures, the weights c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#26377;&#30028;&#30340;&#20010;&#24615;&#21270;PageRank&#31639;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#22270;&#23398;&#20064;&#30340;&#20960;&#31181;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2207.06944</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#22270;&#23398;&#20064;&#30340;&#25935;&#24863;&#24615;&#26377;&#30028;&#20010;&#24615;&#21270;PageRank&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Graph Learning via Sensitivity-Bounded Personalized PageRank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2207.06944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25935;&#24863;&#24615;&#26377;&#30028;&#30340;&#20010;&#24615;&#21270;PageRank&#31639;&#27861;&#65292;&#33021;&#22815;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#35813;&#31639;&#27861;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#22270;&#23398;&#20064;&#30340;&#20960;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;PageRank(PPR)&#26159;&#19968;&#31181;&#22522;&#26412;&#24037;&#20855;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#22270;&#34920;&#31034;&#65292;&#22914;&#33410;&#28857;&#25490;&#24207;&#12289;&#26631;&#27880;&#21644;&#22270;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#38544;&#31169;&#25104;&#20026;&#26368;&#36817;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#20043;&#19968;&#65292;&#29616;&#26377;&#30340;PPR&#31639;&#27861;&#24182;&#26410;&#35774;&#35745;&#29992;&#20110;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;PPR&#23545;&#36755;&#20837;&#22270;&#30340;&#36793;&#38750;&#24120;&#25935;&#24863;&#65306;&#20165;&#24046;&#19968;&#20010;&#36793;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#23548;&#33268;PPR&#21521;&#37327;&#21457;&#29983;&#24040;&#22823;&#25913;&#21464;&#65292;&#20174;&#32780;&#21487;&#33021;&#27844;&#28431;&#29992;&#25143;&#31169;&#23494;&#25968;&#25454;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36755;&#20986;&#36817;&#20284;PPR&#65292;&#24182;&#23545;&#36755;&#20837;&#36793;&#20855;&#26377;&#21487;&#35777;&#26126;&#30340;&#25935;&#24863;&#24615;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36755;&#20837;&#22270;&#20855;&#26377;&#22823;&#24230;&#25968;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36798;&#21040;&#19982;&#38750;&#31169;&#23494;&#31639;&#27861;&#30456;&#20284;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25935;&#24863;&#24615;&#26377;&#30028;PPR&#30452;&#25509;&#24847;&#21619;&#30528;&#22270;&#23398;&#20064;&#30340;&#20960;&#31181;&#31169;&#23494;&#31639;&#27861;&#65292;&#22914;&#24046;&#20998;&#38544;&#31169;(DP)PPR&#25490;&#24207;&#12289;DP&#33410;&#28857;&#20998;&#31867;&#21644;DP&#33410;&#28857;&#23884;&#20837;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized PageRank (PPR) is a fundamental tool in unsupervised learning of graph representations such as node ranking, labeling, and graph embedding. However, while data privacy is one of the most important recent concerns, existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges: the difference of only one edge may cause a big change in the PPR vector, potentially leaking private user data.   In this work, we propose an algorithm which outputs an approximate PPR and has provably bounded sensitivity to input edges. In addition, we prove that our algorithm achieves similar accuracy to non-private algorithms when the input graph has large degrees. Our sensitivity-bounded PPR directly implies private algorithms for several tools of graph learning, such as, differentially private (DP) PPR ranking, DP node classification, and DP node embedding. To complement our theoretical analysis, we also empirically verify the practical perfor
&lt;/p&gt;</description></item><item><title>&#28151;&#21512;&#20351;&#29992;&#29275;&#39039;&#27861;&#21644;&#24815;&#24615;&#26799;&#24230;&#19979;&#38477;&#30340;&#20108;&#38454;&#31639;&#27861;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#36991;&#20813;&#20005;&#26684;&#38797;&#28857;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#20363;&#25903;&#25345;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2111.04596</link><description>&lt;p&gt;
&#36991;&#20813;&#20005;&#26684;&#38797;&#28857;&#30340;&#24815;&#24615;&#29275;&#39039;&#31639;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inertial Newton Algorithms Avoiding Strict Saddle Points
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2111.04596
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20351;&#29992;&#29275;&#39039;&#27861;&#21644;&#24815;&#24615;&#26799;&#24230;&#19979;&#38477;&#30340;&#20108;&#38454;&#31639;&#27861;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#36991;&#20813;&#20005;&#26684;&#38797;&#28857;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#20363;&#25903;&#25345;&#20102;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#20984;&#22320;&#24418;&#20013;&#28151;&#21512;&#20351;&#29992;&#29275;&#39039;&#27861;&#21644;&#24815;&#24615;&#26799;&#24230;&#19979;&#38477;&#30340;&#20108;&#38454;&#31639;&#27861;&#30340;&#28176;&#36817;&#34892;&#20026;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20855;&#26377;&#29275;&#39039;&#29305;&#24615;&#65292;&#23427;&#20204;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#36991;&#20813;&#20005;&#26684;&#38797;&#28857;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#36229;&#21442;&#25968;&#22312;&#20020;&#30028;&#28857;&#38468;&#36817;&#30340;&#23450;&#24615;&#34892;&#20026;&#20013;&#36215;&#21040;&#30340;&#20316;&#29992;&#12290;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#25968;&#20540;&#23454;&#20363;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic behavior of second-order algorithms mixing Newton's method and inertial gradient descent in non-convex landscapes. We show that, despite the Newtonian behavior of these methods, they almost always escape strict saddle points. We also evidence the role played by the hyper-parameters of these methods in their qualitative behavior near critical points. The theoretical results are supported by numerical illustrations.
&lt;/p&gt;</description></item><item><title>&#32452;&#21512;Q&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#29615;&#22659;&#20013;&#23384;&#22312;&#24322;&#36136;&#24615;&#27835;&#30103;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22797;&#21512;&#20219;&#21153;&#32467;&#26500;&#21644;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;Q&#20540;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2110.02879</link><description>&lt;p&gt;
&#19981;&#24179;&#34913;&#24739;&#32773;&#20122;&#32676;&#30340;&#30005;&#35299;&#36136;&#34917;&#20805;&#30340;&#32452;&#21512;Q&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Compositional Q-learning for electrolyte repletion with imbalanced patient sub-populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.02879
&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;Q&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#21307;&#30103;&#29615;&#22659;&#20013;&#23384;&#22312;&#24322;&#36136;&#24615;&#27835;&#30103;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#22797;&#21512;&#20219;&#21153;&#32467;&#26500;&#21644;&#20998;&#31163;&#30340;&#27169;&#22359;&#21270;Q&#20540;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#26377;&#25928;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#24212;&#29992;RL&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#24739;&#32773;&#30340;&#27835;&#30103;&#21453;&#24212;&#30340;&#24322;&#36136;&#24615;&#12290;&#19968;&#20123;&#24739;&#32773;&#21487;&#20197;&#20351;&#29992;&#26631;&#20934;&#26041;&#26696;&#36827;&#34892;&#27835;&#30103;&#65292;&#32780;&#20854;&#20182;&#24739;&#32773;&#65292;&#22914;&#24930;&#24615;&#30142;&#30149;&#24739;&#32773;&#65292;&#38656;&#35201;&#20010;&#24615;&#21270;&#30340;&#27835;&#30103;&#35745;&#21010;&#12290;&#20256;&#32479;&#30340;RL&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#32771;&#34385;&#21040;&#36825;&#31181;&#24322;&#36136;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20551;&#35774;&#25152;&#26377;&#24739;&#32773;&#23545;&#27835;&#30103;&#30340;&#21453;&#24212;&#26159;&#30456;&#21516;&#30340;&#65288;&#21363;&#65292;&#36716;&#31227;&#21160;&#21147;&#23398;&#26159;&#20849;&#20139;&#30340;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32452;&#21512;Fitted $Q$-&#36845;&#20195;&#65288;CFQI&#65289;&#65292;&#23427;&#20351;&#29992;&#22797;&#21512;&#20219;&#21153;&#32467;&#26500;&#26469;&#34920;&#31034;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#24322;&#36136;&#24615;&#27835;&#30103;&#21453;&#24212;&#12290;&#22797;&#21512;&#20219;&#21153;&#30001;&#30456;&#21516;&#20219;&#21153;&#30340;&#20960;&#20010;&#21464;&#20307;&#32452;&#25104;&#65292;&#27599;&#20010;&#21464;&#20307;&#30340;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#65307;&#35299;&#20915;&#36739;&#31616;&#21333;&#30340;&#21464;&#20307;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#26356;&#38590;&#30340;&#21464;&#20307;&#12290;CFQI&#20351;&#29992;&#19968;&#20010;&#22797;&#21512;$Q$&#20540;&#20989;&#25968;&#65292;&#20854;&#20013;&#20026;&#27599;&#20010;&#20219;&#21153;&#27169;&#22359;&#21333;&#29420;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is an effective framework for solving sequential decision-making tasks. However, applying RL methods in medical care settings is challenging in part due to heterogeneity in treatment response among patients. Some patients can be treated with standard protocols whereas others, such as those with chronic diseases, need personalized treatment planning. Traditional RL methods often fail to account for this heterogeneity, because they assume that all patients respond to the treatment in the same way (i.e., transition dynamics are shared). We introduce Compositional Fitted $Q$-iteration (CFQI), which uses a compositional task structure to represent heterogeneous treatment responses in medical care settings. A compositional task consists of several variations of the same task, each progressing in difficulty; solving simpler variants of the task can enable efficient solving of harder variants. CFQI uses a compositional $Q$-value function with separate modules for ea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#37327;&#36873;&#25321;&#30340;&#35745;&#31639;&#39640;&#25928;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#23376;&#31354;&#38388;&#26469;&#20248;&#21270;&#39640;&#32500;&#22495;&#20989;&#25968;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2109.09264</link><description>&lt;p&gt;
&#21464;&#37327;&#36873;&#25321;&#30340;&#35745;&#31639;&#39640;&#25928;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Computationally Efficient High-Dimensional Bayesian Optimization via Variable Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.09264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#37327;&#36873;&#25321;&#30340;&#35745;&#31639;&#39640;&#25928;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#23376;&#31354;&#38388;&#26469;&#20248;&#21270;&#39640;&#32500;&#22495;&#20989;&#25968;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;BO&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#22330;&#26223;&#65292;&#20294;&#26159;&#24320;&#21457;&#33021;&#22815;&#36866;&#29992;&#20110;&#39640;&#32500;&#22495;&#20989;&#25968;&#30340;&#26377;&#25928;BO&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36890;&#36807;&#26222;&#36890;&#30340;BO&#20248;&#21270;&#27492;&#31867;&#20989;&#25968;&#38750;&#24120;&#32791;&#26102;&#12290;&#22522;&#20110;&#23558;&#39640;&#32500;&#31354;&#38388;&#23884;&#20837;&#21040;&#20302;&#32500;&#31354;&#38388;&#30340;&#24605;&#24819;&#30340;&#39640;&#32500;BO&#30340;&#26367;&#20195;&#31574;&#30053;&#23545;&#23884;&#20837;&#32500;&#24230;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65292;&#38656;&#35201;&#39044;&#20808;&#25351;&#23450;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#39640;&#32500;BO&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#21464;&#37327;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#23398;&#20064;&#36724;&#23545;&#40784;&#30340;&#23376;&#31354;&#38388;&#65292;&#21363;&#21253;&#21547;&#36873;&#23450;&#21464;&#37327;&#30340;&#31354;&#38388;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#39044;&#20808;&#25351;&#23450;&#30340;&#36229;&#21442;&#25968;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#24182;&#24471;&#20986;&#20102;&#36951;&#25022;&#30028;&#38480;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a method for globally optimizing black-box functions. While BO has been successfully applied to many scenarios, developing effective BO algorithms that scale to functions with high-dimensional domains is still a challenge. Optimizing such functions by vanilla BO is extremely time-consuming. Alternative strategies for high-dimensional BO that are based on the idea of embedding the high-dimensional space to the one with low dimension are sensitive to the choice of the embedding dimension, which needs to be pre-specified. We develop a new computationally efficient high-dimensional BO method that exploits variable selection. Our method is able to automatically learn axis-aligned sub-spaces, i.e. spaces containing selected variables, without the demand of any pre-specified hyperparameters. We theoretically analyze the computational complexity of our algorithm and derive the regret bound. We empirically show the efficacy of our method on several synthetic and re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#38598;&#25104;&#26694;&#26550;&#8212;&#8212;&#26426;&#22120;&#21327;&#20316;&#65288;MaC&#65289;&#65292;&#36890;&#36807;&#24490;&#29615;&#21644;&#20132;&#20114;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#20351;&#22522;&#30784;&#26426;&#22120;&#33021;&#22815;&#24490;&#29615;&#20256;&#36882;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#26356;&#26032;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MaC&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2105.02569</link><description>&lt;p&gt;
&#26426;&#22120;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Machine Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2105.02569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#38598;&#25104;&#26694;&#26550;&#8212;&#8212;&#26426;&#22120;&#21327;&#20316;&#65288;MaC&#65289;&#65292;&#36890;&#36807;&#24490;&#29615;&#21644;&#20132;&#20114;&#30340;&#23398;&#20064;&#26041;&#24335;&#65292;&#20351;&#22522;&#30784;&#26426;&#22120;&#33021;&#22815;&#24490;&#29615;&#20256;&#36882;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#26356;&#26032;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MaC&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30417;&#30563;&#23398;&#20064;&#38598;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;&#26426;&#22120;&#21327;&#20316;&#65288;MaC&#65289;&#65292;&#21033;&#29992;&#19968;&#32452;&#22522;&#30784;&#26426;&#22120;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#12290;&#19982;&#24182;&#34892;&#19988;&#29420;&#31435;&#30340;bagging/stacking&#26694;&#26550;&#21644;&#39034;&#24207;&#19988;&#33258;&#19978;&#32780;&#19979;&#30340;boosting&#26694;&#26550;&#19981;&#21516;&#65292;MaC&#26159;&#19968;&#31181;&#24490;&#29615;&#21644;&#20132;&#20114;&#23398;&#20064;&#26694;&#26550;&#12290;&#24490;&#29615;&#21644;&#20132;&#20114;&#29305;&#24615;&#24110;&#21161;&#22522;&#30784;&#26426;&#22120;&#24490;&#29615;&#20256;&#36882;&#20449;&#24687;&#24182;&#30456;&#24212;&#22320;&#26356;&#26032;&#20854;&#32467;&#26500;&#21644;&#21442;&#25968;&#12290;&#23545;&#20110;&#20174;MaC&#24471;&#20986;&#30340;&#20272;&#35745;&#22120;&#30340;&#39118;&#38505;&#30028;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#24490;&#29615;&#21644;&#20132;&#20114;&#29305;&#24615;&#21487;&#20197;&#24110;&#21161;MaC&#36890;&#36807;&#31616;&#27905;&#30340;&#38598;&#25104;&#20943;&#23569;&#39118;&#38505;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#25968;&#25454;&#21644;119&#20010;&#22522;&#20934;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;MaC&#30340;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#21253;&#25324;&#20998;&#31867;&#22238;&#24402;&#26641;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#22534;&#21472;&#21644;&#25552;&#21319;&#22312;&#20869;&#30340;&#20854;&#20182;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new ensemble framework for supervised learning, called machine collaboration (MaC), using a collection of base machines for prediction tasks. Unlike bagging/stacking (a parallel &amp; independent framework) and boosting (a sequential &amp; top-down framework), MaC is a type of circular &amp; interactive learning framework. The circular &amp; interactive feature helps the base machines to transfer information circularly and update their structures and parameters accordingly. The theoretical result on the risk bound of the estimator from MaC reveals that the circular &amp; interactive feature can help MaC reduce risk via a parsimonious ensemble. We conduct extensive experiments on MaC using both simulated data and 119 benchmark real datasets. The results demonstrate that in most cases, MaC performs significantly better than several other state-of-the-art methods, including classification and regression trees, neural networks, stacking, and boosting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20856;&#22411;&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24378;&#40065;&#26834;&#24615;&#21644;&#24369;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#25968;&#20540;&#23454;&#39564;&#26469;&#21152;&#24378;&#36825;&#20123;&#27010;&#24565;&#30340;&#27934;&#23519;&#21147;&#12290;</title><link>https://arxiv.org/abs/2104.03527</link><description>&lt;p&gt;
&#31232;&#30095;NMF&#19982;&#20856;&#22411;&#27491;&#21017;&#21270;&#65306;&#35745;&#31639;&#21644;&#40065;&#26834;&#24615;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Sparse NMF with Archetypal Regularization: Computational and Robustness Properties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.03527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20856;&#22411;&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24378;&#40065;&#26834;&#24615;&#21644;&#24369;&#40065;&#26834;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#29702;&#35770;&#20445;&#35777;&#21644;&#25968;&#20540;&#23454;&#39564;&#26469;&#21152;&#24378;&#36825;&#20123;&#27010;&#24565;&#30340;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#20856;&#22411;&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#38382;&#39064;&#12290;&#30446;&#26631;&#26159;&#23558;&#19968;&#32452;&#25968;&#25454;&#28857;&#34920;&#31034;&#20026;&#23569;&#25968;&#38750;&#36127;&#31232;&#30095;&#22240;&#23376;&#30340;&#38750;&#36127;&#32447;&#24615;&#32452;&#21512;&#65292;&#36825;&#20123;&#22240;&#23376;&#20855;&#26377;&#21560;&#24341;&#20154;&#30340;&#20960;&#20309;&#29305;&#24615;&#65292;&#26469;&#33258;&#20110;&#20351;&#29992;&#20856;&#22411;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#22312;Javadi&#21644;Montanari&#65288;2019&#65289;&#20013;&#30740;&#31350;&#30340;&#40065;&#26834;&#24615;&#27010;&#24565;&#65288;&#26080;&#31232;&#30095;&#24615;&#65289;&#25512;&#24191;&#20026;&#65288;a&#65289;&#24378;&#40065;&#26834;&#24615;&#65292;&#21363;&#27599;&#20010;&#20272;&#35745;&#30340;&#20856;&#22411;&#37117;&#25509;&#36817;&#30495;&#23454;&#30340;&#20856;&#22411;&#65292;&#20197;&#21450;&#65288;b&#65289;&#24369;&#40065;&#26834;&#24615;&#65292;&#21363;&#33267;&#23569;&#23384;&#22312;&#19968;&#20010;&#24674;&#22797;&#30340;&#20856;&#22411;&#25509;&#36817;&#30495;&#23454;&#30340;&#20856;&#22411;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#23545;&#20110;&#22522;&#30784;&#25968;&#25454;&#30340;&#20551;&#35774;&#36739;&#20026;&#31616;&#21270;&#65292;&#24182;&#36866;&#29992;&#20110;&#22522;&#20110;&#19981;&#38656;&#35201;&#31232;&#30095;&#24615;&#30340;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sparse nonnegative matrix factorization (NMF) using archetypal regularization. The goal is to represent a collection of data points as nonnegative linear combinations of a few nonnegative sparse factors with appealing geometric properties, arising from the use of archetypal regularization. We generalize the notion of robustness studied in Javadi and Montanari (2019) (without sparsity) to the notions of (a) strong robustness that implies each estimated archetype is close to the underlying archetypes and (b) weak robustness that implies there exists at least one recovered archetype that is close to the underlying archetypes. Our theoretical results on robustness guarantees hold under minimal assumptions on the underlying data, and applies to settings where the underlying archetypes need not be sparse. We present theoretical results and illustrative examples to strengthen the insights underlying the notions of robustness. We propose new algorithms for our optimi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#21019;&#26032;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#20869;&#37096;&#38382;&#39064;&#34920;&#36798;&#26080;&#27861;&#26356;&#25913;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#32570;&#20047;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2012.02282</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#21019;&#36896;&#21147;&#65306;&#27010;&#24565;&#21270;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Creativity of Deep Learning: Conceptualization and Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2012.02282
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21019;&#24847;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20351;&#29992;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#35270;&#35282;&#23545;&#20854;&#36827;&#34892;&#20102;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#32467;&#26524;&#65292;&#20294;&#20854;&#21019;&#26032;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#20869;&#37096;&#38382;&#39064;&#34920;&#36798;&#26080;&#27861;&#26356;&#25913;&#20197;&#21450;&#22312;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#32570;&#20047;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#33258;&#21160;&#21270;&#31616;&#21333;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#32034;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#21019;&#36896;&#24615;&#35774;&#35745;&#65292;&#26080;&#35770;&#26159;&#23436;&#25972;&#30340;&#20135;&#21697;&#21019;&#20316;&#36824;&#26159;&#25903;&#25345;&#20154;&#31867;&#22312;&#21019;&#20316;&#36807;&#31243;&#20013;&#30340;&#35282;&#33394;&#12290;&#26412;&#25991;&#21033;&#29992;&#35745;&#31639;&#21019;&#36896;&#21147;&#30340;&#35265;&#35299;&#65292;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;&#20102;&#25991;&#29486;&#32508;&#36848;&#20013;&#30830;&#23450;&#30340;&#21019;&#36896;&#24615;&#39046;&#22495;&#20013;&#24403;&#21069;&#24212;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#24403;&#21069;&#31995;&#32479;&#19982;&#19981;&#21516;&#27169;&#22411;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#30456;&#20284;&#20043;&#22788;&#20197;&#21450;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#31561;&#39640;&#20215;&#20540;&#32467;&#26524;&#65292;&#20294;&#20854;&#26032;&#39062;&#24615;&#36890;&#24120;&#21463;&#21040;&#22810;&#31181;&#38480;&#21046;&#65292;&#20363;&#22914;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#23450;&#20041;&#30340;&#27010;&#24565;&#31354;&#38388;&#30340;&#38480;&#21046;&#12290;&#24403;&#21069;&#30340;DL&#26041;&#27861;&#20063;&#19981;&#20801;&#35768;&#23545;&#20869;&#37096;&#38382;&#39064;&#34920;&#36798;&#36827;&#34892;&#26356;&#25913;&#65292;&#24182;&#19988;&#23427;&#20204;&#32570;&#20047;&#22312;&#39640;&#24230;&#19981;&#21516;&#30340;&#39046;&#22495;&#20043;&#38388;&#24314;&#31435;&#32852;&#31995;&#30340;&#33021;&#21147;&#65292;&#36825;&#20004;&#28857;&#37117;&#34987;&#35748;&#20026;&#26159;&#20027;&#35201;&#30340;&#25512;&#21160;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the potential of deep learning (DL) for automating simple tasks is already well explored, recent research has started investigating the use of deep learning for creative design, both for complete artifact creation and supporting humans in the creation process. In this paper, we use insights from computational creativity to conceptualize and assess current applications of generative deep learning in creative domains identified in a literature review. We highlight parallels between current systems and different models of human creativity as well as their shortcomings. While deep learning yields results of high value, such as high-quality images, their novelty is typically limited due to multiple reasons such as being tied to a conceptual space defined by training data. Current DL methods also do not allow for changes in the internal problem representation, and they lack the capability to identify connections across highly different domains, both of which are seen as major drivers o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#21644;MCTS&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#19979;&#26827;&#27700;&#24179;&#65292;&#39564;&#35777;&#20102;&#38598;&#25104;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#23637;&#31034;&#20102;&#34701;&#20837;&#19987;&#23478;&#30693;&#35782;&#21644;&#25112;&#30053;&#21407;&#21017;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16852</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#31181;&#26041;&#24335;&#65292;&#23558;&#28151;&#21512;&#19987;&#23478;&#19982;MCTS&#30456;&#32467;&#21512;&#20197;&#25552;&#39640;&#22269;&#38469;&#35937;&#26827;&#20013;&#30340;&#26657;&#39564;
&lt;/p&gt;
&lt;p&gt;
Checkmating One, by Using Many: Combining Mixture of Experts with MCTS to Improve in Chess. (arXiv:2401.16852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#21644;MCTS&#30456;&#32467;&#21512;&#65292;&#26412;&#30740;&#31350;&#22312;&#22269;&#38469;&#35937;&#26827;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#19979;&#26827;&#27700;&#24179;&#65292;&#39564;&#35777;&#20102;&#38598;&#25104;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24182;&#23637;&#31034;&#20102;&#34701;&#20837;&#19987;&#23478;&#30693;&#35782;&#21644;&#25112;&#30053;&#21407;&#21017;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#19982;&#35745;&#31639;&#26426;&#26827;&#30424;&#30456;&#32467;&#21512;&#65292;&#21516;&#26102;&#20351;&#29992;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#22871;&#19987;&#38376;&#35774;&#35745;&#30340;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#38024;&#23545;&#28216;&#25103;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#23450;&#21464;&#21270;&#20570;&#20986;&#21709;&#24212;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#31232;&#30095;&#28608;&#27963;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#19982;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#20351;&#20854;&#19982;&#22269;&#38469;&#35937;&#26827;&#30340;&#25112;&#30053;&#38454;&#27573;&#30456;&#19968;&#33268;&#65292;&#20174;&#32780;&#25670;&#33073;&#20256;&#32479;&#30340;&#8220;&#19968;&#20992;&#20999;&#8221;&#30340;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#21516;&#30340;&#28216;&#25103;&#38454;&#27573;&#23450;&#20041;&#65292;&#23558;&#35745;&#31639;&#20219;&#21153;&#26377;&#25928;&#22320;&#20998;&#37197;&#32473;&#22810;&#20010;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#26174;&#31034;&#65292;&#22312;&#28216;&#25103;&#23454;&#21147;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#36229;&#36807;&#20102;&#20256;&#32479;&#30340;&#21333;&#27169;&#22411;&#26694;&#26550;&#12290;&#36825;&#35777;&#23454;&#20102;&#25105;&#20204;&#38598;&#25104;&#26041;&#27861;&#30340;&#21151;&#25928;&#65292;&#24182;&#20984;&#26174;&#20102;&#23558;&#19987;&#23478;&#30693;&#35782;&#21644;&#25112;&#30053;&#21407;&#21017;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new approach that integrates deep learning with computational chess, using both the Mixture of Experts (MoE) method and Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized models, each designed to respond to specific changes in the game's input data. This results in a framework with sparsely activated models, which provides significant computational benefits. Our framework combines the MoE method with MCTS, in order to align it with the strategic phases of chess, thus departing from the conventional ``one-for-all'' model. Instead, we utilize distinct game phase definitions to effectively distribute computational tasks across multiple expert neural networks. Our empirical research shows a substantial improvement in playing strength, surpassing the traditional single-model framework. This validates the efficacy of our integrated approach and highlights the potential of incorporating expert knowledge and strategic principles into neural net
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15292</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#33258;&#36866;&#24212;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Block sparse regularization under arbitrary linear transform. (arXiv:2401.15292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15292
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25193;&#22823;&#20102;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#26410;&#30693;&#22359;&#32467;&#26500;&#19979;&#30340;&#20219;&#24847;&#32447;&#24615;&#21464;&#25442;&#19979;&#30340;&#22359;&#31232;&#30095;&#20449;&#21495;&#37325;&#26500;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#26159;&#29616;&#26377;&#26041;&#27861;LOP-$\ell_2$/$\ell_1$&#30340;&#25512;&#24191;&#65292;&#21487;&#20197;&#22312;&#38750;&#21487;&#36870;&#21464;&#25442;&#19979;&#37325;&#26500;&#20855;&#26377;&#22359;&#31232;&#30095;&#24615;&#30340;&#20449;&#21495;&#65292;&#32780;LOP-$\ell_2$/$\ell_1$&#19981;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#22823;&#20102;&#22359;&#31232;&#30095;&#27491;&#21017;&#21270;&#30340;&#33539;&#22260;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#39046;&#22495;&#20013;&#24212;&#29992;&#26356;&#21152;&#28789;&#27963;&#21644;&#24378;&#22823;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#19968;&#20010;&#36845;&#20195;&#31639;&#27861;&#26469;&#27714;&#35299;&#35813;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#20854;&#25910;&#25947;&#21040;&#26368;&#20248;&#35299;&#30340;&#26465;&#20214;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a convex signal reconstruction method for block sparsity under arbitrary linear transform with unknown block structure. The proposed method is a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can reconstruct signals with block sparsity under non-invertible transforms, unlike LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse regularization, enabling more versatile and powerful applications across various signal processing domains. We derive an iterative algorithm for solving proposed method and provide conditions for its convergence to the optimal solution. Numerical experiments demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14876</link><description>&lt;p&gt;
&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65306;&#38598;&#25104;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Cross-Space Adaptive Filter: Integrating Graph Topology and Node Attributes for Alleviating the Over-smoothing Problem. (arXiv:2401.14876v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65288;CSF&#65289;&#65292;&#21487;&#20197;&#20174;&#22270;&#25299;&#25169;&#21644;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#65292;&#20197;&#20943;&#36731;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20351;&#29992;&#20302;&#36890;&#28388;&#27874;&#22120;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#20302;&#39057;&#20449;&#21495;&#65292;&#20294;&#24403;GCN&#28145;&#24230;&#22686;&#21152;&#26102;&#21487;&#33021;&#23548;&#33268;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#20174;&#22270;&#25299;&#25169;&#20013;&#25552;&#21462;&#30340;&#39069;&#22806;&#28388;&#27874;&#22120;&#65288;&#22914;&#39640;&#36890;&#28388;&#27874;&#22120;&#65289;&#26469;&#21019;&#24314;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#24573;&#35270;&#20102;&#33410;&#28857;&#23646;&#24615;&#31354;&#38388;&#65292;&#36825;&#20005;&#37325;&#29306;&#29298;&#20102;&#28145;&#23618;GCN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#38750;&#21516;&#37197;&#22270;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#22120;&#65292;&#31216;&#20026;CSF&#65292;&#33021;&#22815;&#20174;&#25299;&#25169;&#21644;&#23646;&#24615;&#31354;&#38388;&#20013;&#25552;&#21462;&#33258;&#36866;&#24212;&#39057;&#29575;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#20110;&#23646;&#24615;&#30340;&#39640;&#36890;&#28388;&#27874;&#22120;&#65292;&#21487;&#20197;&#20174;&#29702;&#35770;&#19978;&#35299;&#37322;&#20026;&#21322;&#30417;&#30563;&#26680;&#23725;&#22238;&#24402;&#30340;&#26368;&#23567;&#21270;&#22120;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#22522;&#20110;&#25299;&#25169;&#30340;&#20302;&#36890;&#28388;&#27874;&#22120;&#35270;&#20026;Mercer's&#26680;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to extract low-frequency signals from graph topology, which may lead to the over-smoothing problem when GCN goes deep. To this end, various methods have been proposed to create an adaptive filter by incorporating an extra filter (e.g., a high-pass filter) extracted from the graph topology. However, these methods heavily rely on topological information and ignore the node attribute space, which severely sacrifices the expressive power of the deep GCNs, especially when dealing with disassortative graphs. In this paper, we propose a cross-space adaptive filter, called CSF, to produce the adaptive-frequency information extracted from both the topology and attribute spaces. Specifically, we first derive a tailored attribute-based high-pass filter that can be interpreted theoretically as a minimizer for semi-supervised kernel ridge regression. Then, we cast the topology-based low-pass filter as a Mercer's kernel within the 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#21453;&#28436;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#25311;&#22120;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#19981;&#21487;&#24494;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13695</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#39063;&#31890;&#27969;&#30340;&#21453;&#28436;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inverse analysis of granular flows using differentiable graph neural network simulator. (arXiv:2401.13695v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13695
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;&#36827;&#34892;&#21453;&#28436;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27169;&#25311;&#22120;&#35745;&#31639;&#24320;&#38144;&#22823;&#12289;&#19981;&#21487;&#24494;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39063;&#31890;&#27969;&#20013;&#30340;&#21453;&#28436;&#38382;&#39064;&#65292;&#22914;&#23665;&#20307;&#28369;&#22369;&#21644;&#30862;&#23633;&#27969;&#65292;&#28041;&#21450;&#22522;&#20110;&#30446;&#26631;&#27874;&#21160;&#21078;&#38754;&#20272;&#35745;&#26448;&#26009;&#21442;&#25968;&#25110;&#36793;&#30028;&#26465;&#20214;&#12290;&#20256;&#32479;&#30340;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#23545;&#36825;&#20123;&#21453;&#28436;&#38382;&#39064;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#65292;&#38480;&#21046;&#20102;&#21487;&#33021;&#30340;&#27169;&#25311;&#27425;&#25968;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#19981;&#21487;&#24494;&#24615;&#20351;&#24471;&#26799;&#24230;&#20248;&#21270;&#26041;&#27861;&#26080;&#27861;&#24212;&#29992;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#32500;&#38382;&#39064;&#20013;&#20197;&#20854;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#34429;&#28982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20195;&#29702;&#27169;&#22411;&#25552;&#20379;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#24494;&#24615;&#65292;&#20294;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#20302;&#32500;&#36755;&#20837;-&#36755;&#20986;&#26144;&#23556;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#39063;&#31890;&#27969;&#30340;&#23436;&#25972;&#29289;&#29702;&#36807;&#31243;&#65292;&#22240;&#27492;&#24448;&#24448;&#38590;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#22120;(GNS)&#65292;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#21521;&#27169;&#24335;&#33258;&#21160;&#24494;&#20998;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#21453;&#28436;&#38382;&#39064;&#12290;GNS&#23398;&#20064;&#20102;&#39063;&#31890;&#27969;&#30340;&#21160;&#21147;&#23398;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inverse problems in granular flows, such as landslides and debris flows, involve estimating material parameters or boundary conditions based on target runout profile. Traditional high-fidelity simulators for these inverse problems are computationally demanding, restricting the number of simulations possible. Additionally, their non-differentiable nature makes gradient-based optimization methods, known for their efficiency in high-dimensional problems, inapplicable. While machine learning-based surrogate models offer computational efficiency and differentiability, they often struggle to generalize beyond their training data due to their reliance on low-dimensional input-output mappings that fail to capture the complete physics of granular flows. We propose a novel differentiable graph neural network simulator (GNS) by combining reverse mode automatic differentiation of graph neural networks with gradient-based optimization for solving inverse problems. GNS learns the dynamics of granula
&lt;/p&gt;</description></item><item><title>TurboSVM-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#19981;&#22686;&#21152;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12012</link><description>&lt;p&gt;
TurboSVM-FL: &#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients. (arXiv:2401.12012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12012
&lt;/p&gt;
&lt;p&gt;
TurboSVM-FL&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;&#65292;&#36890;&#36807;SVM&#32858;&#21512;&#20026;&#25042;&#24816;&#23458;&#25143;&#31471;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#12290;&#36825;&#31181;&#31574;&#30053;&#22312;&#19981;&#22686;&#21152;&#23458;&#25143;&#31471;&#35745;&#31639;&#36127;&#25285;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#65292;&#22312;&#36817;&#24180;&#26469;&#33719;&#24471;&#20102;&#24378;&#28872;&#30340;&#25512;&#21160;&#21147;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#20013;&#22830;&#26381;&#21153;&#22120;&#23450;&#26399;&#36890;&#36807;&#23458;&#25143;&#31471;&#21327;&#35843;&#27169;&#22411;&#65292;&#24182;&#32858;&#21512;&#30001;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#26412;&#22320;&#25968;&#25454;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#32852;&#37030;&#23398;&#20064;&#30340;&#23454;&#26045;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#23548;&#33268;&#30340;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#25910;&#25947;&#36895;&#24230;&#24930;&#22312;&#36328;&#35774;&#22791;&#32852;&#37030;&#23398;&#20064;&#22330;&#26223;&#20013;&#23588;&#20026;&#38382;&#39064;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#21487;&#33021;&#21463;&#21040;&#35745;&#31639;&#33021;&#21147;&#21644;&#23384;&#20648;&#31354;&#38388;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#22240;&#27492;&#23545;&#23458;&#25143;&#31471;&#20135;&#29983;&#39069;&#22806;&#35745;&#31639;&#25110;&#20869;&#23384;&#36127;&#25285;&#30340;&#26041;&#27861;&#65292;&#22914;&#36741;&#21161;&#30446;&#26631;&#39033;&#21644;&#26356;&#22823;&#30340;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#65292;&#21487;&#33021;&#19981;&#23454;&#38469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#32858;&#21512;&#31574;&#30053;TurboSVM-FL&#65292;&#23427;&#19981;&#20250;&#32473;&#23458;&#25143;&#31471;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#26816;&#27979;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#65292;&#24182;&#36890;&#36807;&#34013;&#29273;&#20256;&#36755;&#32467;&#26524;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.10386</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;
&lt;/p&gt;
&lt;p&gt;
Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest Machine Learning. (arXiv:2401.10386v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#30340;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#26816;&#27979;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#65292;&#24182;&#36890;&#36807;&#34013;&#29273;&#20256;&#36755;&#32467;&#26524;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#31561;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24613;&#24615;&#38388;&#23460;&#32508;&#21512;&#24449;&#65288;ACS&#65289;&#26159;&#19968;&#31181;&#39592;&#31185;&#24613;&#30151;&#65292;&#30001;&#32908;&#32905;&#38388;&#23460;&#20869;&#30340;&#21387;&#21147;&#21319;&#39640;&#24341;&#36215;&#65292;&#23548;&#33268;&#27704;&#20037;&#32452;&#32455;&#25439;&#20260;&#24182;&#26368;&#32456;&#33268;&#27515;&#12290;ACS&#30340;&#35786;&#26029;&#20027;&#35201;&#20381;&#36182;&#24739;&#32773;&#25253;&#21578;&#30340;&#30151;&#29366;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20020;&#24202;&#19978;&#19981;&#21487;&#38752;&#65292;&#36890;&#24120;&#38656;&#35201;&#36890;&#36807;&#20405;&#20837;&#24615;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#27979;&#37327;&#36827;&#34892;&#34917;&#20805;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#12289;&#23458;&#35266;&#12289;&#38750;&#20405;&#20837;&#24615;&#30340;ACS&#35786;&#26029;&#26041;&#27861;&#12290;&#35813;&#35774;&#22791;&#36890;&#36807;&#19968;&#20010;&#20351;&#29992;&#36148;&#22312;&#30382;&#32932;&#19978;&#30340;&#21387;&#21147;&#20256;&#24863;&#30005;&#38459;&#22120;&#65288;FSR&#65289;&#30340;&#38543;&#26426;&#26862;&#26519;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;ACS&#12290;&#26368;&#32456;&#35786;&#26029;&#32467;&#26524;&#36890;&#36807;&#34013;&#29273;&#20197;&#23454;&#26102;&#26041;&#24335;&#20256;&#36755;&#21040;Web&#24212;&#29992;&#31243;&#24207;&#12290;&#20026;&#20102;&#39564;&#35777;&#35786;&#26029;&#32467;&#26524;&#65292;&#21019;&#24314;&#20102;&#19968;&#32452;&#21253;&#21547;FSR&#27979;&#37327;&#21644;&#30456;&#24212;&#30340;&#27169;&#25311;&#32908;&#32905;&#38388;&#23460;&#21387;&#21147;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#35786;&#26029;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;97%&#65292;&#19982;&#20405;&#20837;&#24615;&#30340;&#37329;&#26631;&#20934;&#25345;&#24179;&#12290;&#35813;&#35774;&#22791;&#22312;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#21253;&#25324;&#20934;&#30830;&#24230;&#12289;&#28789;&#25935;&#24230;&#21644;F1&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acute compartment syndrome (ACS) is an orthopedic emergency, caused by elevated pressure within a muscle compartment, that leads to permanent tissue damage and eventually death. Diagnosis of ACS relies heavily on patient-reported symptoms, a method that is clinically unreliable and often supplemented with invasive intracompartmental pressure measurements. This study proposes a continuous, objective, noninvasive diagnostic for ACS. The device detects ACS through a random forest machine learning model that uses pressure readings from force-sensitive resistors (FSRs) placed on the skin. The final diagnosis is exported real-time to a web application via Bluetooth. To validate the diagnostic, a data set containing FSR measurements and the corresponding simulated intracompartmental pressure was created. The diagnostic achieved an accuracy, on par to the invasive gold standard, of 97%. The device excelled in key performance metrics including precision, sensitivity, and F1 score. Manufactured 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09192</link><description>&lt;p&gt;
&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20934;&#22791;&#35838;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09192
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#65292;&#20026;&#28176;&#36827;&#24335;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#20102;&#35838;&#31243;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#36164;&#28304;&#28040;&#32791;&#21644;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&#22686;&#21152;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#38271;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23567;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#26032;&#30340;&#27169;&#22411;&#32467;&#26500;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#21487;&#33021;&#24456;&#24930;&#65292;&#24182;&#19988;&#28176;&#36827;&#22534;&#21472;&#23618;&#24448;&#24448;&#26080;&#27861;&#23454;&#29616;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Apollo&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#22312;&#20302;&#23618;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#39640;&#23618;&#21151;&#33021;&#26469;&#20934;&#22791;&#33192;&#32960;&#25805;&#20316;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20302;&#20540;&#20248;&#20808;&#37319;&#26679;(LVPS)&#26469;&#35757;&#32451;&#19981;&#21516;&#28145;&#24230;&#65292;&#24182;&#24341;&#20837;&#26435;&#37325;&#20849;&#20139;&#20197;&#20419;&#36827;&#39640;&#25928;&#25193;&#23637;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#31283;&#23450;&#30340;&#27169;&#22411;&#28145;&#24230;&#25193;&#23637;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Apollo&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21152;&#36895;&#27604;&#29575;&#65292;&#29978;&#33267;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The rapid progress of Transformers in artificial intelligence has come at the cost of increased resource consumption and greenhouse gas emissions due to growing model sizes. Prior work suggests using pretrained small models to improve training efficiency, but this approach may not be suitable for new model structures. On the other hand, training from scratch can be slow, and progressively stacking layers often fails to achieve significant acceleration. To address these challenges, we propose a novel method called Apollo, which prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by \textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of low layers. Our approach involves low-value-prioritized sampling (LVPS) to train different depths and weight sharing to facilitate efficient expansion. We also introduce an interpolation method for stable model depth extension. Experiments demonstrate that Apollo achieves state-of-the-art acceleration ratios, even
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.02333</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#21462;&#65306;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#19978;&#19979;&#25991;&#21270;&#30340;&#34920;&#26684;&#25968;&#25454;&#20197;&#23454;&#29616;&#39640;&#25928;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models. (arXiv:2401.02333v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#34920;&#26684;&#25968;&#25454;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104; (RAG) &#26550;&#26500;&#22312;&#20174;&#21508;&#31181;&#25991;&#20214;&#20013;&#26816;&#32034;&#20449;&#24687;&#26041;&#38754;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#21253;&#21547;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340; PDF &#25991;&#26723;&#20013;&#30340;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#26102;&#20250;&#36935;&#21040;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640; RAG &#31995;&#32479;&#20013;&#22797;&#26434;&#34920;&#26684;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558; PDF &#23384;&#20648;&#22312;&#26816;&#32034;&#25968;&#25454;&#24211;&#20013;&#65292;&#24182;&#21333;&#29420;&#25552;&#21462;&#34920;&#26684;&#20869;&#23481;&#12290;&#25552;&#21462;&#30340;&#34920;&#26684;&#32463;&#36807;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#22788;&#29702;&#65292;&#23558;&#26631;&#39064;&#19982;&#30456;&#24212;&#30340;&#20540;&#36830;&#25509;&#36215;&#26469;&#12290;&#20026;&#20102;&#30830;&#20445;&#23545;&#20016;&#23500;&#25968;&#25454;&#30340;&#20840;&#38754;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340; Llama-2-chat &#35821;&#35328;&#27169;&#22411;&#22312; RAG &#26550;&#26500;&#20013;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#27425;&#24615;&#25552;&#31034;&#20351;&#29992; ChatGPT 3.5 API &#22686;&#24378;&#34920;&#26684;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#20016;&#23500;&#30340;&#25968;&#25454;&#19982;&#20854;&#20182; PDF &#25991;&#20214;&#19968;&#36215;&#36755;&#20837;&#26816;&#32034;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our appr
&lt;/p&gt;</description></item><item><title>Kernel-U-Net&#26159;&#19968;&#31181;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.01479</link><description>&lt;p&gt;
Kernel-U-Net: &#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting. (arXiv:2401.01479v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01479
&lt;/p&gt;
&lt;p&gt;
Kernel-U-Net&#26159;&#19968;&#31181;&#23618;&#27425;&#21644;&#23545;&#31216;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#26159;&#22522;&#20110;&#21382;&#21490;&#20449;&#24687;&#39044;&#27979;&#26410;&#26469;&#36235;&#21183;&#12290;&#26368;&#36817;&#22522;&#20110;U-Net&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#30495;&#23454;&#25968;&#25454;&#38598;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#27604;&#22522;&#20110;&#34917;&#19969;&#27169;&#22411;&#25110;&#32447;&#24615;&#27169;&#22411;&#30340;&#27169;&#22411;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31216;&#21644;&#23618;&#27425;&#21270;&#30340;&#26694;&#26550;&#65292;Kernel-U-Net&#65292;&#23427;&#22312;&#32593;&#32476;&#30340;&#27599;&#19968;&#23618;&#23558;&#36755;&#20837;&#24207;&#21015;&#20999;&#21106;&#25104;&#29255;&#27573;&#65292;&#28982;&#21518;&#20351;&#29992;&#21367;&#31215;&#26680;&#36827;&#34892;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#23427;&#25193;&#23637;&#20102;&#32463;&#20856;U-Net&#20013;&#30340;&#21367;&#31215;&#26680;&#30340;&#27010;&#24565;&#65292;&#21487;&#20197;&#25509;&#21463;&#31526;&#21512;&#30456;&#21516;&#35774;&#35745;&#27169;&#24335;&#30340;&#33258;&#23450;&#20041;&#21367;&#31215;&#26680;&#12290;&#19982;&#29616;&#26377;&#30340;&#32447;&#24615;&#25110;&#22522;&#20110;transformer&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65306;1&#65289;&#21442;&#25968;&#25968;&#37327;&#36739;&#23569;&#65306;&#21442;&#25968;&#22823;&#23567;&#20026;$O(log(L)^2)$&#65292;&#20854;&#20013;$L$&#20026;&#22238;&#28335;&#31383;&#21475;&#22823;&#23567;&#65307;2&#65289;&#28789;&#27963;&#24615;&#65306;&#20854;&#21367;&#31215;&#26680;&#21487;&#20197;&#23450;&#21046;&#21644;&#36866;&#24212;&#25968;&#25454;&#38598;&#65307;3&#65289;&#35745;&#31639;&#25928;&#29575;&#65306;&#22914;&#26524;&#20351;&#29992;&#27492;&#27169;&#22411;&#65292;transformer&#27169;&#22359;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23567;&#20026;$O(log(L)^2)$&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series forecasting task predicts future trends based on historical information. Recent U-Net-based methods have demonstrated superior performance in predicting real-world datasets. However, the performance of these models is lower than patch-based models or linear models. In this work, we propose a symmetric and hierarchical framework, Kernel-U-Net, which cuts the input sequence into slices at each layer of the network and then computes them using kernels. Furthermore, it generalizes the concept of convolutional kernels in classic U-Net to accept custom kernels that follow the same design pattern. Compared to the existing linear or transformer-based solution, our model contains 3 advantages: 1) A small number of parameters: the parameters size is $O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its kernels can be customized and fitted to the datasets, 3) Computation efficiency: the computation complexity of transformer modules is reduced to $O(log(L)^2)$ if th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;</title><link>http://arxiv.org/abs/2401.01404</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23376;&#20108;&#27425;&#26102;&#38388;&#32593;&#32476;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01404
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#32593;&#32476;&#37325;&#24314;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#36890;&#36807;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#20135;&#29983;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#37325;&#24314;&#26159;&#25351;&#22312;&#21482;&#26377;&#20851;&#20110;&#26465;&#20214;&#20598;&#32852;&#30340;&#35266;&#27979;&#25968;&#25454;&#65292;&#20363;&#22914;&#26102;&#38388;&#24207;&#21015;&#25110;&#22270;&#27169;&#22411;&#30340;&#29420;&#31435;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30830;&#23450;N&#20010;&#33410;&#28857;&#20043;&#38388;&#26410;&#35266;&#27979;&#21040;&#30340;&#25104;&#23545;&#32806;&#21512;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#30340;&#20027;&#35201;&#38556;&#30861;&#26159;&#20284;&#20046;&#26080;&#27861;&#36991;&#20813;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;O(N^2)&#65292;&#21363;&#35201;&#32771;&#34385;&#27599;&#31181;&#21487;&#33021;&#30340;&#25104;&#23545;&#32806;&#21512;&#33267;&#23569;&#19968;&#27425;&#65292;&#23613;&#31649;&#22823;&#22810;&#25968;&#24863;&#20852;&#36259;&#30340;&#32593;&#32476;&#37117;&#26159;&#31232;&#30095;&#30340;&#65292;&#38750;&#38646;&#32806;&#21512;&#30340;&#25968;&#37327;&#21482;&#26377;O(N)&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#24191;&#27867;&#37325;&#24314;&#38382;&#39064;&#30340;&#36890;&#29992;&#31639;&#27861;&#65292;&#20854;&#22312;&#23376;&#20108;&#27425;&#26102;&#38388;&#20869;&#23454;&#29616;&#32467;&#26524;&#65292;&#20854;&#25968;&#25454;&#30456;&#20851;&#22797;&#26434;&#24230;&#23485;&#26494;&#19978;&#30028;&#20026;O(N^(3/2)logN)&#65292;&#20294;&#20855;&#26377;&#26356;&#20856;&#22411;&#30340;&#23545;&#25968;&#32447;&#24615;&#22797;&#26434;&#24230;O(Nlog^2 N)&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#38543;&#26426;&#30340;&#20108;&#38454;&#37051;&#23621;&#25628;&#32034;&#65292;&#20135;&#29983;&#20102;&#26368;&#20339;&#30340;&#36793;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
&lt;/p&gt;</description></item><item><title>&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.15910</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15910
&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28040;&#38500;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#35299;&#20915;&#29615;&#22659;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#35813;&#39046;&#22495;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#28040;&#38500;&#23398;&#20064;&#25351;&#30340;&#26159;&#26681;&#25454;&#25968;&#25454;&#25152;&#26377;&#32773;&#30340;&#35831;&#27714;&#65292;&#38477;&#20302;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#28040;&#38500;&#23398;&#20064;&#30340;&#30740;&#31350;&#20013;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37027;&#23601;&#26159;&#24378;&#21270;&#23398;&#20064;&#12290;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#26234;&#33021;&#20307;&#22312;&#29615;&#22659;&#20013;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#20197;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26234;&#33021;&#20307;&#24448;&#24448;&#20250;&#35760;&#24518;&#29615;&#22659;&#30340;&#29305;&#24449;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#22823;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#26681;&#25454;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#65292;&#29615;&#22659;&#30340;&#25152;&#26377;&#32773;&#26377;&#26435;&#25764;&#38144;&#26234;&#33021;&#20307;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#23637;&#19968;&#20010;&#26032;&#39062;&#19988;&#32039;&#36843;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21363;&#8220;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#8221;&#12290;&#24378;&#21270;&#28040;&#38500;&#23398;&#20064;&#20391;&#37325;&#20110;&#25764;&#38144;&#25972;&#20010;&#29615;&#22659;&#32780;&#19981;&#26159;&#21333;&#20010;&#25968;&#25454;&#26679;&#26412;&#12290;&#36825;&#19968;&#29420;&#29305;&#29305;&#24449;&#24102;&#26469;&#20102;&#19977;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#25552;&#20986;&#28040;&#38500;&#23398;&#20064;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
&lt;/p&gt;</description></item><item><title>NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2312.14890</link><description>&lt;p&gt;
NPHardEval: &#36890;&#36807;&#22797;&#26434;&#24615;&#31867;&#21035;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21160;&#24577;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14890
&lt;/p&gt;
&lt;p&gt;
NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#23427;&#20063;&#34987;&#29992;&#20110;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65306;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#22312;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#26041;&#38754;&#36824;&#19981;&#22815;&#65292;&#21516;&#26102;&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#19988;&#38745;&#24577;&#30340;&#65292;&#20351;&#24471;&#27169;&#22411;&#26377;&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#25351;&#26631;&#35843;&#25972;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22840;&#22823;&#20854;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;NPHardEval&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.03731</link><description>&lt;p&gt;
&#22810;&#20010;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#22270;&#24418;&#25552;&#31034;&#30340;MultiGPrompt
&lt;/p&gt;
&lt;p&gt;
MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs. (arXiv:2312.03731v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiGPrompt&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22270;&#24418;&#34920;&#31034;&#23398;&#20064;&#20013;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#21487;&#20197;&#22266;&#26377;&#22320;&#23545;Web&#19978;&#30456;&#20114;&#36830;&#25509;&#30340;&#23545;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25903;&#25345;&#19968;&#31995;&#21015;Web&#24212;&#29992;&#65292;&#27604;&#22914;&#32593;&#32476;&#20998;&#26512;&#21644;&#20869;&#23481;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#27969;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22312;&#31471;&#21040;&#31471;&#30417;&#30563;&#26694;&#26550;&#20013;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#19982;&#20219;&#21153;&#29305;&#23450;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#20026;&#20102;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#24182;&#22686;&#24378;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#22522;&#20110;&#33258;&#30417;&#30563;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#32780;&#25552;&#31034;&#21017;&#34987;&#25552;&#20986;&#26469;&#36827;&#19968;&#27493;&#32553;&#23567;&#39044;&#35757;&#32451;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#30446;&#26631;&#24046;&#36317;&#12290;&#34429;&#28982;&#24050;&#32463;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#22270;&#24418;&#23398;&#20064;&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#25506;&#32034;&#65292;&#20294;&#23427;&#20204;&#20027;&#35201;&#21033;&#29992;&#21333;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23548;&#33268;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#21487;&#33021;&#23398;&#20064;&#30340;&#36890;&#29992;&#30693;&#35782;&#30340;&#23376;&#38598;&#21463;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;MultiGPrompt&#65292;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#23545;&#22270;&#24418;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphs can inherently model interconnected objects on the Web, thereby facilitating a series of Web applications, such as web analyzing and content recommendation. Recently, Graph Neural Networks (GNNs) have emerged as a mainstream technique for graph representation learning. However, their efficacy within an end-to-end supervised framework is significantly tied to the availabilityof task-specific labels. To mitigate labeling costs and enhance robustness in few-shot settings, pre-training on self-supervised tasks has emerged as a promising method, while prompting has been proposed to further narrow the objective gap between pretext and downstream tasks. Although there has been some initial exploration of prompt-based learning on graphs, they primarily leverage a single pretext task, resulting in a limited subset of general knowledge that could be learned from the pre-training data. Hence, in this paper, we propose MultiGPrompt, a novel multi-task pre-training and prompting framework to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.13541</link><description>&lt;p&gt;
&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#19982;&#26080;&#20559;&#38598;&#20013;&#21147;
&lt;/p&gt;
&lt;p&gt;
Linear Log-Normal Attention with Unbiased Concentration. (arXiv:2311.13541v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#20998;&#26512;&#20102;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#19982;&#24207;&#21015;&#38271;&#24230;&#30340;&#20108;&#27425;&#20851;&#31995;&#65292;&#20854;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#24403;&#22788;&#29702;&#38271;&#25991;&#26723;&#25110;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#26102;&#65292;&#36825;&#19968;&#38480;&#21046;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#27880;&#24847;&#21147;&#30697;&#38453;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#33021;&#21147;&#65292;&#23545;&#33258;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34913;&#37327;&#36825;&#20123;&#25968;&#37327;&#30340;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#65292;&#26088;&#22312;&#27169;&#25311;&#21407;&#22987;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#24067;&#21644;&#38598;&#20013;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#24120;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#23545;&#25968;&#27491;&#24577;&#27880;&#24847;&#21147;&#20248;&#20110;&#20854;&#20182;&#32447;&#24615;&#21270;&#27880;&#24847;&#21147;&#26367;&#20195;&#26041;&#27861;&#65292;&#20026;&#22686;&#24378;Transformer&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#38468;&#22312;&#34917;&#20805;&#26448;&#26009;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have achieved remarkable results in a wide range of applications. However, their scalability is hampered by the quadratic time and memory complexity of the self-attention mechanism concerning the sequence length. This limitation poses a substantial obstacle when dealing with long documents or high-resolution images. In this work, we study the self-attention mechanism by analyzing the distribution of the attention matrix and its concentration ability. Furthermore, we propose instruments to measure these quantities and introduce a novel self-attention mechanism, Linear Log-Normal Attention, designed to emulate the distribution and concentration behavior of the original self-attention. Our experimental results on popular natural language benchmarks reveal that our proposed Linear Log-Normal Attention outperforms other linearized attention alternatives, offering a promising avenue for enhancing the scalability of transformer models. Our code is available in supplementary
&lt;/p&gt;</description></item><item><title>DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.02017</link><description>&lt;p&gt;
DeliverAI: &#24378;&#21270;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#29992;&#20110;&#39135;&#21697;&#37197;&#36865;
&lt;/p&gt;
&lt;p&gt;
DeliverAI: Reinforcement Learning Based Distributed Path-Sharing Network for Food Deliveries. (arXiv:2311.02017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02017
&lt;/p&gt;
&lt;p&gt;
DeliverAI&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#36335;&#24452;&#20849;&#20139;&#32593;&#32476;&#65292;&#29992;&#20110;&#20248;&#21270;&#39135;&#21697;&#37197;&#36865;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#24182;&#25552;&#39640;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#20174;&#29983;&#20135;&#32773;&#21040;&#28040;&#36153;&#32773;&#30340;&#29289;&#21697;&#37197;&#36865;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#27969;&#34892;&#30149;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#22686;&#38271;&#12290;&#20122;&#39532;&#36874;&#29983;&#40092;&#12289;Shopify&#12289;UberEats&#12289;InstaCart&#21644;DoorDash&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#28040;&#36153;&#21697;&#25110;&#39135;&#21697;&#37197;&#36865;&#19994;&#21153;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#39135;&#21697;&#37197;&#36865;&#26041;&#27861;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#27599;&#27425;&#37197;&#36865;&#37117;&#26159;&#22312;&#26368;&#30701;&#26102;&#38388;&#36335;&#24452;&#19978;&#20174;&#29983;&#20135;&#32773;&#30452;&#25509;&#21040;&#28040;&#36153;&#32773;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#24403;&#21069;&#27169;&#22411;&#19979;&#65292;&#26377;&#24456;&#22823;&#30340;&#20943;&#23569;&#37197;&#36865;&#25104;&#26412;&#30340;&#31354;&#38388;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#39135;&#21697;&#37197;&#36865;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#28040;&#36153;&#32773;&#28385;&#24847;&#24230;&#21644;&#37197;&#36865;&#25104;&#26412;&#37117;&#38656;&#35201;&#36827;&#34892;&#20248;&#21270;&#12290;&#21463;&#20986;&#31199;&#36710;&#34892;&#19994;&#20013;&#25340;&#36710;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeliverAI - &#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36335;&#24452;&#20849;&#20139;&#31639;&#27861;&#12290;&#19982;&#20197;&#21069;&#30340;&#36335;&#24452;&#20849;&#20139;&#23581;&#35797;&#19981;&#21516;&#65292;DeliverAI&#21487;&#20197;&#25552;&#20379;&#23454;&#26102;&#12289;&#26102;&#38388;&#39640;&#25928;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Delivery of items from the producer to the consumer has experienced significant growth over the past decade and has been greatly fueled by the recent pandemic. Amazon Fresh, Shopify, UberEats, InstaCart, and DoorDash are rapidly growing and are sharing the same business model of consumer items or food delivery. Existing food delivery methods are sub-optimal because each delivery is individually optimized to go directly from the producer to the consumer via the shortest time path. We observe a significant scope for reducing the costs associated with completing deliveries under the current model. We model our food delivery problem as a multi-objective optimization, where consumer satisfaction and delivery costs, both, need to be optimized. Taking inspiration from the success of ride-sharing in the taxi industry, we propose DeliverAI - a reinforcement learning-based path-sharing algorithm. Unlike previous attempts for path-sharing, DeliverAI can provide real-time, time-efficient decision-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01047</link><description>&lt;p&gt;
&#36890;&#36807;&#20542;&#26012;&#25351;&#25968;&#23618;&#25913;&#21892;&#31283;&#20581;&#24615;&#65306;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#65292;&#20174;&#32780;&#25552;&#39640;&#28145;&#24230;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#28145;&#24230;&#32593;&#32476;&#31283;&#20581;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#22823;&#22810;&#20381;&#36182;&#20110;&#21512;&#36866;&#30340;&#25968;&#25454;&#22686;&#24378;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#20449;&#29702;&#35770;&#30340;&#20114;&#34917;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#21644;&#25512;&#29702;&#20013;&#30340;&#31070;&#32463;&#31454;&#20105;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#23618;&#36755;&#20986;&#30340;&#20449;&#22122;&#27604;&#12290;&#38500;&#20102;&#26368;&#23567;&#21270;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#20195;&#20215;&#22806;&#65292;&#31070;&#32463;&#20803;&#36890;&#36807;&#26368;&#22823;&#21270;&#20542;&#26012;&#25351;&#25968;&#65288;TEXP&#65289;&#23618;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#31454;&#20105;&#20197;&#31232;&#30095;&#22320;&#34920;&#31034;&#23618;&#36755;&#20837;&#12290;TEXP&#23398;&#20064;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#22312;&#25968;&#25454;&#22122;&#22768;&#30340;&#39640;&#26031;&#27169;&#22411;&#19979;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#21305;&#37197;&#28388;&#27874;&#22120;&#12290;&#22312;TEXP&#23618;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#20542;&#26012;&#30340;softmax&#26367;&#20195;&#25209;&#37327;&#24402;&#19968;&#21270;&#26469;&#36827;&#34892;&#25512;&#29702;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#35745;&#31639;&#27599;&#20010;&#31070;&#32463;&#20803;&#20195;&#34920;&#30340;&#31454;&#20105;&#20449;&#21495;&#20551;&#35774;&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#36890;&#36807;&#31616;&#21270;&#27169;&#22411;&#25552;&#20379;&#27934;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#26631;&#20934;&#22270;&#20687;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COSTAR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26102;&#38388;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#28151;&#28102;&#22240;&#32032;&#26102;&#32467;&#21512;&#20102;&#26102;&#38388;&#21644;&#29305;&#24449;&#20851;&#27880;&#20197;&#21450;&#20998;&#37327;&#23545;&#27604;&#25439;&#22833;&#65292;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.00886</link><description>&lt;p&gt;
COSTAR: &#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#25913;&#36827;&#30340;&#26102;&#38388;&#21453;&#20107;&#23454;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
COSTAR: Improved Temporal Counterfactual Estimation with Self-Supervised Learning. (arXiv:2311.00886v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COSTAR&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#26102;&#38388;&#21453;&#20107;&#23454;&#32467;&#26524;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#26102;&#38388;&#30456;&#20851;&#28151;&#28102;&#22240;&#32032;&#26102;&#32467;&#21512;&#20102;&#26102;&#38388;&#21644;&#29305;&#24449;&#20851;&#27880;&#20197;&#21450;&#20998;&#37327;&#23545;&#27604;&#25439;&#22833;&#65292;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#23545;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#30005;&#23376;&#21830;&#21153;&#65292;&#22312;&#35266;&#23519;&#21040;&#30340;&#21382;&#21490;&#25968;&#25454;&#20013;&#20272;&#35745;&#26102;&#38388;&#21453;&#20107;&#23454;&#32467;&#26524;&#23545;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;(RCTs)&#25104;&#26412;&#39640;&#25110;&#32773;&#19981;&#21487;&#34892;&#30340;&#24773;&#20917;&#19979;&#12290;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#20110;&#22797;&#26434;&#30340;&#21160;&#24577;&#65292;&#38271;&#33539;&#22260;&#20381;&#36182;&#24615;&#20197;&#21450;&#36807;&#21435;&#30340;&#27835;&#30103;&#21644;&#21327;&#21464;&#37327;&#23545;&#26410;&#26469;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#30340;&#28151;&#28102;&#22240;&#32032;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COunterfactual Self-supervised TrAnsformeR&#65288;COSTAR&#65289;&#65292;&#19968;&#31181;&#25972;&#21512;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#25913;&#36827;&#21382;&#21490;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23558;&#26102;&#38388;&#21644;&#29305;&#24449;&#20851;&#27880;&#19982;&#38024;&#23545;&#26102;&#38388;&#22788;&#29702;&#32467;&#26524;&#35266;&#23519;&#30340;&#20998;&#37327;&#23545;&#27604;&#25439;&#22833;&#30456;&#32467;&#21512;&#65292;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#21040;&#20998;&#24067;&#20043;&#22806;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#32467;&#26524;&#39564;&#35777;&#35813;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of temporal counterfactual outcomes from observed history is crucial for decision-making in many domains such as healthcare and e-commerce, particularly when randomized controlled trials (RCTs) suffer from high cost or impracticality. For real-world datasets, modeling time-dependent confounders is challenging due to complex dynamics, long-range dependencies and both past treatments and covariates affecting the future outcomes. In this paper, we introduce COunterfactual Self-supervised TrAnsformeR (COSTAR), a novel approach that integrates self-supervised learning for improved historical representations. The proposed framework combines temporal and feature-wise attention with a component-wise contrastive loss tailored for temporal treatment outcome observations, yielding superior performance in estimation accuracy and generalization to out-of-distribution data compared to existing models, as validated by empirical results on both synthetic and real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#31354;&#38388;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#21464;&#25442;&#65292;&#21487;&#20197;&#23558;&#19981;&#21516;&#31070;&#32463;&#27169;&#22411;&#23398;&#21040;&#30340;&#34920;&#31034;&#32763;&#35793;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25340;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2311.00664</link><description>&lt;p&gt;
&#28508;&#22312;&#31354;&#38388;&#32763;&#35793;&#36890;&#36807;&#35821;&#20041;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Latent Space Translation via Semantic Alignment. (arXiv:2311.00664v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28508;&#22312;&#31354;&#38388;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#36890;&#36807;&#31616;&#21333;&#30340;&#21464;&#25442;&#65292;&#21487;&#20197;&#23558;&#19981;&#21516;&#31070;&#32463;&#27169;&#22411;&#23398;&#21040;&#30340;&#34920;&#31034;&#32763;&#35793;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25340;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#65292;&#24182;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#19981;&#21516;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#25509;&#35302;&#21040;&#35821;&#20041;&#30456;&#20851;&#30340;&#25968;&#25454;&#26102;&#24448;&#24448;&#20250;&#23637;&#29616;&#20986;&#30456;&#20284;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20294;&#36825;&#31181;&#20869;&#22312;&#30340;&#30456;&#20284;&#24615;&#24182;&#19981;&#24635;&#26159;&#31435;&#21363;&#21487;&#36776;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#27604;&#20197;&#21069;&#35748;&#20026;&#30340;&#26356;&#31616;&#21333;&#30340;&#21464;&#25442;&#23558;&#20174;&#36825;&#20123;&#31070;&#32463;&#27169;&#22359;&#23398;&#21040;&#30340;&#34920;&#31034;&#32763;&#35793;&#21040;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#20043;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#20351;&#29992;&#26631;&#20934;&#30340;&#12289;&#36890;&#29992;&#30340;&#20195;&#25968;&#31243;&#24207;&#26469;&#20272;&#35745;&#36825;&#20123;&#21464;&#25442;&#65292;&#24182;&#19988;&#36825;&#20123;&#31243;&#24207;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20272;&#35745;&#20004;&#20010;&#32473;&#23450;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#30340;&#25340;&#25509;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#20013;&#24191;&#27867;&#39564;&#35777;&#20102;&#36825;&#31181;&#32763;&#35793;&#36807;&#31243;&#30340;&#36866;&#24212;&#24615;&#65306;&#21253;&#25324;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#12289;&#39046;&#22495;&#12289;&#26550;&#26500;&#65288;&#22914;ResNet&#12289;CNN&#12289;ViT&#65289;&#20197;&#21450;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#20998;&#31867;&#12289;&#37325;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
While different neural models often exhibit latent spaces that are alike when exposed to semantically related data, this intrinsic similarity is not always immediately discernible. Towards a better understanding of this phenomenon, our work shows how representations learned from these neural modules can be translated between different pre-trained networks via simpler transformations than previously thought. An advantage of this approach is the ability to estimate these transformations using standard, well-understood algebraic procedures that have closed-form solutions. Our method directly estimates a transformation between two given latent spaces, thereby enabling effective stitching of encoders and decoders without additional training. We extensively validate the adaptability of this translation procedure in different experimental settings: across various trainings, domains, architectures (e.g., ResNet, CNN, ViT), and in multiple downstream tasks (classification, reconstruction). Nota
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20107;&#25925;&#21457;&#29983;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;GraphSAGE&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24182;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2311.00164</link><description>&lt;p&gt;
&#36947;&#36335;&#23433;&#20840;&#24314;&#27169;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29992;&#20110;&#20107;&#25925;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis. (arXiv:2311.00164v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20107;&#25925;&#21457;&#29983;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;GraphSAGE&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24182;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#36830;&#25509;&#21644;&#20132;&#36890;&#27969;&#37327;&#30340;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20351;&#29992;&#21382;&#21490;&#35760;&#24405;&#35774;&#35745;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#32570;&#20047;&#20849;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#32570;&#20047;&#20844;&#20849;&#20107;&#25925;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#32479;&#19968;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#32654;&#22269;&#21508;&#24030;&#23448;&#26041;&#25253;&#21578;&#30340;900&#19975;&#26465;&#35760;&#24405;&#65292;&#20197;&#21450;&#36947;&#36335;&#32593;&#32476;&#21644;&#20132;&#36890;&#27969;&#37327;&#25253;&#21578;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20107;&#25925;&#21457;&#29983;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20687;GraphSAGE&#36825;&#26679;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#19981;&#36229;&#36807;&#23454;&#38469;&#25968;&#30446;&#30340;22%&#65292;&#24182;&#33021;&#22815;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.16363</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Actor Critic&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24040;&#22823;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#24456;&#22823;&#30340;&#26102;&#20505;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;actor critic&#21644;natural actor critic&#31639;&#27861;&#26469;&#22788;&#29702;&#28041;&#21450;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;C-MDP&#65289;&#65292;&#24182;&#22312;&#38750; i.i.d&#65288;&#39532;&#23572;&#21487;&#22827;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#34385;&#38271;&#26399;&#24179;&#22343;&#25104;&#26412;&#20934;&#21017;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#26159;&#26576;&#20123;&#35268;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#36866;&#24403;&#31574;&#30053;&#20381;&#36182;&#30340;&#38271;&#26399;&#24179;&#22343;&#12290;&#25105;&#20204;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#27861;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#20445;&#35777;&#33021;&#25214;&#21040;&#24615;&#33021;&#65288;&#25289;&#26684;&#26391;&#26085;&#65289;&#20989;&#25968;$L(\theta,\gamma)$&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;&#21363;$\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$&#65289;&#65292;&#24182;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.5})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of both C
&lt;/p&gt;</description></item><item><title>FedLoRA&#26159;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35757;&#32451;&#20010;&#24615;&#21270;&#19988;&#24322;&#26500;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.13283</link><description>&lt;p&gt;
FedLoRA&#65306;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedLoRA: Model-Heterogeneous Personalized Federated Learning with LoRA Tuning. (arXiv:2310.13283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13283
&lt;/p&gt;
&lt;p&gt;
FedLoRA&#26159;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;&#32852;&#37030;&#23398;&#20064;&#23458;&#25143;&#31471;&#35757;&#32451;&#20010;&#24615;&#21270;&#19988;&#24322;&#26500;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; Paradig&#65292;&#20854;&#20013;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#21327;&#35843;&#22810;&#20010;&#21442;&#19982;&#32773;&#65288;&#21363;FL&#23458;&#25143;&#31471;&#65289;&#22312;&#20998;&#25955;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#36825;&#31181;&#33539; Paradig &#38480;&#21046;&#20102;&#25152;&#26377;&#23458;&#25143;&#31471;&#24517;&#39035;&#20351;&#29992;&#30456;&#21516;&#32467;&#26500;&#30340;&#27169;&#22411;&#65288;&#21516;&#26500;&#65289;&#12290;&#23454;&#36341;&#20013;&#65292;FL&#32463;&#24120;&#38754;&#20020;&#32479;&#35745;&#24322;&#36136;&#24615;&#12289;&#31995;&#32479;&#24322;&#36136;&#24615;&#21644;&#27169;&#22411;&#24322;&#36136;&#24615;&#31561;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#28608;&#21457;&#20102;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;MHPFL&#65289;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#26088;&#22312;&#20026;&#27599;&#20010;FL&#23458;&#25143;&#31471;&#35757;&#32451;&#19968;&#20010;&#20010;&#24615;&#21270;&#19988;&#24322;&#26500;&#30340;&#26412;&#22320;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;MHPFL&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#25509;&#21463;&#30340;&#35745;&#31639;&#24320;&#38144;&#21644;&#39640;&#25928;&#30340;&#36890;&#20449;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LoRA&#35843;&#25972;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#27169;&#22411;&#24322;&#26500;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;FedLoRA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is an emerging machine learning paradigm in which a central server coordinates multiple participants (a.k.a. FL clients) to train a model collaboratively on decentralized data with privacy protection. This paradigm constrains that all clients have to train models with the same structures (homogeneous). In practice, FL often faces statistical heterogeneity, system heterogeneity and model heterogeneity challenges. These challenging issues inspire the field of Model-Heterogeneous Personalized Federated Learning (MHPFL) which aims to train a personalized and heterogeneous local model for each FL client. Existing MHPFL approaches cannot achieve satisfactory model performance, acceptable computational overhead and efficient communication simultaneously. To bridge this gap, we propose a novel computation- and communication-efficient model-heterogeneous personalized Federated learning framework based on LoRA tuning (FedLoRA). It is designed to incorporate a homogeneous 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12609</link><description>&lt;p&gt;
&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32477;&#32536;&#20307;&#25913;&#21892;&#28909;&#21147;&#25193;&#25955;&#36827;&#34892;&#26080;&#30896;&#25758;&#36816;&#21160;&#35268;&#21010;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#65292;&#20855;&#26377;&#31283;&#20581;&#24615;&#21644;&#22810;&#27169;&#24577;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#28789;&#27963;&#24615;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;&#23613;&#31649;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#22797;&#26434;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20005;&#37325;&#20381;&#36182;&#20110;&#25512;&#29702;&#26102;&#30340;&#38556;&#30861;&#29289;&#26816;&#27979;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#35774;&#22791;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25512;&#29702;&#26102;&#33021;&#22815;&#20174;&#21333;&#19968;&#30340;&#35270;&#35273;&#36755;&#20837;&#20013;&#21516;&#26102;&#29983;&#25104;&#21487;&#36798;&#30446;&#26631;&#24182;&#35268;&#21010;&#36991;&#24320;&#38556;&#30861;&#29289;&#30340;&#36816;&#21160;&#36335;&#24452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26032;&#39062;&#30340;&#30896;&#25758;&#36991;&#20813;&#25193;&#25955;&#26680;&#36827;&#34892;&#20351;&#29992;&#12290;&#36890;&#36807;&#19982;&#34892;&#20026;&#20811;&#38534;&#21644;&#32463;&#20856;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#35777;&#26126;&#20102;&#20854;&#31283;&#20581;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#65292;&#23427;&#33021;&#22815;&#23548;&#33322;&#21040;&#30446;&#26631;&#24182;&#36991;&#24320;&#34987;&#38556;&#30861;&#29289;&#38459;&#25377;&#30340;&#19981;&#21487;&#36798;&#30446;&#26631;&#65292;&#21516;&#26102;&#30830;&#20445;&#36991;&#20813;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have risen as a powerful tool in robotics due to their flexibility and multi-modality. While some of these methods effectively address complex problems, they often depend heavily on inference-time obstacle detection and require additional equipment. Addressing these challenges, we present a method that, during inference time, simultaneously generates only reachable goals and plans motions that avoid obstacles, all from a single visual input. Central to our approach is the novel use of a collision-avoiding diffusion kernel for training. Through evaluations against behavior-cloning and classical diffusion models, our framework has proven its robustness. It is particularly effective in multi-modal environments, navigating toward goals and avoiding unreachable ones blocked by obstacles, while ensuring collision avoidance.
&lt;/p&gt;</description></item><item><title>ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2310.09298</link><description>&lt;p&gt;
ByteStack-ID: &#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#21033;&#29992;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID: Integrated Stacked Model Leveraging Payload Byte Frequency for Grayscale Image-based Network Intrusion Detection. (arXiv:2310.09298v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09298
&lt;/p&gt;
&lt;p&gt;
ByteStack-ID&#26159;&#19968;&#31181;&#22522;&#20110;&#28784;&#24230;&#22270;&#20687;&#21644;&#36127;&#36733;&#23383;&#33410;&#39057;&#29575;&#30340;&#38598;&#25104;&#22534;&#21472;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#12290;&#23427;&#33021;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#65292;&#24182;&#19982;&#20256;&#32479;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#65292;&#36805;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#32593;&#32476;&#27969;&#37327;&#20013;&#30340;&#21508;&#31181;&#25915;&#20987;&#31867;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;"ByteStack-ID"&#65292;&#19968;&#31181;&#19987;&#20026;&#25968;&#25454;&#21253;&#32423;&#20837;&#20405;&#26816;&#27979;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;ByteStack-ID&#26680;&#24515;&#26159;&#21033;&#29992;&#20174;&#36127;&#36733;&#25968;&#25454;&#30340;&#39057;&#29575;&#20998;&#24067;&#29983;&#25104;&#30340;&#28784;&#24230;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#25216;&#26415;&#65292;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#35782;&#21035;&#22797;&#26434;&#25968;&#25454;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23436;&#20840;&#22522;&#20110;&#25968;&#25454;&#21253;&#32423;&#20449;&#24687;&#65292;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#27969;&#37327;&#25968;&#25454;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;NIDS&#65289;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#22522;&#26412;&#22534;&#21472;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;ByteStack-ID&#19982;&#20256;&#32479;&#30340;&#22534;&#21472;&#26041;&#27861;&#19981;&#21516;&#12290;&#23427;&#23558;&#38468;&#21152;&#30340;&#20803;&#23398;&#20064;&#22120;&#23618;&#26080;&#32541;&#38598;&#25104;&#21040;&#36830;&#25509;&#30340;&#22522;&#30784;&#23398;&#20064;&#22120;&#20013;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#20248;&#21270;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the ever-evolving realm of network security, the swift and accurate identification of diverse attack classes within network traffic is of paramount importance. This paper introduces "ByteStack-ID," a pioneering approach tailored for packet-level intrusion detection. At its core, ByteStack-ID leverages grayscale images generated from the frequency distributions of payload data, a groundbreaking technique that greatly enhances the model's ability to discern intricate data patterns. Notably, our approach is exclusively grounded in packet-level information, a departure from conventional Network Intrusion Detection Systems (NIDS) that predominantly rely on flow-based data. While building upon the fundamental concept of stacking methodology, ByteStack-ID diverges from traditional stacking approaches. It seamlessly integrates additional meta learner layers into the concatenated base learners, creating a highly optimized, unified model. Empirical results unequivocally confirm the outstandin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08091</link><description>&lt;p&gt;
&#20998;&#36776;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;&#26469;&#20998;&#37197;&#36164;&#28304;&#65292;&#20197;&#25913;&#21892;&#29366;&#24577;&#20043;&#38388;&#30340;&#36731;&#37325;&#20998;&#37197;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;TD&#23398;&#20064;&#20013;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#37325;&#35201;&#24615;&#21644;TD&#35823;&#24046;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#25913;&#36827;&#20102;&#20540;&#20272;&#35745;&#65292;&#36824;&#21152;&#36895;&#20102;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;(TD)&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26088;&#22312;&#39640;&#25928;&#35780;&#20272;&#31574;&#30053;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;TD($\lambda$)&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#35760;&#24518;&#36712;&#36857;&#23558;&#39044;&#27979;&#35823;&#24046;&#20998;&#25955;&#21040;&#21382;&#21490;&#19978;&#19979;&#25991;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32463;&#24120;&#24573;&#35270;&#21382;&#21490;&#29366;&#24577;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#20256;&#25773;TD&#35823;&#24046;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#36825;&#21463;&#21040;&#35775;&#38382;&#22833;&#34913;&#25110;&#32467;&#26524;&#22122;&#22768;&#31561;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#36776;TD&#23398;&#20064;(DTD)&#30340;&#26032;&#22411;TD&#31639;&#27861;&#65292;&#23427;&#20801;&#35768;&#28789;&#27963;&#30340;&#24378;&#35843;&#20989;&#25968;-&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#39044;&#20808;&#30830;&#23450;&#25110;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#36164;&#28304;&#20197;&#25552;&#39640;&#29366;&#24577;&#20043;&#38388;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#24378;&#35843;&#20989;&#25968;&#20869;&#24314;&#31435;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#28145;&#24230;RL&#29615;&#22659;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21512;&#29702;&#30340;&#24378;&#35843;&#20989;&#25968;&#19981;&#20165;&#21487;&#20197;&#25913;&#36827;&#20540;&#20272;&#35745;&#65292;&#36824;&#21487;&#20197;&#21152;&#36895;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal difference learning (TD) is a foundational concept in reinforcement learning (RL), aimed at efficiently assessing a policy's value function. TD($\lambda$), a potent variant, incorporates a memory trace to distribute the prediction error into the historical context. However, this approach often neglects the significance of historical states and the relative importance of propagating the TD error, influenced by challenges such as visitation imbalance or outcome noise. To address this, we propose a novel TD algorithm named discerning TD learning (DTD), which allows flexible emphasis functions$-$predetermined or adapted during training$-$to allocate efforts effectively across states. We establish the convergence properties of our method within a specific class of emphasis functions and showcase its promising potential for adaptation to deep RL contexts. Empirical results underscore that employing a judicious emphasis function not only improves value estimation but also expedites l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#20013;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#23545;&#25968;&#20284;&#28982;&#24230;&#35780;&#20272;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.06085</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#35757;&#32451;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Quantile-based Maximum Likelihood Training for Outlier Detection. (arXiv:2310.06085v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#25913;&#36827;&#24322;&#24120;&#26816;&#27979;&#20013;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#23545;&#25968;&#20284;&#28982;&#24230;&#35780;&#20272;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#21035;&#24615;&#23398;&#20064;&#26377;&#25928;&#22320;&#23545;&#22270;&#20687;&#20998;&#31867;&#39044;&#27979;&#30495;&#23454;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#24120;&#20540;&#26041;&#38754;&#65292;&#23427;&#32463;&#24120;&#23548;&#33268;&#35823;&#25253;&#38451;&#24615;&#65292;&#36825;&#22312;&#33258;&#20027;&#39550;&#39542;&#21644;&#35270;&#39057;&#30417;&#35270;&#31995;&#32479;&#31561;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20197;&#24448;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#23581;&#35797;&#21253;&#25324;&#20351;&#29992;&#23454;&#38469;&#24322;&#24120;&#20540;&#25968;&#25454;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#25110;&#32773;&#36890;&#36807;&#21512;&#25104;&#24322;&#24120;&#20540;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#20687;&#32032;&#31354;&#38388;&#20013;&#23545;&#20869;&#28857;&#36827;&#34892;&#26080;&#30417;&#30563;&#29983;&#25104;&#24314;&#27169;&#23545;&#20110;&#24322;&#24120;&#26816;&#27979;&#26174;&#31034;&#20986;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#26497;&#22823;&#20284;&#28982;&#30446;&#26631;&#65292;&#29992;&#20110;&#23398;&#20064;&#20869;&#28857;&#20998;&#24067;&#65292;&#20197;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#25552;&#39640;&#24322;&#24120;&#20540;&#30340;&#20998;&#31163;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#27491;&#21017;&#21270;&#27969;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#30340;&#21028;&#21035;&#24615;&#29305;&#24449;&#65292;&#24182;&#26681;&#25454;&#35780;&#20272;&#30340;&#23545;&#25968;&#20284;&#28982;&#24230;&#26469;&#26816;&#27979;&#24322;&#24120;&#20540;&#12290;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discriminative learning effectively predicts true object class for image classification. However, it often results in false positives for outliers, posing critical concerns in applications like autonomous driving and video surveillance systems. Previous attempts to address this challenge involved training image classifiers through contrastive learning using actual outlier data or synthesizing outliers for self-supervised learning. Furthermore, unsupervised generative modeling of inliers in pixel space has shown limited success for outlier detection. In this work, we introduce a quantile-based maximum likelihood objective for learning the inlier distribution to improve the outlier separation during inference. Our approach fits a normalizing flow to pre-trained discriminative features and detects the outliers according to the evaluated log-likelihood. The experimental evaluation demonstrates the effectiveness of our method as it surpasses the performance of the state-of-the-art unsupervi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GNNSync&#30340;&#22522;&#20110;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#35282;&#24230;&#21516;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#32534;&#30721;&#21516;&#27493;&#32422;&#26463;&#12290;</title><link>http://arxiv.org/abs/2310.05842</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#30340;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Robust Angular Synchronization via Directed Graph Neural Networks. (arXiv:2310.05842v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GNNSync&#30340;&#22522;&#20110;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#35282;&#24230;&#21516;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#32534;&#30721;&#21516;&#27493;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#26088;&#22312;&#36890;&#36807;$m$&#20010;&#20559;&#31227;&#37327;$\theta_i-\theta_j \;\mbox{mod} \; 2\pi$&#30340;&#22122;&#22768;&#27979;&#37327;&#20934;&#30830;&#20272;&#35745;&#65288;&#26368;&#22810;&#19968;&#20010;&#24120;&#25968;&#30456;&#20301;&#20559;&#31227;&#65289;&#19968;&#32452;&#26410;&#30693;&#35282;&#24230;$\theta_1, \dots, \theta_n\in[0, 2\pi)$. &#24212;&#29992;&#21253;&#25324;&#20256;&#24863;&#22120;&#32593;&#32476;&#23450;&#20301;&#12289;&#30456;&#20301;&#24674;&#22797;&#21644;&#20998;&#24067;&#24335;&#26102;&#38047;&#21516;&#27493;&#12290;&#35813;&#38382;&#39064;&#30340;&#24322;&#26500;&#25193;&#23637;&#65288;&#31216;&#20026;$k$-&#21516;&#27493;&#65289;&#26159;&#21516;&#26102;&#20272;&#35745;$k$&#32452;&#35282;&#24230;&#65292;&#32473;&#23450;&#27599;&#20010;&#32452;&#30340;&#26410;&#30693;&#32452;&#20998;&#37197;&#30340;&#22122;&#22768;&#35266;&#23519;&#20540;&#12290;&#29616;&#26377;&#30340;&#35282;&#24230;&#21516;&#27493;&#26041;&#27861;&#22312;&#39640;&#22122;&#22768;&#29615;&#22659;&#19979;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#36825;&#22312;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#35282;&#24230;&#21516;&#27493;&#38382;&#39064;&#21450;&#20854;&#24322;&#26500;&#25193;&#23637;&#65292;&#25552;&#20986;&#20102;GNNSync&#65292;&#36825;&#26159;&#19968;&#20010;&#29702;&#35770;&#25903;&#25745;&#30340;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#26377;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32534;&#30721;&#35282;&#24230;&#21516;&#27493;&#30340;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
The angular synchronization problem aims to accurately estimate (up to a constant additive phase) a set of unknown angles $\theta_1, \dots, \theta_n\in[0, 2\pi)$ from $m$ noisy measurements of their offsets $\theta_i-\theta_j \;\mbox{mod} \; 2\pi.$ Applications include, for example, sensor network localization, phase retrieval, and distributed clock synchronization. An extension of the problem to the heterogeneous setting (dubbed $k$-synchronization) is to estimate $k$ groups of angles simultaneously, given noisy observations (with unknown group assignment) from each group. Existing methods for angular synchronization usually perform poorly in high-noise regimes, which are common in applications. In this paper, we leverage neural networks for the angular synchronization problem, and its heterogeneous extension, by proposing GNNSync, a theoretically-grounded end-to-end trainable framework using directed graph neural networks. In addition, new loss functions are devised to encode synchro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24341;&#20837;&#32852;&#31995;&#22270;&#20449;&#24687;&#65292;&#21487;&#20197;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04017</link><description>&lt;p&gt;
PGraphDTA: &#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps. (arXiv:2310.04017v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21644;&#25509;&#35302;&#22270;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#24341;&#20837;&#32852;&#31995;&#22270;&#20449;&#24687;&#65292;&#21487;&#20197;&#25913;&#36827;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#21644;&#24320;&#21457;&#26032;&#33647;&#26159;&#19968;&#39033;&#22797;&#26434;&#19988;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#24037;&#20316;&#65292;&#36890;&#24120;&#28041;&#21450;&#22823;&#37327;&#25104;&#26412;&#12289;&#26102;&#38388;&#25237;&#20837;&#21644;&#23433;&#20840;&#38382;&#39064;&#12290;&#33647;&#29289;&#21457;&#29616;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#30830;&#23450;&#26032;&#39062;&#30340;&#33647;&#29289;-&#38774;&#26631;&#65288;DT&#65289;&#30456;&#20114;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#39044;&#27979;DT&#30456;&#20114;&#20316;&#29992;&#30340;&#35745;&#31639;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#26088;&#22312;&#30830;&#23450;DT&#23545;&#26159;&#21542;&#23384;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#34920;&#29616;&#20986;&#19968;&#31995;&#21015;&#19981;&#21516;&#32467;&#21512;&#24378;&#24230;&#65292;&#21363;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#36825;&#23545;&#20934;&#30830;&#39044;&#27979;&#36896;&#25104;&#20102;&#25345;&#32493;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33647;&#29289;&#38774;&#26631;&#30456;&#20114;&#20316;&#29992;&#65288;DTI&#65289;&#39044;&#27979;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#36827;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#25972;&#21512;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#23558;&#25509;&#35302;&#22270;&#20449;&#24687;&#20316;&#20026;&#29616;&#26377;&#27169;&#22411;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outper
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.03946</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#27169;&#22411;&#25913;&#36827;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improved prediction of ligand-protein binding affinities by meta-modeling. (arXiv:2310.03946v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#20934;&#30830;&#31579;&#36873;&#20505;&#36873;&#33647;&#29289;&#37197;&#20307;&#19982;&#38774;&#34507;&#30333;&#30340;&#32467;&#21512;&#26159;&#33647;&#29289;&#24320;&#21457;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#31579;&#36873;&#28508;&#22312;&#20505;&#36873;&#29289;&#33021;&#22815;&#33410;&#30465;&#25214;&#33647;&#29289;&#30340;&#26102;&#38388;&#21644;&#36153;&#29992;&#12290;&#36825;&#31181;&#34394;&#25311;&#31579;&#36873;&#37096;&#20998;&#20381;&#36182;&#20110;&#39044;&#27979;&#37197;&#20307;&#21644;&#34507;&#30333;&#36136;&#20043;&#38388;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#35745;&#31639;&#27169;&#22411;&#23545;&#19981;&#21516;&#30446;&#26631;&#30340;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#32467;&#26524;&#19981;&#21516;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#24050;&#21457;&#34920;&#30340;&#22522;&#20110;&#32467;&#26500;&#30340;&#23545;&#25509;&#21644;&#22522;&#20110;&#24207;&#21015;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36825;&#20010;&#26694;&#26550;&#26102;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#32452;&#21512;&#30340;&#20010;&#21035;&#27169;&#22411;&#12289;&#35757;&#32451;&#25968;&#25454;&#24211;&#20197;&#21450;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#30340;&#20803;&#27169;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#26174;&#31034;&#20986;&#35768;&#22810;&#20803;&#27169;&#22411;&#22312;&#20146;&#21644;&#21147;&#39044;&#27979;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#20010;&#21035;&#22522;&#30784;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#20803;&#27169;&#22411;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#32431;&#32467;&#26500;&#20026;&#22522;&#30784;&#30340;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#20803;&#27169;&#22411;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#37197;&#20307;-&#34507;&#30333;&#36136;&#32467;&#21512;&#20146;&#21644;&#21147;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate screening of candidate drug ligands against target proteins through computational approaches is of prime interest to drug development efforts, as filtering potential candidates would save time and expenses for finding drugs. Such virtual screening depends in part on methods to predict the binding affinity between ligands and proteins. Given many computational models for binding affinity prediction with varying results across targets, we herein develop a meta-modeling framework by integrating published empirical structure-based docking and sequence-based deep learning models. In building this framework, we evaluate many combinations of individual models, training databases, and linear and nonlinear meta-modeling approaches. We show that many of our meta-models significantly improve affinity predictions over individual base models. Our best meta-models achieve comparable performance to state-of-the-art exclusively structure-based deep learning tools. Overall, we demonstrate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2310.02975</link><description>&lt;p&gt;
&#22312;&#37325;&#23614;&#27874;&#27573;&#30340;&#23436;&#20840;&#33258;&#36866;&#24212;&#36951;&#25022;&#26368;&#23567;&#21270;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Fully Adaptive Regret Minimization in Heavy-Tailed Bandits. (arXiv:2310.02975v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#23436;&#20840;&#33258;&#36866;&#24212;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#25552;&#20986;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#36866;&#24212;&#24615;&#31639;&#27861;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#20250;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#23614;&#20998;&#24067;&#22312;&#37329;&#34701;&#21040;&#30005;&#20449;&#31561;&#22810;&#31181;&#29615;&#22659;&#20013;&#33258;&#28982;&#32780;&#28982;&#22320;&#20986;&#29616;&#12290;&#34429;&#28982;&#22312;&#27425;&#39640;&#26031;&#25110;&#26377;&#30028;&#25903;&#25745;&#22870;&#21169;&#19979;&#30340;&#36951;&#25022;&#26368;&#23567;&#21270;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#37325;&#23614;&#20998;&#24067;&#19978;&#30340;&#23398;&#20064;&#21482;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#38543;&#26426;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#20013;&#65292;&#19968;&#20010;&#20195;&#29702;&#22312;&#20551;&#35774;&#20998;&#24067;&#26377;&#26377;&#30028;&#26368;&#22823;&#38454;&#30340;&#26377;&#38480;&#30697;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#36825;&#20123;&#30697;&#34987;&#24120;&#25968;u&#19968;&#33268;&#26377;&#30028;&#65292;&#23545;&#20110;&#26576;&#20010;&#949;&#8712;(0,1]&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25991;&#29486;&#20013;&#21482;&#25552;&#20379;&#38656;&#35201;&#36825;&#20004;&#20010;&#37327;&#20316;&#20026;&#36755;&#20837;&#30340;&#31639;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#33258;&#36866;&#24212;&#37325;&#23614;&#27874;&#27573;&#38382;&#39064;&#65292;&#36825;&#26159;&#26631;&#20934;&#35774;&#32622;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#20195;&#29702;&#23545;&#949;&#21644;u&#22343;&#19981;&#30693;&#26195;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26159;&#23384;&#22312;&#20195;&#20215;&#30340;&#65292;&#24182;&#24341;&#20837;&#23545;&#20110;&#20219;&#20309;&#33258;&#36866;&#24212;&#31639;&#27861;&#36951;&#25022;&#30340;&#20004;&#20010;&#19979;&#30028;&#65292;&#24847;&#21619;&#30528;&#30456;&#23545;&#20110;&#26631;&#20934;&#35774;&#32622;&#26377;&#26356;&#39640;&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#29305;&#23450;&#30340;&#20998;&#24067;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heavy-tailed distributions naturally arise in many settings, from finance to telecommunications. While regret minimization under sub-Gaussian or bounded support rewards has been widely studied, learning on heavy-tailed distributions only gained popularity over the last decade. In the stochastic heavy-tailed bandit problem, an agent learns under the assumption that the distributions have finite moments of maximum order $1+\epsilon$ which are uniformly bounded by a constant $u$, for some $\epsilon \in (0,1]$. To the best of our knowledge, literature only provides algorithms requiring these two quantities as an input. In this paper, we study the stochastic adaptive heavy-tailed bandit, a variation of the standard setting where both $\epsilon$ and $u$ are unknown to the agent. We show that adaptivity comes at a cost, introducing two lower bounds on the regret of any adaptive algorithm, implying a higher regret w.r.t. the standard setting. Finally, we introduce a specific distributional ass
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01845</link><description>&lt;p&gt;
&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Refinement of Buildings' Segmentation Models using SAM. (arXiv:2310.01845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;SAM&#36827;&#34892;&#24314;&#31569;&#29289;&#20998;&#21106;&#27169;&#22411;&#30340;&#38646;-shot&#32454;&#21270;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#36965;&#24863;&#22270;&#20687;&#24212;&#29992;&#20013;SAM&#24615;&#33021;&#19981;&#20339;&#12289;&#26080;&#27861;&#36827;&#34892;&#35782;&#21035;&#30340;&#38382;&#39064;&#36827;&#34892;&#20102;&#22788;&#29702;&#12290;&#36890;&#36807;&#24341;&#20837;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#24120;&#35268;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#22914;&#36965;&#24863;&#22270;&#20687;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#39046;&#22495;&#12290;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#31934;&#30830;&#30340;&#24314;&#31569;&#29289;&#23454;&#20363;&#20998;&#21106;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21487;&#33021;&#21463;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;&#22522;&#30784;&#27169;&#22411;&#36866;&#24212;&#24050;&#26377;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#20247;&#22810;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;Segment Anything Model&#65288;SAM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#20854;&#25797;&#38271;&#26080;&#31867;&#21035;&#22270;&#20687;&#20998;&#21106;&#33021;&#21147;&#32780;&#38395;&#21517;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;SAM&#30340;&#23616;&#38480;&#24615;&#65292;&#25581;&#31034;&#20102;&#23427;&#22312;&#24212;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#26102;&#24615;&#33021;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;SAM&#19981;&#20855;&#22791;&#35782;&#21035;&#33021;&#21147;&#65292;&#22240;&#27492;&#26080;&#27861;&#23545;&#23450;&#20301;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#31867;&#21644;&#26631;&#35760;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Foundation models have excelled in various tasks but are often evaluated on general benchmarks. The adaptation of these models for specific domains, such as remote sensing imagery, remains an underexplored area. In remote sensing, precise building instance segmentation is vital for applications like urban planning. While Convolutional Neural Networks (CNNs) perform well, their generalization can be limited. For this aim, we present a novel approach to adapt foundation models to address existing models' generalization dropback. Among several models, our focus centers on the Segment Anything Model (SAM), a potent foundation model renowned for its prowess in class-agnostic image segmentation capabilities. We start by identifying the limitations of SAM, revealing its suboptimal performance when applied to remote sensing imagery. Moreover, SAM does not offer recognition abilities and thus fails to classify and tag localized objects. To address these limitations, we introduce different promp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.00290</link><description>&lt;p&gt;
&#23436;&#32654;&#39044;&#27979;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Mathematical structure of perfect predictive reservoir computing for autoregressive type of time series data. (arXiv:2310.00290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00290
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20648;&#22791;&#35745;&#31639;&#22312;&#33258;&#22238;&#24402;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#32467;&#26500;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#38544;&#34255;&#30340;&#26435;&#37325;&#30697;&#38453;&#32467;&#26500;&#65292;&#20197;&#23454;&#29616;&#23545;AR&#31867;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20648;&#22791;&#35745;&#31639;&#65288;RC&#65289;&#26159;&#19968;&#31181;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#27627;&#26080;&#30097;&#38382;&#65292;RC&#23558;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#29992;&#20110;&#26500;&#24314;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#26469;&#39044;&#27979;&#27169;&#22411;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#12289;&#39640;&#36895;&#24230;&#21644;&#39640;&#35745;&#31639;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#30740;&#31350;&#30452;&#21040;&#26368;&#36817;&#25165;&#24320;&#22987;&#12290;Bollt&#65288;2021&#65289;&#38416;&#26126;&#20102;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;RC&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#23398;&#32467;&#26500;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;Wold&#20998;&#35299;&#23450;&#29702;&#26159;&#29702;&#35299;&#36825;&#20123;&#32467;&#26500;&#30340;&#37324;&#31243;&#30865;&#12290;&#22312;&#38125;&#35760;&#36825;&#19968;&#33879;&#21517;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#38416;&#26126;&#20102;RC&#31070;&#32463;&#32593;&#32476;&#20013;&#36755;&#20837;&#21644;&#24490;&#29615;&#26435;&#37325;&#30697;&#38453;&#30340;&#38544;&#34255;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#32467;&#26500;&#23545;&#20110;AR&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#23454;&#29616;&#20102;&#23436;&#32654;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reservoir Computing (RC) is a type of recursive neural network (RNN), and there can be no doubt that the RC will be more and more widely used for building future prediction models for time-series data, with low training cost, high speed and high computational power. However, research into the mathematical structure of RC neural networks has only recently begun. Bollt (2021) clarified the necessity of the autoregressive (AR) model for gaining the insight into the mathematical structure of RC neural networks, and indicated that the Wold decomposition theorem is the milestone for understanding of these. Keeping this celebrated result in mind, in this paper, we clarify hidden structures of input and recurrent weight matrices in RC neural networks, and show that such structures attain perfect prediction for the AR type of time series data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.17348</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#29983;&#29289;&#21512;&#29702;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;&#21453;&#21521;&#20256;&#25773;&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#25191;&#34892;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;ANNs&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#25915;&#20987;&#36890;&#36807;&#24494;&#23567;&#30340;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#26469;&#25913;&#21464;&#36755;&#20837;&#65292;&#20174;&#32780;&#20005;&#37325;&#30772;&#22351;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20351;ANNs&#23545;&#36825;&#20123;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#23545;&#25239;&#35757;&#32451;&#65292;&#20854;&#20013;&#35757;&#32451;&#25968;&#25454;&#38598;&#34987;&#28155;&#21152;&#20102;&#26679;&#26412;&#29992;&#20110;&#23545;&#25239;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#32570;&#28857;&#26159;&#22686;&#21152;&#20102;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#22240;&#20026;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#26159;&#38750;&#24120;&#35745;&#31639;&#28040;&#32791;&#39640;&#30340;&#12290;&#19982;ANNs&#19981;&#21516;&#65292;&#20154;&#31867;&#19981;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#29289;&#21512;&#29702;&#30340;&#23398;&#20064;&#31639;&#27861;&#26159;&#21542;&#27604;BP&#26356;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;BP&#21644;&#8220;Error to Pertu"&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27604;&#36739;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#23454;&#39564;&#24182;&#25910;&#38598;&#20102;52&#21517;&#21442;&#19982;&#32773;&#30340;&#20840;&#38754;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#29366;&#24577;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#21644;&#30561;&#30496;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#20010;&#20307;&#19978;&#23545;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16457</link><description>&lt;p&gt;
&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65306;&#23558;&#35273;&#37266;&#21644;&#30561;&#30496;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#20110;&#19981;&#21516;&#20010;&#20307;&#38388;
&lt;/p&gt;
&lt;p&gt;
Universal Sleep Decoder: Aligning awake and sleep neural representation across subjects. (arXiv:2309.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#23454;&#39564;&#24182;&#25910;&#38598;&#20102;52&#21517;&#21442;&#19982;&#32773;&#30340;&#20840;&#38754;&#33041;&#30005;&#22270;&#25968;&#25454;&#38598;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#29366;&#24577;&#19979;&#31070;&#32463;&#34920;&#31034;&#30340;&#24046;&#24322;&#38382;&#39064;&#12290;&#30740;&#31350;&#22242;&#38431;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#21644;&#30561;&#30496;&#30340;&#31070;&#32463;&#27169;&#24335;&#65292;&#24182;&#21462;&#24471;&#20102;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30456;&#24403;&#30340;&#20934;&#30830;&#29575;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22312;&#27979;&#35797;&#20010;&#20307;&#19978;&#23545;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33041;&#27963;&#21160;&#35299;&#30721;&#30561;&#30496;&#20013;&#30340;&#35760;&#24518;&#20869;&#23481;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#30446;&#26631;&#12290;&#34429;&#28982;&#24050;&#30693;&#21870;&#40831;&#31867;&#21160;&#29289;&#22312;&#30561;&#30496;&#20013;&#33258;&#21457;&#22320;&#37325;&#26032;&#28608;&#27963;&#35760;&#24518;&#20197;&#25903;&#25345;&#35760;&#24518;&#24041;&#22266;&#21644;&#31163;&#32447;&#23398;&#20064;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#32463;&#36807;&#23436;&#25972;&#27880;&#37322;&#30340;&#30561;&#30496;&#25968;&#25454;&#38598;&#20197;&#21450;&#28165;&#37266;&#29366;&#24577;&#21644;&#30561;&#30496;&#29366;&#24577;&#20043;&#38388;&#31070;&#32463;&#27169;&#24335;&#30340;&#24040;&#22823;&#24046;&#24322;&#65292;&#25429;&#25417;&#20154;&#31867;&#30340;&#35760;&#24518;&#20877;&#29616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#26032;&#39062;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#23454;&#39564;&#65292;&#24182;&#20174;52&#21517;&#21442;&#19982;&#32773;&#25910;&#38598;&#20102;&#19968;&#20221;&#20840;&#38754;&#12289;&#23436;&#25972;&#27880;&#37322;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#35273;&#37266;&#21644;&#30561;&#30496;&#20004;&#31181;&#29366;&#24577;&#12290;&#21033;&#29992;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#36890;&#29992;&#30561;&#30496;&#35299;&#30721;&#22120;&#65288;USD&#65289;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#20010;&#20307;&#38388;&#23545;&#40784;&#35273;&#37266;&#19982;&#30561;&#30496;&#30340;&#31070;&#32463;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#20010;&#20307;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;16.6%&#30340;top-1&#38646;&#26679;&#26412;&#20934;&#30830;&#29575;&#65292;&#19982;&#20351;&#29992;&#20010;&#21035;&#30561;&#30496;&#25968;&#25454;&#36827;&#34892;&#35299;&#30721;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#23545;&#27979;&#35797;&#20010;&#20307;&#30340;USD&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding memory content from brain activity during sleep has long been a goal in neuroscience. While spontaneous reactivation of memories during sleep in rodents is known to support memory consolidation and offline learning, capturing memory replay in humans is challenging due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 52 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed the Universal Sleep Decoder (USD) to align neural representations between wakefulness and sleep across subjects. Our model achieves up to 16.6% top-1 zero-shot accuracy on unseen subjects, comparable to decoding performances using individual sleep data. Furthermore, fine-tuning USD on test subjects enhances decoding accuracy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.08630</link><description>&lt;p&gt;
PCN&#65306;&#19968;&#31181;&#21033;&#29992;&#26032;&#39062;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#21644;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#21943;&#27880;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;PCN&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#21033;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#21943;&#27880;&#26631;&#35760;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21943;&#27880;&#26631;&#35760;&#26159;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#20013;&#30340;&#19968;&#20010;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#35782;&#21035;&#31890;&#23376;&#30896;&#25758;&#20135;&#29983;&#30340;&#38181;&#29366;&#21943;&#27880;&#65292;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#21457;&#23556;&#31890;&#23376;&#12290;&#21943;&#27880;&#26631;&#35760;&#30340;&#36827;&#23637;&#20026;&#36229;&#20986;&#26631;&#20934;&#27169;&#22411;&#30340;&#26032;&#29289;&#29702;&#25628;&#32034;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#22797;&#26434;&#30896;&#25758;&#25968;&#25454;&#20013;&#23547;&#25214;&#38544;&#34255;&#30340;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#23558;&#21943;&#27880;&#34920;&#31034;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#30340;&#26041;&#27861;&#22810;&#31181;&#22810;&#26679;&#65292;&#24182;&#19988;&#36890;&#24120;&#20250;&#21521;&#27169;&#22411;&#38544;&#34255;&#26377;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#21943;&#27880;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#23613;&#21487;&#33021;&#22320;&#32534;&#30721;&#26368;&#22810;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#34920;&#31034;&#20013;&#26368;&#22909;&#22320;&#23398;&#20064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;Particle Chebyshev Network&#65288;PCN&#65289;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#24182;&#20351;&#29992;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#65288;ChebConv&#65289;&#12290;ChebConv&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;GNN&#20013;&#30340;&#19968;&#31181;&#26377;&#25928;&#26367;&#20195;&#20256;&#32479;&#22270;&#21367;&#31215;&#30340;&#26041;&#27861;&#65292;&#32780;&#22312;&#21943;&#27880;&#26631;&#35760;&#20013;&#36824;&#27809;&#26377;&#34987;&#25506;&#32034;&#36807;&#12290;PCN&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#24066;&#22330;&#21160;&#24577;&#12289;&#32929;&#31080;&#20195;&#30721;&#21644;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#19978;&#19979;&#25991;&#24066;&#22330;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#23454;&#29616;&#23545;&#37329;&#34701;&#25968;&#25454;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.07708</link><description>&lt;p&gt;
&#28155;&#21152;&#35821;&#20041;&#19978;&#19979;&#25991;&#30340;&#25511;&#21046;&#21147;&#37327;&#65292;&#20026;&#37329;&#34701;&#24066;&#22330;&#25968;&#25454;&#29983;&#25104;&#24341;&#20837;Market-GAN
&lt;/p&gt;
&lt;p&gt;
Market-GAN: Adding Control to Financial Market Data Generation with Semantic Context. (arXiv:2309.07708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#20855;&#26377;&#24066;&#22330;&#21160;&#24577;&#12289;&#32929;&#31080;&#20195;&#30721;&#21644;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#19978;&#19979;&#25991;&#24066;&#22330;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26469;&#23454;&#29616;&#23545;&#37329;&#34701;&#25968;&#25454;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#27169;&#25311;&#22120;&#22312;&#25552;&#21319;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;&#31649;&#29702;&#39118;&#38505;&#21644;&#20419;&#36827;&#25112;&#30053;&#37329;&#34701;&#20915;&#31574;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#37329;&#34701;&#24066;&#22330;&#27169;&#25311;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#26694;&#26550;&#24120;&#24120;&#38590;&#20197;&#36866;&#24212;&#19987;&#38376;&#30340;&#27169;&#25311;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#23558;&#25361;&#25112;&#24402;&#32467;&#20026;&#65306;i&#65289;&#24403;&#21069;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#19981;&#21253;&#21547;&#19978;&#19979;&#25991;&#26631;&#31614;&#65307;ii&#65289;&#24403;&#21069;&#30340;&#25216;&#26415;&#27809;&#26377;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#25511;&#21046;&#30340;&#37329;&#34701;&#25968;&#25454;&#65292;&#19982;&#20854;&#20182;&#27169;&#24577;&#30456;&#27604;&#65292;&#36825;&#35201;&#27714;&#26356;&#39640;&#30340;&#31934;&#24230;&#65307;iii&#65289;&#30001;&#20110;&#37329;&#34701;&#25968;&#25454;&#30340;&#38750;&#24179;&#31283;&#12289;&#22122;&#22768;&#24615;&#36136;&#65292;&#29983;&#25104;&#19982;&#19978;&#19979;&#25991;&#23545;&#40784;&#12289;&#39640;&#20445;&#30495;&#24230;&#30340;&#25968;&#25454;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#65306;i&#65289;&#25552;&#20986;&#20102;&#20855;&#26377;&#24066;&#22330;&#21160;&#24577;&#12289;&#32929;&#31080;&#20195;&#30721;&#21644;&#21382;&#21490;&#29366;&#24577;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#19978;&#19979;&#25991;&#24066;&#22330;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#32447;&#24615;&#22238;&#24402;&#21644;&#21160;&#24577;&#26102;&#38388;&#25197;&#26354;&#32858;&#31867;&#32467;&#21512;&#30340;&#24066;&#22330;&#21160;&#24577;&#24314;&#27169;&#26041;&#27861;&#25552;&#21462;&#24066;&#22330;&#21160;&#24577;&#65307;ii&#65289;&#25105;&#20204;&#39044;&#20808;&#20934;&#22791;&#20102;Market-GAN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26041;&#27861;&#20197;&#21450;&#32534;&#30721;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#23545;&#37329;&#34701;&#25968;&#25454;&#29983;&#25104;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Financial simulators play an important role in enhancing forecasting accuracy, managing risks, and fostering strategic financial decision-making. Despite the development of financial market simulation methodologies, existing frameworks often struggle with adapting to specialized simulation context. We pinpoint the challenges as i) current financial datasets do not contain context labels; ii) current techniques are not designed to generate financial data with context as control, which demands greater precision compared to other modalities; iii) the inherent difficulties in generating context-aligned, high-fidelity data given the non-stationary, noisy nature of financial data. To address these challenges, our contributions are: i) we proposed the Contextual Market Dataset with market dynamics, stock ticker, and history state as context, leveraging a market dynamics modeling method that combines linear regression and Dynamic Time Warping clustering to extract market dynamics; ii) we prese
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.04344</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#23545;&#38646;&#26679;&#26412;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#24378;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Robustification of Zero-Shot Models With Foundation Models. (arXiv:2309.04344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04344
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#24378;&#21270;&#26041;&#27861;RoboShot&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#65292;&#24182;&#24212;&#29992;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#20013;&#20197;&#21435;&#38500;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;&#65292;&#20174;&#32780;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#25512;&#26029;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#36827;&#34892;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#32487;&#25215;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#20256;&#32479;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24494;&#35843;&#65292;&#20294;&#36825;&#21066;&#24369;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20027;&#35201;&#20248;&#21183;&#65292;&#21363;&#21487;&#20197;&#30452;&#25509;&#20351;&#29992;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RoboShot&#65292;&#19968;&#31181;&#23436;&#20840;&#38646;&#26679;&#26412;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#21892;&#39044;&#35757;&#32451;&#27169;&#22411;&#23884;&#20837;&#30340;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20174;&#20219;&#21153;&#25551;&#36848;&#20013;&#33719;&#21462;&#26377;&#29992;&#30340;&#35265;&#35299;&#12290;&#36825;&#20123;&#35265;&#35299;&#34987;&#23884;&#20837;&#24182;&#29992;&#20110;&#21435;&#38500;&#23884;&#20837;&#20013;&#30340;&#26377;&#23475;&#25104;&#20998;&#24182;&#22686;&#24378;&#26377;&#29992;&#25104;&#20998;--&#32780;&#26080;&#38656;&#20219;&#20309;&#30417;&#30563;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#21487;&#35745;&#31639;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38646;&#26679;&#26412;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#65292;&#24182;&#32473;&#20986;&#20102;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#22312;&#20061;&#20010;&#22270;&#20687;&#21644;NLP&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;RoboShot&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot inference is a powerful paradigm that enables the use of large pretrained models for downstream classification tasks without further training. However, these models are vulnerable to inherited biases that can impact their performance. The traditional solution is fine-tuning, but this undermines the key advantage of pretrained models, which is their ability to be used out-of-the-box. We propose RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot fashion. First, we use zero-shot language models (LMs) to obtain useful insights from task descriptions. These insights are embedded and used to remove harmful and boost useful components in embeddings -- without any supervision. Theoretically, we provide a simple and tractable model for biases in zero-shot embeddings and give a result characterizing under what conditions our approach can boost performance. Empirically, we evaluate RoboShot on nine image and NLP classification tasks and s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;Q&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2309.03202</link><description>&lt;p&gt;
&#23545;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Reinforcement Learning Techniques for Trading on a Diverse Portfolio. (arXiv:2309.03202v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22810;&#26679;&#21270;&#25237;&#36164;&#32452;&#21512;&#30340;&#21487;&#34892;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#22312;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;Q&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#22312;S&amp;P 500&#25351;&#25968;&#19978;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#21487;&#34892;&#24615;&#30340;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#20215;&#20540;&#36845;&#20195;&#65288;VI&#65289;&#21644;&#29366;&#24577;-&#21160;&#20316;-&#22870;&#21169;-&#29366;&#24577;-&#21160;&#20316;&#65288;SARSA&#65289;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#22522;&#20110;&#31574;&#30053;&#22806;&#30340;Q&#23398;&#20064;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#21547;2000-2023&#24180;&#22810;&#24180;&#32929;&#24066;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#20998;&#26512;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#19981;&#21516;&#26102;&#38388;&#27573;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65306;&#19968;&#20010;&#21253;&#25324;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#24180;&#20221;&#65292;&#19968;&#20010;&#19981;&#21253;&#25324;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;COVID-19&#26102;&#26399;&#30340;&#24066;&#22330;&#25968;&#25454;&#27604;&#22522;&#20934;&#31574;&#30053;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#31574;&#30053;&#30340;&#26041;&#27861;&#65288;VI&#21644;SARSA&#65289;&#20248;&#20110;Q&#23398;&#20064;&#65292;&#20984;&#26174;&#20102;&#31616;&#21333;&#31574;&#30053;&#30340;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;Q&#23398;&#20064;&#30340;&#24615;&#33021;&#21487;&#33021;&#22240;&#26465;&#20214;&#30340;&#21464;&#21270;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work seeks to answer key research questions regarding the viability of reinforcement learning over the S&amp;P 500 index. The on-policy techniques of Value Iteration (VI) and State-action-reward-state-action (SARSA) are implemented along with the off-policy technique of Q-Learning. The models are trained and tested on a dataset comprising multiple years of stock market data from 2000-2023. The analysis presents the results and findings from training and testing the models using two different time periods: one including the COVID-19 pandemic years and one excluding them. The results indicate that including market data from the COVID-19 period in the training dataset leads to superior performance compared to the baseline strategies. During testing, the on-policy approaches (VI and SARSA) outperform Q-learning, highlighting the influence of bias-variance tradeoff and the generalization capabilities of simpler policies. However, it is noted that the performance of Q-learning may vary depe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DeCoAR&#27169;&#22411;&#22312;&#31934;&#32454;&#35757;&#32451;&#26041;&#26696;&#20013;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;1.81\%&#21644;&#32422;4.56\%&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.01108</link><description>&lt;p&gt;
&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;: &#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#26159;&#21542;&#26377;&#21033;&#65311;
&lt;/p&gt;
&lt;p&gt;
Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?. (arXiv:2309.01108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#32773;&#30340;&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;DeCoAR&#27169;&#22411;&#22312;&#31934;&#32454;&#35757;&#32451;&#26041;&#26696;&#20013;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#20998;&#21035;&#21462;&#24471;&#20102;&#32422;1.81\%&#21644;&#32422;4.56\%&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#21040;&#21457;&#38899;&#21453;&#28436;(ACI)&#28041;&#21450;&#20174;&#22768;&#23398;&#31354;&#38388;&#26144;&#23556;&#21040;&#21457;&#38899;&#31354;&#38388;&#12290;&#20449;&#21495;&#22788;&#29702;&#29305;&#24449;&#22914;MFCCs&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;ACI&#20219;&#21153;&#12290;&#23545;&#20110;&#26377;&#35821;&#38899;&#38556;&#30861;&#30340;&#24739;&#32773;&#65292;&#30001;&#20110;&#19981;&#20934;&#30830;&#21644;&#19981;&#28165;&#26224;&#30340;&#21457;&#38899;&#65292;ACI&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;(SSL)&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23545;&#35821;&#38899;&#38556;&#30861;&#30340;ACI&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#29305;&#24449;&#23545;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;ACI&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;x-vectors&#19982;&#25552;&#21462;&#30340;SSL&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;BLSTM&#32593;&#32476;&#12290;&#22312;&#24050;&#30693;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#19977;&#31181;ACI&#35757;&#32451;&#26041;&#26696;&#65288;&#20027;&#39064;&#29305;&#23450;&#65292;&#32858;&#21512;&#21644;&#24494;&#35843;&#65289;&#12290;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;DeCoAR&#22312;&#24494;&#35843;&#26041;&#26696;&#20013;&#65292;&#30456;&#23545;&#20110;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;&#24739;&#32773;&#65292;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;(CC)&#30340;&#25913;&#36827;&#24133;&#24230;&#20998;&#21035;&#20026;&#32422;1.81\%&#21644;&#32422;4.56\%&#12290;
&lt;/p&gt;
&lt;p&gt;
$ $Acoustic-to-articulatory inversion (AAI) involves mapping from the acoustic space to the articulatory space. Signal-processing features like the MFCCs, have been widely used for the AAI task. For subjects with dysarthric speech, AAI is challenging because of an imprecise and indistinct pronunciation. In this work, we perform AAI for dysarthric speech using representations from pre-trained self-supervised learning (SSL) models. We demonstrate the impact of different pre-trained features on this challenging AAI task, at low-resource conditions. In addition, we also condition x-vectors to the extracted SSL features to train a BLSTM network. In the seen case, we experiment with three AAI training schemes (subject-specific, pooled, and fine-tuned). The results, consistent across training schemes, reveal that DeCoAR, in the fine-tuned scheme, achieves a relative improvement of the Pearson Correlation Coefficient (CC) by ${\sim}$1.81\% and ${\sim}$4.56\% for healthy controls and patients, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20462;&#27491;&#24341;&#21147;&#27169;&#25311;&#20013;&#25552;&#21462;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.00612</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#23431;&#23449;&#23610;&#24230;&#20013;&#30340;&#20462;&#27491;&#24341;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bayesian deep learning for cosmic volumes with modified gravity. (arXiv:2309.00612v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20462;&#27491;&#24341;&#21147;&#27169;&#25311;&#20013;&#25552;&#21462;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#24182;&#23545;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#19968;&#20195;&#30340;&#26143;&#31995;&#35843;&#26597;&#23558;&#25552;&#20379;&#21069;&#25152;&#26410;&#26377;&#30340;&#25968;&#25454;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#23431;&#23449;&#23610;&#24230;&#19978;&#27979;&#35797;&#24341;&#21147;&#12290;&#23545;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#20581;&#22766;&#23431;&#23449;&#23398;&#20998;&#26512;&#38656;&#35201;&#21033;&#29992;&#32534;&#30721;&#22312;&#23431;&#23449;&#32593;&#20013;&#30340;&#38750;&#32447;&#24615;&#20449;&#24687;&#12290;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#21364;&#19981;&#33021;&#25552;&#20379;&#20808;&#39564;&#30340;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#20462;&#27491;&#24341;&#21147;&#65288;MG&#65289;&#27169;&#25311;&#20013;&#25552;&#21462;&#23431;&#23449;&#23398;&#21442;&#25968;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#23454;&#29616;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#36817;&#20284;&#21518;&#39564;&#20998;&#24067;&#65292;&#20998;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#19968;&#36125;&#21494;&#26031;&#26368;&#21518;&#19968;&#23618;&#65288;BLL&#65289;&#30340;&#24773;&#20917;&#65292;&#21644;&#19968;&#20010;&#22312;&#25152;&#26377;&#23618;&#38754;&#19978;&#37117;&#20855;&#26377;&#36125;&#21494;&#26031;&#23618;&#65288;FullB&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#31354;&#38388;&#23494;&#24230;&#22330;&#21644;&#19968;&#22871;2000&#20010;&#20165;&#21253;&#21547;&#26263;&#29289;&#36136;&#31890;&#23376;&#32593;&#26684;$ N $-&#20307;&#27169;&#25311;&#30340;&#21151;&#29575;&#35889;&#23545;&#36825;&#20004;&#20010;BNN&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#27169;&#25311;&#21253;&#25324;&#22522;&#20110;MG-PICOLA&#30340;&#20462;&#27491;&#24341;&#21147;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;&#36793;&#38271;&#20026;256 $h^{-1}$ Mpc&#30340;&#31435;&#26041;&#20307;&#20307;&#31215;&#65292;&#20854;&#20013;&#21253;&#21547;128$&#12290;
&lt;/p&gt;
&lt;p&gt;
The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36716;&#21270;&#38382;&#39064;&#20351;&#20854;&#25104;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#30005;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#21508;&#31181;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#21644;&#36291;&#36801;&#20598;&#26497;&#30697;&#65292;&#24182;&#22312;&#33519;&#31561;&#22823;&#20998;&#23376;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16848</link><description>&lt;p&gt;
&#37327;&#23376;&#31995;&#32479;&#20013;&#28608;&#21457;&#24577;&#30340;&#33258;&#28982;&#37327;&#23376;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Natural Quantum Monte Carlo Computation of Excited States. (arXiv:2308.16848v1 [physics.comp-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16848
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#20013;&#30340;&#28608;&#21457;&#24577;&#65292;&#36890;&#36807;&#36716;&#21270;&#38382;&#39064;&#20351;&#20854;&#25104;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#30005;&#23376;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#20934;&#30830;&#22320;&#35745;&#31639;&#21508;&#31181;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#21644;&#36291;&#36801;&#20598;&#26497;&#30697;&#65292;&#24182;&#22312;&#33519;&#31561;&#22823;&#20998;&#23376;&#19978;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#31639;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#37327;&#23376;&#31995;&#32479;&#30340;&#26368;&#20302;&#28608;&#21457;&#24577;&#65292;&#36825;&#26159;&#23545;&#23547;&#25214;&#22522;&#24577;&#30340;&#20272;&#35745;&#30340;&#33258;&#28982;&#25512;&#24191;&#12290;&#35813;&#26041;&#27861;&#27809;&#26377;&#33258;&#30001;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#26174;&#24335;&#27491;&#20132;&#21270;&#19981;&#21516;&#30340;&#24577;&#65292;&#32780;&#26159;&#23558;&#23547;&#25214;&#32473;&#23450;&#31995;&#32479;&#30340;&#28608;&#21457;&#24577;&#30340;&#38382;&#39064;&#36716;&#21270;&#20026;&#23547;&#25214;&#25193;&#23637;&#31995;&#32479;&#30340;&#22522;&#24577;&#30340;&#38382;&#39064;&#12290;&#21487;&#20197;&#35745;&#31639;&#20219;&#24847;&#21487;&#35266;&#27979;&#37327;&#30340;&#26399;&#26395;&#20540;&#65292;&#21253;&#25324;&#19981;&#21516;&#24577;&#20043;&#38388;&#30340;&#38750;&#23545;&#35282;&#32447;&#26399;&#26395;&#20540;&#65292;&#22914;&#36291;&#36801;&#20598;&#26497;&#30697;&#12290;&#23613;&#31649;&#35813;&#26041;&#27861;&#23436;&#20840;&#36890;&#29992;&#65292;&#20294;&#19982;&#26368;&#36817;&#20851;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22810;&#30005;&#23376;&#31995;&#32479;&#21464;&#20998;&#21442;&#25968;&#30340;&#24037;&#20316;&#32467;&#21512;&#20351;&#29992;&#25928;&#26524;&#29305;&#21035;&#22909;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#35813;&#26041;&#27861;&#19982;FermiNet&#21644;Psiformer&#21464;&#20998;&#21442;&#25968;&#32467;&#21512;&#20351;&#29992;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#24674;&#22797;&#33519;&#31561;&#22823;&#20998;&#23376;&#30340;&#22402;&#30452;&#28608;&#21457;&#33021;&#21644;&#25391;&#23376;&#24378;&#24230;&#12290;&#38500;&#20102;&#22312;&#20998;&#23376;&#19978;&#30340;&#31034;&#20363;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;...
&lt;/p&gt;
&lt;p&gt;
We present a variational Monte Carlo algorithm for estimating the lowest excited states of a quantum system which is a natural generalization of the estimation of ground states. The method has no free parameters and requires no explicit orthogonalization of the different states, instead transforming the problem of finding excited states of a given system into that of finding the ground state of an expanded system. Expected values of arbitrary observables can be calculated, including off-diagonal expectations between different states such as the transition dipole moment. Although the method is entirely general, it works particularly well in conjunction with recent work on using neural networks as variational Ansatze for many-electron systems, and we show that by combining this method with the FermiNet and Psiformer Ansatze we can accurately recover vertical excitation energies and oscillator strengths on molecules as large as benzene. Beyond the examples on molecules presented here, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.09895</link><description>&lt;p&gt;
&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#32534;&#31243;&#35821;&#35328;&#30340;&#30693;&#35782;&#36716;&#31227;&#29992;&#20110;&#20195;&#30721;LLMs
&lt;/p&gt;
&lt;p&gt;
Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#21319;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21517;&#20026;MultiPL-T&#65292;&#36890;&#36807;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20195;&#30721;LLMs&#65288;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65289;&#24320;&#22987;&#23545;&#32534;&#31243;&#23454;&#36341;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#20195;&#30721;LLMs&#36824;&#25104;&#20026;&#32534;&#31243;&#35821;&#35328;&#21644;&#36719;&#20214;&#24037;&#31243;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;LLMs&#29983;&#25104;&#30340;&#20195;&#30721;&#36136;&#37327;&#22312;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20195;&#30721;LLMs&#23545;&#35757;&#32451;&#25968;&#25454;&#20805;&#20998;&#30340;&#32534;&#31243;&#35821;&#35328;&#65288;&#22914;Java&#12289;Python&#25110;JavaScript&#65289;&#20135;&#29983;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20687;OCaml&#21644;Racket&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#34920;&#29616;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21322;&#21512;&#25104;&#25968;&#25454;&#25552;&#39640;&#20195;&#30721;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#24182;&#21487;&#29992;&#20110;&#24494;&#35843;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;LLMs&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;MultiPL-T&#65292;&#23427;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#36716;&#21270;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#29983;&#25104;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#25991;&#26412;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2308.06394</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#39044;&#38450;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Detecting and Preventing Hallucinations in Large Vision Language Models. (arXiv:2308.06394v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#24187;&#35273;&#25991;&#26412;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35843;&#25972;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#22312;&#27867;&#21270;&#36328;&#22810;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20026;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#35814;&#32454;&#22238;&#31572;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LVLM&#65288;InstructBLIP&#65289;&#20173;&#28982;&#23384;&#22312;&#30528;&#24778;&#20154;&#30340;30%&#30340;&#24187;&#35273;&#25991;&#26412;&#65292;&#21253;&#25324;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#12289;&#19981;&#24544;&#23454;&#30340;&#25551;&#36848;&#21644;&#19981;&#20934;&#30830;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;M-HalDetect&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#24187;&#35273;&#26816;&#27979;&#21644;&#39044;&#38450;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;M-HalDetect&#21253;&#21547;&#20102;16k&#20010;&#32454;&#31890;&#24230;&#30340;VQA&#31034;&#20363;&#26631;&#31614;&#65292;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#35814;&#32454;&#22270;&#20687;&#25551;&#36848;&#30340;&#20840;&#38754;&#22810;&#27169;&#24577;&#24187;&#35273;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#21482;&#32771;&#34385;&#23545;&#35937;&#24187;&#35273;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#20102;&#23454;&#20307;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuned Large Vision Language Models (LVLMs) have made significant advancements in generalizing across a diverse set of multimodal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a {M}ultimodal {Hal}lucination {Detect}ion Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained labels on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#65292;&#19981;&#21463;&#35757;&#32451;&#26102;&#27010;&#24565;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#26597;&#35810;&#36827;&#34892;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#25551;&#36848;&#31526;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#65292;&#21516;&#26102;&#32771;&#34385;&#36974;&#25377;&#21644;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.05737</link><description>&lt;p&gt;
Follow Anything: &#23454;&#26102;&#24320;&#25918;&#38598;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Follow Anything: Open-set detection, tracking, and following in real-time. (arXiv:2308.05737v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#8221;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#23454;&#26102;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#65292;&#19981;&#21463;&#35757;&#32451;&#26102;&#27010;&#24565;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#26597;&#35810;&#36827;&#34892;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35270;&#35273;&#25551;&#36848;&#31526;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26816;&#27979;&#12289;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#65292;&#21516;&#26102;&#32771;&#34385;&#36974;&#25377;&#21644;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#21160;&#21270;&#12289;&#29289;&#27969;&#21644;&#20179;&#20648;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#23433;&#20840;&#31561;&#22810;&#31181;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#65292;&#36861;&#36394;&#21644;&#36319;&#38543;&#24863;&#20852;&#36259;&#30340;&#29289;&#20307;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#12289;&#36861;&#36394;&#21644;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#31216;&#20026;&#8220;&#36319;&#38543;&#20219;&#20309;&#29289;&#20307;&#8221;&#65288;FAn&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#24320;&#25918;&#35789;&#27719;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411; - &#19981;&#21463;&#35757;&#32451;&#26102;&#30340;&#27010;&#24565;&#38480;&#21046;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#28857;&#20987;&#26597;&#35810;&#26469;&#24212;&#29992;&#20110;&#25512;&#26029;&#26102;&#30340;&#26032;&#31867;&#21035;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#22522;&#30784;&#27169;&#22411;&#65289;&#30340;&#20016;&#23500;&#35270;&#35273;&#25551;&#36848;&#31526;&#65292;FAn&#21487;&#20197;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#26597;&#35810;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#28857;&#20987;&#65289;&#19982;&#36755;&#20837;&#22270;&#20687;&#24207;&#21015;&#36827;&#34892;&#21305;&#37197;&#26469;&#26816;&#27979;&#21644;&#20998;&#21106;&#29289;&#20307;&#12290;&#36825;&#20123;&#26816;&#27979;&#21644;&#20998;&#21106;&#30340;&#29289;&#20307;&#22312;&#22270;&#20687;&#24103;&#20043;&#38388;&#36827;&#34892;&#36319;&#36394;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#36974;&#25377;&#21644;&#29289;&#20307;&#37325;&#26032;&#20986;&#29616;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#19978;&#65288;&#24494;&#22411;&#39134;&#34892;&#22120;&#65289;&#28436;&#31034;&#20102;FAn&#65292;&#24182;&#25253;&#21578;&#20102;&#20854;&#26080;&#32541;&#36319;&#38543;&#24863;&#20852;&#36259;&#29289;&#20307;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tracking and following objects of interest is critical to several robotics use cases, ranging from industrial automation to logistics and warehousing, to healthcare and security. In this paper, we present a robotic system to detect, track, and follow any object in real-time. Our approach, dubbed ``follow anything'' (FAn), is an open-vocabulary and multimodal model -- it is not restricted to concepts seen at training time and can be applied to novel classes at inference time using text, images, or click queries. Leveraging rich visual descriptors from large-scale pre-trained models (foundation models), FAn can detect and segment objects by matching multimodal queries (text, images, clicks) against an input image sequence. These detected and segmented objects are tracked across image frames, all while accounting for occlusion and object re-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial vehicle) and report its ability to seamlessly follow the objects of inter
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#24773;&#20917;&#19979;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#31934;&#24230;&#21644;&#30828;&#20214;&#36164;&#28304;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;IMU&#24863;&#27979;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02397</link><description>&lt;p&gt;
&#20174;&#31232;&#30095;IMU&#24863;&#27979;&#20013;&#39640;&#25928;&#20934;&#30830;&#22320;&#36827;&#34892;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing. (arXiv:2308.02397v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27169;&#25311;&#24773;&#20917;&#19979;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#65292;&#30740;&#31350;&#20102;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#20013;&#31934;&#24230;&#21644;&#30828;&#20214;&#36164;&#28304;&#20043;&#38388;&#30340;&#25240;&#34935;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;IMU&#24863;&#27979;&#36827;&#34892;&#39640;&#25928;&#20934;&#30830;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25935;&#24863;&#30340;&#20010;&#20154;&#25968;&#25454;&#65292;&#23545;&#20110;&#35780;&#20272;&#36816;&#21160;&#12289;&#24247;&#22797;&#25110;&#24037;&#20316;&#23433;&#20840;&#20013;&#30340;&#20154;&#20307;&#36816;&#21160;&#30340;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#65288;HPE&#65289;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#24863;&#27979;&#12290;&#22240;&#27492;&#65292;&#26412;&#22320;&#22788;&#29702;&#26159;&#24517;&#35201;&#30340;&#65292;&#24182;&#19988;&#22312;&#27492;&#31867;&#31995;&#32479;&#20013;&#65292;&#26377;&#38480;&#30340;&#33021;&#28304;&#39044;&#31639;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#32780;&#19981;&#26159;&#24120;&#35265;&#30340;&#30456;&#26426;&#24863;&#27979;&#26469;&#35299;&#20915;&#12290;&#30740;&#31350;&#20013;&#24456;&#23569;&#35752;&#35770;&#31934;&#24230;&#21644;&#30828;&#20214;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#20043;&#38388;&#30340;&#26680;&#24515;&#25240;&#34935;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#24773;&#20917;&#19979;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#26469;&#35299;&#20915;&#36825;&#20010;&#25240;&#34935;&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#19981;&#21516;&#30340;IMU&#20256;&#24863;&#22120;&#25968;&#37327;&#21644;&#25670;&#25918;&#20301;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21487;&#29992;&#30340;&#20154;&#20307;&#27169;&#22411;&#25968;&#25454;&#38598;&#20026;&#19981;&#21516;&#30340;&#20256;&#24863;&#22120;&#37197;&#32622;&#29983;&#25104;IMU&#25968;&#25454;&#65292;&#24182;&#29992;&#36825;&#20123;&#25968;&#25454;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32452;&#21512;&#24230;&#37327;&#26469;&#35780;&#20272;&#31934;&#24230;&#21644;&#36164;&#28304;&#21033;&#29992;&#30340;&#25240;&#34935;&#12290;&#25105;&#20204;&#23558;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#29992;&#20316;&#35780;&#20272;&#20256;&#24863;&#22120;&#37197;&#32622;&#24182;&#30830;&#23450;&#29305;&#23450;&#29992;&#20363;&#30340;&#26377;&#30410;&#37197;&#32622;&#30340;&#24037;&#20855;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#21516;&#31561;&#37325;&#35201;&#24615;&#30340;&#31995;&#32479;...
&lt;/p&gt;
&lt;p&gt;
Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation or work safety requires accurate sensing without compromising the sensitive underlying personal data. Therefore, local processing is necessary and the limited energy budget in such systems can be addressed by Inertial Measurement Units (IMU) instead of common camera sensing. The central trade-off between accuracy and efficient use of hardware resources is rarely discussed in research. We address this trade-off by a simulative Design Space Exploration (DSE) of a varying quantity and positioning of IMU-sensors. First, we generate IMU-data from a publicly available body model dataset for different sensor configurations and train a deep learning model with this data. Additionally, we propose a combined metric to assess the accuracy-resource trade-off. We used the DSE as a tool to evaluate sensor configurations and identify beneficial ones for a specific use case. Exemplary, for a system with equal importance of ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65288;ProxGD&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#26354;&#29575;&#20449;&#24687;&#23436;&#20840;&#33258;&#36866;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#65292;&#19988;&#20801;&#35768;&#20351;&#29992;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2308.02261</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#30340;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adaptive Proximal Gradient Method for Convex Optimization. (arXiv:2308.02261v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65288;ProxGD&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#23616;&#37096;&#26354;&#29575;&#20449;&#24687;&#23436;&#20840;&#33258;&#36866;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#25910;&#25947;&#24615;&#65292;&#19988;&#20801;&#35768;&#20351;&#29992;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20984;&#20248;&#21270;&#20013;&#30340;&#20004;&#20010;&#22522;&#26412;&#19968;&#38454;&#31639;&#27861;&#65292;&#21363;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65288;ProxGD&#65289;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36890;&#36807;&#21033;&#29992;&#24179;&#28369;&#20989;&#25968;&#30340;&#23616;&#37096;&#26354;&#29575;&#20449;&#24687;&#65292;&#20351;&#36825;&#20123;&#31639;&#27861;&#23436;&#20840;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35266;&#23519;&#21040;&#30340;&#26799;&#24230;&#24046;&#24322;&#30340;&#33258;&#36866;&#24212;&#29256;&#26412;&#30340;GD&#21644;ProxGD&#65292;&#22240;&#27492;&#19981;&#20250;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20165;&#20551;&#35774;&#26799;&#24230;&#30340;&#23616;&#37096;Lipschitz&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#21478;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#29256;&#26412;&#20801;&#35768;&#20351;&#29992;&#27604;[MM20]&#26368;&#21021;&#24314;&#35758;&#30340;&#26356;&#22823;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore two fundamental first-order algorithms in convex optimization, namely, gradient descent (GD) and proximal gradient method (ProxGD). Our focus is on making these algorithms entirely adaptive by leveraging local curvature information of smooth functions. We propose adaptive versions of GD and ProxGD that are based on observed gradient differences and, thus, have no added computational costs. Moreover, we prove convergence of our methods assuming only local Lipschitzness of the gradient. In addition, the proposed versions allow for even larger stepsizes than those initially suggested in [MM20].
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.13771</link><description>&lt;p&gt;
&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#30340;&#20934;&#30830;&#24615;&#22686;&#24378;&#65306;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#20405;&#29359;&#20010;&#20154;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#20005;&#26684;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21487;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20445;&#30041;&#24213;&#23618;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#24046;&#20998;&#38544;&#31169;&#26694;&#26550;&#19979;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22359;&#25552;&#39640;&#24046;&#20998;&#38544;&#31169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#36923;&#36753;&#22238;&#24402;&#27169;&#22411;&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#38598;&#21644;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25968;&#20540;&#32467;&#26524;&#34920;&#26126;&#65292;&#28155;&#21152;&#39044;&#35757;&#32451;&#27169;&#22359;&#26174;&#33879;&#25552;&#39640;&#20102;&#24046;&#20998;&#38544;&#31169;&#36923;&#36753;&#22238;&#24402;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
&lt;/p&gt;</description></item><item><title>Epsilon*&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#39118;&#38505;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#19982;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#38544;&#31169;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11280</link><description>&lt;p&gt;
Epsilon*: &#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38544;&#31169;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Epsilon*: Privacy Metric for Machine Learning Models. (arXiv:2307.11280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11280
&lt;/p&gt;
&lt;p&gt;
Epsilon*&#26159;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38544;&#31169;&#39118;&#38505;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#65292;&#33021;&#19982;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#23545;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#38544;&#31169;&#25439;&#22833;&#30340;&#19979;&#30028;&#65292;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Epsilon*&#65292;&#19968;&#31181;&#26032;&#30340;&#38544;&#31169;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#38544;&#31169;&#20943;&#36731;&#31574;&#30053;&#37096;&#32626;&#20043;&#21069;&#12289;&#26399;&#38388;&#25110;&#20043;&#21518;&#65292;&#27979;&#37327;&#21333;&#20010;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#39118;&#38505;&#12290;&#35813;&#24230;&#37327;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#37319;&#26679;&#25110;&#27169;&#22411;&#35757;&#32451;&#31639;&#27861;&#12290;Epsilon*&#26159;&#19968;&#20010;&#20851;&#20110;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#29575;&#30340;&#20989;&#25968;&#65292;&#29992;&#20110;&#25932;&#25163;&#22312;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20013;&#20351;&#29992;&#30340;&#20551;&#35774;&#26816;&#39564;&#20013;&#12290;&#25105;&#20204;&#21306;&#20998;&#20102;&#37327;&#21270;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#25439;&#22833;&#21644;&#37327;&#21270;&#20135;&#29983;&#35813;&#27169;&#22411;&#23454;&#20363;&#30340;&#35757;&#32451;&#26426;&#21046;&#30340;&#38544;&#31169;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#38544;&#31169;&#23457;&#35745;&#25991;&#29486;&#20013;&#30340;&#26041;&#27861;&#20026;&#21518;&#32773;&#25552;&#20379;&#20102;&#19979;&#30028;&#65292;&#32780;&#25105;&#20204;&#30340;&#24230;&#37327;&#26041;&#27861;&#36890;&#36807;&#20381;&#36182;&#20110;&#35757;&#32451;&#27169;&#22411;&#23454;&#20363;&#30340;&#38544;&#31169;&#30340;&#65288;&#949;&#65292;&#948;&#65289;&#22411;&#37327;&#21270;&#65292;&#20026;&#21069;&#32773;&#25552;&#20379;&#20102;&#19979;&#30028;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20123;&#19979;&#30028;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23454;&#29616;Epsilon*&#20197;&#36991;&#20813;&#25968;&#20540;&#21644;&#22122;&#22768;&#25918;&#22823;&#19981;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Epsilon*, a new privacy metric for measuring the privacy risk of a single model instance prior to, during, or after deployment of privacy mitigation strategies. The metric does not require access to the training data sampling or model training algorithm. Epsilon* is a function of true positive and false positive rates in a hypothesis test used by an adversary in a membership inference attack. We distinguish between quantifying the privacy loss of a trained model instance and quantifying the privacy loss of the training mechanism which produces this model instance. Existing approaches in the privacy auditing literature provide lower bounds for the latter, while our metric provides a lower bound for the former by relying on an (${\epsilon}$,${\delta}$)-type of quantification of the privacy of the trained model instance. We establish a relationship between these lower bounds and show how to implement Epsilon* to avoid numerical and noise amplification instability. We further 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.07572</link><description>&lt;p&gt;
Harpa: &#39640;&#36895;&#29575;&#19979;&#30340;&#30456;&#20301;&#20851;&#32852;&#19982;&#36208;&#26102;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#20851;&#32852;&#26159;&#26681;&#25454;&#20854;&#36215;&#28304;&#22320;&#38663;&#20998;&#32452;&#22320;&#38663;&#27874;&#21040;&#36798;&#30340;&#20219;&#21153;&#12290;&#23427;&#26159;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#20123;&#20107;&#20214;&#25658;&#24102;&#26377;&#20851;&#22320;&#38663;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#24120;&#20551;&#23450;&#19981;&#20934;&#30830;&#30340;&#27874;&#36895;&#27169;&#22411;&#19979;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#20851;&#32852;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21457;&#29983;&#29575;&#36739;&#20302;&#19988;&#23481;&#26131;&#20851;&#32852;&#30340;&#36739;&#22823;&#20107;&#20214;&#65292;&#23613;&#31649;&#24494;&#22320;&#38663;&#27963;&#21160;&#25552;&#20379;&#20102;&#20117;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#20197;&#27604;&#20197;&#21069;&#25253;&#21578;&#30340;&#26356;&#39640;&#30340;&#36895;&#29575;&#36827;&#34892;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Harpa&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#39318;&#20808;&#35299;&#20915;&#32852;&#21512;&#26102;&#31354;&#28304;&#23450;&#20301;&#21644;&#27874;&#36895;&#24674;&#22797;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20301;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2307.07084</link><description>&lt;p&gt;
&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;Wasserstein&#21464;&#20998;&#25512;&#29702;&#65306;&#21487;&#35299;&#37322;&#24615;&#30340;&#24418;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#35299;&#20915;&#20102;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#35299;&#37322;&#21644;&#36879;&#26126;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25110;&#26368;&#20248;&#25511;&#21046;&#21487;&#20197;&#20026;&#20855;&#26377;&#21487;&#21464;&#21160;&#24577;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#25552;&#20379;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#23454;&#26045;&#20013;&#65292;&#35299;&#37322;&#22870;&#21169;&#20989;&#25968;&#21644;&#30456;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#23558;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#65292;&#22240;&#20026;&#27010;&#29575;&#25512;&#29702;&#21407;&#21017;&#19978;&#25552;&#20379;&#20102;&#22810;&#26679;&#19988;&#24378;&#22823;&#30340;&#25968;&#23398;&#24037;&#20855;&#26469;&#25512;&#26029;&#38543;&#26426;&#21160;&#24577;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#22870;&#21169;&#35774;&#35745;&#21644;&#31574;&#30053;&#25910;&#25947;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;Wasserstein&#21464;&#20998;&#20248;&#21270;&#65288;AWaVO&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#24418;&#24335;&#26041;&#27861;&#26469;&#35299;&#37322;&#22870;&#21169;&#35774;&#35745;&#65292;&#36879;&#26126;&#22320;&#35757;&#32451;&#25910;&#25947;&#65292;&#20197;&#21450;&#23545;&#39034;&#24207;&#20915;&#31574;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#20026;&#20102;&#35777;&#26126;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25910;&#25947;&#35757;&#32451;&#24182;&#20445;&#35777;&#20102;&#25910;&#25947;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;(OKO)&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02245</link><description>&lt;p&gt;
&#20934;&#30830;&#21644;&#26657;&#20934;&#27169;&#22411;&#30340;&#38598;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02245
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#23398;&#20064;&#26041;&#27861;(OKO)&#26469;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#21644;&#26657;&#20934;&#19981;&#33391;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#22312;&#24212;&#29992;&#26631;&#20934;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26102;&#24456;&#38590;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22855;&#25968;-$k$-&#21435;&#38500;&#23398;&#20064;&#65288;OKO&#65289;&#65292;&#23427;&#36890;&#36807;&#26368;&#23567;&#21270;&#38598;&#21512;&#30340;&#20132;&#21449;&#29109;&#35823;&#24046;&#32780;&#19981;&#26159;&#21333;&#20010;&#31034;&#20363;&#30340;&#35823;&#24046;&#26469;&#23454;&#29616;&#12290;&#36825;&#33258;&#28982;&#22320;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#31034;&#20363;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#20351;&#29992;&#30828;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#24182;&#19988;&#19981;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#26657;&#20934;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#28201;&#24230;&#32553;&#25918;&#65292;OKO&#36890;&#24120;&#20063;&#33021;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;OKO&#33258;&#28982;&#22320;&#33021;&#22815;&#33719;&#24471;&#26356;&#22909;&#30340;&#26657;&#20934;&#25928;&#26524;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#24378;&#35843;&#65292;OKO&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#36866;&#24212;&#35768;&#22810;&#19981;&#21516;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model overconfidence and poor calibration are common in machine learning and difficult to account for when applying standard empirical risk minimization. In this work, we propose a novel method to alleviate these problems that we call odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets rather than for single examples. This naturally allows the model to capture correlations across data examples and achieves both better accuracy and calibration, especially in limited training data and class-imbalanced regimes. Perhaps surprisingly, OKO often yields better calibration even when training with hard labels and dropping any additional calibration parameter tuning, such as temperature scaling. We provide theoretical justification, establishing that OKO naturally yields better calibration, and provide extensive experimental analyses that corroborate our theoretical findings. We emphasize that OKO is a general framework that can be easily adapted to many settings and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.14043</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#33258;&#24049;&#30340;&#38543;&#26426;&#26631;&#20934;&#65306;&#38543;&#26426;&#24179;&#28369;&#21644;&#22522;&#20110;PRNG&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning needs its own Randomness Standard: Randomised Smoothing and PRNG-based attacks. (arXiv:2306.14043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#38543;&#26426;&#24615;&#21487;&#34987;&#25915;&#20987;&#32773;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;&#27969;&#34892;&#30340;&#38543;&#26426;&#24179;&#28369;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#20854;&#38543;&#26426;&#24615;&#21487;&#33021;&#23548;&#33268;&#31995;&#32479;&#34987;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#24615;&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#35768;&#22810;&#20851;&#38190;&#21151;&#33021;&#65292;&#21253;&#25324;&#20248;&#21270;&#12289;&#25968;&#25454;&#36873;&#25321;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#12290;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23558;&#29983;&#25104;&#25110;&#25910;&#38598;&#38543;&#26426;&#24615;&#30340;&#20219;&#21153;&#22806;&#21253;&#32473;&#20102;&#32534;&#35793;&#22120;&#12289;&#20113;&#26381;&#21153;&#25552;&#20379;&#21830;&#25110;&#24037;&#20855;&#38142;&#20013;&#30340;&#20854;&#20182;&#22320;&#26041;&#12290;&#20294;&#26159;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#19981;&#33391;&#38543;&#26426;&#24615;&#29978;&#33267;&#21019;&#24314;&#38543;&#26426;&#24615;&#30340;&#21382;&#21490;&#24736;&#20037;&#65292;&#23601;&#20687;NSA&#25918;&#32622;&#21518;&#38376;&#22312;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#20013;&#20197;&#30772;&#35299;&#21152;&#23494;&#19968;&#26679;&#12290;&#26412;&#25991;&#32771;&#34385;&#26159;&#21542;&#33021;&#22815;&#20165;&#21033;&#29992;&#25915;&#20987;&#32773;&#36890;&#24120;&#20381;&#36182;&#30340;&#38543;&#26426;&#24615;&#26469;&#21361;&#23475;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#38543;&#26426;&#24179;&#28369;&#19978;&#65292;&#36825;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#35777;&#26126;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#20219;&#24847;&#27169;&#22411;&#30340;&#29305;&#23450;&#36755;&#20837;&#25968;&#25454;&#28857;&#25552;&#20379;&#35748;&#35777;&#12290;&#25105;&#20204;&#36873;&#25321;&#38543;&#26426;&#24179;&#28369;&#26159;&#22240;&#20026;&#23427;&#29992;&#20110;&#23433;&#20840;&#21644;&#23433;&#20840;&#65288;&#29992;&#20110;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#21644;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#20998;&#21035;&#65289;&#12290;&#22312;&#24149;&#21518;&#65292;&#23427;&#20381;&#36182;&#20110;&#37319;&#26679;&#39640;&#26031;&#22122;&#22768;&#26469;&#25506;&#32034;&#22260;&#32469;&#25968;&#25454;&#28857;&#30340;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Randomness supports many critical functions in the field of machine learning (ML) including optimisation, data selection, privacy, and security. ML systems outsource the task of generating or harvesting randomness to the compiler, the cloud service provider or elsewhere in the toolchain. Yet there is a long history of attackers exploiting poor randomness, or even creating it -- as when the NSA put backdoors in random number generators to break cryptography. In this paper we consider whether attackers can compromise an ML system using only the randomness on which they commonly rely. We focus our effort on Randomised Smoothing, a popular approach to train certifiably robust models, and to certify specific input datapoints of an arbitrary model. We choose Randomised Smoothing since it is used for both security and safety -- to counteract adversarial examples and quantify uncertainty respectively. Under the hood, it relies on sampling Gaussian noise to explore the volume around a data poin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.14009</link><description>&lt;p&gt;
&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#21152;&#24378;&#22270;&#19978;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Boosting Multitask Learning on Graphs through Higher-Order Task Affinities. (arXiv:2306.14009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#36890;&#36807;&#26356;&#39640;&#32423;&#20219;&#21153;&#30456;&#20284;&#24615;&#26469;&#21152;&#24378;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#20197;&#24212;&#23545;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#22270;&#19978;&#39044;&#27979;&#33410;&#28857;&#26631;&#31614;&#26159;&#19968;&#20010;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#21253;&#25324;&#31038;&#21306;&#26816;&#27979;&#21644;&#20998;&#23376;&#22270;&#39044;&#27979;&#12290;&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#27492;&#38382;&#39064;&#65292;&#32771;&#34385;&#21516;&#26102;&#22312;&#22270;&#19978;&#39044;&#27979;&#22810;&#20010;&#33410;&#28857;&#26631;&#31614;&#20989;&#25968;&#12290;&#20026;&#20102;&#20855;&#20307;&#35828;&#26126;&#65292;&#32771;&#34385;&#37325;&#21472;&#31038;&#21306;&#26816;&#27979;&#65306;&#27599;&#20010;&#31038;&#21306;&#25104;&#21592;&#36523;&#20221;&#26159;&#19968;&#20010;&#20108;&#36827;&#21046;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#30001;&#20110;&#22797;&#26434;&#30340;&#37325;&#21472;&#27169;&#24335;&#65292;&#24403;&#25105;&#20204;&#23558;&#22810;&#20010;&#31038;&#21306;&#26816;&#27979;&#24212;&#29992;&#21040;naive&#22810;&#20219;&#21153;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36127;&#36801;&#31227;&#24456;&#26222;&#36941;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#33410;&#28857;&#26631;&#31614;&#20043;&#38388;&#30340;&#20219;&#21153;&#20851;&#31995;&#39640;&#24230;&#38750;&#32447;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22522;&#20110;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#23558;&#20219;&#21153;&#20998;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;&#27599;&#20010;&#20219;&#21153;&#32452;&#19978;&#25311;&#21512;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#20135;&#29983;&#22312;&#22522;&#32447;&#27169;&#22411;&#19978;&#30340;&#22686;&#24378;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#26356;&#39640;&#32423;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#24230;&#37327;&#20272;&#35745;&#20026;&#39044;&#27979;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting node labels on a given graph is a widely studied problem with many applications, including community detection and molecular graph prediction. This paper considers predicting multiple node labeling functions on graphs simultaneously and revisits this problem from a multitask learning perspective. For a concrete example, consider overlapping community detection: each community membership is a binary node classification task. Due to complex overlapping patterns, we find that negative transfer is prevalent when we apply naive multitask learning to multiple community detection, as task relationships are highly nonlinear across different node labeling. To address the challenge, we develop an algorithm to cluster tasks into groups based on a higher-order task affinity measure. We then fit a multitask model on each task group, resulting in a boosting procedure on top of the baseline model. We estimate the higher-order task affinity measure between two tasks as the prediction loss o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35748;&#20026;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#25104;&#21151;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#25311;&#21512;&#38543;&#26426;&#29366;&#24577;&#21450;&#38543;&#26426;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26426;&#25968;&#25454;&#30340;&#33021;&#21147;&#36829;&#21453;&#20102;&#24403;&#21069;&#23567;&#27867;&#21270;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#26500;&#24314;&#34917;&#20805;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21487;&#23558;&#20219;&#24847;&#26631;&#35760;&#25311;&#21512;&#21040;&#37327;&#23376;&#29366;&#24577;&#19978;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#36825;&#20123;&#32467;&#26524;&#25490;&#38500;&#20102;&#21333;&#21333;&#22522;&#20110;&#32463;&#20856;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#25152;&#26377;&#21487;&#33021;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.13461</link><description>&lt;p&gt;
&#29702;&#35299;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#27867;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding quantum machine learning also requires rethinking generalization. (arXiv:2306.13461v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35748;&#20026;&#65292;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#25104;&#21151;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#25311;&#21512;&#38543;&#26426;&#29366;&#24577;&#21450;&#38543;&#26426;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#31181;&#35760;&#24518;&#38543;&#26426;&#25968;&#25454;&#30340;&#33021;&#21147;&#36829;&#21453;&#20102;&#24403;&#21069;&#23567;&#27867;&#21270;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#26500;&#24314;&#34917;&#20805;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21487;&#23558;&#20219;&#24847;&#26631;&#35760;&#25311;&#21512;&#21040;&#37327;&#23376;&#29366;&#24577;&#19978;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#36825;&#20123;&#32467;&#26524;&#25490;&#38500;&#20102;&#21333;&#21333;&#22522;&#20110;&#32463;&#20856;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#25152;&#26377;&#21487;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21482;&#29992;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#25104;&#21151;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30340;&#38543;&#26426;&#21270;&#23454;&#39564;&#65292;&#23637;&#31034;&#20256;&#32479;&#30340;&#29702;&#35299;&#27867;&#21270;&#30340;&#26041;&#27861;&#26080;&#27861;&#35299;&#37322;&#36825;&#20123;&#37327;&#23376;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#20934;&#30830;&#22320;&#25311;&#21512;&#38543;&#26426;&#29366;&#24577;&#21644;&#38543;&#26426;&#35757;&#32451;&#25968;&#25454;&#30340;&#26631;&#35760;&#12290;&#36825;&#31181;&#35760;&#24518;&#38543;&#26426;&#25968;&#25454;&#30340;&#33021;&#21147;&#36829;&#21453;&#20102;&#24403;&#21069;&#23567;&#27867;&#21270;&#35823;&#24046;&#30340;&#27010;&#24565;&#65292;&#20351;&#24471;&#24314;&#31435;&#22312;VC&#32500;&#12289;Rademacher&#22797;&#26434;&#24230;&#21644;&#25152;&#26377;&#22343;&#21248;&#30456;&#20851;&#24615;&#24230;&#37327;&#22522;&#30784;&#19978;&#30340;&#26041;&#27861;&#26377;&#20123;&#26840;&#25163;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#29702;&#35770;&#26500;&#24314;&#34917;&#20805;&#20102;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#34920;&#26126;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23558;&#20219;&#24847;&#26631;&#35760;&#25311;&#21512;&#21040;&#37327;&#23376;&#29366;&#24577;&#19978;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24182;&#19981;&#25490;&#38500;&#21482;&#29992;&#23569;&#37327;&#35757;&#32451;&#25968;&#25454;&#23601;&#33021;&#33719;&#24471;&#33391;&#22909;&#27867;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#20294;&#26159;&#25490;&#38500;&#20102;&#21333;&#21333;&#22522;&#20110;&#32463;&#20856;&#22797;&#26434;&#24230;&#24230;&#37327;&#30340;&#25152;&#26377;&#21487;&#33021;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning models have shown successful generalization performance even when trained with few data. In this work, through systematic randomization experiments, we show that traditional approaches to understanding generalization fail to explain the behavior of such quantum models. Our experiments reveal that state-of-the-art quantum neural networks accurately fit random states and random labeling of training data. This ability to memorize random data defies current notions of small generalization error, problematizing approaches that build on complexity measures such as the VC dimension, the Rademacher complexity, and all their uniform relatives. We complement our empirical results with a theoretical construction showing that quantum neural networks can fit arbitrary labels to quantum states, hinting at their memorization ability. Our results do not preclude the possibility of good generalization with few training data but rather rule out any possible guarantees based only
&lt;/p&gt;</description></item><item><title>SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13092</link><description>&lt;p&gt;
&#20174;&#26032;&#30340;&#35282;&#24230;&#21387;&#32553;ImageNet&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;SRe$^2$L
&lt;/p&gt;
&lt;p&gt;
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13092
&lt;/p&gt;
&lt;p&gt;
SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65292;&#31216;&#20026;Squeeze&#12289;Recover&#21644;Relabel&#65288;SRe$^2$L&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#20998;&#31163;&#20102;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#35268;&#27169;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#20687;&#20219;&#24847;&#20998;&#36776;&#29575;&#12289;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;Tiny-ImageNet&#21644;&#23436;&#25972;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312;50IPC&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;42.5&#65285;&#21644;60.8&#65285;&#30340;&#26368;&#39640;&#39564;&#35777;&#31934;&#24230;&#65292;&#36739;&#20043;&#21069;&#25152;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#21644;32.9&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Res&#19978;&#20063;&#27604;MTT&#24555;&#32422;52&#20493;(ConvNet-4)&#21644;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#35757;&#32451;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#21487;&#34987;&#35777;&#26126;&#20026;&#21487;&#21387;&#32553;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#38750;&#24179;&#20961;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2306.08125</link><description>&lt;p&gt;
&#24102;&#26377;&#27785;&#37325;&#23614;&#37096;SGD&#35757;&#32451;&#30340;&#36807;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#21487;&#21387;&#32553;&#24615;
&lt;/p&gt;
&lt;p&gt;
Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD. (arXiv:2306.08125v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#35757;&#32451;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#21487;&#34987;&#35777;&#26126;&#20026;&#21487;&#21387;&#32553;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#38750;&#24179;&#20961;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20943;&#23569;&#35745;&#31639;&#38656;&#27714;&#21644;&#21387;&#32553;&#19982;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#31995;&#65292;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#25104;&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#30740;&#31350;&#23545;&#35937;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#21487;&#20197;&#24433;&#21709;&#23398;&#20064;&#21442;&#25968;&#21521;&#37327;&#30340;&#21387;&#32553;&#24615;&#12290;&#34429;&#28982;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#35757;&#32451;&#21160;&#24577;&#23545;&#21387;&#32553;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#26159;&#23427;&#20204;&#20381;&#36182;&#20110;&#19981;&#21487;&#39564;&#35777;&#30340;&#20551;&#35774;&#65292;&#30001;&#20110;&#38544;&#21547;&#24615;&#36136;&#65292;&#24471;&#20986;&#30340;&#29702;&#35770;&#24182;&#27809;&#26377;&#25552;&#20379;&#23454;&#29992;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;SGD&#20462;&#25913;&#26041;&#27861;&#65292;&#20351;&#24471;&#31639;&#27861;&#30340;&#36755;&#20986;&#33021;&#22815;&#34987;&#35777;&#26126;&#26159;&#21487;&#21387;&#32553;&#30340;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#38750;&#24179;&#20961;&#20551;&#35774;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20351;&#29992;SGD&#35757;&#32451;&#30340;&#21333;&#38544;&#34255;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#27880;&#20837;&#38468;&#21152;&#30340;&#27785;&#37325;&#23614;&#37096;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network compression has been an increasingly important subject, due to its practical implications in terms of reducing the computational requirements and its theoretical implications, as there is an explicit connection between compressibility and the generalization error. Recent studies have shown that the choice of the hyperparameters of stochastic gradient descent (SGD) can have an effect on the compressibility of the learned parameter vector. Even though these results have shed some light on the role of the training dynamics over compressibility, they relied on unverifiable assumptions and the resulting theory does not provide a practical guideline due to its implicitness. In this study, we propose a simple modification for SGD, such that the outputs of the algorithm will be provably compressible without making any nontrivial assumptions. We consider a one-hidden-layer neural network trained with SGD and we inject additive heavy-tailed noise to the iterates at each iteration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;SGD&#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#38408;&#20540;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;&#20854;&#19982;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#21576;&#21333;&#35843;&#38750;&#38477;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21487;&#33021;&#20250;&#24433;&#21709;SGD&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.07850</link><description>&lt;p&gt;
SGD&#30340;&#31934;&#30830;&#24179;&#22343;&#20108;&#27425;&#32447;&#24615;&#31283;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exact Mean Square Linear Stability Analysis for SGD. (arXiv:2306.07850v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;SGD&#31283;&#23450;&#24615;&#30340;&#31934;&#30830;&#38408;&#20540;&#34920;&#36798;&#24335;&#65292;&#21457;&#29616;&#20854;&#19982;&#25209;&#37327;&#22823;&#23567;&#20043;&#38388;&#21576;&#21333;&#35843;&#38750;&#38477;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21487;&#33021;&#20250;&#24433;&#21709;SGD&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#20248;&#21270;&#26041;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#26497;&#23567;&#20540;&#28857;&#38468;&#36817;&#30340;&#21160;&#24577;&#31283;&#23450;&#24615;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#23545;&#20110;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;GD&#65289;&#65292;&#31283;&#23450;&#30340;&#25910;&#25947;&#20165;&#21487;&#33021;&#21457;&#29983;&#22312;&#36275;&#22815;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#22788;&#65292;&#24182;&#19988;&#24050;&#32463;&#19982;&#35757;&#32451;&#27169;&#22411;&#30340;&#33391;&#22909;&#24615;&#33021;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;GD&#30340;&#31283;&#23450;&#24615;&#38408;&#20540;&#24050;&#32463;&#20247;&#25152;&#21608;&#30693;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#65292;&#23578;&#26410;&#25512;&#23548;&#20986;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#31934;&#30830;&#38408;&#20540;&#30340;&#26174;&#24335;&#34920;&#36798;&#24335;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#31181;&#23553;&#38381;&#24418;&#24335;&#30340;&#34920;&#36798;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#27493;&#38271;$\eta$&#30340;&#26174;&#24335;&#26465;&#20214;&#65292;&#26082;&#26159;SGD&#22312;&#22343;&#26041;&#24847;&#20041;&#19979;&#31283;&#23450;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#20063;&#26159;&#20805;&#20998;&#26465;&#20214;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25209;&#37327;&#22823;&#23567;$B$&#30340;&#31934;&#30830;&#20316;&#29992;&#65292;&#29305;&#21035;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#31283;&#23450;&#24615;&#38408;&#20540;&#26159;&#25209;&#37327;&#22823;&#23567;&#30340;&#21333;&#35843;&#38750;&#38477;&#20989;&#25968;&#65292;&#36825;&#24847;&#21619;&#30528;&#20943;&#23567;&#25209;&#37327;&#22823;&#23567;&#21482;&#20250;&#38477;&#20302;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;SGD&#30340;&#31283;&#23450;&#24615;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamical stability of optimization methods at the vicinity of minima of the loss has recently attracted significant attention. For gradient descent (GD), stable convergence is possible only to minima that are sufficiently flat w.r.t. the step size, and those have been linked with favorable properties of the trained model. However, while the stability threshold of GD is well-known, to date, no explicit expression has been derived for the exact threshold of stochastic GD (SGD). In this paper, we derive such a closed-form expression. Specifically, we provide an explicit condition on the step size $\eta$ that is both necessary and sufficient for the stability of SGD in the mean square sense. Our analysis sheds light on the precise role of the batch size $B$. Particularly, we show that the stability threshold is a monotonically non-decreasing function of the batch size, which means that reducing the batch size can only hurt stability. Furthermore, we show that SGD's stability threshold
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22855;&#30340;&#22312;&#32447;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#20284;&#23545;&#25239;&#26680;&#24182;&#20351;&#29992;&#26631;&#20934;&#38750;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#40065;&#26834;&#31574;&#30053;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.05859</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#25239;&#26680;&#36817;&#20284;&#23454;&#29616;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning via Adversarial Kernel Approximation. (arXiv:2306.05859v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05859
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22855;&#30340;&#22312;&#32447;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36817;&#20284;&#23545;&#25239;&#26680;&#24182;&#20351;&#29992;&#26631;&#20934;&#38750;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#23398;&#20064;&#40065;&#26834;&#31574;&#30053;&#65292;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#39640;&#32500;&#24230;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#36716;&#31227;&#26680;&#21457;&#29983;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#24207;&#21015;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22312;&#20855;&#26377;&#39640;&#32500;&#24230;&#22495;&#30340;&#29616;&#23454;&#22312;&#32447;&#29615;&#22659;&#20013;&#65292;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#25193;&#23637;&#12290;&#36890;&#36807;&#34920;&#24449;&#40065;&#26834;&#24615;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#23545;&#25239;&#26680;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#40065;&#26834;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#36817;&#20284;&#23545;&#25239;&#26680;&#24182;&#20351;&#29992;&#26631;&#20934;&#65288;&#38750;&#40065;&#26834;&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#23398;&#20064;&#19968;&#20010;&#40065;&#26834;&#24615;&#26041;&#38024;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#24230;&#22495;&#30340;&#36731;&#26494;&#25193;&#23637;&#12290;&#22312;&#32463;&#20856;&#25511;&#21046;&#20219;&#21153;&#12289;MinAtar&#21644;DeepMind&#25511;&#21046;&#22871;&#20214;&#20013;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust Markov Decision Processes (RMDPs) provide a framework for sequential decision-making that is robust to perturbations on the transition kernel. However, robust reinforcement learning (RL) approaches in RMDPs do not scale well to realistic online settings with high-dimensional domains. By characterizing the adversarial kernel in RMDPs, we propose a novel approach for online robust RL that approximates the adversarial kernel and uses a standard (non-robust) RL algorithm to learn a robust policy. Notably, our approach can be applied on top of any underlying RL algorithm, enabling easy scaling to high-dimensional domains. Experiments in classic control tasks, MinAtar and DeepMind Control Suite demonstrate the effectiveness and the applicability of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EfficientNet&#26550;&#26500;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.04288</link><description>&lt;p&gt;
&#22522;&#20110;&#20572;&#36710;&#22330;&#21344;&#29992;&#26816;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Revising deep learning methods in parking lot occupancy detection. (arXiv:2306.04288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;EfficientNet&#26550;&#26500;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#24182;&#22312;5&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20572;&#36710;&#22330;&#24341;&#23548;&#31995;&#32479;&#20316;&#20026;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#30340;&#19968;&#37096;&#20998;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#27969;&#34892;&#30340;&#36235;&#21183;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#20801;&#35768;&#39550;&#39542;&#21592;&#22312;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#25628;&#32034;&#21487;&#29992;&#20572;&#36710;&#20301;&#30340;&#31639;&#27861;&#12290;&#20256;&#32479;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#24212;&#29992;&#20110;&#25668;&#20687;&#22836;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#22312;&#29305;&#23450;&#30340;&#35270;&#35273;&#26465;&#20214;&#19979;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#21644;&#36866;&#24403;&#30340;&#27979;&#35797;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24191;&#27867;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#20572;&#36710;&#20301;&#21344;&#29992;&#26816;&#27979;&#31639;&#27861;&#65292;&#23558;&#23427;&#20204;&#30340;&#39044;&#27979;&#36136;&#37327;&#19982;&#26368;&#36817;&#20986;&#29616;&#30340;&#35270;&#35273;Transformer&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22522;&#20110;EfficientNet&#26550;&#26500;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31649;&#36947;&#12290;&#36827;&#34892;&#30340;&#35745;&#31639;&#23454;&#39564;&#24050;&#32463;&#35777;&#26126;&#65292;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#24615;&#33021;&#26377;&#20102;&#25552;&#39640;&#65292;&#35813;&#27169;&#22411;&#22312;5&#31181;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parking guidance systems have recently become a popular trend as a part of the smart cities' paradigm of development. The crucial part of such systems is the algorithm allowing drivers to search for available parking lots across regions of interest. The classic approach to this task is based on the application of neural network classifiers to camera records. However, existing systems demonstrate a lack of generalization ability and appropriate testing regarding specific visual conditions. In this study, we extensively evaluate state-of-the-art parking lot occupancy detection algorithms, compare their prediction quality with the recently emerged vision transformers, and propose a new pipeline based on EfficientNet architecture. Performed computational experiments have demonstrated the performance increase in the case of our model, which was evaluated on 5 different datasets.
&lt;/p&gt;</description></item><item><title>MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04120</link><description>&lt;p&gt;
MESSY&#20272;&#35745;&#65306;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
MESSY Estimation: Maximum-Entropy based Stochastic and Symbolic densitY Estimation. (arXiv:2306.04120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04120
&lt;/p&gt;
&lt;p&gt;
MESSY&#20272;&#35745;&#26041;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#26469;&#39640;&#25928;&#22320;&#25214;&#21040;&#26368;&#22823;&#29109;&#20998;&#24067;&#30340;&#21442;&#25968;&#65292;&#25903;&#25345;&#39640;&#32500;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#20110;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#30340;&#38543;&#26426;&#21644;&#31526;&#21495;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;MESSY&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#26799;&#24230;&#27969;&#30340;&#30697;&#23558;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#20174;&#26679;&#26412;&#20013;&#24674;&#22797;&#20026;&#31526;&#21495;&#34920;&#36798;&#24335;&#65292;&#24182;&#23558;ansatz&#20316;&#20026;&#39537;&#21160;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#26799;&#24230;&#30340;&#28418;&#31227;&#25193;&#25955;&#36807;&#31243;&#65292;&#23558;&#26410;&#30693;&#20998;&#24067;&#20989;&#25968;&#30340;&#26679;&#26412;&#19982;&#29468;&#27979;&#30340;&#31526;&#21495;&#34920;&#36798;&#24335;&#30456;&#36830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20986;&#24403;&#29468;&#27979;&#20998;&#24067;&#20855;&#26377;&#26368;&#22823;&#29109;&#24418;&#24335;&#26102;&#65292;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#25552;&#20379;&#30340;&#26679;&#26412;&#30340;&#30697;&#26500;&#24314;&#30340;&#32447;&#24615;&#26041;&#31243;&#32452;&#39640;&#25928;&#22320;&#25214;&#21040;&#35813;&#20998;&#24067;&#30340;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21495;&#22238;&#24402;&#26469;&#25506;&#32034;&#24179;&#28369;&#20989;&#25968;&#30340;&#31354;&#38388;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#26368;&#22823;&#29109;&#27867;&#20989;&#25351;&#25968;&#30340;&#26368;&#20248;&#22522;&#20989;&#25968;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#26465;&#20214;&#12290;&#35813;&#26041;&#27861;&#22312;&#38543;&#26426;&#25628;&#32034;&#30340;&#27599;&#27425;&#36845;&#20195;&#20013;&#30340;&#25104;&#26412;&#19982;&#26679;&#26412;&#25968;&#37327;&#21576;&#32447;&#24615;&#20851;&#31995;&#65292;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#20351;&#20854;&#21487;&#25193;&#23637;&#21040;&#39640;&#32500;&#38382;&#39064;&#12290;&#25968;&#20540;&#23454;&#39564;&#26174;&#31034;&#20986;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#26368;&#26032;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce MESSY estimation, a Maximum-Entropy based Stochastic and Symbolic densitY estimation method. The proposed approach recovers probability density functions symbolically from samples using moments of a Gradient flow in which the ansatz serves as the driving force. In particular, we construct a gradient-based drift-diffusion process that connects samples of the unknown distribution function to a guess symbolic expression. We then show that when the guess distribution has the maximum entropy form, the parameters of this distribution can be found efficiently by solving a linear system of equations constructed using the moments of the provided samples. Furthermore, we use Symbolic regression to explore the space of smooth functions and find optimal basis functions for the exponent of the maximum entropy functional leading to good conditioning. The cost of the proposed method in each iteration of the random search is linear with the number of samples and quadratic with the number 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.00809</link><description>&lt;p&gt;
&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#65306;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#26576;&#20123;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Initial Guessing Bias: How Untrained Networks Favor Some Classes. (arXiv:2306.00809v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#29616;&#35937;&#65292;&#21363;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#27169;&#22411;&#24448;&#24448;&#20250;&#23558;&#25152;&#26377;&#39044;&#27979;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#35813;&#29616;&#35937;&#23545;&#26550;&#26500;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#26377;&#23454;&#38469;&#25351;&#23548;&#24847;&#20041;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#21644;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#21021;&#22987;&#29366;&#24577;&#22312;&#35843;&#33410;&#21518;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#22312;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#21487;&#20197;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#29978;&#33267;&#22312;&#19981;&#23384;&#22312;&#26174;&#24335;&#20559;&#24046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#27169;&#22411;&#23558;&#25152;&#26377;&#39044;&#27979;&#37117;&#25351;&#21521;&#21516;&#19968;&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#23384;&#22312;&#65292;&#31216;&#20026;&#8220;&#21021;&#22987;&#29468;&#27979;&#20559;&#24046;&#8221;&#65288;Initial Guessing Bias&#65292;IGB&#65289;&#65292;&#36825;&#21462;&#20915;&#20110;&#26550;&#26500;&#36873;&#25321;&#65292;&#20363;&#22914;&#28608;&#27963;&#20989;&#25968;&#12289;&#26368;&#22823;&#27744;&#21270;&#23618;&#21644;&#32593;&#32476;&#28145;&#24230;&#12290;&#25105;&#20204;&#23545;IGB&#36827;&#34892;&#30340;&#20998;&#26512;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#65292;&#21487;&#20197;&#25351;&#23548;&#26550;&#26500;&#30340;&#36873;&#25321;&#21644;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#29702;&#35770;&#21518;&#26524;&#65292;&#20363;&#22914;&#33410;&#28857;&#32622;&#25442;&#23545;&#31216;&#24615;&#30340;&#23849;&#28291;&#12289;&#33258;&#24179;&#22343;&#30340;&#30772;&#22351;&#12289;&#26576;&#20123;&#22343;&#22330;&#36817;&#20284;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#28145;&#24230;&#24102;&#26469;&#30340;&#38750;&#24179;&#20961;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The initial state of neural networks plays a central role in conditioning the subsequent training dynamics. In the context of classification problems, we provide a theoretical analysis demonstrating that the structure of a neural network can condition the model to assign all predictions to the same class, even before the beginning of training, and in the absence of explicit biases. We show that the presence of this phenomenon, which we call "Initial Guessing Bias" (IGB), depends on architectural choices such as activation functions, max-pooling layers, and network depth. Our analysis of IGB has practical consequences, in that it guides architecture selection and initialization. We also highlight theoretical consequences, such as the breakdown of node-permutation symmetry, the violation of self-averaging, the validity of some mean-field approximations, and the non-trivial differences arising with depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#35270;&#35282;&#30340;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#35268;&#36991;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#21270;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.00353</link><description>&lt;p&gt;
&#20174;&#27010;&#29575;&#35282;&#24230;&#26500;&#24314;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Constructing Semantics-Aware Adversarial Examples with Probabilistic Perspective. (arXiv:2306.00353v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27010;&#29575;&#35270;&#35282;&#30340;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#24182;&#21487;&#20197;&#26377;&#25928;&#35268;&#36991;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#21270;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#29575;&#35270;&#35282;&#23545;&#25239;&#26679;&#26412;&#26500;&#24314;&#26041;&#27861;&#8212;&#8212;&#31665;&#32422;&#26463; Langevin Monte Carlo&#65288;LMC&#65289;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#29983;&#25104;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#20960;&#20309;&#36317;&#31163;&#25152;&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36873;&#25321;&#20102;&#35821;&#20041;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;&#20102;&#20010;&#20307;&#23558;&#20854;&#23545;&#35821;&#20041;&#30340;&#29702;&#35299;&#34701;&#20837;&#21040;&#27169;&#22411;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;&#20445;&#25345;&#20854;&#22266;&#26377;&#30340;&#21547;&#20041;&#12290;&#22312; MNIST &#21644; SVHN &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35821;&#20041;&#24863;&#30693;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#38024;&#23545;&#20256;&#32479;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#20581;&#24615;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce a novel, probabilistic viewpoint on adversarial examples, achieved through box-constrained Langevin Monte Carlo (LMC). Proceeding from this perspective, we develop an innovative approach for generating semantics-aware adversarial examples in a principled manner. This methodology transcends the restriction imposed by geometric distance, instead opting for semantic constraints. Our approach empowers individuals to incorporate their personal comprehension of semantics into the model. Through human evaluation, we validate that our semantics-aware adversarial examples maintain their inherent meaning. Experimental findings on the MNIST and SVHN datasets demonstrate that our semantics-aware adversarial examples can effectively circumvent robust adversarial training methods tailored for traditional adversarial attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;,&#24182;&#34920;&#26126;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;</title><link>http://arxiv.org/abs/2305.19259</link><description>&lt;p&gt;
Shuffle SGD&#24635;&#26159;&#27604;SGD&#26356;&#22909;&#65306;&#23545;&#20855;&#26377;&#20219;&#24847;&#25968;&#25454;&#39034;&#24207;&#30340;SGD&#36827;&#34892;&#25913;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Shuffle SGD is Always Better than SGD: Improved Analysis of SGD with Arbitrary Data Orders. (arXiv:2305.19259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;,&#24182;&#34920;&#26126;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#20248;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#38543;&#26426;&#37325;&#25490;&#65288;RR&#65289;&#21644;&#21333;&#27425;&#27927;&#29260;&#65288;SS&#65289;&#26159;&#36890;&#36807;&#24490;&#29615;&#36941;&#21382;&#35757;&#32451;&#25968;&#25454;&#30340;&#38543;&#26426;&#25110;&#21333;&#20010;&#25490;&#21015;&#30340;&#24120;&#35265;&#36873;&#25321;&#65292;&#28982;&#32780;&#36825;&#20123;&#31639;&#27861;&#22312;&#38750;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#29616;&#26377;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#30340;&#35757;&#32451;&#22330;&#26223;&#20013;&#65292;&#24403;&#26102;&#20195;&#30340;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#38598;&#22823;&#23567;&#26102;&#65292;RR&#21487;&#33021;&#34920;&#29616;&#19981;&#22914;SGD&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#31181;&#20801;&#35768;&#20219;&#24847;&#25968;&#25454;&#25490;&#24207;&#30340;&#26222;&#36890;SGD&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#20984;&#20989;&#25968;&#24773;&#20917;&#19979;&#30340;&#25913;&#36827;&#25910;&#25947;&#36895;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#38543;&#26426;&#21644;&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#27604;&#32463;&#20856;&#26367;&#25442;&#30340;SGD&#26356;&#24555;&#25110;&#33267;&#23569;&#19982;&#20854;&#19968;&#26679;&#22909;&#65292;&#26080;&#35770;&#36845;&#20195;&#27425;&#25968;&#22914;&#20309;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20984;&#26174;&#20102;&#20351;&#29992;&#38543;&#26426;/&#21333;&#27425;&#27927;&#29260;&#30340;SGD&#30340;&#22909;&#22788;&#65292;&#24182;&#20026;&#20854;&#38750;&#20984;&#25910;&#25947;&#24615;&#36136;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) algorithms are widely used in optimizing neural networks, with Random Reshuffling (RR) and Single Shuffle (SS) being popular choices for cycling through random or single permutations of the training data. However, the convergence properties of these algorithms in the non-convex case are not fully understood. Existing results suggest that, in realistic training scenarios where the number of epochs is smaller than the training set size, RR may perform worse than SGD.  In this paper, we analyze a general SGD algorithm that allows for arbitrary data orderings and show improved convergence rates for non-convex functions. Specifically, our analysis reveals that SGD with random and single shuffling is always faster or at least as good as classical SGD with replacement, regardless of the number of iterations. Overall, our study highlights the benefits of using SGD with random/single shuffling and provides new insights into its convergence properties for non-co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;BadLabel&#65292;&#23427;&#36890;&#36807;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#65288;LNL&#65289;&#31639;&#27861;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;LNL&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18377</link><description>&lt;p&gt;
BadLabel: &#35780;&#20272;&#19982;&#22686;&#24378;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#30340;&#40065;&#26834;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
BadLabel: A Robust Perspective on Evaluating and Enhancing Label-noise Learning. (arXiv:2305.18377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;BadLabel&#65292;&#23427;&#36890;&#36807;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#65288;LNL&#65289;&#31639;&#27861;&#24615;&#33021;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;LNL&#26041;&#27861;&#65292;&#34920;&#29616;&#20986;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25512;&#36827;&#23454;&#29992;&#30340;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#31639;&#27861;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;&#65292;&#20174;&#26465;&#20214;&#31867;&#22122;&#22768;&#21040;&#23454;&#20363;&#20381;&#36182;&#22122;&#22768;&#19981;&#31561;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26631;&#31614;&#22122;&#22768;&#31867;&#22411;BadLabel&#65292;&#21487;&#20197;&#36890;&#36807;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#26174;&#33879;&#38477;&#20302;&#29616;&#26377;LNL&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;BadLabel&#22522;&#20110;&#26631;&#20934;&#20998;&#31867;&#30340;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;&#36827;&#34892;&#21046;&#20316;&#65292;&#36873;&#25321;&#29305;&#23450;&#26679;&#26412;&#24182;&#23558;&#20854;&#26631;&#31614;&#32763;&#36716;&#20026;&#20854;&#20182;&#26631;&#31614;&#65292;&#20351;&#24471;&#24178;&#20928;&#26631;&#31614;&#21644;&#22122;&#22768;&#26631;&#31614;&#30340;&#25439;&#22833;&#20540;&#21464;&#24471;&#26080;&#27861;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;BadLabel&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;LNL&#26041;&#27861;&#65292;&#27599;&#20010;epoch&#23545;&#26631;&#31614;&#36827;&#34892;&#25932;&#23545;&#25200;&#21160;&#65292;&#20351;&#24178;&#20928;&#26631;&#31614;&#21644;&#22122;&#22768;&#26631;&#31614;&#30340;&#25439;&#22833;&#20540;&#20877;&#27425;&#21487;&#21306;&#20998;&#12290;&#19968;&#26086;&#36873;&#25321;&#20102;&#19968;&#23567;&#37096;&#20998;&#65288;&#22823;&#22810;&#25968;&#65289;&#24178;&#20928;&#26631;&#35760;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#21322;&#30417;&#30563;&#32763;&#35793;&#25216;&#26415;&#24212;&#29992;&#21040;LNL&#20013;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;BadLabel&#31867;&#22411;&#21644;&#25552;&#20986;&#30340;&#40065;&#26834;LNL&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;BadLabel&#21487;&#20197;&#23558;&#19971;&#20010;&#29616;&#26377;LNL&#31639;&#27861;&#30340;&#24615;&#33021;&#38477;&#20302;&#39640;&#36798;60&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#27604;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#19971;&#20010;&#29616;&#26377;LNL&#31639;&#27861;&#24615;&#33021;&#25552;&#39640;&#22810;&#36798;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label-noise learning (LNL) aims to increase the model's generalization given training data with noisy labels. To facilitate practical LNL algorithms, researchers have proposed different label noise types, ranging from class-conditional to instance-dependent noises. In this paper, we introduce a novel label noise type called BadLabel, which can significantly degrade the performance of existing LNL algorithms by a large margin. BadLabel is crafted based on the label-flipping attack against standard classification, where specific samples are selected and their labels are flipped to other labels so that the loss values of clean and noisy labels become indistinguishable. To address the challenge posed by BadLabel, we further propose a robust LNL method that perturbs the labels in an adversarial manner at each epoch to make the loss values of clean and noisy labels again distinguishable. Once we select a small set of (mostly) clean labeled data, we can apply the techniques of semi-supervised
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;ODE&#30340;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#23436;&#20840;&#30830;&#23450;&#24615;&#25277;&#26679;&#65292;&#38656;&#35201;&#28385;&#36275;$L^2$&#36817;&#20284;&#35823;&#24046;&#33539;&#22260;&#30340;&#35268;&#24459;&#24615;&#26465;&#20214;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.16860</link><description>&lt;p&gt;
&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#35823;&#24046;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Error Bounds for Flow Matching Methods. (arXiv:2305.16860v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;ODE&#30340;&#27969;&#21305;&#37197;&#26041;&#27861;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#36866;&#29992;&#20110;&#23436;&#20840;&#30830;&#23450;&#24615;&#25277;&#26679;&#65292;&#38656;&#35201;&#28385;&#36275;$L^2$&#36817;&#20284;&#35823;&#24046;&#33539;&#22260;&#30340;&#35268;&#24459;&#24615;&#26465;&#20214;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#26159;&#19968;&#31867;&#20381;&#36182;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#30340;&#27969;&#34892;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#12290;&#33258;&#20174;&#23427;&#20204;&#35806;&#29983;&#20197;&#26469;&#65292;&#23601;&#24050;&#32463;&#24847;&#35782;&#21040;&#21487;&#20197;&#20351;&#29992;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#32780;&#19981;&#26159;SDE&#36827;&#34892;&#29983;&#25104;&#12290;&#36825;&#23548;&#33268;&#20171;&#32461;&#20102;&#27010;&#29575;&#27969;ODE&#26041;&#27861;&#21644;&#21435;&#22122;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#12290;&#27969;&#21305;&#37197;&#26041;&#27861;&#26368;&#36817;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#20123;&#22522;&#20110;ODE&#30340;&#26041;&#27861;&#65292;&#24182;&#36817;&#20284;&#20110;&#20004;&#20010;&#20219;&#24847;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#27969;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#38024;&#23545;&#38543;&#26426;&#25277;&#26679;&#27169;&#24335;&#19979;&#30340;&#25193;&#25955;&#27169;&#22411;&#25512;&#23548;&#20102;&#36817;&#20284;&#35823;&#24046;&#30340;&#36793;&#30028;&#65292;&#20551;&#35774;$L^2$&#25439;&#22833;&#20855;&#26377;&#26576;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#23436;&#20840;&#30830;&#23450;&#24615;&#25277;&#26679;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#27969;&#21305;&#37197;&#36807;&#31243;&#30340;&#35823;&#24046;&#30028;&#38480;&#65292;&#20551;&#35774;$L^2$&#36817;&#20284;&#35823;&#24046;&#33539;&#22260;&#26377;&#19968;&#23450;&#30340;&#35268;&#24459;&#24615;&#26465;&#20214;&#21644;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models are a popular class of generative modelling techniques relying on stochastic differential equations (SDE). From their inception, it was realized that it was also possible to perform generation using ordinary differential equations (ODE) rather than SDE. This led to the introduction of the probability flow ODE approach and denoising diffusion implicit models. Flow matching methods have recently further extended these ODE-based approaches and approximate a flow between two arbitrary probability distributions. Previous work derived bounds on the approximation error of diffusion models under the stochastic sampling regime, given assumptions on the $L^2$ loss. We present error bounds for the flow matching procedure using fully deterministic sampling, assuming an $L^2$ bound on the approximation error and a certain regularity condition on the data distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#22312;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#20043;&#21069;&#28155;&#21152;&#19968;&#20010;dropout&#23618;&#65292;&#21487;&#20197;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16179</link><description>&lt;p&gt;
Dropout&#21487;&#20197;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Dropout Drops Double Descent. (arXiv:2305.16179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#22312;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#20043;&#21069;&#28155;&#21152;&#19968;&#20010;dropout&#23618;&#65292;&#21487;&#20197;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#24182;&#20998;&#26512;&#65292;&#36890;&#36807;&#22312;&#20840;&#36830;&#25509;&#32447;&#24615;&#23618;&#20043;&#21069;&#28155;&#21152;&#19968;&#20010;dropout&#23618;&#65292;&#21487;&#20197;&#36731;&#26494;&#22320;&#32531;&#35299;&#21452;&#19979;&#38477;&#29616;&#35937;&#12290;&#21452;&#19979;&#38477;&#29616;&#35937;&#22312;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#65292;&#21363;&#38543;&#30528;&#26679;&#26412;&#25110;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#39044;&#27979;&#35823;&#24046;&#20250;&#20808;&#19978;&#21319;&#20877;&#19979;&#38477;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#20010;&#26041;&#38754;&#35777;&#26126;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#21644;&#38750;&#32447;&#24615;&#38543;&#26426;&#29305;&#24449;&#22238;&#24402;&#20013;&#20351;&#29992;&#26368;&#20339;&#30340;dropout&#65292;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we find and analyze that we can easily drop the double descent by only adding one dropout layer before the fully-connected linear layer. The surprising double-descent phenomenon has drawn public attention in recent years, making the prediction error rise and drop as we increase either sample or model size. The current paper shows that it is possible to alleviate these phenomena by using optimal dropout in the linear regression model and the nonlinear random feature regression, both theoretically and empirically. % ${y}=X{\beta}^0+{\epsilon}$ with $X\in\mathbb{R}^{n\times p}$. We obtain the optimal dropout hyperparameter by estimating the ground truth ${\beta}^0$ with generalized ridge typed estimator $\hat{{\beta}}=(X^TX+\alpha\cdot\mathrm{diag}(X^TX))^{-1}X^T{y}$. Moreover, we empirically show that optimal dropout can achieve a monotonic test error curve in nonlinear neural networks using Fashion-MNIST and CIFAR-10. Our results suggest considering dropout for risk curve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#25289;&#32461;&#33945;&#38598;&#21512;&#20013;&#20934;&#30830;&#24615;&#31867;&#20284;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#29305;&#24449;&#20132;&#20114;&#20998;&#25968;&#65288;FIS&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30456;&#36739;&#20110;&#20174;&#21333;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#20132;&#20114;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#26356;&#20026;&#21487;&#38752;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.10181</link><description>&lt;p&gt;
&#25506;&#32034;&#25289;&#32461;&#33945;&#38598;&#21512;&#20013;&#29305;&#24449;&#20132;&#20114;&#24471;&#20998;&#30340;&#20113;
&lt;/p&gt;
&lt;p&gt;
Exploring the cloud of feature interaction scores in a Rashomon set. (arXiv:2305.10181v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#25289;&#32461;&#33945;&#38598;&#21512;&#20013;&#20934;&#30830;&#24615;&#31867;&#20284;&#30340;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#29305;&#24449;&#20132;&#20114;&#20998;&#25968;&#65288;FIS&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#30456;&#36739;&#20110;&#20174;&#21333;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#29305;&#24449;&#20132;&#20114;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#26356;&#20026;&#21487;&#38752;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34892;&#20026;&#30340;&#26680;&#24515;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#21333;&#20010;&#39044;&#27979;&#27169;&#22411;&#20013;&#26816;&#27979;&#21644;&#37327;&#21270;&#29305;&#24449;&#20132;&#20114;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#20174;&#21333;&#20010;&#39044;&#20808;&#25351;&#23450;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20132;&#20114;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#22240;&#20026;&#65306;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#39044;&#27979;&#27169;&#22411;&#21487;&#33021;&#19981;&#20250;&#20445;&#30041;&#30495;&#23454;&#30340;&#29305;&#24449;&#20132;&#20114;&#65292;&#23384;&#22312;&#22810;&#20010;&#22312;&#29305;&#24449;&#20132;&#20114;&#24378;&#24230;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#20294;&#20934;&#30830;&#24615;&#30456;&#36817;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#20934;&#30830;&#24615;&#36817;&#20284;&#30456;&#31561;&#30340;&#39044;&#27979;&#27169;&#22411;&#31867;&#20013;&#25506;&#32034;&#29305;&#24449;&#20132;&#20114;&#24378;&#24230;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#25289;&#32461;&#33945;&#38598;&#21512;&#30340;&#19978;&#19979;&#25991;&#20013;&#24341;&#20837;&#20102;&#29305;&#24449;&#20132;&#20114;&#20998;&#25968;&#65288;FIS&#65289;&#65292;&#34920;&#31034;&#22312;&#32473;&#23450;&#20219;&#21153;&#19978;&#23454;&#29616;&#31867;&#20284;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#19988;&#23454;&#29992;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#27169;&#22411;&#31867;&#20013;&#30340;FIS&#12290;&#25105;&#20204;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#23637;&#31034;&#20102;FIS&#30340;&#23646;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19982;&#21333;&#20010;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#20132;&#20114;&#29305;&#24449;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions among features are central to understanding the behavior of machine learning models. Recent research has made significant strides in detecting and quantifying feature interactions in single predictive models. However, we argue that the feature interactions extracted from a single pre-specified model may not be trustworthy since: a well-trained predictive model may not preserve the true feature interactions and there exist multiple well-performing predictive models that differ in feature interaction strengths. Thus, we recommend exploring feature interaction strengths in a model class of approximately equally accurate predictive models. In this work, we introduce the feature interaction score (FIS) in the context of a Rashomon set, representing a collection of models that achieve similar accuracy on a given task. We propose a general and practical algorithm to calculate the FIS in the model class. We demonstrate the properties of the FIS via synthetic data and draw connecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09859</link><description>&lt;p&gt;
&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Smaller Language Models are Better Black-box Machine-Generated Text Detectors. (arXiv:2305.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21487;&#20197;&#26356;&#21152;&#31934;&#30830;&#22320;&#26816;&#27979;&#20986;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#35821;&#26009;&#24211;&#24182;&#19981;&#20250;&#23545;&#26816;&#27979;&#24615;&#33021;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27969;&#30021;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#21487;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#38750;&#24120;&#30456;&#20284;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#35805;&#35821;&#65292;&#22240;&#27492;&#21306;&#20998;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#30340;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#30340;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#35780;&#35770;&#24182;&#27169;&#20223;&#26576;&#20123;&#20316;&#32773;&#21644;&#20154;&#29289;&#12290;&#20026;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340; logits&#65292;&#25110;&#38656;&#35201;&#21487;&#20197;&#20174;&#30446;&#26631;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#33021;&#21147;&#12290;&#20854;&#20013;&#19968;&#31181;&#40657;&#21283;&#23376;&#26816;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#35266;&#23519;&#21040;&#29983;&#25104;&#25991;&#26412;&#22312;&#29983;&#25104;&#22120;&#30340;&#20284;&#28982;&#20989;&#25968;&#19979;&#26159;&#23616;&#37096;&#26368;&#20248;&#30340;&#65292;&#32780;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21017;&#19981;&#26159;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24635;&#20307;&#32780;&#35328;&#65292;&#36739;&#23567;&#19988;&#37096;&#20998;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#21512;&#20316;&#20026;&#36890;&#29992;&#25991;&#26412;&#26816;&#27979;&#22120;&#65306;&#23427;&#20204;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#26816;&#27979;&#26469;&#33258;&#23567;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#21644;&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#30456;&#21516;&#30340;&#26550;&#26500;&#25110;&#30456;&#21516;&#30340;&#35821;&#26009;&#24211;&#23545;&#26816;&#27979;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#22826;&#38451;&#32768;&#26001;&#26631;&#31614;&#12290;&#23427;&#21487;&#29992;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#28436;&#21270;&#20197;&#21450;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#20110;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.09492</link><description>&lt;p&gt;
&#31354;&#38388;&#22825;&#27668;&#30740;&#31350;&#20013;&#30340;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22330;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Solar Active Region Magnetogram Image Dataset for Studies of Space Weather. (arXiv:2305.09492v1 [astro-ph.SR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#22826;&#38451;&#27963;&#21160;&#21306;&#30913;&#22270;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#22826;&#38451;&#32768;&#26001;&#26631;&#31614;&#12290;&#23427;&#21487;&#29992;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#28436;&#21270;&#20197;&#21450;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#65292;&#24182;&#23545;&#20110;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#32654;&#22269;&#22269;&#23478;&#33322;&#31354;&#33322;&#22825;&#23616;&#65288;NASA&#65289;&#22826;&#38451;&#21160;&#21147;&#23398;&#35266;&#27979;&#21355;&#26143;&#65288;SDO&#65289;&#30340;&#30913;&#22270;&#65288;&#34913;&#37327;&#30913;&#22330;&#24378;&#24230;&#30340;&#22270;&#20687;&#65289;&#30340;&#20840;&#38754;&#25910;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#26469;&#33258;&#19977;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#25552;&#20379;SDO&#22320;&#38663;&#23398;&#21644;&#30913;&#23398;&#20202;&#65288;HMI&#65289;&#22826;&#38451;&#27963;&#36291;&#21306;&#65288;&#22823;&#30913;&#36890;&#21306;&#22495;&#65292;&#36890;&#24120;&#26159;&#29190;&#21457;&#20107;&#20214;&#30340;&#28304;&#22836;&#65289;&#30340;&#30913;&#22270;&#20197;&#21450;&#30456;&#24212;&#32768;&#26001;&#27963;&#21160;&#30340;&#26631;&#31614;&#12290;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#30740;&#31350;&#30913;&#32467;&#26500;&#12289;&#20854;&#38543;&#26102;&#38388;&#30340;&#28436;&#21270;&#20197;&#21450;&#19982;&#22826;&#38451;&#32768;&#26001;&#30340;&#20851;&#31995;&#30340;&#22270;&#20687;&#20998;&#26512;&#25110;&#22826;&#38451;&#29289;&#29702;&#23398;&#30740;&#31350;&#23558;&#38750;&#24120;&#26377;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#23545;&#37027;&#20123;&#30740;&#31350;&#33258;&#21160;&#22826;&#38451;&#32768;&#26001;&#39044;&#27979;&#26041;&#27861;&#30340;&#30740;&#31350;&#20154;&#21592;&#20135;&#29983;&#20852;&#36259;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#32463;&#20856;&#21644;&#28145;&#24230;&#65289;&#12289;&#20108;&#20803;&#21644;&#22810;&#31867;&#20998;&#31867;&#20197;&#21450;&#22238;&#24402;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#26368;&#23567;&#22788;&#29702;&#19988;&#29992;&#25143;&#21487;&#37197;&#32622;&#30340;&#19968;&#33268;&#22823;&#23567;&#22826;&#38451;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this dataset we provide a comprehensive collection of magnetograms (images quantifying the strength of the magnetic field) from the National Aeronautics and Space Administration's (NASA's) Solar Dynamics Observatory (SDO). The dataset incorporates data from three sources and provides SDO Helioseismic and Magnetic Imager (HMI) magnetograms of solar active regions (regions of large magnetic flux, generally the source of eruptive events) as well as labels of corresponding flaring activity. This dataset will be useful for image analysis or solar physics research related to magnetic structure, its evolution over time, and its relation to solar flares. The dataset will be of interest to those researchers investigating automated solar flare prediction methods, including supervised and unsupervised machine learning (classical and deep), binary and multi-class classification, and regression. This dataset is a minimally processed, user configurable dataset of consistently sized images of sola
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PITT&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;PDE&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.08757</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer
&lt;/p&gt;
&lt;p&gt;
Physics Informed Token Transformer. (arXiv:2305.08757v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PITT&#30340;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#29289;&#29702;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;PDE&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#26159;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#34429;&#28982;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#65292;&#20294;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21364;&#24448;&#24448;&#26080;&#27861;&#23436;&#25972;&#22320;&#34701;&#20837;&#31995;&#32479;&#20449;&#24687;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;Transformer&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#24182;&#22312;PDE&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30446;&#21069;Transformer&#32570;&#20047;&#19982;&#29289;&#29702;&#21644;&#25512;&#29702;&#30340;&#25972;&#21512;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;PITT&#65306;&#29289;&#29702;&#20449;&#24687;&#21270;&#30340;Token Transformer&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;PITT&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#23558;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#23884;&#20837;&#23398;&#20064;&#36807;&#31243;&#20013;&#26469;&#34701;&#20837;&#29289;&#29702;&#30693;&#35782;&#12290;PITT&#20351;&#29992;&#26041;&#31243;&#26631;&#35760;&#21270;&#26041;&#27861;&#26469;&#23398;&#20064;&#20998;&#26512;&#39537;&#21160;&#30340;&#25968;&#20540;&#26356;&#26032;&#36816;&#31639;&#31526;&#12290;&#36890;&#36807;&#26631;&#35760;&#21270;PDEs&#21644;&#23884;&#20837;&#20559;&#23548;&#25968;&#65292;Transformer&#27169;&#22411;&#21487;&#20197;&#24847;&#35782;&#21040;&#29289;&#29702;&#36807;&#31243;&#30340;&#22522;&#26412;&#30693;&#35782;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#28857;&#65292;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;PITT&#22312;&#22810;&#20010;PDE&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21644;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving Partial Differential Equations (PDEs) is the core of many fields of science and engineering. While classical approaches are often prohibitively slow, machine learning models often fail to incorporate complete system information. Over the past few years, transformers have had a significant impact on the field of Artificial Intelligence and have seen increased usage in PDE applications. However, despite their success, transformers currently lack integration with physics and reasoning. This study aims to address this issue by introducing PITT: Physics Informed Token Transformer. The purpose of PITT is to incorporate the knowledge of physics by embedding partial differential equations (PDEs) into the learning process. PITT uses an equation tokenization method to learn an analytically-driven numerical update operator. By tokenizing PDEs and embedding partial derivatives, the transformer models become aware of the underlying knowledge behind physical processes. To demonstrate this, P
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06989</link><description>&lt;p&gt;
&#36229;&#27969;&#20307;&#30340;&#31070;&#32463;&#27874;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#27874;&#20989;&#25968;&#26041;&#27861;&#30740;&#31350;&#20102;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#36229;&#27969;&#65292;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;FermiNet&#27169;&#22411;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#33719;&#24471;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#36229;&#27969;&#24615;&#20173;&#28982;&#26159;&#20957;&#32858;&#24577;&#29289;&#29702;&#30340;&#19968;&#20010;&#20027;&#35201;&#30446;&#26631;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#24320;&#21457;&#30340;&#36153;&#31859;&#31070;&#32463;&#32593;&#32476;&#65288;FermiNet&#65289;&#27874;&#20989;&#25968;Ansatz&#36827;&#34892;&#21464;&#20998;&#33945;&#29305;&#21345;&#27931;&#35745;&#31639;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#24378;&#28872;&#30701;&#31243;&#21452;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;&#31995;&#32479;-- &#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#65292;&#35813;&#31995;&#32479;&#24050;&#30693;&#23384;&#22312;&#36229;&#27969;&#22522;&#24577;&#65292;&#20294;&#38590;&#20197;&#23450;&#37327;&#25551;&#36848;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30740;&#31350;&#22343;&#21248;&#36153;&#31859;&#27668;&#20307;&#26102;FermiNet Ansatz&#30340;&#20851;&#38190;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20462;&#25913;&#65292;&#20854;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#21407;&#22987;FermiNet&#65292;&#21487;&#20197;&#32473;&#20986;&#39640;&#24230;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25968;&#23398;&#35777;&#26126;&#20102;&#26032;&#30340;Ansatz&#26159;&#21407;&#22987;FermiNet&#20307;&#31995;&#32467;&#26500;&#30340;&#20005;&#26684;&#27010;&#25324;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;FermiNet&#20849;&#20139;&#20960;&#20010;&#20248;&#21183;:&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#28040;&#38500;&#20102;&#24213;&#23618;&#22522;&#32452;&#30340;&#38656;&#27714;;&#32593;&#32476;&#30340;&#28789;&#27963;&#24615;&#22312;&#21464;&#20998;&#37327;&#23376;Monte Carlo&#20013;&#20135;&#29983;&#20102;&#26497;&#20854;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#19988;&#21487;&#29992;&#20110;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.00521</link><description>&lt;p&gt;
StyleLipSync&#65306;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#19988;&#21487;&#29992;&#20110;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StyleLipSync&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#30340;&#20010;&#24615;&#21270;&#21767;&#24418;&#21160;&#30011;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20174;&#20219;&#24847;&#38899;&#39057;&#29983;&#25104;&#26080;&#20851;&#36523;&#20221;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#12290;&#20026;&#20102;&#29983;&#25104;&#20219;&#24847;&#36523;&#20221;&#30340;&#35270;&#39057;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#22521;&#35757;&#30340;StyleGAN&#30340;&#35821;&#20041;&#20016;&#23500;&#28508;&#31354;&#38388;&#20013;&#30340;&#34920;&#36798;&#24615;&#21767;&#37096;&#20808;&#39564;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#21464;&#25442;&#35774;&#35745;&#35270;&#39057;&#19968;&#33268;&#24615;&#12290;&#19982;&#20197;&#24448;&#30340;&#21767;&#24418;&#21516;&#27493;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23039;&#24577;&#24863;&#30693;&#36974;&#32617;&#65292;&#36890;&#36807;&#36880;&#24103;&#21033;&#29992;&#19977;&#32500;&#21442;&#25968;&#21270;&#32593;&#26684;&#39044;&#27979;&#22120;&#21160;&#24577;&#23450;&#20301;&#36974;&#32617;&#65292;&#25552;&#39640;&#20102;&#24103;&#38388;&#33258;&#28982;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#19981;&#38656;&#35201;&#25968;&#25454;&#30340;&#21767;&#24418;&#21516;&#27493;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21516;&#27493;&#27491;&#21017;&#21270;&#22120;&#26469;&#20445;&#30041;&#21767;&#24418;&#21516;&#27493;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#22686;&#24378;&#20154;&#29289;&#29305;&#23450;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21767;&#24418;&#21516;&#27493;&#35270;&#39057;&#65292;&#29978;&#33267;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#22686;&#24378;&#26410;&#35265;&#38754;&#23380;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a fe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20445;&#30495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#38480;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#28385;&#36275;&#30456;&#21516;&#29289;&#29702;&#23450;&#24459;&#25110;&#20855;&#22791;&#29289;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#31561;&#29289;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03894</link><description>&lt;p&gt;
&#22810;&#20445;&#30495;&#24230;&#26041;&#27861;&#24212;&#29992;&#20110;&#29289;&#29702;&#31995;&#32479;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A multifidelity approach to continual learning for physical systems. (arXiv:2304.03894v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03894
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20445;&#30495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#38480;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#28385;&#36275;&#30456;&#21516;&#29289;&#29702;&#23450;&#24459;&#25110;&#20855;&#22791;&#29289;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#31561;&#29289;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#20445;&#30495;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23398;&#20064;&#21069;&#20960;&#20010;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#21644;&#24403;&#21069;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#26399;&#26395;&#36755;&#20986;&#32467;&#26524;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#22810;&#20445;&#30495;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26412;&#36523;&#34920;&#29616;&#20986;&#40065;&#26834;&#32467;&#26524;&#65292;&#21487;&#20197;&#38480;&#21046;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22810;&#20445;&#30495;&#26041;&#27861;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65288;&#21253;&#25324;&#37325;&#25918;&#21644;&#35760;&#24518;&#24863;&#30693;&#31361;&#35302;&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#36827;&#19968;&#27493;&#38480;&#21046;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#35813;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#28385;&#36275;&#27599;&#20010;&#39046;&#22495;&#19978;&#30456;&#21516;&#29289;&#29702;&#23450;&#24459;&#25110;&#20855;&#22791;&#29289;&#29702;&#23398;&#20808;&#39564;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#31561;&#29289;&#29702;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39044;&#35745;&#21069;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#32467;&#26524;&#19982;&#24403;&#21069;&#35757;&#32451;&#39046;&#22495;&#20013;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel continual learning method based on multifidelity deep neural networks. This method learns the correlation between the output of previously trained models and the desired output of the model on the current training dataset, limiting catastrophic forgetting. On its own the multifidelity continual learning method shows robust results that limit forgetting across several datasets. Additionally, we show that the multifidelity method can be combined with existing continual learning methods, including replay and memory aware synapses, to further limit catastrophic forgetting. The proposed continual learning method is especially suited for physical problems where the data satisfy the same physical laws on each domain, or for physics-informed neural networks, because in these cases we expect there to be a strong correlation between the output of the previous model and the model on the current training domain.
&lt;/p&gt;</description></item><item><title>R^2&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#25928;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20854;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.08253</link><description>&lt;p&gt;
R^2: &#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#21387;&#32553;&#19982;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
R^2: Range Regularization for Model Compression and Quantization. (arXiv:2303.08253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08253
&lt;/p&gt;
&lt;p&gt;
R^2&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21306;&#38388;&#27491;&#21017;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26377;&#25928;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#20854;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#27491;&#21017;&#21270;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#20063;&#21487;&#29992;&#20110;&#35843;&#25972;&#26435;&#37325;&#20998;&#24067;&#20197;&#36798;&#21040;&#21508;&#31181;&#30446;&#30340;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#26435;&#37325;&#27491;&#21017;&#21270;&#26469;&#36741;&#21161;&#27169;&#22411;&#37327;&#21270;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#21306;&#38388;&#27491;&#21017;&#21270;(R^2)&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#20248;&#21270;&#30340;&#36136;&#37327;&#65292;&#37325;&#28857;&#25918;&#22312;&#38450;&#27490;&#24322;&#24120;&#20540;&#26041;&#38754;&#12290;&#36890;&#36807;&#26377;&#25928;&#22320;&#35843;&#25972;&#20998;&#24067;&#20013;&#30340;&#26368;&#23567;&#20540;&#21644;&#26368;&#22823;&#20540;&#65292;&#23558;&#25972;&#20010;&#20998;&#24067;&#22609;&#36896;&#25104;&#32039;&#20945;&#30340;&#24418;&#29366;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#21387;&#32553;&#21644;&#37327;&#21270;&#25216;&#26415;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#23427;&#20204;&#26377;&#38480;&#30340;&#25968;&#20540;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;L-inf&#27491;&#21017;&#21270;&#65292;&#20854;&#25193;&#23637;&#38388;&#38548;&#27491;&#21017;&#21270;&#21644;&#26032;&#30340;soft-min-max&#27491;&#21017;&#21270;&#65292;&#20316;&#20026;&#20840;&#31934;&#24230;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30340;&#27491;&#21017;&#21270;&#25439;&#22833;&#12290;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#37327;&#21270;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#21033;&#29992;R^2&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#20302;&#20301;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model parameter regularization is a widely used technique to improve generalization, but also can be used to shape the weight distributions for various purposes. In this work, we shed light on how weight regularization can assist model quantization and compression techniques, and then propose range regularization (R^2) to further boost the quality of model optimization by focusing on the outlier prevention. By effectively regulating the minimum and maximum weight values from a distribution, we mold the overall distribution into a tight shape so that model compression and quantization techniques can better utilize their limited numeric representation powers. We introduce L-inf regularization, its extension margin regularization and a new soft-min-max regularization to be used as a regularization loss during full-precision model training. Coupled with state-of-the-art quantization and compression techniques, models trained with R^2 perform better on an average, specifically at lower bit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#26356;&#24555;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#19988;&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2302.11628</link><description>&lt;p&gt;
&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#65306;&#19968;&#31181;&#24555;&#36895;&#30340;&#23545;$\ell_0$&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks. (arXiv:2302.11628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#30340;&#35748;&#35777;&#38450;&#24481;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25239;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#12290;&#19982;&#29616;&#26377;&#38450;&#24481;&#26041;&#27861;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#26356;&#24555;&#65292;&#25552;&#20379;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#19988;&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#30340;&#25110;$\ell_0$&#23545;&#25239;&#25915;&#20987;&#20250;&#20219;&#24847;&#25200;&#21160;&#26410;&#30693;&#30340;&#29305;&#24449;&#23376;&#38598;&#12290;$\ell_0$&#40065;&#26834;&#24615;&#20998;&#26512;&#29305;&#21035;&#36866;&#29992;&#20110;&#24322;&#26500;&#65288;&#34920;&#26684;&#65289;&#25968;&#25454;&#65292;&#20854;&#20013;&#29305;&#24449;&#20855;&#26377;&#19981;&#21516;&#30340;&#31867;&#22411;&#25110;&#23610;&#24230;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;$\ell_0$&#35748;&#35777;&#38450;&#24481;&#22522;&#20110;&#38543;&#26426;&#24179;&#28369;&#65292;&#24182;&#20165;&#36866;&#29992;&#20110;&#36867;&#36991;&#25915;&#20987;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#29305;&#24449;&#20998;&#21306;&#32858;&#21512;&#65288;FPA&#65289;--&#19968;&#31181;&#38024;&#23545;$\ell_0$&#36867;&#36991;&#12289;&#21518;&#38376;&#21644;&#27745;&#26579;&#25915;&#20987;&#30340;&#35748;&#35777;&#38450;&#24481;&#12290;FPA&#36890;&#36807;&#38598;&#25104;&#29983;&#25104;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#20445;&#35777;&#65292;&#20854;&#23376;&#27169;&#22411;&#26159;&#22312;&#19981;&#30456;&#20132;&#30340;&#29305;&#24449;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;$\ell_0$&#38450;&#24481;&#30456;&#27604;&#65292;FPA&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#36798;3000&#20493;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#20013;&#20301;&#25968;&#40065;&#26834;&#24615;&#20445;&#35777;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;CIFAR10&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;13&#20687;&#32032;&#65292;MNIST&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;12&#20687;&#32032;&#65292;Weather&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;4&#20010;&#29305;&#24449;&#65292;Ames&#30340;&#20013;&#20301;&#25968;&#35777;&#20070;&#20026;3&#20010;&#29305;&#24449;&#65289;&#65292;&#36825;&#24847;&#21619;&#30528;FPA&#33021;&#22815;&#20813;&#36153;&#25552;&#20379;&#39069;&#22806;&#30340;&#40065;&#26834;&#24615;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.00722</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#32508;&#36848;&#65306;&#20174;&#28608;&#27963;&#20989;&#25968;&#21040;Transformer
&lt;/p&gt;
&lt;p&gt;
A Survey of Deep Learning: From Activations to Transformers. (arXiv:2302.00722v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00722
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#21253;&#25324;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#20197;&#21450;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#24635;&#32467;&#20102;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#65292;&#24182;&#35752;&#35770;&#20102;&#26368;&#36817;&#30340;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#26550;&#26500;&#12289;&#23618;&#12289;&#30446;&#26631;&#21644;&#20248;&#21270;&#25216;&#26415;&#30340;&#28044;&#29616;&#12290;&#36825;&#20123;&#21253;&#25324;&#20851;&#27880;&#26426;&#21046;&#12289;&#24402;&#19968;&#21270;&#12289;&#36339;&#36291;&#36830;&#25509;&#12289;Transformer&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#31561;&#22810;&#31181;&#21464;&#20307;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21521;&#20855;&#26377;&#28145;&#24230;&#23398;&#20064;&#22522;&#26412;&#29702;&#35299;&#30340;&#20154;&#25552;&#20379;&#23545;&#36825;&#20123;&#39046;&#22495;&#20013;&#26368;&#26032;&#37325;&#35201;&#36129;&#29486;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#26399;&#26395;&#26159;&#36890;&#36807;&#23545;&#37325;&#35201;&#26368;&#26032;&#20316;&#21697;&#30340;&#32508;&#21512;&#21644;&#20840;&#38754;&#30340;&#25506;&#35752;&#65292;&#20419;&#36827;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20043;&#38388;&#24418;&#25104;&#26032;&#30340;&#32852;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#35752;&#35770;&#20013;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36807;&#21435;&#21313;&#24180;&#20013;&#35768;&#22810;&#25104;&#21151;&#21019;&#26032;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23545;&#26368;&#36817;&#19968;&#20123;&#21830;&#19994;&#38381;&#28304;&#27169;&#22411;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#20363;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;PaLM 2&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decade has witnessed remarkable advancements in deep learning, owing to the emergence of various architectures, layers, objectives, and optimization techniques. These consist of a multitude of variations of attention, normalization, skip connections, transformer, and self-supervised learning methods, among others. Our goal is to furnish a comprehensive survey of significant recent contributions in these domains to individuals with a fundamental grasp of deep learning. Our aspiration is that an integrated and comprehensive approach of influential recent works will facilitate the formation of new connections between different areas of deep learning. In our discussion, we discuss multiple patterns that summarize the key strategies for many of the successful innovations over the last decade. We also include a discussion on recent commercially built, closed-source models such as OpenAI's GPT-4 and Google's PaLM 2.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;NetEffect&#65292;&#19968;&#31181;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#29702;&#35299;&#20855;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#20013;&#30340;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;&#65288;&#22914;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#25110;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25928;&#24212;&#26469;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.00270</link><description>&lt;p&gt;
&#21457;&#29616;&#21644;&#21033;&#29992;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Discovery and Exploitation of Generalized Network Effects. (arXiv:2301.00270v2 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00270
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NetEffect&#65292;&#19968;&#31181;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#21644;&#29702;&#35299;&#20855;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#20013;&#30340;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;&#65288;&#22914;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#25110;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25928;&#24212;&#26469;&#25913;&#36827;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20855;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22823;&#22411;&#22270;&#65292;&#25105;&#20204;&#22914;&#20309;&#65288;a&#65289;&#30830;&#23450;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#24191;&#20041;&#32593;&#32476;&#25928;&#24212;&#65288;GNE&#65289;&#65292;&#65288;b&#65289;&#20272;&#35745;GNE&#20197;&#35299;&#37322;&#33410;&#28857;&#31867;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#20197;&#21450;&#65288;c&#65289;&#21033;&#29992;GNE&#25913;&#36827;&#35832;&#22914;&#20934;&#30830;&#39640;&#25928;&#22320;&#39044;&#27979;&#26410;&#30693;&#26631;&#31614;&#31561;&#19979;&#28216;&#20219;&#21153;&#65311;&#23545;&#20110;&#33410;&#28857;&#20998;&#31867;&#21644;&#23450;&#21521;&#24191;&#21578;&#31561;&#21508;&#31181;&#20219;&#21153;&#65292;&#20102;&#35299;GNE&#26159;&#24456;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33410;&#28857;&#26631;&#31614;&#26377;&#38480;&#19988;&#36793;&#32536;&#22122;&#38899;&#65292;&#35782;&#21035;&#21644;&#29702;&#35299;GNE&#65288;&#22914;&#21516;&#36136;&#24615;&#12289;&#24322;&#36136;&#24615;&#25110;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NetEffect&#65292;&#19968;&#31181;&#22270;&#25366;&#25496;&#26041;&#27861;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#22522;&#20110;&#21407;&#21017;&#65306;&#29992;&#32479;&#35745;&#27979;&#35797;&#30830;&#23450;&#24102;&#26377;&#23569;&#37327;&#33410;&#28857;&#26631;&#31614;&#30340;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;GNE&#65307;&#65288;ii&#65289;&#26222;&#36941;&#19988;&#21487;&#35299;&#37322;&#65306;&#20272;&#35745;&#35266;&#23519;&#21040;&#30340;&#29305;&#23450;&#31867;&#22411;&#30340;GNE&#30340;&#38381;&#24335;&#35299;&#20915;&#26041;&#26696;&#65307;&#65288;iii&#65289;&#20934;&#30830;&#19988;&#21487;&#25193;&#23637;&#65306;&#38598;&#25104;GNE&#20197;&#23454;&#29616;&#20934;&#30830;&#19988;&#24555;&#36895;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a large graph with few node labels, how can we (a) identify whether there is generalized network-effects (GNE) of the graph or not, (b) estimate GNE to explain the interrelations among node classes, and (c) exploit GNE to improve downstream tasks such as predicting the unknown labels accurately and efficiently? The knowledge of GNE is valuable for various tasks like node classification and targeted advertising. However, identifying and understanding GNE such as homophily, heterophily or their combination is challenging in real-world graphs due to limited availability of node labels and noisy edges. We propose NetEffect, a graph mining approach to address the above issues, enjoying the following properties: (i) Principled: a statistical test to determine the presence of GNE in a graph with few node labels; (ii) General and Explainable: a closed-form solution to estimate the specific type of GNE observed; and (iii) Accurate and Scalable: the integration of GNE for accurate and fast
&lt;/p&gt;</description></item><item><title>FED-CD&#26159;&#19968;&#20010;&#32852;&#37030;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#65292;&#36866;&#24212;&#20849;&#20139;&#21644;&#19981;&#30456;&#20132;&#30340;&#24178;&#39044;&#21327;&#21464;&#37327;&#30340;&#22330;&#26223;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.03846</link><description>&lt;p&gt;
FED-CD&#65306;&#26469;&#33258;&#24178;&#39044;&#21644;&#35266;&#27979;&#25968;&#25454;&#30340;&#32852;&#21512;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
FED-CD: Federated Causal Discovery from Interventional and Observational Data. (arXiv:2211.03846v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03846
&lt;/p&gt;
&lt;p&gt;
FED-CD&#26159;&#19968;&#20010;&#32852;&#37030;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#20986;&#22240;&#26524;&#20851;&#31995;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#65292;&#36866;&#24212;&#20849;&#20139;&#21644;&#19981;&#30456;&#20132;&#30340;&#24178;&#39044;&#21327;&#21464;&#37327;&#30340;&#22330;&#26223;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#25968;&#25454;&#22312;&#38598;&#20013;&#20301;&#32622;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23454;&#38469;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#38480;&#21046;&#23545;&#23616;&#37096;&#23454;&#20307;&#25910;&#38598;&#30340;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#20027;&#35201;&#26159;&#20986;&#20110;&#38544;&#31169;&#21644;&#30417;&#31649;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FED-CD&#65292;&#36825;&#26159;&#19968;&#20010;&#32852;&#37030;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#21253;&#21547;&#35266;&#27979;&#25968;&#25454;&#21644;&#24178;&#39044;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#12290;&#36890;&#36807;&#20132;&#25442;&#26356;&#26032;&#32780;&#19981;&#26159;&#25968;&#25454;&#26679;&#26412;&#65292;FED-CD&#30830;&#20445;&#38544;&#31169;&#65292;&#21516;&#26102;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#30340;&#21457;&#29616;&#24213;&#23618;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAG&#65289;&#12290;&#25105;&#20204;&#36866;&#24212;&#20849;&#20139;&#25110;&#19981;&#30456;&#20132;&#30340;&#24178;&#39044;&#21327;&#21464;&#37327;&#30340;&#22330;&#26223;&#65292;&#24182;&#20943;&#36731;&#24178;&#39044;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#21512;&#25104;&#21644;&#30495;&#23454;DAG&#30340;FED-CD&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#30340;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing causal discovery methods typically require the data to be available in a centralized location. However, many practical domains, such as healthcare, limit access to the data gathered by local entities, primarily for privacy and regulatory constraints. To address this, we propose FED-CD, a federated framework for inferring causal structures from distributed datasets containing observational and interventional data. By exchanging updates instead of data samples, FED-CD ensures privacy while enabling decentralized discovery of the underlying directed acyclic graph (DAG). We accommodate scenarios with shared or disjoint intervened covariates, and mitigate the adverse effects of interventional data heterogeneity. We provide empirical evidence for the performance and scalability of FED-CD for decentralized causal discovery using synthetic and real-world DAGs.
&lt;/p&gt;</description></item></channel></rss>